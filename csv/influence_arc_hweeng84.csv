2009.iwslt-evaluation.14,I05-3025,1,0.891679,"follows: 1. Introduction This is the first year that the National University of Singapore (NUS) participated in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT). We submitted a run for the Chinese-English BTEC task1 , where we were ranked second out of twelve participating teams, based on the average of the normalized scores of ten automatic evaluation metrics. We adopted a phrase-based statistical machine translation (SMT) approach, and we investigated the effectiveness of different Chinese word segmentation standards. Using a maximum entropy model [1] and various data sources, we trained six different Chinese word segmenters. Each segmenter was then used to preprocess the Chinese side of the training/development/testing bi-texts, from which a separate phrase-based SMT system was built. Some of the resulting six systems yielded substantial translation performance gains as compared to a system that used the default segmentation provided by the organizers. Finally, we combined the output of all seven systems. The rest of this paper is organized as follows: Section 2 introduces the phrase-based SMT model, Section 3 presents our pre-processing"
2009.iwslt-evaluation.14,N03-1017,0,0.0069836,"Missing"
2009.iwslt-evaluation.14,P03-1021,0,0.0499017,"Missing"
2009.iwslt-evaluation.14,2002.tmi-tutorials.2,0,0.0814266,"Missing"
2009.iwslt-evaluation.14,P07-2045,0,0.0126909,"Missing"
2009.iwslt-evaluation.14,W04-1118,0,0.068797,"ned and used as follows: 4. Word Segmentation and Re-ranking In this section, we describe our experiments with different Chinese word segmentations and how we combine them into a single system using re-ranking. 4.1. Chinese Word Segmentation 1. We ran all seven candidate systems on the development data. The output included the English translation and thirteen associated scores from the SMT toolkit, which we used as features: Chinese word segmentation (CWS) has been shown conclusively as an essential step in machine translation, at least as far as current phrase-based SMT methods are concerned [6]. However, CWS is complicated by the fact that a word is not a well-defined concept in Chinese, where characters, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has b"
2009.iwslt-evaluation.14,2005.iwslt-1.18,0,0.037429,"efined concept in Chinese, where characters, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentat"
2009.iwslt-evaluation.14,P08-1115,0,0.0349163,"efined concept in Chinese, where characters, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentat"
2009.iwslt-evaluation.14,W08-0336,0,0.0478417,"ers, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentation provided by the IWSLT organizers,"
2009.iwslt-evaluation.14,W08-0335,0,0.0323456,"continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentation provided by the IWSLT organizers, and ICTCLAS-generated [11] segmentati"
2009.iwslt-evaluation.14,W03-1730,0,0.0713535,"erpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentation provided by the IWSLT organizers, and ICTCLAS-generated [11] segmentation respectively. Although ICTCLAS was also based on the PKU standard, the output seemed different enough from our PKU segmenter to be included as a separate candidate. (a) five (5) from the distortion model; (b) two (2) from the phrase translation model; (c) two (2) from the lexical translation model; (d) one (1) for the language model; (e) one (1) for the phrase penalty; (f) one (1) for the word penalty; and (g) one (1) for the final overall translation score (as calculated by Moses from all individual scores above and the MERT-tuned parameters). 2. A global fourteenth feature repe"
2009.iwslt-evaluation.14,P09-1104,0,0.0301126,"d various parameters of the phrase-based SMT system. We further describe a novel retraining technique yielding sizeable improvements in BLEU. 5.1. Parameter Tuning 1. We used the training bi-text to build a phrase table and to train an English language model. The Moses phrase-based SMT toolkit has a large number of options. While it comes with very sensible defaults, we found experimentally that varying some of them had a significant impact on the translation quality. Table 1 shows some non-standard settings used in our submission. Note that, for word alignments, we used the Berkeley Aligner2 [12] in unsupervised mode, which we found to outperform GIZA++ significantly. We used the default parameters of the aligner, except that we increased the number of iterations to 40. 2. We used the development dataset to tune the weights of the log-linear model of the phrase-based SMT system using MERT. 3. We concatenated the training and the development datasets; we then re-built the phrase table and retrained the language model on this new dataset. 4. We repeated the above three steps for each of the seven Chinese word segmenters, thus obtaining seven candidate systems. 5.2. Re-training on the De"
2009.iwslt-evaluation.14,P07-1005,1,0.881248,"Missing"
2020.acl-main.90,D18-1241,0,0.225871,"apture topic continuity and topic shift while scoring a particular candidate follow-up question. Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins. 1 Figure 1: Examples illustrating the follow-up question identification task. Introduction Conversational question answering (QA) mimics the process of natural human-to-human conversation. Recently, conversational QA has gained much attention, where a system needs to answer a series of interrelated questions from an associated text passage or a structured knowledge graph (Choi et al., 2018; Reddy et al., 2019; Saha et al., 2018). However, most conversational QA tasks do not explicitly focus on requiring a model to identify the follow-up questions. A practical conversational QA system must possess the ability to understand the conversation history well, and to identify whether the current question is a follow-up of that particular conversation. Consider a user who is trying to have a conversation with a machine (e.g., Siri, Google Home, Alexa, Cortana, etc). First, the user asks a question and the machine answers it. When the user asks the second question, it is very important f"
2020.acl-main.90,D17-1070,0,0.0652975,"ce, we sample invalid follow-up questions from two sources: 960 LIF #Instances Avg #prev QA Avg passage len Avg question len Avg answer len Avg FUQ† len 1. Questions from other conversations in QuAC which can serve as potential distractors, and 2. Non-follow-up questions from the same conversation in QuAC which occurs after the gold valid follow-up question. The sampling from the first source involves a two-step filtering process. We first compare the cosine similarity between the associated passage and all the questions from the other conversations by using embeddings generated by InferSent (Conneau et al., 2017). We take the top 200 questions based on higher similarity scores. In the second step, we concatenate the gold valid candidate follow-up question with the question-answer pairs in the conversation history to form an augmented follow-up question. Then, we calculate the token overlap count between each ranked question obtained in the first step and the augmented follow-up question. We normalize the token overlap count by dividing it by the length of the ranked question (after removing stop words). For each valid instance, we fix a threshold and take at least one but up to two questions with the"
2020.acl-main.90,N19-1423,0,0.00819066,"For CNN, we use equal numbers of unigram, bigram, and trigram filters, and the outputs are concatenated to obtain the final encoding. Next, we apply either global max-pooling or attentive pooling to obtain an aggregated vector representation, followed by a feed-forward layer to score the candidate follow-up question. Let the sequence encoding of the concatenated text be E ∈ RL×H , and et be the tth row ˜ ∈ RH for attentiveof E. The aggregated vector e pooling can be obtained as: ˜ = aE , at ∝ exp(et w> ) ; e (7) where w ∈ RH is a learnable vector. We also develop a baseline model using BERT (Devlin et al., 2019). We first concatenate all the inputs and then apply BERT to derive the contextual vectors. Next, we aggregate them into a single vector using attention. Then a feed-forward layer is used to score each candidate follow-up question. 6 Experiments In this section, we present the experimental settings, results, and performance analysis. 6.1 Experimental Settings We do not update the GloVe vectors during training. We use 100-dimension character-level embedding vectors. The number of hidden units in all the LSTMs is 150 (H = 300). We use dropout (Srivastava et al., 2014) with probability 0.3. Follo"
2020.acl-main.90,W13-4073,0,0.0468908,"Missing"
2020.acl-main.90,P82-1020,0,0.765713,"Missing"
2020.acl-main.90,W06-3001,0,0.167142,"Missing"
2020.acl-main.90,D14-1181,0,0.00369694,"ore each candidate follow-up question. 4.1 We then jointly encode the passage and the conversation history. We apply a row-wise softmax function on A to obtain R ∈ RT ×U . Now, for all the passage words, the aggregated representation of the conversation history is given as G = RQ ∈ RT ×H . The aggregated vectors corresponding to the passage words in G are then concatenated with the passage vectors in D, followed by another BiLSTM to obtain a joint representation V ∈ RT ×H . 4.1.2 Three-Way Attentive Pooling Network Embedding and Encoding We use both character and word embeddings2 . Similar to Kim (2014), we obtain the character-level 1 The source code and data are released at https:// github.com/nusnlp/LIF 2 We also experimented with ELMO and BERT but did not observe any consistent improvement. Joint Encoding Multi-Factor Attention In addition, multi-factor self-attentive encoding (Kundu and Ng, 2018) is applied on the joint representation. If m represents the number of factors, multi-factor attention F[1:m] ∈ RT ×m×T is formulated as: [1:m] F[1:m] = VWf [1:m] V> (1) where Wf ∈ RH×m×H is a 3-way tensor. A max-pooling operation is performed on F[1:m] , over the number of factors, resulting in"
2020.acl-main.90,2007.sigdial-1.8,0,0.0443497,"nuity from the first question-answer pair (i.e., first film is Flesh and Blood) and the topic shift from the second question-answer pair (i.e., genre of films) of the conversation history. The candidate followup question in the second example is invalid since the associated passage does not provide any information about his final years. The last follow-up question example is invalid since Verhoeven is a he, not she. There has been some research in the past which focuses on identifying what part of the conversation history is important for processing follow-up questions (Bertomeu et al., 2006; Kirschner and Bernardi, 2007). However, the recently proposed neural network-based models for conversational QA have not explicitly focused on follow-up questions. In this paper, we propose a three-way attentive pooling network for follow-up question identification in a conversational reading comprehension setting. It evaluates each candidate follow-up question based on two perspectives – topic shift and topic continuity. The proposed model makes use of two attention matrices, which are conditioned over the associated passage, to capture topic shift in a follow-up question. It also relies on another attention matrix to ca"
2020.acl-main.90,D16-1127,0,0.105461,"Missing"
2020.acl-main.90,D14-1162,0,0.0822698,"Missing"
2020.acl-main.90,Q19-1016,0,0.105239,"uity and topic shift while scoring a particular candidate follow-up question. Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins. 1 Figure 1: Examples illustrating the follow-up question identification task. Introduction Conversational question answering (QA) mimics the process of natural human-to-human conversation. Recently, conversational QA has gained much attention, where a system needs to answer a series of interrelated questions from an associated text passage or a structured knowledge graph (Choi et al., 2018; Reddy et al., 2019; Saha et al., 2018). However, most conversational QA tasks do not explicitly focus on requiring a model to identify the follow-up questions. A practical conversational QA system must possess the ability to understand the conversation history well, and to identify whether the current question is a follow-up of that particular conversation. Consider a user who is trying to have a conversation with a machine (e.g., Siri, Google Home, Alexa, Cortana, etc). First, the user asks a question and the machine answers it. When the user asks the second question, it is very important for the machine to un"
2020.acl-main.90,D18-1233,0,0.039678,"Missing"
2020.acl-main.90,N15-1020,0,0.0551861,"Missing"
2020.acl-main.90,W13-4065,0,0.0303092,"Missing"
2020.coling-main.227,P10-1131,0,0.00963561,"ohnson (2016) use two large corpora containing more than 700k sentences. Mareˇcek and Straka (2013) utilize a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Han et al. (2017) use a subset of the BLLIP corpus that contains around 180k sentences. With the advancement of computing power and deep neural models, we expect to see more future work on training with big data. 4.5 Unsupervised Multilingual Parsing To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously (Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example, Berg-Kirkpatrick and Klein (2010) propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of unsupervised dependency parser from t"
2020.coling-main.227,Q13-1007,0,0.0138753,"e to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV. 3.1.2 Inference Given a model parameter"
2020.coling-main.227,D10-1117,0,0.198748,"of the child tokens already generated from a head token. Headden III et al. (2009) propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically l"
2020.coling-main.227,D17-1171,1,0.910925,"s is typically employed as the learning objective function for autoencoder models. For a training dataset including N sentences X = {x1 , x2 , ..., xN }, the objective function is as follows: L(Θ) = N X log P(ˆ x(i) |x(i) ; Θ) (5) i=1 where Θ is the model parameter and x ˆ(i) is a copy of x(i) representing the reconstructed sentence1 . In some cases, there is an additional regularization term (e.g., L1) of Θ. 1 In Han et al. (2019a), x is the word sequence, while x ˆ is the POS tag sequence of the same sentence. 2526 The first autoencoder model for unsupervised dependency parsing, proposed by Cai et al. (2017), is based on the conditional random field autoencoder framework (CRFAE). The encoder is a first-order graph-based discriminative dependency parser mapping an input sentence to the space of dependency trees. The decoder independently generates each token of the reconstructed sentence conditioned on the head of the token specified by the dependency tree. Both the encoder and the decoder are arc-factored, meaning that the encoding and decoding probabilities can be factorized by dependency arcs. Coordinate descent is applied to minimize the reconstruction loss and alternately updates the encoder"
2020.coling-main.227,N09-1009,0,0.056112,"Missing"
2020.coling-main.227,N19-1423,0,0.0363274,"nd Smith (2012) 64.3 Tu and Honavar (2012) 71.4 Bisk and Hockenmaier (2012) 71.5 Spitkovsky et al. (2013) 72.0 Jiang et al. (2016) 72.5 Han et al. (2017) 75.1 He et al. (2018)* 60.2 Discriminative Approaches Daum´e III (2009) Le and Zuidema (2015) † 73.2 Cai et al. (2017) 71.7 Li et al. (2019) 54.7 Han et al. (2019a) 75.6 59.1 53.1 57.0 53.3 64.4 57.6 59.5 47.9 45.4 65.8 55.7 37.8 61.4 Table 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sentences of length ≤ 10 and all lengths. *: without gold POS tags. †: with more training data in addition to WSJ. (Devlin et al., 2019) are even more informative, capturing contextual information. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing. 4.4 Big Data Although unsupervised parsing does not require syntactically annotated training corpora and can theoretically use almost unlimited raw texts for tra"
2020.coling-main.227,K15-1012,0,0.0224171,"ependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but al"
2020.coling-main.227,N16-1024,0,0.0310519,"follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema. Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure. Recurrent Neural Network Grammars (RNNG) (Dyer et al., 2016) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNN"
2020.coling-main.227,P10-2036,0,0.050584,"Missing"
2020.coling-main.227,N12-1069,0,0.0533256,"Missing"
2020.coling-main.227,P15-1133,0,0.0159582,"sampling algorithm. The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector. 2527 3.2.3 Other Discriminative Approaches Apart from the approaches based on autoencoder and variational autoencoder, there are also a few other discriminative approaches based on discriminative clustering (Grave and Elhadad, 2015), self-training (Le and Zuidema, 2015), or searching (Daum´e III, 2009). Because of space limit, below we only introduce the approach based on discriminative clustering called Convex MST (Grave and Elhadad, 2015). Convex MST employs a first-order graph-based discriminative parser. It searches for the parses of all the training sentences and learns the parser simultaneously, with a learning objective that the searched parses are close to the predicted parses by the parser. In other words, the parses should be easily predictable by the parser. The objective function can be relaxed to become conv"
2020.coling-main.227,D17-1176,1,0.909602,"tive approaches all make use of neural networks (Li et al., 2019; Corro and Titov, 2018). 4.3 Lexicalization In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict dependency probabilities that are automatically smoothed. In p"
2020.coling-main.227,P19-1526,1,0.643102,"algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches with more complicated objectives often require more advanced learning algorithms, but many of the algorithms can still be seen as extensions of the EM algorithm that revise either the Estep (e.g., to update Q(z) based on posterior regularization terms) or the M-step (e.g., to optimize the posterior probability that incorporates parameter priors). 2525 Autoencoder Variational Autoencoder CRFAE (Cai et al., 2017) D-NDMV (Han et al., 2019a) Deterministic Variant (Li et al., 2019) D-NDMV (Han et al., 2019a) Variational Variant (Corro and Titov, 2018) Intermediate Representation Z Encoder Decoder P (z|x) P (ˆ x|z) S P (s|x) P (z, x ˆ|s) Z P (z|x) P (z, x) S P (s|x) P (z, x|s) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation. x ˆ is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x. In addition to the EM algorithm,"
2020.coling-main.227,D19-1576,1,0.272781,"algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches with more complicated objectives often require more advanced learning algorithms, but many of the algorithms can still be seen as extensions of the EM algorithm that revise either the Estep (e.g., to update Q(z) based on posterior regularization terms) or the M-step (e.g., to optimize the posterior probability that incorporates parameter priors). 2525 Autoencoder Variational Autoencoder CRFAE (Cai et al., 2017) D-NDMV (Han et al., 2019a) Deterministic Variant (Li et al., 2019) D-NDMV (Han et al., 2019a) Variational Variant (Corro and Titov, 2018) Intermediate Representation Z Encoder Decoder P (z|x) P (ˆ x|z) S P (s|x) P (z, x ˆ|s) Z P (z|x) P (z, x) S P (s|x) P (z, x|s) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation. x ˆ is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x. In addition to the EM algorithm,"
2020.coling-main.227,D18-1160,0,0.676031,"the neural network in order to compute sentence-specific rule probabilities. Compared with generative approaches, it is more natural for discriminative approaches to use neural networks to score dependencies or parsing actions, so recent discriminative approaches all make use of neural networks (Li et al., 2019; Corro and Titov, 2018). 4.3 Lexicalization In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much large"
2020.coling-main.227,P19-1311,0,0.0174769,"dency parsing can serve as the inspiration for studies of other unsupervised tasks, especially unsupervised structured prediction tasks. A recent example is Nishida and Nakayama (2020), who study unsupervised discourse parsing (inducing discourse structures for a given text) by borrowing techniques from unsupervised parsing such as Viterbi EM and heuristically designed initialization. Unsupervised dependency parsing techniques can also be used as building blocks for transfer learning of parsers. Some of the approaches discussed in this paper have already been applied to cross-lingual parsing (He et al., 2019; Li and Tu, 2020), and more such endeavors are expected in the future. 6.3 Interpretability One prominent problem of deep neural networks is that they act as black boxes and are generally not interpretable. How to improve the interpretability of neural networks is a research topic that gains much attention recently. For natural language texts, their linguistic structures reveal important information of the texts and at the same time can be easily understood by human. It is therefore an interesting direction to integrate techniques of unsupervised parsing into various neural models of NLP task"
2020.coling-main.227,N09-1012,0,0.110825,"Missing"
2020.coling-main.227,D16-1073,1,0.963124,"g, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV. 3.1.2 Inference Given a model parameterized by Θ and a sentence x, the model predicts the parse z∗ with the highest probability. z∗ = arg max P (x, z; Θ) z∈Z(x) (1) where Z(x) is the set of all valid dependency trees of the sentence x. Due to the independence assumptions made by generative models, the inference problem can be efficiently solved exactly in most cases. For example, chart parsing can be used for DMV. 2524 3.1.3 Learning Objective Log marginal likelihood is typically employed as the ob"
2020.coling-main.227,D17-1177,1,0.84753,"unction can be relaxed to become convex and then can be optimized exactly. 3.2.4 Pros and Cons Discriminative models are capable of accessing global features from the whole input sentence and are typically more expressive than generative models. On the other hand, discriminative approaches are often more complicated and do not admit tractable exact inference. 4 4.1 Recent Trends Combined Approaches Generative approaches and discriminative approaches have different pros and cons. Therefore, a natural idea is to combine the strengths of the two types of approaches to achieve better performance. Jiang et al. (2017) propose to jointly train two state-of-the-art models of unsupervised dependency parsing, the generative LC-DMV (Noji et al., 2016) and the discriminative Convex MST, with the dual decomposition technique that encourages the two models to gradually influence each other during training. 4.2 Neural Parameterization Traditional generative approaches either directly learn or use manually-designed features to compute dependency rule probabilities. Following the recent rise of deep learning in the field of NLP, Jiang et al. (2016) propose to predict dependency rule probabilities using a neural netwo"
2020.coling-main.227,D19-1148,1,0.821377,"than 700k sentences. Mareˇcek and Straka (2013) utilize a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Han et al. (2017) use a subset of the BLLIP corpus that contains around 180k sentences. With the advancement of computing power and deep neural models, we expect to see more future work on training with big data. 4.5 Unsupervised Multilingual Parsing To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously (Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example, Berg-Kirkpatrick and Klein (2010) propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of unsupervised dependency parser from the knowledge of another language by us"
2020.coling-main.227,P04-1061,0,0.756548,"d word. In unsupervised dependency parsing, the goal is to obtain a dependency parser without using annotated sentences. Some work requires no training data and derives dependency trees from centrality or saliency information (Søgaard, 2012). We focus on learning a dependency parser from an unannotated dataset that consists of a set of sentences without any parse tree annotation. In many cases, part-of-speech (POS) tags of the words in the training sentences are assumed to be available during training. Two evaluation metrics are widely used in previous work of unsupervised dependency parsing (Klein and Manning, 2004): directed dependency accuracy (DDA) and undirected dependency accuracy (UDA). DDA denotes the percentage of correctly predicted dependency edges, while UDA is similar to DDA but disregards the directions of edges when evaluating their correctness. 2.2 Related Areas Supervised Dependency Parsing Supervised dependency parsing aims to train a dependency parser from training sentences that are manually annotated with their dependency parse trees. Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches. A graph-based dependency"
2020.coling-main.227,N15-1067,0,0.0167662,"D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector. 2527 3.2.3 Other Discriminative Approaches Apart from the approaches based on autoencoder and variational autoencoder, there are also a few other discriminative approaches based on discriminative clustering (Grave and Elhadad, 2015), self-training (Le and Zuidema, 2015), or searching (Daum´e III, 2009). Because of space limit, below we only introduce the approach based on discriminative clustering called Convex MST (Grave and Elhadad, 2015). Convex MST employs a first-order graph-based discriminative parser. It searches for the parses of all the training sentences and learns the parser simultaneously, with a learning objective that the searched parses are close to the predicted parses by the parser. In other words, the parses should be easily predictable by the parser. The objective function can be relaxed to become convex and then can be optimized exactly."
2020.coling-main.227,2020.findings-emnlp.193,1,0.735047,"serve as the inspiration for studies of other unsupervised tasks, especially unsupervised structured prediction tasks. A recent example is Nishida and Nakayama (2020), who study unsupervised discourse parsing (inducing discourse structures for a given text) by borrowing techniques from unsupervised parsing such as Viterbi EM and heuristically designed initialization. Unsupervised dependency parsing techniques can also be used as building blocks for transfer learning of parsers. Some of the approaches discussed in this paper have already been applied to cross-lingual parsing (He et al., 2019; Li and Tu, 2020), and more such endeavors are expected in the future. 6.3 Interpretability One prominent problem of deep neural networks is that they act as black boxes and are generally not interpretable. How to improve the interpretability of neural networks is a research topic that gains much attention recently. For natural language texts, their linguistic structures reveal important information of the texts and at the same time can be easily understood by human. It is therefore an interesting direction to integrate techniques of unsupervised parsing into various neural models of NLP tasks, such that the n"
2020.coling-main.227,2020.acl-main.300,1,0.770267,"ncy tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but also nodes of a tree. Consequently, there have been far more papers in unsupervised dependency parsing than in unsupervised constituency parsing over the past decade. More recently, however, there is a surge in interest in unsupervised constituency parsing and several novel approaches were proposed in the past two years (Li et al., 2020). While we focus on unsupervised dependency parsing in this paper, most of our discussions on the classification of approaches and recent trends apply to unsupervised constituency parsing as well. Latent Tree Models with Downstream Tasks Latent tree models treat the parse tree as a latent variable that is used in downstream tasks such as sentiment classification. While no treebank is used in training, these models rely on the performance of the downstream tasks to guide the learning of the latent parse trees. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick ("
2020.coling-main.227,P13-1105,0,0.0406975,"Missing"
2020.coling-main.227,P14-1126,0,0.0191202,"transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce"
2020.coling-main.227,P13-1028,0,0.045653,"Missing"
2020.coling-main.227,D12-1028,0,0.049334,"Missing"
2020.coling-main.227,H05-1066,0,0.398895,"Missing"
2020.coling-main.227,D11-1006,0,0.0357848,"onald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because"
2020.coling-main.227,D10-1120,0,0.243154,"we mentioned earlier, the joint probability of a sentence and its dependency tree can be decomposed into the product of the probabilities of the components in the dependency tree. Apart from the vanilla marginal likelihood, priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibi"
2020.coling-main.227,2020.tacl-1.15,0,0.0780453,"dency parsing and dependency grammar induction) is the most challenging, which aims to obtain a dependency parser without using annotated sentences. Despite its difficulty, unsupervised parsing is an interesting research direction, not only because it would reveal ways to utilize almost unlimited text data without the need for human annotation, but also because it can serve as the basis for studies of transfer and semi-supervised learning of parsers. The techniques developed for unsupervised dependency parsing could also be utilized for other NLP tasks, such as unsupervised discourse parsing (Nishida and Nakayama, 2020). In addition, research in unsupervised parsing inspires and verifies cognitive research of human language acquisition. In this paper, we conduct a survey of unsupervised dependency parsing research. We first introduce the definition and evaluation metrics of unsupervised dependency parsing, and discuss research areas related to it. Then we present in detail two major classes of approaches to unsupervised dependency parsing: generative approaches and discriminative approaches. Finally, we discuss important new techniques and setups of unsupervised dependency parsing that appear in recent years"
2020.coling-main.227,D16-1004,0,0.675614,"Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Learning Algorithm The Expectation-Maximization (EM) algorithm is typically used to optimize log marginal likelihood. For each sentence, the EM algorithm aims to maximize the following lower-bound of the objective function and alternates between the E-step and M-step. log P (x; Θ) − KL(Q(z)kP (z|x, Θ)) (4) where Q(z) is an auxiliary distribution with regard to z. In the E-step, Θ is fixed and Q(z) is set to P (z|x, Θ). A set of so-called expected counts can be derived from Q(z) to faci"
2020.coling-main.227,C16-1003,0,0.0830307,"dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict dependency probabilities that are automatically smoothed. In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. Recently"
2020.coling-main.227,N18-1202,0,0.00783051,"pproaches listed in this table may use different training sets and different external 2529 knowledge in their experiments, and one should check the corresponding papers to understand such differences before comparing these accuracies. While the accuracy of unsupervised dependency parsing has increased by over thirty points in the last fifteen years, it is still well below that of supervised models, which leaves much room for improvement and challenges for future research. 6 6.1 Future Directions Utilization of Syntactic Information in Pretrained Language Modeling Pretrained language modeling (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained langua"
2020.coling-main.227,P07-1049,0,0.0488629,"of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict dependency probabilities that are automatically smoothed. In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similaritie"
2020.coling-main.227,P06-1072,0,0.00842549,", x(2) , ..., x(N ) }: L(Θ) = N X log P(x(i) ; Θ) (2) i=1 where the model parameters are denoted by Θ. The likelihood of each sentence x is as follows: X P (x; Θ) = P (x, z; Θ) (3) z∈Z(x) where Z(x) is the set of all valid dependency trees of sentence x. As we mentioned earlier, the joint probability of a sentence and its dependency tree can be decomposed into the product of the probabilities of the components in the dependency tree. Apart from the vanilla marginal likelihood, priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce"
2020.coling-main.227,N10-1116,0,0.198444,"xiliary distribution with regard to z. In the E-step, Θ is fixed and Q(z) is set to P (z|x, Θ). A set of so-called expected counts can be derived from Q(z) to facilitate the subsequent Mstep and they are typically calculated using the inside-outside algorithm. In the M-step, Θ is optimized based on the expected counts with Q(z) fixed. There are a few variants of the EM algorithm. If Q(z) represents a point-estimation (i.e., the best dependency tree has a probability of 1), the algorithm becomes hard-EM or Viterbi EM, which is found to outperform standard EM in unsupervised dependency parsing (Spitkovsky et al., 2010b). SoftmaxEM (Tu and Honavar, 2012) falls between EM (considering all possible dependency trees) and hard-EM (only considering the best dependency tree), applying a softmax-like transformation to Q(z). During the EM iterations, an annealing schedule (Tu and Honavar, 2012) can be used to gradually shift from hardEM to softmax-EM and finally to the EM algorithm, which leads to better performance than sticking to a single algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches"
2020.coling-main.227,W10-2902,0,0.447046,"xiliary distribution with regard to z. In the E-step, Θ is fixed and Q(z) is set to P (z|x, Θ). A set of so-called expected counts can be derived from Q(z) to facilitate the subsequent Mstep and they are typically calculated using the inside-outside algorithm. In the M-step, Θ is optimized based on the expected counts with Q(z) fixed. There are a few variants of the EM algorithm. If Q(z) represents a point-estimation (i.e., the best dependency tree has a probability of 1), the algorithm becomes hard-EM or Viterbi EM, which is found to outperform standard EM in unsupervised dependency parsing (Spitkovsky et al., 2010b). SoftmaxEM (Tu and Honavar, 2012) falls between EM (considering all possible dependency trees) and hard-EM (only considering the best dependency tree), applying a softmax-like transformation to Q(z). During the EM iterations, an annealing schedule (Tu and Honavar, 2012) can be used to gradually shift from hardEM to softmax-EM and finally to the EM algorithm, which leads to better performance than sticking to a single algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches"
2020.coling-main.227,D11-1118,0,0.371112,"priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Lea"
2020.coling-main.227,W11-0303,0,0.356416,"priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Lea"
2020.coling-main.227,D11-1117,0,0.334101,"priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Lea"
2020.coling-main.227,D12-1063,0,0.0162904,"odel with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden III et al. (2009) propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. Fo"
2020.coling-main.227,D13-1204,0,0.17345,"addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV. Better learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach. 3.1.5 Pros and Cons It is often straightforward to incorporate various inductive biases and manually-designed local features into generative approaches. Moreover, generative models can be easily trained via the EM algorithm and its extensions. On the other hand, generative models often have limited expressive power because of the independence assumptions they m"
2020.coling-main.227,D12-1121,1,0.917515,"biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Learning Algorithm The Expectation-Maximization (EM) algorithm is typically used to optimize log marginal like"
2020.coling-main.227,2020.coling-main.347,1,0.836207,"has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden III et al. (2009) propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute ge"
2020.coling-main.227,W15-2201,0,0.026376,"of its edges (McDonald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dep"
2020.coling-main.227,P19-1230,0,0.0218836,"Utilization of Syntactic Information in Pretrained Language Modeling Pretrained language modeling (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained language models have not been widely used in unsupervised dependency parsing. One major concern is that pretrained language models are too informative and may make unsupervised models more prone to overfitting. Besides, massive syntactic and semantic information is encoded in pretrained language models and how to extract the syntactic part from them is a challenging task. 6.2 Inspiration for Other Tasks Unsupervised dependency parsing is a classic unsupervised learning task. Many techniques developed fo"
2020.coling-main.371,P19-1299,0,0.0248486,"of classification-based metrics and 4208 distribution-based metrics. Pretrained language models have been employed in several dialogue tasks and show good performance. BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) have been utilized to encode system and user utterances in dialogue state tracking (Gao et al., 2019; Ma et al., 2020; Lee et al., 2019). Liu et al. (2020) also incorporated BERT-based contextualized word embeddings for dialogue generation in chat-oriented dialogue systems. Recently, cross-lingual transfer learning has gained much popularity in the NLP research community (Chen et al., 2019). Cross-lingual pretrained language models are able to map words from different languages to one single shared embedding space. Empirical results show that cross-lingual language models trained on one hundred languages outperform language-specific language models on several standard NLP benchmarks (Conneau and Lample, 2019; Conneau et al., 2020). However, cross-lingual transfer learning has not been explored in dialogue breakdown detection, and our work is the first to incorporate a pretrained cross-lingual language model for dialogue breakdown detection. On the one hand, we use cross-lingual"
2020.coling-main.371,D18-1269,0,0.0126954,"ture-based approaches (Wang et al., 2019), the features are extracted from the concatenation of both the dialogue history and the last system response, while in other models using pretrained word embeddings (Hendriksen et al., 2019; Shin et al., 2019; Sugiyama, 2019), the interaction between the dialogue history and the last system response is also not explicitly captured. Recently, cross-lingual language models such as XLM (Conneau and Lample, 2019) and XLMR (Conneau et al., 2020) demonstrate strong performance on many cross-lingual natural language processing (NLP) tasks (Wang et al., 2018; Conneau et al., 2018). They also outperform pretrained monolingual language models on tasks with low-resource languages. By utilizing shared word embeddings over different languages and multilingual parallel texts, cross-lingual language models encode input texts into one single representation space shared by all languages. This removes the cost of language-specific training. In this paper, we utilize pretrained cross-lingual language models to benefit from the multilingual training data. To the best of our knowledge, ours is the first work to incorporate pretrained cross-lingual language models in dialogue breakd"
2020.coling-main.371,2020.acl-main.747,0,0.0283944,"Missing"
2020.coling-main.371,N19-1423,0,0.380564,"s assigned a gold-standard (majority) class, and a probability distribution based on all annotators’ predictions. The task has two tracks involving two different languages: Japanese and English. The evaluation metrics consist of both classification-related metrics and distribution-related metrics. We will introduce them in detail in Section 4.3. Prior work has exploited feature-engineered machine learning approaches like random forests, neural network architectures such as LSTM (Hendriksen et al., 2019; Shin et al., 2019; Wang et al., 2019), and the monolingual pretrained language model BERT (Devlin et al., 2019; Sugiyama, 2019). Most prior work treats the dialogue history and the last system response in the same manner. In feature-based approaches (Wang et al., 2019), the features are extracted from the concatenation of both the dialogue history and the last system response, while in other models using pretrained word embeddings (Hendriksen et al., 2019; Shin et al., 2019; Sugiyama, 2019), the interaction between the dialogue history and the last system response is also not explicitly captured. Recently, cross-lingual language models such as XLM (Conneau and Lample, 2019) and XLMR (Conneau et al., 2"
2020.coling-main.371,L16-1502,0,0.023178,"omputational Linguistics, pages 4201–4210 Barcelona, Spain (Online), December 8-13, 2020 challenges faced by current chat-oriented dialogue systems, but it has still not been carefully studied by the research community. The dialogue breakdown detection task is designed to test a system’s capability of identifying the undesired utterance causing a dialogue breakdown, which is expected to further help us to build more fluent dialogue systems. The dialogue breakdown detection task requires a participating system to determine whether an utterance generated by a system causes a dialogue breakdown (Higashinaka et al., 2016). Table 1 shows an example of a dialogue breakdown in an ongoing conversation. In this example, the last system response causes a dialogue breakdown, since it does not answer the question “As a what?” in the last user utterance but instead gives a completely irrelevant response “I have a lot of friends.” Dialogue Breakdown Detection Challenge 4 (DBDC4)1 is a shared task dedicated to dialogue breakdown detection. In this paper, we use the dataset released in DBDC4 for our experiments. Since whether a system utterance causes a dialogue breakdown is somewhat subjective, the task is modeled as a c"
2020.coling-main.371,P19-1546,0,0.0285144,"bution-based metrics in either the Japanese or English track. This indicates that it is challenging for a single model to perform well on all metrics. The desired model should possess the capability to alleviate the mismatch between the training objective of classification-based metrics and 4208 distribution-based metrics. Pretrained language models have been employed in several dialogue tasks and show good performance. BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) have been utilized to encode system and user utterances in dialogue state tracking (Gao et al., 2019; Ma et al., 2020; Lee et al., 2019). Liu et al. (2020) also incorporated BERT-based contextualized word embeddings for dialogue generation in chat-oriented dialogue systems. Recently, cross-lingual transfer learning has gained much popularity in the NLP research community (Chen et al., 2019). Cross-lingual pretrained language models are able to map words from different languages to one single shared embedding space. Empirical results show that cross-lingual language models trained on one hundred languages outperform language-specific language models on several standard NLP benchmarks (Conneau and Lample, 2019; Conneau et al., 2"
2020.coling-main.371,2020.acl-main.131,0,0.0221392,"s in either the Japanese or English track. This indicates that it is challenging for a single model to perform well on all metrics. The desired model should possess the capability to alleviate the mismatch between the training objective of classification-based metrics and 4208 distribution-based metrics. Pretrained language models have been employed in several dialogue tasks and show good performance. BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) have been utilized to encode system and user utterances in dialogue state tracking (Gao et al., 2019; Ma et al., 2020; Lee et al., 2019). Liu et al. (2020) also incorporated BERT-based contextualized word embeddings for dialogue generation in chat-oriented dialogue systems. Recently, cross-lingual transfer learning has gained much popularity in the NLP research community (Chen et al., 2019). Cross-lingual pretrained language models are able to map words from different languages to one single shared embedding space. Empirical results show that cross-lingual language models trained on one hundred languages outperform language-specific language models on several standard NLP benchmarks (Conneau and Lample, 2019; Conneau et al., 2020). However, cros"
2020.coling-main.371,D14-1162,0,0.0850568,"nging for the model to distinguish an undesired response from a sarcastic but appropriate response. In these cases, our model is most likely to classify them as Possible Breakdown (PB). Overly long responses While the target utterance consists of multiple sentences, it is also challenging to capture how they are interacting with the dialogue history. 6 Related Work Several prior works on the dialogue breakdown detection task are based on long short-term memory (LSTM). Hendriksen et al. (2019) incorporated LSTM with pretrained word embeddings, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Wang et al. (2019) followed a similar idea but added convolutional neural network (CNN) to perform textual feature extraction. Shin et al. (2019) utilized bidirectional LSTM and self-attention layers to incorporate the dialogue in representation learning. Prior works also include feature-engineered models. Wang et al. (2019) proposed to utilize a curated set of features including keyword counts and TF-IDF scores to build a regression model based on random forests. Pretrained language models achieve state-of-the-art performance on many NLP tasks. Sugiyama (2019) developed a model based on BER"
2020.coling-main.371,W18-5446,0,0.0117975,"same manner. In feature-based approaches (Wang et al., 2019), the features are extracted from the concatenation of both the dialogue history and the last system response, while in other models using pretrained word embeddings (Hendriksen et al., 2019; Shin et al., 2019; Sugiyama, 2019), the interaction between the dialogue history and the last system response is also not explicitly captured. Recently, cross-lingual language models such as XLM (Conneau and Lample, 2019) and XLMR (Conneau et al., 2020) demonstrate strong performance on many cross-lingual natural language processing (NLP) tasks (Wang et al., 2018; Conneau et al., 2018). They also outperform pretrained monolingual language models on tasks with low-resource languages. By utilizing shared word embeddings over different languages and multilingual parallel texts, cross-lingual language models encode input texts into one single representation space shared by all languages. This removes the cost of language-specific training. In this paper, we utilize pretrained cross-lingual language models to benefit from the multilingual training data. To the best of our knowledge, ours is the first work to incorporate pretrained cross-lingual language mo"
2020.emnlp-main.599,W19-1909,0,0.0601924,"Missing"
2020.emnlp-main.599,P07-1056,0,0.721381,"mer 3.2.1 ? ?! ? ? ??? ?????????? Transformer Embedding layer 0 ? ? ??? Following BERT (Devlin et al., 2019), most PrLMs consist of an embedding layer and several transformer layers. Suppose a PrLM has L + 1 layers, layer 0 is the embedding layer, and layer L is the last layer. Given an input sentence x = [w1 , w2 , · · · , w|x |], the embedding layer of the PrLM will encode x as: h0 = Embedding(x) ???? Figure 1: Illustration of our model architecture which includes a pre-trained language model, a feature adaptation module, and a classifier. domain-specific words to reduce domain discrepancy (Blitzer et al., 2007; Pan et al., 2010; He et al., 2011). 3 Pre-trained Language Model Preliminary In this section, we introduce the problem definition and the model architecture based on which we build our domain adaptation algorithm presented in the next section. (1) |x| where h0 = [h10 , h20 , · · · , h0 ]. After obtaining the embeddings of the input sentence, we compute the features of the sentence from the transformer blocks of PrLM. In layer l, we compute the transformer feature as: hl = Transformerl (hl−1 ) (2) |x| , hl ] where hl = [h1l , h2l , · · · and l ∈ {1, 2, · · · , L}. Using all the |x |features w"
2020.emnlp-main.599,P19-1299,0,0.0307874,"n. MMD adopts the Maximum Mean Discrepancy loss (Gretton et al., 2012) in which Gaussian Kernel is implemented. Adv (Ganin et al., 2016; Chen et al., 2018) adversarially trains a domain classifier to learn domaininvariant features by reversing the gradients from the domain classifier following Ganin et al. (2016). p is our self-training method introduced in §4.1. p+CFd is our full model that uses CFd to enhance the robustness of self-training. DAS (He et al., 2018) uses semi-supervised learning. CLDFA (Xu and Yang, 2017) is a cross-lingual baseline which uses cross-lingual resources. MAN-MoE (Chen et al., 2019) studies multi-lingual transfer which has multiple languages in the source domain. MoE learns to focus on more transferable source domains for adaptation. xlmr-10, KL, MMD, Adv, p, and p+CFd are all based on the multi-layer representations with last 10-layer features. For KL, MMD, and Adv, to minimize domain discrepancy, we use an unlabeled set of the same size in the source domain as the target domain. xlmr-tuning3 first fine-tunes XLM-R with source labeled data using the representation from the final layer [CLS] and being fed to the classifier (Devlin et al., 2019), then tests on the target."
2020.emnlp-main.599,N19-1112,0,0.301935,"ature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered. We further extend CFd to a cross-language setting, in which language discrepancy is studied. Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings. 1 Introduction Pre-trained language models (PrLMs) such as BERT (Devlin et al., 2019) and its variants (Liu et al., 2019c; Yang et al., 2019) have shown significant success for various downstream NLP tasks. However, these deep neural networks are sensitive to different cross-domain distributions (QuioneroCandela et al., 2009) and their effectiveness will be much weakened in such a scenario. How to ∗ Qingyu Tan is under the Joint PhD Program between Alibaba and National University of Singapore. adapt PrLMs to new domains is important. Unlike the most recent work that fine-tunes PrLMs on the unlabeled data from the new domains (Han and Eisenstein, 2019; Gururangan et al., 2020), we are interested in how to adapt"
2020.emnlp-main.599,2021.ccl-1.108,0,0.099182,"Missing"
2020.emnlp-main.599,P19-1335,0,0.0221745,"ideal joint hypothesis (§4.3,5.4). 2 Related Work Adaptation of PrLMs. Recently, significant improvements on multiple NLP tasks have been enabled by pre-trained language models (PrLMs) (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019c; Howard and Ruder, 2018; Peters et al., 2018). To enhance their performance on new domains, much work has been done to adapt PrLMs. Two main adaptation settings have been studied. The first is the same as what we study in this work: the PrLM provides the features based on which domain adaptation is conducted (Han and Eisenstein, 2019; Cao et al., 2019; Logeswaran et al., 2019; Ma et al., 2019; Li et al., 2020). In the second setting, the corpus for pre-training a language model has large domain discrepancy with the target domain, so in this scenario, we need the target unlabeled data to fine-tune the PrLM after which we train a task-specific model (Gururangan et al., 2020). For example, Lee et al. (2020) and Alsentzer et al. (2019) transfer PrLMs into biomedical and clinical domains. Instead of fine-tuning PrLMs with unlabeled data from the new domain as in most previous work (Rietzler et al., 2019; Han and Eisenstein, 2019; Gururangan et al., 2020), we are intere"
2020.emnlp-main.599,D15-1166,0,0.0488842,"e for classification (Hao et al., 2019; Peters et al., 2018; Liu et al., 2019b). By making a trade-off between speed and model performance, we combine the last N -layer features from the PrLM for domain adaptation, which is called the multi-layer representation of the PrLM. Our FAM consists of a feed-forward neural network (followed by a tanh activation function) and ¯ l from layer l an attention mechanism. We map h into zl with the feed-forward neural network: ¯l) zl = f (h (4) Multi-layer Representation. Since feature effectiveness differs from layer to layer, we use an attention mechanism (Luong et al., 2015) to learn to weight the features from the last N layers. We get 7388 the multi-layer representation z of the PrLM as: L X z = E(x; θ) = αi = PL ? ? Ave αi zi i=L−N +1 tanh(W att zi ) e ??????? FAM Transformer (5) Transformer tanh(Watt zj ) j=L−N +1 e in which Watt is a matrix of trainable parameters. Inspired by Berthelot et al. (2019), we want the model to focus more on the higher-weighted layers, so we further calculate the attention weight as: ℎ""! ???? ? ?????? Transformer ℎ""! Figure 2: Illustration of feature self-distillation. We take the sum of the last N -layerlayer features for distil"
2020.emnlp-main.599,D19-6109,0,0.0326124,"§4.3,5.4). 2 Related Work Adaptation of PrLMs. Recently, significant improvements on multiple NLP tasks have been enabled by pre-trained language models (PrLMs) (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019c; Howard and Ruder, 2018; Peters et al., 2018). To enhance their performance on new domains, much work has been done to adapt PrLMs. Two main adaptation settings have been studied. The first is the same as what we study in this work: the PrLM provides the features based on which domain adaptation is conducted (Han and Eisenstein, 2019; Cao et al., 2019; Logeswaran et al., 2019; Ma et al., 2019; Li et al., 2020). In the second setting, the corpus for pre-training a language model has large domain discrepancy with the target domain, so in this scenario, we need the target unlabeled data to fine-tune the PrLM after which we train a task-specific model (Gururangan et al., 2020). For example, Lee et al. (2020) and Alsentzer et al. (2019) transfer PrLMs into biomedical and clinical domains. Instead of fine-tuning PrLMs with unlabeled data from the new domain as in most previous work (Rietzler et al., 2019; Han and Eisenstein, 2019; Gururangan et al., 2020), we are interested in the featu"
2020.emnlp-main.599,N18-1202,0,0.270565,"nd multilingual Amazon review datasets for sentiment classification: MonoAmazon for cross-domain and MultiAmazon for cross-language experiments. We demonstrate that self-training can be consistently improved by CFd in all settings (§5.3). Further empirical results indicate that the improvements come from learning lower errors of ideal joint hypothesis (§4.3,5.4). 2 Related Work Adaptation of PrLMs. Recently, significant improvements on multiple NLP tasks have been enabled by pre-trained language models (PrLMs) (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019c; Howard and Ruder, 2018; Peters et al., 2018). To enhance their performance on new domains, much work has been done to adapt PrLMs. Two main adaptation settings have been studied. The first is the same as what we study in this work: the PrLM provides the features based on which domain adaptation is conducted (Han and Eisenstein, 2019; Cao et al., 2019; Logeswaran et al., 2019; Ma et al., 2019; Li et al., 2020). In the second setting, the corpus for pre-training a language model has large domain discrepancy with the target domain, so in this scenario, we need the target unlabeled data to fine-tune the PrLM after which we train a task-spec"
2021.eacl-main.283,N19-1423,0,0.0121313,"s to serve as the context for the question. Given an example E = {C, q, a} from HotpotQA, we aim to generate an evaluation example E 0 = {C, q, a, sub q1 , sub a1 , sub q2 , sub a2 }, where sub q1 and sub q2 are the two sub-questions, and sub a1 and sub a2 are their corresponding answers. 2.2 Sub-Question Generation Given a multi-hop question, the first step is to decompose it into sub-questions. We adopt the model introduced in DecompRC (Min et al., 2019b) to generate the sub-questions using a copying and editing mechanism. The multi-hop question is first converted into BERT word embeddings (Devlin et al., 2019), and then sent to a fully connected neural network to predict the splitting points. It is trained on 400 annotated examples. The separated text spans are post-processed to form the two sub-questions, following a set of handcrafted rules. 2.3 Intermediate Answer Extraction One particular characteristic of bridge-type questions from HotpotQA is that the two gold paragraphs are linked by a bridge entity. Since the crowd-workers are required to form a multi-hop question which makes use of information from both paragraphs, there is a high probability that the bridge entity is the answer to the fir"
2021.eacl-main.283,P19-1259,0,0.0114883,"of q and whether sub a1 and sub a2 are valid and to correct them if not. In total, a sample of 1,000 examples generated for the HotpotQA development set are manually verified for use in our evaluation1 . 3 Experiments and Results In order to interpret the behavior of existing models on each hop of the reasoning process required for multi-hop questions and to determine their ability to answer simple questions, we perform sub-question evaluation on three published topperforming QA models with publicly available open-source code: DFGN (Qiu et al., 2019), DecompRC (Min et al., 2019b), and CogQA (Ding et al., 2019). For all experiments, we measure EM and F1 scores for q, sub q1 , and sub q2 on 1,000 human-verified examples. To measure the correctness of a predicted answer, we first use exact string match as the only metric. However, during error analysis, we find that many predicted answers that partially match the gold answers should also be regarded as correct. Some representative examples are shown in Table 1. Although these predicted answers have zero EM scores, they are semantically equivalent to the correct answers given. Therefore, we define a more flexible metric named partial match (PM) as an a"
2021.eacl-main.283,P19-1262,0,0.0300287,"Missing"
2021.eacl-main.283,P17-1147,0,0.0247661,"lti-hop question, followed by extracting the corresponding sub-answers. Contrary to our expectation, multiple state-of-the-art multi-hop QA models fail to answer a large portion of sub-questions, although the corresponding multi-hop questions are correctly answered. Our work takes a step forward towards building a more explainable multi-hop QA system. 1 Introduction Rapid progress has been made in the field of question answering (QA), thanks to the release of many large-scale, high-quality QA datasets. Early datasets (Hermann et al., 2015; Rajpurkar et al., 2016, 2018; Trischler et al., 2017; Joshi et al., 2017) mainly consist of single-hop questions, where an answer with supporting justification can be found within a short segment of text. These benchmarks focus on evaluating QA models’ ability to perform local pattern matching between a passage and a question. Existing models (Lan et al., 2020; Zhang et al., 2020) have achieved super-human performance. Recently, multi-hop QA datasets (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018) have gained increasing attention. They require models to retrieve multiple pieces of supporting evidence from different documents and to reason over the ev"
2021.eacl-main.283,N18-1023,0,0.0239703,"he field of question answering (QA), thanks to the release of many large-scale, high-quality QA datasets. Early datasets (Hermann et al., 2015; Rajpurkar et al., 2016, 2018; Trischler et al., 2017; Joshi et al., 2017) mainly consist of single-hop questions, where an answer with supporting justification can be found within a short segment of text. These benchmarks focus on evaluating QA models’ ability to perform local pattern matching between a passage and a question. Existing models (Lan et al., 2020; Zhang et al., 2020) have achieved super-human performance. Recently, multi-hop QA datasets (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018) have gained increasing attention. They require models to retrieve multiple pieces of supporting evidence from different documents and to reason over the evidence collected to answer a question. The standard evaluation metrics of QA datasets include exact match (EM) and F1 scores averaged over the test set. HotpotQA (Yang et al., 2018) also provides sentence-level supporting facts required for reasoning. However, providing supporting sentences is not sufficient for us to interpret the choice of an answer for end-to-end complex QA systems. It is unclear w"
2021.eacl-main.283,P19-1416,0,0.06121,"ences which help to determine the answer. Then, eight other related distracting paragraphs are retrieved from Wikipedia and mixed with the two gold paragraphs to serve as the context for the question. Given an example E = {C, q, a} from HotpotQA, we aim to generate an evaluation example E 0 = {C, q, a, sub q1 , sub a1 , sub q2 , sub a2 }, where sub q1 and sub q2 are the two sub-questions, and sub a1 and sub a2 are their corresponding answers. 2.2 Sub-Question Generation Given a multi-hop question, the first step is to decompose it into sub-questions. We adopt the model introduced in DecompRC (Min et al., 2019b) to generate the sub-questions using a copying and editing mechanism. The multi-hop question is first converted into BERT word embeddings (Devlin et al., 2019), and then sent to a fully connected neural network to predict the splitting points. It is trained on 400 annotated examples. The separated text spans are post-processed to form the two sub-questions, following a set of handcrafted rules. 2.3 Intermediate Answer Extraction One particular characteristic of bridge-type questions from HotpotQA is that the two gold paragraphs are linked by a bridge entity. Since the crowd-workers are requi"
2021.eacl-main.283,P19-1613,0,0.0911767,"ences which help to determine the answer. Then, eight other related distracting paragraphs are retrieved from Wikipedia and mixed with the two gold paragraphs to serve as the context for the question. Given an example E = {C, q, a} from HotpotQA, we aim to generate an evaluation example E 0 = {C, q, a, sub q1 , sub a1 , sub q2 , sub a2 }, where sub q1 and sub q2 are the two sub-questions, and sub a1 and sub a2 are their corresponding answers. 2.2 Sub-Question Generation Given a multi-hop question, the first step is to decompose it into sub-questions. We adopt the model introduced in DecompRC (Min et al., 2019b) to generate the sub-questions using a copying and editing mechanism. The multi-hop question is first converted into BERT word embeddings (Devlin et al., 2019), and then sent to a fully connected neural network to predict the splitting points. It is trained on 400 annotated examples. The separated text spans are post-processed to form the two sub-questions, following a set of handcrafted rules. 2.3 Intermediate Answer Extraction One particular characteristic of bridge-type questions from HotpotQA is that the two gold paragraphs are linked by a bridge entity. Since the crowd-workers are requi"
2021.eacl-main.283,P19-1617,0,0.0145727,"nd se3246 q c c c c w w w w mantically correct sub-questions of q and whether sub a1 and sub a2 are valid and to correct them if not. In total, a sample of 1,000 examples generated for the HotpotQA development set are manually verified for use in our evaluation1 . 3 Experiments and Results In order to interpret the behavior of existing models on each hop of the reasoning process required for multi-hop questions and to determine their ability to answer simple questions, we perform sub-question evaluation on three published topperforming QA models with publicly available open-source code: DFGN (Qiu et al., 2019), DecompRC (Min et al., 2019b), and CogQA (Ding et al., 2019). For all experiments, we measure EM and F1 scores for q, sub q1 , and sub q2 on 1,000 human-verified examples. To measure the correctness of a predicted answer, we first use exact string match as the only metric. However, during error analysis, we find that many predicted answers that partially match the gold answers should also be regarded as correct. Some representative examples are shown in Table 1. Although these predicted answers have zero EM scores, they are semantically equivalent to the correct answers given. Therefore, we d"
2021.eacl-main.283,P18-2124,0,0.0466281,"Missing"
2021.eacl-main.283,W17-2623,0,0.028258,"e sub-questions for a multi-hop question, followed by extracting the corresponding sub-answers. Contrary to our expectation, multiple state-of-the-art multi-hop QA models fail to answer a large portion of sub-questions, although the corresponding multi-hop questions are correctly answered. Our work takes a step forward towards building a more explainable multi-hop QA system. 1 Introduction Rapid progress has been made in the field of question answering (QA), thanks to the release of many large-scale, high-quality QA datasets. Early datasets (Hermann et al., 2015; Rajpurkar et al., 2016, 2018; Trischler et al., 2017; Joshi et al., 2017) mainly consist of single-hop questions, where an answer with supporting justification can be found within a short segment of text. These benchmarks focus on evaluating QA models’ ability to perform local pattern matching between a passage and a question. Existing models (Lan et al., 2020; Zhang et al., 2020) have achieved super-human performance. Recently, multi-hop QA datasets (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018) have gained increasing attention. They require models to retrieve multiple pieces of supporting evidence from different documents and"
2021.eacl-main.283,Q18-1021,0,0.0274427,"swering (QA), thanks to the release of many large-scale, high-quality QA datasets. Early datasets (Hermann et al., 2015; Rajpurkar et al., 2016, 2018; Trischler et al., 2017; Joshi et al., 2017) mainly consist of single-hop questions, where an answer with supporting justification can be found within a short segment of text. These benchmarks focus on evaluating QA models’ ability to perform local pattern matching between a passage and a question. Existing models (Lan et al., 2020; Zhang et al., 2020) have achieved super-human performance. Recently, multi-hop QA datasets (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018) have gained increasing attention. They require models to retrieve multiple pieces of supporting evidence from different documents and to reason over the evidence collected to answer a question. The standard evaluation metrics of QA datasets include exact match (EM) and F1 scores averaged over the test set. HotpotQA (Yang et al., 2018) also provides sentence-level supporting facts required for reasoning. However, providing supporting sentences is not sufficient for us to interpret the choice of an answer for end-to-end complex QA systems. It is unclear whether the systems h"
2021.eacl-main.283,D18-1259,0,0.0916972,"to the release of many large-scale, high-quality QA datasets. Early datasets (Hermann et al., 2015; Rajpurkar et al., 2016, 2018; Trischler et al., 2017; Joshi et al., 2017) mainly consist of single-hop questions, where an answer with supporting justification can be found within a short segment of text. These benchmarks focus on evaluating QA models’ ability to perform local pattern matching between a passage and a question. Existing models (Lan et al., 2020; Zhang et al., 2020) have achieved super-human performance. Recently, multi-hop QA datasets (Khashabi et al., 2018; Welbl et al., 2018; Yang et al., 2018) have gained increasing attention. They require models to retrieve multiple pieces of supporting evidence from different documents and to reason over the evidence collected to answer a question. The standard evaluation metrics of QA datasets include exact match (EM) and F1 scores averaged over the test set. HotpotQA (Yang et al., 2018) also provides sentence-level supporting facts required for reasoning. However, providing supporting sentences is not sufficient for us to interpret the choice of an answer for end-to-end complex QA systems. It is unclear whether the systems have performed the de"
2021.findings-emnlp.365,2021.naacl-main.371,0,0.0162013,"the impact of addi2020) further improves on this approach by using tional lexical information on the sense reprea bi-encoder approach that independently embeds sentations with an ablation study, to investithe ambiguous word with its surrounding context gate why our model performs better. and the sense gloss of each queried sense. Since they are jointly optimized in the same representaOur source code and trained models are available tion space, disambiguation is performed by finding at https://github.com/nusnlp/esr. the nearest sense embedding. 2 Related Work Unlike GlossBERT and BEM, ESCHER (Barba et al., 2021) also utilizes sense gloss, but formulates In this paper, we address the English all-words WSD task, where a system disambiguates every am- the task as a span extraction problem. The input is a sentence pair where the first sentence contains biguous word in the dataset (Palmer et al., 2001). the context of the ambiguous word and the second In general, supervised methods have been shown sentence contains the concatenation of glosses from to perform better on the task, utilizing expensive human annotated data to achieve superior results. all candidate senses. The system is trained to find Combin"
2021.findings-emnlp.365,2020.emnlp-main.683,0,0.0237087,"dNet (Miller, Other approaches make use of relational infor1995), lexical information has proven to be use- mation in the lexical knowledge graphs. For exful in other methods. For example, the well-known ample, LMMS (Loureiro and Jorge, 2019) uses Lesk algorithm (Lesk, 1986) shows that sense gloss annotated data to generate sense embeddings us4312 ing BERT. These embeddings are then propagated through the WordNet graph to infer senses that do not appear in SemCor. Similarly, ARES (Scarlini et al., 2020) also achieves full sense coverage but through extraction of relevant contexts. SparseLMMS (Berend, 2020) further makes the embeddings sparse through a dictionary matrix. Connections are made between each dimension of the sparse embeddings and human interpretable semantic content. EWISE (Kumar et al., 2019), on the other hand, learns sense embeddings by pre-training a gloss encoder with sense definitions and knowledge graph information. The learned sense gloss embeddings are then scored via dot product with a contextual vector to perform prediction. EWISER (Bevilacqua and Navigli, 2020) extends EWISE by injecting additional relational knowledge from the lexical knowledge graph via a simple sparse"
2021.findings-emnlp.365,2020.acl-main.255,0,0.0257803,"ilarly, ARES (Scarlini et al., 2020) also achieves full sense coverage but through extraction of relevant contexts. SparseLMMS (Berend, 2020) further makes the embeddings sparse through a dictionary matrix. Connections are made between each dimension of the sparse embeddings and human interpretable semantic content. EWISE (Kumar et al., 2019), on the other hand, learns sense embeddings by pre-training a gloss encoder with sense definitions and knowledge graph information. The learned sense gloss embeddings are then scored via dot product with a contextual vector to perform prediction. EWISER (Bevilacqua and Navigli, 2020) extends EWISE by injecting additional relational knowledge from the lexical knowledge graph via a simple sparse dot product operation with an adjacency matrix formulated with the knowledge graph. Since the pre-trained sense embeddings are used to classify the ambiguous word, the model is able to predict synsets that are not present in the training set, improving zero-shot performance. Our system surpasses previous published systems despite using minimal knowledge graph information (only the sense gloss of hypernyms). 3 Methodology In this section, we describe the model architecture of our sys"
2021.findings-emnlp.365,2020.acl-main.95,0,0.0625244,"Missing"
2021.findings-emnlp.365,P07-1005,1,0.742745,"Missing"
2021.findings-emnlp.365,N19-1423,0,0.0445037,"Missing"
2021.findings-emnlp.365,D19-1533,1,0.880668,"Missing"
2021.findings-emnlp.365,D19-1355,0,0.032944,"Missing"
2021.findings-emnlp.365,W16-5307,0,0.0466308,"Missing"
2021.findings-emnlp.365,P19-1568,0,0.0177478,"-known ample, LMMS (Loureiro and Jorge, 2019) uses Lesk algorithm (Lesk, 1986) shows that sense gloss annotated data to generate sense embeddings us4312 ing BERT. These embeddings are then propagated through the WordNet graph to infer senses that do not appear in SemCor. Similarly, ARES (Scarlini et al., 2020) also achieves full sense coverage but through extraction of relevant contexts. SparseLMMS (Berend, 2020) further makes the embeddings sparse through a dictionary matrix. Connections are made between each dimension of the sparse embeddings and human interpretable semantic content. EWISE (Kumar et al., 2019), on the other hand, learns sense embeddings by pre-training a gloss encoder with sense definitions and knowledge graph information. The learned sense gloss embeddings are then scored via dot product with a contextual vector to perform prediction. EWISER (Bevilacqua and Navigli, 2020) extends EWISE by injecting additional relational knowledge from the lexical knowledge graph via a simple sparse dot product operation with an adjacency matrix formulated with the knowledge graph. Since the pre-trained sense embeddings are used to classify the ambiguous word, the model is able to predict synsets t"
2021.findings-emnlp.365,2021.ccl-1.108,0,0.0257815,"Missing"
2021.findings-emnlp.365,P19-1569,0,0.0142103,". (2020) utilizes usage examples from WordNet et al. (2019) investigates different ways of using to generate more training data. In contrast, our pre-trained BERT to perform WSD, with the GLU system uses example sentences to improve sense model outperforming previous work. representations instead. While supervised methods traditionally do not leverage lexical resources such as WordNet (Miller, Other approaches make use of relational infor1995), lexical information has proven to be use- mation in the lexical knowledge graphs. For exful in other methods. For example, the well-known ample, LMMS (Loureiro and Jorge, 2019) uses Lesk algorithm (Lesk, 1986) shows that sense gloss annotated data to generate sense embeddings us4312 ing BERT. These embeddings are then propagated through the WordNet graph to infer senses that do not appear in SemCor. Similarly, ARES (Scarlini et al., 2020) also achieves full sense coverage but through extraction of relevant contexts. SparseLMMS (Berend, 2020) further makes the embeddings sparse through a dictionary matrix. Connections are made between each dimension of the sparse embeddings and human interpretable semantic content. EWISE (Kumar et al., 2019), on the other hand, learn"
2021.findings-emnlp.365,P18-1230,0,0.0382738,"Missing"
2021.findings-emnlp.365,K16-1006,0,0.0607353,"Missing"
2021.findings-emnlp.365,2020.findings-emnlp.4,0,0.0502882,"Missing"
2021.findings-emnlp.365,P10-4014,1,0.820352,"Missing"
2021.findings-emnlp.365,P12-1029,1,0.724587,"Missing"
2021.findings-emnlp.365,S01-1005,0,0.0986935,"and the sense gloss of each queried sense. Since they are jointly optimized in the same representaOur source code and trained models are available tion space, disambiguation is performed by finding at https://github.com/nusnlp/esr. the nearest sense embedding. 2 Related Work Unlike GlossBERT and BEM, ESCHER (Barba et al., 2021) also utilizes sense gloss, but formulates In this paper, we address the English all-words WSD task, where a system disambiguates every am- the task as a span extraction problem. The input is a sentence pair where the first sentence contains biguous word in the dataset (Palmer et al., 2001). the context of the ambiguous word and the second In general, supervised methods have been shown sentence contains the concatenation of glosses from to perform better on the task, utilizing expensive human annotated data to achieve superior results. all candidate senses. The system is trained to find Combined with recent pre-trained language models, the text span corresponding to the correct sense. Another challenge faced by supervised systems supervised neural architectures have gained popis the limited training data size. The work from Yap ularity in recent years. For example, Hadiwinoto et"
2021.findings-emnlp.365,E17-1010,0,0.016254,"y uses the sense definition of the synset, but also incorporates words related to the synset to enrich the sense representation. 4 Experiments The related words are constructed by first conIn this section, we provide the details of our expercatenating the words from the following three iments and a comparison with other systems. sources in order: (i) all the lemmas belonging to the synset (synonyms); (ii) WordNet example phrases 4.1 Datasets or sentences of the synset; (iii) hypernym gloss of We follow the unified evaluation framework for the synset. Table 1 shows an example for the word WSD (Raganato et al., 2017). The SemCor dataset plant, with the words from synonyms, example sentences, and hypernym glosses listed accordingly. for training contains 226,036 annotated instances We then remove stop words (which are not so in- from 37,176 sentences. By creating positive and negative examples for each instance, we generformative), and keep one occurrence of a word if 1 it appears multiple times. By appending related For brevity, the neighboring sentences of the context senwords to the sense representation of a synset, we tence are not shown in the table. 4314 M Baseline Systems WordNet S1 Baseline Without"
2021.findings-emnlp.365,2020.emnlp-main.285,0,0.0259962,"ork. representations instead. While supervised methods traditionally do not leverage lexical resources such as WordNet (Miller, Other approaches make use of relational infor1995), lexical information has proven to be use- mation in the lexical knowledge graphs. For exful in other methods. For example, the well-known ample, LMMS (Loureiro and Jorge, 2019) uses Lesk algorithm (Lesk, 1986) shows that sense gloss annotated data to generate sense embeddings us4312 ing BERT. These embeddings are then propagated through the WordNet graph to infer senses that do not appear in SemCor. Similarly, ARES (Scarlini et al., 2020) also achieves full sense coverage but through extraction of relevant contexts. SparseLMMS (Berend, 2020) further makes the embeddings sparse through a dictionary matrix. Connections are made between each dimension of the sparse embeddings and human interpretable semantic content. EWISE (Kumar et al., 2019), on the other hand, learns sense embeddings by pre-training a gloss encoder with sense definitions and knowledge graph information. The learned sense gloss embeddings are then scored via dot product with a contextual vector to perform prediction. EWISER (Bevilacqua and Navigli, 2020) extend"
2021.findings-emnlp.365,L18-1166,0,0.0461486,"Missing"
2021.findings-emnlp.419,D19-1119,0,0.0118326,"G, S focuses more on professional writings and contains fewer errors. CWEB-dev is the combination of the first 1,000 sentences from the CWEB-S development set (2,862 sentences in total) and the first 1,000 sentences from the CWEB-G development set (3,867 sentences in total), and the remaining 4,729 sentences are regarded as CWEB-train, similar to the setting used in (Flachs et al., 2020). When testing on CWEB-S/G-test, we use BEA-train for training and CWEB-train for fine-tuning. 3.2 GEC Systems In this paper, we employ our CL approach on two state-of-the-art seq2seq GEC systems, i.e., GECPD (Kiyono et al., 2019), and GEC-BART (Kat• If s(i) is not identical to t(i) in the positive sumata and Komachi, 2020) to verify its effectivesample pair hs(i) , t(i) i (i.e., some edits are ness. The detailed description of these two systems made to s(i) to generate the corrected sentence follows. t(i) ), we further form a new negative sample GEC-PD uses a Transformer-based framework pair hs(i) , s(i) i. (Vaswani et al., 2017) with the Transformer-big setting. This system is first pre-trained on 70 mil3 Experiments lion parallel synthetic sentences. Then, it is further In this section, we demonstrate the effectiven"
2021.findings-emnlp.419,2020.acl-main.703,0,0.0502238,"Missing"
2021.findings-emnlp.419,N19-1333,0,0.01861,"atical Error Correction The state-of-the-art approach in GEC uses sequence-to-sequence learning with transformer neural networks (Grundkiewicz et al., 2019; Choe et al., 2019; Omelianchuk et al., 2020). Several task-specific techniques have been proposed for the seq2seq GEC models. (Zhao et al., 2019) incorporated a copy mechanism into transformer networks (Vaswani et al., 2017), since many words in a source sentence are often correct and they should be kept. Diverse ensembles (Chollampatt and Ng, 2018a), rescoring (Chollampatt and Ng, 2018b), and iterative decoding (Omelianchuk et al., 2020; Lichtarge et al., 2019) have also been applied to improve the accuracy of GEC. of semi-supervised learning and self-supervised learning in computer vision. In natural language processing, contrastive learning has also been used. In word2vec (Mikolov et al., 2013), a center word and a word in its surrounding context are regarded as a positive sample and their vector representations are pushed together, while a center word and a randomly chosen word are regarded as a negative sample and their vector representations are pushed further apart. Besides word2vec, contrastive learning has also been used in natural language"
2021.findings-emnlp.419,W13-1703,1,0.826873,"Missing"
2021.findings-emnlp.419,2020.emnlp-main.680,0,0.149219,"nnakoudakis et al., 2011), Lang-8 (Tajiri et al., 2012), and W&I (Bryant et al., 2019). Detailed statistics of the datasets are shown in Table 1. CWEB is a low error density dataset consisting of two domains, S and G. Compared to G, S focuses more on professional writings and contains fewer errors. CWEB-dev is the combination of the first 1,000 sentences from the CWEB-S development set (2,862 sentences in total) and the first 1,000 sentences from the CWEB-G development set (3,867 sentences in total), and the remaining 4,729 sentences are regarded as CWEB-train, similar to the setting used in (Flachs et al., 2020). When testing on CWEB-S/G-test, we use BEA-train for training and CWEB-train for fine-tuning. 3.2 GEC Systems In this paper, we employ our CL approach on two state-of-the-art seq2seq GEC systems, i.e., GECPD (Kiyono et al., 2019), and GEC-BART (Kat• If s(i) is not identical to t(i) in the positive sumata and Komachi, 2020) to verify its effectivesample pair hs(i) , t(i) i (i.e., some edits are ness. The detailed description of these two systems made to s(i) to generate the corrected sentence follows. t(i) ), we further form a new negative sample GEC-PD uses a Transformer-based framework pair"
2021.findings-emnlp.419,2020.bea-1.16,0,0.0211241,"consists of erroneous sentences from the dataset that require some correction. Through the above negative sampling method, we make the model avoid over-correcting a correct sentence or neglect to correct an erroneous sentence. The main contributions of this paper are as follows:1 Grammatical error correction (GEC) is the task of correcting errors in a source sentence and generating a well-written and grammatically correct target sentence. Good results have been achieved by state-of-the-art GEC systems based on the seq2seq transformer architecture (Grundkiewicz et al., 2019; Choe et al., 2019; Omelianchuk et al., 2020). However, most prior approaches in GEC are all targeting English-as-a-second-language (ESL) datasets, where GEC systems are trained to correct errors made by ESL learners. In fact, grammatical and other writing errors are made not only by ESL speakers but also by native speakers. Therefore, correcting grammatical errors made by native speakers should also be considered, which helps to broaden the application of GEC. Compared to the errors made by ESL learners, • We propose a new loss function based on CL, native English speakers are less likely to make which allows the model to achieve higher"
2021.findings-emnlp.419,P12-2039,0,0.0913564,"Missing"
2021.findings-emnlp.419,P19-1623,0,0.0171956,"e sample and their vector representations are pushed further apart. Besides word2vec, contrastive learning has also been used in natural language inference (Cui et al., 2020), language modeling (Liza and Grzes, 2018), and knowledge graph embeddings (Bose et al., 2018). Most of the above methods work at the sample level and have to generate both positive and negative samples. However, since the positive samples are hard to generate in the GEC task, the above methods are not suitable for GEC. Compared to the above methods, our approach does not need to generate extra positive samples. Although (Yang et al., 2019) propose a sentence-level margin loss-based method for machine translation to reduce the word omission errors and do not need positive samples too, their negative samples are generated by word omission at the token level and cannot be used in GEC. In contrast, our approach uses beam search to generate erroneous sentences as negative samples at the sentence level, which effectively prevents the model from making mistakes and thus is more suitable for the GEC task. 5 Conclusion In this paper, we propose a contrastive learning approach and a corresponding negative sampling method to improve the p"
2021.findings-emnlp.419,P11-1019,0,0.11429,"Missing"
2021.findings-emnlp.419,N19-1014,0,0.0171618,"EC-BART systems on the CWEB test set. We have successfully reduced the IE ratio and the OE ratio for both systems in S and G domain, except for the case of GEC-PD in G domain. This result demonstrates that CL can effectively reduce the over correction problem and ignored correction problem. 4 4.1 Related Work Grammatical Error Correction The state-of-the-art approach in GEC uses sequence-to-sequence learning with transformer neural networks (Grundkiewicz et al., 2019; Choe et al., 2019; Omelianchuk et al., 2020). Several task-specific techniques have been proposed for the seq2seq GEC models. (Zhao et al., 2019) incorporated a copy mechanism into transformer networks (Vaswani et al., 2017), since many words in a source sentence are often correct and they should be kept. Diverse ensembles (Chollampatt and Ng, 2018a), rescoring (Chollampatt and Ng, 2018b), and iterative decoding (Omelianchuk et al., 2020; Lichtarge et al., 2019) have also been applied to improve the accuracy of GEC. of semi-supervised learning and self-supervised learning in computer vision. In natural language processing, contrastive learning has also been used. In word2vec (Mikolov et al., 2013), a center word and a word in its surro"
C02-1025,A97-1029,0,0.00994639,"nly information from the same document. Mikheev et al. (1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities. The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules. 6 Conclusion We have shown that the maximum entropy framework is able to use global information directly. This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997). Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs. Information from a sentence is sometimes insufficient to classify a name correctly. Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier. We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources. Borthwick (1999) successfully made use of other handcoded systems as input for his MENE system, and achieved excellent results. However,"
C02-1025,M98-1028,0,0.0253529,"s on MUC-6 and MUC-7 test data. 1 Introduction Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. In MUC-6 and MUC-7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC-6 and MUC7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability  , where  is the sequence of words in a sentence, and  is the sequence of named-entity tags assigned to Hwee Tou Ng Department of Computer Science School of Computing National University of Singapore 3 Science Drive 2 Singapore 117543 nght@comp.nus.edu.sg the words in  . Attempts have been made to use global information (e.g., the same named entity occurring in diffe"
C02-1025,M95-1018,0,0.0131303,"Missing"
C02-1025,M98-1021,0,0.0443619,"e sequence of tags that maximizes the probability  , where  is the sequence of words in a sentence, and  is the sequence of named-entity tags assigned to Hwee Tou Ng Department of Computer Science School of Computing National University of Singapore 3 Science Drive 2 Singapore 117543 nght@comp.nus.edu.sg the words in  . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999). We propose maximizing    , where  is the sequence of namedentity tags assigned to the words in the sentence  , and  is the information that can be extracted from the whole document containing  . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC-6 and MUC-7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information). As far as we know, no other NERs have used information from the whole document (global) as we"
C02-1025,E99-1001,0,0.0484093,"e where the next token   is a hyphen, then  is also used as a feature: (initCaps,  )  is set to 1. This is because in many cases, the use of hyphens can be considered to be optional (e.g., third-quarter or third quarter). Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1. Dictionaries: Due to the limited amount of training material, name dictionaries have been found to be useful in the named entity task. The importance of dictionaries in NERs has been investigated in the literature (Mikheev et al., 1999). The sources of our dictionaries are listed in Table 2. For all lists except locations, the lists are processed into a list of tokens (unigrams). Location list is processed into a list of unigrams and bigrams (e.g., New York). For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams. A list of words occurring more than 10 times in the training data is also collected (commonWords). Only tokens with initCaps not found in commonWords are tested against each list in Table 2. If they are found in a list, then a feature for that list wi"
C02-1025,M98-1004,0,\N,Missing
C02-1025,M98-1012,0,\N,Missing
C02-1025,M98-1014,0,\N,Missing
C04-1089,P02-1051,0,0.559515,"ore suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context"
C04-1089,C02-1011,0,0.0532749,"pora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on Chinese-English comparable corpora. We translated Chinese word"
C04-1089,P98-1069,0,0.858754,"for uncommon language pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w t"
C04-1089,N04-1036,0,0.0438152,"Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language. In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone. Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling. But their similar spelling clue uses the longest common subsequence ratio and works only for cognates (words with a very similar spelling). The work that is most similar to ours is the recent research of (Huang et al., 2004). They attempted to improve named entity translation by combining phonetic and semantic information. Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity. It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-ofspeech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. 7. Conclusion In this paper, we proposed a new method to mine new word translations from comparable corpor"
C04-1089,J98-4003,0,0.707534,"new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implement"
C04-1089,W02-0902,0,0.367339,"uiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on Chinese-English comparable corpora. We translated Chinese words into English. That is, Chinese is the sour"
C04-1089,W97-0311,0,0.0073653,"approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. 1. Introduction New words such as person names, organization names, technical terms, etc. appear frequently. In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations. Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003). But parallel corpora are scarce resources, especially for uncommon language pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous resear"
C04-1089,E03-1035,0,0.0190599,"ing new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. 1. Introduction New words such as person names, organization names, technical terms, etc. appear frequently. In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations. Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003). But parallel corpora are scarce resources, especially for uncommon language pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on"
C04-1089,W04-3236,1,0.624284,"ytes, and the size of the Chinese corpus from Jul to Dec 1995 was about 120M bytes. We used a Chinese-English dictionary which contained about 10,000 entries for translating the words in the context. For the training of transliteration probability, we required a ChineseEnglish name list. We used a list of 1,580 Chinese-English name pairs as training data for the EM algorithm. 5.2 Preprocessing Unlike English, Chinese text is composed of Chinese characters with no demarcation for words. So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text from a half-month period. Then we determined the new Chinese words in each half-month period p. By new Chinese words, we refer to those words that appeared in this period p but not from Jan to Jun 1995 or any other periods that preceded p. Among all these new words, we selected those occurring at least 5 times. These words made up our test set. We call these words Chinese source words. They were the words that we were supposed to find translations from the English corpus. For the English corpus, we"
C04-1089,P95-1050,0,0.269655,"e pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its"
C04-1089,P99-1067,0,0.879737,"parable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation"
C04-1089,C98-1066,0,\N,Missing
C04-1089,W02-0505,0,\N,Missing
C10-1147,M95-1004,0,0.187879,"4 and M = 6. On all the different M values we have tried, MMST outperforms both the SNL-style baseline and the All-style baseline on the development test set. We then fixed M = 6, and evaluated different δ values. The results are shown in Figure 4. The best F-measure was obtained when δ = 1.0. Again, on all the different δ values we have tried, MMST outperforms both baselines on the development test set. The rows “MMST” in Table 1 and 2 show the performance of MMST on the test sets, with the tuned parameters indicated. In our experiments, the statistical significance test was conducted as in Chinchor (1995). ∗ and ∗∗ stand for p &lt; 0.05 and p &lt; 0.01 over the SNL-style baseline, respectively. † and †† stand for p &lt; 0.05 and p &lt; 0.01 over the All-style baseline, respectively. For the MUC metric, when compared to the All-style baseline, MMST gains 3.4, 2.5, 3.2, 2.4, and 2.8 improvement in F-measure on MUC6, MUC7, BNEWS, NPAPER, and NWIRE, respectively. The experimental results clearly show that MMST gains not only consistent, but also statistically significant improvement over both the SNL-style baseline and the All-style baseline in all combinations (five data sets and two baselines) on the MUC me"
C10-1147,N07-1030,0,0.013671,"tances, using a standard supervised learning algorithm. During testing, all preceding markables of a candidate anaphor are considered as potential antecedents, and are tested in a backto-front manner. The process stops if either an antecedent is found or the beginning of the text is reached. This framework has been widely used in the community of coreference resolution. Recent work boosted the performance of coreference resolution by exploiting fine-tuned feature sets under the above framework, or adopting alternative resolution methods during testing (Ng and Cardie, 2002b; Yang et al., 2003; Denis and Baldridge, 2007; Versley et al., 2008). Ng (2005) proposed a ranking model to maximize F-measure during testing. In the approach, n different coreference outputs for each test text are generated, by varying four components in a coreference resolution system, i.e., the learning algorithm, the instance creation method, the feature set, and the clustering algorithm. An SVM-based ranker then picks the output that is likely to have the highest F-measure. However, this approach is time-consuming during testing, as F-measure maximization is performed during testing. This limits its usage on a very large corpus. In"
C10-1147,E06-1015,0,0.0119769,"and evaluated their performances on a development test set. In our experiments, the development training set contained 2/3 of the texts in the training set of each individual corpus, while the development test set contained the remaining 1/3 of the texts. After having picked the best M and δ values, we trained a classifier on the entire training set with the chosen parameters. The learnt classifier was then applied to the test set. 68 MMST SNL−Style Baseline All−Style Baseline 66 64 62 F−measure we include the results of the original BART system (with its extended feature set and SVM-lightTK (Moschitti, 2006), as reported in Versley et al. (2008)) as the first system for comparison. Versley et al. (2008) reported only the results on the three ACE data sets with the MUC evaluation metric. Since we used all the five data sets in our experiments, for fair comparison, we also include the MUC results reported in Ng (2004). To the best of our knowledge, Ng (2004) was the only prior work which reported MUC metric scores on all the five data sets. The MUC metric scores of Versley et al. (2008) and Ng (2004) are listed in the row “Versley et al. 08” and “Ng 04”, respectively, in Table 1. For the B-CUBED me"
C10-1147,M95-1025,0,0.967929,"Missing"
C10-1147,W02-1008,0,0.176882,"ric score. Furthermore, the extracted training instances are not equally easy to be classified. Introduction Coreference resolution refers to the process of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing tasks. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002b). A large body of prior research on coreference resolution follows the same process: durIn this paper, we propose a novel approach comprising the use of instance weighting and beam search to address the above issues. Our proposed maximum metric score training (MMST) approach performs maximization of the chosen evaluation metric score on the training corpus during training. It iteratively assigns higher weights to the hard-to-classify training instances. The output of training is a standard classifier. Hence, during testing, MMST is faster than approaches which optimize the assignment of core"
C10-1147,P02-1014,0,0.420823,"ric score. Furthermore, the extracted training instances are not equally easy to be classified. Introduction Coreference resolution refers to the process of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing tasks. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002b). A large body of prior research on coreference resolution follows the same process: durIn this paper, we propose a novel approach comprising the use of instance weighting and beam search to address the above issues. Our proposed maximum metric score training (MMST) approach performs maximization of the chosen evaluation metric score on the training corpus during training. It iteratively assigns higher weights to the hard-to-classify training instances. The output of training is a standard classifier. Hence, during testing, MMST is faster than approaches which optimize the assignment of core"
C10-1147,P05-1020,0,0.241548,"ithm. During testing, all preceding markables of a candidate anaphor are considered as potential antecedents, and are tested in a backto-front manner. The process stops if either an antecedent is found or the beginning of the text is reached. This framework has been widely used in the community of coreference resolution. Recent work boosted the performance of coreference resolution by exploiting fine-tuned feature sets under the above framework, or adopting alternative resolution methods during testing (Ng and Cardie, 2002b; Yang et al., 2003; Denis and Baldridge, 2007; Versley et al., 2008). Ng (2005) proposed a ranking model to maximize F-measure during testing. In the approach, n different coreference outputs for each test text are generated, by varying four components in a coreference resolution system, i.e., the learning algorithm, the instance creation method, the feature set, and the clustering algorithm. An SVM-based ranker then picks the output that is likely to have the highest F-measure. However, this approach is time-consuming during testing, as F-measure maximization is performed during testing. This limits its usage on a very large corpus. In the community of machine learning,"
C10-1147,J01-4004,1,0.588229,"e impact on the metric score. Furthermore, the extracted training instances are not equally easy to be classified. Introduction Coreference resolution refers to the process of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing tasks. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002b). A large body of prior research on coreference resolution follows the same process: durIn this paper, we propose a novel approach comprising the use of instance weighting and beam search to address the above issues. Our proposed maximum metric score training (MMST) approach performs maximization of the chosen evaluation metric score on the training corpus during training. It iteratively assigns higher weights to the hard-to-classify training instances. The output of training is a standard classifier. Hence, during testing, MMST is faster than approaches which optimize t"
C10-1147,P09-1074,0,0.0353229,"Soon et al. (2001) training and testing framework. We used the BART package in our experiments, and implemented the proposed MMST algorithm on top of it. In our experiments reported in this paper, the features we used are identical to the features output by the preprocessing code of BART reported in Versley et al. (2008), except that we did not use their tree-valued and stringvalued features (see the next subsection for details). Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al. (2009). How to use the B-CUBED metric for evaluating twinless markables has been explored recently. In this paper, we adopt the B 3 all variation proposed by Stoyanov et al. (2009), which retains all twinless markables. We also experimented with their B 3 0 variation, which gave similar results. Note that no matter which variant of the B-CUBED metric is used, it is a fair comparison as long as the baseline and our proposed MMST algorithm are compared against each other using the same B-CUBED variant. 5.2 The Baseline Systems We include state-of-the-art coreference resolution systems in the literatur"
C10-1147,N09-3001,0,0.0209234,"f the evaluation metric during training. Our approach is general and applicable to any supervised learning classifiers. Recently, Wick and McCallum (2009) proposed a partition-wise model for coreference resolution to maximize a chosen evaluation metric using the Metropolis-Hastings algorithm (Metropolis et al., 1953; Hastings, 1970). However, they found that training on classification accuracy, in most cases, outperformed training on the coreference evaluation metrics. Furthermore, similar to Ng (2005), their approach requires the generation of multiple coreference assignments during testing. Vemulapalli et al. (2009) proposed a documentlevel boosting technique for coreference resolution by re-weighting the documents that have the lowest F-measures. By combining multiple classifiers generated in multiple iterations, they 1309 achieved a CEAF score slightly better than the baseline. Different from them, our approach works at the instance level, and we output a single classifier. 3 Coreference Evaluation Metrics In this section, we review two commonly used evaluation metrics for coreference resolution. First, we introduce the terminology. The gold standard annotation and the output by a coreference resolutio"
C10-1147,M95-1005,0,0.649311,"ection, we review two commonly used evaluation metrics for coreference resolution. First, we introduce the terminology. The gold standard annotation and the output by a coreference resolution system are called key and response, respectively. In both the key and the response, a coreference chain is formed by a set of coreferential mentions. A mention (or markable) is a noun phrase which satisfies the markable definition in an individual corpus. A link refers to a pair of coreferential mentions. If a mention has no links to other mentions, it is called a singleton. 3.1 The MUC Evaluation Metric Vilain et al. (1995) introduced the link-based MUC evaluation metric for the MUC-6 and MUC7 coreference tasks. Let Si be an equivalence class generated by the key (i.e., Si is a coreference chain), and p(Si ) be a partition of Si relative to the response. Recall is the number of correctly identified links over the number of links in the key. Recall = P (|Si |− |p(Si )|) P (|Si |− 1) Precision, on the other hand, is defined in the opposite way by switching the role of key and response. F-measure is a trade-off between recall and precision. F = 3.2 2 · Recall · P recision Recall + P recision The B-CUBED Evaluation"
C10-1147,P03-1023,0,0.0211623,"on all training instances, using a standard supervised learning algorithm. During testing, all preceding markables of a candidate anaphor are considered as potential antecedents, and are tested in a backto-front manner. The process stops if either an antecedent is found or the beginning of the text is reached. This framework has been widely used in the community of coreference resolution. Recent work boosted the performance of coreference resolution by exploiting fine-tuned feature sets under the above framework, or adopting alternative resolution methods during testing (Ng and Cardie, 2002b; Yang et al., 2003; Denis and Baldridge, 2007; Versley et al., 2008). Ng (2005) proposed a ranking model to maximize F-measure during testing. In the approach, n different coreference outputs for each test text are generated, by varying four components in a coreference resolution system, i.e., the learning algorithm, the instance creation method, the feature set, and the clustering algorithm. An SVM-based ranker then picks the output that is likely to have the highest F-measure. However, this approach is time-consuming during testing, as F-measure maximization is performed during testing. This limits its usage"
C14-1138,N01-1016,0,0.716273,"Missing"
C14-1138,N09-2028,0,0.253274,"Missing"
C14-1138,P04-1005,0,0.866129,"e F1 scores We split all POS tags into 6 classes, by first sorting all POS tags in descending order of their F1 scores. Next, for POS tags with higher F1 scores, we form larger classes (higher total proportion), and for POS tags with lower F1 scores, we form smaller classes. The POS classes are shown in Table 4. The algorithm dynamically selects posteriors from different M3N models, depending on the POS tag of the current word. 5 5.1 Experiments Experimental Setup We tested all the systems on the Switchboard Treebank corpus (LDC99T42), using the same train/develop/test split as previous work (Johnson and Charniak, 2004; Qian and Liu, 2013). We removed all partial words and punctuation symbols to simulate the condition when automatic speech recognition (ASR) output is used. Our training set contains 1.3M words in 174K sentences; our development set contains 86K words in 10K sentences; and our test set contains 66K words in 8K sentences. The original system has high precision but low recall, i.e., the system tends to miss out edit words. The imbalance can be solved by setting a larger penalty for mis-labeling edits as fluent, i.e., 2 instead of 1 for the weighted hamming loss. We used the loss matrix, v(e yt"
C14-1138,H05-1030,0,0.773791,"Missing"
C14-1138,P03-1021,0,0.0130544,"h hypothesis produced, the decoder cleans up the sentence by removing all the predicted filler words and edit words so that subsequent operations can act on the cleaned-up sentence w ¯ if needed. Each hypothesis evaluator produces a score f which measures the quality of the current hypothesis based on certain aspects of fluency specific to that hypothesis evaluator. The overall score of a hypothesis is the weighted sum of the scores from all the hypothesis evaluators: X score(h, w) = λi fi (h, w) (5) i The weights λi s are tuned on the development set using minimum error rate training (MERT) (Och, 2003). The decoding algorithm is shown in Algorithm 1. In our description, hi denotes the hypothesized label at the ith position; wi denotes the word at the th i position; |h |denotes the length of the label sequence; fM 3N (hi , w) denotes the M3N log-posterior probability of the label (at the ith position of hypothesis h) being the hypothesized label; fM 3N (h, w) denotes the normalized joint log probability of hypothesis h given the M3N model (‘normalized’ means divided by the length of the label sequence); w ¯ denotes the cleaned-up sentence; fLM (w) ¯ = fLM (h, w) denotes the language model sc"
C14-1138,N13-1102,0,0.373915,"eature function, f (x, y, t). For example, f (w0 =‘so’, y0 =‘F’, y−1 =‘O’, t) has a value of 1 only when the word at node t is ‘so’, the label at node t is ‘F’ (filler word), and the label at node (t − 1) is ‘O’ (outside edit/filler region, i.e., fluent). The maximum length of the y history (for this feature function, it is 2 since only y0 and y−1 are covered) is called the clique order of the feature. L(x, y) is the loss function. A standard M3N uses an unweighted hamming loss, which is the number of incorrect nodes: ( X 1, if a = b δ(e yt , y¯t ) where δ(a, b) = L(x, y) = (3) 0, otherwise t Qian and Liu (2013) proposed using label-weighted M3N to balance precision and recall by adjusting the penalty on false positives and false negatives, i.e., v(e yt , y¯t ) in Eqn. 4. In this work, we further extend this technique to individual nodes to train expert models, each specialized in a specific part-of-speech (POS) class. Our loss function is: ( X Bc , if POS(t) ∈ Sc L(x, y) = uc (t)v(e yt , y¯t )δ(e yt , y¯t ) where uc (t) = (4) 1, otherwise t where Bc is a factor which controls the extent to which the model is biased to minimize errors on specific nodes, POS(t) is the POS tag of the word at node t, an"
C14-1138,N13-1050,1,0.894642,"Missing"
C14-1138,P06-1071,0,0.740059,"Missing"
C14-1138,P11-1071,0,0.740368,"Missing"
C14-1138,D12-1052,1,\N,Missing
C18-1096,D16-1171,0,0.0262867,"d, we propose to learn the probability distribution over aspects for the given target, and use the weighted summation of aspect embeddings for target representation. The probability distribution and the aspect embeddings are learned via an unsupervised objective, which is jointly trained with the neural attention-based sentiment classifier. 3 Model Description We propose two approaches to improve the effectiveness of the attention mechanism. The approaches may be applied more generally but we use them on attention-based LSTM as it has been widely used in previous works for sentiment analysis (Chen et al., 2016; Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Ma et al., 2018). We first give the task definition in (§3.1). Then, we briefly describe the architecture of attention-based LSTM (§3.2) and introduce the two proposed approaches (§3.3 & §3.4). Finally, we describe the overall architecture of our model for aspect-level sentiment classification and the training objective (§3.5). 3.1 Task Definition and Notation Given a review sentence s = (w1 , w2 , ..., wn ) consisting of n words, and an opinion target occurring in the sentence a = (a1 , a2 , ..., am ) consisting of a subsequence of"
C18-1096,D17-1047,0,0.832668,"cation is typically considered as a classification problem. Early works (Boiy and Moens, 2009; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014) mainly used manually designed features such as sentiment lexicon, n-grams, and dependency information. However, these methods highly depend on the quality of the designed features, which is labor-intensive. With the advances of deep learning methods, various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Tang et al., 2016b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; He et al., 2018) have been proposed for automatically learning target-dependent sentence representations for classification. The main idea behind these works is to develop neural architectures that are capable of learning continuous features without feature engineering and at the same time capturing the intricate relatedness between a target and context words. Among these works, attention-based neural models have attracted growing interest due to their ability to explicitly capture the importance of context words. Tang et al. (2016b) have shown that a better sentence representation could be"
C18-1096,P14-2009,0,0.152918,"odel outperforms all baseline methods on aspect-level sentiment classification. 2 Related Work Under supervised learning conditions, aspect-level sentiment classification is typically considered as a classification problem. Early works (Boiy and Moens, 2009; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014) mainly used manually designed features such as sentiment lexicon, n-grams, and dependency information. However, these methods highly depend on the quality of the designed features, which is labor-intensive. With the advances of deep learning methods, various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Tang et al., 2016b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; He et al., 2018) have been proposed for automatically learning target-dependent sentence representations for classification. The main idea behind these works is to develop neural architectures that are capable of learning continuous features without feature engineering and at the same time capturing the intricate relatedness between a target and context words. Among these works, attention-based neural models have attracted growing"
C18-1096,P17-1036,1,0.895486,"ets that only contain one word but may fail to capture the semantics of more complex expressions, as also mentioned by Tang et al. (2016b). For example, we cannot obtain a good representation for “hot dog” by averaging the word vectors of “hot” and “dog”. Hot would be close to words like warm or cold and dog would be close to animals like cat. The average would not be close to other food like burgers or spaghetti. Another example is “hong kong style food”. As it consists of many words, the averaged word vector could be far away from “food” in vector space. To address this problem, inspired by He et al. (2017), we instead model each target as a mixture of K aspect embeddings where we would like each embedded aspect to represent a cluster of closely related targets. We use an autoencoder structure to learn both the aspect embeddings as well as the representation of the target as a weighted combination of the aspect embeddings. The weight vector represents the probability distribution over aspects for the given target. The autoencoder structure is jointly trained with a neural attention-based sentiment classifier to provide a good target representation as well as a high accuracy on the predicted sent"
C18-1096,P18-2092,1,0.691225,"considered as a classification problem. Early works (Boiy and Moens, 2009; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014) mainly used manually designed features such as sentiment lexicon, n-grams, and dependency information. However, these methods highly depend on the quality of the designed features, which is labor-intensive. With the advances of deep learning methods, various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Tang et al., 2016b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; He et al., 2018) have been proposed for automatically learning target-dependent sentence representations for classification. The main idea behind these works is to develop neural architectures that are capable of learning continuous features without feature engineering and at the same time capturing the intricate relatedness between a target and context words. Among these works, attention-based neural models have attracted growing interest due to their ability to explicitly capture the importance of context words. Tang et al. (2016b) have shown that a better sentence representation could be obtained by stacki"
C18-1096,P11-1016,0,0.129276,"on the syntactic path which is obtained by applying a dependency parser on the review sentence. We conducted experiments on attention-based LSTM models using the SemEval 2014, 2015, and 2016 datasets. The results show that attention-based LSTM can be substantially improved by incorporating our two proposed methods, and that the resulting model outperforms all baseline methods on aspect-level sentiment classification. 2 Related Work Under supervised learning conditions, aspect-level sentiment classification is typically considered as a classification problem. Early works (Boiy and Moens, 2009; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014) mainly used manually designed features such as sentiment lexicon, n-grams, and dependency information. However, these methods highly depend on the quality of the designed features, which is labor-intensive. With the advances of deep learning methods, various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Tang et al., 2016b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; He et al., 2018) have been proposed for automatically learning target-dependent sentence repre"
C18-1096,S14-2076,0,0.388379,"h which is obtained by applying a dependency parser on the review sentence. We conducted experiments on attention-based LSTM models using the SemEval 2014, 2015, and 2016 datasets. The results show that attention-based LSTM can be substantially improved by incorporating our two proposed methods, and that the resulting model outperforms all baseline methods on aspect-level sentiment classification. 2 Related Work Under supervised learning conditions, aspect-level sentiment classification is typically considered as a classification problem. Early works (Boiy and Moens, 2009; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014) mainly used manually designed features such as sentiment lexicon, n-grams, and dependency information. However, these methods highly depend on the quality of the designed features, which is labor-intensive. With the advances of deep learning methods, various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Tang et al., 2016b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; He et al., 2018) have been proposed for automatically learning target-dependent sentence representations for classificat"
C18-1096,E17-2091,0,0.738055,"yed an important role in state-of-the-art neural models for this task. It assigns a positive weight atti for each context word wi , which can be interpreted as the probability that wi is the right word to focus on when inferring the sentiment polarity of the given target. The weight atti is generally computed as a function of the hidden representation hi of wi and the target representation t as follows: atti ∝ fscore (hi , t) (1) It has been shown that adding an attention model substantially improves the accuracy of aspect-level sentiment classification (Tang et al., 2016b; Wang et al., 2016; Liu and Zhang, 2017). Our work builds upon this line of research. We propose two novel approaches for improving the effectiveness of attention models. The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression. The target representation is crucial since attention weights are computed based on it as shown in Eq. 1. In representing the target, we are mapping a word or This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1121 Proceedings of the 27th Internationa"
C18-1096,D15-1298,0,0.286837,"l baseline methods on aspect-level sentiment classification. 2 Related Work Under supervised learning conditions, aspect-level sentiment classification is typically considered as a classification problem. Early works (Boiy and Moens, 2009; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014) mainly used manually designed features such as sentiment lexicon, n-grams, and dependency information. However, these methods highly depend on the quality of the designed features, which is labor-intensive. With the advances of deep learning methods, various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Tang et al., 2016b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; He et al., 2018) have been proposed for automatically learning target-dependent sentence representations for classification. The main idea behind these works is to develop neural architectures that are capable of learning continuous features without feature engineering and at the same time capturing the intricate relatedness between a target and context words. Among these works, attention-based neural models have attracted growing interest due to their ab"
C18-1096,D14-1162,0,0.0830924,"Missing"
C18-1096,S14-2004,0,0.422772,"Missing"
C18-1096,S15-2082,0,0.183839,"Missing"
C18-1096,C16-1311,0,0.387837,"o this end, attention mechanism has played an important role in state-of-the-art neural models for this task. It assigns a positive weight atti for each context word wi , which can be interpreted as the probability that wi is the right word to focus on when inferring the sentiment polarity of the given target. The weight atti is generally computed as a function of the hidden representation hi of wi and the target representation t as follows: atti ∝ fscore (hi , t) (1) It has been shown that adding an attention model substantially improves the accuracy of aspect-level sentiment classification (Tang et al., 2016b; Wang et al., 2016; Liu and Zhang, 2017). Our work builds upon this line of research. We propose two novel approaches for improving the effectiveness of attention models. The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression. The target representation is crucial since attention weights are computed based on it as shown in Eq. 1. In representing the target, we are mapping a word or This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http://"
C18-1096,D16-1021,0,0.251734,"o this end, attention mechanism has played an important role in state-of-the-art neural models for this task. It assigns a positive weight atti for each context word wi , which can be interpreted as the probability that wi is the right word to focus on when inferring the sentiment polarity of the given target. The weight atti is generally computed as a function of the hidden representation hi of wi and the target representation t as follows: atti ∝ fscore (hi , t) (1) It has been shown that adding an attention model substantially improves the accuracy of aspect-level sentiment classification (Tang et al., 2016b; Wang et al., 2016; Liu and Zhang, 2017). Our work builds upon this line of research. We propose two novel approaches for improving the effectiveness of attention models. The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression. The target representation is crucial since attention weights are computed based on it as shown in Eq. 1. In representing the target, we are mapping a word or This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http://"
C18-1096,S14-2036,0,0.0210419,"lying a dependency parser on the review sentence. We conducted experiments on attention-based LSTM models using the SemEval 2014, 2015, and 2016 datasets. The results show that attention-based LSTM can be substantially improved by incorporating our two proposed methods, and that the resulting model outperforms all baseline methods on aspect-level sentiment classification. 2 Related Work Under supervised learning conditions, aspect-level sentiment classification is typically considered as a classification problem. Early works (Boiy and Moens, 2009; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014) mainly used manually designed features such as sentiment lexicon, n-grams, and dependency information. However, these methods highly depend on the quality of the designed features, which is labor-intensive. With the advances of deep learning methods, various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Tang et al., 2016b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; He et al., 2018) have been proposed for automatically learning target-dependent sentence representations for classification. The main idea beh"
C18-1096,D16-1058,0,0.241064,"n mechanism has played an important role in state-of-the-art neural models for this task. It assigns a positive weight atti for each context word wi , which can be interpreted as the probability that wi is the right word to focus on when inferring the sentiment polarity of the given target. The weight atti is generally computed as a function of the hidden representation hi of wi and the target representation t as follows: atti ∝ fscore (hi , t) (1) It has been shown that adding an attention model substantially improves the accuracy of aspect-level sentiment classification (Tang et al., 2016b; Wang et al., 2016; Liu and Zhang, 2017). Our work builds upon this line of research. We propose two novel approaches for improving the effectiveness of attention models. The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression. The target representation is crucial since attention weights are computed based on it as shown in Eq. 1. In representing the target, we are mapping a word or This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1121 Proceedings of"
C18-1231,I17-2058,0,0.614488,"s to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human judgments. In the case of MT, WMT organizes a metrics shared task each year, where the correlation of system-level rankings generated by metrics and humans is measured. There is also a segment-level evaluation subtask where the agreement of metrics is measured at the sentence level. Prior work in GEC has followed the systemlevel evaluation approach to compare metrics (Grundkiewicz et al., 2015; Napoles et al., 2015; Sakaguchi et al., 2016). However, there has been no prior work on sentence-level ev"
C18-1231,W16-2302,0,0.0561161,"Missing"
C18-1231,W17-4755,0,0.0342338,", for each input sentence, a random reference correction is chosen from the set of reference corrections to compute the GLEU score. This makes GLEU non-deterministic, unlike M2 and I-measure. The average GLEU score over m GLEU score computations is reported as the final score (m = 500 in standard GLEU). 3 Quantitative Evaluation The three GEC metrics are evaluated by measuring their correlation with human quality judgments treated as the ground truth. Following the experimental methodology in WMT metrics shared tasks (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016b; Bojar et al., 2017), we evaluate system-level correlation as well as sentence-level agreement of metrics with human judgments. We utilize the collection of human judgments of GEC outputs released in (Grundkiewicz et al., 2015). The dataset is annotated similar to the relative-ranking method adopted in the WMT metrics shared tasks in (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015). Human judgments are obtained for the system outputs of the 12 participating systems from the CoNLL-2014 shared task (Ng et al., 2014) and the input text as the thirteenth system (referred to as INPUT). Each human judgment consi"
C18-1231,P15-1068,1,0.887369,"s a mistake6 in GLEU computation that produced different results and incorrect conclusions in earlier studies. Moreover, no proper significance tests to compare the differences in correlations between metrics were done. Significance tests that reject the null hypothesis of having no correlation to humans is not adequate for comparing differences in correlations between metrics (Graham and Baldwin, 2014). Also, (Napoles et al., 2016b; Asano et al., 2017) use 18 references for CoNLL-2014 sentences for generating metric rankings, of which 2 are the references used in the shared task, 8 are from (Bryant and Ng, 2015), and another 8 are annotated by both experts and non-experts as sentence-rewrites (Sakaguchi 6 The brevity penalty was incorrectly implemented as exp(1−lh /lr ) instead of exp(1−lr /lh ). This was fixed in the version that we use (fixed on 10 June, 2017: https://github.com/cnap/gec-ranking/commit/50b503) 2738 et al., 2016). Automatically constructing M2 and I-measure annotations from sentence rewrites will be sub-optimal. Also, we observed that the non-determinism of GLEU scores is more pronounced when more number of references are used, resulting in large differences in correlation values. I"
C18-1231,P17-1074,0,0.0566337,"been proposed for GEC, namely, MaxMatch (M2 ), I-measure, and GLEU. 2.1 MaxMatch (M2 ) The M2 metric (Dahlmeier and Ng, 2012) computes precision, recall, and F-measure by maximally matching phrase-level edits made by a system to gold-standard edits annotated by humans. A goldstandard edit used by M2 includes the location of the error within the tokenized input sentence and the proposed correction. Although annotations about the type of errors can be included in the gold standard, they are not used while scoring. For computing scores for specific error types, a variant of M2 has been proposed (Bryant et al., 2017). The standard M2 computation is defined as follows. Consider a set of input sentences S = {s1 , ..., sn } and their corresponding system corrected hypotheses H = {h1 , ..., hn }. The set of gold-standard edits for each input sentence si is denoted by gi and the set of edits made by the system to transform si to hi is denoted by ei . Precision, recall, and F-measure are given by: Pn No. of correct edits made by the system i=1 |ei ∩ gi | precision = = P n Total no. of edits made by the system i=1 |ei | Pn No. of correct edits made by the system i=1 |ei ∩ gi | recall = = P n Total no. of gold-st"
C18-1231,C12-1038,0,0.0600729,"Missing"
C18-1231,N12-1067,1,0.949201,"hine Translation (WMT) (Bojar et al., 2016a) held annually. In the absence of annual evaluations and periodic human-judged shared tasks for GEC, robust automatic evaluation metrics are necessary to reliably assess improvements. Automatic evaluation methods can also help in system development and validation. Previous shared tasks on GEC relied on automatic evaluation metrics to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human judgments. In the case of MT, WMT organizes a metrics shared task each year, where the correlation of system-level rankings generate"
C18-1231,W11-2838,0,0.218511,"em. In the related field of machine translation (MT), human judgments are used as ground truth to rank systems in the evaluation campaigns as part of the Workshop on Machine Translation (WMT) (Bojar et al., 2016a) held annually. In the absence of annual evaluations and periodic human-judged shared tasks for GEC, robust automatic evaluation metrics are necessary to reliably assess improvements. Automatic evaluation methods can also help in system development and validation. Previous shared tasks on GEC relied on automatic evaluation metrics to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to"
C18-1231,W12-2006,0,0.122433,"machine translation (MT), human judgments are used as ground truth to rank systems in the evaluation campaigns as part of the Workshop on Machine Translation (WMT) (Bojar et al., 2016a) held annually. In the absence of annual evaluations and periodic human-judged shared tasks for GEC, robust automatic evaluation metrics are necessary to reliably assess improvements. Automatic evaluation methods can also help in system development and validation. Previous shared tasks on GEC relied on automatic evaluation metrics to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their corre"
C18-1231,N15-1060,0,0.757201,"necessary to reliably assess improvements. Automatic evaluation methods can also help in system development and validation. Previous shared tasks on GEC relied on automatic evaluation metrics to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human judgments. In the case of MT, WMT organizes a metrics shared task each year, where the correlation of system-level rankings generated by metrics and humans is measured. There is also a segment-level evaluation subtask where the agreement of metrics is measured at the sentence level. Prior work in GEC has followed the"
C18-1231,D14-1020,0,0.124783,"cted Wins with y¯ as the mean human score. On the other hand, Spearman correlation (ρ) computes a correlation coefficient based on ranks instead of scores: q P 6 (di )2 ρ = 1 − i=1 q(q 2 − 1) where di represents the difference in the human rank and metric rank of the ith system. Spearman correlation is more relaxed compared to Pearson correlation in terms of the assumptions made about variables and also less sensitive to outliers in the sample. In order to determine if a metric outperforms another, it is inadequate to measure differences in correlation alone. Following the recommendations in (Graham and Baldwin, 2014), we evaluate for significance of differences of correlation between metrics using William’s test (Williams, 1959). Note that prior work in human evaluation of GEC systems has not reported significance tests that account for the dependence between two metrics and hence the derived conclusions are not justified. 3.2 Measuring Sentence-Level Agreement Since system-level evaluation is done on a few systems (13 in our case), we also compute a fine-grained sentence-level agreement of metrics to human pairwise rankings. For this, sentence-level metric scores are computed5 . Agreement to humans is co"
C18-1231,D15-1052,0,0.274908,"a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human judgments. In the case of MT, WMT organizes a metrics shared task each year, where the correlation of system-level rankings generated by metrics and humans is measured. There is also a segment-level evaluation subtask where the agreement of metrics is measured at the sentence level. Prior work in GEC has followed the systemlevel evaluation approach to compare metrics (Grundkiewicz et al., 2015; Napoles et al., 2015; Sakaguchi et al., 2016). However, there has been no prior work on sentence-level evaluation. From prior studies, This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/ License details: http: 2730 Proceedings of the 27th International Conference on Computational Linguistics, pages 2730–2741 Santa Fe, New Mexico, USA, August 20-26, 2018. it may appear that GLEU performs better than the de-facto M2 metric. However, previous system-level correlation studies turn out to be inadequate, primarily due to the"
C18-1231,W14-3336,0,0.0589421,"cision like BLEU, nor picks the best like M2 and I-measure. Instead, for each input sentence, a random reference correction is chosen from the set of reference corrections to compute the GLEU score. This makes GLEU non-deterministic, unlike M2 and I-measure. The average GLEU score over m GLEU score computations is reported as the final score (m = 500 in standard GLEU). 3 Quantitative Evaluation The three GEC metrics are evaluated by measuring their correlation with human quality judgments treated as the ground truth. Following the experimental methodology in WMT metrics shared tasks (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016b; Bojar et al., 2017), we evaluate system-level correlation as well as sentence-level agreement of metrics with human judgments. We utilize the collection of human judgments of GEC outputs released in (Grundkiewicz et al., 2015). The dataset is annotated similar to the relative-ranking method adopted in the WMT metrics shared tasks in (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015). Human judgments are obtained for the system outputs of the 12 participating systems from the CoNLL-2014 shared task (Ng et al., 2014) and the input text as the"
C18-1231,P15-2097,0,0.706359,"ements. Automatic evaluation methods can also help in system development and validation. Previous shared tasks on GEC relied on automatic evaluation metrics to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human judgments. In the case of MT, WMT organizes a metrics shared task each year, where the correlation of system-level rankings generated by metrics and humans is measured. There is also a segment-level evaluation subtask where the agreement of metrics is measured at the sentence level. Prior work in GEC has followed the systemlevel evaluation approac"
C18-1231,D16-1228,0,0.512184,"uation methods can also help in system development and validation. Previous shared tasks on GEC relied on automatic evaluation metrics to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human judgments. In the case of MT, WMT organizes a metrics shared task each year, where the correlation of system-level rankings generated by metrics and humans is measured. There is also a segment-level evaluation subtask where the agreement of metrics is measured at the sentence level. Prior work in GEC has followed the systemlevel evaluation approach to compare metrics ("
C18-1231,W13-3601,1,0.913732,"n (MT), human judgments are used as ground truth to rank systems in the evaluation campaigns as part of the Workshop on Machine Translation (WMT) (Bojar et al., 2016a) held annually. In the absence of annual evaluations and periodic human-judged shared tasks for GEC, robust automatic evaluation metrics are necessary to reliably assess improvements. Automatic evaluation methods can also help in system development and validation. Previous shared tasks on GEC relied on automatic evaluation metrics to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human j"
C18-1231,W14-1701,1,0.957942,"gments are used as ground truth to rank systems in the evaluation campaigns as part of the Workshop on Machine Translation (WMT) (Bojar et al., 2016a) held annually. In the absence of annual evaluations and periodic human-judged shared tasks for GEC, robust automatic evaluation metrics are necessary to reliably assess improvements. Automatic evaluation methods can also help in system development and validation. Previous shared tasks on GEC relied on automatic evaluation metrics to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human judgments. In the"
C18-1231,P02-1040,0,0.110073,"input text. I-measure falls in the range1 [−1, 1], a negative value indicates degradation and a positive value indicates improvement. Unlike M2 , I measure can mix and match annotations from different annotators to produce more alternative references2 . 2.3 GLEU Unlike M2 and I-measure, GLEU (Napoles et al., 2015; Napoles et al., 2016a) only requires human annotators to correct by re-writing the source sentence without requiring annotations for individual errors. GLEU computes the precision of n-grams in the hypothesis that match part of the reference sentence, similar to the MT metric BLEU (Papineni et al., 2002). Additionally, GLEU penalizes n-grams in the hypotheses that match part of the input but not the reference. The original formulation (Napoles et al., 2015) included a weight parameter that had to be re-tuned according to the number of reference corrections used. Following the recommendation of Napoles et al. (2016a), we use the new formulation of GLEU3 which does not include this weight parameter and can work for any number of references. The computation of GLEU is done as follows. Consider a set of input sentences S = {s1 , ..., sn }, their corresponding corrected hypotheses H = {h1 , ..., h"
C18-1231,P16-1112,0,0.0127495,"erm in GLEU that penalizes preservation of n-grams in the 2737 input sentence that were supposed to be changed according to the reference. It can be argued that GLEU intends to reward GEC systems for detecting errors by assigning a partial credit to systems that make spurious changes at locations where corrections are deemed necessary by human annotators. However, this will encourage building GEC systems that provide inaccurate feedback and potentially mislead the end users (primarily language learners). Hence, it is better to build and evaluate grammatical error detection systems separately (Rei and Yannakoudakis, 2016). I-measure, on the other hand, assigns a negative score for Hypothesis 2 as it is considered to ‘degrade’ the input, although it is arguable that both hypotheses 1 and 2 are equally ungrammatical. In Example 2, when multiple references are used, GLEU gives a higher score to Hypothesis 3 (ungrammatical) than Hypothesis 1 and 2, both of which are grammatical and each matches one of the two references exactly. On the other hand, both M2 and I-measure assign a lower score to Hypothesis 3 and conform to our intuition. 5 5.1 Related Work GEC Evaluation When GEC was restricted to specific error type"
C18-1231,W15-3204,0,0.0191519,"s ground truth to rank systems in the evaluation campaigns as part of the Workshop on Machine Translation (WMT) (Bojar et al., 2016a) held annually. In the absence of annual evaluations and periodic human-judged shared tasks for GEC, robust automatic evaluation metrics are necessary to reliably assess improvements. Automatic evaluation methods can also help in system development and validation. Previous shared tasks on GEC relied on automatic evaluation metrics to evaluate the performance of participating systems (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015). The CoNLL-2014 shared task test set and evaluation metric, M2 (Dahlmeier and Ng, 2012), have been used as the primary benchmark for evaluating English GEC. After the CoNLL2014 shared task, two reference-based evaluation metrics, namely I-measure (Felice and Briscoe, 2015) and GLEU (Napoles et al., 2015; Napoles et al., 2016a), were proposed for GEC. More recently, some reference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human judgments. In the case of MT, WMT organizes"
C18-1231,W14-3301,0,0.0502636,"Missing"
C18-1231,Q16-1013,0,0.821626,"ference-less metrics were also proposed (Napoles et al., 2016b; Asano et al., 2017). A standard way to identify the best metric among them is to compare their correlation to human judgments. In the case of MT, WMT organizes a metrics shared task each year, where the correlation of system-level rankings generated by metrics and humans is measured. There is also a segment-level evaluation subtask where the agreement of metrics is measured at the sentence level. Prior work in GEC has followed the systemlevel evaluation approach to compare metrics (Grundkiewicz et al., 2015; Napoles et al., 2015; Sakaguchi et al., 2016). However, there has been no prior work on sentence-level evaluation. From prior studies, This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/ License details: http: 2730 Proceedings of the 27th International Conference on Computational Linguistics, pages 2730–2741 Santa Fe, New Mexico, USA, August 20-26, 2018. it may appear that GLEU performs better than the de-facto M2 metric. However, previous system-level correlation studies turn out to be inadequate, primarily due to the absence of significance tests and the choice of"
C18-1231,W17-5019,0,0.0495436,"t al., 2015) was proposed. It relied on whole sentence rewrites instead of span-based error annotations. However, our analysis shows that GLEU has several weaknesses in its formulation and has a non-deterministic behavior that makes it an unreliable alternative to prior metrics. A few reference-less evaluation methods have been proposed of which (Napoles et al., 2016b) considers the grammaticality of the output sentences alone to evaluate GEC systems. Recently, Asano et al. (2017) proposed improvements to (Napoles et al., 2016b) by additionally accounting for fluency and meaning preservation. Sakaguchi et al. (2017) suggested future directions of improving GEC evaluation such as considering whole document rewrites. 5.2 Human Judgments and Metric Quality Inspired from WMT, two collections of human judgments of GEC system outputs were released (Grundkiewicz et al., 2015; Napoles et al., 2015). We use the former in our study as it has a much higher number of human pairwise comparisons (109,098) compared to the latter (28,146). Also, (Napoles et al., 2015) includes references as one of the compared systems in order to act as a control measure to ensure quality of human judgments. However, this is unfair to s"
C18-1231,W15-3031,0,0.0270876,"Missing"
D07-1057,N04-1001,0,0.0278763,"Missing"
D07-1057,J95-2003,0,0.0672543,"e learning approach for Chinese zero pronoun resolution is comparable to her state-of-the-art rule-based approach. 7 Related Work Converse (2006) assumed that the gold standard Chinese anaphoric zero pronouns and the gold standard parse trees of the texts in Penn Chinese TreeBank (CTB) were given as input to her system, which performed resolution of the anaphoric zero pronouns using the Hobbs algorithm (Hobbs, 1978). Her system did not identify the anaphoric zero pronouns automatically. Yeh and Chen (2004) proposed an approach for Chinese zero pronoun resolution based on the Centering Theory (Grosz et al., 1995). Their system used a set of hand-engineered rules to perform zero pronoun identification, and resolved zero pronouns with a set of hand-engineered resolution rules. In Iida et al. (2006), they proposed a machine learning approach to resolve zero pronouns in Japanese using syntactic patterns. Their system also did not perform zero pronoun identification, and assumed that correctly identified zero pronouns were given as input to their system. Acknowledgements We thank Susan Converse and Martha Palmer for sharing their Chinese third-person pronoun and zero pronoun coreference corpus. References"
D07-1057,P06-1079,0,0.485854,"phoric zero pronouns and the gold standard parse trees of the texts in Penn Chinese TreeBank (CTB) were given as input to her system, which performed resolution of the anaphoric zero pronouns using the Hobbs algorithm (Hobbs, 1978). Her system did not identify the anaphoric zero pronouns automatically. Yeh and Chen (2004) proposed an approach for Chinese zero pronoun resolution based on the Centering Theory (Grosz et al., 1995). Their system used a set of hand-engineered rules to perform zero pronoun identification, and resolved zero pronouns with a set of hand-engineered resolution rules. In Iida et al. (2006), they proposed a machine learning approach to resolve zero pronouns in Japanese using syntactic patterns. Their system also did not perform zero pronoun identification, and assumed that correctly identified zero pronouns were given as input to their system. Acknowledgements We thank Susan Converse and Martha Palmer for sharing their Chinese third-person pronoun and zero pronoun coreference corpus. References Susan Converse. 2006. Pronominal Anaphora Resolution in Chinese. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania. Antonio Ferr´andez and Jes´us Pe"
D07-1057,C02-1139,0,0.0199796,"approach. To our knowledge, our work is the first to perform both identification and resolution of Chinese anaphoric zero pronouns using a machine learning approach. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases refer to the same entity in a text. It is an important task in discourse analysis, and successful coreference resolution benefits many natural language processing applications such as information extraction, question answering, etc. In the literature, much of the work on coreference resolution is for English text (Soon et al., 2001; Ng and Cardie, 2002b; Yang et al., 2003; McCallum and Wellner, 2005). Publicly available corpora for coreference resolution are mostly in English, e.g., the Message Understanding Conference tasks (MUC6 and MUC7)1 . Relatively less work has 1 http://www-nlpir.nist.gov/related_ projects/muc/ been done on coreference resolution for Chinese. Recently, the ACE Entity Detection and Tracking (EDT) task2 included annotated Chinese corpora for coreference resolution. Florian et al. (2004) and Zhou et al. (2005) reported research on Chinese coreference resolution. A prominent phenomenon in Chinese coreference resolution i"
D07-1057,P02-1014,0,0.0812275,"approach. To our knowledge, our work is the first to perform both identification and resolution of Chinese anaphoric zero pronouns using a machine learning approach. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases refer to the same entity in a text. It is an important task in discourse analysis, and successful coreference resolution benefits many natural language processing applications such as information extraction, question answering, etc. In the literature, much of the work on coreference resolution is for English text (Soon et al., 2001; Ng and Cardie, 2002b; Yang et al., 2003; McCallum and Wellner, 2005). Publicly available corpora for coreference resolution are mostly in English, e.g., the Message Understanding Conference tasks (MUC6 and MUC7)1 . Relatively less work has 1 http://www-nlpir.nist.gov/related_ projects/muc/ been done on coreference resolution for Chinese. Recently, the ACE Entity Detection and Tracking (EDT) task2 included annotated Chinese corpora for coreference resolution. Florian et al. (2004) and Zhou et al. (2005) reported research on Chinese coreference resolution. A prominent phenomenon in Chinese coreference resolution i"
D07-1057,P04-1020,0,0.0385447,"Missing"
D07-1057,C02-1078,0,0.518011,"Missing"
D07-1057,J01-4004,1,0.594894,"euristic rule-based approach. To our knowledge, our work is the first to perform both identification and resolution of Chinese anaphoric zero pronouns using a machine learning approach. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases refer to the same entity in a text. It is an important task in discourse analysis, and successful coreference resolution benefits many natural language processing applications such as information extraction, question answering, etc. In the literature, much of the work on coreference resolution is for English text (Soon et al., 2001; Ng and Cardie, 2002b; Yang et al., 2003; McCallum and Wellner, 2005). Publicly available corpora for coreference resolution are mostly in English, e.g., the Message Understanding Conference tasks (MUC6 and MUC7)1 . Relatively less work has 1 http://www-nlpir.nist.gov/related_ projects/muc/ been done on coreference resolution for Chinese. Recently, the ACE Entity Detection and Tracking (EDT) task2 included annotated Chinese corpora for coreference resolution. Florian et al. (2004) and Zhou et al. (2005) reported research on Chinese coreference resolution. A prominent phenomenon in Chinese cor"
D07-1057,P03-1023,0,0.013797,"ledge, our work is the first to perform both identification and resolution of Chinese anaphoric zero pronouns using a machine learning approach. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases refer to the same entity in a text. It is an important task in discourse analysis, and successful coreference resolution benefits many natural language processing applications such as information extraction, question answering, etc. In the literature, much of the work on coreference resolution is for English text (Soon et al., 2001; Ng and Cardie, 2002b; Yang et al., 2003; McCallum and Wellner, 2005). Publicly available corpora for coreference resolution are mostly in English, e.g., the Message Understanding Conference tasks (MUC6 and MUC7)1 . Relatively less work has 1 http://www-nlpir.nist.gov/related_ projects/muc/ been done on coreference resolution for Chinese. Recently, the ACE Entity Detection and Tracking (EDT) task2 included annotated Chinese corpora for coreference resolution. Florian et al. (2004) and Zhou et al. (2005) reported research on Chinese coreference resolution. A prominent phenomenon in Chinese coreference resolution is the prevalence of"
D07-1057,I05-2040,0,0.0266691,"Missing"
D07-1057,P00-1022,0,\N,Missing
D07-1085,P05-1055,0,0.0914742,"red in each phone lattice with a sufficiently high normalized log likelihood, and these counts were then used in retrieval under a vector space model with tf · idf weighting. Jones et al. (1996) combined retrieval from phone lattices using variations of James’ method with retrieval from 1best word transcripts to achieve better results. Since then, a number of different methods for SDR using lattices have been proposed. For instance, Siegler (1999) used word lattices instead of phone lattices as the basis of retrieval, and generalized the tf · idf formalism to allow uncertainty in word counts. Chelba and Acero (2005) preprocessed lattices into more compact Position Specific Posterior Lattices (PSPL), and computed an aggregate score for each document based on the posterior probability of edges and the proximity of search terms in the document. Mamou et al. (2006) converted each lattice into a word confusion network (Mangu et al., 2000), and estimated the inverse document frequency (idf ) of each word t as the ratio of the total number of words in the document collection to the total number of occurrences of t. Despite the differences in the details, the above lattice-based SDR methods have all been based o"
D07-1085,N04-1017,0,0.0649589,"rs. To represent the uncertainty in speech recognition, and to incorporate information from multiple transcription hypotheses rather than only the 1-best, it is desirable to use expected word counts from lattices output by a speech recognizer. In the context of spoken document search, Siegler (1999) described expected word counts and formulated a way to estimate expected word counts from lattices based on the relative ranks of word hypothesis probabilities; Chelba and Acero (2005) used a more explicit formula for computing word counts based on summing edge posterior probabilities in lattices; Saraclar and Sproat (2004) performed word-spotting in speech lattices by looking for word occurrences whose expected counts were above a certain threshold; and Yu et al. (2005) searched for phrases in spoken documents using a similar measure, the expected word relevance. Expected counts have also been used to summarize the phonotactics of a speech recording represented in a lattice: Hatch et al. (2005) performed speaker recognition by computing the expected counts of phone bigrams in a phone lattice, and estimating an unsmoothed probability distribution of phone bigrams. Although many uses of expected counts have been"
D07-1085,I05-3025,1,0.885174,"Missing"
D07-1085,H05-1119,0,0.0322144,"desirable to use expected word counts from lattices output by a speech recognizer. In the context of spoken document search, Siegler (1999) described expected word counts and formulated a way to estimate expected word counts from lattices based on the relative ranks of word hypothesis probabilities; Chelba and Acero (2005) used a more explicit formula for computing word counts based on summing edge posterior probabilities in lattices; Saraclar and Sproat (2004) performed word-spotting in speech lattices by looking for word occurrences whose expected counts were above a certain threshold; and Yu et al. (2005) searched for phrases in spoken documents using a similar measure, the expected word relevance. Expected counts have also been used to summarize the phonotactics of a speech recording represented in a lattice: Hatch et al. (2005) performed speaker recognition by computing the expected counts of phone bigrams in a phone lattice, and estimating an unsmoothed probability distribution of phone bigrams. Although many uses of expected counts have been studied, the use of statistical language models built from expected word counts has not been well explored. 2.3 Retrieval via Statistical Language Mod"
D08-1064,W07-0414,0,0.0217744,"tion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained. It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim. However, there is again a dovetailing engineering concern which is quite legitimate. If one wants to select the minimum-Bayes-risk translation from a lattice (or shared forest) instead of an n-best list (Tromble et al., 2008), or to select an oracle translation from a lattice (Tillmann and Zhang, 2006; Dreyer et al., 2007; Leusch et al., 2008), or to perform discriminative training on all the examples contained in a lattice (Taskar et al., 2004), one would need a metric that can be calculated on the edges of the lattice. Of the metrics surveyed in the WMT 2007 evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate (Nießen et al., 2000). Here, we deal with the related word recognition rate (McCowan et al., 2005), WRR = 1 − WER I+D+S = 1 − min |r| M−I = max |r| (5) where I is the number of insertions, D of deletions, S of substitutions, and M = |r |− D − S the number of matc"
D08-1064,N04-1035,0,0.021,"so tended to have shorter reference sentences (relative to the source sentences) than the harder genre, weblogs. For example, in one dataset, the newswire reference sets had between 1.3 and 1.37 English words per Arabic word, but the weblog reference set had 1.52 English words per Arabic word. Thus, a system that is uniformly verbose across both genres will apportion more of its output to newswire than to weblogs, serendipitously leading to a higher score. This phenomenon has subsequently been observed by Och (2008) as well. We trained three Arabic-English syntax-based statistical MT systems (Galley et al., 2004; Galley et al., 2006) using max-B training (Och, 2003): one on a newswire development set, one on a weblog development set, and one on a combined development set containing documents from both genres. We then translated a new mixed-genre test set in two ways: (1) each document with its appropriate genrespecific system, and (2) all documents with the system trained on the combined (mixed-genre) development set. In Table 3, we report the results of both approaches on the entire test dataset as well as the portion of the test dataset in each genre, for both the genre-specific and mixed-genre"
D08-1064,D08-1088,0,0.049341,"Missing"
D08-1064,W08-0301,0,0.0193383,"Missing"
D08-1064,P06-1096,0,0.0465977,"Missing"
D08-1064,C04-1072,0,0.034405,"t al., 2006), and ParaEval-Recall (Zhou et al., 2006). Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis. A variation on B is often used for these purposes, in which the k-gram precisions are “smoothed” by adding one to the numerator and denominator (Lin and Och, 2004); this addresses the problem of a zero k-gram match canceling out the entire score, but it does not address the problems illustrated above. The remaining issue, word deletion, is more difficult to assess. It could be argued that part of the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained. It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim. However, there is again a dovetailing engi"
D08-1064,N03-2021,0,0.178812,"ere one can obtain improvements in B scores that are questionable or even absurd. These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification to B and a cross between B and word error rate that address these issues while improving correlation with human judgments. 1 Introduction B (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Banerjee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MT literature. Callison-Burch et al. (2006) have subjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments. Both cases involve comparisons between statistical MT systems and other translation methods (human post-editing and a rule-based MT system), and they recommend that the use of B be restricted to comparisons between related systems or different versions of the same systems."
D08-1064,niessen-etal-2000-evaluation,0,0.283501,"have examined—except for word deletion—are traceable to the fact that B is not a sentence-level metric. Any metric which is defined as a weighted average of sentence-level scores, where the weights are system-independent, will be immune to these problems. Note that any metric involving micro-averaged precision (in which the sentence-level counts of matches and guesses are summed separately before forming their ratio) cannot have this property. Of the metrics surveyed in the WMT 2007 evaluation-evaluation (CallisonBurch et al., 2007), at least the following metrics have this property: WER (Nießen et al., 2000), TER (Snover et al., 2006), and ParaEval-Recall (Zhou et al., 2006). Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis. A variation on B is often used for these purposes, in which the k-gram precisions are “smoothed” by adding one to the numerator an"
D08-1064,P03-1021,0,0.0258309,"ce sentences) than the harder genre, weblogs. For example, in one dataset, the newswire reference sets had between 1.3 and 1.37 English words per Arabic word, but the weblog reference set had 1.52 English words per Arabic word. Thus, a system that is uniformly verbose across both genres will apportion more of its output to newswire than to weblogs, serendipitously leading to a higher score. This phenomenon has subsequently been observed by Och (2008) as well. We trained three Arabic-English syntax-based statistical MT systems (Galley et al., 2004; Galley et al., 2006) using max-B training (Och, 2003): one on a newswire development set, one on a weblog development set, and one on a combined development set containing documents from both genres. We then translated a new mixed-genre test set in two ways: (1) each document with its appropriate genrespecific system, and (2) all documents with the system trained on the combined (mixed-genre) development set. In Table 3, we report the results of both approaches on the entire test dataset as well as the portion of the test dataset in each genre, for both the genre-specific and mixed-genre trainings. The genre-specific systems each outperform the"
D08-1064,P02-1040,0,0.112252,"development of statistical machine translation systems. We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in B scores that are questionable or even absurd. These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification to B and a cross between B and word error rate that address these issues while improving correlation with human judgments. 1 Introduction B (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Banerjee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MT literature. Callison-Burch et al. (2006) have subjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments. Both cases involve comparisons between statistical MT systems and other translation methods (human post-editin"
D08-1064,2006.amta-papers.25,0,0.340133,"hat are questionable or even absurd. These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification to B and a cross between B and word error rate that address these issues while improving correlation with human judgments. 1 Introduction B (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Banerjee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MT literature. Callison-Burch et al. (2006) have subjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments. Both cases involve comparisons between statistical MT systems and other translation methods (human post-editing and a rule-based MT system), and they recommend that the use of B be restricted to comparisons between related systems or different versions of the same systems. In B’s defense, comparisons between differe"
D08-1064,P06-1091,0,0.0228924,"the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained. It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim. However, there is again a dovetailing engineering concern which is quite legitimate. If one wants to select the minimum-Bayes-risk translation from a lattice (or shared forest) instead of an n-best list (Tromble et al., 2008), or to select an oracle translation from a lattice (Tillmann and Zhang, 2006; Dreyer et al., 2007; Leusch et al., 2008), or to perform discriminative training on all the examples contained in a lattice (Taskar et al., 2004), one would need a metric that can be calculated on the edges of the lattice. Of the metrics surveyed in the WMT 2007 evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate (Nießen et al., 2000). Here, we deal with the related word recognition rate (McCowan et al., 2005), WRR = 1 − WER I+D+S = 1 − min |r| M−I = max |r| (5) where I is the number of insertions, D of deletions, S of substitutions, and M = |r |− D −"
D08-1064,D08-1065,0,0.0305721,"ord deletion, is more difficult to assess. It could be argued that part of the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained. It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim. However, there is again a dovetailing engineering concern which is quite legitimate. If one wants to select the minimum-Bayes-risk translation from a lattice (or shared forest) instead of an n-best list (Tromble et al., 2008), or to select an oracle translation from a lattice (Tillmann and Zhang, 2006; Dreyer et al., 2007; Leusch et al., 2008), or to perform discriminative training on all the examples contained in a lattice (Taskar et al., 2004), one would need a metric that can be calculated on the edges of the lattice. Of the metrics surveyed in the WMT 2007 evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate (Nießen et al., 2000). Here, we deal with the related word recognition rate (McCowan et al., 2005), WRR = 1 − WER I+D+S = 1 − min |r| M−I = max |r| (5) where I is th"
D08-1064,D07-1080,0,0.0145448,"ely before forming their ratio) cannot have this property. Of the metrics surveyed in the WMT 2007 evaluation-evaluation (CallisonBurch et al., 2007), at least the following metrics have this property: WER (Nießen et al., 2000), TER (Snover et al., 2006), and ParaEval-Recall (Zhou et al., 2006). Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis. A variation on B is often used for these purposes, in which the k-gram precisions are “smoothed” by adding one to the numerator and denominator (Lin and Och, 2004); this addresses the problem of a zero k-gram match canceling out the entire score, but it does not address the problems illustrated above. The remaining issue, word deletion, is more difficult to assess. It could be argued that part of the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which"
D08-1064,zhang-etal-2004-interpreting,0,0.0604902,"Missing"
D08-1064,W06-1610,0,0.0203609,"B is not a sentence-level metric. Any metric which is defined as a weighted average of sentence-level scores, where the weights are system-independent, will be immune to these problems. Note that any metric involving micro-averaged precision (in which the sentence-level counts of matches and guesses are summed separately before forming their ratio) cannot have this property. Of the metrics surveyed in the WMT 2007 evaluation-evaluation (CallisonBurch et al., 2007), at least the following metrics have this property: WER (Nießen et al., 2000), TER (Snover et al., 2006), and ParaEval-Recall (Zhou et al., 2006). Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis. A variation on B is often used for these purposes, in which the k-gram precisions are “smoothed” by adding one to the numerator and denominator (Lin and Och, 2004); this addresses the problem of a z"
D08-1064,E06-1032,0,\N,Missing
D08-1064,koen-2004-pharaoh,0,\N,Missing
D08-1064,W07-0738,0,\N,Missing
D08-1064,W05-0909,0,\N,Missing
D08-1064,P08-1007,1,\N,Missing
D08-1064,W07-0734,0,\N,Missing
D08-1064,N04-1022,0,\N,Missing
D08-1064,W07-0718,0,\N,Missing
D08-1064,P05-1066,0,\N,Missing
D08-1064,P06-1121,1,\N,Missing
D08-1064,W04-3250,0,\N,Missing
D08-1064,J07-2003,1,\N,Missing
D08-1082,J93-2003,0,0.0329288,"d. 2 S TATE : exclude (S TATE S TATE) S TATE : state (all) S TATE : loc 1 (R IVER) R IVER : river (all) Figure 1: An example MR structure Related Work In Section 9, we will compare performance with the three existing systems that were evaluated on the same data sets we consider. S ILT (Kate et al., 2005) learns deterministic rules to transform either sentences or their syntactic parse trees to meaning structures. WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. K RISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work ha"
D08-1082,J05-1003,0,0.0283535,"erceptron algorithms usually optimize the accuracy measure, we extend it to allow optimization of the F-measure by introducing an explicit separating plane on the feature space that rejects certain predictions even when they score highest. The idea is to find a threshold b after w is learned, such that a prediction with score below b gets rejected. We pick the threshold that leads to the optimal F-measure when applied to the training set. 8.2 Features We list in Table 2 the set of features we used. Examples are given based on the hybrid tree in Figure 789 3. Some of the them are adapted from (Collins and Koo, 2005) for a natural language parsing task. Features 1-5 are indicator functions (i.e., it takes value 1 if a certain combination as the ones listed in Table 2 is present, 0 otherwise), while feature 6 is real valued. Features that do not appear more than once in the training set are discarded. 9 Evaluation Our evaluations were performed on two corpora, G EOQUERY and ROBOCUP. The G EOQUERY corpus contains MR defined by a Prolog-based language used in querying a database on U.S. geography. The ROBOCUP corpus contains MR defined by a coaching language used in a robot coaching competition. There are in"
D08-1082,W02-1001,0,0.00973314,"el’s joint probability Example : S TATE : loc 1(R IVER ) → have R IVER : S TATE : loc 1(R IVER ) → hhave, R IVER : river(all)i : S TATE : exclude(S TATE S TATE ) → rivers : S TATE : loc 1(R IVER ) → rivers : hR IVER : river(all), S TATE : loc 1(R IVER )i → rivers   log b P(w, m, T ) . f1 f2 f3 f4 f5 Table 2: All the features used. There is one feature for each possible combination, under feature type 1-5. It takes value 1 if the combination is present, and 0 otherwise. Feature 6 takes real values. 8.1 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm (Collins, 2002) has previously been applied to various NLP tasks (Collins, 2002; Collins, 2001) for discriminative reranking. The detailed algorithm can be found in (Collins, 2002). In this section, we extend the conventional averaged perceptron by introducing an explicit separating plane on the feature space. Our reranking approach requires three components during training: a GEN function that defines for each NL sentence a set of candidate hybrid trees; a single correct reference hybrid tree for each training instance; and a feature function Φ that defines a mapping from a hybrid tree to a feature vector."
D08-1082,J03-4003,0,0.0881966,"well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not make use of an explicit grammar but, instead, models the correspondence between sentences and their meanings with a generative process. This model is defined over hybrid trees whose nodes include both natural language words and meaning representation tokens. Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. This implicit grammar representation leads to flexible learned models that generalize well. In practice, we observe that it can correctly parse a wider range of test examples than previous approaches. The generative model is learned from data that consists of sentences paired with their meaning representations. However, there is no explicit labeling of the correspondence between words and meaning tokens that is necessary for building the hybrid trees. This creates a challenging, hidde"
D08-1082,W05-0602,0,0.281678,"e translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. K RISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR s"
D08-1082,P06-2034,0,0.375014,"ues. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. K RISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider th"
D08-1082,P06-1115,0,0.830701,"ranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. 1 Introduction To enable computers to understand natural human language is one of the classic goals of research in natural language processing. Recently, researchers have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning (Wong and Mooney, 2006; Kate and Mooney, 2006). One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences. This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not make use of an explicit gr"
D08-1082,N06-1056,0,0.872814,"ith a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. 1 Introduction To enable computers to understand natural human language is one of the classic goals of research in natural language processing. Recently, researchers have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning (Wong and Mooney, 2006; Kate and Mooney, 2006). One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences. This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not ma"
D08-1082,P07-1121,0,0.687661,"s a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the G EO QUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hiera"
D08-1082,P01-1067,0,0.0121173,"accessible. In other words, there is a single deterministic derivation associated with each training instance. Therefore model parameters can be directly estimated from the training corpus by counting. However, in our task, the correct correspondence between NL words and MR structures is unknown. Many possible derivations could reach the same NL-MR pair, where each such derivation forms a hybrid tree. The hybrid tree is constructed using hidden variables and estimated from the training set. An efficient inside-outside style algorithm can be used for model estimation, similar to that used in (Yamada and Knight, 2001), as discussed next. 5.2.1 The Inside-Outside Algorithm with EM In this section, we discuss how to estimate the emission and pattern parameters with the Expectation Maximization (EM) algorithm (Dempster et al., 1977), by using an inside-outside (Baker, 1979) dynamic programming approach. Denote ni ≡ hmi , wi i as the i-th training instance, where mi and wi are the MR structure and the NL sentence of the i-th instance respectively. We also denote nv ≡ hmv , wv i as an aligned pair of MR substructure and contiguous NL substring, where the MR substructure rooted by MR production mv will correspon"
D08-1082,D07-1071,1,0.4397,"on structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 N UM : count (S TATE) Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the G EO QUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hierarchical tree 784 structure, as shown in Figure 1. Following an"
D08-1082,P02-1062,0,\N,Missing
D08-1105,D07-1007,0,0.111068,"Missing"
D08-1105,W05-0620,0,0.109896,"Missing"
D08-1105,P07-1007,1,0.907742,"Missing"
D08-1105,P07-1005,1,0.939931,"t required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types. 1 Introduction In language, many words have multiple meanings. The process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD). WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation (MT) (Chan et al., 2007a; Carpuat and Wu, 2007), information retrieval (IR), etc. WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process. In current WSD research, WordNet (Miller, 1990) is usually used as the sense inventory. WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems. Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data. To provide a standardized test-bed for evaluatio"
D08-1105,S07-1054,1,0.918993,"t required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types. 1 Introduction In language, many words have multiple meanings. The process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD). WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation (MT) (Chan et al., 2007a; Carpuat and Wu, 2007), information retrieval (IR), etc. WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process. In current WSD research, WordNet (Miller, 1990) is usually used as the sense inventory. WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems. Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data. To provide a standardized test-bed for evaluatio"
D08-1105,P05-1022,0,0.0182829,"Missing"
D08-1105,N06-1016,0,0.082362,"Missing"
D08-1105,P07-1033,0,0.0614218,"Missing"
D08-1105,W04-0827,0,0.0726415,"Missing"
D08-1105,W00-1322,0,0.316328,"Missing"
D08-1105,J98-4002,0,0.08872,"Missing"
D08-1105,N06-2015,0,0.126059,"Missing"
D08-1105,W02-1006,1,0.850205,"m OntoNotes. In Section 4, we investigate the WSD performance when we train our system on examples that are gathered from a different domain as compared to the OntoNotes evaluation data. In Section 5, we perform domain adaptation experiments using a recently introduced feature augmentation technique. In Section 6, we investigate the use of active learning to reduce the annotation effort required to adapt our WSD system to the domain of the OntoNotes data, before concluding in Section 7. 2 The WSD System For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words. For local collocations, we use 11 features: C−1,−1 , C1,1 , C−2,−2 , C2,2 , C−2,−1 , C−1,1 , C1,2 , C−3,−1 , C−2,1 , C−1,2 , and C1,3 , where Ci,j refers to the ordered sequence of tokens in the local context of an ambiguous word w. Offsets i and j denote the starting and ending position (relative to w) of the sequence, where a negative (positive) offset refers to a token to its left (right). For parts-of-speech, we use 7 features: P−3 , P−2 , P−"
D08-1105,J93-2004,0,0.032076,"Missing"
D08-1105,W00-1326,0,0.509439,"Missing"
D08-1105,S01-1031,0,0.0782408,"Missing"
D08-1105,H94-1046,0,0.154908,"Missing"
D08-1105,S07-1006,0,0.0738468,"Missing"
D08-1105,S01-1005,0,0.0502282,"Missing"
D08-1105,J05-1004,0,0.0838555,"Missing"
D08-1105,W04-0811,0,0.0787707,"Missing"
D08-1105,S07-1057,0,0.0446079,"Missing"
D08-1105,D07-1082,0,0.0606699,"Missing"
D08-1105,J03-4003,0,\N,Missing
D08-1105,D07-1096,0,\N,Missing
D09-1036,de-marneffe-etal-2006-generating,0,0.007779,"Missing"
D09-1036,C08-2022,0,0.643572,"can assist in answering why questions. Detecting contrast and restatements is useful for paraphrasing and summarization systems. While different discourse frameworks have been proposed from different perspectives (Mann and Thompson, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1998; Webber, 2004), most admit these basic types of discourse relationships between textual units. When there is a discourse connective (e.g., because) between two text spans, it is often easy to recognize the relation between the spans, as most connectives are unambiguous (Miltsakaki et al., 2005; Pitler et al., 2008). On the other hand, it is difficult to recognize the discourse relations when there are no explicit textual cues. We term these cases explicit and implicit relations, respectively. 2 Related Work One of the first works that use statistical methods to detect implicit discourse relations is that of Marcu and Echihabi (2002). They showed that word pairs extracted from two text spans provide clues for detecting the discourse relation between 343 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 343–351, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP t"
D09-1036,P09-1077,0,0.586044,"ourse relations in the Discourse Graphbank (Wolf and Gibson, 2005). Their experiments show that discourse connectives and the distance between the two text spans have the most impact, and event-based features also contribute to the performance. However, their system may not work well for implicit relations alone, as the two most prominent features only apply to explicit relations: implicit relations do not have discourse connectives and the two text spans of an implicit relation are usually adjacent to each other. The work that is most related to ours is the forthcoming paper of Pitler et al. (2009) on implicit relation classification on the second version of the PDTB. They performed classification of implicit discourse relations using several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. 3 Overview of the Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is a discourse level annotation (Prasad et al., 2008) over the one million word Wall Street Journal corpus. The PDTB adopts the predicate-argument view of discourse relations, where a discourse connective (e.g., because"
D09-1036,W06-1617,1,0.216405,"f Example 4. Fully embedded argument: prev embedded in curr.Arg1 next embedded in curr.Arg2 curr embedded in prev.Arg2 curr embedded in next.Arg1 Shared argument: prev.Arg2 = curr.Arg1 curr.Arg2 = next.Arg1 S-TPC-1 NP-SBJ VP PRP Table 2: Six contextual features derived from two discourse dependency patterns. curr is the relation we want to classify. We VBD NP had ADVP NP Constituent Parse Features. Research work from other NLP areas, such as semantic role labeling, has shown that features derived from syntactic trees are useful in semantic understanding. Such features include syntactic paths (Jiang and Ng, 2006) and tree fragments (Moschitti, 2004). From our observation of the PDTB relations, syntactic structure within one argument may constrain the relation type and the syntactic structure of the other argument. For example, the constituent parse structure in Figure 2(a) usually signals an Asynchronous relation when it appears in Arg2, as shown in Example 3, while the structure in Figure 2(b) usually acts as a clue for a Cause relation when it appears in Arg1, as shown in Example 4. In both examples, the lexicalized parts of the parse structure are bolded. DT NN NNS no operating problems IN DT at al"
D09-1036,prasad-etal-2008-penn,0,0.73134,"urse connectives and the two text spans of an implicit relation are usually adjacent to each other. The work that is most related to ours is the forthcoming paper of Pitler et al. (2009) on implicit relation classification on the second version of the PDTB. They performed classification of implicit discourse relations using several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. 3 Overview of the Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is a discourse level annotation (Prasad et al., 2008) over the one million word Wall Street Journal corpus. The PDTB adopts the predicate-argument view of discourse relations, where a discourse connective (e.g., because) is treated as a predicate that takes two text spans as its arguments. The argument that the discourse connective structurally attaches to is called Arg2, and the other argument is called Arg1. The PDTB provides annotations for explicit and implicit discourse relations. By definition, an explicit relation contains an explicit discourse connective. In the PDTB, 100 explicit connectives are annotated. Example 1 shows an explicit Co"
D09-1036,N06-2034,0,0.0171112,"lation between 343 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 343–351, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP the text spans. They used a set of textual patterns to automatically construct a large corpus of text span pairs from the web. These text spans were assumed to be instances of specific discourse relations. They removed the discourse connectives from the pairs to form an implicit relation corpus. From this corpus, they collected word pair statistics, which were used in a Na¨ıve Bayes framework to classify discourse relations. Saito et al. (2006) extended this theme, to show that phrasal patterns extracted from a text span pair provide useful evidence in the relation classification. For example, the pattern “... should have done ...” usually signals a contrast. The authors combined word pairs with phrasal patterns, and conducted experiments with these two feature classes to recognize implicit relations between adjacent sentences in a Japanese corpus. Both of these previous works have the shortcoming of downgrading explicit relations to implicit ones by removing the explicit discourse connectives. While this is a good approach to autom"
D09-1036,P02-1047,0,0.487895,"most admit these basic types of discourse relationships between textual units. When there is a discourse connective (e.g., because) between two text spans, it is often easy to recognize the relation between the spans, as most connectives are unambiguous (Miltsakaki et al., 2005; Pitler et al., 2008). On the other hand, it is difficult to recognize the discourse relations when there are no explicit textual cues. We term these cases explicit and implicit relations, respectively. 2 Related Work One of the first works that use statistical methods to detect implicit discourse relations is that of Marcu and Echihabi (2002). They showed that word pairs extracted from two text spans provide clues for detecting the discourse relation between 343 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 343–351, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP the text spans. They used a set of textual patterns to automatically construct a large corpus of text span pairs from the web. These text spans were assumed to be instances of specific discourse relations. They removed the discourse connectives from the pairs to form an implicit relation corpus. From this corpus, they colle"
D09-1036,W06-1317,0,0.0957313,"entences in a Japanese corpus. Both of these previous works have the shortcoming of downgrading explicit relations to implicit ones by removing the explicit discourse connectives. While this is a good approach to automatically create large corpora, natively implicit relations may be signaled in different ways. The fact that explicit relations are explicitly signaled indicates that such relations need a cue to be unambiguous to human readers. Thus, such an artificial implicit relation corpus may exhibit marked differences from a natively implicit one. We validate this claim later in this work. Wellner et al. (2006) used multiple knowledge sources to produce syntactic and lexico-semantic features, which were then used to automatically identify and classify explicit and implicit discourse relations in the Discourse Graphbank (Wolf and Gibson, 2005). Their experiments show that discourse connectives and the distance between the two text spans have the most impact, and event-based features also contribute to the performance. However, their system may not work well for implicit relations alone, as the two most prominent features only apply to explicit relations: implicit relations do not have discourse conne"
D09-1036,W98-0301,0,0.105757,"Missing"
D09-1036,J05-2005,0,0.138204,"arge corpora, natively implicit relations may be signaled in different ways. The fact that explicit relations are explicitly signaled indicates that such relations need a cue to be unambiguous to human readers. Thus, such an artificial implicit relation corpus may exhibit marked differences from a natively implicit one. We validate this claim later in this work. Wellner et al. (2006) used multiple knowledge sources to produce syntactic and lexico-semantic features, which were then used to automatically identify and classify explicit and implicit discourse relations in the Discourse Graphbank (Wolf and Gibson, 2005). Their experiments show that discourse connectives and the distance between the two text spans have the most impact, and event-based features also contribute to the performance. However, their system may not work well for implicit relations alone, as the two most prominent features only apply to explicit relations: implicit relations do not have discourse connectives and the two text spans of an implicit relation are usually adjacent to each other. The work that is most related to ours is the forthcoming paper of Pitler et al. (2009) on implicit relation classification on the second version o"
D09-1036,P04-1043,0,0.00818605,"ev embedded in curr.Arg1 next embedded in curr.Arg2 curr embedded in prev.Arg2 curr embedded in next.Arg1 Shared argument: prev.Arg2 = curr.Arg1 curr.Arg2 = next.Arg1 S-TPC-1 NP-SBJ VP PRP Table 2: Six contextual features derived from two discourse dependency patterns. curr is the relation we want to classify. We VBD NP had ADVP NP Constituent Parse Features. Research work from other NLP areas, such as semantic role labeling, has shown that features derived from syntactic trees are useful in semantic understanding. Such features include syntactic paths (Jiang and Ng, 2006) and tree fragments (Moschitti, 2004). From our observation of the PDTB relations, syntactic structure within one argument may constrain the relation type and the syntactic structure of the other argument. For example, the constituent parse structure in Figure 2(a) usually signals an Asynchronous relation when it appears in Arg2, as shown in Example 3, while the structure in Figure 2(b) usually acts as a clue for a Cause relation when it appears in Arg1, as shown in Example 4. In both examples, the lexicalized parts of the parse structure are bolded. DT NN NNS no operating problems IN DT at all Figure 3: A gold standard subtree f"
D09-1036,miltsakaki-etal-2004-penn,0,\N,Missing
D09-1036,C04-1020,0,\N,Missing
D09-1042,D08-1082,1,0.787788,"of Computer Science National University of Singapore luwei@nus.edu.sg {nght,leews}@comp.nus.edu.sg Abstract methods for constructing a natural language generation system. Given a set of pairs, where each pair consists of a natural language (NL) sentence and its formal meaning representation (MR), a learning method induces an algorithm that can be used for performing language generation from other previously unseen meaning representations. A crucial question in any natural language processing system is the representation used. Meaning representations can be in the form of a tree structure. In Lu et al. (2008), we introduced a hybrid tree framework together with a probabilistic generative model to tackle semantic parsing, where tree structured meaning representations are used. The hybrid tree gives a natural joint tree representation of a natural language sentence and its meaning representation. A joint generative model for natural language and its meaning representation, such as that used in Lu et al. (2008) has several advantages over various previous approaches designed for semantic parsing. First, unlike most previous approaches, the generative approach models a simultaneous generation process"
D09-1042,I05-1015,0,0.0166827,"amework and the LNLZ08 System Q UERY : answer(R IVER) R IVER : longest(R IVER) R IVER : exclude(R IVER1 R IVER2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL"
D09-1042,J08-1002,0,0.0111796,"vertex features, and the last are edge features. Examples are given based on Figure 3. All the features are indicator functions, i.e., a feature takes value 1 if a certain combination is present, and 0 otherwise. The last three features explicitly encode information from the tree structure of MR. Adjacent hybrid sequence features : two adjacent hybrid sequences, together with their associated MR productions. For example: f1 : hrun through S TATE1 , R IVER1 that does not R IVER2 , R IVER : traverse(S TATE), R IVER : exclude(R IVER1 , R IVER2 )i . For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. The model enables efficient training over packed trees that potentially represent exponential number of trees. The tree conditional random fields model can be effectively represented using the feature forest model. The model has also been successfully applied to the HPSG parsing task. To train the model, we run the Viterbi algorithm on the trained LNLZ08 model and perform convex optimization using the feature forest model. The LNLZ08 model is trained using an EM algori"
D09-1042,W05-1510,0,0.0211552,"stem Q UERY : answer(R IVER) R IVER : longest(R IVER) R IVER : exclude(R IVER1 R IVER2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most"
D09-1042,P03-1021,0,0.0178115,"Missing"
D09-1042,P02-1040,0,0.0946926,"Missing"
D09-1042,W05-0602,0,0.0529684,"ammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008), we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1. The MR is a tree consisting of nodes called MR productions. For example, the node “Q UERY : answer(R IVER)” is one MR production. Each MR production consists of a semantic category (“Q UERY”), a function symbol (“answer”) which can be optionally omitted, as well as an argument list which possibly contains 401 Q UERY : answer(R IVER) R IVER : longest(R IVER) what is R IVER : exclude(R IVER1 R IVER2"
D09-1042,W03-2316,0,0.0225286,"2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney,"
D09-1042,P06-1115,0,0.080526,"d Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semantic parsing. They introduced a system named WASP−1 what is the longest river that does not run through texas Figure 1: An example MR paired with its NL sentence. Following most previous works in this area (Kate et al., 2005; Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008), we consider MRs in the form of tree structures. An example MR and its corresponding natural language sentence are shown in Figure 1. The MR is a tree consisting of nodes called MR productions. For example, the node “Q UERY : answer(R IVER)” is one MR production. Each MR production consists of a semantic category (“Q UERY”), a function symbol (“answer”) which can be optionally omitted, as well as an argument list which possibly contains 401 Q UERY : answer(R IVER) R IVER : longest(R IVER) what is R IVER : exclude(R IVER1 R IVER2 ) the longest R IVER :"
D09-1042,N06-1056,0,0.0477113,"Missing"
D09-1042,P96-1027,0,0.181319,"OH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1 ++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System Q UERY : answer(R IVER) R IVER : longest(R IVER) R IVER : exclude(R IVER1 R IVER2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods for transforming MR to their corresponding NL sentences. Most of the recent systems tackled the problem through the architecture of chart generation introduced by Kay (1996). Examples of such systems include the chart generator for Head-Driven Phrase Structure Grammar (HPSG) (Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005), and more recently for Combinatory Categorial Grammar (CCG) (White and Baldridge, 2003; White, 2004). However, most of these systems only focused on surface realization (inflection and ordering of NL words) and ignored lexical selection (learning the mappings from MR domain concepts to NL words). The recent work by Wong and Mooney (2007) explored methods for generation by inverting a system originally designed for semanti"
D09-1042,N07-1022,0,0.783615,"eneration at the phrase level. Motivated by conditional random fields (CRF) (Lafferty et al., 2001), a different parameterization of the conditional probability of the hybrid tree that enables the model to encode some longer range dependencies amongst phrases and MRs is used. This novel model is referred to as the tree CRF-based model. Evaluation results for both models are presented, through which we demonstrate that the tree CRF -based model performs better than the direct inversion model. We also compare the tree CRFbased model against the previous state-of-the-art model of Wong and Mooney (2007). Furthermore, we evaluate our model on a dataset annotated with several natural languages other than English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972). The system took in a linearized MR tree as input, and translated it into a natural language sentence as output. Unlike most previous systems, their system integrated both lexical selection and surface realization in a single fr"
D09-1042,koen-2004-pharaoh,0,0.0100966,"n English (Japanese, Spanish, and Turkish). Evaluation results show that our proposed tree CRF-based model outperforms the previous model. 2 that employed techniques from statistical machine translation using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972). The system took in a linearized MR tree as input, and translated it into a natural language sentence as output. Unlike most previous systems, their system integrated both lexical selection and surface realization in a single framework. The performance of the system was enhanced by incorporating models borrowed from P HARAOH (Koehn, 2004). Experiments show that this new hybrid system named WASP−1 ++ gives state-of-the-art accuracies and outperforms the direct translation model obtained from P HARAOH, when evaluated on two corpora. We will compare our system’s performance against that of WASP−1 ++ in Section 5. 3 The Hybrid Tree Framework and the LNLZ08 System Q UERY : answer(R IVER) R IVER : longest(R IVER) R IVER : exclude(R IVER1 R IVER2 ) R IVER : river(all) R IVER : traverse(S TATE) S TATE : stateid(S TATE NAME) S TATE NAME : texas Related Work There have been substantial earlier research efforts on investigating methods f"
D09-1047,P05-1006,0,0.14483,"Missing"
D09-1047,J02-3001,0,0.557652,"f one classification step into the next stage. Finally, we present a joint model to determine the preposition sense and semantic role that maximize the joint probability. 3.1 WSD model Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. However, in this paper we use maximum entropy models2 (instead of support vector machines (SVM) reported in (Lee 2 Zhang Le’s Maximum Entropy Modeling Toolkit, http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html 452 Baseline Features (Gildea and Jurafsky, 2002) pred predicate lemma path path from constituent to predicate ptype syntactic category (NP, PP, etc.) pos relative position to the predicate voice active or passive voice hw syntactic head word of the phrase sub-cat rule expanding the predicate’s parent Advanced Features (Pradhan et al., 2005) hw POS POS of the syntactic head word PP hw/POS head word and POS of the rightmost NP child if the phrase is a PP first/last word first/last word and POS in the constituent parent ptype syntactic category of the parent node parent hw/POS head word and POS of the parent sister ptype phrase type of left an"
D09-1047,W05-0625,0,0.0249161,"//verbs.colorado.edu/framesets/arrive-v.html 450 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 450–458, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP has been identified as an argument of the predicate, we jointly infer its semantic role and the sense of the preposition that is the lexical head of the prepositional phrase. That is, our model maximizes the joint probability of the semantic role and the preposition sense. Previous research has shown the benefit of jointly learning semantic roles of multiple constituents (Toutanova et al., 2008; Koomen et al., 2005). In contrast, our joint model makes predictions for a single constituent, but multiple tasks (WSD and SRL) . Our experiments show that adding the SRL information leads to statistically significant improvements over an independent, state-of-the-art WSD classifier. For the SRL task, we show statistically significant improvements of our joint model over an independent, state-of-the-art SRL classifier for locative and temporal adjunctive arguments, even though the overall improvement over all semantic roles is small. To the best of our knowledge, no previous research has attempted to perform prep"
D09-1047,W02-1006,1,0.904265,"dels output probability distributions, unlike SVM. This property is useful in the joint model, as we will see later. Maxent models have been successfully applied to various NLP tasks and achieve state-of-the-art performance. There are two training parameters that have to be adjusted for maxent models: the number of training iterations and the Gaussian smoothing parameter. We find optimal values for both parameters through 10-fold crossvalidation on the training set. For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of Lee and Ng (2002). These features encode three knowledge sources: task of determining the semantic role of prepositional phrases. The reason is that argument identification mostly relies on syntactic features, like the path from the constituent to the predicate (Pradhan et al., 2005). Consider, for example, the phrase in the dark in the sentence: “We are in the dark”, he said. The phrase is clearly not an argument to the verb say. But if we alter the syntactic structure of the sentence appropriately (while the sense of the preposition in remains unchanged), the same phrase suddenly becomes an adjunctive argume"
D09-1047,S07-1005,0,0.227627,"Missing"
D09-1047,J93-2004,0,0.0322374,"s a set of fine-grained senses, which are grouped together into a smaller number of coarse-grained senses. In our experiments, we only focus on coarse-grained senses since better inter-annotator agreement can be achieved on coarse-grained senses, which also results in higher accuracy of the trained WSD classifier. 2.2 The task of semantic role labeling in the context of PropBank (Palmer et al., 2005) is to label tree nodes with semantic roles in a syntactic parse tree. The PropBank corpus adds a semantic layer to parse trees from the Wall Street Journal section of the Penn Treebank II corpus (Marcus et al., 1993). There are two classes of semantic roles: core arguments and adjunctive arguments. Core arguments are verb sense specific, i.e., their meaning is defined relative to a specific verb sense. They are labeled with consecutive numbers A RG 0, A RG 1, etc. A RG 0 usually denotes the AGENT and A RG 1 the T HEME of the event. Besides the core arguments, a verb can have a number of adjunctive arguments that express more general properties like time, location, or manner. They are labeled as A RGM plus a functional tag, e.g., LOC for locative or TMP for temporal modifiers. Prepositional phrases can app"
D09-1047,W03-0411,0,0.485588,"Missing"
D09-1047,J09-2002,0,0.557608,"Missing"
D09-1047,W04-3220,0,0.0613291,"Missing"
D09-1047,J05-1004,0,0.0798401,"ational linguistics research. For each of the over 300 prepositions and phrasal prepositions, the database contains a set of sense definitions, which are based on the Oxford Dictionary of English. Every preposition has a set of fine-grained senses, which are grouped together into a smaller number of coarse-grained senses. In our experiments, we only focus on coarse-grained senses since better inter-annotator agreement can be achieved on coarse-grained senses, which also results in higher accuracy of the trained WSD classifier. 2.2 The task of semantic role labeling in the context of PropBank (Palmer et al., 2005) is to label tree nodes with semantic roles in a syntactic parse tree. The PropBank corpus adds a semantic layer to parse trees from the Wall Street Journal section of the Penn Treebank II corpus (Marcus et al., 1993). There are two classes of semantic roles: core arguments and adjunctive arguments. Core arguments are verb sense specific, i.e., their meaning is defined relative to a specific verb sense. They are labeled with consecutive numbers A RG 0, A RG 1, etc. A RG 0 usually denotes the AGENT and A RG 1 the T HEME of the event. Besides the core arguments, a verb can have a number of adjun"
D09-1047,J08-2002,0,0.0961004,"Missing"
D09-1047,W04-3212,0,0.205729,"ase sub-cat rule expanding the predicate’s parent Advanced Features (Pradhan et al., 2005) hw POS POS of the syntactic head word PP hw/POS head word and POS of the rightmost NP child if the phrase is a PP first/last word first/last word and POS in the constituent parent ptype syntactic category of the parent node parent hw/POS head word and POS of the parent sister ptype phrase type of left and right sister sister hw/POS head word and POS of left and right sister temporal temporal key words present partPath partial path predicate proPath projected path without directions Feature Combinations (Xue and Palmer, 2004) pred & ptype predicate and phrase type pred & hw predicate and head word pred & path predicate and path pred & pos predicate and relative position conditional probability of the sense, given the feature representation of the surrounding context c. The classifier outputs the sense that receives the highest probability: sˆ = argmax P (s|Ψ(c)) s (1) where Ψ(·) is a feature map from the surrounding context to the feature representation. To ensure that our model is competitive, we tested our system on the data set from the SemEval 2007 preposition WSD task (Litkowski and Hargraves, 2007). Our base"
D09-1047,S07-1051,0,0.203764,"Missing"
D09-1047,W06-1617,1,\N,Missing
D09-1141,P05-1074,0,0.122401,"Missing"
D09-1141,P07-1083,0,0.0560722,"and Portuguese. Malay and Indonesian are mutually intelligible, but differ in pronunciation and vocabulary. An example follows5 : • Malay: Semua manusia dilahirkan bebas dan samarata dari segi kemuliaan dan hakhak. Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸a˜ o corresponds to the Spanish suffix -ci´on, e.g., evoluc¸a˜ o vs. evoluci´on. Such correspondences can be quite frequent and thus easy to learn automatically4 . Even 3 more frequent can be the inflectional variations. For example,"
D09-1141,W07-0702,0,0.0504892,"Missing"
D09-1141,J93-2003,0,0.022832,"acted pairs of sentences from the matched document pairs using competitive linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold. The ml-en was built in a similar manner. 7 Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (X∈{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phr"
D09-1141,N06-1003,0,0.0650842,"what inconsistent in that respect. The latter method performs worst and is the only one to go below the baseline (for 10K ml-en pairs). Table 3 shows the results when using pt-en data to improve Spanish→English SMT. Overall, the results and the conclusions that can be made are consistent with those for Table 2. We can further observe that, as the size of the original bi-text increases, the gain in Bleu decreases, which is to be expected. Note also that here transliteration is very important: it doubles the absolute gain in Bleu. Finally, Table 4 shows a comparison to the pivoting technique of Callison-Burch et al. (2006). for English→Spanish SMT. Despite using just Portuguese, we achieve an improvement that is, in five out of six cases, much better than what they achieve with eight pivot languages (which include not only Portuguese, but also two other Romance languages, French and Italian, which are closely related to Spanish). Moreover, our method yields improvements for very large original datasets – 1.2M pairs, while theirs stops improving at 160K. However, our improvements are only statistically significant for 160K original pairs or less. Finally, note that our translation direction is reversed. Based on"
D09-1141,P05-1066,0,0.0517566,"Missing"
D09-1141,I08-8003,0,0.0348734,"Missing"
D09-1141,W09-0431,0,0.0442964,"Missing"
D09-1141,A00-1002,0,0.438597,"Missing"
D09-1141,2005.iwslt-1.8,0,0.0183452,"omatically extracted likely cognates. The system was applied on the Portuguese side of the pt-en training bi-text. Classic approaches to automatic cognate extraction look for non-stopwords with similar spelling that appear in parallel sentences in a bi-text (Kondrak et al., 2003). In our case, however, we need to extract cognates between Spanish and Portuguese given pt-en and es-en bi-texts only, i.e., without having a pt-es bi-text. Although it is easy to construct a pt-es bi-text from the Europarl corpus, we chose not to do so since, in general, synthe8 We also tried lexicalized reordering (Koehn et al., 2005). While it yielded higher absolute Bleu scores, the relative improvement for a sample of our experiments was very similar to that achieved with distance-based re-ordering. 9 We used version 11b of the NIST scoring tool: http://www.nist.gov/speech/tools/ 1361 sizing a bi-text for X1 -X2 would be impossible: e.g., it cannot be done for ml-in given our training datasets for in-en and ml-en since the English sides of these bi-texts have no sentences in common. Thus, we extracted the list of likely cognates between Portuguese and Spanish from the training pt-en and es-en bi-texts using English as a"
D09-1141,P07-2045,0,0.00797891,"parallel sentences for X1 -Y and a larger bi-text for X2 -Y for some resource-rich language X2 that is closely related to X1 . The evaluation for Indonesian→English (using Malay) and Spanish→English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data. 1 Introduction Recent developments in statistical machine translation (SMT), e.g., the availability of efficient implementations of integrated open-source toolkits like Moses (Koehn et al., 2007), have made it possible to build a prototype system with decent translation quality for any language pair in a few days or even hours. In theory. In practice, doing so requires having a large set of parallel sentencealigned bi-lingual texts (a bi-text) for that language pair, which is often unavailable. Large highquality bi-texts are rare; except for Arabic, Chinese, and some official languages of the European Union (EU), most of the 6,500+ world languages remain resource-poor from an SMT viewpoint. While manually creating a small bi-text could be relatively easy, building a large one is hard,"
D09-1141,2005.mtsummit-papers.11,0,0.0119969,"t of the 6,500+ world languages remain resource-poor from an SMT viewpoint. While manually creating a small bi-text could be relatively easy, building a large one is hard, e.g., because of copyright. Most bi-texts for SMT come from parliament debates and legislation of multi-lingual countries (e.g., French-English from Canada, and Chinese-English from Hong Kong), or from international organizations like the United Nations and the European Union. For example, the Europarl corpus of parliament proceedings consists of about 1.3M parallel sentences (up to 44M words) per language for 11 languages (Koehn, 2005), and the JRC-Acquis corpus provides a comparable amount of European legislation in 22 languages (Steinberger et al., 2006). The official languages of the EU are especially lucky in that respect; while this includes such “classic SMT languages” like English and French, and some important international ones like Spanish and Portuguese, most of the rest have a limited number of speakers and were resource-poor until recently; this is changing quickly because of the increasing volume of EU parliament debates and the ever-growing European legislation. Thus, becoming an official language of the EU h"
D09-1141,N03-2016,0,0.586373,"inally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9 . 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list of automatically extracted likely cognates. The system was applied on the Portuguese side of the pt-en training bi-text. Classic approaches to automatic cognate extraction look for non-stopwords with similar spelling that appear in parallel sentences in a bi-text (Kondrak et al., 2003). In our case, however, we need to extract cognates between Spanish and Portuguese given pt-en and es-en bi-texts only, i.e., without having a pt-es bi-text. Although it is easy to construct a pt-es bi-text from the Europarl corpus, we chose not to do so since, in general, synthe8 We also tried lexicalized reordering (Koehn et al., 2005). While it yielded higher absolute Bleu scores, the relative improvement for a sample of our experiments was very similar to that achieved with distance-based re-ordering. 9 We used version 11b of the NIST scoring tool: http://www.nist.gov/speech/tools/ 1361 si"
D09-1141,N01-1020,0,0.0617917,"Indonesian are mutually intelligible, but differ in pronunciation and vocabulary. An example follows5 : • Malay: Semua manusia dilahirkan bebas dan samarata dari segi kemuliaan dan hakhak. Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸a˜ o corresponds to the Spanish suffix -ci´on, e.g., evoluc¸a˜ o vs. evoluci´on. Such correspondences can be quite frequent and thus easy to learn automatically4 . Even 3 more frequent can be the inflectional variations. For example, in Portuguese and Spanis"
D09-1141,W95-0115,0,0.228601,"ng pj is conditionally independent of sk given ei , we can simplify the above expression: Pr(pj |sk ) = P i Pr(pj |ei )Pr(ei |sk ) Similarly, for Pr(sk |pj ), we obtain Pr(sk |pj ) = P i Pr(sk |ei )Pr(ei |pj ) We excluded all stopwords, words of length less than three, and those containing digits. We further calculated Prod(pj , sk ) = Pr(pj |sk )Pr(sk |pj ), and we excluded all Portuguese-Spanish word pairs (pj , sk ) for which Prod(pj , sk ) &lt; 0.01. From the remaining pairs, we extracted likely cognates based on Prod(pj , sk ) and on the orthographic similarity between pj and sk . Following Melamed (1995), we measured the orthographic similarity using the longest common subsequence ratio (LCSR), defined as follows: LCSR (s1 , s2 ) = |LCS(s1 ,s2 )| max(|s1 |,|s2 |) where LCS(s1 , s2 ) is the longest common subsequence of s1 and s2 , and |s |is the length of s. We retained as likely cognates all pairs for which LCSR was 0.58 or higher; that value was found by Kondrak et al. (2003) to be optimal for a number of language pairs in the Europarl corpus. Finally, we performed competitive linking (Melamed, 2000), assuming that each Portuguese wordform had at most one Spanish best cognate match. Thus, u"
D09-1141,J99-1003,0,0.0125866,"ntelligible, but differ in pronunciation and vocabulary. An example follows5 : • Malay: Semua manusia dilahirkan bebas dan samarata dari segi kemuliaan dan hakhak. Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸a˜ o corresponds to the Spanish suffix -ci´on, e.g., evoluc¸a˜ o vs. evoluci´on. Such correspondences can be quite frequent and thus easy to learn automatically4 . Even 3 more frequent can be the inflectional variations. For example, in Portuguese and Spanish respectively,"
D09-1141,J00-2004,0,0.0250236,"uilt from release v.3 of the Europarl corpus, excluding the Q4/2000 portion out of which we created our testing and development datasets. We built the in-en bi-texts from texts that we downloaded from the Web. We translated the Indonesian texts to English using Google Translate, and we matched7 them against the English texts using a cosine similarity measure and heuristic constraints based on document length in words and in sentences, overlap of numbers, words in uppercase, and words in the title. Next, we extracted pairs of sentences from the matched document pairs using competitive linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold. The ml-en was built in a similar manner. 7 Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (X∈{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and ext"
D09-1141,J03-1002,0,0.0113431,"ive linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold. The ml-en was built in a similar manner. 7 Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (X∈{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 200"
D09-1141,J04-4002,0,0.0195743,"in a similar manner. 7 Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (X∈{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish)"
D09-1141,P03-1021,0,0.0047784,"s of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish), and we used them in a beam search decoder (Koehn et al., 2007) to translate 2,000 test sentences (Indonesian or Spanish) into English. Finally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9 . 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list of automatically extracted likely cognates. The system"
D09-1141,P02-1040,0,0.0912337,"(Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish), and we used them in a beam search decoder (Koehn et al., 2007) to translate 2,000 test sentences (Indonesian or Spanish) into English. Finally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9 . 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list o"
D09-1141,W99-0626,0,0.0192186,"eration can help a lot in case of systematic spelling variations between the original and the additional source languages. 5 Related Work In this section, we describe two general lines of related previous research: using cognates between the source and the target language, and sourcelanguage side paraphrasing with a pivot language. 5.1 Cognates Many researchers have used likely cognates to obtain improved word alignments and thus build better SMT systems. Al-Onaizan et al. (1999) extracted such likely cognates for Czech-English using one of the variations of LCSR (Melamed, 1995) described in (Tiedemann, 1999) as a similarity measure. They used these cognates to improve word alignments with IBM models 1-4 in three different ways: (1) by seeding the parameters of IBM model 1, (2) by constraining the word cooccurrences when training IBM models 1-4, and (3) by adding the cognate pairs to the bi-text as additional “sentence pairs”. The last approach performed best and was later used by Kondrak et al. (2003) who demonstrated improved SMT for nine European languages. Unlike these approaches, which extract cognates between the source and the target language, we use cognates between the source and some oth"
D09-1141,N07-1061,0,0.157311,"Missing"
D09-1141,P07-1108,0,0.123603,"ven our training datasets for in-en and ml-en since the English sides of these bi-texts have no sentences in common. Thus, we extracted the list of likely cognates between Portuguese and Spanish from the training pt-en and es-en bi-texts using English as a pivot as follows: We started with IBM model 4 word alignments, from which we extracted four conditional lexical translation probabilities: Pr(pj |ei ) and Pr(ei |pj ) for Portuguese-English, and Pr(sk |ei ) and Pr(ei |sk ) for Spanish-English, where pj , ei and sk stand for a Portuguese, an English and a Spanish word respectively. Following Wu and Wang (2007), we then induced conditional lexical translation probabilities Pr(pj |sk ) and Pr(sk |pj ) for Portuguese-Spanish as follows: Pr(pj |sk ) = P i Pr(pj |ei , sk )Pr(ei |sk ) Assuming pj is conditionally independent of sk given ei , we can simplify the above expression: Pr(pj |sk ) = P i Pr(pj |ei )Pr(ei |sk ) Similarly, for Pr(sk |pj ), we obtain Pr(sk |pj ) = P i Pr(sk |ei )Pr(ei |pj ) We excluded all stopwords, words of length less than three, and those containing digits. We further calculated Prod(pj , sk ) = Pr(pj |sk )Pr(sk |pj ), and we excluded all Portuguese-Spanish word pairs (pj , sk"
D09-1141,W10-1751,0,\N,Missing
D09-1141,N04-1035,0,\N,Missing
D09-1141,steinberger-etal-2006-jrc,0,\N,Missing
D09-1141,2008.iwslt-papers.1,0,\N,Missing
D09-1141,N06-1011,0,\N,Missing
D09-1141,H93-1039,0,\N,Missing
D09-1141,C96-2141,0,\N,Missing
D09-1141,P00-1037,0,\N,Missing
D09-1141,W02-2026,0,\N,Missing
D09-1141,W09-1117,0,\N,Missing
D09-1141,D07-1005,0,\N,Missing
D09-1141,D08-1021,0,\N,Missing
D09-1141,P06-1012,1,\N,Missing
D09-1141,P07-1034,0,\N,Missing
D09-1141,P98-2238,0,\N,Missing
D09-1141,C98-2233,0,\N,Missing
D09-1141,D08-1090,0,\N,Missing
D09-1141,C10-1027,0,\N,Missing
D09-1141,W07-0705,0,\N,Missing
D09-1141,P07-1007,1,\N,Missing
D09-1141,N09-2056,0,\N,Missing
D09-1141,W02-0902,0,\N,Missing
D09-1141,P07-3010,0,\N,Missing
D09-1141,W09-0412,1,\N,Missing
D09-1141,2009.eamt-1.3,0,\N,Missing
D09-1141,N09-1025,0,\N,Missing
D09-1141,P08-1088,0,\N,Missing
D09-1141,P05-1033,0,\N,Missing
D09-1141,N03-1017,0,\N,Missing
D09-1141,P05-1034,0,\N,Missing
D09-1141,J10-3007,0,\N,Missing
D09-1141,E09-1082,0,\N,Missing
D09-1141,P07-1092,0,\N,Missing
D09-1141,W06-2003,0,\N,Missing
D09-1141,2005.eamt-1.19,0,\N,Missing
D09-1141,mulloni-pekar-2006-automatic,0,\N,Missing
D09-1141,W06-2005,0,\N,Missing
D10-1018,P96-1041,0,0.0614142,"sequentially from left to right. In the cascaded approach, we format the training sentences by replacing all sentence-ending punctuation symbols with special sentence boundary symbols first. A model for sentence boundary prediction is learned based on such training data. This step is then followed by predicting the actual punctuation symbols. Both trigram and 5-gram language models are tried for all combinations of the above settings. This gives us a total of 8 possible combinations based on the hidden event language model. When training all the language models, modified Kneser-Ney smoothing (Chen and Goodman, 1996) for n-grams is used. To assess the performance of the punctuation prediction task, we compute precision (prec.), recall (rec.), and F1-measure (F1 ), as defined by the following equations: http://www.cis.upenn.edu/∼treebank/tokenization.html 182 = # Correctly predicted punctuation symbols # predicted punctuation symbols # Correctly predicted punctuation symbols # expected punctuation symbols 2 1/prec. + 1/rec. Performance on Correctly Recognized Texts The performance of punctuation prediction on both Chinese (C N) and English (E N) texts in the correctly recognized output of the BTEC and CT d"
D10-1018,1993.eamt-1.1,0,0.156259,"w many/much . . . ). These pose difficulties for the simple hidden event language model, which only encodes simple dependencies over surrounding words by means of n-gram language modeling. By adopting a discriminative model which exploits non-independent, overlapping features, the LC RF model generally outperforms the hidden event language model. By introducing an additional layer of tags for performing sentence segmentation and sentence type prediction, the F-C RF model further boosts the performance over the L-C RF model. We perform statistical significance tests using bootstrap resampling (Efron et al., 1993). The improvements of F-C RF over L-C RF are statistically significant (p < 0.01) on Chinese and English texts in the CT BTEC LM ORDER Prec. CN Rec. F1 Prec. EN Rec. F1 N O D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 85.96 84.80 86.48 85.12 81.87 82.78 83.15 82.78 83.86 83.78 84.78 83.94 62.38 59.29 56.86 54.22 64.17 60.99 58.76 56.21 63.27 60.13 57.79 55.20 U SE D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 66.86 68.76 68.00 68.75 63.92 66.12 65.38 66.48 65.36 67.41 66.67 67.60 85.23 87.29 84.49 81.32 88.22 89.65 87.58 84.55 86.70 88.45 86.00 82.90 L-C RF F-C RF 92.81 85.16 88.83 90.67 88.22 89"
D10-1018,2007.iwslt-1.13,0,0.0167097,"ze the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as 178 part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts consider the boundary detection and punctuation"
D10-1018,2005.iwslt-1.8,0,0.0189298,"ll the default settings of Moses, except with the lexicalized reordering model enabled. This is because 3 http://code.google.com/p/berkeleyaligner/ LM ORDER CN → EN EN → CN N O D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.77 30.71 30.98 30.64 21.21 21.00 21.16 20.76 U SE D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.16 30.26 30.33 30.42 23.03 24.04 23.61 23.34 L-C RF F-C RF 31.27 23.44 31.30 24.18 Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of B LEU) lexicalized reordering gives better performance than simple distance-based reordering (Koehn et al., 2005). Specifically, the default lexicalized reordering model (msd-bidirectional-fe) is used. For tuning the parameters of Moses, we use the official IWSLT05 evaluation set where the correct punctuation symbols are present. Evaluations are performed on the ASR outputs of the IWSLT08 BTEC evaluation dataset, with punctuation symbols inserted by each punctuation prediction method. The tuning set and evaluation set include 7 reference translations. Following a common practice in statistical machine translation, we report B LEU-4 scores (Papineni et al., 2002), which were shown to have good correlation"
D10-1018,P07-2045,0,0.0509435,"feeding the punctuated ASR texts to a state-of-the-art machine translation system, and evaluate the resulting translation performance. The translation performance is in turn measured by an automatic evaluation metric which correlates well with human judgments. We believe that such a task-oriented approach for evaluating the quality of punctuation prediction for ASR output texts is useful, since it tells us how well the punctuated ASR output texts from each punctuation prediction system can be used for further processing, such as in statistical machine translation. In this paper, we use Moses (Koehn et al., 2007), a state-of-the-art phrase-based statistical machine translation toolkit, as our translation engine. We use the entire IWSLT09 BTEC training set for training the translation system. The state-of-the-art unsupervised Berkeley aligner3 (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexicalized reordering model enabled. This is because 3 http://code.google.com/p/berkeleyaligner/ LM ORDER CN → EN EN → CN N O D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.77 30.71 30.98 30.64 21.21 21.00 21.16 20.76 U SE D UPLICATION S INGLE"
D10-1018,N06-1014,0,0.0268783,"ents. We believe that such a task-oriented approach for evaluating the quality of punctuation prediction for ASR output texts is useful, since it tells us how well the punctuated ASR output texts from each punctuation prediction system can be used for further processing, such as in statistical machine translation. In this paper, we use Moses (Koehn et al., 2007), a state-of-the-art phrase-based statistical machine translation toolkit, as our translation engine. We use the entire IWSLT09 BTEC training set for training the translation system. The state-of-the-art unsupervised Berkeley aligner3 (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexicalized reordering model enabled. This is because 3 http://code.google.com/p/berkeleyaligner/ LM ORDER CN → EN EN → CN N O D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.77 30.71 30.98 30.64 21.21 21.00 21.16 20.76 U SE D UPLICATION S INGLE PASS C ASCADED 3 5 3 5 30.16 30.26 30.33 30.42 23.03 24.04 23.61 23.34 L-C RF F-C RF 31.27 23.44 31.30 24.18 Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of B LEU) lexicalized reordering gives better"
D10-1018,P05-1056,0,0.0417853,"task. Kim and Woodland (2001) performed punctuation insertion during speech recognition. Prosodic features together with language model probabilities were used within a decision tree framework. Christensen et al. (2001) focused on the broadcast news domain and investigated both finite state and multi-layer perceptron methods for the task, where prosodic and lexical information was incorporated. Huang and Zweig (2002) presented a maximum entropy-based tagging approach to punctuation insertion in spontaneous English conversational speech, where both lexical and prosodic features were exploited. Liu et al. (2005) focused on the sentence boundary detection task, by making use of conditional random fields (CRF) (Lafferty et al., 2001). Their method was shown to improve over a previous method based on hidden Markov model (HMM). There is relatively less work that exploited lexical features only. Beeferman et al. (1998) focused on comma prediction with a trigram language model. A joint language model was learned from punctuated texts, and commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted p"
D10-1018,2006.iwslt-papers.1,0,0.0189916,"olkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts consider the boundary detection and punctuation insertion task as a hidden event detection task. One such well-known approach was introduced by Stolcke et al. (1998). They adopted a HMM to describe a joint distribution over words and interword events, where the observations are the words, and the word/event pairs are encoded as hidden states. Specifically, in this task word boundaries and punctuation symbols are encoded as interword events. The training phase involves training an n-g"
D10-1018,P03-1021,0,0.0395812,"the official IWSLT05 evaluation set where the correct punctuation symbols are present. Evaluations are performed on the ASR outputs of the IWSLT08 BTEC evaluation dataset, with punctuation symbols inserted by each punctuation prediction method. The tuning set and evaluation set include 7 reference translations. Following a common practice in statistical machine translation, we report B LEU-4 scores (Papineni et al., 2002), which were shown to have good correlation with human judgments, with the closest reference length as the effective reference length. The minimum error rate training (MERT) (Och, 2003) procedure is used for tuning the model parameters of the translation system. Due to the unstable nature of MERT, we perform 10 runs for each translation task, with a different random initialization of parameters in each run, and report the B LEU4 scores averaged over 10 runs. The results are reported in Table 7. The best translation performances for both translation directions are achieved by applying F-C RF as the punctuation prediction model to the ASR texts. Such improvements are observed to be consistent over different runs. The improvement of F-C RF over LC RF in translation quality is s"
D10-1018,P02-1040,0,0.103814,"rmance than simple distance-based reordering (Koehn et al., 2005). Specifically, the default lexicalized reordering model (msd-bidirectional-fe) is used. For tuning the parameters of Moses, we use the official IWSLT05 evaluation set where the correct punctuation symbols are present. Evaluations are performed on the ASR outputs of the IWSLT08 BTEC evaluation dataset, with punctuation symbols inserted by each punctuation prediction method. The tuning set and evaluation set include 7 reference translations. Following a common practice in statistical machine translation, we report B LEU-4 scores (Papineni et al., 2002), which were shown to have good correlation with human judgments, with the closest reference length as the effective reference length. The minimum error rate training (MERT) (Och, 2003) procedure is used for tuning the model parameters of the translation system. Due to the unstable nature of MERT, we perform 10 runs for each translation task, with a different random initialization of parameters in each run, and report the B LEU4 scores averaged over 10 runs. The results are reported in Table 7. The best translation performances for both translation directions are achieved by applying F-C RF as"
D10-1018,N03-1028,0,0.0295433,"Missing"
D10-1018,I05-3027,0,0.0104152,"hniques based on conditional random fields to tackle the difficulties due to long range dependencies. 4 Linear-Chain Conditional Random Fields One natural approach to relax the strong dependency assumptions encoded by the hidden event language model is to adopt an undirected graphical model, where arbitrary overlapping features can be exploited. Conditional random fields (CRF) (Lafferty et al., 2001) have been widely used in various sequence labeling and segmentation tasks (Sha and Pereira, 1 http://mastarpj.nict.go.jp/IWSLT2008/downloads/ case+punc tool using SRILM.instructions.txt 179 2003; Tseng et al., 2005). Unlike a HMM which models the joint distribution of both the label sequence and the observation, a CRF is a discriminative model of the conditional distribution of the complete label sequence given the observation. Specifically, a first-order linear-chain CRF which assumes first-order Markov property is defined by the following equation: ! XX 1 pλ (y|x) = exp λk fk (x, yt−1 , yt , t) Z(x) t k (1) where x is the observation and y is the label sequence. Feature functions fk with time step t are defined over the entire observation x and two adjacent hidden labels. Z(x) is a normalization factor"
D10-1018,2008.iwslt-evaluation.18,0,0.0118497,"d commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as 178 part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts con"
D10-1018,2008.iwslt-evaluation.1,0,\N,Missing
D10-1018,2009.iwslt-evaluation.1,0,\N,Missing
D10-1090,P05-1074,0,0.88956,"aphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the ap"
D10-1090,N03-1003,0,0.0509299,"the first fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on wha"
D10-1090,N06-1003,0,0.0315297,"hen align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; 925 P r(p1 , p2 , . . . , pn |S) = n 1 Y P r(pi ) Z(S) i=1 (2) where Z(S) is a normalizing constant. The best segmentation of S according to Equation 2 can be calculated efficiently using a dynamic programming algorithm. Note that Z(S) does not need to be calculated, as it is the same for all different segmentations of S. The formula has a strong preference for longer phrases, since every P r(pi ) has a large denominator. Many sentences are impossible to segment into known phrases, including all those containing outof-vocabulary words. We therefore allow any single word w to be considered as"
D10-1090,C08-1013,0,0.136968,"lude in Section 8. method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation. Inspired by the success of automatic MT evaluation, Lin (2004) and Hovy et al. (2006) propose automatic metrics for summary evaluation. The former is entirely lexical based, whereas the latter also exploits constituent and dependency parses, and semantic features derived from WordNet. The only prior attempt to devise an automatic evaluation metric for paraphrases that we are aware of is ParaMetric (Callison-Burch et al., 2008), which compares the collection of paraphrases discovered by automatic paraphrasing algorithms against a manual gold standard collected over the same sentences. The recall and precision of several current paraphrase generation systems are evaluated. ParaMetric does not attempt to propose a single metric to correlate well with human judgments. Rather, it consists of a few indirect and partial measures of the quality of PG systems. 2 The first step in defining a paraphrase evaluation metric is to define a good paraphrase. MerriamWebster dictionary gives the following definition: a restatement of"
D10-1090,P08-1007,1,0.864453,"well to MT techniques that directly optimize the evaluation metric. The weakness of BLEU is that it operates purely at the lexical surface level. Later works attempt to take more syntactic and semantic features into consideration (see (Callison-Burch et al., 2009) for an overview). The whole spectrum of NLP resources has found application in machine translation evaluation, including POS tags, constituent and dependency parses, WordNet (Fellbaum, 1998), semantic roles, textual entailment features, and more. Many of these metrics have been shown to correlate better with human judges than BLEU (Chan and Ng, 2008; Liu et al., 2010). Interestingly, few MT evaluation metrics exploit parallel texts as a source of information, when statistical MT is centered almost entirely around mining parallel texts. Compared to these MT evaluation metrics, our 924 3 Task definition equivalence. The task of paraphrase evaluation is then defined as follows: Given an original sentence R and a paraphrase candidate P , output a numeric score b estimating the quality of P as a paraphrase of R by considering adequacy, fluency, and lexical dissimilarity. In this study, we use a scale of 1 to 5 (inclusive) for b, although that"
D10-1090,P09-1053,0,0.0230441,"importantly, BPNG does not depend on lexical or syntactic similarity, allowing us to address the conflicting requirements of paraphrase evaluation. The only linguistic re923 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923–932, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics source required to evaluate BPNG is a parallel text of the target language and an arbitrary other language, known as the pivot language. We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010; Das and Smith, 2009; Wan et al., 2006; Qiu et al., 2006) are related yet distinct tasks. Consider two sentences S1 and S2 that are the same except for the substitution of a single synonym. A paraphrase recognition system should assign them a very high score, but a paraphrase evaluation system would assign a relatively low one. Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier. The rest of the paper is organized as follows. We survey other automatic evaluation metrics in natural language processing (NLP) in Section 2. We define the"
D10-1090,N06-2009,0,0.0363488,"an judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system. Second, and as a consequence, no automatic evaluation metric exists for paraphrases. Most works in this area resort to ad hoc manual evaluations, such as the percentage of “yes” judgments to the question of “is the meaning preserved”. This type of evaluation is incomprehensive, e"
D10-1090,P09-1104,0,0.0345616,"N (·) is the count of a phrase in the phrase table, and the denominator is a constant for all p. We define the likelihood of segmenting a sentence S into a sequence of phrases (p1 , p2 , . . . , pn ) by: Phrase-level semantic representation Without loss of generality, suppose we are to evaluate English paraphrases, and have been supplied many sentence-aligned parallel texts of French and English as an additional resource. We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; 925 P r(p1 , p2 , . . . , pn |S) = n 1 Y P r(pi ) Z(S) i=1 (2) where Z(S) is a normalizing constant. The best segmentation of S according to Equation 2 can be"
D10-1090,N10-1145,0,0.0207682,"language sentences. Most importantly, BPNG does not depend on lexical or syntactic similarity, allowing us to address the conflicting requirements of paraphrase evaluation. The only linguistic re923 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923–932, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics source required to evaluate BPNG is a parallel text of the target language and an arbitrary other language, known as the pivot language. We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010; Das and Smith, 2009; Wan et al., 2006; Qiu et al., 2006) are related yet distinct tasks. Consider two sentences S1 and S2 that are the same except for the substitution of a single synonym. A paraphrase recognition system should assign them a very high score, but a paraphrase evaluation system would assign a relatively low one. Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier. The rest of the paper is organized as follows. We survey other automatic evaluation metrics in natural language processing (NLP) in Sec"
D10-1090,hovy-etal-2006-automated,0,0.0110151,"ge processing (NLP) in Section 2. We define the task of paraphrase evaluation in Section 3 and develop our metric in Section 4. We conduct a human evaluation and analyze the results in Section 5. The correlation of PEM with human judgments is studied in Section 6. Finally, we discuss our findings and future work in Section 7 and conclude in Section 8. method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation. Inspired by the success of automatic MT evaluation, Lin (2004) and Hovy et al. (2006) propose automatic metrics for summary evaluation. The former is entirely lexical based, whereas the latter also exploits constituent and dependency parses, and semantic features derived from WordNet. The only prior attempt to devise an automatic evaluation metric for paraphrases that we are aware of is ParaMetric (Callison-Burch et al., 2008), which compares the collection of paraphrases discovered by automatic paraphrasing algorithms against a manual gold standard collected over the same sentences. The recall and precision of several current paraphrase generation systems are evaluated. ParaM"
D10-1090,N06-1058,0,0.0148825,"f paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases ten"
D10-1090,N03-1017,0,0.116562,"lenge is to measure the adequacy, or semantic similarity, completely independent of any lexical similarity. We address this problem in Sections 4.1 to 4.3. The remaining two criteria are addressed in Section 4.4, and we describe the final combined metric PEM in Section 4.5. 4.1 Snover et al., 2009). We can treat the distribution of aligned French phrases as a semantic representation of the English phrase. The semantic distance between two English phrases can then be measured by their degree of overlap in this representation. In this work, we use the widely-used phrase extraction heuristic in (Koehn et al., 2003) to extract phrase pairs from parallel texts into a phrase table1 . The phrases extracted do not necessarily correspond to the speakers’ intuition. Rather, they are units whose boundaries are preserved during translation. However, the distinction does not affect our work. 4.2 Segmenting a sentence into phrases Having established a way to measure the similarity of two English phrases, we now extend the concept to sentences. Here we discuss how to segment an English sentence (the original or the paraphrase) into phrases. From the phrase table, we know the frequencies of all the phrases and we ap"
D10-1090,2005.mtsummit-papers.11,0,0.0149324,"it makes no provisions for syntactic paraphrasing at the sentence level, which is probably a much greater challenge, and the literature offers few successes to draw inspirations from. We hope to be able to partially address this deficiency in future work. The only external linguistic resource required by PEM is a parallel text of the target language and another arbitrary language. While we only use Chinese-English parallel text in this study, other language pairs need to be explored too. Another alternative is to collect parallel texts against multiple foreign languages, e.g., using Europarl (Koehn, 2005). We leave this for future work. Our evaluation method does not require humangenerated references like in MT evaluation. Therefore, we can easily formulate a paraphrase generator by directly optimizing the PEM metric, although solving it is not trivial: paraphrase(R) = arg max PEM(P, R) P where R is the original sentence and P is the paraphrase. Finally, the PEM metric, in particular the semantic representation BPNG, can be useful in many other contexts, such as MT evaluation, summary evaluation, and paraphrase recognition. To facilitate future research, we will package and release PEM under a"
D10-1090,N06-1014,0,0.262452,"(p ) P r(p) = P (1) N (·) is the count of a phrase in the phrase table, and the denominator is a constant for all p. We define the likelihood of segmenting a sentence S into a sequence of phrases (p1 , p2 , . . . , pn ) by: Phrase-level semantic representation Without loss of generality, suppose we are to evaluate English paraphrases, and have been supplied many sentence-aligned parallel texts of French and English as an additional resource. We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; 925 P r(p1 , p2 , . . . , pn |S) = n 1 Y P r(pi ) Z(S) i=1 (2) where Z(S) is a normalizing constant. The best segmentation of S accord"
D10-1090,W04-1013,0,0.025417,"natural language processing (NLP) in Section 2. We define the task of paraphrase evaluation in Section 3 and develop our metric in Section 4. We conduct a human evaluation and analyze the results in Section 5. The correlation of PEM with human judgments is studied in Section 6. Finally, we discuss our findings and future work in Section 7 and conclude in Section 8. method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation. Inspired by the success of automatic MT evaluation, Lin (2004) and Hovy et al. (2006) propose automatic metrics for summary evaluation. The former is entirely lexical based, whereas the latter also exploits constituent and dependency parses, and semantic features derived from WordNet. The only prior attempt to devise an automatic evaluation metric for paraphrases that we are aware of is ParaMetric (Callison-Burch et al., 2008), which compares the collection of paraphrases discovered by automatic paraphrasing algorithms against a manual gold standard collected over the same sentences. The recall and precision of several current paraphrase generation syste"
D10-1090,W10-1754,1,0.79968,"ues that directly optimize the evaluation metric. The weakness of BLEU is that it operates purely at the lexical surface level. Later works attempt to take more syntactic and semantic features into consideration (see (Callison-Burch et al., 2009) for an overview). The whole spectrum of NLP resources has found application in machine translation evaluation, including POS tags, constituent and dependency parses, WordNet (Fellbaum, 1998), semantic roles, textual entailment features, and more. Many of these metrics have been shown to correlate better with human judges than BLEU (Chan and Ng, 2008; Liu et al., 2010). Interestingly, few MT evaluation metrics exploit parallel texts as a source of information, when statistical MT is centered almost entirely around mining parallel texts. Compared to these MT evaluation metrics, our 924 3 Task definition equivalence. The task of paraphrase evaluation is then defined as follows: Given an original sentence R and a paraphrase candidate P , output a numeric score b estimating the quality of P as a paraphrase of R by considering adequacy, fluency, and lexical dissimilarity. In this study, we use a scale of 1 to 5 (inclusive) for b, although that can be transformed"
D10-1090,I05-3025,1,0.749264,"e quality. 3 We automatically add jitter (small amounts of noise) for ease of presentation. 929 6.1 Experimental setup We build the phrase table used to evaluate the pivot language F1 from the FBIS Chinese-English corpus, consisting of about 250,000 Chinese sentences, each with a single English translation. The paraphrases are taken from the MTC corpus in the same way as the human experiment described in Section 5.1. Both FBIS and MTC are in the Chinese newswire domain. We stem all English words in both data sets with the Porter stemmer (Porter, 1980). We use the maximum entropy segmenter of (Low et al., 2005) to segment the Chinese part of the FBIS corpus. Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009) with five iterations of training. Phrases are then extracted with the widelyused heuristic in Koehn et al. (2003). We extract phrases of up to four words in length. Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3. For computational efficiency, we consider only edges of the confusion network with probabilities higher than 0.1, and only N-grams with probab"
D10-1090,W07-0716,0,0.0387343,"ntic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system. Second, and as a consequence, no automatic evaluation metric exists for p"
D10-1090,2008.amta-papers.13,0,0.199803,"e based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system. Second, and as a consequence, no automatic evaluation metric exists for paraphrases. Most works"
D10-1090,J03-1002,0,0.00608359,"e probability of a phrase p by: N (p) 0 p0 N (p ) P r(p) = P (1) N (·) is the count of a phrase in the phrase table, and the denominator is a constant for all p. We define the likelihood of segmenting a sentence S into a sequence of phrases (p1 , p2 , . . . , pn ) by: Phrase-level semantic representation Without loss of generality, suppose we are to evaluate English paraphrases, and have been supplied many sentence-aligned parallel texts of French and English as an additional resource. We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; 925 P r(p1 , p2 , . . . , pn |S) = n 1 Y P r(pi ) Z(S) i=1 (2) where Z(S) is a normalizing"
D10-1090,W06-3112,0,0.0378631,"Missing"
D10-1090,N03-1024,0,0.0367579,"c metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes charac"
D10-1090,P02-1040,0,0.111408,"Although lexical dissimilarity is seemingly the easiest to judge automatically among the three, it poses an interesting challenge to automatic evaluation metrics, as overlap with the reference has been the basis of almost all evaluation metrics. That is, while MT evaluation and paraphrase evaluation are conceptually closely related, the latter actually highlights the deficiencies of the former, namely that in most automatic evaluations, semantic equivalence is underrepresented and substituted by lexical and syntactic Related work The most well-known automatic evaluation metric in NLP is BLEU (Papineni et al., 2002) for MT, based on N-gram matching precisions. The simplicity of BLEU lends well to MT techniques that directly optimize the evaluation metric. The weakness of BLEU is that it operates purely at the lexical surface level. Later works attempt to take more syntactic and semantic features into consideration (see (Callison-Burch et al., 2009) for an overview). The whole spectrum of NLP resources has found application in machine translation evaluation, including POS tags, constituent and dependency parses, WordNet (Fellbaum, 1998), semantic roles, textual entailment features, and more. Many of these"
D10-1090,W06-1603,0,0.0121901,"xical or syntactic similarity, allowing us to address the conflicting requirements of paraphrase evaluation. The only linguistic re923 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923–932, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics source required to evaluate BPNG is a parallel text of the target language and an arbitrary other language, known as the pivot language. We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010; Das and Smith, 2009; Wan et al., 2006; Qiu et al., 2006) are related yet distinct tasks. Consider two sentences S1 and S2 that are the same except for the substitution of a single synonym. A paraphrase recognition system should assign them a very high score, but a paraphrase evaluation system would assign a relatively low one. Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier. The rest of the paper is organized as follows. We survey other automatic evaluation metrics in natural language processing (NLP) in Section 2. We define the task of paraphrase evaluation in Sec"
D10-1090,W04-3219,0,0.110671,"e the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraph"
D10-1090,W09-0441,0,0.0176424,"coherent. However, when accompanied by high adequacy and fluency scores, it differentiates the mediocre paraphrases from the good ones. 4 Paraphrase Evaluation Metric (PEM) In this section we devise our metric according to the three proposed evaluation criteria, namely adequacy, fluency, and dissimilarity. The main challenge is to measure the adequacy, or semantic similarity, completely independent of any lexical similarity. We address this problem in Sections 4.1 to 4.3. The remaining two criteria are addressed in Section 4.4, and we describe the final combined metric PEM in Section 4.5. 4.1 Snover et al., 2009). We can treat the distribution of aligned French phrases as a semantic representation of the English phrase. The semantic distance between two English phrases can then be measured by their degree of overlap in this representation. In this work, we use the widely-used phrase extraction heuristic in (Koehn et al., 2003) to extract phrase pairs from parallel texts into a phrase table1 . The phrases extracted do not necessarily correspond to the speakers’ intuition. Rather, they are units whose boundaries are preserved during translation. However, the distinction does not affect our work. 4.2 Seg"
D10-1090,U06-1019,0,0.00620319,"s not depend on lexical or syntactic similarity, allowing us to address the conflicting requirements of paraphrase evaluation. The only linguistic re923 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923–932, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics source required to evaluate BPNG is a parallel text of the target language and an arbitrary other language, known as the pivot language. We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010; Das and Smith, 2009; Wan et al., 2006; Qiu et al., 2006) are related yet distinct tasks. Consider two sentences S1 and S2 that are the same except for the substitution of a single synonym. A paraphrase recognition system should assign them a very high score, but a paraphrase evaluation system would assign a relatively low one. Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier. The rest of the paper is organized as follows. We survey other automatic evaluation metrics in natural language processing (NLP) in Section 2. We define the task of paraphras"
D10-1090,P03-1016,0,0.0894542,"rench and English as an additional resource. We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text. If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning. Similar observations have been made by previous researchers (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; 925 P r(p1 , p2 , . . . , pn |S) = n 1 Y P r(pi ) Z(S) i=1 (2) where Z(S) is a normalizing constant. The best segmentation of S according to Equation 2 can be calculated efficiently using a dynamic programming algorithm. Note that Z(S) does not need to be calculated, as it is the same for all different segmentations of S. The formula has a strong preference for longer phrases, since every P r(pi ) has a large denominator. Many sentences are impossible to segment into known phrases, including all those containing outof-vocabulary"
D10-1090,P08-1116,0,0.0499692,"ems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their ow"
D10-1090,P09-1094,0,0.14831,"based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view"
D10-1090,W06-1610,0,0.014296,"equacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system. Second, and as a consequence, no automatic evaluation metric exists for paraphrases. Most works in this area resort to ad hoc manual evaluations, such as the"
D10-1090,N06-1057,0,0.0384495,"equacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments. 1 Introduction In recent years, there has been an increasing interest in the task of paraphrase generation (PG) (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Kauchak and Barzilay, 2006; Zhao et al., 2008; Zhao et al., 2009). At the same time, the task has seen applications such as machine translation (MT) (CallisonBurch et al., 2006; Madnani et al., 2007; Madnani et al., 2008), MT evaluation (Kauchak and Barzilay, 2006; Zhou et al., 2006a; Owczarzak et al., 2006), summary evaluation (Zhou et al., 2006b), and question answering (Duboue and Chu-Carroll, 2006). Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system. Second, and as a consequence, no automatic evaluation metric exists for paraphrases. Most works in this area resort to ad hoc manual evaluations, such as the"
D10-1090,W09-0401,0,\N,Missing
D10-1090,C04-1046,0,\N,Missing
D11-1010,P05-1074,0,0.151159,"matically extract from an L1-English parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight di"
D11-1010,P06-1032,0,0.104104,"for spelling, homophones, and synonyms, respectively. (2) Typical features include a phrase translation probability p(e|f ), an inverse phrase translation probability p(f |e), a language model score p(e), and a constant phrase penalty. The optimization of the feature weights λi , i = 1, . . . , n can be done using minimum error rate training (MERT) (Och, 2003) on a development set of input sentences and their reference translations. Because of the great flexibility of the log-linear model, researchers have used the framework for other tasks outside SMT, including grammatical error correction (Brockett et al., 2006). We adopt a similar approach in this work. We modify the phrase table of the popular phrase-based SMT decoder MOSES (Koehn et al., 2007) to include collocation corrections with features derived from spelling, homophones, synonyms, and L1-induced paraphrases. • All We combine the phrase tables from spelling, homophones, synonyms, and L1paraphrases. The combined phrase table contains five features: three binary features for spelling, homophones, and synonyms, and two real-valued features for the L1-paraphrase probability and inverse L1-paraphrase probability. Additionally, each phrase table con"
D11-1010,P11-1092,1,0.12394,"Missing"
D11-1010,W06-1607,0,0.0103423,"the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as p(e1 |e2 ) = X f p(e1 |f )p(f |e2 ) (1) where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 |f ) and p(f |e2 ) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al., 2006). Finally, we only keep para• Synonyms: For each English word, the phrase table contains entries consisting of the word itself and each of its synonyms in WordNet. If a word has more than one sense, we consider all its senses. Each entry has a constant feature of 1.0. phrases with a probability above a certain threshold (set to 0.001 in our work). 4.2 Collocation Correction with Phrase-based SMT We implement our approach in the framework of phrase-based statistical machine translation (SMT) (Koehn et al., 2003). Phrase-based SMT tries to find the highest scoring translation e given an input se"
D11-1010,N10-1019,0,0.216805,"Missing"
D11-1010,P09-1104,0,0.0132978,"ntence-aligned L1-English parallel corpus. As most of the essays in our corpus are written by native Chinese speakers, we use the FBIS ChineseEnglish corpus, which consists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as p(e1 |e2 ) = X f p(e1 |f )p(f |e2 ) (1) where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 |f ) and p(f |e2 ) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al., 2006). Finally, we only keep para• Synonyms: For each English word, the phrase table"
D11-1010,N03-1017,0,0.00189843,"sists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as p(e1 |e2 ) = X f p(e1 |f )p(f |e2 ) (1) where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 |f ) and p(f |e2 ) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al., 2006). Finally, we only keep para• Synonyms: For each English word, the phrase table contains entries consisting of the word itself and each of its synonyms in WordNet. If a word has more than one sense, we consider all its senses. Each entry has a"
D11-1010,P07-2045,0,0.00822619,"se translation probability p(f |e), a language model score p(e), and a constant phrase penalty. The optimization of the feature weights λi , i = 1, . . . , n can be done using minimum error rate training (MERT) (Och, 2003) on a development set of input sentences and their reference translations. Because of the great flexibility of the log-linear model, researchers have used the framework for other tasks outside SMT, including grammatical error correction (Brockett et al., 2006). We adopt a similar approach in this work. We modify the phrase table of the popular phrase-based SMT decoder MOSES (Koehn et al., 2007) to include collocation corrections with features derived from spelling, homophones, synonyms, and L1-induced paraphrases. • All We combine the phrase tables from spelling, homophones, synonyms, and L1paraphrases. The combined phrase table contains five features: three binary features for spelling, homophones, and synonyms, and two real-valued features for the L1-paraphrase probability and inverse L1-paraphrase probability. Additionally, each phrase table contains the standard constant phrase penalty feature. The first four tables only contain collocation candidates for individual words. We le"
D11-1010,N06-1014,0,0.00697498,"candidates from a sentence-aligned L1-English parallel corpus. As most of the essays in our corpus are written by native Chinese speakers, we use the FBIS ChineseEnglish corpus, which consists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as p(e1 |e2 ) = X f p(e1 |f )p(f |e2 ) (1) where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 |f ) and p(f |e2 ) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al., 2006). Finally, we only keep para• Synonyms: For each Englis"
D11-1010,W09-2107,0,0.119078,"related work on collocation error correction. We also highlight differences between collocation error correction and related NLP tasks like context-sensitive spelling error correction, synonym extraction, lexical substitution, and paraphrasing. Most work in collocation error correction has relied on dictionaries or manually created databases 108 to generate collocation candidates (Shei and Pain, 2000; Wible et al., 2003; Futagi et al., 2008). Other work has focused on finding candidates that collocate with similar words, e.g., verbs that appear with the same noun objects form a confusion set (Liu et al., 2009; Wu et al., 2010). The work most similar to ours is probably the one presented by Chang et al. (2008), as they also use translation information to generate collocation candidates. However, they do not use automatically derived paraphrases from parallel corpora but bilingual dictionaries. Dictionaries usually have lower coverage, do not contain longer phrases or inflected forms, and do not provide any translation probability estimates. Also, their work focuses solely on verb-noun collocations, while we target collocations of arbitrary syntactic type. Context-sensitive spelling error correction"
D11-1010,D10-1090,1,0.735719,"sh parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight differences between"
D11-1010,W10-1754,1,0.829929,"sh parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight differences between"
D11-1010,I05-3025,1,0.760697,"ique of paraphrasing with parallel corpora (Bannard and Callison-Burch, 2005) to automatically find collocation candidates from a sentence-aligned L1-English parallel corpus. As most of the essays in our corpus are written by native Chinese speakers, we use the FBIS ChineseEnglish corpus, which consists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as p(e1 |e2 ) = X f p(e1 |f )p(f |e2 ) (1) where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 |f ) and p(f |e2 ) are estimated by maximum likelihood estimation and"
D11-1010,J10-3003,0,0.00619701,"or frequent spelling errors. Even when the confusion sets are formed automatically, the similarity of words in a confusion set has been based on edit distance or phonetic similarity (Carlson et al., 2001). In contrast, we focus on words that are confusable due to their similar semantics instead of similar spelling or pronunciation. Also, we do not assume that the set of confusion sets is already given to us. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. However, there is a subtle but important difference between these tasks and collocation correction. In the former, the main criterion is whether the original phrase and the synonym/paraphrase candidate are substitutable, i.e., both form a grammatical sentence when substituted for each other in a particular context. In contrast, in collocation correction, we are primarily interested in finding candidates which are not substitutable in their English context but appear to be substit"
D11-1010,S07-1009,0,0.0158014,"f pre-defined confusion sets, like homophones or frequent spelling errors. Even when the confusion sets are formed automatically, the similarity of words in a confusion set has been based on edit distance or phonetic similarity (Carlson et al., 2001). In contrast, we focus on words that are confusable due to their similar semantics instead of similar spelling or pronunciation. Also, we do not assume that the set of confusion sets is already given to us. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. However, there is a subtle but important difference between these tasks and collocation correction. In the former, the main criterion is whether the original phrase and the synonym/paraphrase candidate are substitutable, i.e., both form a grammatical sentence when substituted for each other in a particular context. In contrast, in collocation correction, we are primarily interested in finding candidates which are not substitutable in thei"
D11-1010,S07-1010,1,0.826615,"pproach generates L1-induced paraphrases which we automatically extract from an L1-English parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview"
D11-1010,W04-3236,1,0.670891,"the popular technique of paraphrasing with parallel corpora (Bannard and Callison-Burch, 2005) to automatically find collocation candidates from a sentence-aligned L1-English parallel corpus. As most of the essays in our corpus are written by native Chinese speakers, we use the FBIS ChineseEnglish corpus, which consists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as p(e1 |e2 ) = X f p(e1 |f )p(f |e2 ) (1) where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 |f ) and p(f |e2 ) are estimated by maximum likelih"
D11-1010,P03-1058,1,0.411955,"rrection. The key component in our approach generates L1-induced paraphrases which we automatically extract from an L1-English parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work"
D11-1010,P03-1021,0,0.00236129,"valued features: a paraphrase probability according to Equation 1 and an inverse paraphrase probability. • Baseline We combine the phrase tables built for spelling, homophones, and synonyms. The combined phrase table contains three binary features for spelling, homophones, and synonyms, respectively. (2) Typical features include a phrase translation probability p(e|f ), an inverse phrase translation probability p(f |e), a language model score p(e), and a constant phrase penalty. The optimization of the feature weights λi , i = 1, . . . , n can be done using minimum error rate training (MERT) (Och, 2003) on a development set of input sentences and their reference translations. Because of the great flexibility of the log-linear model, researchers have used the framework for other tasks outside SMT, including grammatical error correction (Brockett et al., 2006). We adopt a similar approach in this work. We modify the phrase table of the popular phrase-based SMT decoder MOSES (Koehn et al., 2007) to include collocation corrections with features derived from spelling, homophones, synonyms, and L1-induced paraphrases. • All We combine the phrase tables from spelling, homophones, synonyms, and L1pa"
D11-1010,P02-1040,0,0.0978155,"ntity. We remove phrase table entries where the phrase and the candidate correction are identical, thus practically forcing the system to change the identified phrase. We set the distortion limit of the decoder to zero to achieve monotone decoding. We previously observed that word order errors are virtually absent in our collocation errors. For the language model, we use a 5-gram language model trained on the English Gigaword corpus with modified Kneser-Ney smoothing. All experiments use the same language model to allow a fair comparison. We perform MERT training with the popular BLEU metric (Papineni et al., 2002) on the development set of erroneous sentences and their corrections. As the search space is restricted to changing a single phrase per sentence, training converges relatively quickly after two or three iterations. After convergence, the model can be used to automatically correct new collocation errors. 6 Results We evaluate the performance of the proposed method on our test set of 856 sentences, each with one collocation error. We conduct both an automatic and a human evaluation. In the automatic evaluation, the system’s performance is measured by computing the rank of the gold answer provide"
D11-1010,D10-1094,0,0.189625,"Missing"
D11-1010,W09-0441,0,0.00818435,"aditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight differences between collocation error correction and related NLP tasks like con"
D11-1010,P10-2065,0,0.0699478,"Missing"
D11-1010,P03-1016,0,0.0080818,"traditionally focused on a small number of pre-defined confusion sets, like homophones or frequent spelling errors. Even when the confusion sets are formed automatically, the similarity of words in a confusion set has been based on edit distance or phonetic similarity (Carlson et al., 2001). In contrast, we focus on words that are confusable due to their similar semantics instead of similar spelling or pronunciation. Also, we do not assume that the set of confusion sets is already given to us. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. However, there is a subtle but important difference between these tasks and collocation correction. In the former, the main criterion is whether the original phrase and the synonym/paraphrase candidate are substitutable, i.e., both form a grammatical sentence when substituted for each other in a particular context. In contrast, in collocation correction, we are primarily interested in fin"
D11-1010,P10-2021,0,0.258702,"llocation error correction. We also highlight differences between collocation error correction and related NLP tasks like context-sensitive spelling error correction, synonym extraction, lexical substitution, and paraphrasing. Most work in collocation error correction has relied on dictionaries or manually created databases 108 to generate collocation candidates (Shei and Pain, 2000; Wible et al., 2003; Futagi et al., 2008). Other work has focused on finding candidates that collocate with similar words, e.g., verbs that appear with the same noun objects form a confusion set (Liu et al., 2009; Wu et al., 2010). The work most similar to ours is probably the one presented by Chang et al. (2008), as they also use translation information to generate collocation candidates. However, they do not use automatically derived paraphrases from parallel corpora but bilingual dictionaries. Dictionaries usually have lower coverage, do not contain longer phrases or inflected forms, and do not provide any translation probability estimates. Also, their work focuses solely on verb-noun collocations, while we target collocations of arbitrary syntactic type. Context-sensitive spelling error correction is the task of co"
D11-1035,W05-0909,0,0.0608731,"coder, which performs the actual translation. The The first automatic MT evaluation metric to show a high correlation with human judgment is BLEU (Papineni et al., 2002). Together with its close variant the NIST metric, they have quickly become the standard way of tuning statistical machine translation systems. While BLEU is an impressively simple and effective metric, recent evaluations have shown that many new generation metrics can outperform BLEU in terms of correlation with human judgment (Callison-Burch et al., 2009; CallisonBurch et al., 2010). Some of these new metrics include METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007), TER (Snover et al., 2006), MAXSIM (Chan and Ng, 2008; Chan and Ng, 2009), and TESLA (Liu et al., 2010). Given the close relationship between automatic MT and automatic MT evaluation, the logical expectation is that a better MT evaluation metric would 375 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 375–384, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics lead to better MT systems. However, this linkage has not yet been realized. In the SMT community, MT tuning still uses BLE"
D11-1035,W09-0401,0,0.0583539,"today is statistical machine translation (SMT) (Hutchins, 2007). At the core of the system is the decoder, which performs the actual translation. The The first automatic MT evaluation metric to show a high correlation with human judgment is BLEU (Papineni et al., 2002). Together with its close variant the NIST metric, they have quickly become the standard way of tuning statistical machine translation systems. While BLEU is an impressively simple and effective metric, recent evaluations have shown that many new generation metrics can outperform BLEU in terms of correlation with human judgment (Callison-Burch et al., 2009; CallisonBurch et al., 2010). Some of these new metrics include METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007), TER (Snover et al., 2006), MAXSIM (Chan and Ng, 2008; Chan and Ng, 2009), and TESLA (Liu et al., 2010). Given the close relationship between automatic MT and automatic MT evaluation, the logical expectation is that a better MT evaluation metric would 375 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 375–384, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics lead to better MT systems"
D11-1035,W10-1703,0,0.0883913,"ence and the translation candidate respectively, both in English. As an example, Table 2: Selected system-level Spearman’s rho correlation with the human judgment for the out-ofEnglish task, as reported in WMT 2010. tags. While such tools are usually available even for languages other than English, it does make TESLAM more troublesome to port to non-English languages. TESLA-M did well in the WMT 2010 evaluation campaign. According to the system-level correlation with human judgments (Tables 1 and 2), it ranks top for the out-of-English task and very close to the top for the into-English task (Callison-Burch et al., 2010). 2.4 Querrien / 1.0 TESLA-F3 TESLA-F builds on top of TESLA-M. While wordlevel synonyms are handled in TESLA-M by examining WordNet synsets, no modeling of phrase-level synonyms is possible. TESLA-F attempts to remedy this shortcoming by exploiting a phrase table between the target language and another language, known as the pivot language. Assume the target language is English and the pivot language is French, i.e., we are provided with an English-French phrase table. Let R and T be the 3 TESLA-F refers to the metric called TESLA in (Liu et al., 2010). To minimize confusion, in this work we"
D11-1035,N10-1080,0,0.476833,"MT community, MT tuning still uses BLEU almost exclusively. Some researchers have investigated the use of better metrics for MT tuning, with mixed results. Most notably, Pad´o et al. (2009) reported improved human judgment using their entailment-based metric. However, the metric is heavy weight and slow in practice, with an estimated runtime of 40 days on the NIST MT 2002/2006/2008 dataset, and the authors had to resort to a two-phase MERT process with a reduced n-best list. As we shall see, our experiments use the similarly sized WMT 2010 dataset, and most of our runs take less than one day. Cer et al. (2010) compared tuning a phrase-based SMT system with BLEU, NIST, METEOR, and TER, and concluded that BLEU and NIST are still the best choices for MT tuning, despite the proven higher correlation of METEOR and TER with human judgment. In this work, we investigate the effect of MERT using BLEU, TER, and two variants of TESLA, TESLA-M and TESLA-F, on Joshua (Li et al., 2009), a state-of-the-art hierarchical phrase-based SMT system (Chiang, 2005; Chiang, 2007). Our empirical study is carried out in the context of WMT 2010, for the French-English, Spanish-English, and German-English machine translation"
D11-1035,P08-1007,1,0.155987,"ric to show a high correlation with human judgment is BLEU (Papineni et al., 2002). Together with its close variant the NIST metric, they have quickly become the standard way of tuning statistical machine translation systems. While BLEU is an impressively simple and effective metric, recent evaluations have shown that many new generation metrics can outperform BLEU in terms of correlation with human judgment (Callison-Burch et al., 2009; CallisonBurch et al., 2010). Some of these new metrics include METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007), TER (Snover et al., 2006), MAXSIM (Chan and Ng, 2008; Chan and Ng, 2009), and TESLA (Liu et al., 2010). Given the close relationship between automatic MT and automatic MT evaluation, the logical expectation is that a better MT evaluation metric would 375 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 375–384, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics lead to better MT systems. However, this linkage has not yet been realized. In the SMT community, MT tuning still uses BLEU almost exclusively. Some researchers have investigated the use of better metri"
D11-1035,P05-1033,0,0.0314284,"rocess with a reduced n-best list. As we shall see, our experiments use the similarly sized WMT 2010 dataset, and most of our runs take less than one day. Cer et al. (2010) compared tuning a phrase-based SMT system with BLEU, NIST, METEOR, and TER, and concluded that BLEU and NIST are still the best choices for MT tuning, despite the proven higher correlation of METEOR and TER with human judgment. In this work, we investigate the effect of MERT using BLEU, TER, and two variants of TESLA, TESLA-M and TESLA-F, on Joshua (Li et al., 2009), a state-of-the-art hierarchical phrase-based SMT system (Chiang, 2005; Chiang, 2007). Our empirical study is carried out in the context of WMT 2010, for the French-English, Spanish-English, and German-English machine translation tasks. We show that Joshua responds well to the change of evaluation metric, in that a system trained on metric M typically does well when judged by the same metric M. We further evaluate the different systems with manual judgments and show that the TESLA family of metrics (both TESLA-M and TESLA-F) significantly outperforms BLEU when used to guide the MERT search. The rest of this paper is organized as follows. In Section 2, we describ"
D11-1035,J07-2003,0,0.0189347,"reduced n-best list. As we shall see, our experiments use the similarly sized WMT 2010 dataset, and most of our runs take less than one day. Cer et al. (2010) compared tuning a phrase-based SMT system with BLEU, NIST, METEOR, and TER, and concluded that BLEU and NIST are still the best choices for MT tuning, despite the proven higher correlation of METEOR and TER with human judgment. In this work, we investigate the effect of MERT using BLEU, TER, and two variants of TESLA, TESLA-M and TESLA-F, on Joshua (Li et al., 2009), a state-of-the-art hierarchical phrase-based SMT system (Chiang, 2005; Chiang, 2007). Our empirical study is carried out in the context of WMT 2010, for the French-English, Spanish-English, and German-English machine translation tasks. We show that Joshua responds well to the change of evaluation metric, in that a system trained on metric M typically does well when judged by the same metric M. We further evaluate the different systems with manual judgments and show that the TESLA family of metrics (both TESLA-M and TESLA-F) significantly outperforms BLEU when used to guide the MERT search. The rest of this paper is organized as follows. In Section 2, we describe the four eval"
D11-1035,P09-1104,0,0.0195917,"vely. Similarly, the test data are 2489 four-way translated sentences. As a consequence, all MT evaluations involve only single references. We follow the standard approach for training hierarchical phrase-based SMT systems. First, we tokenize and lowercase the training texts and create 379 BLEU TER TESLA-M TESLA-F fr-en 3:49 (4) 4:03 (4) 13:00 (3) 35:07 (4) es-en 5:09 (6) 3:59 (4) 17:34 (5) 40:54 (4) de-en 2:41 (4) 3:59 (5) 13:40 (4) 40:28 (5) Table 3: Z-MERT training times in hours:minutes and number of iterations in parenthesis word alignments using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009) with five iterations of training. Then, we create suffix arrays and extract translation grammars for the development and test set with Joshua in its default setting. The maximum phrase length is 10. For the language model, we use SRILM (Stolcke, 2002) to build a trigram model with modified Kneser-Ney smoothing from the monolingual training data supplied in WMT 2010. Parameter tuning is carried out using ZMERT (Zaidan, 2009). TER and BLEU are already implemented in the publicly released version of Z-MERT, and Z-MERT’s modular design makes it easy to integrate TESLA-M and TESLA-F into the packa"
D11-1035,W07-0734,0,0.0361348,"actual translation. The The first automatic MT evaluation metric to show a high correlation with human judgment is BLEU (Papineni et al., 2002). Together with its close variant the NIST metric, they have quickly become the standard way of tuning statistical machine translation systems. While BLEU is an impressively simple and effective metric, recent evaluations have shown that many new generation metrics can outperform BLEU in terms of correlation with human judgment (Callison-Burch et al., 2009; CallisonBurch et al., 2010). Some of these new metrics include METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007), TER (Snover et al., 2006), MAXSIM (Chan and Ng, 2008; Chan and Ng, 2009), and TESLA (Liu et al., 2010). Given the close relationship between automatic MT and automatic MT evaluation, the logical expectation is that a better MT evaluation metric would 375 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 375–384, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics lead to better MT systems. However, this linkage has not yet been realized. In the SMT community, MT tuning still uses BLEU almost exclusively. Some"
D11-1035,W09-0424,0,0.0238464,"40:28 (5) Table 3: Z-MERT training times in hours:minutes and number of iterations in parenthesis word alignments using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009) with five iterations of training. Then, we create suffix arrays and extract translation grammars for the development and test set with Joshua in its default setting. The maximum phrase length is 10. For the language model, we use SRILM (Stolcke, 2002) to build a trigram model with modified Kneser-Ney smoothing from the monolingual training data supplied in WMT 2010. Parameter tuning is carried out using ZMERT (Zaidan, 2009). TER and BLEU are already implemented in the publicly released version of Z-MERT, and Z-MERT’s modular design makes it easy to integrate TESLA-M and TESLA-F into the package. The maximum number of MERT iterations is set to 100, although we observe that in practice, the algorithm converges after 3 to 6 iterations. The number of intermediate initial points per iteration is set to 20 and the n-best list is capped to 300 translations. Table 3 shows the training times and the number of MERT iterations for each of the language pairs and evaluation metrics. We use the publicly available version of T"
D11-1035,N06-1014,0,0.00564145,"and German respectively. Similarly, the test data are 2489 four-way translated sentences. As a consequence, all MT evaluations involve only single references. We follow the standard approach for training hierarchical phrase-based SMT systems. First, we tokenize and lowercase the training texts and create 379 BLEU TER TESLA-M TESLA-F fr-en 3:49 (4) 4:03 (4) 13:00 (3) 35:07 (4) es-en 5:09 (6) 3:59 (4) 17:34 (5) 40:54 (4) de-en 2:41 (4) 3:59 (5) 13:40 (4) 40:28 (5) Table 3: Z-MERT training times in hours:minutes and number of iterations in parenthesis word alignments using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009) with five iterations of training. Then, we create suffix arrays and extract translation grammars for the development and test set with Joshua in its default setting. The maximum phrase length is 10. For the language model, we use SRILM (Stolcke, 2002) to build a trigram model with modified Kneser-Ney smoothing from the monolingual training data supplied in WMT 2010. Parameter tuning is carried out using ZMERT (Zaidan, 2009). TER and BLEU are already implemented in the publicly released version of Z-MERT, and Z-MERT’s modular design makes it easy to integrate TESLA-M an"
D11-1035,W10-1754,1,0.84044,"is BLEU (Papineni et al., 2002). Together with its close variant the NIST metric, they have quickly become the standard way of tuning statistical machine translation systems. While BLEU is an impressively simple and effective metric, recent evaluations have shown that many new generation metrics can outperform BLEU in terms of correlation with human judgment (Callison-Burch et al., 2009; CallisonBurch et al., 2010). Some of these new metrics include METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007), TER (Snover et al., 2006), MAXSIM (Chan and Ng, 2008; Chan and Ng, 2009), and TESLA (Liu et al., 2010). Given the close relationship between automatic MT and automatic MT evaluation, the logical expectation is that a better MT evaluation metric would 375 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 375–384, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics lead to better MT systems. However, this linkage has not yet been realized. In the SMT community, MT tuning still uses BLEU almost exclusively. Some researchers have investigated the use of better metrics for MT tuning, with mixed results. Most notably"
D11-1035,P03-1021,0,0.144675,"Missing"
D11-1035,P02-1040,0,0.114895,"arge datasets. We release all our implementation under an open source license. It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems. 1 Introduction The dominant framework of machine translation (MT) today is statistical machine translation (SMT) (Hutchins, 2007). At the core of the system is the decoder, which performs the actual translation. The The first automatic MT evaluation metric to show a high correlation with human judgment is BLEU (Papineni et al., 2002). Together with its close variant the NIST metric, they have quickly become the standard way of tuning statistical machine translation systems. While BLEU is an impressively simple and effective metric, recent evaluations have shown that many new generation metrics can outperform BLEU in terms of correlation with human judgment (Callison-Burch et al., 2009; CallisonBurch et al., 2010). Some of these new metrics include METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007), TER (Snover et al., 2006), MAXSIM (Chan and Ng, 2008; Chan and Ng, 2009), and TESLA (Liu et al., 2010). Given the clo"
D11-1035,2006.amta-papers.25,0,0.1719,"st automatic MT evaluation metric to show a high correlation with human judgment is BLEU (Papineni et al., 2002). Together with its close variant the NIST metric, they have quickly become the standard way of tuning statistical machine translation systems. While BLEU is an impressively simple and effective metric, recent evaluations have shown that many new generation metrics can outperform BLEU in terms of correlation with human judgment (Callison-Burch et al., 2009; CallisonBurch et al., 2010). Some of these new metrics include METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007), TER (Snover et al., 2006), MAXSIM (Chan and Ng, 2008; Chan and Ng, 2009), and TESLA (Liu et al., 2010). Given the close relationship between automatic MT and automatic MT evaluation, the logical expectation is that a better MT evaluation metric would 375 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 375–384, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics lead to better MT systems. However, this linkage has not yet been realized. In the SMT community, MT tuning still uses BLEU almost exclusively. Some researchers have investiga"
D11-1035,W06-1610,0,0.0578923,"and hence the tendency to produce short translations, BLEU introduces a brevity penalty, defined as ( 1 BP = e1−|R|/|T | if|T |> |R| if|T |≤ |R| where the |· |operator denotes the size of a bag or the number of words in a text. The metric is finally defined as BLEU(R, T) = BP × p 4 P1 P2 P3 P4 BLEU is a very simple metric requiring neither training nor language-specific resources. Its use of the brevity penalty is however questionable, as subsequent research on n-gram-based metrics has consistently found that recall is in fact a more potent indicator than precision (Banerjee and Lavie, 2005; Zhou et al., 2006; Chan and Ng, 2009). As we shall see, despite the BP term, BLEU still exhibits a strong tendency to produce short translations. 2.2 TER TER is based on counting transformations rather than n-gram matches. The metric is defined as the minimum number of edits needed to change a candidate translation T to the reference R, normalized by the length of the reference, i.e., Good morning morning , , sir sir . w=1.0 w=0.1 w=0.1 w=0.1 s=0.4 s=0.1 number of edits TER(R, T) = |R| One edit is defined as one insertion, deletion, or substitution of a single word, or the shift of a contiguous sequence of wor"
D11-1149,J93-2003,0,0.0591676,"metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996; Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models. Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the mappings from λ-sub-expressions to (possibly discontiguous) word sequences with a joint generative process. This motivates us to extract grammar rules from the λ-hybrid trees. Thus, we first find the Vi"
D11-1149,P96-1041,0,0.0386177,"automatic lexical acquisition. We thus compare our system against two stateof-the-art machine translation systems: a phrasebased translation system, implemented in the Moses toolkit (Koehn et al., 2007)6 , and a hierarchical phrase-based translation system, implemented in the Joshua toolkit (Li et al., 2009), which is a reimplementation of the original Hiero system (Chiang, 2005; Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing M ERT. 5.1 Automatic Evaluation For automatic evaluation, we measure the original IBM B LEU score (Papineni et al., 2002) (4-gram precision with brevity penalty) and the T ER score (Snover et al., 2006) (the amount of edits required to change a system output into the reference)7 . Note that T ER measures the translation error rate, thus a 6 We used the default settings, and enabled the default lexicali"
D11-1149,P05-1033,0,0.373602,"pLM (ˆ s)wLM (2) where r ∈ D refers to a rule r that appears in the derivation D, sˆ is the target side (sentence) associated with the derivation D, and fi is a rulespecific feature (one of features 1–3 above) which 1615 is weighted with wi . The language model feature is weighted with wLM . Once the feature values are computed, our goal is to find the optimal weight vector w ¯ ∗ that maximizes a certain evaluation metric when used for decoding, as we will discuss in Section 4.4. Following popular approaches to learning feature weights in the machine translation community (Och and Ney, 2004; Chiang, 2005), we use the minimum error rate training (M ERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-al"
D11-1149,J07-2003,0,0.391248,"n, which has been extensively studied in the machine translation community, translating from logical forms into text presents additional challenges. Specifically, logical forms such as λ-expressions may have complex internal structures and variable dependencies across subexpressions. Problems arise when performing automatic acquisition of a translation lexicon, as well as performing lexical selection and surface realization during generation. In this work, we tackle these challenges by making the following contributions: • A novel forest-to-string generation algorithm: Inspired by the work of Chiang (2007), we introduce a novel reduction-based weighted binary synchronous context-free grammar formalism for generation from logical forms (λexpressions), which can then be integrated with a probabilistic forest-to-string generation algo1611 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1611–1622, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics rithm. • A novel grammar induction algorithm: To automatically induce such synchronous grammar rules, we propose a novel generative model that establishes phrasal corre"
D11-1149,N10-1140,0,0.0318778,"translation community (Och and Ney, 2004; Chiang, 2005), we use the minimum error rate training (M ERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996; Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models. Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the"
D11-1149,P09-1069,0,0.0114433,"guage generation model using the same meaning representation based on tree conditional random fields. Angeli et al. (2010) presented a domain-independent probabilistic approach for generation from database entries. All these models are probabilistic models. Recently there are also substantial research efforts on the task of mapping natural language to meaning 1612 representations in various formalisms – the inverse task of language generation called semantic parsing. Examples include Zettlemoyer and Collins (2005; 2007; 2009), Kate and Mooney (2006), Wong and Mooney (2007b), Lu et al. (2008), Ge and Mooney (2009), as well as Kwiatkowski et al. (2010). Of particular interest is our prior work Lu et al. (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. One important property of the model in our prior work is that it induces a hybrid tree structure automatically in an unsupervised manner, which reveals the correspondences between natural language word sequences and semantic elements. We extend our prior model in th"
D11-1149,P06-1115,0,0.0443551,"ing representation into sentences. Lu et al. (2009) presented a language generation model using the same meaning representation based on tree conditional random fields. Angeli et al. (2010) presented a domain-independent probabilistic approach for generation from database entries. All these models are probabilistic models. Recently there are also substantial research efforts on the task of mapping natural language to meaning 1612 representations in various formalisms – the inverse task of language generation called semantic parsing. Examples include Zettlemoyer and Collins (2005; 2007; 2009), Kate and Mooney (2006), Wong and Mooney (2007b), Lu et al. (2008), Ge and Mooney (2009), as well as Kwiatkowski et al. (2010). Of particular interest is our prior work Lu et al. (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. One important property of the model in our prior work is that it induces a hybrid tree structure automatically in an unsupervised manner, which reveals the correspondences between natural language word"
D11-1149,N03-1017,0,0.0774625,"ture values. For generality, we only consider the following four simple features in this work: 1. p˜(hw |pλ ): the relative frequency estimate of a hybrid sequence hw given the λ-production pλ ; 2. p˜(pλ |hw , τ ): the relative frequency estimate of a λ-production pλ given the phrase hw and the type τ ; 3. exp(−wc(hw )): the number of words generated, where wc(hw ) refers to the number of words in hw (i.e., word penalty); and 4. pLM (ˆ s): the language model score of the generated sentence sˆ. The first three features, which are also widely used in state-of-the-art machine translation models (Koehn et al., 2003; Chiang, 2007), are rule-specific and thus can be computed before decoding. The last feature is computed during the decoding phase in combination with the sibling rules used. We score a derivation D with a log-linear model: w(D) = YY r∈D i fi (r) wi ! × pLM (ˆ s)wLM (2) where r ∈ D refers to a rule r that appears in the derivation D, sˆ is the target side (sentence) associated with the derivation D, and fi is a rulespecific feature (one of features 1–3 above) which 1615 is weighted with wi . The language model feature is weighted with wLM . Once the feature values are computed, our goal is to"
D11-1149,P07-2045,0,0.00288058,"abase records with an additional focus on content selection (selection of records and their subfields for generation). It is not obvious how to adopt their algorithm in our context where content selection is not required but the more complex logical semantic representation is used as input. Other earlier approaches such as the work of Wang (1980) and Shieber et al. (1990) made use of rule-based approaches without automatic lexical acquisition. We thus compare our system against two stateof-the-art machine translation systems: a phrasebased translation system, implemented in the Moses toolkit (Koehn et al., 2007)6 , and a hierarchical phrase-based translation system, implemented in the Joshua toolkit (Li et al., 2009), which is a reimplementation of the original Hiero system (Chiang, 2005; Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems w"
D11-1149,W04-3250,0,0.016236,"presents a very different syntactic structure and word ordering from English. 8 We assume word unigrams are generated from free variables, quantifiers, and logical connectives in IBM model 1. 1618 Our system, on the other hand, employs a packed forest representation for λ-expressions. Therefore, it eliminates the ordering constraint by encompassing exponentially many possible tree structures during both the alignment and decoding stage. As a result, our system obtains significant improvements in both B LEU and 1−T ER using the significance test under the paired bootstrap resampling method of Koehn (2004). We obtain p &lt; 0.01 for all cases, except when comparing against Joshua-preorder for English, where we obtain p &lt; 0.05 for both metrics. text preorder inorder postorder text preorder Joshua inorder postorder This work (t) (t) w/o type 2 rules (t) w/o type 3 rules Moses English B LEU 1−T ER 48.93 61.08 51.13 63.73 46.72 57.59 44.30 55.05 37.40 48.97 51.40 64.69 40.31 50.47 31.10 42.44 54.58 67.65 53.77 66.43 53.68 66.17 Chinese B LEU 1−T ER 43.23 51.71 42.08 50.43 48.03 55.29 46.36 54.59 36.60 46.20 40.05 49.70 48.32 54.64 41.31 49.71 55.11 63.77 54.30 62.49 50.96 60.13 Table 1: Performance on"
D11-1149,D10-1119,0,0.361659,"same meaning representation based on tree conditional random fields. Angeli et al. (2010) presented a domain-independent probabilistic approach for generation from database entries. All these models are probabilistic models. Recently there are also substantial research efforts on the task of mapping natural language to meaning 1612 representations in various formalisms – the inverse task of language generation called semantic parsing. Examples include Zettlemoyer and Collins (2005; 2007; 2009), Kate and Mooney (2006), Wong and Mooney (2007b), Lu et al. (2008), Ge and Mooney (2009), as well as Kwiatkowski et al. (2010). Of particular interest is our prior work Lu et al. (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. One important property of the model in our prior work is that it induces a hybrid tree structure automatically in an unsupervised manner, which reveals the correspondences between natural language word sequences and semantic elements. We extend our prior model in the next section, so as to support λ-exp"
D11-1149,A00-2023,0,0.0616365,"rface realization and lexical acquisition. We demonstrate the effectiveness of our model in Section 5. 2 Related Work The task of language generation from logical forms has a long history. Many early works do not rely on probabilistic approaches. Wang (1980) presented an approach for generation from an extended predicate logic formalism using hand-written rules. Shieber et al. (1990) presented a semantic head-driven approach for generation from logical forms based on rules written in Prolog. Shemtov (1996) presented a system for generation of multiple paraphrases from ambiguous logical forms. Langkilde (2000) presented a probabilistic model for generation from a packed forest meaning representation, without concerning lexical acquisition. Specifically, we are not aware of any prior work that handles both automatic unsupervised lexical acquisition and surface realization for generation from logical forms in a single framework. Another line of research efforts focused on the task of language generation from other meaning representation formalisms. Wong and Mooney (2007a) as well as Chen and Mooney (2008) made use of synchronous grammars to transform a variablefree tree-structured meaning representat"
D11-1149,W09-0424,0,0.027091,"ration). It is not obvious how to adopt their algorithm in our context where content selection is not required but the more complex logical semantic representation is used as input. Other earlier approaches such as the work of Wang (1980) and Shieber et al. (1990) made use of rule-based approaches without automatic lexical acquisition. We thus compare our system against two stateof-the-art machine translation systems: a phrasebased translation system, implemented in the Moses toolkit (Koehn et al., 2007)6 , and a hierarchical phrase-based translation system, implemented in the Joshua toolkit (Li et al., 2009), which is a reimplementation of the original Hiero system (Chiang, 2005; Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing M ERT. 5.1 Automatic Evaluation For automatic evaluation, we measure the original IBM B LEU"
D11-1149,N06-1014,0,0.189371,"gorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996; Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models. Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the mappings from λ-sub-expressions to (possibly discontiguous) word sequences with a joint generative process. This motivates us to extract grammar rules from the λ-hybrid trees. Thus, we first find the Viterbi λ-hybrid trees for all training instances, he, ti 2 : λg.λf.λx.g(x"
D11-1149,D08-1082,1,0.859768,"ith a probabilistic forest-to-string generation algo1611 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1611–1622, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics rithm. • A novel grammar induction algorithm: To automatically induce such synchronous grammar rules, we propose a novel generative model that establishes phrasal correspondences between logical sub-expressions and natural language word sequences, by extending a previous model proposed for parsing natural language into meaning representations (Lu et al., 2008). To our best knowledge, this is the first probabilistic model for generating sentences from the lambda calculus encodings of their underlying formal meaning representations, that concerns both surface realization and lexical acquisition. We demonstrate the effectiveness of our model in Section 5. 2 Related Work The task of language generation from logical forms has a long history. Many early works do not rely on probabilistic approaches. Wang (1980) presented an approach for generation from an extended predicate logic formalism using hand-written rules. Shieber et al. (1990) presented a seman"
D11-1149,D09-1042,1,0.882966,"ral language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al., 2009) or database entries (Angeli et al., 2010). While these algorithms usually work well on specific semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we propose a general probabilistic model that performs generation from underlying formal semantics in the form of typed lambda calculus expressions (we refer to them as λ-expressions throughout this paper), where both lexical acquisition and surface realization are integrated in a single framework. One natural proposal is to adopt a state-of-the-art statistical machine translation appr"
D11-1149,J04-4002,0,0.0807474,"r∈D i fi (r) wi ! × pLM (ˆ s)wLM (2) where r ∈ D refers to a rule r that appears in the derivation D, sˆ is the target side (sentence) associated with the derivation D, and fi is a rulespecific feature (one of features 1–3 above) which 1615 is weighted with wi . The language model feature is weighted with wLM . Once the feature values are computed, our goal is to find the optimal weight vector w ¯ ∗ that maximizes a certain evaluation metric when used for decoding, as we will discuss in Section 4.4. Following popular approaches to learning feature weights in the machine translation community (Och and Ney, 2004; Chiang, 2005), we use the minimum error rate training (M ERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin"
D11-1149,P03-1021,0,0.0124825,"n the derivation D, sˆ is the target side (sentence) associated with the derivation D, and fi is a rulespecific feature (one of features 1–3 above) which 1615 is weighted with wi . The language model feature is weighted with wLM . Once the feature values are computed, our goal is to find the optimal weight vector w ¯ ∗ that maximizes a certain evaluation metric when used for decoding, as we will discuss in Section 4.4. Following popular approaches to learning feature weights in the machine translation community (Och and Ney, 2004; Chiang, 2005), we use the minimum error rate training (M ERT) (Och, 2003) algorithm to learn the feature weights that directly optimize certain automatic evaluation metric. Specifically, the Z-MERT (Zaidan, 2009) implementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alig"
D11-1149,P02-1040,0,0.091148,"is a reimplementation of the original Hiero system (Chiang, 2005; Chiang, 2007). The state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing M ERT. 5.1 Automatic Evaluation For automatic evaluation, we measure the original IBM B LEU score (Papineni et al., 2002) (4-gram precision with brevity penalty) and the T ER score (Snover et al., 2006) (the amount of edits required to change a system output into the reference)7 . Note that T ER measures the translation error rate, thus a 6 We used the default settings, and enabled the default lexicalized reordering model, which yielded better performance. 7 We used tercom version 0.7.25 with the default settings. smaller score indicates a better result. For clarity, we report 1−T ER scores. Following the tuning procedure as conducted in Galley and Manning (2010), we perform M ERT using B LEU as the metric. We c"
D11-1149,C96-2155,0,0.305681,"the lambda calculus encodings of their underlying formal meaning representations, that concerns both surface realization and lexical acquisition. We demonstrate the effectiveness of our model in Section 5. 2 Related Work The task of language generation from logical forms has a long history. Many early works do not rely on probabilistic approaches. Wang (1980) presented an approach for generation from an extended predicate logic formalism using hand-written rules. Shieber et al. (1990) presented a semantic head-driven approach for generation from logical forms based on rules written in Prolog. Shemtov (1996) presented a system for generation of multiple paraphrases from ambiguous logical forms. Langkilde (2000) presented a probabilistic model for generation from a packed forest meaning representation, without concerning lexical acquisition. Specifically, we are not aware of any prior work that handles both automatic unsupervised lexical acquisition and surface realization for generation from logical forms in a single framework. Another line of research efforts focused on the task of language generation from other meaning representation formalisms. Wong and Mooney (2007a) as well as Chen and Moone"
D11-1149,J90-1004,0,0.638979,"Missing"
D11-1149,2006.amta-papers.25,0,0.0302407,"he state-of-the-art unsupervised Berkeley aligner (Liang et al., 2006) with default setting is used to construct word alignments. We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an n-best list of size 100 for all three systems when performing M ERT. 5.1 Automatic Evaluation For automatic evaluation, we measure the original IBM B LEU score (Papineni et al., 2002) (4-gram precision with brevity penalty) and the T ER score (Snover et al., 2006) (the amount of edits required to change a system output into the reference)7 . Note that T ER measures the translation error rate, thus a 6 We used the default settings, and enabled the default lexicalized reordering model, which yielded better performance. 7 We used tercom version 0.7.25 with the default settings. smaller score indicates a better result. For clarity, we report 1−T ER scores. Following the tuning procedure as conducted in Galley and Manning (2010), we perform M ERT using B LEU as the metric. We compare our model against state-of-the-art statistical machine translation systems"
D11-1149,C96-2141,0,0.0907415,"ementation of the algorithm is used in this work. 4.3 Grammar Induction Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003; Chiang, 2005; Galley and Manning, 2010) typically begin with a word-aligned corpus to construct phrasal correspondences. Word-alignment information can be estimated from alignment models, such as the IBM alignment models (Brown et al., 1993) and HMM-based alignment models (Vogel et al., 1996; Liang et al., 2006). However, unlike texts, logical forms have complex internal structures and variable dependencies across sub-expressions. It is not obvious how to establish alignments between logical terms and texts with such alignment models. Fortunately, the generative model for λ-hybrid tree introduced in Section 3 explicitly models the mappings from λ-sub-expressions to (possibly discontiguous) word sequences with a joint generative process. This motivates us to extract grammar rules from the λ-hybrid trees. Thus, we first find the Viterbi λ-hybrid trees for all training instances, he"
D11-1149,C80-1061,0,0.75627,"sions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation. 1 Introduction This work focuses on the task of generating natural language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al., 2009) or database entries (Angeli et al., 2010). While these algorithms usually work well on specific semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we pr"
D11-1149,N06-1056,0,0.103481,"each (fi , si ) ∈ (f, s), find the most probable λ-hybrid tree hi , and then extract the grammar rules from it: hi = F IND H YBRID T REE(fi , si , θ¯∗ ) Γ = Γ ∪ E XTRACT RULES(hi ) 3. Output the learned grammar rule set Γ. Figure 5: The algorithm for learning the grammar rules 5 Experiments For experiments, we evaluated on the G EOQUERY dataset, which consists of 880 queries on U.S. geography. The dataset was manually labeled with λexpressions as their semantics in Zettlemoyer and Collins (2005). It was used in many previous research efforts on semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). The original dataset was annotated with English sentences only. In order to assess the generation performance across different languages, in our work the entire dataset was also manually annotated with Chinese by a native Chinese speaker with linguistics background5 . For all the experiments we present in this section, we use the same split as that of Kwiatkowski 5 The annotator created annotations with both λ-expressions and corresponding English sentences available as references. 1617 et al. (2010), where 280 instances are used for"
D11-1149,N07-1022,0,0.427542,"task of generating natural language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al., 2009) or database entries (Angeli et al., 2010). While these algorithms usually work well on specific semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we propose a general probabilistic model that performs generation from underlying formal semantics in the form of typed lambda calculus expressions (we refer to them as λ-expressions throughout this paper), where both lexical acquisition and surface realization are integrated in a single framework. One natural proposal is to adopt a state-of-the-art statistical machi"
D11-1149,P07-1121,0,0.330828,"task of generating natural language sentences from their underlying meaning representations in the form of formal logical expressions (typed lambda calculus). Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980; Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore lexical acquisition. Recent approaches start to employ corpusbased probabilistic methods, but many of them assume the underlying meaning representations are of specific forms such as variable-free tree-structured representations (Wong and Mooney, 2007a; Lu et al., 2009) or database entries (Angeli et al., 2010). While these algorithms usually work well on specific semantic formalisms, it is unclear how well they could be applied to a different semantic formalism. In this work, we propose a general probabilistic model that performs generation from underlying formal semantics in the form of typed lambda calculus expressions (we refer to them as λ-expressions throughout this paper), where both lexical acquisition and surface realization are integrated in a single framework. One natural proposal is to adopt a state-of-the-art statistical machi"
D11-1149,D07-1071,0,0.0271805,"), find the most probable λ-hybrid tree hi , and then extract the grammar rules from it: hi = F IND H YBRID T REE(fi , si , θ¯∗ ) Γ = Γ ∪ E XTRACT RULES(hi ) 3. Output the learned grammar rule set Γ. Figure 5: The algorithm for learning the grammar rules 5 Experiments For experiments, we evaluated on the G EOQUERY dataset, which consists of 880 queries on U.S. geography. The dataset was manually labeled with λexpressions as their semantics in Zettlemoyer and Collins (2005). It was used in many previous research efforts on semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). The original dataset was annotated with English sentences only. In order to assess the generation performance across different languages, in our work the entire dataset was also manually annotated with Chinese by a native Chinese speaker with linguistics background5 . For all the experiments we present in this section, we use the same split as that of Kwiatkowski 5 The annotator created annotations with both λ-expressions and corresponding English sentences available as references. 1617 et al. (2010), where 280 instances are used for testing, and the remaining inst"
D11-1149,P09-1110,0,0.097247,"Missing"
D11-1149,D10-1049,0,\N,Missing
D12-1027,P06-2005,0,0.0601586,"ypically used. In the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. In fact, this is a more general problem, which arises with informal sources like SMS messages and Tweets for just any language (Aw et al., 2006; Han and Baldwin, 2011). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language into another. A second relevant line of research is on language adaptation and normalization, when done specifically for improving SMT into another language. 1 The Egyptian Wikipedia is one notable exception. 287 For example, Marujo et al. (2011) described a rule-based system for adapting Brazilia"
D12-1027,baldwin-awab-2006-open,0,0.0531602,"h bitext but are relatively frequent in the larger Malay– English one; it also helps for some frequent words. Cross-lingual morphological variants. We increase the Indonesian options for a Malay word using morphology. Since the set of Indonesian options for a Malay word in pivoting is restricted to the Indonesian vocabulary of the small Indonesian– English bi-text, this is a severe limitation of pivoting. Thus, assuming a large monolingual Indonesian text, we first build a lexicon of the words in the text. Then, we lemmatize these words using two different lemmatizers: the Malay lemmatizer of Baldwin and Awab (2006), and a similar Indonesian lemmatizer. Since these two analyzers have different strengths and weaknesses, we combine their outputs to increase recall. Next, we group all Indonesian words that share the same lemma, e.g., for minum, we obtain {diminum, diminumkan, diminumnya, makan-minum, 4.1.3 Further Refinements makananminuman, meminum, meminumkan, meminumnya, meminumMany of our paraphrases are bad: some have very low probabilities, while others involve rare words for which the probability estimates are unreliable. minuman, minum, minum-minum, minum-minuman, minuman, minu5 For balance, in case"
D12-1027,W07-0702,0,0.0222216,"ns its original scores, which are further augmented with 1–3 additional feature scores indicating its origin: the first/second/third feature is 1 if the pair came from the first/second/both table(s), and 0 otherwise. We experiment using all three, the first two, or the first feature only; we also try setting the features to 0.5 instead of 0. This makes the following six combinations (0, 00, 000, .5, .5.5, .5.5.5); on testing, we use the one that achieves the highest BLEU score on the development set. Other possibilities for combining the phrase tables include using alternative decoding paths (Birch et al., 2007), simple linear interpolation, and direct phrase table merging with extra features (CallisonBurch et al., 2006); they were previously found inferior to the last two approaches above (Nakov and Ng, 2009; Nakov and Ng, 2012). 291 5 Experiments We run two kinds of experiments: (a) isolated, where we train on the synthetic “Indonesian”– English bi-text only, and (b) combined, where we combine it with the Indonesian–English bi-text. 5.1 Datasets In our experiments, we use the following datasets, normally required for Indonesian–English SMT: • Indonesian–English train bi-text (IN2EN): 28,383 sentenc"
D12-1027,N06-1003,0,0.232179,"Missing"
D12-1027,P07-1092,0,0.110876,"ttempt language adaptation, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches. Another alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bi-text, which we do not have. Pivoting over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bi-text). 3 Malay and Indonesian Malay and Indonesian are closely related, mutual"
D12-1027,P05-1066,0,0.0330941,"Missing"
D12-1027,A00-1002,0,0.757957,"Missing"
D12-1027,P11-1038,0,0.020239,"the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. In fact, this is a more general problem, which arises with informal sources like SMS messages and Tweets for just any language (Aw et al., 2006; Han and Baldwin, 2011). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language into another. A second relevant line of research is on language adaptation and normalization, when done specifically for improving SMT into another language. 1 The Egyptian Wikipedia is one notable exception. 287 For example, Marujo et al. (2011) described a rule-based system for adapting Brazilian Portuguese (BP) to Eur"
D12-1027,2011.eamt-1.19,0,0.574391,"Missing"
D12-1027,D09-1141,1,0.921289,"ource-rich language, with whom they overlap in vocabulary and share cognates, which offers opportunities for bi-text reuse. Example pairs of such resource rich–poor languages include Spanish–Catalan, Finnish–Estonian, Swedish–Norwegian, Russian–Ukrainian, Irish– Gaelic Scottish, Standard German–Swiss German, Modern Standard Arabic–Dialectical Arabic (e.g., Gulf, Egyptian), Turkish–Azerbaijani, etc. Previous work has already demonstrated the benefits of using a bi-text for a related resource-rich language to X (e.g., X=English) to improve machine translation from a resource-poor language to X (Nakov and Ng, 2009; Nakov and Ng, 2012). Here we take a different, orthogonal approach: we adapt the resource-rich language to get closer to the resource-poor one. We assume a small bi-text for the resource-poor language, which we use to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the two languages. Assuming translation into the same target language X, we adapt (the source side of) a large training bi-text for a related resource-rich language and X. Training on the adapted large bi-text yields very significant improvements in translation quality compared to bot"
D12-1027,P11-1130,1,0.835885,"ants, relying on the Indonesian language model to make the right contextual choice. We also try to model context more directly by generating adaptation options at the phrase level. 7 While the different morphological forms typically have different meanings, e.g., minum (‘drink’) vs. peminum (‘drinker’), in some cases the forms could have the same translation in English, e.g., minum (‘drink’, verb) vs. minuman (‘drink’, noun). This is our motivation for trying morphological variants, even though they are almost exclusively derivational, and thus quite risky as translational variants; see also (Nakov and Ng, 2011). 290 Phrase-level paraphrase induction. We use standard phrase-based SMT techniques to build separate phrase tables for the Indonesian–English and the Malay–English bi-texts, where we have four conditional probabilities: forward/reverse phrase translation probability, and forward/reverse lexicalized phrase translation probability. We pivot over English to generate Indonesian-Malay phrase pairs, whose probabilities are derived from the corresponding ones in the two phrase tables using Eq. 1. Cross-lingual morphological variants. While phrase-level paraphrasing models context better, it remains"
D12-1027,P12-2059,1,0.766512,"tional Linguistics 2 Related Work One relevant line of research is on machine translation between closely related languages, which is arguably simpler than general SMT, and thus can be handled using word-for-word translation, manual language-specific rules that take care of the necessary morphological and syntactic transformations, or character-level translation/transliteration. This has been tried for a number of language pairs including Czech–Slovak (Hajiˇc et al., 2000), Turkish– Crimean Tatar (Altintas and Cicekli, 2002), Irish– Scottish Gaelic (Scannell, 2006), and Bulgarian– Macedonian (Nakov and Tiedemann, 2012). In contrast, we have a different objective – we do not carry out full translation but rather adaptation since our ultimate goal is to translate into a third language X. A special case of this same line of research is the translation between dialects of the same language, e.g., between Cantonese and Mandarin (Zhang, 1998), or between a dialect of a language and a standard version of that language, e.g., between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011). Here again, manual rules and/or language-specific tools are"
D12-1027,P02-1040,0,0.0838775,"e. 292 System ML2EN IN2EN Simple concatenation Balanced concatenation Sophisticated phrase table combination BLEU 14.50 18.67 18.49 19.79 20.10(.5.5) Table 2: The five baselines. The subscript indicates the parameters found on IN2EN-dev and used for IN2EN-test. The scores that are statistically significantly better than ML2EN and IN2EN (p &lt; 0.01, Collins’ sign test) are shown in bold and are underlined, respectively. 6.1 Baseline Experiments The results for the baseline systems are shown in Table 2. We can see that training on ML2EN instead of IN2EN yields over 4 points absolute drop in BLEU (Papineni et al., 2002) score, even though ML2EN is about 10 times larger than IN2EN and both bi-texts are from the same domain. This confirms the existence of important differences between Malay and Indonesian. While simple concatenation does not help, balanced concatenation with repetitions improves by 1.12 BLEU points over IN2EN, which shows the importance of giving IN2EN a proper weight in the combined bi-text. This is further reconfirmed by the sophisticated phrase table combination, which yields an additional absolute gain of 0.31 BLEU points. 6.2 Isolated Experiments Table 3 shows the results for the isolated"
D12-1027,W11-2602,0,0.0342981,"Scottish Gaelic (Scannell, 2006), and Bulgarian– Macedonian (Nakov and Tiedemann, 2012). In contrast, we have a different objective – we do not carry out full translation but rather adaptation since our ultimate goal is to translate into a third language X. A special case of this same line of research is the translation between dialects of the same language, e.g., between Cantonese and Mandarin (Zhang, 1998), or between a dialect of a language and a standard version of that language, e.g., between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011). Here again, manual rules and/or language-specific tools are typically used. In the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. In fact, this is a more general problem, which arises with inform"
D12-1027,2010.amta-papers.5,0,0.191633,"002), Irish– Scottish Gaelic (Scannell, 2006), and Bulgarian– Macedonian (Nakov and Tiedemann, 2012). In contrast, we have a different objective – we do not carry out full translation but rather adaptation since our ultimate goal is to translate into a third language X. A special case of this same line of research is the translation between dialects of the same language, e.g., between Cantonese and Mandarin (Zhang, 1998), or between a dialect of a language and a standard version of that language, e.g., between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011). Here again, manual rules and/or language-specific tools are typically used. In the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. In fact, this is a more general proble"
D12-1027,N07-1061,0,0.09152,"our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches. Another alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bi-text, which we do not have. Pivoting over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bi-text). 3 Malay and Indonesian Malay and Indonesian are"
D12-1027,P09-1018,0,0.122162,"ion, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches. Another alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bi-text, which we do not have. Pivoting over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bi-text). 3 Malay and Indonesian Malay and Indonesian are closely related, mutually intelligible Aust"
D12-1027,P98-2238,0,0.356824,"haracter-level translation/transliteration. This has been tried for a number of language pairs including Czech–Slovak (Hajiˇc et al., 2000), Turkish– Crimean Tatar (Altintas and Cicekli, 2002), Irish– Scottish Gaelic (Scannell, 2006), and Bulgarian– Macedonian (Nakov and Tiedemann, 2012). In contrast, we have a different objective – we do not carry out full translation but rather adaptation since our ultimate goal is to translate into a third language X. A special case of this same line of research is the translation between dialects of the same language, e.g., between Cantonese and Mandarin (Zhang, 1998), or between a dialect of a language and a standard version of that language, e.g., between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011). Here again, manual rules and/or language-specific tools are typically used. In the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain a"
D12-1027,C98-2233,0,\N,Missing
D12-1052,P06-1032,0,0.708601,"Missing"
D12-1052,W07-1604,0,0.081495,"al corrections. Indeed, they point out that the language model often fails to distinguish grammatical and ungrammatical sentences. To the best of our knowledge, our work is the first discriminatively trained decoder for whole-sentence grammatical error correction. 3 2 Decoder Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has re569 In this section, we describe the proposed beamsearch decoder and its components. The task of the de"
D12-1052,D09-1052,0,0.0213778,"003) for constituent chunking, and the MALT parser (Nivre et al., 2007) for dependency parsing. For language modeling, we use RandLM (Talbot and Osborne, 2007). For spelling correction, we use GNU Aspell5 . Words that contain upper-case characters inside the word or are shorter than four characters are excluded from spell checking. The spelling dictionary is augmented with all words that appear at least 10 times in the ACL- ANTHOLOGY data set. Article correction is cast as a multi-class classification problem. As the learning algorithm, we choose multi-class confidence-weighted (CW) learning (Crammer et al., 2009) which has been shown to perform well for NLP problems with high dimensional and sparse feature spaces. The possible classes are the articles a, the, and the empty article . The article an is normalized as a and restored later using a rule-based heuristic. We consider all NPs that are not pronouns and do not have a non-article determiner, e.g., this, that. The classifier is trained on over 5 million instances from ACLANTHOLOGY . We use a combination of features proposed by (Rozovskaya et al., 2011) (which include lexical and POS N-grams, lexical head words, etc.), web-scale N-gram count featu"
D12-1052,D11-1010,1,0.514573,"Missing"
D12-1052,P11-1092,1,0.640664,"Missing"
D12-1052,N12-1067,1,0.125996,"put is The data is similar to the test set., and the gold-standard edits are two corrections with → to,  → the that change with to to and insert the before test set. The official HOO scorer however extracts a single system edit with → to the for this instance. As the extracted system edit is different from the gold-standard edits, the system would be considered wrong, although it proposes the exact same corrected sentence as the gold standard edits. This problem has also been recognized by the HOO shared task organizers (see (Dale and Kilgarriff, 2011), Section 5). Our MaxMatch (M2 ) scorer (Dahlmeier and Ng, 2012) overcomes this problem through an efficient algorithm that computes the set of system edits which has the maximum overlap with the goldstandard edits. We use the M2 scorer as the main evaluation metric in our experiments. Additionally, we also report results with the official HOO scorer. Once the set of system edits is extracted, precision, recall, and F1 measure are computed as follows. 3 “Without bonus” means that a system does not receive extra credit for not making corrections that are considered optional in the gold standard. Pn i=1 |di ∩ gi | P P = n |di | Pn i=1 i=1 |di ∩ gi | P R = n"
D12-1052,W11-2838,0,0.338488,"might require multiple corrections, for example spelling correction followed by noun num572 ber correction. Errors can also be inter-dependent, where correcting one word makes it necessary to change another word, for example to preserve agreement. Our decoding algorithm has the option to correct some words multiple times, while leaving other words unchanged. 4 Experiments We evaluate our decoder in the context of the HOO shared task on grammatical error correction. The goal of the task is to automatically correct errors in academic papers from NLP. The readers can refer to the overview paper (Dale and Kilgarriff, 2011) for details. We compare our proposed method with two baselines: a phrase-based SMT system (described in Section 4.3) and a pipeline of classifiers (described in Section 4.4). 4.1 Data We split the HOO development data into an equal sized training (HOO- TRAIN) and tuning (HOOTUNE ) set. The official HOO test data (HOO- TEST ) is used for evaluation. In the HOO shared task, participants were allowed to raise objections regarding the gold-standard annotations (corrections) of the test data after the test data was released. As a result, the gold-standard annotations could be biased in faData Set"
D12-1052,2009.mtsummit-posters.3,0,0.193966,"Missing"
D12-1052,N10-1019,0,0.39114,"age model often fails to distinguish grammatical and ungrammatical sentences. To the best of our knowledge, our work is the first discriminatively trained decoder for whole-sentence grammatical error correction. 3 2 Decoder Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has re569 In this section, we describe the proposed beamsearch decoder and its components. The task of the decoder is to find the best hypothesis (i.e.,"
D12-1052,D11-1125,0,0.161891,"coder to learn a bias against overcorrecting sentences and to learn which types of corrections are more likely and which are less likely. 3.4 Decoder Model The hypothesis features described in the previous subsection are combined to compute the score of a hypothesis according to the following linear model: s = wT fE (h), (4) where w is the decoder model weight vector and fE is a feature map that computes the hypothesis features described above, given a set of experts E. The weight vector w is tuned on a development set of error-annotated sentences using the PRO ranking optimization algorithm (Hopkins and May, 2011).1 1 We also experimented with the MERT algorithm (Och, 2003) but found that PRO achieved better results. 571 PRO performs decoder parameter tuning through a pair-wise ranking approach. The algorithm starts by sampling hypothesis pairs from the N-best list of the decoder. The metric score for each hypothesis induces a ranking of the two hypotheses in each pair. The task of finding a weight vector that correctly ranks hypotheses can then be reduced to a simple binary classification task. In this work, we use PRO to optimize the F1 correction score, which is defined in Section 4.2. PRO requires"
D12-1052,N03-1017,0,0.0103096,"i | P R = n i=1 |gi | P ×R F1 = 2 × P +R (5) (6) (7) We note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO- TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO- TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the same as the final evaluation F1 score. Also, the training and tuning data are small by SMT standards. The aim for the SMT baseline is not to achieve a state-of-the-art system, but to serve as the simplest possible baseline that uses only off-the-shelf software. 4.4 Pipeline Baseline The second baseline system is a pipeline of class"
D12-1052,P07-2045,0,0.00767977,"ows. 3 “Without bonus” means that a system does not receive extra credit for not making corrections that are considered optional in the gold standard. Pn i=1 |di ∩ gi | P P = n |di | Pn i=1 i=1 |di ∩ gi | P R = n i=1 |gi | P ×R F1 = 2 × P +R (5) (6) (7) We note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO- TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO- TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the same as the final evaluation F1 score. Also, the training and tuning data are small by SMT standards. The aim for the SMT baseline i"
D12-1052,W04-3250,0,0.0126784,"the official HOO scorer. Once the set of system edits is extracted, precision, recall, and F1 measure are computed as follows. 3 “Without bonus” means that a system does not receive extra credit for not making corrections that are considered optional in the gold standard. Pn i=1 |di ∩ gi | P P = n |di | Pn i=1 i=1 |di ∩ gi | P R = n i=1 |gi | P ×R F1 = 2 × P +R (5) (6) (7) We note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO- TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO- TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the sa"
D12-1052,P03-1004,0,0.0202388,"he SMT baseline is not to achieve a state-of-the-art system, but to serve as the simplest possible baseline that uses only off-the-shelf software. 4.4 Pipeline Baseline The second baseline system is a pipeline of classifier-based and rule-based correction steps. Each step takes sentence segmented plain text as input, corrects one particular error category, and feeds the corrected text into the next step. No search or global inference is applied. The correction steps are: 1. Spelling errors 2. Article errors We use the following tools for syntactic processing: OpenNLP4 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for constituent chunking, and the MALT parser (Nivre et al., 2007) for dependency parsing. For language modeling, we use RandLM (Talbot and Osborne, 2007). For spelling correction, we use GNU Aspell5 . Words that contain upper-case characters inside the word or are shorter than four characters are excluded from spell checking. The spelling dictionary is augmented with all words that appear at least 10 times in the ACL- ANTHOLOGY data set. Article correction is cast as a multi-class classification problem. As the learning algorithm, we choose multi-class confidence-weighted (CW) learning (Cram"
D12-1052,C04-1072,0,0.0566496,"est list of the decoder. The metric score for each hypothesis induces a ranking of the two hypotheses in each pair. The task of finding a weight vector that correctly ranks hypotheses can then be reduced to a simple binary classification task. In this work, we use PRO to optimize the F1 correction score, which is defined in Section 4.2. PRO requires a sentence-level score for each hypothesis. As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentencelevel BLEU approximation (Lin and Och, 2004) instead of the corpus-level BLEU score (Papineni et al., 2002). We observed that optimizing sentencelevel F1 score worked well in practice in our experiments. 3.5 Decoder Search Given a set of proposers, experts, and a tuned decoder model, the decoder can be used to correct new unseen sentences. This is done by performing a search over possible hypothesis candidates. The decoder starts with the input sentence as the initial hypothesis, i.e., assuming that all words are correct. It then performs a beam search over the space of possible hypotheses to find the best hypothesis corˆ for an input s"
D12-1052,J03-1002,0,0.00228768,"ptional in the gold standard. Pn i=1 |di ∩ gi | P P = n |di | Pn i=1 i=1 |di ∩ gi | P R = n i=1 |gi | P ×R F1 = 2 × P +R (5) (6) (7) We note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO- TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO- TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the same as the final evaluation F1 score. Also, the training and tuning data are small by SMT standards. The aim for the SMT baseline is not to achieve a state-of-the-art system, but to serve as the simplest possible baseline that uses only off-the-shelf"
D12-1052,P03-1021,0,0.00405549,"types of corrections are more likely and which are less likely. 3.4 Decoder Model The hypothesis features described in the previous subsection are combined to compute the score of a hypothesis according to the following linear model: s = wT fE (h), (4) where w is the decoder model weight vector and fE is a feature map that computes the hypothesis features described above, given a set of experts E. The weight vector w is tuned on a development set of error-annotated sentences using the PRO ranking optimization algorithm (Hopkins and May, 2011).1 1 We also experimented with the MERT algorithm (Och, 2003) but found that PRO achieved better results. 571 PRO performs decoder parameter tuning through a pair-wise ranking approach. The algorithm starts by sampling hypothesis pairs from the N-best list of the decoder. The metric score for each hypothesis induces a ranking of the two hypotheses in each pair. The task of finding a weight vector that correctly ranks hypotheses can then be reduced to a simple binary classification task. In this work, we use PRO to optimize the F1 correction score, which is defined in Section 4.2. PRO requires a sentence-level score for each hypothesis. As F1 score is no"
D12-1052,P02-1040,0,0.0991532,"is induces a ranking of the two hypotheses in each pair. The task of finding a weight vector that correctly ranks hypotheses can then be reduced to a simple binary classification task. In this work, we use PRO to optimize the F1 correction score, which is defined in Section 4.2. PRO requires a sentence-level score for each hypothesis. As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentencelevel BLEU approximation (Lin and Och, 2004) instead of the corpus-level BLEU score (Papineni et al., 2002). We observed that optimizing sentencelevel F1 score worked well in practice in our experiments. 3.5 Decoder Search Given a set of proposers, experts, and a tuned decoder model, the decoder can be used to correct new unseen sentences. This is done by performing a search over possible hypothesis candidates. The decoder starts with the input sentence as the initial hypothesis, i.e., assuming that all words are correct. It then performs a beam search over the space of possible hypotheses to find the best hypothesis corˆ for an input sentence e. The search prorection h ceeds in iterations until th"
D12-1052,P11-1094,0,0.490089,"Missing"
D12-1052,P11-1027,0,0.0098027,"poser and the language model expert. We then add the article proposer and expert, the preposition proposer and expert, the punctuation proposer, and finally the noun number proposer and expert. We refer to the final configuration with all proposers and experts as the full decoder model. Note that error categories are corrected jointly and not in sequential steps as in the pipeline. To make the results directly comparable to the pipeline, the decoder uses the same resources as the pipeline. As the expert models, we use a 5-gram language model from the Web 1T 5-gram corpus with the Berkeley LM (Pauls and Klein, 2011)7 in the decoder and the CW-classifiers described in the last section. The spelling proposer uses the same spellchecker as the pipeline, and the punctuation proposer uses the same rules as the pipeline. The beam width and the maximum number of iterations are set to 10. In earlier experiments, we found that larger values had no effect on the result. The simuunderneath, until, up, upon, with, within, without 7 Berkeley LM is written in Java and was easier to integrate into our Java-based decoder than RandLM. 575 lated annealing temperature T is initialized to 10 and the exponential cooling sched"
D12-1052,P11-1093,0,0.303549,"rammatical and ungrammatical sentences. To the best of our knowledge, our work is the first discriminatively trained decoder for whole-sentence grammatical error correction. 3 2 Decoder Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has re569 In this section, we describe the proposed beamsearch decoder and its components. The task of the decoder is to find the best hypothesis (i.e., the best corrected sentence) for a given input sente"
D12-1052,W11-2843,0,0.225662,"ation problem. As the learning algorithm, we choose multi-class confidence-weighted (CW) learning (Crammer et al., 2009) which has been shown to perform well for NLP problems with high dimensional and sparse feature spaces. The possible classes are the articles a, the, and the empty article . The article an is normalized as a and restored later using a rule-based heuristic. We consider all NPs that are not pronouns and do not have a non-article determiner, e.g., this, that. The classifier is trained on over 5 million instances from ACLANTHOLOGY . We use a combination of features proposed by (Rozovskaya et al., 2011) (which include lexical and POS N-grams, lexical head words, etc.), web-scale N-gram count features from the Web 1T 5-gram corpus following (Bergsma et al., 2009), and dependency head and child features. During testing, a correction is proposed if the predicted article is different from the observed article used by the writer, and the difference between the confidence score for the predicted article and the confidence score for the observed article is larger than a threshold. Threshold parameters are tuned via a grid-search on the HOO- TUNE data. We tune a separate threshold value for each cla"
D12-1052,P07-1065,0,0.0168195,"line Baseline The second baseline system is a pipeline of classifier-based and rule-based correction steps. Each step takes sentence segmented plain text as input, corrects one particular error category, and feeds the corrected text into the next step. No search or global inference is applied. The correction steps are: 1. Spelling errors 2. Article errors We use the following tools for syntactic processing: OpenNLP4 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for constituent chunking, and the MALT parser (Nivre et al., 2007) for dependency parsing. For language modeling, we use RandLM (Talbot and Osborne, 2007). For spelling correction, we use GNU Aspell5 . Words that contain upper-case characters inside the word or are shorter than four characters are excluded from spell checking. The spelling dictionary is augmented with all words that appear at least 10 times in the ACL- ANTHOLOGY data set. Article correction is cast as a multi-class classification problem. As the learning algorithm, we choose multi-class confidence-weighted (CW) learning (Crammer et al., 2009) which has been shown to perform well for NLP problems with high dimensional and sparse feature spaces. The possible classes are the artic"
D12-1052,C08-1109,0,0.32245,"they point out that the language model often fails to distinguish grammatical and ungrammatical sentences. To the best of our knowledge, our work is the first discriminatively trained decoder for whole-sentence grammatical error correction. 3 2 Decoder Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has re569 In this section, we describe the proposed beamsearch decoder and its components. The task of the decoder is to find the best hypo"
D12-1052,P10-2065,0,0.320346,"ion are analogous to article correction. They differ only in terms of the classes and the features. For preposition correction, the classes are 36 frequent English prepositions6 . The features are surrounding 4 http://opennlp.sourceforge.net http://aspell.net 6 about, along, among, around, as, at, beside, besides, between, by, down, during, except, for, from, in, inside, into, of, off, on, onto, outside, over, through, to, toward, towards, under, 5 3. Preposition errors 4. Punctuation errors 5. Noun number errors 574 lexical N-grams, web-scale N-gram counts, and dependency features following (Tetreault et al., 2010). The preposition classifier is trained on 1 million training examples from the ACL- ANTHOLOGY. For noun number correction, the classes are singular and plural. The features are lexical N-grams, webscale N-gram counts, dependency features, the noun lemma, and a binary countability feature. The noun number classifier is trained on over 5 million examples from ACL- ANTHOLOGY. During testing, the singular or plural word surface form is generated using WordNet (Fellbaum, 1998) and simple heuristics. Punctuation correction is done using a set of simple rules developed on the HOO development data. A"
D13-1028,W12-4503,0,0.0338585,"Missing"
D13-1028,W12-4504,0,0.0202855,"trate the impact of zero pronouns on parsing performance, consider the following example:3 Example (1): 将来我们有一 一个 重 建 计 划 。 • Employing gold mentions further boosts our system significantly. In comparison with using gold mention boundaries, the performance improvement is attributed more to an increase in precision. #分公园成七个区域，#带来多一些的 景点。 ... 这 个 计 划 我们现在是等到政府的批准 我们就可以再进行，预算是#明年可以 动工了。 In comparison with the three best systems of CoNLL-2012 in the Chinese closed track (shown in Table 5), considering average F-measure, we find that using automatic mentions, our system is only inferior to that of Chen and Ng (2012); using gold mention boundaries, our system achieves the best performance; and using gold mentions, our system is only a little worse than that of Chen and Ng (2012). 3 (In future, we have a reconstruction plan. Divide the park into seven regions, and bring some more attractions. ... Now we wait for approval of the government before implementing this plan again. It is expected that work can start next year.) Motivation In order to analyze the impact of zero pronouns on Chinese coreference resolution, we first use the released OntoNotes v5.0 data (i.e., the training and development portions of"
D13-1028,D10-1062,0,0.15849,"l., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless improves machine translation performance by 0.96 in BLEU score. 7 Conclusion In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns, to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to detect zero pronouns effectively, and two effective methods are employed to"
D13-1028,D10-1086,1,0.802141,"et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless improves machine translation performance by 0.96 in BLEU score. 7 Conclusion In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns, to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to detect zero pronouns"
D13-1028,D09-1103,1,0.908145,"Missing"
D13-1028,W11-1902,0,0.0269508,"Missing"
D13-1028,P02-1014,0,0.0935082,"for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution. 1 Introduction As one of the most important tasks in discourse analysis, coreference resolution aims to link a given mention (i.e., entity or event) to its co-referring expression in a text and has been a focus of research in natural language processing (NLP) for decades. Over the last decade, various machine learning techniques have been applied to coreference resolution and have performed reasonably well (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012). Current techniques rely primarily on surface level features such as string match, syntactic features such as apposition, and shallow semantic features such as number, gender, semantic class, etc. Despite similarities between Chinese and English, there are differences that have a significant impact on coreference resolution. In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns (ZPs), to improve Chinese coreference resolution. In particular, a simplified semantic role labeling (SRL) framework is proposed to identify Chines"
D13-1028,P07-1068,0,0.0152873,"the zero pronouns occurring in the second sentence help to bridge the coreferential relation between the mention “这 这 个 计 划 /this plan” in the last sentence and the mention “一 一个 重 建 计 划/a reconstruction plan” in the first sentence. 6 Related Work In the last decade, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless"
D13-1028,P06-1055,0,0.106659,"Missing"
D13-1028,N06-1025,0,0.0708199,"resolution. In Example (1), the zero pronouns occurring in the second sentence help to bridge the coreferential relation between the mention “这 这 个 计 划 /this plan” in the last sentence and the mention “一 一个 重 建 计 划/a reconstruction plan” in the first sentence. 6 Related Work In the last decade, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it ne"
D13-1028,J01-4004,1,0.765479,"ploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution. 1 Introduction As one of the most important tasks in discourse analysis, coreference resolution aims to link a given mention (i.e., entity or event) to its co-referring expression in a text and has been a focus of research in natural language processing (NLP) for decades. Over the last decade, various machine learning techniques have been applied to coreference resolution and have performed reasonably well (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012). Current techniques rely primarily on surface level features such as string match, syntactic features such as apposition, and shallow semantic features such as number, gender, semantic class, etc. Despite similarities between Chinese and English, there are differences that have a significant impact on coreference resolution. In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns (ZPs), to improve Chinese coreference resolution. In particular, a simplified semantic role labeling (SRL) framework is propos"
D13-1028,D07-1052,0,0.012666,"pronouns occurring in the second sentence help to bridge the coreferential relation between the mention “这 这 个 计 划 /this plan” in the last sentence and the mention “一 一个 重 建 计 划/a reconstruction plan” in the first sentence. 6 Related Work In the last decade, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless improves machin"
D13-1028,J08-2004,0,0.0798648,"Missing"
D13-1028,C10-2158,0,0.334217,"Missing"
D13-1028,W12-4507,0,0.0395675,"Missing"
D13-1028,D07-1057,1,0.903265,"le-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless improves machine translation performance by 0.96 in BLEU score. 7 Conclusion In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns, to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is propose"
D13-1028,P10-4014,1,0.784163,"t mention and another phrase in the previous context are in an appositive relation. Whether another NP is nested in the current mention. Whether the current mention is nested in another NP. Whether the current mention is the first NP of the sentence. The number of words between the current mention and the nearest previous clause. The number of words between the current mention and the nearest following clause. Whether the current mention and another phrase in the previous context have the same word sense. Word sense annotation is provided in the CoNLL-2012 data set, based on the IMS software (Zhong and Ng, 2010). Table 1: Features employed in our anaphoricity determination system Feature AN/CAPronounType AN/CAGrammaticalRole AN/CAOwnerClauseType AN/CARootPath ANPronounRanking AN/CAClosestNP AN/CAPartDistance AN/CASameSpeaker Description Whether the anaphor or the antecedent candidate is a zero pronoun, first person, second person, third person, neutral pronoun, or others. In our coreference resolution system, a zero pronoun is viewed as a kind of special pronoun. Whether the anaphor or the antecedent candidate is a subject, object, or others. Whether the anaphor or the antecedent candidate is in a ma"
D13-1028,W12-4502,0,\N,Missing
D14-1008,A00-2018,0,0.297316,"Missing"
D14-1008,W05-0305,0,0.0196111,"Missing"
D14-1008,I11-1120,0,0.784703,"e current sentence containing the connective and its immediately preceding sentence as the text span where Arg1 occurs, similar to what was done in (Lin et al., 2014). 3 Related Work For argument labeling in discourse parsing on the PDTB corpus, the related work can be classified into two categories: locating parts of arguments, and labeling full argument spans. As a representative on locating parts of arguments, Wellner and Pustejovsky (2007) proposed several machine learning approaches to identify the head words of the two arguments for discourse As a representative linear tagging approach, Ghosh et al. (2011) cast argument labeling as a linear tagging task using conditional random fields. Ghosh et al. (2012) further improved the perfor69 S VP NP CC But PRP its NNS competitors VBP have and NP ADJP RB VP CC RB VP JJR NN so are NNS VP VBP ADVP VBN RBR cushioned business interests better much broader PP NP IN against NN NNS price swings Figure 1: The gold-standard parse tree corresponding to Example (1) stituents marked NULL) may overwhelm positive instances. To address this problem, we use a simple algorithm to prune out these constituents which are clearly not arguments to the connective in question"
D14-1008,W12-1622,0,0.525034,"where Arg1 occurs, similar to what was done in (Lin et al., 2014). 3 Related Work For argument labeling in discourse parsing on the PDTB corpus, the related work can be classified into two categories: locating parts of arguments, and labeling full argument spans. As a representative on locating parts of arguments, Wellner and Pustejovsky (2007) proposed several machine learning approaches to identify the head words of the two arguments for discourse As a representative linear tagging approach, Ghosh et al. (2011) cast argument labeling as a linear tagging task using conditional random fields. Ghosh et al. (2012) further improved the perfor69 S VP NP CC But PRP its NNS competitors VBP have and NP ADJP RB VP CC RB VP JJR NN so are NNS VP VBP ADVP VBN RBR cushioned business interests better much broader PP NP IN against NN NNS price swings Figure 1: The gold-standard parse tree corresponding to Example (1) stituents marked NULL) may overwhelm positive instances. To address this problem, we use a simple algorithm to prune out these constituents which are clearly not arguments to the connective in question. mance with integration of the n-best results. While the subtree extraction approach locates argumen"
D14-1008,J93-2004,0,0.0501949,"Missing"
D14-1008,miltsakaki-etal-2004-penn,0,0.0218776,"Missing"
D14-1008,P06-1055,0,0.0816101,"Missing"
D14-1008,prasad-etal-2008-penn,0,0.20935,"oach via integer linear programming. Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system. It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments. 1 Introduction Discourse parsing determines the internal structure of a text and identifies the discourse relations between its text units. It has attracted increasing attention in recent years due to its importance in text understanding, especially since the release of the Penn Discourse Treebank (PDTB) corpus (Prasad et al., 2008), which adds a layer of discourse annotations on top of the Penn Treebank ∗ The research reported in this paper was carried out while Fang Kong was a research fellow at the National University of Singapore. 68 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68–77, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2 Penn Discourse Treebank connectives. Following this work, Elwell and Baldridge (2008) combined general and connective specific rankers to improve the performance of labeling the head words of the"
D14-1008,prasad-etal-2010-exploiting,0,0.315938,"Missing"
D14-1008,J08-2005,0,0.035491,"label Arg1 or Arg2. k X E E E Constraint 4: Since we view the previous complete sentence as a special Arg1 constituent candidate, denoted as m, there is at least one candidate assigned as Arg1 for every connective. X D D Constraint 3: For a connective, there is at least one constituent candidate assigned as Arg2. X D $ • How do we unify the two candidate lists? In principle, constituents spanning the same sequence of words should be viewed as the same candidate. That is, for different candidates, we can unify them by adding phantom candidates. This is similar to the approach proposed by Punyakanok et al. (2008) for the semantic role labeling task. For example, Figure 2 shows the candidate lists generated by our pruning algorithm based on two different parse trees given the segment “its competitors have much broader business interests”. Dashed lines are used for phantom candidates and solid lines for true candidates. Here, system A produces one candidate a1, with two phantom candidates a2 and a3 added. Analogously, phantom candidate b3 is added to the candidate list output by System B. In this way, we can get the unified candidate list: “its competitors have much broader business interests”, “its com"
D14-1008,D07-1010,0,0.0634122,"Missing"
D14-1008,W04-3212,0,0.0497855,"then determining the role of every constituent as part of Arg1, Arg2, or NULL, and finally, merging all the constituents for Arg1 and Arg2 to obtain the Arg1 and Arg2 text spans respectively. Obviously, the key to the success of our constituent-based approach is constituent-based argument classification, which determines the role of every constituent argument candidate. As stated above, the PDTB views a connective as the predicate of a discourse relation. Similar to semantic role labeling (SRL), for a given connective, the majority of the constituents in a parse tree may not be its arguments (Xue and Palmer, 2004). This indicates that negative instances (con70 Feature CON-Str CON-LStr CON-Cat CON-iLSib CON-iRSib NT-Ctx CON-NT-Path CON-NT-Position CON-NT-Path-iLsib Description The string of the given connective (case-sensitive) The lowercase string of the given connective The syntactic category of the given connective: subordinating, coordinating, or discourse adverbial Number of left siblings of the connective Number of right siblings of the connective The context of the constituent. We use POS combination of the constituent, its parent, left sibling and right sibling to represent the context. When the"
D14-1008,D09-1161,0,0.0359182,"Missing"
D14-1013,N09-2028,0,0.0210892,"other hand, for the second utterance, if disfluency prediction is performed first, it might mark “I am sorry” as disfluent in the first place and remove it before passing into the second-stage punctuation prediction. Therefore, no matter which task is performed first, certain utterances can always cause confusion. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There are many ways to combine the two tasks. For example, we can perform one task first followed by another, which is called the cascade ap"
D14-1013,P04-1005,0,0.19172,"ediction will treat the whole utterance as three sentences, and thus may not be able to detect any disfluency because each one of the three sentences is legitimate on its own. On the other hand, for the second utterance, if disfluency prediction is performed first, it might mark “I am sorry” as disfluent in the first place and remove it before passing into the second-stage punctuation prediction. Therefore, no matter which task is performed first, certain utterances can always cause confusion. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and a"
D14-1013,P05-1056,0,0.04079,", Qatar. 2014 Association for Computational Linguistics Edit Filler Repair 2 z } |{ z } |{ z } |{ I want a flight to Boston uh I mean to Denver The phrase “to Boston” forms the edit region to be replaced by “to Denver”. The words “uh I mean” are filler words that serve to cue the listener about the error and subsequent corrections. Previous Work There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. The motivation of combining the two tasks can be illustrated by the following two utterances: I am uh I am not going with you . I am sorry . I am not going with you . Notice that the bi-gram “I am” is repeated in both sentences. For the first utterance, if punctuation prediction is perfo"
D14-1013,D10-1018,1,0.930277,"ency Prediction: An Empirical Study Xuancong Wang1,3 Khe Chai Sim2 Hwee Tou Ng1,2 NUS Graduate School for Integrative Sciences and Engineering 2 Department of Computer Science, National University of Singapore 3 Human Language Technology, Institute for Infocomm Research, Singapore xuancong84@gmail.com, {simkc, nght}@comp.nus.edu.sg 1 Abstract aries or punctuation symbols. Spontaneous speech also contains a significant proportion of disfluency. Researchers have shown that splitting input sequences into sentences and adding in punctuation symbols improve machine translation (Favre et al., 2008; Lu and Ng, 2010). Moreover, disfluencies in speech also introduce noise in downstream tasks like machine translation and information extraction (Wang et al., 2010). Thus, punctuation prediction (PU) and disfluency prediction (DF) are two important post-processing tasks for automatic speech recognition because they improve not only the readability of ASR output, but also the performance of downstream Natural Language Processing (NLP) tasks. The task of punctuation prediction is to insert punctuation symbols into conversational speech texts. Punctuation prediction on long, unsegmented texts also achieves the pu"
D14-1013,C14-1138,1,0.760656,"es purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There are many ways to combine the two tasks. For example, we can perform one task first followed by another, which is called the cascade approach. We can also mix the labels, or take the cross-product of the labels, or use joint prediction models. In this paper, we study the mutual influence between the two tasks and compare a variety of common state-of-the-art joint prediction techniques on this joint task. There were also some works that addressed both tasks. Liu et al. (2006) and Baron et al. (1998) car"
D14-1013,P13-1074,0,0.0154729,"he listener about the error and subsequent corrections. Previous Work There were many works on punctuation prediction or disfluency prediction as an isolated task. For punctuation prediction, Huang and Zweig (2002) used maximum entropy model; Christensen et al. (2001) used finite state and multi-layer perceptron method; Liu et al. (2005) used conditional random fields; Lu and Ng (2010) proposed using dynamic conditional random fields for joint sentence boundary type and punctuation prediction; Wang et al. (2012) has added prosodic features for the dynamic conditional random field approach and Zhang et al. (2013) used transition-based parsing. The motivation of combining the two tasks can be illustrated by the following two utterances: I am uh I am not going with you . I am sorry . I am not going with you . Notice that the bi-gram “I am” is repeated in both sentences. For the first utterance, if punctuation prediction is performed first, it might break the utterance both before and after “uh” so that the second-stage disfluency prediction will treat the whole utterance as three sentences, and thus may not be able to detect any disfluency because each one of the three sentences is legitimate on its own"
D14-1013,W04-3236,1,0.872403,"ng a rulebased system as a separate step. In Section 2, we briefly introduce previous work on the two tasks. In Section 3, we describe our baseline system which performs punctuation and disfluency prediction separately (i.e., in isolation). In Section 4, we compare the soft cascade approach with the hard cascade approach. We also examine the effect of task order, i.e., performing which task first benefits more. In Section 5, we compare the cascade approach with bi-directional n-best rescoring. In Section 6, we compare the 2layer Factorial CRF (Sutton et al., 2007) with the cross-product LCRF (Ng and Low, 2004), mixedlabel LCRF (Stolcke et al., 1998), the cascade approach, and the baseline isolated prediction. Section 7 gives a summary of our overall findings. Section 8 gives the conclusion. In this paper, we treat punctuation prediction and disfluency prediction as a joint prediction task, and compare various state-of-the-art joint prediction methods on this task. 122 3 3.1 The Baseline System POS tagger and a filler predictor both using CRF (i.e., using the same approach as that in Qian and Liu (2013)). The same predicted POS tags and fillers are used for feature extraction in all the experiments"
D14-1013,P11-1071,0,0.0218466,"rry” as disfluent in the first place and remove it before passing into the second-stage punctuation prediction. Therefore, no matter which task is performed first, certain utterances can always cause confusion. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There are many ways to combine the two tasks. For example, we can perform one task first followed by another, which is called the cascade approach. We can also mix the labels, or take the cross-product of the labels, or use joint prediction models. In this"
D14-1013,P03-1021,0,0.0107934,"ilar rescoring approach using two models is described in Shi and Wang (2007). The experimental framework is shown in Figure 1. For PU→DF, we first use P (PU|x) to generate an n-best list. Then, for each hypothesis in the n-best list, we use P (DF|PU, x) to obtain another n-best list. So we have n2 -best joint hypotheses. We do the same for DF→PU to obtain another n2 -best joint hypotheses. We rescore the 2n2 -best list using the four models. The four weights α1 , α2 , β1 , and β2 are tuned to optimize the overall F1 score on the development set. We used the MERT (minimum-error-rate training, (Och, 2003)) algorithm to tune the weights. We also vary the size of n. 126 2 n2-best 2 n-best joint-hypo-1 PU-hypo-1 PU-hypo-n DF-hypo-1 P(PU|DF,x) joint-hypo-2 … … P(DF|x) joint-hypo-1 … Input Sequence joint-hypo-2 … … … P(PU|x) P(DF|PU,x) Rescore using: ?1 ∙ log? PU|x +?2 ∙ log? DF|PU, x +?1 ∙ log? DF|x +?2 ∙ log? PU|DF, x DF-hypo-n Figure 1: Illustration of the rescoring pipeline framework using the four M3N models used in the softcascade method: P (PU|x), P (DF|PU, x), P (DF|x) and P (PU|DF, x) get 12 PU-DF-joint labels (Ng and Low, 2004). Figure 2 shows a comparison of these three models in the joi"
D14-1013,N13-1102,0,0.11364,", no matter which task is performed first, certain utterances can always cause confusion. For disfluency prediction, Shriberg et al. (1997) uses purely prosodic features to perform the task. Johnson and Charniak (2004) proposed a TAGbased (Tree-Adjoining Grammar) noisy channel model. Maskey et al. (2006) proposed a phraselevel machine translation approach for this task. Georgila (2009) used integer linear programming (ILP) which can incorporate local and global constraints. Zwarts and Johnson (2011) has investigated the effect of using extra language models as features in the reranking stage. Qian and Liu (2013) proposed using weighted Max-margin Markov Networks (M3N) to balance precision and recall to further improve the F1-score. Wang et al. (2014) proposed a beam-search decoder which integrates M3N and achieved further improvements. There are many ways to combine the two tasks. For example, we can perform one task first followed by another, which is called the cascade approach. We can also mix the labels, or take the cross-product of the labels, or use joint prediction models. In this paper, we study the mutual influence between the two tasks and compare a variety of common state-of-the-art joint"
D14-1102,P06-1032,0,0.714765,"2013; Ng et al., 2014). Each shared task comes with an annotated corpus of learner texts and a benchmark test set, facilitating further research in GEC. Many approaches have been proposed to detect and correct grammatical errors. The most dominant approaches are based on classification (a set of classifier modules where each module addresses a specific error type) and statistical ma951 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951–962, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics work was done by Brockett et al. (2006), where they used SMT to correct mass noun errors. The major impediment in using the SMT approach for GEC is the lack of error-annotated learner (“parallel”) corpora. Mizumoto et al. (2011) mined a learner corpus from the social learning platform Lang-8 and built an SMT system for correcting grammatical errors in Japanese. They further tried their method for English (Mizumoto et al., 2012). Other approaches combine the advantages of classification and SMT (Dahlmeier and Ng, 2012a) and sometimes also include rule-based components. Note that in the hybrid approaches proposed previously, the outp"
D14-1102,W07-1604,0,0.170518,"ment step is critical in system combination. If there is an alignment error, the resulting combined output sentence may be ungrammatical. Rosti et al. (2007a) evaluated three system combination methods in their work: Early research in grammatical error correction focused on a single error type in isolation. For example, Knight and Chander (1994) built an article correction system for post-editing machine translation output. The classification approach has been used to deal with the most common grammatical mistakes made by ESL learners, such as article and preposition errors (Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Wu and Ng, 2013), and more recently, verb errors (Rozovskaya et al., 2014b). Statistical classifiers are trained either from learner or non-learner texts. Features are extracted from the sentence context. Typically, these are shallow features, such as surrounding n-grams, part-of-speech (POS) tags, chunks, etc. Different sets of features are employed depending on the error type addressed. The statistical machine translation (SMT) approach has gained more interest recently. Earlier 952 Step 1 2 3 4 5"
D14-1102,D09-1052,0,0.0137925,"features are employed depending on the error type addressed. The statistical machine translation (SMT) approach has gained more interest recently. Earlier 952 Step 1 2 3 4 5 6 • Sentence level This method looks at the combined N-best list of the systems and selects the best output. • Phrase level This method creates new hypotheses using a new phrase translation table, built according to the phrase alignments of the systems. We model each of the article, preposition, and noun number correction task as a multi-class classification problem. A separate multi-class confidence weighted classifier (Crammer et al., 2009) is used for correcting each of these error types. A correction is only made if the difference between the scores of the original class and the proposed class is larger than a threshold tuned on the development set. The features of the article and preposition classifiers follow the features used by the NUS system from HOO 2012 (Dahlmeier et al., 2012). For the noun number error type, we use lexical n-grams, ngram counts, dependency relations, noun lemma, and countability features. For article correction, the classes are the articles a, the, and the null article. The article an is considered to"
D14-1102,P11-1092,1,0.760527,"ent error, the resulting combined output sentence may be ungrammatical. Rosti et al. (2007a) evaluated three system combination methods in their work: Early research in grammatical error correction focused on a single error type in isolation. For example, Knight and Chander (1994) built an article correction system for post-editing machine translation output. The classification approach has been used to deal with the most common grammatical mistakes made by ESL learners, such as article and preposition errors (Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Wu and Ng, 2013), and more recently, verb errors (Rozovskaya et al., 2014b). Statistical classifiers are trained either from learner or non-learner texts. Features are extracted from the sentence context. Typically, these are shallow features, such as surrounding n-grams, part-of-speech (POS) tags, chunks, etc. Different sets of features are employed depending on the error type addressed. The statistical machine translation (SMT) approach has gained more interest recently. Earlier 952 Step 1 2 3 4 5 6 • Sentence level This method looks at the combined N-best list o"
D14-1102,D12-1052,1,0.801326,", pages 951–962, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics work was done by Brockett et al. (2006), where they used SMT to correct mass noun errors. The major impediment in using the SMT approach for GEC is the lack of error-annotated learner (“parallel”) corpora. Mizumoto et al. (2011) mined a learner corpus from the social learning platform Lang-8 and built an SMT system for correcting grammatical errors in Japanese. They further tried their method for English (Mizumoto et al., 2012). Other approaches combine the advantages of classification and SMT (Dahlmeier and Ng, 2012a) and sometimes also include rule-based components. Note that in the hybrid approaches proposed previously, the output of each component system might be only partially corrected for some subset of error types. This is different from our system combination approach, where the output of each component system is a complete correction of the input sentence where all error types are dealt with. State-of-the-art performance is achieved by both the classification (Dahlmeier et al., 2012; Rozovskaya et al., 2013; Rozovskaya et al., 2014a) and the SMT approach (Felice et al., 2014; Junczys-Dowmunt and"
D14-1102,N12-1067,1,0.460601,", pages 951–962, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics work was done by Brockett et al. (2006), where they used SMT to correct mass noun errors. The major impediment in using the SMT approach for GEC is the lack of error-annotated learner (“parallel”) corpora. Mizumoto et al. (2011) mined a learner corpus from the social learning platform Lang-8 and built an SMT system for correcting grammatical errors in Japanese. They further tried their method for English (Mizumoto et al., 2012). Other approaches combine the advantages of classification and SMT (Dahlmeier and Ng, 2012a) and sometimes also include rule-based components. Note that in the hybrid approaches proposed previously, the output of each component system might be only partially corrected for some subset of error types. This is different from our system combination approach, where the output of each component system is a complete correction of the input sentence where all error types are dealt with. State-of-the-art performance is achieved by both the classification (Dahlmeier et al., 2012; Rozovskaya et al., 2013; Rozovskaya et al., 2014a) and the SMT approach (Felice et al., 2014; Junczys-Dowmunt and"
D14-1102,W05-0909,0,0.16656,"munt and Grundkiewicz, 2014). This feature is not included in S1. 4 • Length The number of tokens in a hypothesis. It is useful to normalize the impact of sentence length. • Language model Log probability from a language model. It is especially useful in maintaining sentence fluency. • Backoff The average n-gram length found in the language model. • Match The number of n-gram matches between the outputs of the component systems and the hypothesis, counted for small order n-grams. System Combination We use MEMT (Heafield and Lavie, 2010) to combine the outputs of our systems. MEMT uses METEOR (Banerjee and Lavie, 2005) to perform alignment of each pair of outputs from the component systems. The METEOR matcher can identify exact matches, words with identical stems, synonyms, and unigram paraphrases. MEMT uses an approach similar to the confusion network approach in SMT system combination. The difference is that it performs alignment The weights of these features are tuned using ZMERT (Zaidan, 2009) on a development set. This system combination approach has a few advantages in grammatical error correction. METEOR not only can match words with exact matches, but also words with identical stems, synonyms, and u"
D14-1102,W13-1703,1,0.868159,"= arg max P (e|f ) e = arg max exp e M X ! λm hm (e, f ) (1) m=1 where f is the input sentence, e is the corrected output sentence, hm is a feature function, and λm is its weight. The feature functions include a translation model learned from a sentence-aligned parallel corpus and a language model learned from a large English corpus. More feature functions can be integrated into the log-linear model. A decoder finds the best correction eˆ that maximizes Equation 1 above. The parallel corpora that we use to train the translation model come from two different sources. The first corpus is NUCLE (Dahlmeier et al., 2013), containing essays written by students at the National University of Singapore (NUS) which have been manually corrected by English instructors at NUS. The other corpus is collected from the language exchange social networking website Lang-8. We develop two versions of SMT systems: one with two phrase tables trained on NUCLE and Lang-8 separately (S1), and the other with a single phrase table trained on the concatenation of NUCLE and Lang-8 data (S2). Multiple phrase tables are used with alternative decoding paths (Birch et al., 2007). We add a word-level Levenshtein distance feature in the ph"
D14-1102,W07-0702,0,0.0131322,"from two different sources. The first corpus is NUCLE (Dahlmeier et al., 2013), containing essays written by students at the National University of Singapore (NUS) which have been manually corrected by English instructors at NUS. The other corpus is collected from the language exchange social networking website Lang-8. We develop two versions of SMT systems: one with two phrase tables trained on NUCLE and Lang-8 separately (S1), and the other with a single phrase table trained on the concatenation of NUCLE and Lang-8 data (S2). Multiple phrase tables are used with alternative decoding paths (Birch et al., 2007). We add a word-level Levenshtein distance feature in the phrase table used by S2, similar to (Felice et al., 2014; JunczysDowmunt and Grundkiewicz, 2014). This feature is not included in S1. 4 • Length The number of tokens in a hypothesis. It is useful to normalize the impact of sentence length. • Language model Log probability from a language model. It is especially useful in maintaining sentence fluency. • Backoff The average n-gram length found in the language model. • Match The number of n-gram matches between the outputs of the component systems and the hypothesis, counted for small orde"
D14-1102,W10-4236,0,0.0148245,"the CoNLL-2014 shared task, outperforming the best system in the shared task. 1 Introduction Grammatical error correction (GEC) refers to the task of detecting and correcting grammatical errors present in a text written by a second language learner. For example, a GEC system to correct English promises to benefit millions of learners around the world, since it functions as a learning aid by providing instantaneous feedback on ESL writing. Research in this area has attracted much interest recently, with four shared tasks organized in the past several years: Helping Our Own (HOO) 2011 and 2012 (Dale and Kilgarriff, 2010; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). Each shared task comes with an annotated corpus of learner texts and a benchmark test set, facilitating further research in GEC. Many approaches have been proposed to detect and correct grammatical errors. The most dominant approaches are based on classification (a set of classifier modules where each module addresses a specific error type) and statistical ma951 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951–962, c October 25-29, 2014, Do"
D14-1102,W12-2006,0,0.0821405,"outperforming the best system in the shared task. 1 Introduction Grammatical error correction (GEC) refers to the task of detecting and correcting grammatical errors present in a text written by a second language learner. For example, a GEC system to correct English promises to benefit millions of learners around the world, since it functions as a learning aid by providing instantaneous feedback on ESL writing. Research in this area has attracted much interest recently, with four shared tasks organized in the past several years: Helping Our Own (HOO) 2011 and 2012 (Dale and Kilgarriff, 2010; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). Each shared task comes with an annotated corpus of learner texts and a benchmark test set, facilitating further research in GEC. Many approaches have been proposed to detect and correct grammatical errors. The most dominant approaches are based on classification (a set of classifier modules where each module addresses a specific error type) and statistical ma951 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951–962, c October 25-29, 2014, Doha, Qatar. 2014 Asso"
D14-1102,P07-2045,0,0.0155697,"aining data of the CoNLL2014 shared task, to train our component systems. The grammatical errors in this corpus are categorized into 28 different error types. We also use the “Lang-8 Corpus of Learner English v1.0”2 (Tajiri et al., 2012) to obtain additional learner data. English Wikipedia3 is used for language modeling and collecting n-gram counts. All systems are tuned on the CoNLL-2013 test data (which serves as the development data set) and tested on the CoNLL-2014 test data. The statistics of the data sets can be found in Table 2. 5.2 (5) 5.4 SMT System The system is trained using Moses (Koehn et al., 2007), with Giza++ (Och and Ney, 2003) for word alignment. The translation table is trained using the “parallel” corpora of NUCLE and Lang-8. The table contains phrase pairs of maximum length seven. We include five standard parameters in the translation table: forward and reverse phrase translations, forward and reverse lexical translations, Evaluation System performance is evaluated based on precision, recall, and F0.5 (which weights precision twice as much as recall). Given a set of n sentences, where gi is the set of gold-standard edits 2 4 3 5 http://www.comp.nus.edu.sg/∼nlp/sw/m2scorer.tar.gz"
D14-1102,W14-1702,0,0.318083,"fication and SMT (Dahlmeier and Ng, 2012a) and sometimes also include rule-based components. Note that in the hybrid approaches proposed previously, the output of each component system might be only partially corrected for some subset of error types. This is different from our system combination approach, where the output of each component system is a complete correction of the input sentence where all error types are dealt with. State-of-the-art performance is achieved by both the classification (Dahlmeier et al., 2012; Rozovskaya et al., 2013; Rozovskaya et al., 2014a) and the SMT approach (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014), which motivates us to attempt system output combination from both approaches. lows: • It is the first work that makes use of a system combination strategy to improve grammatical error correction; • It gives a detailed description of methods and experimental setup for building component systems using two state-of-the-art approaches; and • It provides a detailed analysis of how one approach can benefit from the other approach through system combination. We evaluate our system combination approach on the CoNLL-2014 shared task. The approach achieves an F"
D14-1102,I11-1017,0,0.17563,"o detect and correct grammatical errors. The most dominant approaches are based on classification (a set of classifier modules where each module addresses a specific error type) and statistical ma951 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951–962, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics work was done by Brockett et al. (2006), where they used SMT to correct mass noun errors. The major impediment in using the SMT approach for GEC is the lack of error-annotated learner (“parallel”) corpora. Mizumoto et al. (2011) mined a learner corpus from the social learning platform Lang-8 and built an SMT system for correcting grammatical errors in Japanese. They further tried their method for English (Mizumoto et al., 2012). Other approaches combine the advantages of classification and SMT (Dahlmeier and Ng, 2012a) and sometimes also include rule-based components. Note that in the hybrid approaches proposed previously, the output of each component system might be only partially corrected for some subset of error types. This is different from our system combination approach, where the output of each component syst"
D14-1102,N10-1019,0,0.0546919,"is an alignment error, the resulting combined output sentence may be ungrammatical. Rosti et al. (2007a) evaluated three system combination methods in their work: Early research in grammatical error correction focused on a single error type in isolation. For example, Knight and Chander (1994) built an article correction system for post-editing machine translation output. The classification approach has been used to deal with the most common grammatical mistakes made by ESL learners, such as article and preposition errors (Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Wu and Ng, 2013), and more recently, verb errors (Rozovskaya et al., 2014b). Statistical classifiers are trained either from learner or non-learner texts. Features are extracted from the sentence context. Typically, these are shallow features, such as surrounding n-grams, part-of-speech (POS) tags, chunks, etc. Different sets of features are employed depending on the error type addressed. The statistical machine translation (SMT) approach has gained more interest recently. Earlier 952 Step 1 2 3 4 5 6 • Sentence level This method looks at th"
D14-1102,C12-2084,0,0.407264,"ceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951–962, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics work was done by Brockett et al. (2006), where they used SMT to correct mass noun errors. The major impediment in using the SMT approach for GEC is the lack of error-annotated learner (“parallel”) corpora. Mizumoto et al. (2011) mined a learner corpus from the social learning platform Lang-8 and built an SMT system for correcting grammatical errors in Japanese. They further tried their method for English (Mizumoto et al., 2012). Other approaches combine the advantages of classification and SMT (Dahlmeier and Ng, 2012a) and sometimes also include rule-based components. Note that in the hybrid approaches proposed previously, the output of each component system might be only partially corrected for some subset of error types. This is different from our system combination approach, where the output of each component system is a complete correction of the input sentence where all error types are dealt with. State-of-the-art performance is achieved by both the classification (Dahlmeier et al., 2012; Rozovskaya et al., 201"
D14-1102,W13-3601,1,0.787883,"ion Grammatical error correction (GEC) refers to the task of detecting and correcting grammatical errors present in a text written by a second language learner. For example, a GEC system to correct English promises to benefit millions of learners around the world, since it functions as a learning aid by providing instantaneous feedback on ESL writing. Research in this area has attracted much interest recently, with four shared tasks organized in the past several years: Helping Our Own (HOO) 2011 and 2012 (Dale and Kilgarriff, 2010; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). Each shared task comes with an annotated corpus of learner texts and a benchmark test set, facilitating further research in GEC. Many approaches have been proposed to detect and correct grammatical errors. The most dominant approaches are based on classification (a set of classifier modules where each module addresses a specific error type) and statistical ma951 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951–962, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics work was done by Broc"
D14-1102,W09-0408,0,0.0926499,"ne-best hypothesis. The search is carried out from left to right, one word at a time, creating a partial hypothesis. During beam search, it can freely switch among the component systems, combining the outputs together into a sentence. When it adds a word to its hypothesis, all the words aligned to it in the other systems are also marked as “used”. If it switches to another input sentence, it has to use the first “unused” word in that sentence. This is done to make sure that every aligned word in the sentences is used. In some cases, a heuristic could be used to allow skipping over some words (Heafield et al., 2009). During beam search, MEMT uses a few features to score the hypotheses (both partial hypotheses and full hypotheses): Statistical Machine Translation The other two component systems on phrase-based statistical machine (Koehn et al., 2003). It follows known log-linear model formulation Ney, 2002): are based translation the well(Och and eˆ = arg max P (e|f ) e = arg max exp e M X ! λm hm (e, f ) (1) m=1 where f is the input sentence, e is the corrected output sentence, hm is a feature function, and λm is its weight. The feature functions include a translation model learned from a sentence-aligne"
D14-1102,W14-1701,1,0.770916,"rror correction (GEC) refers to the task of detecting and correcting grammatical errors present in a text written by a second language learner. For example, a GEC system to correct English promises to benefit millions of learners around the world, since it functions as a learning aid by providing instantaneous feedback on ESL writing. Research in this area has attracted much interest recently, with four shared tasks organized in the past several years: Helping Our Own (HOO) 2011 and 2012 (Dale and Kilgarriff, 2010; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). Each shared task comes with an annotated corpus of learner texts and a benchmark test set, facilitating further research in GEC. Many approaches have been proposed to detect and correct grammatical errors. The most dominant approaches are based on classification (a set of classifier modules where each module addresses a specific error type) and statistical ma951 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951–962, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics work was done by Brockett et al. (2006)"
D14-1102,P13-2121,0,0.048557,"Missing"
D14-1102,P02-1038,0,0.341875,"Missing"
D14-1102,W11-2123,0,0.00687615,"The intuition is that most error types do not involve long-range reordering and local reordering can be easily captured in the phrase translation table. The distortion limit is set to 0 to prohibit reordering during hypothesis generation. We build two 5-gram language models using the corrected side of NUCLE and English Wikipedia. The language models are estimated using the KenLM toolkit (Heafield et al., 2013) with modified Kneser-Ney smoothing. These two language models are used as separate feature functions in the log-linear model. Finally, they are binarized into a probing data structure (Heafield, 2011). Tuning is done on the development data set with MERT (Och, 2003). We use BLEU (Papineni et al., 2002) as the tuning metric, which turns out to work well in our experiment. 5.5 35.44 34.70 35.80 34.69 38.19 37.90 39.39 37.33 36.79 35.01 30.88 Table 3: Performance of the pipeline, SMT, and combined systems on the CoNLL-2014 test set. All improvements of combined systems over their component systems are statistically significant (p &lt; 0.01). The differences between P1 and S1 and between P2 and S2 are not statistically significant. Combined System We use an open source MEMT implementation by Heaf"
D14-1102,J03-1002,0,0.0101374,"d task, to train our component systems. The grammatical errors in this corpus are categorized into 28 different error types. We also use the “Lang-8 Corpus of Learner English v1.0”2 (Tajiri et al., 2012) to obtain additional learner data. English Wikipedia3 is used for language modeling and collecting n-gram counts. All systems are tuned on the CoNLL-2013 test data (which serves as the development data set) and tested on the CoNLL-2014 test data. The statistics of the data sets can be found in Table 2. 5.2 (5) 5.4 SMT System The system is trained using Moses (Koehn et al., 2007), with Giza++ (Och and Ney, 2003) for word alignment. The translation table is trained using the “parallel” corpora of NUCLE and Lang-8. The table contains phrase pairs of maximum length seven. We include five standard parameters in the translation table: forward and reverse phrase translations, forward and reverse lexical translations, Evaluation System performance is evaluated based on precision, recall, and F0.5 (which weights precision twice as much as recall). Given a set of n sentences, where gi is the set of gold-standard edits 2 4 3 5 http://www.comp.nus.edu.sg/∼nlp/sw/m2scorer.tar.gz https://code.google.com/p/clearnl"
D14-1102,W14-1703,0,0.568006,"Missing"
D14-1102,P03-1021,0,0.0376748,"ering and local reordering can be easily captured in the phrase translation table. The distortion limit is set to 0 to prohibit reordering during hypothesis generation. We build two 5-gram language models using the corrected side of NUCLE and English Wikipedia. The language models are estimated using the KenLM toolkit (Heafield et al., 2013) with modified Kneser-Ney smoothing. These two language models are used as separate feature functions in the log-linear model. Finally, they are binarized into a probing data structure (Heafield, 2011). Tuning is done on the development data set with MERT (Och, 2003). We use BLEU (Papineni et al., 2002) as the tuning metric, which turns out to work well in our experiment. 5.5 35.44 34.70 35.80 34.69 38.19 37.90 39.39 37.33 36.79 35.01 30.88 Table 3: Performance of the pipeline, SMT, and combined systems on the CoNLL-2014 test set. All improvements of combined systems over their component systems are statistically significant (p &lt; 0.01). The differences between P1 and S1 and between P2 and S2 are not statistically significant. Combined System We use an open source MEMT implementation by Heafield and Lavie (2010) to combine the outputs of our systems. Param"
D14-1102,P02-1040,0,0.0970436,"g can be easily captured in the phrase translation table. The distortion limit is set to 0 to prohibit reordering during hypothesis generation. We build two 5-gram language models using the corrected side of NUCLE and English Wikipedia. The language models are estimated using the KenLM toolkit (Heafield et al., 2013) with modified Kneser-Ney smoothing. These two language models are used as separate feature functions in the log-linear model. Finally, they are binarized into a probing data structure (Heafield, 2011). Tuning is done on the development data set with MERT (Och, 2003). We use BLEU (Papineni et al., 2002) as the tuning metric, which turns out to work well in our experiment. 5.5 35.44 34.70 35.80 34.69 38.19 37.90 39.39 37.33 36.79 35.01 30.88 Table 3: Performance of the pipeline, SMT, and combined systems on the CoNLL-2014 test set. All improvements of combined systems over their component systems are statistically significant (p &lt; 0.01). The differences between P1 and S1 and between P2 and S2 are not statistically significant. Combined System We use an open source MEMT implementation by Heafield and Lavie (2010) to combine the outputs of our systems. Parameters are set to the values recommend"
D14-1102,N07-1029,0,0.0167807,"Missing"
D14-1102,P07-1040,0,0.0130575,"results. Section 7 describes further experiments on system combination. Finally, Section 8 concludes the paper. 2 2.1 Related Work 2.2 Grammatical Error Correction System Combination System combination is the task of combining the outputs of multiple systems to produce an output better than each of its individual component systems. In machine translation (MT), combining multiple MT outputs has been attempted in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2009; Bojar et al., 2011). One of the common approaches in system combination is the confusion network approach (Rosti et al., 2007b). In this approach, a confusion network is created by aligning the outputs of multiple systems. The combined output is generated by choosing the output of one single system as the “backbone”, and aligning the outputs of all other systems to this backbone. The word order of the combined output will then follow the word order of the backbone. The alignment step is critical in system combination. If there is an alignment error, the resulting combined output sentence may be ungrammatical. Rosti et al. (2007a) evaluated three system combination methods in their work: Early research in grammatical"
D14-1102,P11-1093,0,0.116002,"combined output sentence may be ungrammatical. Rosti et al. (2007a) evaluated three system combination methods in their work: Early research in grammatical error correction focused on a single error type in isolation. For example, Knight and Chander (1994) built an article correction system for post-editing machine translation output. The classification approach has been used to deal with the most common grammatical mistakes made by ESL learners, such as article and preposition errors (Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Wu and Ng, 2013), and more recently, verb errors (Rozovskaya et al., 2014b). Statistical classifiers are trained either from learner or non-learner texts. Features are extracted from the sentence context. Typically, these are shallow features, such as surrounding n-grams, part-of-speech (POS) tags, chunks, etc. Different sets of features are employed depending on the error type addressed. The statistical machine translation (SMT) approach has gained more interest recently. Earlier 952 Step 1 2 3 4 5 6 • Sentence level This method looks at the combined N-best list of the systems and selects t"
D14-1102,W13-3602,0,0.0366959,"Mizumoto et al., 2012). Other approaches combine the advantages of classification and SMT (Dahlmeier and Ng, 2012a) and sometimes also include rule-based components. Note that in the hybrid approaches proposed previously, the output of each component system might be only partially corrected for some subset of error types. This is different from our system combination approach, where the output of each component system is a complete correction of the input sentence where all error types are dealt with. State-of-the-art performance is achieved by both the classification (Dahlmeier et al., 2012; Rozovskaya et al., 2013; Rozovskaya et al., 2014a) and the SMT approach (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014), which motivates us to attempt system output combination from both approaches. lows: • It is the first work that makes use of a system combination strategy to improve grammatical error correction; • It gives a detailed description of methods and experimental setup for building component systems using two state-of-the-art approaches; and • It provides a detailed analysis of how one approach can benefit from the other approach through system combination. We evaluate our system combinati"
D14-1102,W14-1704,0,0.67937,"ther approaches combine the advantages of classification and SMT (Dahlmeier and Ng, 2012a) and sometimes also include rule-based components. Note that in the hybrid approaches proposed previously, the output of each component system might be only partially corrected for some subset of error types. This is different from our system combination approach, where the output of each component system is a complete correction of the input sentence where all error types are dealt with. State-of-the-art performance is achieved by both the classification (Dahlmeier et al., 2012; Rozovskaya et al., 2013; Rozovskaya et al., 2014a) and the SMT approach (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014), which motivates us to attempt system output combination from both approaches. lows: • It is the first work that makes use of a system combination strategy to improve grammatical error correction; • It gives a detailed description of methods and experimental setup for building component systems using two state-of-the-art approaches; and • It provides a detailed analysis of how one approach can benefit from the other approach through system combination. We evaluate our system combination approach on the CoNLL-"
D14-1102,E14-1038,0,0.19437,"ther approaches combine the advantages of classification and SMT (Dahlmeier and Ng, 2012a) and sometimes also include rule-based components. Note that in the hybrid approaches proposed previously, the output of each component system might be only partially corrected for some subset of error types. This is different from our system combination approach, where the output of each component system is a complete correction of the input sentence where all error types are dealt with. State-of-the-art performance is achieved by both the classification (Dahlmeier et al., 2012; Rozovskaya et al., 2013; Rozovskaya et al., 2014a) and the SMT approach (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014), which motivates us to attempt system output combination from both approaches. lows: • It is the first work that makes use of a system combination strategy to improve grammatical error correction; • It gives a detailed description of methods and experimental setup for building component systems using two state-of-the-art approaches; and • It provides a detailed analysis of how one approach can benefit from the other approach through system combination. We evaluate our system combination approach on the CoNLL-"
D14-1102,P12-2039,0,0.034909,"eposition, and noun number correctors use the classifier approach to correct errors. Each classifier is trained using multi-class confidence weighted learning on the NUCLE and Lang8 corpora. The classifier threshold is tuned using a simple grid search on the development data set for each class of a classifier. Data We use NUCLE version 3.2 (Dahlmeier et al., 2013), the official training data of the CoNLL2014 shared task, to train our component systems. The grammatical errors in this corpus are categorized into 28 different error types. We also use the “Lang-8 Corpus of Learner English v1.0”2 (Tajiri et al., 2012) to obtain additional learner data. English Wikipedia3 is used for language modeling and collecting n-gram counts. All systems are tuned on the CoNLL-2013 test data (which serves as the development data set) and tested on the CoNLL-2014 test data. The statistics of the data sets can be found in Table 2. 5.2 (5) 5.4 SMT System The system is trained using Moses (Koehn et al., 2007), with Giza++ (Och and Ney, 2003) for word alignment. The translation table is trained using the “parallel” corpora of NUCLE and Lang-8. The table contains phrase pairs of maximum length seven. We include five standard"
D14-1102,C08-1109,0,0.0944076,"n system combination. If there is an alignment error, the resulting combined output sentence may be ungrammatical. Rosti et al. (2007a) evaluated three system combination methods in their work: Early research in grammatical error correction focused on a single error type in isolation. For example, Knight and Chander (1994) built an article correction system for post-editing machine translation output. The classification approach has been used to deal with the most common grammatical mistakes made by ESL learners, such as article and preposition errors (Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Wu and Ng, 2013), and more recently, verb errors (Rozovskaya et al., 2014b). Statistical classifiers are trained either from learner or non-learner texts. Features are extracted from the sentence context. Typically, these are shallow features, such as surrounding n-grams, part-of-speech (POS) tags, chunks, etc. Different sets of features are employed depending on the error type addressed. The statistical machine translation (SMT) approach has gained more interest recently. Earlier 952 Step 1 2 3 4 5 6 • Sentence level This metho"
D14-1102,P13-1143,1,0.861748,"ay be ungrammatical. Rosti et al. (2007a) evaluated three system combination methods in their work: Early research in grammatical error correction focused on a single error type in isolation. For example, Knight and Chander (1994) built an article correction system for post-editing machine translation output. The classification approach has been used to deal with the most common grammatical mistakes made by ESL learners, such as article and preposition errors (Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Wu and Ng, 2013), and more recently, verb errors (Rozovskaya et al., 2014b). Statistical classifiers are trained either from learner or non-learner texts. Features are extracted from the sentence context. Typically, these are shallow features, such as surrounding n-grams, part-of-speech (POS) tags, chunks, etc. Different sets of features are employed depending on the error type addressed. The statistical machine translation (SMT) approach has gained more interest recently. Earlier 952 Step 1 2 3 4 5 6 • Sentence level This method looks at the combined N-best list of the systems and selects the best output. •"
D14-1102,W09-0401,0,\N,Missing
D14-1102,N03-1017,0,\N,Missing
D14-1102,W12-2025,1,\N,Missing
D14-1102,W11-2101,0,\N,Missing
D15-1049,S13-2046,0,0.0126649,"fer among the systems, ranging from simple features (e.g., word length, 2 Domain Adaptation Φs (x) = hx, x, 0i Φt (x) = hx, 0, xi, and 0 is a zero vector of length |x|. This adaptation scheme is attractive because of its simplicity and ease of use as a pre-processing step, and also because it performs quite well despite its simplicity. It has been used in various NLP tasks such http://www.vantagelearning.com/products/intellimetric/ 432 as word segmentation (Monroe et al., 2014), machine translation (Green et al., 2014), word sense disambiguation (Zhong et al., 2008), and short answer scoring (Heilman and Madnani, 2013). Our work is an extension of this scheme in the sense that our work is a generalization of EasyAdapt. 3 number of source domain essays is much larger than the target domain essays. EASE uses NLTK (Bird et al., 2009) for POS tagging and stemming, aspell for spellchecking, and WordNet (Fellbaum, 1998) to get the synonyms. Correct POS tags are generated using a grammatically correct text (provided by EASE). The POS tag sequences not included in the correct POS tags are considered as bad POS. EASE uses scikit-learn (Pedregosa et al., 2011) for extracting unigram and bigram features. For linear re"
D15-1049,P07-1034,0,0.0133726,"n the sense that they could apply to all kinds of prompts. Such features include the number of spelling errors, grammatical errors, lexical complexity, etc. Others are prompt-specific features such as bag of words features. Related Work We first introduce related work on automated essay scoring, followed by domain adaptation in the context of natural language processing. 2.1 2.2 The knowledge learned from a single domain might not be directly applicable to another domain. For example, a named entity recognition system trained on labeled news data might not perform as well on biomedical texts (Jiang and Zhai, 2007). We can solve this problem either by getting labeled data from the other domain, which might not be available, or by performing domain adaptation. Domain adaptation is the task of adapting knowledge learned in a source domain to a target domain. Various approaches to this task have been proposed and used in the context of NLP. Some commonly used approaches include EasyAdapt (Daum´e III, 2007), instance weighting (IW) (Jiang and Zhai, 2007), and structural correspondence learning (SCL) (Blitzer et al., 2006). We can divide the approaches of domain adaptation into two categories based on the av"
D15-1049,P14-2034,0,0.0174566,"scale them easily in the case of different score ranges between the source essay prompt and the target essay prompt. The features used differ among the systems, ranging from simple features (e.g., word length, 2 Domain Adaptation Φs (x) = hx, x, 0i Φt (x) = hx, 0, xi, and 0 is a zero vector of length |x|. This adaptation scheme is attractive because of its simplicity and ease of use as a pre-processing step, and also because it performs quite well despite its simplicity. It has been used in various NLP tasks such http://www.vantagelearning.com/products/intellimetric/ 432 as word segmentation (Monroe et al., 2014), machine translation (Green et al., 2014), word sense disambiguation (Zhong et al., 2008), and short answer scoring (Heilman and Madnani, 2013). Our work is an extension of this scheme in the sense that our work is a generalization of EasyAdapt. 3 number of source domain essays is much larger than the target domain essays. EASE uses NLTK (Bird et al., 2009) for POS tagging and stemming, aspell for spellchecking, and WordNet (Fellbaum, 1998) to get the synonyms. Correct POS tags are generated using a grammatically correct text (provided by EASE). The POS tag sequences not included in the corre"
D15-1049,D14-1190,0,0.0262031,"Missing"
D15-1049,W06-1615,0,0.0417682,"ion system trained on labeled news data might not perform as well on biomedical texts (Jiang and Zhai, 2007). We can solve this problem either by getting labeled data from the other domain, which might not be available, or by performing domain adaptation. Domain adaptation is the task of adapting knowledge learned in a source domain to a target domain. Various approaches to this task have been proposed and used in the context of NLP. Some commonly used approaches include EasyAdapt (Daum´e III, 2007), instance weighting (IW) (Jiang and Zhai, 2007), and structural correspondence learning (SCL) (Blitzer et al., 2006). We can divide the approaches of domain adaptation into two categories based on the availability of labeled target data. The case where a small number of labeled target data is available is usually referred to as supervised domain adaptation (such as EasyAdapt and IW). The case where no labeled target domain data is available is usually referred to as unsupervised domain adaptation (such as SCL). In our work, we focus on supervised domain adaptation. Daum´e III (2007) described a domain adaptation scheme called EasyAdapt which makes use of feature augmentation. Suppose we have a feature vecto"
D15-1049,D13-1180,0,0.52238,"sing (NLP) techniques to extract a smaller number of complex features, such as grammatical error and lexical complexity, and uses them with stepwise linear regression (Attali and Burstein, 2004). Others like (Larkey, 1998) take the classification approach. (Rudner and Liang, 2002) uses Bayesian models for classification and treats AES as a text classification problem. Intelligent Essay Assessor uses Latent Semantic Analysis (LSA) (Landauer et al., 1998) as a measure of semantic similarity between essays. Other recent work uses the preference ranking based approach (Yannakoudakis et al., 2011; Chen and He, 2013). In this paper, we also treat AES as a regression problem, following PEG and e-rater. We use regression because the range of scores of the essays could be very large and a classification approach does not work well in this case. It also allows us to model essay scores as continuous values and scale them easily in the case of different score ranges between the source essay prompt and the target essay prompt. The features used differ among the systems, ranging from simple features (e.g., word length, 2 Domain Adaptation Φs (x) = hx, x, 0i Φt (x) = hx, 0, xi, and 0 is a zero vector of length |x|"
D15-1049,P07-1033,0,0.108702,"Missing"
D15-1049,P11-1019,0,0.198965,"uses natural language processing (NLP) techniques to extract a smaller number of complex features, such as grammatical error and lexical complexity, and uses them with stepwise linear regression (Attali and Burstein, 2004). Others like (Larkey, 1998) take the classification approach. (Rudner and Liang, 2002) uses Bayesian models for classification and treats AES as a text classification problem. Intelligent Essay Assessor uses Latent Semantic Analysis (LSA) (Landauer et al., 1998) as a measure of semantic similarity between essays. Other recent work uses the preference ranking based approach (Yannakoudakis et al., 2011; Chen and He, 2013). In this paper, we also treat AES as a regression problem, following PEG and e-rater. We use regression because the range of scores of the essays could be very large and a classification approach does not work well in this case. It also allows us to model essay scores as continuous values and scale them easily in the case of different score ranges between the source essay prompt and the target essay prompt. The features used differ among the systems, ranging from simple features (e.g., word length, 2 Domain Adaptation Φs (x) = hx, x, 0i Φt (x) = hx, 0, xi, and 0 is a zero"
D15-1049,D08-1105,1,0.864185,"Missing"
D15-1049,W14-3360,0,\N,Missing
D16-1193,P16-1068,0,0.473727,"Missing"
D16-1193,D13-1180,0,0.0851282,"1999) are two notable examples of AES systems. In 2012, a competition on automated essay scoring called ‘Automated Student Assessment Prize’ (ASAP)3 was organized by Kaggle and sponsored by the Hewlett Foundation. A comprehensive comparison of AES systems was made in the ASAP competition. Although many AES systems have been developed to date, they have been built with hand-crafted features and supervised machine learning algorithms. Researchers have devoted a substantial amount of effort to design effective features for automated essay scoring. These features can be as simple as essay length (Chen and He, 2013) or more complicated such as lexical complexity, grammaticality of a text (Attali and Burstein, 2004), or syntactic features (Chen and He, 2013). Readability features (Zesch et al., 2015) have also been proposed in the literature as another source of information. Moreover, text coherence has also been exploited to assess the flow of information and argumentation of an essay (Chen and He, 2013). A detailed overview of the features used in AES systems can be found in (Zesch et al., 2015). Moreover, some attempts have been made to address different aspects of essay writing independently. For exam"
D16-1193,D14-1080,0,0.0132003,"ral network approaches have recently been used successfully in a number of other NLP tasks. For example, Bahdanau et al. (2015) have proposed an attentive neural approach to machine translation based on gated recurrent units (Cho et al., 2014). Neural approaches have also been used for syntactic parsing. In (Vinyals et al., 2015), long short-term memory networks have been used to obtain parse trees by using a sequence-to-sequence model and formulating the parsing task as a sequence generation problem. Apart from these examples, recurrent neural networks have also been used for opinion mining (Irsoy and Cardie, 2014), sequence labeling (Ma and Hovy, 2016), language modeling (Kim et al., 2016; Sundermeyer et al., 2015), etc. 3 Automated Essay Scoring In this section, we define the automated essay scoring task and the evaluation metric used for assessing the quality of AES systems. 3.1 Task Description Automated essay scoring systems are used in evaluating and scoring student essays written based on a given prompt. The performance of these systems is assessed by comparing their scores assigned to a set of essays to human-assigned gold-standard scores. Since the output of AES systems is usually a realvalued"
D16-1193,P16-1101,0,0.00625894,"ed successfully in a number of other NLP tasks. For example, Bahdanau et al. (2015) have proposed an attentive neural approach to machine translation based on gated recurrent units (Cho et al., 2014). Neural approaches have also been used for syntactic parsing. In (Vinyals et al., 2015), long short-term memory networks have been used to obtain parse trees by using a sequence-to-sequence model and formulating the parsing task as a sequence generation problem. Apart from these examples, recurrent neural networks have also been used for opinion mining (Irsoy and Cardie, 2014), sequence labeling (Ma and Hovy, 2016), language modeling (Kim et al., 2016; Sundermeyer et al., 2015), etc. 3 Automated Essay Scoring In this section, we define the automated essay scoring task and the evaluation metric used for assessing the quality of AES systems. 3.1 Task Description Automated essay scoring systems are used in evaluating and scoring student essays written based on a given prompt. The performance of these systems is assessed by comparing their scores assigned to a set of essays to human-assigned gold-standard scores. Since the output of AES systems is usually a realvalued number, the task is often addressed as"
D16-1193,P15-1053,0,0.0261266,"source of information. Moreover, text coherence has also been exploited to assess the flow of information and argumentation of an essay (Chen and He, 2013). A detailed overview of the features used in AES systems can be found in (Zesch et al., 2015). Moreover, some attempts have been made to address different aspects of essay writing independently. For example, argument strength and organization of essays have been tackled by some 2 3 https://github.com/nusnlp/nea https://www.kaggle.com/c/asap-aes 1883 researchers through designing task-specific features for each aspect (Persing et al., 2010; Persing and Ng, 2015). Our system, however, accepts an essay text as input directly and learns the features automatically from the data. To do so, we have developed a method based on recurrent neural networks to score the essays in an end-to-end manner. We have explored a variety of neural network models in this paper to identify the most suitable model. Our best model is a long short-term memory neural network (Hochreiter and Schmidhuber, 1997) and is trained as a regression method. Similar recurrent neural network approaches have recently been used successfully in a number of other NLP tasks. For example, Bahdan"
D16-1193,D10-1023,0,0.387956,"literature as another source of information. Moreover, text coherence has also been exploited to assess the flow of information and argumentation of an essay (Chen and He, 2013). A detailed overview of the features used in AES systems can be found in (Zesch et al., 2015). Moreover, some attempts have been made to address different aspects of essay writing independently. For example, argument strength and organization of essays have been tackled by some 2 3 https://github.com/nusnlp/nea https://www.kaggle.com/c/asap-aes 1883 researchers through designing task-specific features for each aspect (Persing et al., 2010; Persing and Ng, 2015). Our system, however, accepts an essay text as input directly and learns the features automatically from the data. To do so, we have developed a method based on recurrent neural networks to score the essays in an end-to-end manner. We have explored a variety of neural network models in this paper to identify the most suitable model. Our best model is a long short-term memory neural network (Hochreiter and Schmidhuber, 1997) and is trained as a regression method. Similar recurrent neural network approaches have recently been used successfully in a number of other NLP tas"
D16-1193,D15-1049,1,0.333002,"es are summarized below: • Convolutional vs. recurrent neural network • RNN unit type (basic RNN, GRU, or LSTM) • Using mean-over-time over all recurrent states vs. using only the last recurrent state • Length-based features • Parts-of-Speech (POS) • Using mean-over-time vs. an attention mechanism • Word overlap with the prompt • Bag of n-grams After extracting the features, a regression algorithm is used to build a model based on the training data. The details of the features and the results of using support vector regression (SVR) and Bayesian linear ridge regression (BLRR) are reported in (Phandi et al., 2015). We use these two regression methods as our baseline systems. Our system has several hyper-parameters that need to be set. We use the RMSProp optimizer with decay rate (ρ) set to 0.9 to train the network and we set the base learning rate to 0.001. The mini-batch size is 32 in our experiments7 and we train the network for 50 epochs. The vocabulary is the 4,000 most frequent words in the training data and all other words are mapped to a special token that represents unknown words. We regularize the network by using dropout (Srivastava et al., 2014) and we set the dropout probability to 0.5. Dur"
D16-1193,W15-0625,0,0.0397276,"on a given prompt. The performance of these systems is assessed by comparing their scores assigned to a set of essays to human-assigned gold-standard scores. Since the output of AES systems is usually a realvalued number, the task is often addressed as a supervised machine learning task (mostly by regression or preference ranking). Machine learning algorithms are used to learn the relationship between the essays and reference scores. 3.2 Evaluation Metric The output of an AES system can be compared to the ratings assigned by human annotators using various measures of correlation or agreement (Yannakoudakis and Cummins, 2015). These measures include Pearson’s correlation, Spearman’s correlation, Kendall’s Tau, and quadratic weighted Kappa (QWK). The ASAP competition adopted QWK as the official evaluation metric. Since we use the ASAP data set for evaluation in this paper, we also use QWK as the evaluation metric in our experiments. Quadratic weighted Kappa is calculated as follows. First, a weight matrix W is constructed according to Equation 1: Wi,j = (i − j)2 (N − 1)2 (1) where i and j are the reference rating (assigned by a human annotator) and the hypothesis rating (assigned by an AES system), respectively, an"
D16-1193,W15-0626,0,0.18115,"by the Hewlett Foundation. A comprehensive comparison of AES systems was made in the ASAP competition. Although many AES systems have been developed to date, they have been built with hand-crafted features and supervised machine learning algorithms. Researchers have devoted a substantial amount of effort to design effective features for automated essay scoring. These features can be as simple as essay length (Chen and He, 2013) or more complicated such as lexical complexity, grammaticality of a text (Attali and Burstein, 2004), or syntactic features (Chen and He, 2013). Readability features (Zesch et al., 2015) have also been proposed in the literature as another source of information. Moreover, text coherence has also been exploited to assess the flow of information and argumentation of an essay (Chen and He, 2013). A detailed overview of the features used in AES systems can be found in (Zesch et al., 2015). Moreover, some attempts have been made to address different aspects of essay writing independently. For example, argument strength and organization of essays have been tackled by some 2 3 https://github.com/nusnlp/nea https://www.kaggle.com/c/asap-aes 1883 researchers through designing task-spe"
D16-1193,D13-1141,0,0.014255,"and all other words are mapped to a special token that represents unknown words. We regularize the network by using dropout (Srivastava et al., 2014) and we set the dropout probability to 0.5. During training, the norm of the gradient is clipped to a maximum value of 10. We set the word embedding dimension (dLT ) to 50 and the output dimension of the recurrent layer (dr ) to 300. If a convolution layer is used, the window size (l) is set to 3 and the output dimension of this layer (dc ) is set to 50. Finally, we initialize the lookup table layer using pre-trained word embeddings8 released by Zou et al. (2013). Moreover, the bias value of the linear layer is initialized such that the network’s output before training is almost equal to the average score in the training data. 7 To create mini-batches for training, we pad all essays in a mini-batch using a dummy token to make them have the same length. To eliminate the effect of padding tokens during training, we mask them to prevent the network from miscalculating the gradients. 8 http://ai.stanford.edu/∼wzou/mt 1887 • Using a recurrent layer vs. a convolutional recurrent layer • Unidirectional vs. bidirectional LSTM We have used 8 Tesla K80 GPUs to"
D16-1195,W14-1603,0,0.0188425,"the world. The popularity of this problem grew further through Helping Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). The majority of the published work on GEC aimed at building classifiers or rule-based systems 1902 for specific error types and combined them to build hybrid systems (Dahlmeier et al., 2012; Rozovskaya et al., 2014). The cross-linguistic influences between L1 and L2 have been mainly used for the task of native language identification (Massung and Zhai, 2016). It has also been used in typology prediction (Berzak et al., 2014) and predicting error distributions in ESL data (Berzak et al., 2015). L1-based adaptation has previously shown to improve GEC for specific error types using the classification approach. Rozovskaya and Roth (2010) used an approach to correct preposition errors by restricting the candidate corrections to those observed in L1-specific data. They further added artificial training data that mimic the error frequency in L1-specific text to improve accuracy. In their later work, Rozovskaya and Roth (2011) focused on L1-based adaptation for preposition and article correction, by modifying the prior p"
D16-1195,K15-1010,0,0.0417155,"ng Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). The majority of the published work on GEC aimed at building classifiers or rule-based systems 1902 for specific error types and combined them to build hybrid systems (Dahlmeier et al., 2012; Rozovskaya et al., 2014). The cross-linguistic influences between L1 and L2 have been mainly used for the task of native language identification (Massung and Zhai, 2016). It has also been used in typology prediction (Berzak et al., 2014) and predicting error distributions in ESL data (Berzak et al., 2015). L1-based adaptation has previously shown to improve GEC for specific error types using the classification approach. Rozovskaya and Roth (2010) used an approach to correct preposition errors by restricting the candidate corrections to those observed in L1-specific data. They further added artificial training data that mimic the error frequency in L1-specific text to improve accuracy. In their later work, Rozovskaya and Roth (2011) focused on L1-based adaptation for preposition and article correction, by modifying the prior probabilities in the na¨ıve Bayes classifier during decision time base"
D16-1195,P15-1068,1,0.827493,"th (2016) (classifiers + spelling + SMT) (SMT) Chollampatt et al. (2016) (SMT) Shared Task Teams CAMB (classifiers, rules, SMT) CUUI (classifiers) AMU (SMT) 40 39 38 37 36 Figure L1: Chinese 1: Effect L1: Russian of L1: Spanish regularization for SC ONCAT + NNJMA DAPTED (FCE) data compared to L1 Chinese. 7.6 Evaluation on Benchmark Dataset We also evaluate our system on the benchmark CoNLL-2014 shared task (Ng et al., 2014) test set for GEC in English. The CoNLL-2014 shared task consists of 1,312 sentences with two annotators. We also perform evaluation on the extension of CoNLL2014 test set (Bryant and Ng, 2015), which contains eight additional sets of annotations over the two sets of annotations provided in the original test set. Following the settings of the CoNLL-2014 shared task, we tune our unadapted baseline system and the L1adapted systems on the CoNLL-2013 shared task test set consisting of 1,381 test sentences. The results are summarized in Table 5. We find that only the systems adapted based on L1 Chinese improves over the unadapted baseline system (SC ONCAT + NNJMBASELINE ). When the smallersized, high-quality FCE data is used for adaptation the margin of improvement is higher. This could"
D16-1195,P11-2031,0,0.0364142,": Chinese D EV T EST L1: Russian D EV T EST D EV T EST Table 3: Statistics of the FCE dataset for each L1 For evaluation, we use the F0.5 measure, computed by the M2 scorer v3.2 (Dahlmeier and Ng, 2012), as our evaluation metric. The error annotations in FCE are converted to the format required by the M2 scorer. The statistics of error annotations after converting to this format are given in Table 3. To deal with the instability of parameter tuning in SMT, we perform five runs of tuning and calculate the statistical significance by stratified approximate randomization test, as recommended by (Clark et al., 2011). 7 Table 2: Statistics of L1-specific data in Lang-8 We use the publicly available CLC-FCE (Yannakoudakis et al., 2011) corpus to obtain the de1 #sents #scripts L1: Spanish We obtain L1-specific in-domain data for adaptation based on the L1 information provided in Lang8. Adaptation is performed on English texts written by learners of three different L1 backgrounds: Chinese, Russian, and Spanish. The statistics of the in-domain data from Lang-8 for each L1 are given in Table 2. For each L1, its out-of-domain data are obtained by excluding the L1-specific in-domain data (from Table 2) from the"
D16-1195,D11-1010,1,0.859926,"s by restricting the candidate corrections to those observed in L1-specific data. They further added artificial training data that mimic the error frequency in L1-specific text to improve accuracy. In their later work, Rozovskaya and Roth (2011) focused on L1-based adaptation for preposition and article correction, by modifying the prior probabilities in the na¨ıve Bayes classifier during decision time based on L1-specific ESL learner text. Both approaches use native data for training, but rely on non-native L1-specific text to introduce artificial errors or to modify the prior probabilities. Dahlmeier and Ng (2011) implemented a system to correct collocation errors, by adding paraphrases derived from L1 into the confusion set. Specifically, they use a bilingual L1-L2 corpus, to obtain L2 paraphrases, which are likely to be translated to the same phrase in L1. There is no prior work on L1-based adaptation for GEC using the machine translation approach, which is a one of the most popular approaches for GEC. With the availability of large-scale error corrected data (Mizumoto et al., 2011), the statistical machine translation (SMT) approach to GEC became popular and was employed in state-of-the-art GEC syst"
D16-1195,N12-1067,1,0.92332,"or generating hypotheses, is trained using parallel data, i.e., learner-written sentences (source data) and their corresponding corrected sentences (target data). It also scores the hypotheses using features like forward and inverse phrase translation probabilities and lexical weights. The LM is trained on well-formed text and ensures the fluency of the corrected output. The feature weights µi are computed by minimum error rate training (MERT), optimizing the F0.5 measure (JunczysDowmunt and Grundkiewicz, 2014) using a devel1903 opment set. The F0.5 measure computed using the MaxMatch scorer (Dahlmeier and Ng, 2012) is the standard evaluation metric for GEC used in the CoNLL-2014 shared task (Ng et al., 2014), weighting precision twice as much as recall. Apart from the TM and the n-gram LM, we add a neural network joint model (NNJM) (Devlin et al., 2014) as a feature, following Chollampatt et al. (2016), who reported that NNJM improves the performance of a state-of-the-art SMT-based GEC system. Unlike Recurrent Neural Networks (RNNs) and Long Short Term Memory networks (LSTMs), NNJMs have a feed-forward architecture which relies on a fixed context. This makes it easy to integrate NNJMs into a machine tra"
D16-1195,W13-1703,1,0.890465,"ll sizes to fine tune a general NNJM. (3) Their method requires retraining of the model on complete training data in order to adapt to each domain. On the contrary, our method can adapt a single general model to different domains using small in-domain data, leading to a considerable reduction in training time. We re-implement their method by incorporating this regularization term into the log likelihood objective function with self-normalization, L (Equation 2), during adaptive training. 6 Data and Evaluation The training data consist of two corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and the Lang-8 Learner Corpora v2 (Mizumoto et al., 2011). We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py1 (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy sourcetarget sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2. A sentence pair where the source or target sentence has more than 80 words is also removed from both NUCLE and Lang-8. The statistics of the data after"
D16-1195,W11-2838,0,0.17131,"wo major contributions of this paper are as follows. (1) This is the first work that performs L1-based adaptation for GEC using the SMT approach and covering all error types. (2) We introduce a novel method of NNJM adaptation and demonstrate that this method can work with indomain data that are much smaller than the general domain data. 2 Related Work In the past decade, there has been increasing attention on GEC in English, mainly due to the growing number of English as second language (ESL) learners around the world. The popularity of this problem grew further through Helping Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). The majority of the published work on GEC aimed at building classifiers or rule-based systems 1902 for specific error types and combined them to build hybrid systems (Dahlmeier et al., 2012; Rozovskaya et al., 2014). The cross-linguistic influences between L1 and L2 have been mainly used for the task of native language identification (Massung and Zhai, 2016). It has also been used in typology prediction (Berzak et al., 2014) and predicting error distributions in ESL data (Berzak et al., 2015). L1-based adaptation h"
D16-1195,W12-2006,0,0.189321,"his paper are as follows. (1) This is the first work that performs L1-based adaptation for GEC using the SMT approach and covering all error types. (2) We introduce a novel method of NNJM adaptation and demonstrate that this method can work with indomain data that are much smaller than the general domain data. 2 Related Work In the past decade, there has been increasing attention on GEC in English, mainly due to the growing number of English as second language (ESL) learners around the world. The popularity of this problem grew further through Helping Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). The majority of the published work on GEC aimed at building classifiers or rule-based systems 1902 for specific error types and combined them to build hybrid systems (Dahlmeier et al., 2012; Rozovskaya et al., 2014). The cross-linguistic influences between L1 and L2 have been mainly used for the task of native language identification (Massung and Zhai, 2016). It has also been used in typology prediction (Berzak et al., 2014) and predicting error distributions in ESL data (Berzak et al., 2015). L1-based adaptation has previously shown"
D16-1195,P14-1129,0,0.331002,"h has been used in state-ofthe-art GEC systems (Rozovskaya and Roth, 2016; 1901 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1901–1911, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Chollampatt et al., 2016; Hoang et al., 2016). The SMT approach does not model error types specifically, nor does it require linguistic analysis like parsing and part-of-speech (POS) tagging. We adopt a phrase-based SMT approach to GEC in this paper. Additionally, we implement and incorporate a neural network joint model (NNJM) (Devlin et al., 2014) as a feature in our SMT-based GEC system. It is easy to integrate an NNJM into the SMT decoding framework as it uses a fixed-window context and it has shown to improve SMT-based GEC (Chollampatt et al., 2016). We adapt the NNJM to L1specific data (i.e., English text written by writers of a particular L1) and obtain significant improvements over the baseline which uses an unadapted NNJM. Adaptation is done by using the unadapted NNJM trained on general domain data (i.e., not L1-specific) using a log likelihood objective function with selfnormalization (Devlin et al., 2014) as the starting poin"
D16-1195,W07-0717,0,0.036978,"nd neural network joint models (NNJMs) by adapting an NNJM based on the L1 background of the writers and integrating it into the SMT framework. We perform KL divergence regularized adaptation to prevent overfitting on the smaller in-domain data. KL divergence regularization was previously used by Yu et al. (2013) for speaker adaptation. Joty et al. (2015) proposed another NNJM adaptation method, which uses a regularized objective function that encourages a network trained on general-domain data to be closer to an indomain NNJM. Other adaptation techniques used in SMT include mixture modeling (Foster and Kuhn, 2007; Moore and Lewis, 2010; Sennrich, 2012) and alternative decoding paths (Koehn and Schroeder, 2007). 3 A Machine Translation Framework for Grammatical Error Correction We formulate GEC as a translation task from a possibly erroneous input sentence to a corrected sentence. We use the popular phrase-based SMT system, Moses (Koehn et al., 2007), which employs a log linear model to find the best correction hypothesis T ∗ given an input sentence S: ∗ T = argmax P (T |S) = argmax T T N X µi fi (T, S) i=1 where µi and fi (T, S) are the ith feature weight and feature function, respectively. We use the"
D16-1195,P13-2121,0,0.0665686,"Missing"
D16-1195,D15-1147,0,0.0446354,"Missing"
D16-1195,W14-1703,0,0.134303,"riting, and considering this factor can potentially improve the performance of GEC systems. For example, consider the following sentence written by a Finnish writer (Jarvis and Odlin, 2000): “When they had escaped in the police car they sat under the tree.” The preposition in appears to be grammatically correct. However, in the given context, the preposition ‘from’ is The two most popular approaches for grammatical error correction are the classification approach (Dahlmeier et al., 2012; Rozovskaya et al., 2014) and the statistical machine translation (SMT) approach (Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2014). The SMT approach has emerged as a popular paradigm for GEC because of its ability to learn text transformations from illformed to well-formed text enabling it to correct a wide variety of errors including complex errors that are difficult to handle for the classification approach (Rozovskaya and Roth, 2016). The phrasebased SMT approach has been used in state-ofthe-art GEC systems (Rozovskaya and Roth, 2016; 1901 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1901–1911, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Ling"
D16-1195,W07-0733,0,0.0452714,"riters and integrating it into the SMT framework. We perform KL divergence regularized adaptation to prevent overfitting on the smaller in-domain data. KL divergence regularization was previously used by Yu et al. (2013) for speaker adaptation. Joty et al. (2015) proposed another NNJM adaptation method, which uses a regularized objective function that encourages a network trained on general-domain data to be closer to an indomain NNJM. Other adaptation techniques used in SMT include mixture modeling (Foster and Kuhn, 2007; Moore and Lewis, 2010; Sennrich, 2012) and alternative decoding paths (Koehn and Schroeder, 2007). 3 A Machine Translation Framework for Grammatical Error Correction We formulate GEC as a translation task from a possibly erroneous input sentence to a corrected sentence. We use the popular phrase-based SMT system, Moses (Koehn et al., 2007), which employs a log linear model to find the best correction hypothesis T ∗ given an input sentence S: ∗ T = argmax P (T |S) = argmax T T N X µi fi (T, S) i=1 where µi and fi (T, S) are the ith feature weight and feature function, respectively. We use the standard features in Moses, without any re-ordering models. The two main components of an SMT syst"
D16-1195,I11-1062,0,0.0165947,"ral model to different domains using small in-domain data, leading to a considerable reduction in training time. We re-implement their method by incorporating this regularization term into the log likelihood objective function with self-normalization, L (Equation 2), during adaptive training. 6 Data and Evaluation The training data consist of two corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and the Lang-8 Learner Corpora v2 (Mizumoto et al., 2011). We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py1 (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy sourcetarget sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2. A sentence pair where the source or target sentence has more than 80 words is also removed from both NUCLE and Lang-8. The statistics of the data after pre-processing are shown in Table 1. Corpus #sents #src tokens #tgt tokens NUCLE L ANG -8 57,063 2,048,654 1,156,460 24,649,768 1,151,278 25,912,707 C ONCAT 2,105,717 25,806,228 27,063,985 Table 1:"
D16-1195,I11-1017,0,0.671699,"training, but rely on non-native L1-specific text to introduce artificial errors or to modify the prior probabilities. Dahlmeier and Ng (2011) implemented a system to correct collocation errors, by adding paraphrases derived from L1 into the confusion set. Specifically, they use a bilingual L1-L2 corpus, to obtain L2 paraphrases, which are likely to be translated to the same phrase in L1. There is no prior work on L1-based adaptation for GEC using the machine translation approach, which is a one of the most popular approaches for GEC. With the availability of large-scale error corrected data (Mizumoto et al., 2011), the statistical machine translation (SMT) approach to GEC became popular and was employed in state-of-the-art GEC systems. Comparison of the classification approach and the machine translation approach can be found in (Rozovskaya and Roth, 2016) and (Susanto et al., 2014). Recently, an end-to-end neural machine translation framework was proposed for GEC (Yuan and Briscoe, 2016), which was shown to achieve competitive results. Neural network joint models have shown to be improve SMT-based GEC systems (Chollampatt et al., 2016) due to their ability to model words and phrases in a continuous sp"
D16-1195,P10-2041,0,0.0194463,"models (NNJMs) by adapting an NNJM based on the L1 background of the writers and integrating it into the SMT framework. We perform KL divergence regularized adaptation to prevent overfitting on the smaller in-domain data. KL divergence regularization was previously used by Yu et al. (2013) for speaker adaptation. Joty et al. (2015) proposed another NNJM adaptation method, which uses a regularized objective function that encourages a network trained on general-domain data to be closer to an indomain NNJM. Other adaptation techniques used in SMT include mixture modeling (Foster and Kuhn, 2007; Moore and Lewis, 2010; Sennrich, 2012) and alternative decoding paths (Koehn and Schroeder, 2007). 3 A Machine Translation Framework for Grammatical Error Correction We formulate GEC as a translation task from a possibly erroneous input sentence to a corrected sentence. We use the popular phrase-based SMT system, Moses (Koehn et al., 2007), which employs a log linear model to find the best correction hypothesis T ∗ given an input sentence S: ∗ T = argmax P (T |S) = argmax T T N X µi fi (T, S) i=1 where µi and fi (T, S) are the ith feature weight and feature function, respectively. We use the standard features in M"
D16-1195,W13-3601,1,0.893184,"irst work that performs L1-based adaptation for GEC using the SMT approach and covering all error types. (2) We introduce a novel method of NNJM adaptation and demonstrate that this method can work with indomain data that are much smaller than the general domain data. 2 Related Work In the past decade, there has been increasing attention on GEC in English, mainly due to the growing number of English as second language (ESL) learners around the world. The popularity of this problem grew further through Helping Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). The majority of the published work on GEC aimed at building classifiers or rule-based systems 1902 for specific error types and combined them to build hybrid systems (Dahlmeier et al., 2012; Rozovskaya et al., 2014). The cross-linguistic influences between L1 and L2 have been mainly used for the task of native language identification (Massung and Zhai, 2016). It has also been used in typology prediction (Berzak et al., 2014) and predicting error distributions in ESL data (Berzak et al., 2015). L1-based adaptation has previously shown to improve GEC for specific error types"
D16-1195,W14-1701,1,0.945572,"rforms L1-based adaptation for GEC using the SMT approach and covering all error types. (2) We introduce a novel method of NNJM adaptation and demonstrate that this method can work with indomain data that are much smaller than the general domain data. 2 Related Work In the past decade, there has been increasing attention on GEC in English, mainly due to the growing number of English as second language (ESL) learners around the world. The popularity of this problem grew further through Helping Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). The majority of the published work on GEC aimed at building classifiers or rule-based systems 1902 for specific error types and combined them to build hybrid systems (Dahlmeier et al., 2012; Rozovskaya et al., 2014). The cross-linguistic influences between L1 and L2 have been mainly used for the task of native language identification (Massung and Zhai, 2016). It has also been used in typology prediction (Berzak et al., 2014) and predicting error distributions in ESL data (Berzak et al., 2015). L1-based adaptation has previously shown to improve GEC for specific error types using the classifi"
D16-1195,D10-1094,0,0.0539949,"the published work on GEC aimed at building classifiers or rule-based systems 1902 for specific error types and combined them to build hybrid systems (Dahlmeier et al., 2012; Rozovskaya et al., 2014). The cross-linguistic influences between L1 and L2 have been mainly used for the task of native language identification (Massung and Zhai, 2016). It has also been used in typology prediction (Berzak et al., 2014) and predicting error distributions in ESL data (Berzak et al., 2015). L1-based adaptation has previously shown to improve GEC for specific error types using the classification approach. Rozovskaya and Roth (2010) used an approach to correct preposition errors by restricting the candidate corrections to those observed in L1-specific data. They further added artificial training data that mimic the error frequency in L1-specific text to improve accuracy. In their later work, Rozovskaya and Roth (2011) focused on L1-based adaptation for preposition and article correction, by modifying the prior probabilities in the na¨ıve Bayes classifier during decision time based on L1-specific ESL learner text. Both approaches use native data for training, but rely on non-native L1-specific text to introduce artificial"
D16-1195,P11-1093,0,0.153784,"ative language identification (Massung and Zhai, 2016). It has also been used in typology prediction (Berzak et al., 2014) and predicting error distributions in ESL data (Berzak et al., 2015). L1-based adaptation has previously shown to improve GEC for specific error types using the classification approach. Rozovskaya and Roth (2010) used an approach to correct preposition errors by restricting the candidate corrections to those observed in L1-specific data. They further added artificial training data that mimic the error frequency in L1-specific text to improve accuracy. In their later work, Rozovskaya and Roth (2011) focused on L1-based adaptation for preposition and article correction, by modifying the prior probabilities in the na¨ıve Bayes classifier during decision time based on L1-specific ESL learner text. Both approaches use native data for training, but rely on non-native L1-specific text to introduce artificial errors or to modify the prior probabilities. Dahlmeier and Ng (2011) implemented a system to correct collocation errors, by adding paraphrases derived from L1 into the confusion set. Specifically, they use a bilingual L1-L2 corpus, to obtain L2 paraphrases, which are likely to be translate"
D16-1195,P16-1208,0,0.736762,"in the given context, the preposition ‘from’ is The two most popular approaches for grammatical error correction are the classification approach (Dahlmeier et al., 2012; Rozovskaya et al., 2014) and the statistical machine translation (SMT) approach (Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2014). The SMT approach has emerged as a popular paradigm for GEC because of its ability to learn text transformations from illformed to well-formed text enabling it to correct a wide variety of errors including complex errors that are difficult to handle for the classification approach (Rozovskaya and Roth, 2016). The phrasebased SMT approach has been used in state-ofthe-art GEC systems (Rozovskaya and Roth, 2016; 1901 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1901–1911, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Chollampatt et al., 2016; Hoang et al., 2016). The SMT approach does not model error types specifically, nor does it require linguistic analysis like parsing and part-of-speech (POS) tagging. We adopt a phrase-based SMT approach to GEC in this paper. Additionally, we implement and incorporate a neural"
D16-1195,W14-1704,0,0.672745,"guage (L1) background of the writer has a noticeable influence on the errors made in second language (L2) writing, and considering this factor can potentially improve the performance of GEC systems. For example, consider the following sentence written by a Finnish writer (Jarvis and Odlin, 2000): “When they had escaped in the police car they sat under the tree.” The preposition in appears to be grammatically correct. However, in the given context, the preposition ‘from’ is The two most popular approaches for grammatical error correction are the classification approach (Dahlmeier et al., 2012; Rozovskaya et al., 2014) and the statistical machine translation (SMT) approach (Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2014). The SMT approach has emerged as a popular paradigm for GEC because of its ability to learn text transformations from illformed to well-formed text enabling it to correct a wide variety of errors including complex errors that are difficult to handle for the classification approach (Rozovskaya and Roth, 2016). The phrasebased SMT approach has been used in state-ofthe-art GEC systems (Rozovskaya and Roth, 2016; 1901 Proceedings of the 2016 Conference on Empirical Methods in"
D16-1195,E12-1055,0,0.362217,"ting an NNJM based on the L1 background of the writers and integrating it into the SMT framework. We perform KL divergence regularized adaptation to prevent overfitting on the smaller in-domain data. KL divergence regularization was previously used by Yu et al. (2013) for speaker adaptation. Joty et al. (2015) proposed another NNJM adaptation method, which uses a regularized objective function that encourages a network trained on general-domain data to be closer to an indomain NNJM. Other adaptation techniques used in SMT include mixture modeling (Foster and Kuhn, 2007; Moore and Lewis, 2010; Sennrich, 2012) and alternative decoding paths (Koehn and Schroeder, 2007). 3 A Machine Translation Framework for Grammatical Error Correction We formulate GEC as a translation task from a possibly erroneous input sentence to a corrected sentence. We use the popular phrase-based SMT system, Moses (Koehn et al., 2007), which employs a log linear model to find the best correction hypothesis T ∗ given an input sentence S: ∗ T = argmax P (T |S) = argmax T T N X µi fi (T, S) i=1 where µi and fi (T, S) are the ith feature weight and feature function, respectively. We use the standard features in Moses, without any"
D16-1195,D14-1102,1,0.884815,"use a bilingual L1-L2 corpus, to obtain L2 paraphrases, which are likely to be translated to the same phrase in L1. There is no prior work on L1-based adaptation for GEC using the machine translation approach, which is a one of the most popular approaches for GEC. With the availability of large-scale error corrected data (Mizumoto et al., 2011), the statistical machine translation (SMT) approach to GEC became popular and was employed in state-of-the-art GEC systems. Comparison of the classification approach and the machine translation approach can be found in (Rozovskaya and Roth, 2016) and (Susanto et al., 2014). Recently, an end-to-end neural machine translation framework was proposed for GEC (Yuan and Briscoe, 2016), which was shown to achieve competitive results. Neural network joint models have shown to be improve SMT-based GEC systems (Chollampatt et al., 2016) due to their ability to model words and phrases in a continuous space, access to larger contexts from source side, and ability to learn non-linear mappings from input to output. In this paper, we exploit the advantages of the SMT approach and neural network joint models (NNJMs) by adapting an NNJM based on the L1 background of the writers"
D16-1195,P11-1019,0,0.299707,"aluation, we use the F0.5 measure, computed by the M2 scorer v3.2 (Dahlmeier and Ng, 2012), as our evaluation metric. The error annotations in FCE are converted to the format required by the M2 scorer. The statistics of error annotations after converting to this format are given in Table 3. To deal with the instability of parameter tuning in SMT, we perform five runs of tuning and calculate the statistical significance by stratified approximate randomization test, as recommended by (Clark et al., 2011). 7 Table 2: Statistics of L1-specific data in Lang-8 We use the publicly available CLC-FCE (Yannakoudakis et al., 2011) corpus to obtain the de1 #sents #scripts L1: Spanish We obtain L1-specific in-domain data for adaptation based on the L1 information provided in Lang8. Adaptation is performed on English texts written by learners of three different L1 backgrounds: Chinese, Russian, and Spanish. The statistics of the in-domain data from Lang-8 for each L1 are given in Table 2. For each L1, its out-of-domain data are obtained by excluding the L1-specific in-domain data (from Table 2) from the combined training data (C ONCAT). L1 velopment and test data. The FCE corpus contains 1,244 scripts written by 1,244 dis"
D16-1195,N16-1042,0,0.163668,"ase in L1. There is no prior work on L1-based adaptation for GEC using the machine translation approach, which is a one of the most popular approaches for GEC. With the availability of large-scale error corrected data (Mizumoto et al., 2011), the statistical machine translation (SMT) approach to GEC became popular and was employed in state-of-the-art GEC systems. Comparison of the classification approach and the machine translation approach can be found in (Rozovskaya and Roth, 2016) and (Susanto et al., 2014). Recently, an end-to-end neural machine translation framework was proposed for GEC (Yuan and Briscoe, 2016), which was shown to achieve competitive results. Neural network joint models have shown to be improve SMT-based GEC systems (Chollampatt et al., 2016) due to their ability to model words and phrases in a continuous space, access to larger contexts from source side, and ability to learn non-linear mappings from input to output. In this paper, we exploit the advantages of the SMT approach and neural network joint models (NNJMs) by adapting an NNJM based on the L1 background of the writers and integrating it into the SMT framework. We perform KL divergence regularized adaptation to prevent overf"
D16-1195,P07-2045,0,\N,Missing
D18-1274,I17-2058,0,0.0157858,"e contributions of this paper are: (1) we propose the first supervised approach to QE of GEC system outputs, (2) we present neural QE models that outperform a strong feature-based baseline for estimating post-editing effort and an automatic GEC evaluation metric, (3) we propose new convolutional neural architectures for QE that can be potentially utilized for QE tasks in other language applications, and (4) we show that the performance of a state-of-the-art GEC system can be improved by adding QE scores as features in re-ranking the N-best candidates. outputs independently for grammaticality. Asano et al. (2017) improved their method to account for fluency as well as faithfulness to the source sentence. Choshen and Abend (2018b) provide another measurement for meaning preservation using a semantic annotation scheme. Contrary to prior work in GEC reference-less evaluation, our work is aimed at estimating post-editing effort in terms of translation error rate (Snover et al., 2006) and an automatic evaluation metric, MaxMatch (Dahlmeier and Ng, 2012), in a supervised approach. We propose variants of the predictorestimator architecture (Kim and Lee, 2016b; Kim et al., 2017a) and compare them to a competi"
D18-1274,C04-1046,0,0.0572716,"rate (Snover et al., 2006) and an automatic evaluation metric, MaxMatch (Dahlmeier and Ng, 2012), in a supervised approach. We propose variants of the predictorestimator architecture (Kim and Lee, 2016b; Kim et al., 2017a) and compare them to a competitive feature-based baseline, QuEst (Specia et al., 2013, 2015) that has been successfully used for a number of language pairs in MT QE and for other applications (Stewart et al., 2018). It has also been the baseline for WMT QE tasks. 3 2 Related Work The task of quality estimation became popular in machine translation (MT) through the studies by Blatz et al. (2004) and Specia et al. (2009). Much of the later work in QE of MT was through the shared tasks in Workshop on Machine Translation (WMT) campaigns (Bojar et al., 2016b) from 2012 onwards (Callison-Burch et al., 2012). Supervised methods of quality assessment have been applied to other natural language processing tasks such as ˇ text simplification (Stajner et al., 2016), language generation (Duˇsek et al., 2017), and in assisting interpreters (Stewart et al., 2018). In the context of GEC, Heilman et al. (2014) attempted to predict grammaticality of learner sentences using regression with a variety"
D18-1274,buck-etal-2014-n,0,0.0385103,"Missing"
D18-1274,W17-5037,1,0.855469,"tem (Base GEC) in terms of document-level F0.5 computed by M2 scorer on the FCE and CoNLL-2014 test sets is reported in Table 4. When we use the development set used by Chollampatt and Ng (2018) consisting of 5.4k sentences from NUCLE to train the re-scorer, Base GEC system achieves a competitive performance compared to the top GEC systems with the best-published results on CoNLL-2014 test set: G&J (Grundkiewicz and Junczys-Dowmunt, 2018), JGGH (Junczys-Dowmunt et al., 2018), and C&N (Chollampatt and Ng, 2018). When we make use of the spelling error correction system (+SpellCheck) proposed by Chollampatt and Ng (2017), which is also used by G&J and C&N, our baseline achieves the highest reported F0.5 on the CoNLL-2014 test set (56.43) when trained on public corpora alone. To the Base GEC system, we add the sentencelevel M2 scores estimated by the final NQE model (NQE A LL ) as a feature in the re-scorer. Since our NQE models use NUCLE during training, we use our development set consisting of 3.6k sentences from FCE and CoNLL-2013 to re-train the rescorer instead of sentences from NUCLE so that the feature weights will not be biased. We observe a slight drop in performance upon retraining, potentially due t"
D18-1274,P18-1059,0,0.305823,"get better corrective feedback if needed. In this paper, we propose a neural approach to automatic quality estimation of GEC output. Quality of language output applications can refer to several aspects such as fluency, grammaticality, adequacy, and post-editing effort. While reference-based metrics such as MaxMatch or M2 (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2016a, 2015) are used to evaluate GEC systems with human-annotated references, a few reference-less GEC metrics have been proposed to evaluate fluency, grammaticality, and adequacy (Napoles et al., 2016b; Asano et al., 2017; Choshen and Abend, 2018b). However, there has been no work in GEC addressing the estimation of post-editing effort. Also, to our knowledge, this is the first supervised approach to quality estimation (QE) for GEC system outputs, similar to the supervised QE task in machine translation (MT) (Specia et al., 2009). Our neural models for GEC QE are 2528 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2528–2539 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics based on variants of the predictor-estimator architecture (Kim et al"
D18-1274,N18-2020,0,0.238598,"get better corrective feedback if needed. In this paper, we propose a neural approach to automatic quality estimation of GEC output. Quality of language output applications can refer to several aspects such as fluency, grammaticality, adequacy, and post-editing effort. While reference-based metrics such as MaxMatch or M2 (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2016a, 2015) are used to evaluate GEC systems with human-annotated references, a few reference-less GEC metrics have been proposed to evaluate fluency, grammaticality, and adequacy (Napoles et al., 2016b; Asano et al., 2017; Choshen and Abend, 2018b). However, there has been no work in GEC addressing the estimation of post-editing effort. Also, to our knowledge, this is the first supervised approach to quality estimation (QE) for GEC system outputs, similar to the supervised QE task in machine translation (MT) (Specia et al., 2009). Our neural models for GEC QE are 2528 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2528–2539 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics based on variants of the predictor-estimator architecture (Kim et al"
D18-1274,N12-1067,1,0.939428,"nificantly make the process of post-editing easier and faster. Such quality estimates can also directly help end users — the language learners — to decide on the extent to which the system’s corrections can be trusted and seek assistance from instructors and other sources to get better corrective feedback if needed. In this paper, we propose a neural approach to automatic quality estimation of GEC output. Quality of language output applications can refer to several aspects such as fluency, grammaticality, adequacy, and post-editing effort. While reference-based metrics such as MaxMatch or M2 (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2016a, 2015) are used to evaluate GEC systems with human-annotated references, a few reference-less GEC metrics have been proposed to evaluate fluency, grammaticality, and adequacy (Napoles et al., 2016b; Asano et al., 2017; Choshen and Abend, 2018b). However, there has been no work in GEC addressing the estimation of post-editing effort. Also, to our knowledge, this is the first supervised approach to quality estimation (QE) for GEC system outputs, similar to the supervised QE task in machine translation (MT) (Specia et al., 2009). Our neural models for GEC QE are 2"
D18-1274,W13-1703,1,0.947554,"t al., 2006). HTER is the minimum number of edit operations (insertions, deletions, substitutions, or shifts of word sequences) needed to transform the hypothesis sentence to the reference sentence, normalized by the length of the reference. A low HTER score indicates less post-editing effort. HTER = https://github.com/nusnlp/neuqe 2529 number of edits number of reference tokens In MT, the reference translations for HTER are targeted, i.e., they are created by post-editing system translated sentences. However, in GEC, highquality datasets annotated by experts with minimal edits are available (Dahlmeier et al., 2013; Yannakoudakis et al., 2011) and GEC systems are typically trained to make minimal changes to input sentences. Hence, the actual human annotated references can be substituted for post-edited references of output sentences2 . We also experiment with estimating an automatic GEC evaluation metric, MaxMatch or M2 (Dahlmeier and Ng, 2012) as the quality score. M2 is the most widely used GEC evaluation metric that computes the F0.5 score of phrase-level edits made by a system. 4 Neural Quality Estimation Model Our neural quality estimation (NQE) model uses the predictor-estimator architecture (Kim"
D18-1274,N18-1033,0,0.0290203,"candidates from a high-performing GEC baseline. Our GEC baseline is built on the multilayer convolutional architecture initialized with pre-trained embeddings and re-scoring (Chollampatt and Ng, 2018). We use the same hyper-parameter settings. The baseline GEC system consists of an ensemble of 3 sets of 4 models each. The first set consists of the 4 models released by Chollampatt and Ng (2018). The second set of 4 models is trained using a label-smoothed cross entropy loss function (Szegedy et al., 2016) which has been found to be effective in neural machine translation (Vaswani et al., 2017; Edunov et al., 2018). We use a smoothing parameter of 0.1 following Vaswani et al. (2017). The third set of 4 models consists of high-recall models that make use of three techniques proposed by Junczys-Dowmunt et al. (2018): (1) pre-training decoder parameters (2) source word dropout, and (3) edit-weighted negative log-likelihood. The parameters of the decoder are initialized using the parameters from a pretrained neural language model (NLM) of the same architecture as our decoder except for the attention mechanism. We train this NLM using 100 million sentences (1.42 billion words) from the Common Crawl corpora r"
D18-1274,P15-1174,0,0.0123063,"level features in QuEst++ (Specia et al., 2015) which has been used as the baseline for WMT QE tasks from 2012 to 2017. The features are based on word-level statistics from the source-hypothesis sentence pairs, and statistics and features from language and lexical translation models trained using a parallel corpus. The descriptions of the 17 features can be found in (Bojar et al., 2017). We use the Lang-8 corpus to train the language and translation models for QuEst. 5.5 Evaluation We evaluate primarily using the Pearson’s correlation coefficient (PCC) metric following the recommendations in (Graham, 2015) and the recent WMT shared tasks (Bojar et al., 2016a, 2017). It is shown in (Graham, 2015) that aggregates of gold score distributions are easier to predict and metrics such as mean absolute error (MAE) and root mean square error (RMSE) over-estimate systems that predict the aggregates accurately despite these systems performing poorly on tail ends of the distribution (higher quality and lower quality samples). PCC does not suffer from this weakness. We use William’s Test (Williams, 1959) following (Graham, 2015) to assess the significance of the improvements. However, we also report the root"
D18-1274,N18-2046,0,0.233002,"iques. The 12-best candidates produced by this ensemble are then re-scored using edit operations and language model features following Chollampatt and Ng (2018). The performance of the baseline system (Base GEC) in terms of document-level F0.5 computed by M2 scorer on the FCE and CoNLL-2014 test sets is reported in Table 4. When we use the development set used by Chollampatt and Ng (2018) consisting of 5.4k sentences from NUCLE to train the re-scorer, Base GEC system achieves a competitive performance compared to the top GEC systems with the best-published results on CoNLL-2014 test set: G&J (Grundkiewicz and Junczys-Dowmunt, 2018), JGGH (Junczys-Dowmunt et al., 2018), and C&N (Chollampatt and Ng, 2018). When we make use of the spelling error correction system (+SpellCheck) proposed by Chollampatt and Ng (2017), which is also used by G&J and C&N, our baseline achieves the highest reported F0.5 on the CoNLL-2014 test set (56.43) when trained on public corpora alone. To the Base GEC system, we add the sentencelevel M2 scores estimated by the final NQE model (NQE A LL ) as a feature in the re-scorer. Since our NQE models use NUCLE during training, we use our development set consisting of 3.6k sentences from FCE and CoNLL-2"
D18-1274,P14-2029,0,0.04776,"ask of quality estimation became popular in machine translation (MT) through the studies by Blatz et al. (2004) and Specia et al. (2009). Much of the later work in QE of MT was through the shared tasks in Workshop on Machine Translation (WMT) campaigns (Bojar et al., 2016b) from 2012 onwards (Callison-Burch et al., 2012). Supervised methods of quality assessment have been applied to other natural language processing tasks such as ˇ text simplification (Stajner et al., 2016), language generation (Duˇsek et al., 2017), and in assisting interpreters (Stewart et al., 2018). In the context of GEC, Heilman et al. (2014) attempted to predict grammaticality of learner sentences using regression with a variety of linguistic features such as the number of misspellings, language model scores, etc. They use a dataset of learner sentences manually annotated with subjective scores of grammaticality. However, their method was to assess learner writing and not for system evaluation. To evaluate GEC systems, Napoles et al. (2015) developed reference-less metrics known as grammaticality-based metrics or GBMs. GBM scores are based on the number of errors detected using third-party tools or determined by a grammaticality"
D18-1274,N18-1055,0,0.13354,"2018). We use the same hyper-parameter settings. The baseline GEC system consists of an ensemble of 3 sets of 4 models each. The first set consists of the 4 models released by Chollampatt and Ng (2018). The second set of 4 models is trained using a label-smoothed cross entropy loss function (Szegedy et al., 2016) which has been found to be effective in neural machine translation (Vaswani et al., 2017; Edunov et al., 2018). We use a smoothing parameter of 0.1 following Vaswani et al. (2017). The third set of 4 models consists of high-recall models that make use of three techniques proposed by Junczys-Dowmunt et al. (2018): (1) pre-training decoder parameters (2) source word dropout, and (3) edit-weighted negative log-likelihood. The parameters of the decoder are initialized using the parameters from a pretrained neural language model (NLM) of the same architecture as our decoder except for the attention mechanism. We train this NLM using 100 million sentences (1.42 billion words) from the Common Crawl corpora released by Buck et al. (2014) for one epoch. We use the reported hyper-parameters PCC↑ RMSE↓ PCC↑ RMSE↓ 0.2506 0.3594 0.4066 0.4129 0.4028 0.4186 0.4529 0.4585 0.4235 0.4153 0.4123 0.4158 0.4106 0.2182 0"
D18-1274,W16-2384,0,0.236234,"dates. outputs independently for grammaticality. Asano et al. (2017) improved their method to account for fluency as well as faithfulness to the source sentence. Choshen and Abend (2018b) provide another measurement for meaning preservation using a semantic annotation scheme. Contrary to prior work in GEC reference-less evaluation, our work is aimed at estimating post-editing effort in terms of translation error rate (Snover et al., 2006) and an automatic evaluation metric, MaxMatch (Dahlmeier and Ng, 2012), in a supervised approach. We propose variants of the predictorestimator architecture (Kim and Lee, 2016b; Kim et al., 2017a) and compare them to a competitive feature-based baseline, QuEst (Specia et al., 2013, 2015) that has been successfully used for a number of language pairs in MT QE and for other applications (Stewart et al., 2018). It has also been the baseline for WMT QE tasks. 3 2 Related Work The task of quality estimation became popular in machine translation (MT) through the studies by Blatz et al. (2004) and Specia et al. (2009). Much of the later work in QE of MT was through the shared tasks in Workshop on Machine Translation (WMT) campaigns (Bojar et al., 2016b) from 2012 onwards"
D18-1274,N16-1059,0,0.222418,"dates. outputs independently for grammaticality. Asano et al. (2017) improved their method to account for fluency as well as faithfulness to the source sentence. Choshen and Abend (2018b) provide another measurement for meaning preservation using a semantic annotation scheme. Contrary to prior work in GEC reference-less evaluation, our work is aimed at estimating post-editing effort in terms of translation error rate (Snover et al., 2006) and an automatic evaluation metric, MaxMatch (Dahlmeier and Ng, 2012), in a supervised approach. We propose variants of the predictorestimator architecture (Kim and Lee, 2016b; Kim et al., 2017a) and compare them to a competitive feature-based baseline, QuEst (Specia et al., 2013, 2015) that has been successfully used for a number of language pairs in MT QE and for other applications (Stewart et al., 2018). It has also been the baseline for WMT QE tasks. 3 2 Related Work The task of quality estimation became popular in machine translation (MT) through the studies by Blatz et al. (2004) and Specia et al. (2009). Much of the later work in QE of MT was through the shared tasks in Workshop on Machine Translation (WMT) campaigns (Bojar et al., 2016b) from 2012 onwards"
D18-1274,W17-4763,0,0.350789,"end, 2018b). However, there has been no work in GEC addressing the estimation of post-editing effort. Also, to our knowledge, this is the first supervised approach to quality estimation (QE) for GEC system outputs, similar to the supervised QE task in machine translation (MT) (Specia et al., 2009). Our neural models for GEC QE are 2528 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2528–2539 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics based on variants of the predictor-estimator architecture (Kim et al., 2017a), where knowledge from a pre-trained network for a word-prediction task is transferred to another network that estimates the quality score. Apart from re-implementing the recurrent predictor-estimator models, we propose convolutional variants that are faster to train and run. We release our source code1 publicly. In summary, the contributions of this paper are: (1) we propose the first supervised approach to QE of GEC system outputs, (2) we present neural QE models that outperform a strong feature-based baseline for estimating post-editing effort and an automatic GEC evaluation metric, (3) w"
D18-1274,W04-3250,0,0.0521485,"ntences from NUCLE so that the feature weights will not be biased. We observe a slight drop in performance upon retraining, potentially due to the fewer number of sentences and error annotations in this new development set. The added feature scores are also in the logarithmic scale, similar to LM and the encoder-decoder model score. When the estimated M2 score is added, we find a significant improvement of 1.18 F0.5 on the FCE test set and a significant improvement of 0.25 F0.5 score on the CoNLL-2014 test set (p &lt; 0.001). Significance testing is done using sign test by bootstrap re-sampling (Koehn, 2004) with 100 samples. The smaller margin of improvement on CoNLL-2014 is expected due to the low PCC values (Table 3). When we add spelling error correction, the results reach 48.70 F0.5 score on FCE and 56.52 F0.5 score on CoNLL-2014. However, the results obtained by training the rescorer with our development set (FCE+CoNLL) and adding the NQE models should not be directly compared to the top systems (G&J, JGGH, and FCE CoNLL-2014 Best published results G&J (2018) w/ SpellCheck JGGH (2018) C&N (2018) w/ SpellCheck – – – 56.25 55.8 54.79 Re-scorer trained with 5.4k sents. from NUCLE Base GEC + Sp"
D18-1274,P15-2097,0,0.0492776,"e processing tasks such as ˇ text simplification (Stajner et al., 2016), language generation (Duˇsek et al., 2017), and in assisting interpreters (Stewart et al., 2018). In the context of GEC, Heilman et al. (2014) attempted to predict grammaticality of learner sentences using regression with a variety of linguistic features such as the number of misspellings, language model scores, etc. They use a dataset of learner sentences manually annotated with subjective scores of grammaticality. However, their method was to assess learner writing and not for system evaluation. To evaluate GEC systems, Napoles et al. (2015) developed reference-less metrics known as grammaticality-based metrics or GBMs. GBM scores are based on the number of errors detected using third-party tools or determined by a grammaticality prediction model (Heilman et al., 2014). Their method ignores the source sentence completely and judges the system 1 Quality Estimation of GEC Quality estimation (QE) of GEC can be defined as the task of estimating a quality score qˆ given a source sentence S and its corresponding GEC system-corrected hypothesis, H. We formulate the GEC QE task as a supervised regression task to predict the quality score"
D18-1274,D16-1228,0,0.0646268,"Missing"
D18-1274,W14-1701,1,0.838214,"re-scoring. 5.2 Datasets For training the QE models, we use sentences from the NUS Corpus of Learner English or NUCLE (Dahlmeier et al., 2013) and sentences from the training scripts of the Cambridge Learner Corpus-First Certificate Examination3 (FCE) (Yannakoudakis et al., 2011) and their corresponding hypotheses generated by the GEC system described in §5.1. We use sentences from the FCE development set and CoNLL2013 (Ng et al., 2013) test set and their GEC system-generated hypotheses as our development data. We separately test on two datasets, the FCE test set and the CoNLL-2014 test set (Ng et al., 2014). The statistics of the datasets are given in Table 1. The gold-standard scores are obtained by computing HTER using TERp (Snover et al., 2009) and sentence-level M2 using the MaxMatch scorer with the GEC hypotheses and the reference 3 The file IDs of training, development, and testing scripts of FCE are obtained from the FCE dataset for error detection at https://www.ilexir.co.uk/datasets/index.html 2532 sentences src. words hyp. words Train Development FCE (test) CoNLL-2014 86,293 1,614,120 3,635 63,782 2,769 41,457 1,312 30,144 1,620,399 63,890 41,531 30,109 Table 1: Statistics of the datas"
D18-1274,W13-3601,1,0.893273,"ntation (1.28M sentence pairs with 18.50M source sub-words and 21.88 target sub-words). During decoding, a beam width of 12 is used and the top candidate is chosen without any re-scoring. 5.2 Datasets For training the QE models, we use sentences from the NUS Corpus of Learner English or NUCLE (Dahlmeier et al., 2013) and sentences from the training scripts of the Cambridge Learner Corpus-First Certificate Examination3 (FCE) (Yannakoudakis et al., 2011) and their corresponding hypotheses generated by the GEC system described in §5.1. We use sentences from the FCE development set and CoNLL2013 (Ng et al., 2013) test set and their GEC system-generated hypotheses as our development data. We separately test on two datasets, the FCE test set and the CoNLL-2014 test set (Ng et al., 2014). The statistics of the datasets are given in Table 1. The gold-standard scores are obtained by computing HTER using TERp (Snover et al., 2009) and sentence-level M2 using the MaxMatch scorer with the GEC hypotheses and the reference 3 The file IDs of training, development, and testing scripts of FCE are obtained from the FCE dataset for error detection at https://www.ilexir.co.uk/datasets/index.html 2532 sentences src. w"
D18-1274,2006.amta-papers.25,0,0.24102,"other language applications, and (4) we show that the performance of a state-of-the-art GEC system can be improved by adding QE scores as features in re-ranking the N-best candidates. outputs independently for grammaticality. Asano et al. (2017) improved their method to account for fluency as well as faithfulness to the source sentence. Choshen and Abend (2018b) provide another measurement for meaning preservation using a semantic annotation scheme. Contrary to prior work in GEC reference-less evaluation, our work is aimed at estimating post-editing effort in terms of translation error rate (Snover et al., 2006) and an automatic evaluation metric, MaxMatch (Dahlmeier and Ng, 2012), in a supervised approach. We propose variants of the predictorestimator architecture (Kim and Lee, 2016b; Kim et al., 2017a) and compare them to a competitive feature-based baseline, QuEst (Specia et al., 2013, 2015) that has been successfully used for a number of language pairs in MT QE and for other applications (Stewart et al., 2018). It has also been the baseline for WMT QE tasks. 3 2 Related Work The task of quality estimation became popular in machine translation (MT) through the studies by Blatz et al. (2004) and Sp"
D18-1274,W09-0441,0,0.0244514,"013) and sentences from the training scripts of the Cambridge Learner Corpus-First Certificate Examination3 (FCE) (Yannakoudakis et al., 2011) and their corresponding hypotheses generated by the GEC system described in §5.1. We use sentences from the FCE development set and CoNLL2013 (Ng et al., 2013) test set and their GEC system-generated hypotheses as our development data. We separately test on two datasets, the FCE test set and the CoNLL-2014 test set (Ng et al., 2014). The statistics of the datasets are given in Table 1. The gold-standard scores are obtained by computing HTER using TERp (Snover et al., 2009) and sentence-level M2 using the MaxMatch scorer with the GEC hypotheses and the reference 3 The file IDs of training, development, and testing scripts of FCE are obtained from the FCE dataset for error detection at https://www.ilexir.co.uk/datasets/index.html 2532 sentences src. words hyp. words Train Development FCE (test) CoNLL-2014 86,293 1,614,120 3,635 63,782 2,769 41,457 1,312 30,144 1,620,399 63,890 41,531 30,109 Table 1: Statistics of the datasets used for QE. sentences. When multiple references are available, the gold-standard score is chosen to be the best score (lowest HTER or high"
D18-1274,P15-4020,0,0.0336245,"den layer dimension of 100. They are trained with the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 0.0005 and batch size of 32. We use dropout with a probability of 0.5 during training. Our final model is NQE ALL , which averages the estimated scores of all variants, namely, NQE RR , NQE CR , NQE CC , and NQE RC . 5.4 Baselines We use two non-neural baselines for comparison to our neural QE models. AVERAGE: The average score of training sentences is used as the estimated score for all test sentences. Q U E ST: We use the standard 17 sentence-level features in QuEst++ (Specia et al., 2015) which has been used as the baseline for WMT QE tasks from 2012 to 2017. The features are based on word-level statistics from the source-hypothesis sentence pairs, and statistics and features from language and lexical translation models trained using a parallel corpus. The descriptions of the 17 features can be found in (Bojar et al., 2017). We use the Lang-8 corpus to train the language and translation models for QuEst. 5.5 Evaluation We evaluate primarily using the Pearson’s correlation coefficient (PCC) metric following the recommendations in (Graham, 2015) and the recent WMT shared tasks ("
D18-1274,P13-4014,0,0.0611413,"Missing"
D18-1274,2009.eamt-1.5,0,0.42716,"metrics such as MaxMatch or M2 (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2016a, 2015) are used to evaluate GEC systems with human-annotated references, a few reference-less GEC metrics have been proposed to evaluate fluency, grammaticality, and adequacy (Napoles et al., 2016b; Asano et al., 2017; Choshen and Abend, 2018b). However, there has been no work in GEC addressing the estimation of post-editing effort. Also, to our knowledge, this is the first supervised approach to quality estimation (QE) for GEC system outputs, similar to the supervised QE task in machine translation (MT) (Specia et al., 2009). Our neural models for GEC QE are 2528 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2528–2539 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics based on variants of the predictor-estimator architecture (Kim et al., 2017a), where knowledge from a pre-trained network for a word-prediction task is transferred to another network that estimates the quality score. Apart from re-implementing the recurrent predictor-estimator models, we propose convolutional variants that are faster to train and run. We"
D18-1274,P18-2105,0,0.0153891,"servation using a semantic annotation scheme. Contrary to prior work in GEC reference-less evaluation, our work is aimed at estimating post-editing effort in terms of translation error rate (Snover et al., 2006) and an automatic evaluation metric, MaxMatch (Dahlmeier and Ng, 2012), in a supervised approach. We propose variants of the predictorestimator architecture (Kim and Lee, 2016b; Kim et al., 2017a) and compare them to a competitive feature-based baseline, QuEst (Specia et al., 2013, 2015) that has been successfully used for a number of language pairs in MT QE and for other applications (Stewart et al., 2018). It has also been the baseline for WMT QE tasks. 3 2 Related Work The task of quality estimation became popular in machine translation (MT) through the studies by Blatz et al. (2004) and Specia et al. (2009). Much of the later work in QE of MT was through the shared tasks in Workshop on Machine Translation (WMT) campaigns (Bojar et al., 2016b) from 2012 onwards (Callison-Burch et al., 2012). Supervised methods of quality assessment have been applied to other natural language processing tasks such as ˇ text simplification (Stajner et al., 2016), language generation (Duˇsek et al., 2017), and i"
D18-1274,P11-1019,0,0.631671,"e minimum number of edit operations (insertions, deletions, substitutions, or shifts of word sequences) needed to transform the hypothesis sentence to the reference sentence, normalized by the length of the reference. A low HTER score indicates less post-editing effort. HTER = https://github.com/nusnlp/neuqe 2529 number of edits number of reference tokens In MT, the reference translations for HTER are targeted, i.e., they are created by post-editing system translated sentences. However, in GEC, highquality datasets annotated by experts with minimal edits are available (Dahlmeier et al., 2013; Yannakoudakis et al., 2011) and GEC systems are typically trained to make minimal changes to input sentences. Hence, the actual human annotated references can be substituted for post-edited references of output sentences2 . We also experiment with estimating an automatic GEC evaluation metric, MaxMatch or M2 (Dahlmeier and Ng, 2012) as the quality score. M2 is the most widely used GEC evaluation metric that computes the F0.5 score of phrase-level edits made by a system. 4 Neural Quality Estimation Model Our neural quality estimation (NQE) model uses the predictor-estimator architecture (Kim et al., 2017a) to model the r"
D18-1383,P07-1056,0,0.880637,"s reviews would contain opinion words such as “tender”, “tasty”, or “undercooked” and movie reviews would contain “thrilling”, “horrific”, or “hilarious”. The intersection between these two sets of opinion words could be small which makes domain adaptation difficult. Several techniques have been proposed for addressing the problem of domain shifting. The aim is to bridge the source and target domains by learning domain-invariant feature representations so that a classifier trained on a source domain can be adapted to another target domain. In cross-domain sentiment classification, many works (Blitzer et al., 2007; Pan et al., 2010; Zhou et al., 2015; Wu and Huang, 2016; Yu and Jiang, 2016) utilize a key intuition that domain-specific features could be aligned with the help of domaininvariant features (pivot features). For instance, “hilarious” and “tasty” could be aligned as both of them are relevant to “good”. Despite their promising results, these works share two major limitations. First, they highly depend on the heuristic selection of pivot features, which may be sensitive to different applications. Thus the learned new representations may not effectively reduce the domain difference. Furthermore,"
D18-1383,P11-1013,0,0.0346632,"h certain opinion words are completely distinct for each domain, they can be aligned if they have high correlation with some domain-invariant opinion words (pivot words) such as “excellent” or “terrible”. Blitzer et al. (2007) proposed a method based on structural correspondence learning (SCL), which uses pivot feature prediction to induce a projected feature space that works well for both the source and the target domains. The pivot words are selected in a way to cover common domain-invariant opinion words. Subsequent research aims to better align the domain-specific words (Pan et al., 2010; He et al., 2011; Wu and Huang, 2016) such that the domain discrepancy could be reduced. More recently, Yu and Jiang (2016) borrow the idea of pivot feature prediction from SCL and extend it to a neural network-based solution with auxiliary tasks. In their experiment, substantial improvement over SCL has been observed due to the use of real-valued word embeddings. Unsupervised representation learning with deep neural networks (DNN) such as denoising autoencoders has also been explored for feature adaptation (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014). It has been shown that DNNs could"
D18-1383,P07-1034,0,0.815587,"sarial training framework for reducing domain difference. In their model, a subnetwork is added as a domain discriminator while deep features are learned to confuse the discriminator. The feature adaptation component in our model shares similar intuition with MMD and adversary training. We will show a detailed comparison with them in our experiments. Semi-supervised Learning: We attempt to treat domain adaptation as a semi-supervised learning task by considering the target instances as unlabeled data. Some efforts have been initiated on transfer learning from unlabeled data (Dai et al., 2007; Jiang and Zhai, 2007; Wu et al., 2009). In our model, we reduce the domain discrepancy by feature adaptation, and thereafter adopt semi-supervised learning techniques to learn from unlabeled data. Primarily motivated by (Grandvalet and Bengio, 2004) and (Laine and Aila, 2017), we employed entropy minimization and self-ensemble bootstrapping as regularizations to incorporate unlabeled data. Our experimental results show that both methods are effective when jointly trained with the feature adaptation objective, which confirms to our motivation. 3 3.1 Model Description Notations and Model Overview We conduct most of"
D18-1383,D14-1181,0,0.0154368,"m in Appendix A for completeness since most of previous works reported results on it. Total 6000 6000 6000 6000 6000 6000 6000 6000 (a) Small-scale datasets Domain IMDB Yelp Cell Phone Baby #Pos 55,242 155,625 148,657 126,525 #Neg 11,735 29,597 24,343 17,012 #Neu 17,942 45,941 21,439 17,255 Total 84,919 231,163 194,439 160,792 (b) Large-scale datasets Table 1: Summary of datasets. 4 4.1 Experiments CNN Encoder Implementation We have left the feature encoder G unspecified, for which, a few options can be considered. In our implementation, we adopt a one-layer CNN structure from previous works (Kim, 2014; Yu and Jiang, 2016), as it has been demonstrated to work well for sentiment classification tasks. Given a review document x = (x1 , x2 , ..., xn ) consisting of n words, we begin by associating each word with a continuous word embedding (Mikolov et al., 2013) ex from an embedding matrix E 2 RV ⇥d , where V is the vocabulary size and d is the embedding dimension. E is jointly updated with other network parameters during training. Given a window of dense word embeddings ex1 , ex2 , ..., exl , the convolution layer first concatenates these vectors to form a vector x ˆ of length ld and then the"
D18-1383,D14-1162,0,0.0819563,"acy of the source domain. Other neural network models also have the same problem for hyper-parameter tuning. Therefore, our strategy is to use the development set from the target domain to optimize 1 and 2 for one problem (e.g., we only do this on E!BK), and fix their values on the other problems. This setting assumes that we have at least two labeled domains such that we can optimize the hyper-parameters, and then we fix them for other new unlabeled domains to transfer to. 4.4 Training Details and Hyper-parameters We initialize word embeddings using the 300dimension GloVe vectors supplied by Pennington et al., (2014), which were trained on 840 billion tokens from the Common Crawl. For each pair of domains, the vocabulary consists of the top 10000 most frequent words. For words in the vocabulary but not present in the pre-trained embeddings, we randomly initialize them. We set hyper-parameters of the CNN encoder following previous works (Kim, 2014; Yu and Jiang, 2016) without specific tuning on our datasets. The window size is set to 3 and the size of the hidden layer is set to 300. The nonlinear activation function is Relu. For regularization, we also follow their settings and employ dropout with probabil"
D18-1383,P15-1098,0,0.015808,"sampled from millions of reviews, it better reflects real-life sentiment distribution. Large-scale datasets: We further conduct experiments on four much larger datasets: IMDB4 2 http://jmcauley.ucsd.edu/data/amazon/ The original reviews were rated on a 5-point scale. We label them with rating < 3, > 3, and = 3 as negative, positive, and neutral respectively. 4 IMDB is rated on a 10-point scale, and we label reviews with rating < 5, > 6, and = 5/6 as negative, positive, and neutral respectively. 3471 3 (I), Yelp2014 (Y), Cell Phone (C), and Baby (B). IMDB and Yelp2014 were previously used in (Tang et al., 2015; Yang et al., 2017). Cell phone and Baby are from the large-scale Amazon dataset (McAuley et al., 2015; He and McAuley, 2016). Detailed statistics are summarized in Table 1b. We keep all reviews in the original datasets and consider a transductive setting where all target examples are used for both training (without label information) and evaluation. We perform sampling to balance the classes of labeled source data in each minibatch B (s) during training. 4.3 Selection of Development Set Ideally, the development set should be drawn from the same distribution as the test set. However, under th"
D18-1383,D09-1158,1,0.700141,"ork for reducing domain difference. In their model, a subnetwork is added as a domain discriminator while deep features are learned to confuse the discriminator. The feature adaptation component in our model shares similar intuition with MMD and adversary training. We will show a detailed comparison with them in our experiments. Semi-supervised Learning: We attempt to treat domain adaptation as a semi-supervised learning task by considering the target instances as unlabeled data. Some efforts have been initiated on transfer learning from unlabeled data (Dai et al., 2007; Jiang and Zhai, 2007; Wu et al., 2009). In our model, we reduce the domain discrepancy by feature adaptation, and thereafter adopt semi-supervised learning techniques to learn from unlabeled data. Primarily motivated by (Grandvalet and Bengio, 2004) and (Laine and Aila, 2017), we employed entropy minimization and self-ensemble bootstrapping as regularizations to incorporate unlabeled data. Our experimental results show that both methods are effective when jointly trained with the feature adaptation objective, which confirms to our motivation. 3 3.1 Model Description Notations and Model Overview We conduct most of our experiments u"
D18-1383,P16-1029,0,0.0841724,"sty”, or “undercooked” and movie reviews would contain “thrilling”, “horrific”, or “hilarious”. The intersection between these two sets of opinion words could be small which makes domain adaptation difficult. Several techniques have been proposed for addressing the problem of domain shifting. The aim is to bridge the source and target domains by learning domain-invariant feature representations so that a classifier trained on a source domain can be adapted to another target domain. In cross-domain sentiment classification, many works (Blitzer et al., 2007; Pan et al., 2010; Zhou et al., 2015; Wu and Huang, 2016; Yu and Jiang, 2016) utilize a key intuition that domain-specific features could be aligned with the help of domaininvariant features (pivot features). For instance, “hilarious” and “tasty” could be aligned as both of them are relevant to “good”. Despite their promising results, these works share two major limitations. First, they highly depend on the heuristic selection of pivot features, which may be sensitive to different applications. Thus the learned new representations may not effectively reduce the domain difference. Furthermore, these works only utilize the unlabeled target data for r"
D18-1383,D17-1312,0,0.0198492,"ons of reviews, it better reflects real-life sentiment distribution. Large-scale datasets: We further conduct experiments on four much larger datasets: IMDB4 2 http://jmcauley.ucsd.edu/data/amazon/ The original reviews were rated on a 5-point scale. We label them with rating < 3, > 3, and = 3 as negative, positive, and neutral respectively. 4 IMDB is rated on a 10-point scale, and we label reviews with rating < 5, > 6, and = 5/6 as negative, positive, and neutral respectively. 3471 3 (I), Yelp2014 (Y), Cell Phone (C), and Baby (B). IMDB and Yelp2014 were previously used in (Tang et al., 2015; Yang et al., 2017). Cell phone and Baby are from the large-scale Amazon dataset (McAuley et al., 2015; He and McAuley, 2016). Detailed statistics are summarized in Table 1b. We keep all reviews in the original datasets and consider a transductive setting where all target examples are used for both training (without label information) and evaluation. We perform sampling to balance the classes of labeled source data in each minibatch B (s) during training. 4.3 Selection of Development Set Ideally, the development set should be drawn from the same distribution as the test set. However, under the unsupervised domai"
D18-1383,P14-2088,0,0.0853454,"n the domain-specific words (Pan et al., 2010; He et al., 2011; Wu and Huang, 2016) such that the domain discrepancy could be reduced. More recently, Yu and Jiang (2016) borrow the idea of pivot feature prediction from SCL and extend it to a neural network-based solution with auxiliary tasks. In their experiment, substantial improvement over SCL has been observed due to the use of real-valued word embeddings. Unsupervised representation learning with deep neural networks (DNN) such as denoising autoencoders has also been explored for feature adaptation (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014). It has been shown that DNNs could learn transferable representations that disentangle the underlying factors of variation behind data samples. Although the aforementioned methods aim to reduce the domain discrepancy, they do not explicitly minimize the distance between distributions, and some of them highly rely on the selection of pivot features. In our method, we formally construct an objective for this purpose. Similar ideas have been explored in many computer vision problems, where the representations of the underlying domains are encouraged to be similar through explicit objectives (Tze"
D18-1383,D16-1023,0,0.392817,"d” and movie reviews would contain “thrilling”, “horrific”, or “hilarious”. The intersection between these two sets of opinion words could be small which makes domain adaptation difficult. Several techniques have been proposed for addressing the problem of domain shifting. The aim is to bridge the source and target domains by learning domain-invariant feature representations so that a classifier trained on a source domain can be adapted to another target domain. In cross-domain sentiment classification, many works (Blitzer et al., 2007; Pan et al., 2010; Zhou et al., 2015; Wu and Huang, 2016; Yu and Jiang, 2016) utilize a key intuition that domain-specific features could be aligned with the help of domaininvariant features (pivot features). For instance, “hilarious” and “tasty” could be aligned as both of them are relevant to “good”. Despite their promising results, these works share two major limitations. First, they highly depend on the heuristic selection of pivot features, which may be sensitive to different applications. Thus the learned new representations may not effectively reduce the domain difference. Furthermore, these works only utilize the unlabeled target data for representation learnin"
D18-1383,P16-1031,0,0.159056,"Missing"
D18-1456,P16-1223,0,0.0138513,"that the integration of our proposed framework significantly outperforms several strong baseline systems that use pipeline or threshold-based approaches. 1 Introduction Machine comprehension (MC) systems mimic the process of reading comprehension (RC) by answering questions after understanding natural language text. Several datasets and resources have been developed recently. Richardson et al. (2013) developed a small-scale multiple-choice question answering (QA) dataset. Hermann et al. (2015) created a large cloze-style MC dataset based on CNN and Daily Mail news article summaries. However, Chen et al. (2016) reported that the task is not challenging enough and hence, advanced models had to be evaluated on more realistic datasets. Subsequently, SQuAD (Rajpurkar et al., 2016) was released, where, unlike previous datasets, the answers to different questions can vary in length. In previous datasets, questions and answers are formulated given text passages. Hence, a valid answer can always be found in the associated passage for every question created. Trischler et al. (2017) proposed a more challenging and realistic dataset, NewsQA, where the questions were formed using CNN article summaries without a"
D18-1456,P17-1171,0,0.376794,"alid answer can always be found in the associated passage for every question created. Trischler et al. (2017) proposed a more challenging and realistic dataset, NewsQA, where the questions were formed using CNN article summaries without accessing the original full texts. As such, some questions have no valid answers in the associated passages (referred to as nil questions). Recently, several neural models for answer span extraction have been proposed (Wang and Jiang, 2017; Seo et al., 2017; Yang et al., 2017; Xiong et al., 2017; Weissenborn et al., 2017; Wang et al., 2017; Shen et al., 2017b; Chen et al., 2017; Kundu and Ng, 2018). However, none of the models considered nil questions, although it is crucial for a practical QA system to be able to determine whether a text passage contains a valid answer for a question. In this paper, we focus on developing QA systems that extract an answer for a question if and only if the associated passage contains a valid answer. Otherwise, they are expected to return Nil as answer. We propose a nil-aware answer extraction framework which returns Nil or a span of text as answer, when integrated with end-toend neural MC models. Our proposed framework is based on e"
D18-1456,P82-1020,0,0.829675,"Missing"
D18-1456,D14-1181,0,0.0024567,"when, which, where, why}. The final question representation, q ˜ ∈ RH , is formulated by applying a feed-forward neural network on the concatenated representation of qma and qf . Nil-Aware AMANDA The architecture of Nil-aware AMANDA (NAMANDA) is given in Figure 1. 3.2.1 Embeddings To obtain the embeddings, we concatenate word and character-level embedding vectors. We use pre-trained vectors from GloVe (Pennington et al., 2014) for word-level embeddings. For character embeddings, a trainable character-based lookup table is used followed by a convolutional neural network (CNN) and max-pooling (Kim, 2014). 3.2.2 Sequence Encoding We use bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) on the embedding vectors to incorporate contextual information. We represent the outputs as D ∈ RT ×H and Q ∈ RU ×H for passage and question respectively. H is the number of hidden units of the BiLSTMs. 3.2.3 Similarity Matrix The similarity matrix is obtained by computing the dot product of passage and question sequencelevel encoding vectors. The similarity matrix A ∈ RT ×U can be expressed as A = D Q&gt; , where Ai,j is the similarity between the ith passage word and the jth question word. 3.2.5 Que"
D18-1456,D14-1162,0,0.0824389,"of the first wh-word qtwh and its following word qtwh +1 . It can be given as qf = qtwh ||qtwh +1 , where ||denotes the concatenation operation. The set of wh-words we used is {what, who, how, when, which, where, why}. The final question representation, q ˜ ∈ RH , is formulated by applying a feed-forward neural network on the concatenated representation of qma and qf . Nil-Aware AMANDA The architecture of Nil-aware AMANDA (NAMANDA) is given in Figure 1. 3.2.1 Embeddings To obtain the embeddings, we concatenate word and character-level embedding vectors. We use pre-trained vectors from GloVe (Pennington et al., 2014) for word-level embeddings. For character embeddings, a trainable character-based lookup table is used followed by a convolutional neural network (CNN) and max-pooling (Kim, 2014). 3.2.2 Sequence Encoding We use bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) on the embedding vectors to incorporate contextual information. We represent the outputs as D ∈ RT ×H and Q ∈ RU ×H for passage and question respectively. H is the number of hidden units of the BiLSTMs. 3.2.3 Similarity Matrix The similarity matrix is obtained by computing the dot product of passage and question sequencele"
D18-1456,P18-2124,0,0.0328764,"though SQuAD became very popular and served as a good test set to develop advanced end-toend neural network architectures, it does not include any nil questions. In practical QA, it is critical to decide whether or not a passage contains a valid answer for a given question. Subsequently, the NewsQA (Trischler et al., 2017) dataset has been released which attempts to overcome this deficiency. However, all the proposed models for NewsQA so far have excluded nil questions during evaluation. Contrary to prior work, we focus on developing models for nil-aware answer span extraction. Very recently, Rajpurkar et al. (2018) released the SQUADRUN dataset by augmenting the SQuAD dataset with unanswerable questions. The unanswerable questions are written adversarially by crowdworkers to look similar to the answerable ones. 7 Conclusion In this paper, we have focused on nil-aware answer span extraction for RC-based QA. A nil-aware QA system only extracts a span of text from the associated passage as an answer to a given question if and only if the passage contains a valid answer. We have proposed a nil-aware answer span extraction framework based on evidence decomposition and aggregation that can be easily integrate"
D18-1456,D16-1264,0,0.394486,"ction Machine comprehension (MC) systems mimic the process of reading comprehension (RC) by answering questions after understanding natural language text. Several datasets and resources have been developed recently. Richardson et al. (2013) developed a small-scale multiple-choice question answering (QA) dataset. Hermann et al. (2015) created a large cloze-style MC dataset based on CNN and Daily Mail news article summaries. However, Chen et al. (2016) reported that the task is not challenging enough and hence, advanced models had to be evaluated on more realistic datasets. Subsequently, SQuAD (Rajpurkar et al., 2016) was released, where, unlike previous datasets, the answers to different questions can vary in length. In previous datasets, questions and answers are formulated given text passages. Hence, a valid answer can always be found in the associated passage for every question created. Trischler et al. (2017) proposed a more challenging and realistic dataset, NewsQA, where the questions were formed using CNN article summaries without accessing the original full texts. As such, some questions have no valid answers in the associated passages (referred to as nil questions). Recently, several neural model"
D18-1456,D13-1020,0,0.0212615,"Our proposed nil-aware answer extraction neural network decomposes pieces of evidence into relevant and irrelevant parts and then combines them to infer the existence of any answer. Experiments on the NewsQA dataset show that the integration of our proposed framework significantly outperforms several strong baseline systems that use pipeline or threshold-based approaches. 1 Introduction Machine comprehension (MC) systems mimic the process of reading comprehension (RC) by answering questions after understanding natural language text. Several datasets and resources have been developed recently. Richardson et al. (2013) developed a small-scale multiple-choice question answering (QA) dataset. Hermann et al. (2015) created a large cloze-style MC dataset based on CNN and Daily Mail news article summaries. However, Chen et al. (2016) reported that the task is not challenging enough and hence, advanced models had to be evaluated on more realistic datasets. Subsequently, SQuAD (Rajpurkar et al., 2016) was released, where, unlike previous datasets, the answers to different questions can vary in length. In previous datasets, questions and answers are formulated given text passages. Hence, a valid answer can always b"
D18-1456,W17-2623,0,0.134921,"A) dataset. Hermann et al. (2015) created a large cloze-style MC dataset based on CNN and Daily Mail news article summaries. However, Chen et al. (2016) reported that the task is not challenging enough and hence, advanced models had to be evaluated on more realistic datasets. Subsequently, SQuAD (Rajpurkar et al., 2016) was released, where, unlike previous datasets, the answers to different questions can vary in length. In previous datasets, questions and answers are formulated given text passages. Hence, a valid answer can always be found in the associated passage for every question created. Trischler et al. (2017) proposed a more challenging and realistic dataset, NewsQA, where the questions were formed using CNN article summaries without accessing the original full texts. As such, some questions have no valid answers in the associated passages (referred to as nil questions). Recently, several neural models for answer span extraction have been proposed (Wang and Jiang, 2017; Seo et al., 2017; Yang et al., 2017; Xiong et al., 2017; Weissenborn et al., 2017; Wang et al., 2017; Shen et al., 2017b; Chen et al., 2017; Kundu and Ng, 2018). However, none of the models considered nil questions, although it is"
D18-1456,P17-1018,0,0.404118,"mulated given text passages. Hence, a valid answer can always be found in the associated passage for every question created. Trischler et al. (2017) proposed a more challenging and realistic dataset, NewsQA, where the questions were formed using CNN article summaries without accessing the original full texts. As such, some questions have no valid answers in the associated passages (referred to as nil questions). Recently, several neural models for answer span extraction have been proposed (Wang and Jiang, 2017; Seo et al., 2017; Yang et al., 2017; Xiong et al., 2017; Weissenborn et al., 2017; Wang et al., 2017; Shen et al., 2017b; Chen et al., 2017; Kundu and Ng, 2018). However, none of the models considered nil questions, although it is crucial for a practical QA system to be able to determine whether a text passage contains a valid answer for a question. In this paper, we focus on developing QA systems that extract an answer for a question if and only if the associated passage contains a valid answer. Otherwise, they are expected to return Nil as answer. We propose a nil-aware answer extraction framework which returns Nil or a span of text as answer, when integrated with end-toend neural MC model"
D18-1456,C16-1127,0,0.265346,"k In this section, we first describe our proposed evidence decomposition-aggregation framework for nil-aware answer extraction. Then, we provide a detailed description of how we extend a state-ofthe-art model AMANDA (Kundu and Ng, 2018) to NAMANDA1 (nil-aware AMANDA). We also provide brief descriptions of how we integrate our proposed framework with the other three models. 1 Our source code is available at https://github. com/nusnlp/namanda 3.1 Nil-Aware Answer Extraction Decomposition of lexical semantics over sentences has been successfully used in the past for sentence similarity learning (Wang et al., 2016). Most of the recently proposed machine reading comprehension models can be generalized based on a common pattern observed in their network architecture. They have a question-passage joint encoding layer (also known as question-aware passage encoding layer) followed by an evidence encoding layer. In this work, we decompose the evidence vectors for each passage word obtained from the evidence encoding layer with respect to question-passage joint encoding vectors to derive semantically relevant and irrelevant components. We decompose the evidence vectors for each passage word, because passage ve"
D18-1456,K17-1028,0,0.021577,"stions and answers are formulated given text passages. Hence, a valid answer can always be found in the associated passage for every question created. Trischler et al. (2017) proposed a more challenging and realistic dataset, NewsQA, where the questions were formed using CNN article summaries without accessing the original full texts. As such, some questions have no valid answers in the associated passages (referred to as nil questions). Recently, several neural models for answer span extraction have been proposed (Wang and Jiang, 2017; Seo et al., 2017; Yang et al., 2017; Xiong et al., 2017; Weissenborn et al., 2017; Wang et al., 2017; Shen et al., 2017b; Chen et al., 2017; Kundu and Ng, 2018). However, none of the models considered nil questions, although it is crucial for a practical QA system to be able to determine whether a text passage contains a valid answer for a question. In this paper, we focus on developing QA systems that extract an answer for a question if and only if the associated passage contains a valid answer. Otherwise, they are expected to return Nil as answer. We propose a nil-aware answer extraction framework which returns Nil or a span of text as answer, when integrated with end-to"
D18-1456,D17-1122,0,0.043734,"passages. Hence, a valid answer can always be found in the associated passage for every question created. Trischler et al. (2017) proposed a more challenging and realistic dataset, NewsQA, where the questions were formed using CNN article summaries without accessing the original full texts. As such, some questions have no valid answers in the associated passages (referred to as nil questions). Recently, several neural models for answer span extraction have been proposed (Wang and Jiang, 2017; Seo et al., 2017; Yang et al., 2017; Xiong et al., 2017; Weissenborn et al., 2017; Wang et al., 2017; Shen et al., 2017b; Chen et al., 2017; Kundu and Ng, 2018). However, none of the models considered nil questions, although it is crucial for a practical QA system to be able to determine whether a text passage contains a valid answer for a question. In this paper, we focus on developing QA systems that extract an answer for a question if and only if the associated passage contains a valid answer. Otherwise, they are expected to return Nil as answer. We propose a nil-aware answer extraction framework which returns Nil or a span of text as answer, when integrated with end-toend neural MC models. Our proposed fra"
D19-1533,W06-2911,0,0.06073,"P). Unfortunately, while it is easy for a human to infer the correct sense of a word given a context, it is a challenge for NLP systems. As such, WSD is an important task and it has been shown that WSD helps downstream NLP tasks, such as machine translation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2017; Peters"
D19-1533,P18-1230,0,0.406914,"enation into an affine transformation followed by softmax normalization, similar to the approach to incorporate a bidirectional LSTM adopted in sequence labeling tasks such as part-ofspeech tagging and named entity recognition (Ma and Hovy, 2016). Raganato et al. (2017b) proposed a self-attention layer on top of the concatenated bidirectional LSTM hidden states for WSD and introduced multi-task learning with part-ofspeech tagging and semantic labeling as auxiliary tasks. However, on average across the test sets, their approach did not outperform SVM with word embedding features. Subsequently, Luo et al. (2018) proposed the incorporation of glosses from WordNet in a bidirectional LSTM for WSD, and reported better results than both SVM and prior bidirectional LSTM models. A neural language model (LM) is aimed at predicting a word given its surrounding context. As such, the resulting hidden representation vector captures the context of a word in a sentence. Melamud et al. (2016) designed context2vec, which is a one-layer bidirectional LSTM trained to maximize the similarity between the hidden state representation of the LSTM and the target word embedding. Peters et al. (2018) designed ELMo, which is a"
D19-1533,P07-1005,1,0.721409,"nt margins on multiple benchmark WSD datasets. 1 Introduction Word sense disambiguation (WSD) automatically assigns a pre-defined sense to a word in a text. Different senses of a word reflect different meanings a word has in different contexts. Identifying the correct word sense given a context is crucial in natural language processing (NLP). Unfortunately, while it is easy for a human to infer the correct sense of a word given a context, it is a challenge for NLP systems. As such, WSD is an important task and it has been shown that WSD helps downstream NLP tasks, such as machine translation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) fe"
D19-1533,P16-1101,0,0.0338982,"ution of surrounding word features as their distances to the target word increased. The neural sequence tagging approach has also been explored for WSD. K˚ageb¨ack and Salomonsson (2016) proposed bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) for WSD. They concatenated the hidden states of the forward and backward LSTMs and fed the concatenation into an affine transformation followed by softmax normalization, similar to the approach to incorporate a bidirectional LSTM adopted in sequence labeling tasks such as part-ofspeech tagging and named entity recognition (Ma and Hovy, 2016). Raganato et al. (2017b) proposed a self-attention layer on top of the concatenated bidirectional LSTM hidden states for WSD and introduced multi-task learning with part-ofspeech tagging and semantic labeling as auxiliary tasks. However, on average across the test sets, their approach did not outperform SVM with word embedding features. Subsequently, Luo et al. (2018) proposed the incorporation of glosses from WordNet in a bidirectional LSTM for WSD, and reported better results than both SVM and prior bidirectional LSTM models. A neural language model (LM) is aimed at predicting a word given"
D19-1533,S07-1054,1,0.692529,"nt margins on multiple benchmark WSD datasets. 1 Introduction Word sense disambiguation (WSD) automatically assigns a pre-defined sense to a word in a text. Different senses of a word reflect different meanings a word has in different contexts. Identifying the correct word sense given a context is crucial in natural language processing (NLP). Unfortunately, while it is easy for a human to infer the correct sense of a word given a context, it is a challenge for NLP systems. As such, WSD is an important task and it has been shown that WSD helps downstream NLP tasks, such as machine translation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) fe"
D19-1533,K16-1006,0,0.126423,"d using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019) have been shown to improve downstream NLP tasks. Pre-trained contextualized word representations are obtained through neural sentence encoders trained on a huge amount of raw texts. When the resulting sentence encoder is fine-tuned on the downstream task, such as question answering, named entity recognition, and sentiment analysis, with much smaller annotated training data, it has been shown that the trained model, with the pre-trained sentence encoder component, achieves new state-of-the-art results on those tasks. While demonst"
D19-1533,N19-1423,0,0.327998,"2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019) have been shown to improve downstream NLP tasks. Pre-trained contextualized word representations are obtained through neural sentence encoders trained on a huge amount of raw texts. When the resulting sentence encoder is fine-tuned on the downstream task, such as question answering, named entity recognition, and sentiment analysis, with much smaller annotated training data, it has been shown that the trained model, with the pre-trained sentence encoder component, achieves new state-of-the-art results on those tasks. While demonstrating superior performance in downstream NLP tasks, pre-trained"
D19-1533,P82-1020,0,0.796691,"Missing"
D19-1533,P16-1085,0,0.376255,"and it has been shown that WSD helps downstream NLP tasks, such as machine translation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019) have been shown to improve downstream NLP tasks. Pre-trained contextualized word representations are obtained through neural sentence encoders traine"
D19-1533,W16-5307,0,0.251398,"Missing"
D19-1533,W04-0834,1,0.543558,"age processing (NLP). Unfortunately, while it is easy for a human to infer the correct sense of a word given a context, it is a challenge for NLP systems. As such, WSD is an important task and it has been shown that WSD helps downstream NLP tasks, such as machine translation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2"
D19-1533,H94-1046,0,0.712102,"16 sentences. As update progresses, we pick the best model parameters from a series of neural network updates based on accuracy on a held-out development set, disjoint from the training set. The state-of-the-art supervised WSD approach takes into account features from the neighboring sentences, typically one sentence to the left and one to the right apart from the current sentence that contains the ambiguous words. We also exploit this in our model, as BERT supports inputs with multiple sentences separated by a special [SEP] symbol. For English all-words WSD, we train our WSD model on SemCor (Miller et al., 1994), and test it on Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15). This common benchmark, which has been annotated with WordNet-3.0 senses (Raganato et al., 2017a), has recently been adopted in English all-words WSD. Following (Raganato et al., 2017b), we choose SemEval 2007 Task 17 (SE07) as our development data to pick the # Instances # Lexelts 226,036 455 2,282 1,850 1,664 1,022 22,436 330 1,093 977 751 512 8,611 4,328 8,022 3,944 177 146 57 57 66,409 9,523 1,769 3,227 1,876 1,483 8,355 431 341 160 253 223 143 324 Table 1: Statistics of the da"
D19-1533,D17-2018,0,0.0774124,"mber of annotations including word senses for Chinese. We follow the data setup of Pradhan et al. (2013) and conduct an evaluation on four genres, i.e., broadcast conversation (BC), broadcast news (BN), magazine (MZ), and newswire (NW), as well as the concatenation of all genres. While the training and development datasets are divided into genres, we train on the concatenation of all genres and test on each individual genre. 5301 1 LDC2013T19 System SE07 SE2 Reported in previous papers MFS baseline 54.5 65.6 IMS (Zhong and Ng, 2010) 61.3 70.9 IMS+emb (Iacobacci et al., 2016) 60.9 71.0 SupWSD (Papandrea et al., 2017) 60.2 71.3 SupWSD+emb (Papandrea et al., 2017) 63.1 72.7 BiLSTMatt+LEX (Raganato et al., 2017b) 63.7 72.0 GASext Concat (Luo et al., 2018) – 72.2 context2vec (Melamud et al., 2016) 61.3 71.8 ELMo (Peters et al., 2018) 62.2 71.6 BERT nearest neighbor (ours) 1nn (1sent) 64.0 73.0 1nn (1sent+1sur) 63.3 73.8 BERT linear projection (ours) Simple (1sent) 67.0 75.0 ∗ Simple (1sent+1sur) 69.3 75.9∗ LW (1sent) 66.7 75.0 ∗ LW (1sent+1sur) 69.0 76.4∗ GLU (1sent) 64.9 74.1 ∗ GLU (1sent+1sur) 68.1 75.5∗ GLU+LW (1sent) 65.7 74.0 GLU+LW (1sent+1sur) 68.5∗ 75.5∗ SE3 SE13 SE15 Avg 66.0 69.3 69.3 68.8 70.6 69.4"
D19-1533,N18-1202,0,0.275979,"o, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019) have been shown to improve downstream NLP tasks. Pre-trained contextualized word representations are obtained through neural sentence encoders trained on a huge amount of raw texts. When the resulting sentence encoder is fine-tuned on the downstream task, such as question answering, named entity recognition, and sentiment analysis, with much smaller annotated training data, it has been shown that the trained model, with the pre-trained sentence encoder component, achieves new state-of-the-art results on those tasks. While demonstrating superior performance in downstream"
D19-1533,W13-3516,1,0.890553,"Missing"
D19-1533,E17-1010,0,0.302413,"slation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019) have been shown to improve downstream NLP tasks. Pre-trained contextualized word representations are obtained through neural sentence encoders trained on a huge amount of raw texts. When the resulting sentence encoder is fine-"
D19-1533,D17-1120,0,0.252193,"slation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019) have been shown to improve downstream NLP tasks. Pre-trained contextualized word representations are obtained through neural sentence encoders trained on a huge amount of raw texts. When the resulting sentence encoder is fine-"
D19-1533,N15-1035,1,0.840435,"SD is an important task and it has been shown that WSD helps downstream NLP tasks, such as machine translation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019) have been shown to improve downstream NLP tasks. Pre-trained contextualized word representations are obtained through neural"
D19-1533,P10-1040,0,0.0718144,"xtualized word representation. Section 4 proposes different strategies to incorporate the contextualized word representation for WSD. Section 5 describes our experimental setup. Section 6 presents the experimental results. Section 7 discusses the findings from the experiments. Finally, Section 8 presents the conclusion. 2 Related Work Continuous word representations in real-valued vectors, or commonly known as word embeddings, have been shown to help improve NLP performance. Initially, exploiting continuous representations was achieved by adding real-valued vectors as classification features (Turian et al., 2010). Taghipour and Ng (2015) fine-tuned non-contextualized word embeddings by a feedforward neural network such that those word embeddings were more suited for WSD. The finetuned embeddings were incorporated into an SVM classifier. Iacobacci et al. (2016) explored different strategies of incorporating word embeddings and found that their best strategy involved exponential decay that decreased the contribution of surrounding word features as their distances to the target word increased. The neural sequence tagging approach has also been explored for WSD. K˚ageb¨ack and Salomonsson (2016) proposed"
D19-1533,P10-4014,1,0.408829,"asy for a human to infer the correct sense of a word given a context, it is a challenge for NLP systems. As such, WSD is an important task and it has been shown that WSD helps downstream NLP tasks, such as machine translation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neural network that captures the whole sentence and the word representation in the sentence. However, in both approaches, the word representations are independent of the context. Recently, pre-trained contextualized word representations (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019) have b"
D19-1533,P12-1029,1,0.892234,"1 Introduction Word sense disambiguation (WSD) automatically assigns a pre-defined sense to a word in a text. Different senses of a word reflect different meanings a word has in different contexts. Identifying the correct word sense given a context is crucial in natural language processing (NLP). Unfortunately, while it is easy for a human to infer the correct sense of a word given a context, it is a challenge for NLP systems. As such, WSD is an important task and it has been shown that WSD helps downstream NLP tasks, such as machine translation (Chan et al., 2007a) and information retrieval (Zhong and Ng, 2012). A WSD system assigns a sense to a word by taking into account its context, comprising the other words in the sentence. This can be done through discrete word features, which typically involve surrounding words and collocations trained using a classifier (Lee et al., 2004; Ando, 2006; Chan et al., 2007b; Zhong and Ng, 2010). The classifier can also make use of continuous word representations of the surrounding words (Taghipour and Ng, 2015; Iacobacci et al., 2016). Neural WSD systems (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017b) feed the continuous word representations into a neu"
I05-3025,W04-3236,1,0.765756,"valuated our Chinese word segmenter in the open track, on all four corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and Peking University (PKU). Based on a maximum entropy approach, our word segmenter achieved the highest F measure for AS, CITYU, and PKU, and the second highest for MSR. We found that the use of an external dictionary and additional training corpora of different segmentation standards helped to further improve segmentation accuracy. 1.1 Basic Features The basic features of our word segmenter are similar to our previous work (Ng and Low, 2004): (a) Cn (n = −2, −1, 0, 1, 2) (b) Cn Cn+1 (n = −2, −1, 0, 1) (c) C−1 C1 (d) P u(C0 ) (e) T (C−2 )T (C−1 )T (C0 )T (C1 )T (C2 ) 1 Chinese Word Segmenter The Chinese word segmenter we built is similar to the maximum entropy word segmenter we employed in our previous work (Ng and Low, 2004). Our word segmenter uses a maximum entropy framework (Ratnaparkhi, 1998; Xue and Shen, 2003) and is trained on manually segmented sentences. It classifies each Chinese character given the features derived from its surrounding context. Each Chinese character can be assigned one of four possible boundary tags:"
I05-3025,W03-1728,0,0.0598336,"ctionary and additional training corpora of different segmentation standards helped to further improve segmentation accuracy. 1.1 Basic Features The basic features of our word segmenter are similar to our previous work (Ng and Low, 2004): (a) Cn (n = −2, −1, 0, 1, 2) (b) Cn Cn+1 (n = −2, −1, 0, 1) (c) C−1 C1 (d) P u(C0 ) (e) T (C−2 )T (C−1 )T (C0 )T (C1 )T (C2 ) 1 Chinese Word Segmenter The Chinese word segmenter we built is similar to the maximum entropy word segmenter we employed in our previous work (Ng and Low, 2004). Our word segmenter uses a maximum entropy framework (Ratnaparkhi, 1998; Xue and Shen, 2003) and is trained on manually segmented sentences. It classifies each Chinese character given the features derived from its surrounding context. Each Chinese character can be assigned one of four possible boundary tags: s for a character that occurs as a single-character word, b for a character that begins a multi-character (i.e., two or more characters) word, e for a character that ends a multi-character word, and m for a character that is 161 In the above feature templates, C refers to a Chinese character. Templates (a) – (c) refer to a context of five characters (the current character and two"
J01-4004,P95-1017,0,0.0423032,"Missing"
J01-4004,W97-1306,0,0.0123528,"Missing"
J01-4004,W99-0611,0,0.0854217,"Missing"
J01-4004,M98-1001,0,0.527064,"rence task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets. 1. Introduction Coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world. It is an important subtask in natural language processing systems. In particular, information extraction (IE) systems like those built in the DARPA Message Understanding Conferences (Chinchor 1998; Sundheim 1995) have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6 (MUC-6 1995). In this paper, we focus on the task of determining coreference relations as defined in MUC-6 (MUC-6 1995) and MUC-7 (MUC-7 1997). Specifically, a coreference relation denotes an identity of reference and holds between two textual elements known as markables, which can be definite noun phrases, demonstrative noun phrases, proper names, appositives, sub-noun phrases that act as modifiers, pronouns, and s"
J01-4004,A88-1019,0,0.0305218,"s used, as shown in Figure 1. They consist of tokenization, sentence segmentation, morphological processing, part-of-speech tagging, noun phrase identification, named entity recognition, nested noun phrase extraction, and semantic class determination. As far as coreference resolution is concerned, the goal of these NLP modules is to determine the boundary of the markables, and to provide the necessary information about each markable for subsequent generation of features in the training examples. Our part-of-speech tagger is a standard statistical tagger based on the Hidden Markov Model (HMM) (Church 1988). Similarly, we built a statistical HMM-based noun phrase identification module that determines the noun phrase boundaries solely based on the part-of-speech tags assigned to the words in a sentence. We also implemented a module that recognizes MUC-style named entities, that is, organization, person, location, date, time, money, and percent. Our named entity recognition module uses the H M M approach of Bikel, Schwartz, and Weischedel (1999), which learns from a tagged corpus of named entities. That is, our part-of-speech tagger, noun phrase identification module, and named entity recognition"
J01-4004,M95-1011,0,0.0123628,"Missing"
J01-4004,W98-1119,0,0.0335581,"Missing"
J01-4004,W97-1307,0,0.0575395,"Missing"
J01-4004,W97-0319,0,0.0306697,"Missing"
J01-4004,J94-4002,0,0.541977,"Missing"
J01-4004,W97-1310,0,0.0442273,"Missing"
J01-4004,W97-1303,0,0.0762783,"Missing"
J01-4004,M95-1025,0,0.173063,"Missing"
J01-4004,M95-1002,0,0.0209283,"learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets. 1. Introduction Coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world. It is an important subtask in natural language processing systems. In particular, information extraction (IE) systems like those built in the DARPA Message Understanding Conferences (Chinchor 1998; Sundheim 1995) have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6 (MUC-6 1995). In this paper, we focus on the task of determining coreference relations as defined in MUC-6 (MUC-6 1995) and MUC-7 (MUC-7 1997). Specifically, a coreference relation denotes an identity of reference and holds between two textual elements known as markables, which can be definite noun phrases, demonstrative noun phrases, proper names, appositives, sub-noun phrases that act as modifiers, pronouns, and so on. Thus, our"
J01-4004,M95-1006,0,\N,Missing
J16-2004,P06-2005,0,0.0376243,"networks, chats, forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. Sajjad, Darwish, and Belinkov (2013) first normalized a dialectal Egyptian Arabic to look like Modern Standard Arabic, and then translated the transformed text to English. In fact, this is a more general problem, which arises with informal sources such as SMS messages and Tweets for just any language (Aw et al. 2006; Han and Baldwin 2011; Wang and Ng 2013; Bojja, Nedunchezhian, and Wang 2015). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language to another. A second relevant line of research is on language adaptation and normalization, when done specifically for improving SMT into another language. For example, Marujo et al. (2011) described a rule-based system for adapting Brazilian P"
J16-2004,baldwin-awab-2006-open,0,0.0335019,"Missing"
J16-2004,W05-0909,0,0.0268591,"xt. This is further reconfirmed by the sophisticated phrase table combination, which yields an additional absolute gain of 0.31 BLEU points. 6.2 Isolated Experiments Table 4 shows the results for the isolated experiments. We can see that word-level paraphrasing (CN:*) improves by up to 5.56 and 1.39 BLEU points over the two baselines (both results are statistically significant). Compared with ML2EN, CN:word yields an absolute improvement of 4.41 BLEU points, CN:word0 adds another 0.59, and CN:word0 +morph adds 0.56 more. The scores for TER (v. 0.7.25) (Snover et al. 2006) and METEOR (v. 1.3) (Banerjee and Lavie 2005) are on par with those for BLEU (NIST v. 13). Table 4 further shows that the optimal parameters for the word-level systems involve a very low probability cut-off, and a high number of n-best sentences. This indicates that they are robust to noise, probably because bad source-side phrases are Table 4 Isolated experiments. The subscript shows the parameters found on IN2EN-dev and used for IN2EN-test. The superscript shows the absolute test improvement over the ML2EN and the IN2EN baselines. Scores that are statistically significantly better than ML2EN and IN2EN (p &lt; 0.01, Collins’ sign test) are"
J16-2004,W07-0702,0,0.0758696,"Missing"
J16-2004,2015.mtsummit-users.2,1,0.821723,"Missing"
J16-2004,J93-2003,0,0.136161,"Missing"
J16-2004,N06-1003,0,0.125458,"Missing"
J16-2004,P07-1092,0,0.124127,"did not attempt language adaptation, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; because it does not substitute a word with a completely different word, transliteration did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bitext, it makes sense to try to combine it further with the small bitext; thus, in the following we will directly compare and combine these two approaches. One alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Cohn and Lapata 2007; Utiyama and Isahara 2007; Wu and Wang 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bitext, which we do not have. Pivoting 279 Computational Linguistics Volume 42, Number 2 over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bitext)"
J16-2004,P05-1066,0,0.154994,"Missing"
J16-2004,D10-1041,0,0.0471951,"Missing"
J16-2004,2007.iwslt-1.28,0,0.0181645,"xt to get closer to Indonesian. We use monotone translation, that is, we allow no phrase reordering. We tune the parameters of the log-linear model on a development set using minimum error rate training (MERT) (Och 2003). 4.2.2 Cross-Lingual Morphological Variants. Although phrase-level paraphrasing models context better, it remains limited in the size of its Indonesian vocabulary by the small Indonesian–English bitext, just like word-level paraphrasing. We address this by transforming the Indonesian sentences in the development and the test Indonesian–English bitexts into confusion networks (Dyer 2007; Du, Jiang, and Way 2010), where we add Malay morphological variants for the Indonesian words, weighting them based on Equation (2). Note that we do not alter the training bitext; we just transform the source side of the development and the test data sets into confusion networks. 4.3 Text Rewriting with a Specialized Decoder In this section, we introduce a third approach to source language adaptation, which uses a text rewriting decoder to iteratively find the best adaptation for an input sentence. We first discuss the differences between traditional left-to-right decoders and the text rewrit"
J16-2004,2013.mtsummit-papers.23,0,0.0880309,"Missing"
J16-2004,W07-0717,0,0.0260779,"the adapted bitexts. This work leaves several interesting directions for future research: r r 302 One direction is to add more word editing operations, for example, word deletion, insertion, splitting, and concatenation (because we mainly focused on word substitution in this study). Another promising direction is to add more sentence-level feature functions to the text rewriting decoder to further improve language adaptation. Wang, Nakov, and Ng r r r r r Source Language Adaptation for Resource-Poor MT Future work could also experiment with other phrase table combination methods, for example, Foster and Kuhn (2007) proposed a mixture model whose weights are learned with an EM algorithm (Foster, Chen, and Kuhn 2013). Another direction is to add word reordering. In the current work, we assume no word reordering is necessary (apart from what can be achieved within a phrase), but there actually can exist word-order differences between closely related languages. A further direction is to utilize the relationships between the source and the target sides of the input resource-rich bitext to perform language adaptation, since only the source side was used in our current work. For example, Malay–Indonesian adapt"
J16-2004,A00-1002,0,0.179914,"Missing"
J16-2004,P11-1038,0,0.011923,"forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. Sajjad, Darwish, and Belinkov (2013) first normalized a dialectal Egyptian Arabic to look like Modern Standard Arabic, and then translated the transformed text to English. In fact, this is a more general problem, which arises with informal sources such as SMS messages and Tweets for just any language (Aw et al. 2006; Han and Baldwin 2011; Wang and Ng 2013; Bojja, Nedunchezhian, and Wang 2015). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language to another. A second relevant line of research is on language adaptation and normalization, when done specifically for improving SMT into another language. For example, Marujo et al. (2011) described a rule-based system for adapting Brazilian Portuguese (BP) to Euro"
J16-2004,P13-4033,0,0.0293805,"Missing"
J16-2004,D11-1125,0,0.019765,"cal mapping, that is, the summation of the logarithms of all morphological variant mapping scores (see Equation (2)) used so far 4.3.5 Model. We use a log-linear model, which combines all features to obtain the score for a hypothesis h as follows: score(h) = X λi fi (h) (3) i where fi is the ith feature function with weight λi . The text rewriting decoder prunes bad hypotheses based on score(h); it also selects the best hypothesis as the one with the highest score(h) across all beams. We tune the weights of the feature functions on a development set using pairwise ranking optimization or PRO (Hopkins and May 2011). We optimize BLEU+1 (Liang et al. 2006), a sentence-level approximation of BLEU, as is standard with PRO. 4.4 Combining Bitexts We have presented our source language adaptation approaches in Sections 4.1, 4.2, and 4.3. Now we explain how we combine the Indonesian–English bitext with the synthetic “Indonesian”–English bitext we have generated. We consider the following three bitext combination approaches: Simple concatenation. Assuming the two bitexts are of comparable quality, we simply train an SMT system on their concatenation. Balanced concatenation with repetitions. The two bitexts are no"
J16-2004,2011.eamt-1.19,0,0.834667,"Missing"
J16-2004,N04-1034,0,0.0985059,"Missing"
J16-2004,D09-1141,1,0.872483,"Missing"
J16-2004,P12-2059,1,0.877922,"Missing"
J16-2004,P03-1021,0,0.195581,"English bitexts. We then pivot over the English phrases to generate Indonesian– Malay phrase pairs. As in the case of word-level pivoting, we derive the paraphrase probabilities from the corresponding probabilities in the two phrase tables, again using Equation (1). We then use the Moses phrase-based SMT decoder (Koehn et al. 2007) to “translate” the Malay side of the Malay–English bitext to get closer to Indonesian. We use monotone translation, that is, we allow no phrase reordering. We tune the parameters of the log-linear model on a development set using minimum error rate training (MERT) (Och 2003). 4.2.2 Cross-Lingual Morphological Variants. Although phrase-level paraphrasing models context better, it remains limited in the size of its Indonesian vocabulary by the small Indonesian–English bitext, just like word-level paraphrasing. We address this by transforming the Indonesian sentences in the development and the test Indonesian–English bitexts into confusion networks (Dyer 2007; Du, Jiang, and Way 2010), where we add Malay morphological variants for the Indonesian words, weighting them based on Equation (2). Note that we do not alter the training bitext; we just transform the source s"
J16-2004,J03-1002,0,0.0396448,"adapted “Indonesian” sentences. In the following we first describe how we generate the word-level Indonesian options and the corresponding weights for the Malay words. Then, we explain how we build, decode, and improve the confusion network. 4.1.1 Inducing Word-Level Paraphrases. We use pivoting over English to induce potential Indonesian word translations for a given Malay word. First, we build separate directed word alignments for the Malay–English bitext and for the Indonesian–English bitext using IBM model 4 (Brown et al. 1993), and then we combine them using the intersect+grow heuristic (Och and Ney 2003). We then induce Malay–Indonesian word translation pairs assuming that if an Indonesian word i and a Malay word m are aligned to the same English word e, they could be mutual translations. Each translation pair is associated with a conditional probability, estimated by pivoting over English: Pr(i|m) = X Pr(i|e)Pr(e|m) (1) e Pr(i|e) and Pr(e|m) are estimated using maximum likelihood from the word alignments. Following Callison-Burch, Koehn, and Osborne (2006), we further assume that i is conditionally independent of m given e. 282 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor"
J16-2004,P02-1040,0,0.0957287,"res (Koehn 2013): forward and reverse translation probabilities, forward and reverse lexicalized phrase translation probabilities, and a phrase penalty. We further used a 5-gram language model trained using the SRILM toolkit (Stolcke 2002) with modified Kneser-Ney smoothing (Kneser and Ney 1995). We combined all features in a log-linear model, namely: (1) the five features in the phrase table, (2) a language model score, (3) a word penalty, that is, the number of words in the output translation, and (4) distance-based reordering cost. We tuned the weights of these features by optimizing BLEU (Papineni et al. 2002) on the development set IN2EN-dev using MERT (Och 2003), and we used them for translation with the phrase-based SMT decoder of Moses. We evaluated all systems on the same test set, IN2EN-test. 3 http://translate.google.com/. 292 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor MT 5.3 Isolated Experiments In the isolated experiments, we train the SMT system on the adapted “Indonesian”– English bitext only, which allows for a direct comparison to using ML2EN or IN2EN only. 5.3.1 Using Word-Level Paraphrases. In our word-level paraphrasing experiments, we adapted Malay to Indonesi"
J16-2004,P13-2001,0,0.0300217,"Missing"
J16-2004,W11-2602,0,0.0155359,"st, we have a different objective: We do not carry out full 278 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor MT translation but rather adaptation (since our ultimate goal is to translate into a third language X). A special case of this same line of research is the translation between dialects of the same language, for example, between Cantonese and Mandarin (Zhang 1998), or between a dialect of a language and a standard version of that language, for example, between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr, Shaalan, and Ziedan 2008; Sawaf 2010; Salloum and Habash 2011; Sajjad, Darwish, and Belinkov 2013). Here again, manual rules and/or language-specific tools and resources are typically used. In the case of Arabic dialects, a further complication arises due to the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online media such as social networks, chats, forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other thing"
J16-2004,2010.amta-papers.5,0,0.021765,"). In contrast, we have a different objective: We do not carry out full 278 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor MT translation but rather adaptation (since our ultimate goal is to translate into a third language X). A special case of this same line of research is the translation between dialects of the same language, for example, between Cantonese and Mandarin (Zhang 1998), or between a dialect of a language and a standard version of that language, for example, between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr, Shaalan, and Ziedan 2008; Sawaf 2010; Salloum and Habash 2011; Sajjad, Darwish, and Belinkov 2013). Here again, manual rules and/or language-specific tools and resources are typically used. In the case of Arabic dialects, a further complication arises due to the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online media such as social networks, chats, forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic re"
J16-2004,D08-1090,0,0.0327916,"Missing"
J16-2004,2006.amta-papers.25,0,0.0219,"N2EN a proper weight in the combined bitext. This is further reconfirmed by the sophisticated phrase table combination, which yields an additional absolute gain of 0.31 BLEU points. 6.2 Isolated Experiments Table 4 shows the results for the isolated experiments. We can see that word-level paraphrasing (CN:*) improves by up to 5.56 and 1.39 BLEU points over the two baselines (both results are statistically significant). Compared with ML2EN, CN:word yields an absolute improvement of 4.41 BLEU points, CN:word0 adds another 0.59, and CN:word0 +morph adds 0.56 more. The scores for TER (v. 0.7.25) (Snover et al. 2006) and METEOR (v. 1.3) (Banerjee and Lavie 2005) are on par with those for BLEU (NIST v. 13). Table 4 further shows that the optimal parameters for the word-level systems involve a very low probability cut-off, and a high number of n-best sentences. This indicates that they are robust to noise, probably because bad source-side phrases are Table 4 Isolated experiments. The subscript shows the parameters found on IN2EN-dev and used for IN2EN-test. The superscript shows the absolute test improvement over the ML2EN and the IN2EN baselines. Scores that are statistically significantly better than ML2E"
J16-2004,W13-5634,0,0.0471894,"Missing"
J16-2004,N07-1061,0,0.109504,"ge adaptation, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; because it does not substitute a word with a completely different word, transliteration did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bitext, it makes sense to try to combine it further with the small bitext; thus, in the following we will directly compare and combine these two approaches. One alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Cohn and Lapata 2007; Utiyama and Isahara 2007; Wu and Wang 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bitext, which we do not have. Pivoting 279 Computational Linguistics Volume 42, Number 2 over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bitext). Yet another alternative"
J16-2004,D12-1027,1,0.658828,"Missing"
J16-2004,N13-1050,1,0.890749,"Missing"
J16-2004,P09-1018,0,0.0204545,"very simple transliteration for Portuguese–Spanish that ignored context entirely; because it does not substitute a word with a completely different word, transliteration did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bitext, it makes sense to try to combine it further with the small bitext; thus, in the following we will directly compare and combine these two approaches. One alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Cohn and Lapata 2007; Utiyama and Isahara 2007; Wu and Wang 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bitext, which we do not have. Pivoting 279 Computational Linguistics Volume 42, Number 2 over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bitext). Yet another alternative approach for improv"
J16-2004,P98-2238,0,0.100485,"age pairs including Czech– Slovak (Hajiˇc, Hric, and Kubonˇ 2000), Turkish–Crimean Tatar (Altintas and Cicekli 2002), Irish–Scottish Gaelic (Scannell 2006), and Macedonian–Bulgarian (Nakov and Tiedemann 2012). In contrast, we have a different objective: We do not carry out full 278 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor MT translation but rather adaptation (since our ultimate goal is to translate into a third language X). A special case of this same line of research is the translation between dialects of the same language, for example, between Cantonese and Mandarin (Zhang 1998), or between a dialect of a language and a standard version of that language, for example, between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr, Shaalan, and Ziedan 2008; Sawaf 2010; Salloum and Habash 2011; Sajjad, Darwish, and Belinkov 2013). Here again, manual rules and/or language-specific tools and resources are typically used. In the case of Arabic dialects, a further complication arises due to the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online media such as social networks, chats, fo"
J16-2004,C98-2233,0,\N,Missing
J16-2004,P07-2045,0,\N,Missing
J16-2004,P06-1096,0,\N,Missing
J16-2004,D12-1108,0,\N,Missing
K15-1037,S13-2050,0,0.0161883,"ne. Finally, it can be observed that in most cases, adding training instances from MUN significantly improves IMS (SC) and IMS (SC+DSO). We evaluated our system using all measures used in the shared task. The results are presented in Table 5. The columns in this table denote the scores of the various systems according to the different evaluation metrics used in the WSI shared task, which are Jaccard Index, Kδsim , WNDCG, Fuzzy NMI, and Fuzzy B-Cubed. See (Jurgens and Klapaftis, 2013) for details of the evaluation metrics. This table also includes the top two systems in the shared task, AI-KU (Baskaya et al., 2013) and Unimelb (Lau et al., 2013), as well as Wang-15 (Wang et al., 2015). AI-KU uses a language model to find the most likely substitutes for a target word to represent the context. The clustering method used in AI-KU is K-means and the system gives good performance in the shared task. Unimelb relies on Hierarchical Dirichlet Process (Teh et al., 2006) to identify the sense of a target word using positional word features. Finally, Wang-15 uses Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model the word sense and topic jointly. This system obtains high scores, according to Fuzzy B-Cu"
K15-1037,Q14-1019,0,0.469097,"orming several task submissions and strong baselines. 1 Introduction Identifying the meaning of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. 338 Proceedings of the 19th Conference on Computational Language Learning, pages 338–344, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 4. Annotation: In order to assign a WordNet sense tag to an English word we in a sentence, we make use of the aligned Chinese translation wc of we , based on the automatic word alignment formed b"
K15-1037,S07-1006,0,0.0260963,"a supervised WSD system based on support vector machines (SVM). This WSD system comes with outof-the-box pre-trained models. However, since the original training set is not released, we use our own training set (see Section 2) to train IMS and then evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 (Pradhan et al., 2007; Navigli et al., 2007). These all-words WSD shared tasks provide no training data to the participants. The selected word sense induction task in our experiments is 3.2 SemEval-2013 Word Sense Induction Task In order to evaluate our system on a word sense induction task, we selected SemEval-2013 task 13, the latest WSI shared task. Unlike most other tasks that assume a single sense is sufficient for representing word senses, this task allows each instance to be associated with multiple sense labels with their applicability weights. This WSI task considers 50 lemmas, including 20 nouns, 20 verbs, and 10 adjectives, a"
K15-1037,P04-1039,0,0.0491074,"for Word Sense Disambiguation and Induction Kaveh Taghipour Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 kaveh@comp.nus.edu.sg Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg Abstract To overcome these limitations, automatic methods have been developed for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsupervised bootstrapping method to automatically generate a senseannotated dataset. Another example of automatically created datasets is the semi-supervised method used in (K¨ubler and Zhekova, 2009), which employed a supervised classifier to label instances. The two main contributions of this paper are as follows. First, we employ the same method used in (Ng et al., 2003; Chan and Ng, 2005) to semi-automatically annotate one million training samples based on the WordNet sense inventory (Miller, 1995) and release the annotated corpus for public use. To our knowledge, this annota"
K15-1037,S01-1001,0,0.87132,"e-tagged training set can be found in Table 1 to Table 3. 3 WSD All-Words Tasks Evaluation For the WSD system, we use IMS (Zhong and Ng, 2010) in our experiments. IMS is a supervised WSD system based on support vector machines (SVM). This WSD system comes with outof-the-box pre-trained models. However, since the original training set is not released, we use our own training set (see Section 2) to train IMS and then evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 (Pradhan et al., 2007; Navigli et al., 2007). These all-words WSD shared tasks provide no training data to the participants. The selected word sense induction task in our experiments is 3.2 SemEval-2013 Word Sense Induction Task In order to evaluate our system on a word sense induction task, we selected SemEval-2013 task 13, the latest WSI shared task. Unlike most other tasks that assume a single sense is sufficient for representing word senses, this task allows each instan"
K15-1037,P96-1006,1,0.14898,"of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. 338 Proceedings of the 19th Conference on Computational Language Learning, pages 338–344, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 4. Annotation: In order to assign a WordNet sense tag to an English word we in a sentence, we make use of the aligned Chinese translation wc of we , based on the automatic word alignment formed by GIZA++. For each sense i of we in the WordNet sense inventory (WordNet 1.7.1), a list of"
K15-1037,eisele-chen-2010-multiun,0,0.0187266,"Missing"
K15-1037,P03-1058,1,0.21761,"veloped for annotating training samples. For example, Ng et al. (2003), Chan and Ng (2005), and Zhong and Ng (2009) used Chinese-English parallel corpora to extract samples for training their supervised WSD system. Diab (2004) proposed an unsupervised bootstrapping method to automatically generate a senseannotated dataset. Another example of automatically created datasets is the semi-supervised method used in (K¨ubler and Zhekova, 2009), which employed a supervised classifier to label instances. The two main contributions of this paper are as follows. First, we employ the same method used in (Ng et al., 2003; Chan and Ng, 2005) to semi-automatically annotate one million training samples based on the WordNet sense inventory (Miller, 1995) and release the annotated corpus for public use. To our knowledge, this annotated set of sense-tagged samples is the largest publicly available dataset for word sense disambiguation. Second, we train an open source supervised WSD system, IMS (Zhong and Ng, 2010), using our data and evaluate it against standard WSD and WSI benchmarks. We show that our system outperforms other state-of-the-art systems in most cases. As any WSD system is also a WSI system when we tr"
K15-1037,S13-2049,0,0.0965373,"gged samples is the largest publicly available dataset for word sense disambiguation. Second, we train an open source supervised WSD system, IMS (Zhong and Ng, 2010), using our data and evaluate it against standard WSD and WSI benchmarks. We show that our system outperforms other state-of-the-art systems in most cases. As any WSD system is also a WSI system when we treat the pre-defined sense inventory of the WSD system as the induced word senses, a WSD system can also be evaluated and used for WSI. Some researchers believe that, in some cases, WSI methods may perform better than WSD systems (Jurgens and Klapaftis, 2013; Wang et al., 2015). However, we argue that WSI systems have few advantages compared to WSD methods and according to our results, disambiguation systems consistently outperform induction systems. Although there are some cases where WSI systems can be useful (e.g., for resource-poor languages), in most cases WSD systems are preferable because Supervised word sense disambiguation (WSD) systems are usually the best performing systems when evaluated on standard benchmarks. However, these systems need annotated training data to function properly. While there are some publicly available open source"
K15-1037,P00-1056,0,0.0293057,"f senses with fewer than 500 samples are included in the training data. This sampling method ensures that rare sense tags also have training samples during the selection process. In order to improve the coverage of the training set, we augment it by adding samples from SEMCOR (SC) (Miller et al., 1993) and the DSO cor1. Tokenization and word segmentation: The English side of the corpus is tokenized using the Penn TreeBank tokenizer3 , while the Chinese side of the corpus is segmented using the Chinese word segmenter of (Low et al., 2005). 2. Word alignment: After tokenizing the texts, GIZA++ (Och and Ney, 2000) is used to align English and Chinese words. 3. Part-of-speech (POS) tagging and lemmatization: After running GIZA++, we use the OpenNLP POS tagger4 and then the WordNet lemmatizer to obtain POS tags and word lemmas of the English sentence. 1 http://www.euromatrixplus.eu/multi-un http://opus.lingfil.uu.se/MultiUN.php 3 http://www.cis.upenn.edu/∼treebank/tokenization.html 4 http://opennlp.apache.org 2 339 MUN (before sampling) MUN MUN+SC MUN+SC+DSO Avg. # samples per word type 19,837.6 852.5 55.4 63.7 SemEval-2013 task 13 (Jurgens and Klapaftis, 2013). 3.1 The results of our experiments on WSD"
K15-1037,R09-1037,0,0.0594013,"Missing"
K15-1037,passonneau-etal-2012-masc,0,0.0595302,"cally has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. 338 Proceedings of the 19th Conference on Computational Language Learning, pages 338–344, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 4. Annotation: In order to assign a WordNet sense tag to an English word we in a sentence, we make use of the aligned Chinese translation wc of we , based on the automatic word alignment formed by GIZA++. For each sense i of we in the WordNet sense inventory (WordNet 1.7.1), a list of Chinese translations of se"
K15-1037,S13-2051,0,0.0627155,"Missing"
K15-1037,P10-1154,0,0.0709325,"Missing"
K15-1037,W04-0834,1,0.605366,"urce IMS WSD system trained on our dataset achieves stateof-the-art results in standard disambiguation tasks and a recent word sense induction task, outperforming several task submissions and strong baselines. 1 Introduction Identifying the meaning of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. 338 Proceedings of the 19th Conference on Computational Language Learning, pages 338–344, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 4. Annotation: In order to assign a W"
K15-1037,S07-1016,0,0.064172,"r experiments. IMS is a supervised WSD system based on support vector machines (SVM). This WSD system comes with outof-the-box pre-trained models. However, since the original training set is not released, we use our own training set (see Section 2) to train IMS and then evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 (Pradhan et al., 2007; Navigli et al., 2007). These all-words WSD shared tasks provide no training data to the participants. The selected word sense induction task in our experiments is 3.2 SemEval-2013 Word Sense Induction Task In order to evaluate our system on a word sense induction task, we selected SemEval-2013 task 13, the latest WSI shared task. Unlike most other tasks that assume a single sense is sufficient for representing word senses, this task allows each instance to be associated with multiple sense labels with their applicability weights. This WSI task considers 50 lemmas, including 20 nouns, 20 verb"
K15-1037,W04-0811,0,0.0575852,"to Table 3. 3 WSD All-Words Tasks Evaluation For the WSD system, we use IMS (Zhong and Ng, 2010) in our experiments. IMS is a supervised WSD system based on support vector machines (SVM). This WSD system comes with outof-the-box pre-trained models. However, since the original training set is not released, we use our own training set (see Section 2) to train IMS and then evaluate it on standard WSD and WSI benchmarks. This section presents the results obtained on four WSD and one WSI shared tasks. The four all-words WSD shared tasks are SensEval-2 (Edmonds and Cotton, 2001), SensEval-3 task 1 (Snyder and Palmer, 2004), and both the fine-grained task 17 and coarse-grained task 7 of SemEval2007 (Pradhan et al., 2007; Navigli et al., 2007). These all-words WSD shared tasks provide no training data to the participants. The selected word sense induction task in our experiments is 3.2 SemEval-2013 Word Sense Induction Task In order to evaluate our system on a word sense induction task, we selected SemEval-2013 task 13, the latest WSI shared task. Unlike most other tasks that assume a single sense is sufficient for representing word senses, this task allows each instance to be associated with multiple sense label"
K15-1037,I05-3025,1,0.744323,"s and limit the number of samples per sense to 500. However, all samples of senses with fewer than 500 samples are included in the training data. This sampling method ensures that rare sense tags also have training samples during the selection process. In order to improve the coverage of the training set, we augment it by adding samples from SEMCOR (SC) (Miller et al., 1993) and the DSO cor1. Tokenization and word segmentation: The English side of the corpus is tokenized using the Penn TreeBank tokenizer3 , while the Chinese side of the corpus is segmented using the Chinese word segmenter of (Low et al., 2005). 2. Word alignment: After tokenizing the texts, GIZA++ (Och and Ney, 2000) is used to align English and Chinese words. 3. Part-of-speech (POS) tagging and lemmatization: After running GIZA++, we use the OpenNLP POS tagger4 and then the WordNet lemmatizer to obtain POS tags and word lemmas of the English sentence. 1 http://www.euromatrixplus.eu/multi-un http://opus.lingfil.uu.se/MultiUN.php 3 http://www.cis.upenn.edu/∼treebank/tokenization.html 4 http://opennlp.apache.org 2 339 MUN (before sampling) MUN MUN+SC MUN+SC+DSO Avg. # samples per word type 19,837.6 852.5 55.4 63.7 SemEval-2013 task 1"
K15-1037,H93-1061,0,0.869672,"ntifying the meaning of a word automatically has been an interesting research topic for a few decades. The approaches used to solve this problem can be roughly categorized into two main classes: Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) (Navigli, 2009). For word sense disambiguation, some systems are based on supervised machine learning algorithms (Lee et al., 2004; Zhong and Ng, 2010), while others use ontologies and other structured knowledge sources (Ponzetto and Navigli, 2010; Agirre et al., 2014; Moro et al., 2014). There are several sense-annotated datasets for WSD (Miller et al., 1993; Ng and Lee, 1996; Passonneau et al., 2012). However, these datasets either include few samples per word sense or only cover a small set of polysemous words. 338 Proceedings of the 19th Conference on Computational Language Learning, pages 338–344, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 4. Annotation: In order to assign a WordNet sense tag to an English word we in a sentence, we make use of the aligned Chinese translation wc of we , based on the automatic word alignment formed by GIZA++. For each sense i of we in the WordNet sense inventory (WordNet"
K15-1037,Q15-1005,0,0.0419373,"ublicly available dataset for word sense disambiguation. Second, we train an open source supervised WSD system, IMS (Zhong and Ng, 2010), using our data and evaluate it against standard WSD and WSI benchmarks. We show that our system outperforms other state-of-the-art systems in most cases. As any WSD system is also a WSI system when we treat the pre-defined sense inventory of the WSD system as the induced word senses, a WSD system can also be evaluated and used for WSI. Some researchers believe that, in some cases, WSI methods may perform better than WSD systems (Jurgens and Klapaftis, 2013; Wang et al., 2015). However, we argue that WSI systems have few advantages compared to WSD methods and according to our results, disambiguation systems consistently outperform induction systems. Although there are some cases where WSI systems can be useful (e.g., for resource-poor languages), in most cases WSD systems are preferable because Supervised word sense disambiguation (WSD) systems are usually the best performing systems when evaluated on standard benchmarks. However, these systems need annotated training data to function properly. While there are some publicly available open source WSD systems, very f"
K15-1037,P10-4014,1,0.715688,"rvised method used in (K¨ubler and Zhekova, 2009), which employed a supervised classifier to label instances. The two main contributions of this paper are as follows. First, we employ the same method used in (Ng et al., 2003; Chan and Ng, 2005) to semi-automatically annotate one million training samples based on the WordNet sense inventory (Miller, 1995) and release the annotated corpus for public use. To our knowledge, this annotated set of sense-tagged samples is the largest publicly available dataset for word sense disambiguation. Second, we train an open source supervised WSD system, IMS (Zhong and Ng, 2010), using our data and evaluate it against standard WSD and WSI benchmarks. We show that our system outperforms other state-of-the-art systems in most cases. As any WSD system is also a WSI system when we treat the pre-defined sense inventory of the WSD system as the induced word senses, a WSD system can also be evaluated and used for WSI. Some researchers believe that, in some cases, WSI methods may perform better than WSD systems (Jurgens and Klapaftis, 2013; Wang et al., 2015). However, we argue that WSI systems have few advantages compared to WSD methods and according to our results, disambi"
K15-1037,J14-1003,0,\N,Missing
K15-2001,P09-1075,0,0.0149588,"course of the sixteen CoNLL shared 1 http://www.seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative a"
K15-2001,P12-1007,0,0.0215894,"omputational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the com"
K15-2001,P14-1048,0,0.0131111,"tational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “d"
K15-2001,W12-1622,0,0.047129,"anguage Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain cruci"
K15-2001,P14-1002,0,0.0686762,"re generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learn"
K15-2001,K15-2004,0,0.0849821,"Missing"
K15-2001,P13-2013,0,0.189614,"uly 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard”"
K15-2001,P13-1047,0,0.0359978,"ociation for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based lear"
K15-2001,W14-4320,0,0.0531135,"of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The re"
K15-2001,K15-2006,0,0.17442,"Missing"
K15-2001,P14-1003,0,0.00827391,"or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The rest of this overvi"
K15-2001,K15-2007,0,0.107459,"Missing"
K15-2001,D09-1036,1,0.729252,"Missing"
K15-2001,P14-5010,0,0.00276217,", 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the trainBrown clusters VerbNet Sentiment lexicon Word embeddings (word2vec) • Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014)) As it turned out, all of the teams this year chose to participate in the closed track. 4.2 Evaluation Platform: TIRA We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. This year, however, we shifted this evaluation paradigm, asking participants to deploy their systems on a remote virtual machine, and to use the TIRA web platform (tira.io) to run their syste"
K15-2001,W11-1901,1,0.0252812,"Nianwen Xue∗ Hwee Tou Ng† Sameer Pradhan‡ Rashmi Prasad3 Christopher Bryant† Attapol T. Rutherford∗ ∗ Brandeis University xuen,tet@brandeis.edu † National University of Singapore nght,bryant@comp.nus.edu.sg ‡ Boulder Language Technologies pradhan@bltek.com 3 University of Wisconsin-Milwaukee prasadr@uwm.edu Abstract tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks ("
K15-2001,J93-2004,0,0.058942,"if present; 2. Identify the spans of text that serve as the two arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments; 4. Predict the sense of the discourse relation (e.g., “Cause”, “Condition”, “Contrast”). 3 Data 3.1 Training and Development The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse TreeBank 2.0. (PDTB-2.0.) (Prasad et al., 2008; Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005). The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text. Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two. A limited but well-defined set of implicit relations are also annotated within sentences. Arguments of relations are annotated in each case, f"
K15-2001,W12-4501,1,0.544745,"Ng† Sameer Pradhan‡ Rashmi Prasad3 Christopher Bryant† Attapol T. Rutherford∗ ∗ Brandeis University xuen,tet@brandeis.edu † National University of Singapore nght,bryant@comp.nus.edu.sg ‡ Boulder Language Technologies pradhan@bltek.com 3 University of Wisconsin-Milwaukee prasadr@uwm.edu Abstract tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et"
K15-2001,W04-2703,1,0.817359,"Missing"
K15-2001,W15-0210,1,0.806958,"shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annot"
K15-2001,K15-2010,0,0.14262,"Missing"
K15-2001,prasad-etal-2008-penn,1,0.768574,"course relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.1 The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text. A discourse relation can be expressed explicitly or imp"
K15-2001,J05-1004,0,0.00719659,"wo arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments; 4. Predict the sense of the discourse relation (e.g., “Cause”, “Condition”, “Contrast”). 3 Data 3.1 Training and Development The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse TreeBank 2.0. (PDTB-2.0.) (Prasad et al., 2008; Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005). The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text. Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two. A limited but well-defined set of implicit relations are also annotated within sentences. Arguments of relations are annotated in each case, following the minimality principle for selecting all and only t"
K15-2001,J14-4007,1,0.868703,"een discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.1 The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text. A discourse relation can be expressed explicitly or implicitly, and takes two"
K15-2001,W12-1614,0,0.349783,"ared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the rela"
K15-2001,E14-1068,1,0.369296,"s. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The rest of this overview paper is structured as follows. In Section"
K15-2001,N07-1051,0,0.00516358,"like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the trainBrown clusters VerbNet Sentiment lexicon Word embeddings (word2vec) • Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014)) As it turned out, all of the teams this year chose to participate in the closed track. 4.2 Evaluation Platform: TIRA We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. This year, however, we shifted this evaluation paradigm, asking participants t"
K15-2001,K15-2012,0,0.0750505,"Missing"
K15-2001,C08-2022,0,0.230737,"arsing (SDP). In the course of the sixteen CoNLL shared 1 http://www.seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques o"
K15-2001,P09-1077,0,0.665554,"seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared tas"
K15-2001,N09-1064,0,0.0519034,"Missing"
K15-2001,K15-2002,0,0.195871,"Standard “shallow” architectures typically make use of discrete features while neural networks generally use continuous real-valued features such as word and paragraph embeddings. For discourse connective and argument extraction, token level features extracted from a fixed window centered on the target word token are generally used, and so are features extracted from syntactic parses. Distributional representations such as Brown clusters have generally been used to determine the senses (Chiarcos and Schenk, 2015; Devi et al., 2015; Kong et al., 2015; Song et al., 2015; Stepanov et al., 2015; Wang and Lan, 2015; Wang et al., 2015; Yoshida et al., 2015), although one team also used them in the sequence labeling task for argument extraction (Nguyen et al., 2015). Additional resources used by some systems for sense determination include word embeddings (Chiarcos and Schenk, 2015; Wang et al., 2015), VerbNet classes (Devi et al., 2015; Kong et al., 2015), and the MPQA polarity lexicon (Devi et al., 2015; Kong et al., 2015; Wang and Lan, 2015). Table 4 provides a summary of the different approaches. 6 Results Table 5 shows the performance of all participating systems across the three test evaluation sets"
K15-2001,C12-1168,0,0.04565,"c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and"
K15-2001,K15-2014,0,0.132851,"Missing"
K15-2001,K15-2015,0,0.0995344,"Missing"
K15-2001,C10-2172,0,0.24464,"nth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an idea"
K15-2001,W01-1605,0,\N,Missing
K15-2001,K15-2003,0,\N,Missing
K16-2001,K16-2003,0,0.135731,"nvolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes, MaxEnt word embeddings no no cip2016 (Kang et al., 2016) Institute of Automation, CAS syntactic parses, MPQA subjectivity, VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitted a system description paper are marked with ∗. subtask are represented in this competition. One is to collect all candidate discourse connective by looking up a list of possible connectives compiled from the training data and train a classifier to disambiguate them. There are two variants in this approach: one strategy is to train a"
K16-2001,K16-2015,0,0.0587712,"6) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes, MaxEnt word embeddings no no cip2016 (Kang et al., 2016) Institute of Automation, CAS syntactic parses, MPQA subjectivity, VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitt"
K16-2001,K16-2018,0,0.0493477,"Missing"
K16-2001,P13-2013,0,0.0614398,"unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse relation senses, participants have generally adopted “conventional” machine learning techniques such as SVM and MaxEnt models that rely on manually designed features. Explicit discourse relation senses can be predicted with high accuracy. The main challenge is predicting implicit discourse relation senses, which has received a considerable amount of attention in recent years (Pitler et al., 2009; Biran and McKeown, 2013; Rutherford and Xue, 2014). Determining implicit discourse relation senses relies on information from the two arguments of the relation. For this subtask, there is a good balance between “conventional” machine learning techniques such as Support Vector Machines and Maximum Entropy models that rely heavily on handcrafted features, and neural network based approaches. A wide variety of features have been used for this subtask, and they include features extracted from syntactic parses (Kang et al., 2016; Kong et al., 2016; Stepanov and Riccardi, 2016; Jain and Majumder, 2016; Wang and Lan, 2016;"
K16-2001,K16-2016,0,0.0652388,"olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labeling and sense) SVM and Maxent (Scikit-learn) syntactic parses, Brown clusters none Word embeddings, parse trees,"
K16-2001,K15-2004,0,0.140524,"Missing"
K16-2001,K16-2009,0,0.166348,"IST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labe"
K16-2001,K16-2021,0,0.296275,"lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labeling and sense) SVM and Maxent (Scikit-learn)"
K16-2001,K16-2013,0,0.191078,"tional overhead. On TIRA, the blind test set can only be accessed in the Connective identification The identification of discourse connectives is not a simple dictionary lookup as some discourse connective expressions are ambiguous and may function as discourse connectives in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia T"
K16-2001,K16-2011,0,0.0335294,"Missing"
K16-2001,K16-2017,0,0.107933,"ay function as discourse connectives in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for i"
K16-2001,K16-2008,0,0.0961919,"Missing"
K16-2001,K16-2022,0,0.0226855,"Missing"
K16-2001,P09-1077,0,0.372145,"ourse connectives are unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse relation senses, participants have generally adopted “conventional” machine learning techniques such as SVM and MaxEnt models that rely on manually designed features. Explicit discourse relation senses can be predicted with high accuracy. The main challenge is predicting implicit discourse relation senses, which has received a considerable amount of attention in recent years (Pitler et al., 2009; Biran and McKeown, 2013; Rutherford and Xue, 2014). Determining implicit discourse relation senses relies on information from the two arguments of the relation. For this subtask, there is a good balance between “conventional” machine learning techniques such as Support Vector Machines and Maximum Entropy models that rely heavily on handcrafted features, and neural network based approaches. A wide variety of features have been used for this subtask, and they include features extracted from syntactic parses (Kang et al., 2016; Kong et al., 2016; Stepanov and Riccardi, 2016; Jain and Majumder,"
K16-2001,I05-3025,1,0.0947611,"as follows: • News articles were extracted from the Wikinews XML dump2 using the publicly available WikiExtractor.py script.3 • Additional processing was done to remove any remaining XML annotations and produce a raw text version of each article (including its title and date). 4 Evaluation The scorer that computes all of the available evaluation metrics is open-source with some contribution from the participants during the task period6 . • Articles written purely in simplified Chinese were identified using the Dragon Mapper4 Python library, and segmented using the NUS Chinese word segmenter (Low et al., 2005). 4.1 Main evaluation metric: End-to-end discourse parsing 1 https://zh.wikinews.org/ https://dumps.wikimedia.org/zhwikinews/20151020/ zhwikinews-20151020-pages-meta-current.xml.bz2 3 http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 4 http://dragonmapper.readthedocs.io/en/latest/index. html A shallow discourse parser (SDP) is evaluated based on the end-to-end F1 score on a per2 5 6 4 https://www.seas.upenn.edu/~pdtb/tools.shtml#annotator http://www.github.com/attapol/conll16st. Sense Definition Alternative Causation Condition Conjunction Contrast Relation between two alternatives Relation"
K16-2001,P14-5010,0,0.00268188,"nefits from the precise evaluation of the progress and improvement since the system is based off the exact same implementation. • Brown clusters (implementation from (Liang, 2005)) • Word embeddings (word2vec) To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation produced using state-of-the-art NLP tools: For English, • Phrase structure parses predicted using the Berkeley parser (Petrov and Klein, 2007); • Dependency parses converted from phrase structure parses using the Stanford converter (Manning et al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and L"
K16-2001,prasad-etal-2008-penn,1,0.839291,"ilingual Shallow Discourse Parsing (SDP). While the 2015 task focused on newswire text data in English, this year we added a new language, Chinese. Given a natural language text as input, the goal of an SDP system is to detect and categorize discourse relations between discourse segments in the text. The conceptual framework of the Shallow Discourse Parsing 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1–19, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics task is that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014), where a discourse relation is viewed as a predicate that takes two abstract objects as arguments. The two arguments may be realized as clauses or sentences, or occasionally phrases. It is “shallow” in that sense that the system is not required to output a tree or graph that covers the entire text, and the discourse relations are not hierarchically organized. As such, it differs from analyses according to either Rhetorical Structure (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). The rest of this overview paper"
K16-2001,K16-2014,0,0.241056,"Missing"
K16-2001,J14-4007,1,0.889259,"ourse Parsing (SDP). While the 2015 task focused on newswire text data in English, this year we added a new language, Chinese. Given a natural language text as input, the goal of an SDP system is to detect and categorize discourse relations between discourse segments in the text. The conceptual framework of the Shallow Discourse Parsing 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1–19, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics task is that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014), where a discourse relation is viewed as a predicate that takes two abstract objects as arguments. The two arguments may be realized as clauses or sentences, or occasionally phrases. It is “shallow” in that sense that the system is not required to output a tree or graph that covers the entire text, and the discourse relations are not hierarchically organized. As such, it differs from analyses according to either Rhetorical Structure (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). The rest of this overview paper is structured as follo"
K16-2001,K16-2020,0,0.0356651,"Missing"
K16-2001,K16-2010,0,0.282836,"ources used Extra resources nguyenlab (Nguyen, 2016) JAIST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters n"
K16-2001,K16-2002,0,0.416621,", VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitted a system description paper are marked with ∗. subtask are represented in this competition. One is to collect all candidate discourse connective by looking up a list of possible connectives compiled from the training data and train a classifier to disambiguate them. There are two variants in this approach: one strategy is to train a classifier for each individual discourse connective expression (Oepen et al., 2016), and the other is to train one classifier for all discourse connective expressions (Wang and Lan, 2016; Kong et al., 2015; Laali et al., 2016). Alternatively, connective identification is treated as a token-level sequence labeling task, solved with sequence labeling models like CRF (Stepanov and Riccardi, 2016). Argument extraction Different strategies were used for extracting the arguments for explicit and for implicit discourse relations. Determining the arguments of implicit discourse relations is relatively straightforward. Most systems adopted a heuristics–based extraction strategy that"
K16-2001,E14-1068,1,0.449193,"ent algorithms and models can be more meaningfully compared. In the open track, the focus of the evaluation is on the overall performance and the use of all possible means to improve the performance of a task. This distinction was easier to maintain for early CoNLL tasks such as noun phrase chunking and named entity recognition, where competitive performance could be achieved without having to use resources other than the provided training set. However, this is no longer true for a high-level task like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants in the closed track to use the following linguistic resources, in addition to the training set: For English, For purposes of evaluation, an explicit discourse connective predicted by a parser is considered correct if and only if the predicted raw connective includes the gold raw connective head, while allowing for the tokens of the pre"
K16-2001,K16-2019,0,0.096966,"Missing"
K16-2001,K16-2007,1,0.844548,"relations. A variety of neural network architectures are represented. (Schenk et al., 2016) used a feedforward neural network, with dependency structures used to re-weight the word embeddings used as input to the network. (Wang and Lan, 2016; Qin et al., 2016) achieved competitive performance using a Convolutional Neural Network architecture for this subtask. Finally, (Weiss and Bajec, 2016) produced competitive results with a focused RNN. Word embeddings were typically used as input to the neural network models and different pooling methods have been used to derive the vectors for arguments. Rutherford and Xue (2016) used simple summation pooling in a feedforward network and achieved competitive performance in classifying implicit discourse relation senses. Relation sense classification All systems have separate classifiers for explicit and implicit discourse connectives. For explicit relations, the discourse connective itself is the best predictor of the discourse relation. Many discourse connectives are unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse rela"
K16-2001,K16-2012,0,0.156471,"Missing"
K16-2001,K15-2002,0,0.156384,"al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and Lan (2015), which has components for identifying discourse connectives and extracting their arguments, for determining the presence or absence of discourse relations in a particular context, and for predicting the senses of the discourse relations. Here we briefly summarize the approaches used in each subtask. We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams have been asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. Sta"
K16-2001,K16-2004,0,0.188138,"ves in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes,"
K16-2001,P14-1069,1,0.15687,"ation from (Liang, 2005)) • Word embeddings (word2vec) To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation produced using state-of-the-art NLP tools: For English, • Phrase structure parses predicted using the Berkeley parser (Petrov and Klein, 2007); • Dependency parses converted from phrase structure parses using the Stanford converter (Manning et al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and Lan (2015), which has components for identifying discourse connectives and extracting their arguments, for determining the presence or absence of discours"
K16-2001,K16-2006,0,0.0402375,"ments for explicit and for implicit discourse relations. Determining the arguments of implicit discourse relations is relatively straightforward. Most systems adopted a heuristics–based extraction strategy that parallels the PDTB annotation strategy for implicit discourse relations: for each pair of adjacent sentences that do not straddle a paragraph boundary, if an explicit discourse relation does not already exist, posit 8 ID Institution Learning methods Resources used Extra resources nguyenlab (Nguyen, 2016) JAIST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16"
K16-2001,K15-2001,1,0.641779,"prises a long pipeline, and it is hard for teams that do not have a pre-existing system to put together a competitive full system. This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments. 3 Data 3.1 Training and Development The training and development sets for English remain exactly the same as those used in the CoNLL-2015 shared task. Details regarding how the data was adapted from the Penn Discourse TreeBank 2.0 (PDTB 2.0) are provided in the overview paper of the CoNLL 2015 shared task (Xue et al., 2015). The Chinese training and development sets are taken from the Chinese Discourse TreeBank (CDTB) 0.5 (Zhou and Xue, 2012; Zhou and Xue, 2015), available from the LDC (http://ldc.upenn.edu), supplemented with additional annotated data from the Chinese TreeBank (Xue et al., 2005). The CDTB adopts the general annotation strategy of the PDTB, associating discourse relations with explicit or implicit discourse connectives and the two spans that serve as their arguments. In the case of explicit discourse relations (Example 1), there is an overt discourse connective, which may be realized syntactical"
K16-2001,P12-1008,1,0.873541,"ull system. This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments. 3 Data 3.1 Training and Development The training and development sets for English remain exactly the same as those used in the CoNLL-2015 shared task. Details regarding how the data was adapted from the Penn Discourse TreeBank 2.0 (PDTB 2.0) are provided in the overview paper of the CoNLL 2015 shared task (Xue et al., 2015). The Chinese training and development sets are taken from the Chinese Discourse TreeBank (CDTB) 0.5 (Zhou and Xue, 2012; Zhou and Xue, 2015), available from the LDC (http://ldc.upenn.edu), supplemented with additional annotated data from the Chinese TreeBank (Xue et al., 2005). The CDTB adopts the general annotation strategy of the PDTB, associating discourse relations with explicit or implicit discourse connectives and the two spans that serve as their arguments. In the case of explicit discourse relations (Example 1), there is an overt discourse connective, which may be realized syntactically as a subordinating or coordinating conjunction, or a discourse adverbial. Implicit discourse relations are cases wher"
K19-1056,W14-4012,0,0.11761,"Missing"
K19-1056,P19-1024,0,0.0196178,"revious research has exploited the dependency structure of a sentence in different ways for relation extraction. Xu et al. (2015) and Miwa and Bansal (2016) used an LSTM network and the shortest dependency path between two entities to find the relation between them. Huang et al. (2017) used the dependency structure of a sentence for the slot-filling task which is close to the relation extraction task. Liu et al. (2015) exploited the shortest dependency path between two entities and the sub-trees attached to that path (augmented dependency path) for relation extraction. Zhang et al. (2018) and Guo et al. (2019) Table 4: Effectiveness of model components (m = 4) on the NYT11 dataset. tween the two entities in the two datasets in Figures 7 and 8. We observe that with increasing sentence length and increasing distance between the two entities, the performance of all models drops. This shows that finding the relation between entities located far from each other is a more difficult task. Our multi-factor attention model with dependency-distance weight factor increases the F1 score in all configurations when compared to previous state-of-the-art models on both datasets. 6 Related Work Relation extraction"
K19-1056,C18-1096,1,0.747782,"Missing"
K19-1056,P11-1055,0,0.442855,"ormation of the sentence and a multi-factor attention mechanism. Experiments on the New York Times corpus show that our proposed model outperforms prior state-of-the-art models. 1 Introduction Relation extraction from unstructured text is an important task to build knowledge bases (KB) automatically. Banko et al. (2007) used open information extraction (Open IE) to extract relation triples from sentences where verbs were considered as the relation, whereas supervised information extraction systems extract a set of pre-defined relations from text. Mintz et al. (2009), Riedel et al. (2010), and Hoffmann et al. (2011) proposed distant supervision to generate the training data for sentence-level relation extraction, where relation tuples (two entities and the relation between them) from a knowledge base such as Freebase (Bollacker et al., 2008) were mapped to free text (Wikipedia articles or New York Times articles). 1 The code and data of this paper can be found at https://github.com/nusnlp/MFA4RE 603 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 603–612 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics → − ← − ht ∈ Rdw +dz and ht"
K19-1056,D17-1274,0,0.021019,"ndently and are evaluated at the sentence level. Since there may not be multiple sentences that contain a pair of entities, it is important to improve the task performance at the sentence level. Future work can explore the integration of our sentencelevel attention model in a multi-instance relation extraction framework. Not much previous research has exploited the dependency structure of a sentence in different ways for relation extraction. Xu et al. (2015) and Miwa and Bansal (2016) used an LSTM network and the shortest dependency path between two entities to find the relation between them. Huang et al. (2017) used the dependency structure of a sentence for the slot-filling task which is close to the relation extraction task. Liu et al. (2015) exploited the shortest dependency path between two entities and the sub-trees attached to that path (augmented dependency path) for relation extraction. Zhang et al. (2018) and Guo et al. (2019) Table 4: Effectiveness of model components (m = 4) on the NYT11 dataset. tween the two entities in the two datasets in Figures 7 and 8. We observe that with increasing sentence length and increasing distance between the two entities, the performance of all models drop"
K19-1056,P16-1200,0,0.0604856,"Missing"
K19-1056,P15-2047,0,0.0252015,"t to improve the task performance at the sentence level. Future work can explore the integration of our sentencelevel attention model in a multi-instance relation extraction framework. Not much previous research has exploited the dependency structure of a sentence in different ways for relation extraction. Xu et al. (2015) and Miwa and Bansal (2016) used an LSTM network and the shortest dependency path between two entities to find the relation between them. Huang et al. (2017) used the dependency structure of a sentence for the slot-filling task which is close to the relation extraction task. Liu et al. (2015) exploited the shortest dependency path between two entities and the sub-trees attached to that path (augmented dependency path) for relation extraction. Zhang et al. (2018) and Guo et al. (2019) Table 4: Effectiveness of model components (m = 4) on the NYT11 dataset. tween the two entities in the two datasets in Figures 7 and 8. We observe that with increasing sentence length and increasing distance between the two entities, the performance of all models drops. This shows that finding the relation between entities located far from each other is a more difficult task. Our multi-factor attentio"
K19-1056,D15-1166,0,0.0846963,"Missing"
K19-1056,N19-1288,0,0.0129861,"ts in Figures 5 and 6. We also analyze the effects of our attention model with different distances be609 Figure 7: Performance comparison across different distances between entities on the NYT10 dataset. (A1) BiLSTM-CNN (A2) Standard attention (A3) Window size (ws) = 5 (A4) Window size (ws) = 10 (A5) Softmax (A6) Max-pool Prec. 0.473 0.466 0.507 0.510 0.490 0.492 Rec. 0.606 0.638 0.652 0.640 0.658 0.600 Figure 8: Performance comparison across different distances between entities on the NYT11 dataset. F1 0.531 0.539 0.571 0.568 0.562 0.541 (2016), Vashishth et al. (2018), Wu et al. (2019), and Ye and Ling (2019) used multiple sentences in a multi-instance relation extraction setting to capture the features located in multiple sentences for a pair of entities. In their evaluation setting, they evaluated model performance by considering multiple sentences having the same pair of entities as a single test instance. On the other hand, our model and the previous models that we compare to in this paper (Zeng et al., 2014, 2015; Shen and Huang, 2016; Jat et al., 2017) work on each sentence independently and are evaluated at the sentence level. Since there may not be multiple sentences that contain a pair of"
K19-1056,P09-1113,0,0.492887,"ttention model which incorporates syntactic information of the sentence and a multi-factor attention mechanism. Experiments on the New York Times corpus show that our proposed model outperforms prior state-of-the-art models. 1 Introduction Relation extraction from unstructured text is an important task to build knowledge bases (KB) automatically. Banko et al. (2007) used open information extraction (Open IE) to extract relation triples from sentences where verbs were considered as the relation, whereas supervised information extraction systems extract a set of pre-defined relations from text. Mintz et al. (2009), Riedel et al. (2010), and Hoffmann et al. (2011) proposed distant supervision to generate the training data for sentence-level relation extraction, where relation tuples (two entities and the relation between them) from a knowledge base such as Freebase (Bollacker et al., 2008) were mapped to free text (Wikipedia articles or New York Times articles). 1 The code and data of this paper can be found at https://github.com/nusnlp/MFA4RE 603 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 603–612 c Hong Kong, China, November 3-4, 2019. 2019 Association for Comp"
K19-1056,D15-1203,0,0.152215,"and r is the 606 Train Test # relations # instances # valid relation tuples # None relation tuples avg. sentence length avg. distance between entity pairs # instances # valid relation tuples # None relation tuples avg. sentence length avg. distance between entity pairs NYT10 53 455,412 124,636 330,776 41.1 NYT11 25 335,843 100,671 235,172 37.2 12.8 12.2 172,415 6,441 165,974 41.7 1,450 520 930 39.7 13.1 11.0 work (CNN) with max-pooling is applied to extract the sentence-level feature vector. This feature vector is passed to a feed-forward layer with softmax to classify the relation. (2) PCNN (Zeng et al., 2015): Words are represented using word embeddings and two positional embeddings. A convolutional neural network (CNN) is applied to the word representations. Rather than applying a global max-pooling operation on the entire sentence, three max-pooling operations are applied on three segments of the sentence based on the location of the two entities (hence the name Piecewise Convolutional Neural Network (PCNN)). The first max-pooling operation is applied from the beginning of the sentence to the end of the entity appearing first in the sentence. The second max-pooling operation is applied from the"
K19-1056,P16-1105,0,0.0150268,"odels that we compare to in this paper (Zeng et al., 2014, 2015; Shen and Huang, 2016; Jat et al., 2017) work on each sentence independently and are evaluated at the sentence level. Since there may not be multiple sentences that contain a pair of entities, it is important to improve the task performance at the sentence level. Future work can explore the integration of our sentencelevel attention model in a multi-instance relation extraction framework. Not much previous research has exploited the dependency structure of a sentence in different ways for relation extraction. Xu et al. (2015) and Miwa and Bansal (2016) used an LSTM network and the shortest dependency path between two entities to find the relation between them. Huang et al. (2017) used the dependency structure of a sentence for the slot-filling task which is close to the relation extraction task. Liu et al. (2015) exploited the shortest dependency path between two entities and the sub-trees attached to that path (augmented dependency path) for relation extraction. Zhang et al. (2018) and Guo et al. (2019) Table 4: Effectiveness of model components (m = 4) on the NYT11 dataset. tween the two entities in the two datasets in Figures 7 and 8. We"
K19-1056,C14-1220,0,0.619768,"sentence contains both entities of a tuple, it is chosen as a training sentence of that tuple. Although this process can generate some noisy training instances, it can give a significant amount of training data which can be used to build supervised models for this task. Mintz et al. (2009), Riedel et al. (2010), and Hoffmann et al. (2011) proposed feature-based learning models and used entity tokens and their nearby tokens, their part-of-speech tags, and other linguistic features to train their models. Recently, many neural network-based models have been proposed to avoid feature engineering. Zeng et al. (2014, 2015) used convolutional neural networks (CNN) with max-pooling to find the relation between two given entities. Though these models have been shown to perform reasonably well on distantly supervised data, they sometimes fail to find the relation when sentences are long and entities are located far from each other. CNN models with max-pooling have limitations in understanding the semantic similarity of words with the given entities and they also fail to capture the longdistance dependencies among the words and entities such as co-reference. In addition, all the words in a sentence may not be"
K19-1056,D18-1244,0,0.019761,"on framework. Not much previous research has exploited the dependency structure of a sentence in different ways for relation extraction. Xu et al. (2015) and Miwa and Bansal (2016) used an LSTM network and the shortest dependency path between two entities to find the relation between them. Huang et al. (2017) used the dependency structure of a sentence for the slot-filling task which is close to the relation extraction task. Liu et al. (2015) exploited the shortest dependency path between two entities and the sub-trees attached to that path (augmented dependency path) for relation extraction. Zhang et al. (2018) and Guo et al. (2019) Table 4: Effectiveness of model components (m = 4) on the NYT11 dataset. tween the two entities in the two datasets in Figures 7 and 8. We observe that with increasing sentence length and increasing distance between the two entities, the performance of all models drops. This shows that finding the relation between entities located far from each other is a more difficult task. Our multi-factor attention model with dependency-distance weight factor increases the F1 score in all configurations when compared to previous state-of-the-art models on both datasets. 6 Related Wor"
K19-1056,C16-1238,0,0.351531,"ution operation. We use dropout in our network with a dropout rate of 0.5, and in convolutional layers, we use the tanh activation function. We use the sequence of tokens starting from 5 words before the entity to 5 words after the entity as its context. We train our models with mini-batch size of 50 and optimize the network parameters using the Adagrad optimizer (Duchi et al., 2011). We use the dependency parser from spaCy2 to obtain the dependency distance of the words from the entities and use ws = 5 as the window size for dependency distance-based attention. 4.4 (3) Entity Attention (EA) (Shen and Huang, 2016): This is the combination of a CNN model and an attention model. Words are represented using word embeddings and two positional embeddings. A CNN with max-pooling is used to extract global features. Attention is applied with respect to the two entities separately. The vector representation of every word is concatenated with the word embedding of the last token of the entity. This concatenated representation is passed to a feed-forward layer with tanh activation and then another feed-forward layer to get a scalar attention score for every word. The original word representations are averaged bas"
K19-1056,D12-1042,0,0.0351018,"cy-distance weight factor increases the F1 score in all configurations when compared to previous state-of-the-art models on both datasets. 6 Related Work Relation extraction from a distantly supervised dataset is an important task and many researchers (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011) tried to solve this task using feature-based classification models. Recently, Zeng et al. (2014, 2015) used CNN models for this task which can extract features automatically. Shen and Huang (2016) and Jat et al. (2017) used attention mechanism in their model to improve performance. Surdeanu et al. (2012), Lin et al. 610 used graph convolution networks with pruned dependency tree structures for this task. In this work, we have incorporated the dependency distance of the words in a sentence from the two entities in a multi-factor attention mechanism to improve sentence-level relation extraction. Attention-based neural networks are quite successful for many other NLP tasks. Bahdanau et al. (2015) and Luong et al. (2015) used attention models for neural machine translation, Seo et al. (2017) used attention mechanism for answer span extraction. Vaswani et al. (2017) and Kundu and Ng (2018) used mu"
K19-1056,D18-1157,0,0.0382762,"Missing"
L18-1003,E17-2025,0,0.020562,"eanwhile, our deep transition RNN contains 4 encoder transitions and 8 decoder transitions. Training for each individual model progresses by updating the model parameters at each mini-batch of 40 sentence pairs to minimize the negative log-likelihood loss function on the parallel training data. We use the Adam algorithm (Kingma and Ba, 2015) with learning rate of 0.0001. At each update, we clip the gradient norm to 1.0. We apply layer normalization (Ba et al., 2016) on the model parameters for faster convergence and tie the target-side embedding with the transpose of the output weight matrix (Press and Wolf, 2017). Model parameters are saved at every checkpoint of 10,000 update iterations. At this stage, the negative log-likelihood loss function on the development set is checked. Training stops when there has been no improvement over the lowest loss function value on the development set for 10 consecutive checkpoints. The main difference between our system and (Sennrich et al., 2017a) is that while they only built NMT models with GRU, we also made use of LSTM. Another difference is in the usage of the larger monolingual English text. They built a synthetic Chinese-English parallel corpus by translating"
L18-1003,P16-1162,0,0.0916921,"on decoding is performed by a beam search algorithm, which produces translation output sequentially in the target language order. NMT decoding proceeds by generating one word at each time step. In NMT, as described in Equation 3, computing the probability involves mapping the hidden state vector to a vector with the dimension of the vocabulary size. Therefore, to make computation tractable, the NMT vocabulary size is limited. To cope with the limitation of the vocabulary size, we adopt fragmentation of words into sub-words of character sequences through the byte pair encoding (BPE) algorithm (Sennrich et al., 2016). This algorithm finds the N most frequent character sequences of variable length, through N character merge operations, and splits less frequent words based on this list of character sub-sequences. Wt transforms the intermediate vector representation to a vocabulary-sized probability vector. The decoder hidden state at a time step i is computed by si = gy (E[yi−1 ], si−1 , ci ) (4) where gy is the RNN unit function to compute the current hidden state given the hidden state of the previous time step, the previous word embedding, and the context. Equation 3 indicates that the target word to be"
L18-1003,E17-3017,0,0.0311685,"Missing"
L18-1003,Q16-1027,0,0.0553315,"representations to the next layer of RNN. This is done subsequently depending on the number of RNN layers. Meanwhile, the deep transition RNN passes an input at each time step through a series of transitions (i.e., recurrent unit functions) and passes the hidden state of the last transition to the next time step. ηj = GRU (χj , ηj−1 ) = (1 − zj ) ◦ ηj−1 + zj ◦ η j (14) where 2.3.1. Deep Stacked RNN We adopt the deep stacked RNN that involves residual connection, summing the output of the previous RNN layer with the computed hidden state of the current RNN layer, and alternation of direction (Zhou et al., 2016), as illustrated in Figure 1. In this model, for each layer l and time step j, we need to distinguish between the computed hidden state of the current RNN unit function without residual → − connection, i.e., h lj , and the hidden state which includes − the residual connection, i.e., → w lj . The alternation of direction is designed such that the odd-numbered and evennumbered RNN layers process the sequence in the left-toright and right-to-left directions respectively. In the stacked RNN with layer depth Dx , the forward encoder hidden state → − at time step j in Equation 11, h j , is computed"
L18-1003,L16-1561,0,0.0162843,"as a transi→ − tion, which outputs an intermediate state h j,l for the encoder and sj,l for the decoder. With Lx transitions, the hidden state representation at j is equivalent to the output of the last layer, so for the forward encoder state, Equation 11 defines → − h j as: Similarly, the decoder recurrent unit function gy,l at each layer l can be instantiated by LSTM or GRU. 3. 3.1. Experimental Setup Datasets We conducted experiments using the parallel training corpora from LDC to test on the NIST test sets. In addition, we also conducted experiments on the United Nations Parallel Corpus (Ziemski et al., 2016), following (JunczysDowmunt et al., 2016). We used the pre-defined training, development, and test sets of the corpus following (Junczys-Dowmunt et al., 2016) and conducted NMT experiments accordingly. We pre-processed our parallel training corpora by segmenting Chinese sentences, which originally have no spaces to demarcate words, and tokenizing English sentences to split punctuation symbols from words. Chinese word segmentation was performed by a maximum entropy model (Low et al., 2005) trained on the Chinese Penn Treebank (CTB) segmentation standard. To alleviate the effect of rare words in"
L18-1003,D14-1179,0,0.0067702,"Missing"
L18-1003,W17-3203,0,0.0363543,"Missing"
L18-1003,D13-1053,0,0.0293906,"Missing"
L18-1003,P07-2045,0,0.00583669,"the combined corpus consists of 107M sentences and 3.8B tokens. Each individual Gigaword sub-corpus (i.e., AFP, APW, CNA, LTW, NYT, and Xinhua) is used to train a separate N -gram language model. The English side of FBIS is also used to train another separate language model (LM). These individual language models are then interpolated to build one single large LM, via perplexity tuning on the English side of the development data. We use this LM for translation output re-ranking. To recover the original casing on the translation output, we trained a statistical MT recaser model by using Moses (Koehn et al., 2007) on the English side of FBIS parallel text and the Xinhua portion of English Gigaword Fourth Edition. Due to computation time and memory consideration, parallel sentences in the corpora that are longer than 50 subwords in either Chinese or English are discarded. In the end, the final parallel training examples consist of 7.65M sentence pairs, 169M Chinese sub-word tokens (equivalent to 166M word tokens), and 186M English sub-word tokens (equivalent to 184M word tokens). 3 LDC2002E18, LDC2003E14, LDC2004E12, LDC2005T06, and LDC2005T10. 4 LDC2007T23, LDC2008T06, LDC2008T08, LDC2009T02, LDC2009T0"
L18-1003,W04-3250,0,0.182681,"th re-ranking is 0.3 BLEU point better than without re-ranking, and the improvement is statistically significant (p &lt; 0.05). Both of our systems achieve higher BLEU scores than the best published result for Chinese-to-English translation reported in (Junczys-Dowmunt et al., 2016). Experimental Results For experiments using the LDC corpora, translation quality is measured by case-insensitive BLEU (Papineni et al., 2002), for which the brevity penalty is computed based on the shortest reference (NIST-BLEU)6 . Statistical significance testing between systems is conducted by bootstrap resampling (Koehn, 2004). As shown in Table 1, among the individual model types, the deep stacked LSTM NMT model gives the best performance. However the best result is achieved by the ensemble of 4 independent model types, combining deep stacked and deep transition architectures with GRU and LSTM recurrent unit functions. This ensemble model gives an improvement of 4.40 BLEU points over the best deep stacked LSTM model. Applying re-ranking by the N -gram language model on top of the ensemble system of 4 independent models gives a further improvement of 0.11 BLEU point on average, which gives the best result of our sy"
L18-1003,I05-3025,1,0.518208,"the NIST test sets. In addition, we also conducted experiments on the United Nations Parallel Corpus (Ziemski et al., 2016), following (JunczysDowmunt et al., 2016). We used the pre-defined training, development, and test sets of the corpus following (Junczys-Dowmunt et al., 2016) and conducted NMT experiments accordingly. We pre-processed our parallel training corpora by segmenting Chinese sentences, which originally have no spaces to demarcate words, and tokenizing English sentences to split punctuation symbols from words. Chinese word segmentation was performed by a maximum entropy model (Low et al., 2005) trained on the Chinese Penn Treebank (CTB) segmentation standard. To alleviate the effect of rare words in NMT, we fragmented words to sub-words through the byte pair encoding (BPE) algorithm (Sennrich et al., 2016) with 59,500 merge operations. All our training sentences are lowercased. → − → − → − − hj = → g x (E[xj ], h j−1 ) = h j,Lx → − → − − h j,1 = → g x,1 (E[xj ], h j−1,Lx ) → − → − − h j,l = → g x,l (0, h j,l−1 ), for 1 &lt; l ≤ Lx where the input to the first recurrent unit transition is E[xj ], the embedding of the input word at j, while the subsequent higher level transitions only re"
L18-1003,W17-4710,0,0.069406,"k depth (Dx ) of 4 and decoder stack depth (Dy ) of 4. tions 8 and 9, i.e., deep transition decoder RNN is extended similarly to the deep transition encoder RNN, such that the decoder hidden state with depth Ly is computed as: 1 s1i = gatt (E[yi−1 ], s1i−1 , ci ) 1 s0i = gy,1 (E[yi−1 ], s1i−1 ) si,1 = s0i = gy,1 (E[yi−1 ], si−1,Ly ) 1 s1i = gy,2 (ci , s0i ) si,2 = gy,2 (ci , si,1 ) Like the encoders, the decoder recurrent unit function gyl at each layer l can be instantiated by LSTM or GRU. si,l = gy,l (0, si,l−1 ), for 2 &lt; l ≤ Ly si = si,Ly 2.3.2. Deep Transition RNN The deep transition RNN (Miceli-Barone et al., 2017) involves a number of layers within a time step j through which an input word is fed, as illustrated in Figure 2. The recurrent unit function of each layer l is defined as a transi→ − tion, which outputs an intermediate state h j,l for the encoder and sj,l for the decoder. With Lx transitions, the hidden state representation at j is equivalent to the output of the last layer, so for the forward encoder state, Equation 11 defines → − h j as: Similarly, the decoder recurrent unit function gy,l at each layer l can be instantiated by LSTM or GRU. 3. 3.1. Experimental Setup Datasets We conducted ex"
L18-1003,P02-1040,0,0.109558,"in Table 3, our best result is obtained by an ensemble of 4 independent models with k-best output reranking using N -gram LM trained on the whole English side of the UN Parallel Corpus. Our system with re-ranking is 0.3 BLEU point better than without re-ranking, and the improvement is statistically significant (p &lt; 0.05). Both of our systems achieve higher BLEU scores than the best published result for Chinese-to-English translation reported in (Junczys-Dowmunt et al., 2016). Experimental Results For experiments using the LDC corpora, translation quality is measured by case-insensitive BLEU (Papineni et al., 2002), for which the brevity penalty is computed based on the shortest reference (NIST-BLEU)6 . Statistical significance testing between systems is conducted by bootstrap resampling (Koehn, 2004). As shown in Table 1, among the individual model types, the deep stacked LSTM NMT model gives the best performance. However the best result is achieved by the ensemble of 4 independent model types, combining deep stacked and deep transition architectures with GRU and LSTM recurrent unit functions. This ensemble model gives an improvement of 4.40 BLEU points over the best deep stacked LSTM model. Applying r"
L18-1003,W17-4739,0,\N,Missing
N12-1067,J93-2004,0,\N,Missing
N12-1067,W11-2838,0,\N,Missing
N13-1050,P06-2005,0,0.894494,"d a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing words. In contrast, our work also performs other normalization operations such as missing word recovery and punctuation correction, to further improve machine translation. Previously, Aw et al. (2006) adopted phrase-based MT to perform SMS normalization, and required a relatively large number of manually normalized SMS messages. In contrast, our approach performs beam search at the sentence level, and does not require large training data. We evaluate the success of social media text normalization in the context of machine translation, so research on machine translation of social media text is relevant to our work. However, there is not much comparative evaluation of social media text translation other than the Haitian Creole to English SMS translation task in the 2011 Workshop on Statistic"
N13-1050,W11-2103,0,0.0154729,"SMS normalization, and required a relatively large number of manually normalized SMS messages. In contrast, our approach performs beam search at the sentence level, and does not require large training data. We evaluate the success of social media text normalization in the context of machine translation, so research on machine translation of social media text is relevant to our work. However, there is not much comparative evaluation of social media text translation other than the Haitian Creole to English SMS translation task in the 2011 Workshop on Statistical Machine Translation (WMT 2011) (Callison-Burch et al., 2011). However, the setup of the WMT 2011 task is different from ours, in that the task provided parallel training data of SMS texts and their translations. As such, text normalization is not necessary in that task. For example, the best reported system in that task (Costa-juss`a and Banchs, 2011) did not perform SMS message normalization. In speech to speech translation (Paul, 2009; Nakov et al., 2009), the input texts contain wrongly transcribed words due to errors in automatic speech recognition, whereas social media texts contain abbreviations, new words, etc. Although the input texts in both c"
N13-1050,W09-2010,0,0.075338,"performed text normalization of informally written email messages using CRF (Lafferty et al., 2001). Due to its importance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1 2 A Chinese version of Twitter at www.weibo.com Available at www.comp.nus.edu.sg/∼nlp/corpora.html 472 morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spa"
N13-1050,W11-2156,0,0.0320789,"Missing"
N13-1050,D12-1052,1,0.714577,"e model is trained on synthetically created training texts in which be has been randomly deleted with probability 0.5. 4.3 A Decoder for Text Normalization When designing our text normalization system, we aim for a general framework that can be applied to text normalization across different languages with minimal effort. This is a challenging task, since social media texts in different languages exhibit different informal characteristics, as illustrated in Section 3. Motivated by the beam-search decoders for SMT (Koehn et al., 2007), ASR (Young et al., 2002), and grammatical error correction (Dahlmeier and Ng, 2012), we propose a novel beam-search decoder for normalization of social media text. Given an input message, the normalization decoder searches for its best normalization, i.e., the best hypothesis, by iteratively performing two subtasks: (1) producing new sentence-level hypotheses from hypotheses in the current stack, carried out by hypothesis producers; and (2) evaluating the new hypotheses to retain good ones, carried out by feature functions. Each hypothesis is the result of applying successive normalization operations on the initial input message, where each normalization operation is carried"
N13-1050,W11-2140,0,0.0662461,"d on to the MT system for translation. We call this baseline O RIGINAL. The second baseline, L ATTICE, is to use a lattice to normalize text. For each input message, a lattice is generated in which each informal word is augmented with its formal candidates taken from the same normalization dictionary (downloaded from Internet) used in our text normalization decoder. The lattice is then decoded by the same language model used in our text normalization decoder to generate the normalized text (Stolcke, 2002). Another possible way of using lattice is to directly feed the lattice to the MT system (Eidelman et al., 2011), but since in this paper, we assume that the MT system can only translate plain text, we leave this as future work. The third baseline, P BMT, is a competitive baseline that performs text normalization via phrasebased MT, as proposed in Aw et al. (2006). Moses (Koehn et al., 2007) is used to perform text normalization, by “translating” un-normalized text to normalized text. The training data used is the same development set used in our text normalization decoder. The normalized text is then sent to our MT system for translation. This method was also used in the SMS translation task of WMT 201"
N13-1050,W11-2210,0,0.116418,"Missing"
N13-1050,P11-1038,0,0.751129,"it takes thirty seconds to normalize the same message, a six-fold increase in speed. After training a text normalization system to normalize social media texts, we can use an existing statistical machine translation (SMT) system trained on normal texts (non-social media texts) to carry out translation. So we argue that normalization followed by regular translation is a more practical approach. Thus, text normalization is important for social media text processing. Most previous work on normalization of social media text focused on word substitution (Beaufort et al., 2010; Gouws et al., 2011; Han and Baldwin, 2011; Liu et al., 2012). However, we argue that some other normalization operations besides word substitution are also critical for subsequent natural language processing (NLP) applications, such as missing word recovery (e.g., zero pronouns) and punctuation correction. In this paper, we propose a novel beam-search decoder for normalization of social media text for MT. Our decoder can effectively integrate different normalization operations together. In contrast to previous work, some of our normalization operations are specifically designed for MT, e.g., missing word recovery based on conditional"
N13-1050,D11-1125,0,0.00985257,"ere stack i stores all hypotheses obtained by applying i hypothesis producers on the input message. The beam-search algorithm is shown tions are used by the decoder to distinguish good hypotheses from bad ones. All feature functions are combined using a linear model to obtain the score for a hypothesis h: X score(h) = λi fi (h), (1) whr u Dictionary: u=&gt;you whr you be whr you are Abbreviation: whr=&gt;where where you i Punctuation Be where you . where fi is the i-th feature function with weight λi . The weights of the feature functions are tuned using the pairwise ranking optimization algorithm (Hopkins and May, 2011) on the development set. where are you Punctuation where are you ? 4.4 Figure 1: An example search tree when normalizing “whr u”. The solid (dashed) boxes represent good (bad) hypotheses. The hypothesis producers are indicated on the edges. in Algorithm 1, and Figure 1 shows an example search tree for an English message. Algorithm 1 The beam-search decoder INPUT: a raw message M whose length is N RETURN: the best normalization for M 1: initialize hypothesisStacks[N+1] and hypothesisProducers; 2: add the initial hypothesis M to stack hypothesisStacks[0]; 3: for i ← 0 to N-1 do 4: for each hypo"
N13-1050,C08-1056,0,0.0439443,"nese/English corpora for normalization and translation of social media text2 . 2 Related Work Zhu et al. (2007) performed text normalization of informally written email messages using CRF (Lafferty et al., 2001). Due to its importance, normalization of social media text has been extensively studied recently. Aw et al. (2005) proposed a noisy channel model consisting of different operations: substitution of non-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1 2 A Chinese version of Twitter at www.weibo.com Available at www.comp.nus.edu.sg/∼nlp/corpora.html 472 morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a"
N13-1050,D08-1108,0,0.0132591,"at www.weibo.com Available at www.comp.nus.edu.sg/∼nlp/corpora.html 472 morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing words. In contrast, our work also performs other normalization operations such as missing word recovery and punctuation correction, to further improve machine translation. Previously, Aw et al. (2006) adopted phrase-based MT to perform SMS normalization, and required a relatively large number of manually normalized SMS messages. In contrast, our approach performs beam search at the sentence level, and does not require large training data. We evaluate the success of social media text normalization in"
N13-1050,P11-1037,0,0.0756727,"Missing"
N13-1050,P12-1109,0,0.462219,"s to normalize the same message, a six-fold increase in speed. After training a text normalization system to normalize social media texts, we can use an existing statistical machine translation (SMT) system trained on normal texts (non-social media texts) to carry out translation. So we argue that normalization followed by regular translation is a more practical approach. Thus, text normalization is important for social media text processing. Most previous work on normalization of social media text focused on word substitution (Beaufort et al., 2010; Gouws et al., 2011; Han and Baldwin, 2011; Liu et al., 2012). However, we argue that some other normalization operations besides word substitution are also critical for subsequent natural language processing (NLP) applications, such as missing word recovery (e.g., zero pronouns) and punctuation correction. In this paper, we propose a novel beam-search decoder for normalization of social media text for MT. Our decoder can effectively integrate different normalization operations together. In contrast to previous work, some of our normalization operations are specifically designed for MT, e.g., missing word recovery based on conditional random fields 471"
N13-1050,D10-1018,1,0.849531,"applications are typically trained on formal texts with correct punctuation. We define punctuation correction as correcting punctuation in sentences which may have no or unreliable punctuation. The task performs three punctuation operations: insertion, deletion, and substitution. To our knowledge, no previous work has been done on punctuation correction for normalization of social media text. In ASR, punctuation prediction only inserts punctuation symbols into ASR output that has no punctuation (Kim and Woodland, 2001; Huang and Zweig, 2002), but without punctuation deletion or substitution. Lu and Ng (2010) argued that punctuation prediction should be jointly performed with sentence boundary detection, so they modeled punctuation prediction using a two-layer DCRF model (Sutton et al., 2004). We also believe that punctuation correction is closely related to sentence boundary detection. Thus, we propose a two-layer DCRF model for punctuation correction. Layer 1 gives the actual punctuation tags None, Comma, Period, QuestionMark, and Exclamatory-Mark. Layer 2 gives the sentence boundary, including tags DeclarativeBegin, Declarative-In, Question-Begin, Question-In, Exclamatory-Begin, and Exclamatory"
N13-1050,2009.iwslt-evaluation.14,1,0.840228,"uch comparative evaluation of social media text translation other than the Haitian Creole to English SMS translation task in the 2011 Workshop on Statistical Machine Translation (WMT 2011) (Callison-Burch et al., 2011). However, the setup of the WMT 2011 task is different from ours, in that the task provided parallel training data of SMS texts and their translations. As such, text normalization is not necessary in that task. For example, the best reported system in that task (Costa-juss`a and Banchs, 2011) did not perform SMS message normalization. In speech to speech translation (Paul, 2009; Nakov et al., 2009), the input texts contain wrongly transcribed words due to errors in automatic speech recognition, whereas social media texts contain abbreviations, new words, etc. Although the input texts in both cases deviate from normal texts, the exact deviations are different. Category Freq. Example Punctuation 81 你好[hi] ～(你好 。[hi .]); Pronunciation 47 表[watch](不要[don’t]); 酱紫(这样子[this]); New word 43 萌[bud](可爱[cute]); Interjection 27 好的[ok] 哦[oh](好的[ok]); 23 想要[want](我[i] 想要[want]); Pronoun Segmentation 14 表酱紫(不要[don’t] 这样子[this]); Pronunciation 288 4(for); oredi(already); Abbreviation 98 slp(sleep); whr("
N13-1050,J03-1002,0,0.00553817,"ally normalized English/Chinese texts. The formal corpus used (as described in Section 4) is the concatenation of two Chinese-English spoken parallel corpora: the IWSLT 2009 corpus (Paul, 2009) and another spoken text corpus collected at the Harbin Institute of Technology3 . The language model used for Chinese (English) text normalization is the Chinese (English) side of the formal corpus and the LDC Chinese (English) Gigaword corpus. To evaluate the effect of text normalization on MT, we build phrase-based MT systems using Moses (Koehn et al., 2007), with word alignments generated by GIZA++ (Och and Ney, 2003). The MT training data contains the above formal corpus and some LDC4 parallel corpora (LDC2000T46, LDC2002E18, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2007T23, LDC2008T06, LDC2008T08, LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03). In total, 214M/192M English/Chinese tokens are used to train our MT systems. The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus. Our MT systems are tuned on the manually normalized messages of our development sets. Follo"
N13-1050,P02-1040,0,0.0927863,"LDC4 parallel corpora (LDC2000T46, LDC2002E18, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2007T23, LDC2008T06, LDC2008T08, LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03). In total, 214M/192M English/Chinese tokens are used to train our MT systems. The language model of the Chinese-English (English-Chinese) MT system is the English (Chinese) side of the FBIS corpus (LDC2003E14) and the English (Chinese) Gigaword corpus. Our MT systems are tuned on the manually normalized messages of our development sets. Following (Aw et al., 2006; Oliva et al., 2013), we use BLEU scores (Papineni et al., 2002) to evaluate text normalization. We also use BLEU scores to evaluate MT quality. We use the sign test to determine statistical significance, for both text normalization and translation. 3 4 http://mitlab.hit.edu.cn/ http://www.ldc.upenn.edu/Catalog/ 477 Baselines We compare our text normalization decoder against three baseline methods for performing text normalization. We then send the respective normalized texts to the same MT system to evaluate the effect of text normalization on MT. The simplest baseline for text normalization is one that does no text normalization. The raw text (un-normali"
N13-1050,I11-1109,0,0.0822478,"-standard acronyms, deletion of flavor words, and insertion of auxiliary verbs and subject pronouns. Choudhury et al. (2007) used hidden Markov model to perform word-level normalization. Kobus et al. (2008) combined MT and automatic speech recognition (ASR) to better normalize French SMS message. Cook and Stevenson (2009) used an unsupervised noisy channel model considering different word formation processes. Han and Baldwin (2011) normalized informal words using 1 2 A Chinese version of Twitter at www.weibo.com Available at www.comp.nus.edu.sg/∼nlp/corpora.html 472 morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing word"
N13-1050,D11-1141,0,0.0336426,"Missing"
N13-1050,W11-2159,0,0.059419,"since in this paper, we assume that the MT system can only translate plain text, we leave this as future work. The third baseline, P BMT, is a competitive baseline that performs text normalization via phrasebased MT, as proposed in Aw et al. (2006). Moses (Koehn et al., 2007) is used to perform text normalization, by “translating” un-normalized text to normalized text. The training data used is the same development set used in our text normalization decoder. The normalized text is then sent to our MT system for translation. This method was also used in the SMS translation task of WMT 2011 by (Stymne, 2011). In the tables showing experimental results, normalization and translation BLEU scores that are significantly higher than (p &lt; 0.01) the L ATTICE or P BMT baseline are in bold or underlined, respectively. 5.3 Chinese-English Experimental Results The Chinese-English normalization and translation results are shown in Table 4. The first group of experiments is the three baselines, and the second group is an oracle experiment using manually normalized messages as the output of text normalizaSystem O RIGINAL baseline L ATTICE baseline P BMT baseline ORACLE Dictionary Punctuation Pronunciation Pron"
N13-1050,I05-3013,0,0.0180676,"rmalized informal words using 1 2 A Chinese version of Twitter at www.weibo.com Available at www.comp.nus.edu.sg/∼nlp/corpora.html 472 morphophonemic similarity. Pennell and Liu (2011) only dealt with SMS abbreviations. Xue et al. (2011) normalized social media texts incorporating orthographic, phonetic, contextual, and acronym factors. Liu et al. (2012) designed a system combining different human perspectives to perform word-level normalization. Oliva et al. (2013) normalized Spanish SMS messages using a normalization and a phonetic dictionary. For normalization of Chinese social media text, Xia et al. (2005) investigated informal phrase detection, and Li and Yarowsky (2008) mined informal-formal phrase pairs from Web corpora. All the above work focused on normalizing words. In contrast, our work also performs other normalization operations such as missing word recovery and punctuation correction, to further improve machine translation. Previously, Aw et al. (2006) adopted phrase-based MT to perform SMS normalization, and required a relatively large number of manually normalized SMS messages. In contrast, our approach performs beam search at the sentence level, and does not require large training"
N13-1050,P07-1087,1,0.733198,"Missing"
N13-1050,P07-2045,0,\N,Missing
N13-1050,P10-1079,0,\N,Missing
N13-1050,2009.iwslt-evaluation.1,0,\N,Missing
N15-1035,W09-2420,0,0.0814875,"Missing"
N15-1035,W06-2911,0,0.0489361,"called smoothed co-training, and reported improved results. Another related study was done by (Pham et al., 2005). In (Pham et al., 2005), some semi-supervised learning techniques were used for word sense disambiguation. Pham et al. employed co-training and spectral graph transduction methods in their experiments and obtained significant improvements over a supervised method. Another semi-supervised learning method used for word sense disambiguation is Alternating Structure Optimization (ASO), first introduced by (Ando and Zhang, 2005) and later applied to word sense disambiguation tasks by (Ando, 2006). This algorithm learns a predictive structure shared between different problems (disambiguation of a target word). Semi-supervised application of the ASO algorithm was shown to be useful for word sense disambiguation and improvements can be achieved over a supervised predictor (Ando, 2006). This paper uses a different method proposed by (Turian et al., 2010) that can be applied to a wide variety of supervised tasks in natural language processing. This method uses distributed word representations (word embeddings) as additional feature functions in supervised tasks and is shown to improve the"
N15-1035,D07-1007,0,0.0138174,"Drive Singapore 117417 kaveh@comp.nus.edu.sg Abstract to achieve its goal. Part of this ambiguity may be resolved by considering part-of-speech (POS) tags but the word senses are still highly ambiguous even for the same part-of-speech. Machine translation is probably the most important application of word sense disambiguation. In machine translation, different senses of a word cause a great amount of ambiguity for automated translation and it negatively affects the results. Hence, an accurate WSD system can benefit machine translation significantly and improve the results (Chan et al., 2007; Carpuat and Wu, 2007; Vickrey et al., 2005). Moreover, Zhong and Ng (2012) have shown that word sense disambiguation improves information retrieval by proposing a method to use word senses in a language modeling approach to information retrieval. The rest of this paper is organized as follows. Section 2 gives a literature review of related work, including a review of semi-supervised word sense disambiguation and distributed word representation called word embeddings. The method and framework used in this paper are explained in Section 3. Finally, we evaluate the system in Section 4 and conclude the paper in Secti"
N15-1035,P07-1005,1,0.290147,"gapore 13 Computing Drive Singapore 117417 kaveh@comp.nus.edu.sg Abstract to achieve its goal. Part of this ambiguity may be resolved by considering part-of-speech (POS) tags but the word senses are still highly ambiguous even for the same part-of-speech. Machine translation is probably the most important application of word sense disambiguation. In machine translation, different senses of a word cause a great amount of ambiguity for automated translation and it negatively affects the results. Hence, an accurate WSD system can benefit machine translation significantly and improve the results (Chan et al., 2007; Carpuat and Wu, 2007; Vickrey et al., 2005). Moreover, Zhong and Ng (2012) have shown that word sense disambiguation improves information retrieval by proposing a method to use word senses in a language modeling approach to information retrieval. The rest of this paper is organized as follows. Section 2 gives a literature review of related work, including a review of semi-supervised word sense disambiguation and distributed word representation called word embeddings. The method and framework used in this paper are explained in Section 3. Finally, we evaluate the system in Section 4 and concl"
N15-1035,H05-1053,0,0.0104428,"ith dimension of word embeddings. Like (Turian et al., 2010), we also found that σ = 0.1 is a good choice for the target standard deviation and works well. 4 Results and Discussion We evaluate our word sense disambiguation system experimentally by using standard benchmarks. The two major tasks in word sense disambiguation are lexical sample task and all-words task. For each task, we explain our experimental setup first and then present the results of our experiments for the two mentioned tasks. Although most benchmarks are general domain test sets, a few domain-specific test sets also exist (Koeling et al., 2005; Agirre et al., 2010). 4.1 Lexical Sample Tasks We have evaluated our system on SensEval-2 (SE2) and SensEval-3 (SE3) lexical sample tasks and also the domain-specific test set (we call it DS05) published by (Koeling et al., 2005). This subsection describes our experiments and presents the results of these tasks. 4.1.1 Experimental Setup Most lexical sample tasks provide separate training and test sets. Some statistics about these tasks are given in Table 1. #Word types #Training samples #Test samples SE2 73 8,611 4,328 SE3 57 8,022 3,944 DS05 41 10,272 Table 1: Statistics of lexical sample t"
N15-1035,W02-1006,1,0.463441,"ations consistently improve the accuracy of the selected supervised WSD system. Moreover, our experiments on a domainspecific dataset show that our supervised baseline system beats the best knowledge-based systems by a large margin. 1 Introduction Because of the ambiguity of natural language, many words can have different meanings in different contexts. For example, the word “bank” has two different meanings in “the bank of a river” and “a bank loan”. While it seems simple for humans to identify the meaning of a word according to the context, word sense disambiguation (WSD) (Ng and Lee, 1996; Lee and Ng, 2002) is a difficult task for computers and thus requires sophisticated means 2 Related Work The method that we use in this paper is a semisupervised learning method which incorporates knowledge from unlabeled datasets by using word embeddings. This section is a literature review of previous work on semi-supervised word sense disambiguation and various methods of obtaining word embeddings. 314 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 314–323, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2.1 S"
N15-1035,W04-2405,0,0.0172958,"guation Among various types of semi-supervised learning approaches, co-training and self-training are probably the most common. These methods randomly select a subset of a large unlabeled dataset and classify these samples using one (self-training) or two (co-training) classifiers, trained on a smaller set of labeled samples. After assigning labels to the new samples, these methods select the samples that were classified with a high confidence (according to a selection criterion) and add them to the set of labeled data. These methods have been used in the context of word sense disambiguation. Mihalcea (2004) used both co-training and self-training to make use of unlabeled datasets for word sense disambiguation. Mihalcea also introduced a technique for combining co-training and majority voting, called smoothed co-training, and reported improved results. Another related study was done by (Pham et al., 2005). In (Pham et al., 2005), some semi-supervised learning techniques were used for word sense disambiguation. Pham et al. employed co-training and spectral graph transduction methods in their experiments and obtained significant improvements over a supervised method. Another semi-supervised learnin"
N15-1035,N13-1090,0,0.303489,"relationships between class labels and words. In addition to using raw word embeddings, we also propose a method to adapt embeddings for each classification task. Since word embeddings do not include much task-specific discriminative information, we use a neural network to modify word vectors to tune them for our WSD tasks. We show that this process results in improved accuracy compared to raw word embeddings. Recently, obtaining word embeddings in an unsupervised manner from large text corpora has attracted the attention of many researchers (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013a; Mikolov et al., 2013b). Subsequently, there have been some published word embeddings and some software for training word embeddings. For word sense disambiguation, there are very few open source programs. Since we are interested in a fully supervised WSD tool, IMS (It Makes Sense) (Zhong and Ng, 2010) is selected in our work. This system allows addition of extra features in a simple way and hence is a good choice for testing the effect of word embeddings as additional features. Moreover, the scores reported for IMS are competitive with or better than state-of-the-art systems (Zhong and Ng,"
N15-1035,H94-1046,0,0.15385,"Missing"
N15-1035,P96-1006,1,0.170166,"hat such representations consistently improve the accuracy of the selected supervised WSD system. Moreover, our experiments on a domainspecific dataset show that our supervised baseline system beats the best knowledge-based systems by a large margin. 1 Introduction Because of the ambiguity of natural language, many words can have different meanings in different contexts. For example, the word “bank” has two different meanings in “the bank of a river” and “a bank loan”. While it seems simple for humans to identify the meaning of a word according to the context, word sense disambiguation (WSD) (Ng and Lee, 1996; Lee and Ng, 2002) is a difficult task for computers and thus requires sophisticated means 2 Related Work The method that we use in this paper is a semisupervised learning method which incorporates knowledge from unlabeled datasets by using word embeddings. This section is a literature review of previous work on semi-supervised word sense disambiguation and various methods of obtaining word embeddings. 314 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 314–323, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computationa"
N15-1035,P03-1058,1,0.686343,"Missing"
N15-1035,P00-1056,0,0.0701785,"Missing"
N15-1035,P10-1154,0,0.023978,"Missing"
N15-1035,rose-etal-2002-reuters,0,0.014841,"r et al., 1994) as part of our training data. Table 2 shows some statistics of our training data. POS Adj. Adv. Noun Verb Total #word types 5,129 28 11,445 4,705 21,307 Table 2: Number of word types in each part-of-speech (POS) in our training set Since the dataset used by (Zhong and Ng, 2010) does not cover the specific domains of DS05 (Sports and Finance), we added a few samples from these domains to improve our baseline system. For each target word, we randomly selected 5 instances (a sentence including the target word) for Sports domain and 5 instances for Finance domain from the Reuters (Rose et al., 2002) dataset’s Sports and Finance sections and manually sense annotated them. Annotating 5 instances per word and domain takes about 5 minutes. To make sure that these instances are not the same samples in the test set, we filtered out all documents containing at least one of the test instances and selected our training samples from the rest of the collection. After removing samples with unclear tags, we added the remaining instances (187 instances for Sports domain and 179 instances for Finance domain) to our original training data (Zhong and Ng, 2010). We highlight this setting in our experiment"
N15-1035,P10-1040,0,0.107326,"s over a supervised method. Another semi-supervised learning method used for word sense disambiguation is Alternating Structure Optimization (ASO), first introduced by (Ando and Zhang, 2005) and later applied to word sense disambiguation tasks by (Ando, 2006). This algorithm learns a predictive structure shared between different problems (disambiguation of a target word). Semi-supervised application of the ASO algorithm was shown to be useful for word sense disambiguation and improvements can be achieved over a supervised predictor (Ando, 2006). This paper uses a different method proposed by (Turian et al., 2010) that can be applied to a wide variety of supervised tasks in natural language processing. This method uses distributed word representations (word embeddings) as additional feature functions in supervised tasks and is shown to improve the accuracy of named-entity recognition (NER) and chunking. In this paper, we also follow the same approach for word sense disambiguation. The key idea is that a system without a continuous315 space representation of words ignores the similarity of words completely and relies only on their discrete form. However, when a distributed representation for words is ad"
N15-1035,H05-1097,0,0.00988352,"7 kaveh@comp.nus.edu.sg Abstract to achieve its goal. Part of this ambiguity may be resolved by considering part-of-speech (POS) tags but the word senses are still highly ambiguous even for the same part-of-speech. Machine translation is probably the most important application of word sense disambiguation. In machine translation, different senses of a word cause a great amount of ambiguity for automated translation and it negatively affects the results. Hence, an accurate WSD system can benefit machine translation significantly and improve the results (Chan et al., 2007; Carpuat and Wu, 2007; Vickrey et al., 2005). Moreover, Zhong and Ng (2012) have shown that word sense disambiguation improves information retrieval by proposing a method to use word senses in a language modeling approach to information retrieval. The rest of this paper is organized as follows. Section 2 gives a literature review of related work, including a review of semi-supervised word sense disambiguation and distributed word representation called word embeddings. The method and framework used in this paper are explained in Section 3. Finally, we evaluate the system in Section 4 and conclude the paper in Section 5. One of the weakne"
N15-1035,P10-4014,1,0.956238,"for our WSD tasks. We show that this process results in improved accuracy compared to raw word embeddings. Recently, obtaining word embeddings in an unsupervised manner from large text corpora has attracted the attention of many researchers (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013a; Mikolov et al., 2013b). Subsequently, there have been some published word embeddings and some software for training word embeddings. For word sense disambiguation, there are very few open source programs. Since we are interested in a fully supervised WSD tool, IMS (It Makes Sense) (Zhong and Ng, 2010) is selected in our work. This system allows addition of extra features in a simple way and hence is a good choice for testing the effect of word embeddings as additional features. Moreover, the scores reported for IMS are competitive with or better than state-of-the-art systems (Zhong and Ng, 2010). 2.2 Word Embeddings There are several types of word representations. A one-hot representation is a vector where all components except one are set to zero and the component at the index associated with a word is set to one. This type of representation is the sparsest word representation and does no"
N15-1035,P12-1029,1,0.886247,"to achieve its goal. Part of this ambiguity may be resolved by considering part-of-speech (POS) tags but the word senses are still highly ambiguous even for the same part-of-speech. Machine translation is probably the most important application of word sense disambiguation. In machine translation, different senses of a word cause a great amount of ambiguity for automated translation and it negatively affects the results. Hence, an accurate WSD system can benefit machine translation significantly and improve the results (Chan et al., 2007; Carpuat and Wu, 2007; Vickrey et al., 2005). Moreover, Zhong and Ng (2012) have shown that word sense disambiguation improves information retrieval by proposing a method to use word senses in a language modeling approach to information retrieval. The rest of this paper is organized as follows. Section 2 gives a literature review of related work, including a review of semi-supervised word sense disambiguation and distributed word representation called word embeddings. The method and framework used in this paper are explained in Section 3. Finally, we evaluate the system in Section 4 and conclude the paper in Section 5. One of the weaknesses of current supervised word"
N15-1035,S10-1013,0,\N,Missing
N15-1035,J14-1003,0,\N,Missing
P02-1061,C02-1025,1,0.805894,"that of the mixed case NER. However, we still manage to achieve a considerable reduction of errors between the two NERs when they are tested on the official MUC-6 and MUC-7 test data. 3 System Description We use the maximum entropy framework to build two classifiers: an upper case NER and a mixed case NER. The upper case NER does not have access to case information of the training and test data, and hence cannot make use of all the features used by the mixed case NER. We will first describe how the mixed case NER is built. More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002). Our approach is similar to the MENE system of (Borthwick, 1999). Each word is assigned a name class based on its features. Each name class is subdivided into 4 classes, i.e., N begin, N continue, N end, and N unique. Hence, there is a total of 29 classes (7 name classes  4 sub-classes  1 not-a-name class). 3.1 Maximum Entropy The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible, other than the constraints imposed. Such constraints are derived from training data, expressing some relationship between features and outcome. The p"
P02-1061,W99-0613,0,0.0203119,"s on NERs, partly due to the Message Understanding Conferences (MUC-6, 1995; MUC-7, 1998). Machine learning methods such as BBN’s IdentiFinder (Bikel, Schwartz, and Weischedel, 1999) and Borthwick’s MENE (Borthwick, 1999) have shown that machine learning NERs can achieve comparable performance with systems using hand-coded rules. Bikel, Schwartz, and Weischedel (1999) have also shown how mixed case text can be automatically converted to upper case SNOR or OCR format to train NERs to work on such formats. There is also some work on unsupervised learning for mixed case named entity recognition (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999). Collins and Singer (1999) investigated named entity classification using Adaboost, CoBoost, and the EM algorithm. However, features were extracted using a parser, and performance was evaluated differently (the classes were person, organization, location, and noise). Cucerzan and Yarowsky (1999) built a cross language NER, and the performance on English was low compared to supervised single-language NER such as IdentiFinder. We suspect that it will be hard for purely unsupervised methods to perform as well as supervised ones. Seeger (2001) gave a comprehensive su"
P02-1061,W99-0612,0,0.0411308,"he Message Understanding Conferences (MUC-6, 1995; MUC-7, 1998). Machine learning methods such as BBN’s IdentiFinder (Bikel, Schwartz, and Weischedel, 1999) and Borthwick’s MENE (Borthwick, 1999) have shown that machine learning NERs can achieve comparable performance with systems using hand-coded rules. Bikel, Schwartz, and Weischedel (1999) have also shown how mixed case text can be automatically converted to upper case SNOR or OCR format to train NERs to work on such formats. There is also some work on unsupervised learning for mixed case named entity recognition (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999). Collins and Singer (1999) investigated named entity classification using Adaboost, CoBoost, and the EM algorithm. However, features were extracted using a parser, and performance was evaluated differently (the classes were person, organization, location, and noise). Cucerzan and Yarowsky (1999) built a cross language NER, and the performance on English was low compared to supervised single-language NER such as IdentiFinder. We suspect that it will be hard for purely unsupervised methods to perform as well as supervised ones. Seeger (2001) gave a comprehensive summary of recent work in learni"
P02-1061,W01-0501,0,0.0165517,"racted using a parser, and performance was evaluated differently (the classes were person, organization, location, and noise). Cucerzan and Yarowsky (1999) built a cross language NER, and the performance on English was low compared to supervised single-language NER such as IdentiFinder. We suspect that it will be hard for purely unsupervised methods to perform as well as supervised ones. Seeger (2001) gave a comprehensive summary of recent work in learning with labeled and unlabeled data. There is much recent research on co-training, such as (Blum and Mitchell, 1998; Collins and Singer, 1999; Pierce and Cardie, 2001). Most cotraining methods involve using two classifiers built on different sets of features. Instead of using distinct sets of features, Goldman and Zhou (2000) used different classification algorithms to do co-training. Blum and Mitchell (1998) showed that in order for PAC-like guarantees to hold for co-training, features should be divided into two disjoint sets satisfying: (1) each set is sufficient for a classifier to learn a concept correctly; and (2) the two sets are conditionally independent of each other. Each set of features can be used to build a classifier, resulting in two independe"
P03-1028,C02-1025,1,0.840763,"xts is easier than from free texts, since the layout and format of a semi-structured text provide additional useful clues AYACUCHO, 19 JAN 89 – TODAY TWO PEOPLE WERE WOUNDED WHEN A BOMB EXPLODED IN SAN JUAN BAUTISTA MUNICIPALITY. OFFICIALS SAID THAT SHINING PATH MEMBERS WERE RESPONSIBLE FOR THE ATTACK ... ... POLICE SOURCES STATED THAT THE BOMB ATTACK INVOLVING THE SHINING PATH CAUSED SERIOUS DAMAGES ... ... Figure 1: Snippet of a MUC-4 document to aid in extraction. Several benchmark data sets have been used to evaluate IE approaches on semistructured texts (Soderland, 1999; Ciravegna, 2001; Chieu and Ng, 2002a). For the task of extracting information from free texts, a series of Message Understanding Conferences (MUC) provided benchmark data sets for evaluation. Several subtasks for IE from free texts have been identified. The named entity (NE) task extracts person names, organization names, location names, etc. The template element (TE) task extracts information centered around an entity, like the acronym, category, and location of a company. The template relation (TR) task extracts relations between entities. Finally, the full-scale IE task, the scenario template (ST) task, deals with extracting"
P03-1028,M95-1011,0,0.0148754,"named entity extraction (Chieu and Ng, 2002b), template element extraction, and template relation extraction (Miller et al., 1998). These machine learning approaches have been successful for these tasks, achieving accuracy comparable to the knowledge-engineering approach. However, for the full-scale ST task of generic IE from free texts, the best reported method to date is still the knowledge-engineering approach. For example, almost all participating IE systems in MUC used the knowledge-engineering approach for the full-scale ST task. The one notable exception is the work of UMass at MUC-6 (Fisher et al., 1995). Unfortunately, their learning approach did considerably worse than the best MUC-6 systems. Soderland (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only. In this paper, we present a learning approach to the full-scale ST task of extracting information from free texts. The task we tackle is considerably more complex than that of (Soderland, 1999; Chieu and Ng, 2002a), since we need to deal with merging information from multiple sentences"
P03-1028,M92-1008,0,0.487277,"following) verb of a noun phrase, and the voice of the verb. With full parsing, these verbs were obtained based on traversing the full parse tree. The results indicate that verb features contribute to the performance of the system, even without full parsing. With full parsing, verbs can be determined more accurately, leading to better overall performance. 5 Discussion Although the best MUC-4 participating systems, GE/GE-CMU, still outperform A LICE -ME, it must be noted that for GE, “10 1/2 person months” were spent on MUC-4 using the GE NLTOOLSET , after spending “15 person months” on MUC-3 (Rau et al., 1992). With a learning approach, IE systems are more portable across domains. Not all occurrences of a string in a document that match a slot fill of a template provide good positive training examples. For example, in the same document, there might be the following sentences “THE MNR REPORTS THE KIDNAPPING OF OQUELI COLINDRES...”, followed by “OQUELI COLINDRES ARRIVED IN GUATEMALA ON 11 JANUARY”. In this case, only the first occurrence of OQUELI COLINDRES should be used as a positive example for the human target slot. However, A LICE does not have access to such information, since the MUC-4 trainin"
P03-1028,J01-4004,1,0.634149,"un phrase  generates one test example, and it is presented to the classifier of a template slot  to determine whether  fills the slot  . A separate template manager decides whether a new template should be created to include slot  , or slot  should fill the existing template. 3.1 Preprocessing All the preprocessing modules of A LICE were built with supervised learning techniques. They include sentence segmentation (Ratnaparkhi, 1998), part-ofspeech tagging (Charniak et al., 1993), named entity recognition (Chieu and Ng, 2002b), full parsing (Collins, 1999), and coreference resolution (Soon et al., 2001). Each module performs at or near stateof-the-art accuracy, but errors are unavoidable, and later modules in the preprocessing chain have to deal with errors made by the previous modules. 3.2 Features in Training and Test Examples As mentioned earlier, the features of an example are generated based on a base noun phrase (denoted as baseNP), which is a candidate for filling a template slot. While most strings that fill a string slot are base noun phrases, this is not always the case. For instance, consider the two examples in Figure 4. In the first example, “BOMB” should fill the string slot IN"
P03-1028,P00-1065,0,\N,Missing
P03-1028,J03-4003,0,\N,Missing
P03-1028,M95-1006,0,\N,Missing
P03-1058,P91-1034,0,0.712303,"his alone would have accounted for 0.088 of the accuracy difference between the two approaches. That domain dependence is an important issue affecting the performance of WSD programs has been pointed out by (Escudero et al., 2000). Our work confirms the importance of domain dependence in WSD. As to the problem of insufficient sense coverage, with the steady increase and availability of parallel corpora, we believe that getting sufficient sense coverage from larger parallel corpora should not be a problem in the near future for most of the commonly occurring words in a language. 5 Related Work Brown et al. (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. However, they only looked at assigning at most two senses to a word, and their method only asked a single question about a single word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parall"
P03-1058,W02-0805,0,0.0193174,"lel corpora should not be a problem in the near future for most of the commonly occurring words in a language. 5 Related Work Brown et al. (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. However, they only looked at assigning at most two senses to a word, and their method only asked a single question about a single word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parallel corpora. Resnik and Yarowsky (2000) considered word sense disambiguation using multiple languages. Our present work can be similarly extended beyond bilingual corpora to multilingual corpora. The research most similar to ours is the work of Diab and Resnik (2002). However, they used machine translated parallel corpus instead of human translated parallel corpus. In addition, they used an unsupervised method of noun group disambiguation, and evaluated on the English all"
P03-1058,P02-1033,0,0.421933,"ngle word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parallel corpora. Resnik and Yarowsky (2000) considered word sense disambiguation using multiple languages. Our present work can be similarly extended beyond bilingual corpora to multilingual corpora. The research most similar to ours is the work of Diab and Resnik (2002). However, they used machine translated parallel corpus instead of human translated parallel corpus. In addition, they used an unsupervised method of noun group disambiguation, and evaluated on the English all-words task. 6 Conclusion In this paper, we reported an empirical study to evaluate an approach of automatically acquiring sense-tagged training data from English-Chinese parallel corpora, which were then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task. Our investigation reveals that this method of acquiring sense-tagged data is promising and provides an al"
P03-1058,S01-1001,0,0.0187407,"lthough attempts have been made to mine parallel corpora from the Web (Resnik, 1999). However, large-scale, good-quality parallel corpora have recently become available. For example, six English-Chinese parallel corpora are now available from Linguistic Data Consortium. These parallel corpora are listed in Table 2, with a combined size of 280 MB. In this paper, we address the above issues and report our findings, exploiting the English-Chinese parallel corpora in Table 2 for word sense disambiguation. We evaluated our approach on all the nouns in the English lexical sample task of SENSEVAL-2 (Edmonds and Cotton, 2001; Kilgarriff 2001), which used the WordNet 1.7 sense inventory (Miller, 1990). While our approach has only been tested on English and Chinese, it is completely general and applicable to other language pairs. 2 Approach Our approach of exploiting parallel texts for word sense disambiguation consists of four steps: (1) parallel text alignment (2) manual selection of target translations (3) training of WSD classifier (4) WSD of words in new contexts. 2.1 Parallel Text Alignment In this step, parallel texts are first sentence-aligned and then word-aligned. Various alignment algorithms (Melamed 200"
P03-1058,W00-1322,0,0.0422819,"Missing"
P03-1058,W02-1004,0,0.00976279,"surrounding channel then forms a training example for a supervised WSD program in the next step. The average time taken to perform manual selection of target translations for one SENSEVAL-2 English noun is less than 15 minutes. This is a relatively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training examples. This step could also be potentially automated if we have a suitable bilingual translation lexicon. 2.3 Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and Moldovan, 2001; Yarowsky et al., 2001). In this paper, we used the WSD program reported in (Lee and Ng, 2002). In particular, our method made use of the knowledge sources of part-of-speech, surrounding words, and local collocations. We used naïve Bayes as the learning algorithm. Our previous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance. 2.4 WSD of Words in New Contexts Given an occurrence of w in a new context, we then used the naïve Bayes classifier to determine the most probable sense of w. noun chi"
P03-1058,W02-0808,0,0.218976,"y occurring words in a language. 5 Related Work Brown et al. (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. However, they only looked at assigning at most two senses to a word, and their method only asked a single question about a single word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parallel corpora. Resnik and Yarowsky (2000) considered word sense disambiguation using multiple languages. Our present work can be similarly extended beyond bilingual corpora to multilingual corpora. The research most similar to ours is the work of Diab and Resnik (2002). However, they used machine translated parallel corpus instead of human translated parallel corpus. In addition, they used an unsupervised method of noun group disambiguation, and evaluated on the English all-words task. 6 Conclusion In this paper, we reported an empirical study to"
P03-1058,S01-1004,0,0.0358498,"made to mine parallel corpora from the Web (Resnik, 1999). However, large-scale, good-quality parallel corpora have recently become available. For example, six English-Chinese parallel corpora are now available from Linguistic Data Consortium. These parallel corpora are listed in Table 2, with a combined size of 280 MB. In this paper, we address the above issues and report our findings, exploiting the English-Chinese parallel corpora in Table 2 for word sense disambiguation. We evaluated our approach on all the nouns in the English lexical sample task of SENSEVAL-2 (Edmonds and Cotton, 2001; Kilgarriff 2001), which used the WordNet 1.7 sense inventory (Miller, 1990). While our approach has only been tested on English and Chinese, it is completely general and applicable to other language pairs. 2 Approach Our approach of exploiting parallel texts for word sense disambiguation consists of four steps: (1) parallel text alignment (2) manual selection of target translations (3) training of WSD classifier (4) WSD of words in new contexts. 2.1 Parallel Text Alignment In this step, parallel texts are first sentence-aligned and then word-aligned. Various alignment algorithms (Melamed 2001; Och and Ney 200"
P03-1058,W02-1006,1,0.369826,"ms a training example for a supervised WSD program in the next step. The average time taken to perform manual selection of target translations for one SENSEVAL-2 English noun is less than 15 minutes. This is a relatively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training examples. This step could also be potentially automated if we have a suitable bilingual translation lexicon. 2.3 Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and Moldovan, 2001; Yarowsky et al., 2001). In this paper, we used the WSD program reported in (Lee and Ng, 2002). In particular, our method made use of the knowledge sources of part-of-speech, surrounding words, and local collocations. We used naïve Bayes as the learning algorithm. Our previous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance. 2.4 WSD of Words in New Contexts Given an occurrence of w in a new context, we then used the naïve Bayes classifier to determine the most probable sense of w. noun child detention feeli"
P03-1058,P02-1044,0,0.0625916,"in WSD. As to the problem of insufficient sense coverage, with the steady increase and availability of parallel corpora, we believe that getting sufficient sense coverage from larger parallel corpora should not be a problem in the near future for most of the commonly occurring words in a language. 5 Related Work Brown et al. (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. However, they only looked at assigning at most two senses to a word, and their method only asked a single question about a single word of context. Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here. Their method also does not require a parallel corpus. The research of (Chugur et al., 2002) dealt with sense distinctions across multiple languages. Ide et al. (2002) investigated word sense distinctions using parallel corpora. Resnik and Yarowsky (2000) considered word sense disambiguation using multiple languages. Our present work can be similarly extended beyond bilingual corpora to multilingual corpora. The research most similar to ours is the work of Diab and Resnik (2002). However, they"
P03-1058,S01-1031,0,0.0159488,"ple for a supervised WSD program in the next step. The average time taken to perform manual selection of target translations for one SENSEVAL-2 English noun is less than 15 minutes. This is a relatively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training examples. This step could also be potentially automated if we have a suitable bilingual translation lexicon. 2.3 Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and Moldovan, 2001; Yarowsky et al., 2001). In this paper, we used the WSD program reported in (Lee and Ng, 2002). In particular, our method made use of the knowledge sources of part-of-speech, surrounding words, and local collocations. We used naïve Bayes as the learning algorithm. Our previous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance. 2.4 WSD of Words in New Contexts Given an occurrence of w in a new context, we then used the naïve Bayes classifier to determine the most probable sense of w. noun child detention feeling holiday lady material yew"
P03-1058,P00-1056,0,0.0171877,"ilgarriff 2001), which used the WordNet 1.7 sense inventory (Miller, 1990). While our approach has only been tested on English and Chinese, it is completely general and applicable to other language pairs. 2 Approach Our approach of exploiting parallel texts for word sense disambiguation consists of four steps: (1) parallel text alignment (2) manual selection of target translations (3) training of WSD classifier (4) WSD of words in new contexts. 2.1 Parallel Text Alignment In this step, parallel texts are first sentence-aligned and then word-aligned. Various alignment algorithms (Melamed 2001; Och and Ney 2000) have been developed in the past. For the six bilingual corpora that we used, they already come with sentences pre-aligned, either manually when the corpora were prepared or automatically by sentencealignment programs. After sentence alignment, the English texts are tokenized so that a punctuation symbol is separated from its preceding word. For the Chinese texts, we performed word segmentation, so that Chinese characters are segmented into words. The resulting parallel texts are then input to the GIZA++ software (Och and Ney 2000) for word alignment. In the output of GIZA++, each English word"
P03-1058,P99-1068,0,0.00954089,"agged corpus without manual annotation. However, are current word alignment algorithms accurate enough for our purpose? (iii) Ultimately, using a state-of-the-art supervised WSD program, what is its disambiguation accuracy when it is trained on a “sense-tagged” corpus obtained via parallel text alignment, compared with training on a manually sense-tagged corpus? Much research remains to be done to investigate all of the above issues. The lack of large-scale parallel corpora no doubt has impeded progress in this direction, although attempts have been made to mine parallel corpora from the Web (Resnik, 1999). However, large-scale, good-quality parallel corpora have recently become available. For example, six English-Chinese parallel corpora are now available from Linguistic Data Consortium. These parallel corpora are listed in Table 2, with a combined size of 280 MB. In this paper, we address the above issues and report our findings, exploiting the English-Chinese parallel corpora in Table 2 for word sense disambiguation. We evaluated our approach on all the nouns in the English lexical sample task of SENSEVAL-2 (Edmonds and Cotton, 2001; Kilgarriff 2001), which used the WordNet 1.7 sense invento"
P03-1058,W97-0213,0,0.0607482,"e inventory in a dictionary. This annotated corpus then serves as the training material for a learning algorithm. After training, a model is automatically learned and it is used to assign the correct sense to any previously unseen occurrence of w in a new context. While the supervised learning approach has been successful, it has the drawback of requiring manually sense-tagged data. This problem is particular severe for WSD, since sense-tagged data must be collected separately for each word in a language. One source to look for potential training data for WSD is parallel texts, as proposed by Resnik and Yarowsky (1997). Given a word-aligned parallel corpus, the different translations in a target language serve as the “sense-tags” of an ambiguous word in the source language. For example, some possible Chinese translations of the English noun channel are listed in Table 1. To illustrate, if the sense of an occurrence of the noun channel is “a path over which electrical signals can pass”, then this occurrence can be translated as “频道” in Chinese. WordNet 1.7 sense id 1 2 3 4 5 6 7 Lumped sense id 1 2 3 4 5 6 1 Chinese translations WordNet 1.7 English sense descriptions 频道 水道 水渠 排水渠 沟 海峡 途径 导管 频道 A path over wh"
P03-1058,S01-1040,0,0.00957864,"ram in the next step. The average time taken to perform manual selection of target translations for one SENSEVAL-2 English noun is less than 15 minutes. This is a relatively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training examples. This step could also be potentially automated if we have a suitable bilingual translation lexicon. 2.3 Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and Moldovan, 2001; Yarowsky et al., 2001). In this paper, we used the WSD program reported in (Lee and Ng, 2002). In particular, our method made use of the knowledge sources of part-of-speech, surrounding words, and local collocations. We used naïve Bayes as the learning algorithm. Our previous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance. 2.4 WSD of Words in New Contexts Given an occurrence of w in a new context, we then used the naïve Bayes classifier to determine the most probable sense of w. noun child detention feeling holiday lady material yew bar bum chair day dyke f"
P03-1058,J04-1001,0,\N,Missing
P06-1012,H94-1046,0,0.023657,"Ng and Lee, 1996) contains 192,800 annotated examples for 121 nouns and 70 verbs, drawn from BC and WSJ. BC was built as a balanced corpus and contains texts in various categories such as religion, fiction, etc. In contrast, the focus of the WSJ corpus is on financial and business news. Escudero et al. (2000) exploited the difference in coverage between these two corpora to separate the DSO corpus into its BC and WSJ parts for investigating the domain dependence of several WSD algorithms. Following their setup, we also use the DSO corpus in our experiments. The widely used SEMCOR (SC) corpus (Miller et al., 1994) is one of the few currently available manually sense-annotated corpora for WSD. SEMCOR is a subset of BC. Since BC is a balanced corpus, and training a classifier on a general corpus before applying it to a more specific corpus is a natural scenario, we will use examples from BC as training data, and examples from WSJ as evaluation data, or the target dataset. 4.2 Parallel Texts Scalability is a problem faced by current supervised WSD systems, as they usually rely on manually annotated data for training. To tackle this problem, in one of our recent work (Ng et al., 2003), we had gathered trai"
P06-1012,P03-1058,1,0.811601,"MCOR (SC) corpus (Miller et al., 1994) is one of the few currently available manually sense-annotated corpora for WSD. SEMCOR is a subset of BC. Since BC is a balanced corpus, and training a classifier on a general corpus before applying it to a more specific corpus is a natural scenario, we will use examples from BC as training data, and examples from WSJ as evaluation data, or the target dataset. 4.2 Parallel Texts Scalability is a problem faced by current supervised WSD systems, as they usually rely on manually annotated data for training. To tackle this problem, in one of our recent work (Ng et al., 2003), we had gathered training data from parallel texts and obtained encouraging results in our 92 there is a significant change in sense priors between the training and target dataset, such as when there is a change in domain between the datasets. Hence, in our experiments involving the DSO corpus, we focused on the set of nouns and verbs which had different predominant senses between the BC and WSJ parts of the corpus. This gave us a set of 37 nouns and 28 verbs. For experiments involving the nouns of SENSEVAL-2 and SENSEVAL-3 English lexical sample task, we used the approach we described in (Ch"
P06-1012,W04-3204,0,0.0170281,"Missing"
P06-1012,W00-1322,0,0.10172,"roduction Many words have multiple meanings, and the process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD). Among the various approaches to WSD, corpus-based supervised machine learning methods have been the most successful to date. With this approach, one would need to obtain a corpus in which each ambiguous word has been manually annotated with the correct sense, to serve as training data. However, supervised WSD systems faced an important issue of domain dependence when using such a corpus-based approach. To investigate this, Escudero et al. (2000) conducted experiments using the DSO corpus, which contains sentences drawn from two different corpora, namely Brown Corpus (BC) and Wall Street Journal (WSJ). They found that training a WSD system on one part (BC or WSJ) of the DSO corpus and applying it to the To build WSD systems that are portable across different domains, estimation of the sense priors (i.e., determining the proportions of the different senses of a word) occurring in a text corpus drawn from a domain is important. McCarthy et al. (2004) provided a partial solution by describing a method to predict the predominant sense, or"
P06-1012,S01-1004,0,0.0186804,"Missing"
P06-1012,W02-1006,1,0.785176,"bed in (Ng et al., 2003) and (Chan and Ng, 2005b). We then evaluated our estimation of sense priors on the nouns of SENSEVAL-2 English lexical sample task, similar to the evaluation we conducted in (Chan and Ng, 2005b). Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al., 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. 5 Experimental Results Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. Knowledge sources used include partsof-speech, surrounding words, and local collocations. This approach achieves state-of-the-art accuracy. All accuracies reported in our experiments are micro-averages over all test examples. In (Chan and Ng, 2005b), we used a multiclass naive Bayes classifier (denoted by NB) for each word. Following this approach, we noted the WSD accuracies achieved without any adjustment, in the  column L under NB in Table 1. The predictions &apos;    & of these naive Bayes classifiers are then  use"
P06-1012,P04-1036,0,0.640999,"ssue of domain dependence when using such a corpus-based approach. To investigate this, Escudero et al. (2000) conducted experiments using the DSO corpus, which contains sentences drawn from two different corpora, namely Brown Corpus (BC) and Wall Street Journal (WSJ). They found that training a WSD system on one part (BC or WSJ) of the DSO corpus and applying it to the To build WSD systems that are portable across different domains, estimation of the sense priors (i.e., determining the proportions of the different senses of a word) occurring in a text corpus drawn from a domain is important. McCarthy et al. (2004) provided a partial solution by describing a method to predict the predominant sense, or the most frequent sense, of a word in a corpus. Using the noun interest as an example, their method will try to predict that sense 1 is the predominant sense in the BC part of the DSO corpus, while sense 4 is the predominant sense in the WSJ part of the 89 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 89–96, c Sydney, July 2006. 2006 Association for Computational Linguistics we used a confusion matrix algorithm (Vucetic and Obradovic"
P06-1012,W04-0807,0,0.0194025,"Missing"
P06-1012,P96-1006,1,\N,Missing
P07-1005,P05-1033,1,0.670826,"not be used by state-of-the-art statistical MT systems. 33 David Chiang Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using Engl"
P07-1005,J07-2003,1,0.188328,"o uses a general log-linear model (Och and Ney, 2002) where the weight of a derivation D for a particular source sentence and its translation is Y w(D) = φi (D)λi (3) i where φi is a feature function and λi is the weight for feature φi . To ensure efficient decoding, the φi are subject to certain locality restrictions. Essentially, they should be defined as products of functions defined on isolated synchronous CGF rules; however, it is possible to extend the domain of locality of the features somewhat. A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed. In principle we could add features that depend on arbitrary source-side context. 3.1 New Features in Hiero for WSD To incorporate WSD into Hiero, we use the translations proposed by the WSD system to help Hiero obtain a better or more probable derivation during the translation of each source sentence. To achieve this, when a grammar rule R is considered during decoding, a"
P07-1005,P05-1066,0,0.0538264,"iero+WSD Results We then added the WSD features of Section 3.1 into Hiero and reran the experiment. The weights obtained by MERT are shown in the row Hiero+WSD of Table 2. We note that a negative weight is learnt for P tywsd . This means that in general, the model prefers grammar rules having chunks that matches WSD translations. This matches our intuition. Using the weights obtained, we translated the test sentences and obtained a BLEU score of 30.30, as shown in the row Hiero+WSD of Table 1. The improvement of 0.57 is statistically significant at p < 0.05 using the sign-test as described by Collins et al. (2005), with 374 (+1), 318 (−1) and 227 (0). Using the bootstrap-sampling test described in (Koehn, 2004b), the improvement is statistically significant at p < 0.05. Though the improvement is modest, it is statistically significant and this positive result is important in view of the negative findings in (Carpuat and Wu, 2005) that WSD does not help MT. Furthermore, note that Hiero+WSD has higher n-gram precisions than Hiero. 7 Analysis Ideally, the WSD system should be suggesting highquality translations which are frequently part of the reference sentences. To determine this, we note the set of gra"
P07-1005,N03-1010,0,0.00474864,"izing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as the WSD model’s predicted translation. They did not incorporate their WSD model or its predictions into their translation model; rather, they used the WSD predictions either to constrain the options available to their decoder, or to postedit the output of their decoder. They reported the negative result that WSD decreased the performance of MT based on their experiments. In another work (Vickr"
P07-1005,N03-1017,0,0.11125,"ystem will be able to help an MT system to determine the correct translation for an ambiguous word. To determine the correct sense of a word, WSD systems typically use a wide array of features that are not limited to the local context of w, and some of these features may not be used by state-of-the-art statistical MT systems. 33 David Chiang Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT sys"
P07-1005,koen-2004-pharaoh,0,0.320543,"Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as th"
P07-1005,W04-3250,0,0.533954,"Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as th"
P07-1005,W02-1006,1,0.648396,"next section, we describe our WSD system. 34 Then, in Section 3, we describe the Hiero MT system and introduce the two new features used to integrate the WSD system into Hiero. In Section 4, we describe the training data used by the WSD system. In Section 5, we describe how the WSD translations provided are used by the decoder of the MT system. In Section 6 and 7, we present and analyze our experimental results, before concluding in Section 8. 2 Word Sense Disambiguation Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). For our experiments, we use the SVM implementation of (Chang and Lin, 2001) as it is able to work on multiclass problems to output the classification probability for each class. Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). For local collocations, we use 3 features, w−1 w+1 , w−1 , and w+1 , where w−1 (w+1 ) is the token immediately to the left (right) of the current ambiguous word occurrence w. For parts-of-speech, we use 3 features, P−1 , P0 , and P+1 ,"
P07-1005,W02-1018,0,0.0157549,"nse ambiguity, a WSD system will be able to help an MT system to determine the correct translation for an ambiguous word. To determine the correct sense of a word, WSD systems typically use a wide array of features that are not limited to the local context of w, and some of these features may not be used by state-of-the-art statistical MT systems. 33 David Chiang Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the perf"
P07-1005,P05-1048,0,0.48789,"and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as the WSD model’s predicted translation. They did not incorporate their WSD model or its predictions into their translation model; rather, they used the WSD predictions either to constrain the options availabl"
P07-1005,P00-1056,0,0.186151,".1611 Table 2: Weights for each feature obtained by MERT training. The first eight features are those used by Hiero in (Chiang, 2005). the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus, we trained a trigram language model using the SRI Language Modelling Toolkit (Stolcke, 2002). Following (Chiang, 2005), we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores (Papineni et al., 2002) based on case-insensitive ngram matching, where n is up to 4. First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. The word alignments of both directions are then combined into a single set of alignments using the “diag-and” method of Koehn et al. (2003). Based on these alignments, synchronous CFG rules are then extracted from the corpus. While Hiero is extracting grammar rules, we gathered WSD training data by following the procedure described in section 4. 6.1 Hiero Results Using the MT 2002 test set, we ran the minimumerror rate training (MERT) (Och, 2003) with the decoder to tune the weights for each feature. The weights obtained are shown in the row Hiero of Table 2. Using these w"
P07-1005,P02-1038,0,0.0683074,"there is a one-to-one correspondence between the non-terminals in γ and α indicated by co-indexation. Hence, γ and α always have the same number of non-terminal symbols. For instance, we could have the following grammar rule: X → hd d d X 1 , go to X 1 every month toi (2) where boxed indices represent the correspondences between non-terminal symbols. Hiero extracts the synchronous CFG rules automatically from a word-aligned parallel corpus. To translate a source sentence, the goal is to find its most probable derivation using the extracted grammar rules. Hiero uses a general log-linear model (Och and Ney, 2002) where the weight of a derivation D for a particular source sentence and its translation is Y w(D) = φi (D)λi (3) i where φi is a feature function and λi is the weight for feature φi . To ensure efficient decoding, the φi are subject to certain locality restrictions. Essentially, they should be defined as products of functions defined on isolated synchronous CGF rules; however, it is possible to extend the domain of locality of the features somewhat. A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult bu"
P07-1005,J04-4002,0,0.020172,"o help an MT system to determine the correct translation for an ambiguous word. To determine the correct sense of a word, WSD systems typically use a wide array of features that are not limited to the local context of w, and some of these features may not be used by state-of-the-art statistical MT systems. 33 David Chiang Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu"
P07-1005,P03-1021,0,0.0215816,"nsitive ngram matching, where n is up to 4. First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. The word alignments of both directions are then combined into a single set of alignments using the “diag-and” method of Koehn et al. (2003). Based on these alignments, synchronous CFG rules are then extracted from the corpus. While Hiero is extracting grammar rules, we gathered WSD training data by following the procedure described in section 4. 6.1 Hiero Results Using the MT 2002 test set, we ran the minimumerror rate training (MERT) (Och, 2003) with the decoder to tune the weights for each feature. The weights obtained are shown in the row Hiero of Table 2. Using these weights, we run Hiero’s decoder to perform the actual translation of the MT 2003 test sentences and obtained a BLEU score of 29.73, as shown in the row Hiero of Table 1. This is higher than the score of 28.77 reported in (Chiang, 2005), perhaps due to differences in word segmentation, etc. Note that comparing with the MT systems used in (Carpuat and Wu, 2005) and (Cabezas and Resnik, 2005), the Hiero system we are using represents a much stronger baseline MT system up"
P07-1005,P02-1040,0,0.104543,"666 0.1124 Pw (γ|α) 0.0393 0.0487 Features Pw (α|γ) P typhr 0.1357 0.0665 0.0380 0.0988 Glue −0.0582 −0.0305 P tyword −0.4806 −0.1747 Pwsd (t|s) 0.1051 P tywsd −0.1611 Table 2: Weights for each feature obtained by MERT training. The first eight features are those used by Hiero in (Chiang, 2005). the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus, we trained a trigram language model using the SRI Language Modelling Toolkit (Stolcke, 2002). Following (Chiang, 2005), we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores (Papineni et al., 2002) based on case-insensitive ngram matching, where n is up to 4. First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. The word alignments of both directions are then combined into a single set of alignments using the “diag-and” method of Koehn et al. (2003). Based on these alignments, synchronous CFG rules are then extracted from the corpus. While Hiero is extracting grammar rules, we gathered WSD training data by following the procedure described in section 4. 6.1 Hiero Results Using the MT 2002 test set, we ran the minimumerror rat"
P07-1005,W04-0822,0,0.10486,"a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as the WSD model’s predicted translation. They did not incorporate their WSD model or its predictions into their translation model; rather, they used the WSD predictions either to constrain the options available to their decoder, or to postedit the output of their decoder. They reported the negati"
P07-1005,H05-1097,0,0.763143,"2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as the WSD model’s predicted translation. They did not incorporate their WSD model or its predictions into their translation model; rather, they used the WSD predictions either to constrain the options available to their decoder, or to postedit the output of their decoder. They reported the negative result that WSD decreased the performance of MT based on their experiments. In another work (Vickrey et al., 2005), the WSD problem was recast as a word translation task. The Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33–40, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics translation choices for a word w were defined as the set of words or phrases aligned to w, as gathered from a word-aligned parallel corpus. The authors showed that they were able to improve their model’s accuracy on two simplified translation tasks: word translation and blank-filling. Recently, Cabezas and Resnik (2005) experimented with incorporating"
P07-1005,P96-1021,0,0.0796472,"ules. Hiero uses a general log-linear model (Och and Ney, 2002) where the weight of a derivation D for a particular source sentence and its translation is Y w(D) = φi (D)λi (3) i where φi is a feature function and λi is the weight for feature φi . To ensure efficient decoding, the φi are subject to certain locality restrictions. Essentially, they should be defined as products of functions defined on isolated synchronous CGF rules; however, it is possible to extend the domain of locality of the features somewhat. A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed. In principle we could add features that depend on arbitrary source-side context. 3.1 New Features in Hiero for WSD To incorporate WSD into Hiero, we use the translations proposed by the WSD system to help Hiero obtain a better or more probable derivation during the translation of each source sentence. To achieve this, when a grammar rule R is considered dur"
P07-1007,W02-1006,1,0.85787,"Missing"
P07-1007,magnini-cavaglia-2000-integrating,0,0.0239633,"erform our experiments on domain adaptation. Among the few currently available manually sense-annotated corpora for WSD, the SEMCOR (SC) corpus (Miller et al., 1994) is the most widely used. SEMCOR is a subset of BC which is senseannotated. Since BC is a balanced corpus, and since performing adaptation from a general corpus to a more specific corpus is a natural scenario, we focus on adapting a WSD system trained on BC to WSJ in this paper. Henceforth, out-of-domain data will refer to BC examples, and in-domain data will refer to WSJ examples. 2.2 Choice of Nouns The WordNet Domains resource (Magnini and Cavaglia, 2000) assigns domain labels to synsets in WordNet. Since the focus of the WSJ corpus is on business and financial news, we can make use of WordNet Domains to select the set of nouns having at least one synset labeled with a business or finance related domain label. This is similar to the approach taken in (Koeling et al., 2005) where they focus on determining the predominant sense of words in corpora drawn from finance versus sports domains.1 Hence, we select the subset of DSO nouns that have at least one synset labeled with any of these domain labels: commerce, enterprise, money, finance, banking,"
P07-1007,W00-1326,0,0.590856,"Missing"
P07-1007,P04-1036,0,0.0622224,"ficient training examples in BC for some sense s, whatever available examples of s are used. This approach gives an average of 195 BC training examples for the 21 nouns. With this new set of training examples, we perform adaptation using active learning and obtain the a-truePrior curve in Figure 2. The a-truePrior curve shows that by ensuring that the sense priors in the BC training data adhere as much as possible to the sense priors in the WSJ data, we start off with a higher WSD accuracy. However, the performance is no different from the a 6.3 Using Predominant Sense Information Research by McCarthy et al. (2004) and Koeling et al. (2005) pointed out that a change of predominant sense is often indicative of a change in domain. For example, the predominant sense of the noun interest in the BC part of the DSO corpus has the meaning “a sense of concern with and curiosity about someone or something”. In the WSJ part of the DSO corpus, the noun interest has a different predominant sense with the meaning “a fixed charge for borrowing money”, which is reflective of the business and finance focus of the WSJ corpus. Instead of restricting the BC training data to adhere strictly to the sense priors in WSJ, anot"
P07-1007,H94-1046,0,0.0487078,"ntaining texts in various categories such as religion, politics, humanities, fiction, etc, the WSJ corpus consists primarily of business and financial news. Exploiting the difference in coverage between these two corpora, Escudero et al. (2000) separated the DSO corpus into 50 its BC and WSJ parts to investigate the domain dependence of several WSD algorithms. Following the setup of (Escudero et al., 2000), we similarly made use of the DSO corpus to perform our experiments on domain adaptation. Among the few currently available manually sense-annotated corpora for WSD, the SEMCOR (SC) corpus (Miller et al., 1994) is the most widely used. SEMCOR is a subset of BC which is senseannotated. Since BC is a balanced corpus, and since performing adaptation from a general corpus to a more specific corpus is a natural scenario, we focus on adapting a WSD system trained on BC to WSJ in this paper. Henceforth, out-of-domain data will refer to BC examples, and in-domain data will refer to WSJ examples. 2.2 Choice of Nouns The WordNet Domains resource (Magnini and Cavaglia, 2000) assigns domain labels to synsets in WordNet. Since the focus of the WSJ corpus is on business and financial news, we can make use of Word"
P07-1007,P96-1006,1,0.575295,"Missing"
P07-1007,N03-1027,0,0.0508006,"raining data. Note that in the experiments reported in this paper, all the adaptation examples are already preannotated before the experiments start, since all the WSJ adaptation examples come from the DSO corpus which have already been sense-annotated. Hence, the annotation of an example needed during each adaptation iteration is simulated by performing a lookup without any manual annotation. 4 Count-merging We also employ a technique known as countmerging in our domain adaptation study. Countmerging assigns different weights to different examples to better reflect their relative importance. Roark and Bacchiani (2003) showed that weighted count-merging is a special case of maximum a posteriori (MAP) estimation, and successfully used it for probabilistic context-free grammar domain adaptation (Roark and Bacchiani, 2003) and language model adaptation (Bacchiani and Roark, 2003). Count-merging can be regarded as scaling of counts obtained from different data sets. We let e c denote the counts from out-of-domain training data, c¯ denote the counts from in-domain adaptation data, and pb denote the probability estimate by count-merging. We can scale the out-of-domain and in-domain counts with different factors,"
P07-1007,N06-1016,0,0.192091,"ess of the adaptation process by using the predicted predominant sense of the new domain and adopting the count-merging technique. 7 Related Work In applying active learning for domain adaptation, Zhang et al. (2003) presented work on sentence boundary detection using generalized Winnow, while Tur et al. (2004) performed language model adaptation of automatic speech recognition systems. In both papers, out-of-domain and indomain data were simply mixed together without MAP estimation such as count-merging. For WSD, Fujii et al. (1998) used selective sampling for a Japanese language WSD system, Chen et al. (2006) used active learning for 5 verbs using coarse-grained evaluation, and H. T. Dang (2004) employed active learning for another set of 5 verbs. However, their work only investigated the use of active learning to reduce the annotation effort necessary for WSD, but did not deal with the porting of a WSD system to a different domain. Escudero et al. (2000) used the DSO corpus to highlight the importance of the issue of domain dependence of WSD systems, but did not propose methods such as active learning or countmerging to address the specific problem of how to perform domain adaptation for WSD. 8 C"
P07-1007,W03-0408,0,0.0403394,"Missing"
P07-1007,W00-1322,0,0.705497,"of automatic speech recognition systems. In both papers, out-of-domain and indomain data were simply mixed together without MAP estimation such as count-merging. For WSD, Fujii et al. (1998) used selective sampling for a Japanese language WSD system, Chen et al. (2006) used active learning for 5 verbs using coarse-grained evaluation, and H. T. Dang (2004) employed active learning for another set of 5 verbs. However, their work only investigated the use of active learning to reduce the annotation effort necessary for WSD, but did not deal with the porting of a WSD system to a different domain. Escudero et al. (2000) used the DSO corpus to highlight the importance of the issue of domain dependence of WSD systems, but did not propose methods such as active learning or countmerging to address the specific problem of how to perform domain adaptation for WSD. 8 Conclusion Domain adaptation is important to ensure the general applicability of WSD systems across different domains. In this paper, we have shown that active learning is effective in reducing the annotation effort required in porting a WSD system to a new domain. Also, we have successfully used an EM-based algorithm to detect a change in predominant"
P07-1007,J98-4002,0,0.103028,"n effort required for domain adaptation, we have further improved the effectiveness of the adaptation process by using the predicted predominant sense of the new domain and adopting the count-merging technique. 7 Related Work In applying active learning for domain adaptation, Zhang et al. (2003) presented work on sentence boundary detection using generalized Winnow, while Tur et al. (2004) performed language model adaptation of automatic speech recognition systems. In both papers, out-of-domain and indomain data were simply mixed together without MAP estimation such as count-merging. For WSD, Fujii et al. (1998) used selective sampling for a Japanese language WSD system, Chen et al. (2006) used active learning for 5 verbs using coarse-grained evaluation, and H. T. Dang (2004) employed active learning for another set of 5 verbs. However, their work only investigated the use of active learning to reduce the annotation effort necessary for WSD, but did not deal with the porting of a WSD system to a different domain. Escudero et al. (2000) used the DSO corpus to highlight the importance of the issue of domain dependence of WSD systems, but did not propose methods such as active learning or countmerging t"
P07-1027,P05-1001,0,0.257558,"ng of NomBank is a multiclass classification problem by nature. Using the one-vs-all arrangement, that is, one binary classifier for each possible outcome, the SRL task can be treated as multiple binary classification problems. In the latter view, we are presented with the opportunity to exploit the common structures of these related problems. This is known as multi-task learning in the machine learning literature (Caruana, 1997; Ben-David and Schuller, 2003; Evgeniou and Pontil, 2004; Micchelli and Pontil, 2005; Maurer, 2006). In this paper, we apply Alternating Structure Optimization (ASO) (Ando and Zhang, 2005a) to the semantic role labeling task on NomBank. ASO is a recently proposed linear multi-task learning algorithm based on empirical risk minimization. The method requires the use of multiple auxiliary problems, and its effectiveness may vary depending on the specific auxiliary problems used. ASO has been shown to be effective on the following natural language processing tasks: text categorization, named entity recognition, part-of-speech tagging, and word sense disambiguation (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006). This paper makes two significant contributions. First, we"
P07-1027,W06-2911,0,0.0641481,"we apply Alternating Structure Optimization (ASO) (Ando and Zhang, 2005a) to the semantic role labeling task on NomBank. ASO is a recently proposed linear multi-task learning algorithm based on empirical risk minimization. The method requires the use of multiple auxiliary problems, and its effectiveness may vary depending on the specific auxiliary problems used. ASO has been shown to be effective on the following natural language processing tasks: text categorization, named entity recognition, part-of-speech tagging, and word sense disambiguation (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006). This paper makes two significant contributions. First, we present a novel application of ASO to the SRL task on NomBank. We explore the effect of different auxiliary problems, and show that learning predictive structures with ASO results in significantly improved SRL accuracy. Second, we achieve accuracy higher than that reported in (Jiang and Ng, 2006) and advance the state of the art in SRL research. The rest of this paper is organized as follows. We give an overview of NomBank and ASO in Sections 2 and 3 respectively. The baseline linear classifier is described in detail in Section 4, fol"
P07-1027,P98-1013,0,0.0205533,"the use of auxiliary problems. In this paper, we explore a number of different auxiliary problems, and we are able to significantly improve the accuracy of the NomBank SRL task using this approach. To our knowledge, our proposed approach achieves the highest accuracy published to date on the English NomBank SRL task. 1 Introduction The task of Semantic Role Labeling (SRL) is to identify predicate-argument relationships in natural language texts in a domain-independent fashion. In recent years, the availability of large human-labeled corpora such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998) has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language texts. A large number of SRL systems have been evaluated and compared on the standard data set in the CoNLL shared tasks (Carreras and Marquez, 2004; Carreras and Marquez, 2005), and many systems have performed reasonably well. Compared to the previous CoNLL shared tasks (noun phrase bracketing, chunking, clause identification, and named entity recognition), SRL represents a significant step 208 towards processing the semantic content of natural language texts. Although verbs are"
P07-1027,W04-2412,0,0.0220173,"t accuracy published to date on the English NomBank SRL task. 1 Introduction The task of Semantic Role Labeling (SRL) is to identify predicate-argument relationships in natural language texts in a domain-independent fashion. In recent years, the availability of large human-labeled corpora such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998) has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language texts. A large number of SRL systems have been evaluated and compared on the standard data set in the CoNLL shared tasks (Carreras and Marquez, 2004; Carreras and Marquez, 2005), and many systems have performed reasonably well. Compared to the previous CoNLL shared tasks (noun phrase bracketing, chunking, clause identification, and named entity recognition), SRL represents a significant step 208 towards processing the semantic content of natural language texts. Although verbs are probably the most obvious predicates in a sentence, many nouns are also capable of having complex argument structures, often with much more flexibility than its verb counterpart. For example, compare affect and effect: [subj Auto prices] [arg−ext greatly] [pred a"
P07-1027,W05-0620,0,0.0515272,"on the English NomBank SRL task. 1 Introduction The task of Semantic Role Labeling (SRL) is to identify predicate-argument relationships in natural language texts in a domain-independent fashion. In recent years, the availability of large human-labeled corpora such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998) has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language texts. A large number of SRL systems have been evaluated and compared on the standard data set in the CoNLL shared tasks (Carreras and Marquez, 2004; Carreras and Marquez, 2005), and many systems have performed reasonably well. Compared to the previous CoNLL shared tasks (noun phrase bracketing, chunking, clause identification, and named entity recognition), SRL represents a significant step 208 towards processing the semantic content of natural language texts. Although verbs are probably the most obvious predicates in a sentence, many nouns are also capable of having complex argument structures, often with much more flexibility than its verb counterpart. For example, compare affect and effect: [subj Auto prices] [arg−ext greatly] [pred affect] [obj the PPI]. [subj A"
P07-1027,P05-1022,0,0.0278434,"ombined task, we run the identification task with gold parse trees, and then the classification task with the output of the identification task. This way the combined effect of errors from both stages on the final classification output can be assessed. The scores of this complete SRL system are presented in the third row of Table 3. To test the performance of the combined task on automatic parse trees, we employ two different configurations. First, we train the various classifiers on sections 2 to 21 using gold argument labels and automatic parse trees produced by Charniak’s reranking parser (Charniak and Johnson, 2005), and test them on section 23 with automatic parse trees. 212 Our maximum entropy classifier consistently outperforms (Jiang and Ng, 2006), which also uses a maximum entropy classifier. The primary difference is that we use a later version of NomBank (September 2006 release vs. September 2005 release). In addition, we use somewhat different features and treat overlapping arguments differently. 5 Applying ASO to SRL Our ASO classifier uses the same features as the baseline linear classifier. The defining characteristic, and also the major challenge in successfully applying the ASO algorithm is"
P07-1027,J02-3001,0,0.12933,"o apply machine learning techniques to the task. So far we are aware of only one English NomBank-based SRL system (Jiang and Ng, 2006), which uses the maximum entropy classifier, although similar efforts are reported on the Chinese NomBank by (Xue, 2006) Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208–215, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and on FrameNet by (Pradhan et al., 2004) using a small set of hand-selected nominalizations. Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004). Semantic role labeling of NomBank is a multiclass classification problem by nature. Using the one-vs-all arrangement, that is, one binary classifier for each possible outcome, the SRL task can be treated as multiple binary classification problems. In the latter view, we are presented with the opportunity to exploit the common structures of these related problems. This is known as multi-task learning in the machine learning literature (Caruana, 1997; Ben-David and Schuller, 2003; Evgeniou and Pontil, 2004; Micchelli"
P07-1027,W06-1617,1,0.654787,"n the PPI]. The [pred effect] [subj of auto prices] [obj on the PPI] is [arg−ext big]. [subj The auto prices’] [pred effect] [obj on the PPI] is [arg−ext big]. The arguments of noun predicates can often be more easily omitted compared to the verb predicates: The [pred effect] [subj of auto prices] is [arg−ext big]. The [pred effect] [obj on the PPI] is [arg−ext big]. The [pred effect] is [arg−ext big]. With the recent release of NomBank (Meyers et al., 2004), it becomes possible to apply machine learning techniques to the task. So far we are aware of only one English NomBank-based SRL system (Jiang and Ng, 2006), which uses the maximum entropy classifier, although similar efforts are reported on the Chinese NomBank by (Xue, 2006) Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208–215, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and on FrameNet by (Pradhan et al., 2004) using a small set of hand-selected nominalizations. Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004). Semantic role labeling of Nom"
P07-1027,W04-0803,0,0.0140388,"h NomBank-based SRL system (Jiang and Ng, 2006), which uses the maximum entropy classifier, although similar efforts are reported on the Chinese NomBank by (Xue, 2006) Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208–215, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and on FrameNet by (Pradhan et al., 2004) using a small set of hand-selected nominalizations. Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004). Semantic role labeling of NomBank is a multiclass classification problem by nature. Using the one-vs-all arrangement, that is, one binary classifier for each possible outcome, the SRL task can be treated as multiple binary classification problems. In the latter view, we are presented with the opportunity to exploit the common structures of these related problems. This is known as multi-task learning in the machine learning literature (Caruana, 1997; Ben-David and Schuller, 2003; Evgeniou and Pontil, 2004; Micchelli and Pontil, 2005; Maurer, 2006). In this paper, we apply Alternating Structur"
P07-1027,W04-2705,0,0.0569717,"ple, compare affect and effect: [subj Auto prices] [arg−ext greatly] [pred affect] [obj the PPI]. [subj Auto prices] have a [arg−ext big] [pred effect] [obj on the PPI]. The [pred effect] [subj of auto prices] [obj on the PPI] is [arg−ext big]. [subj The auto prices’] [pred effect] [obj on the PPI] is [arg−ext big]. The arguments of noun predicates can often be more easily omitted compared to the verb predicates: The [pred effect] [subj of auto prices] is [arg−ext big]. The [pred effect] [obj on the PPI] is [arg−ext big]. The [pred effect] is [arg−ext big]. With the recent release of NomBank (Meyers et al., 2004), it becomes possible to apply machine learning techniques to the task. So far we are aware of only one English NomBank-based SRL system (Jiang and Ng, 2006), which uses the maximum entropy classifier, although similar efforts are reported on the Chinese NomBank by (Xue, 2006) Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208–215, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and on FrameNet by (Pradhan et al., 2004) using a small set of hand-selected nominalizations. Noun predicates also appear in FrameNet s"
P07-1027,J05-1004,0,0.0738608,"ple tasks to improve accuracy, via the use of auxiliary problems. In this paper, we explore a number of different auxiliary problems, and we are able to significantly improve the accuracy of the NomBank SRL task using this approach. To our knowledge, our proposed approach achieves the highest accuracy published to date on the English NomBank SRL task. 1 Introduction The task of Semantic Role Labeling (SRL) is to identify predicate-argument relationships in natural language texts in a domain-independent fashion. In recent years, the availability of large human-labeled corpora such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998) has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language texts. A large number of SRL systems have been evaluated and compared on the standard data set in the CoNLL shared tasks (Carreras and Marquez, 2004; Carreras and Marquez, 2005), and many systems have performed reasonably well. Compared to the previous CoNLL shared tasks (noun phrase bracketing, chunking, clause identification, and named entity recognition), SRL represents a significant step 208 towards processing the semantic content of natural"
P07-1027,N04-4036,0,0.496391,"PPI] is [arg−ext big]. The [pred effect] is [arg−ext big]. With the recent release of NomBank (Meyers et al., 2004), it becomes possible to apply machine learning techniques to the task. So far we are aware of only one English NomBank-based SRL system (Jiang and Ng, 2006), which uses the maximum entropy classifier, although similar efforts are reported on the Chinese NomBank by (Xue, 2006) Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208–215, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and on FrameNet by (Pradhan et al., 2004) using a small set of hand-selected nominalizations. Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004). Semantic role labeling of NomBank is a multiclass classification problem by nature. Using the one-vs-all arrangement, that is, one binary classifier for each possible outcome, the SRL task can be treated as multiple binary classification problems. In the latter view, we are presented with the opportunity to exploit the common structures of these related problems. This is know"
P07-1027,P05-1073,0,0.0636168,"tification Eighteen baseline features and six additional features are proposed in (Jiang and Ng, 2006) for NomBank argument identification. As the improvement of the F1 score due to the additional features is not statistically significant, we use the set of eighteen baseline features for simplicity. These features are reproduced in Table 1 for easy reference. Unlike in (Jiang and Ng, 2006), we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training data. Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al., 2005). After the features of every constituent are extracted, each constituent is simply classified independently as either argument or non-argument. The linear classifier described above is trained on sections 2 to 21 and tested on section 23. A maximum entropy classifier is trained and tested in the same manner. The F1 scores are presented in the first row of Table 3, in columns linear and maxent respectively. The J&N column presents the result reported in (Jiang and Ng, 2006) using both baseline and additional features. The last column aso presents the best result from this work, to be explained"
P07-1027,W04-3212,0,0.0413667,"m C to the lowest common ancestor with P ptype & length of path pred & hw pred & path pred & position Table 1: Features used in argument identification data, we pick the first and discard the rest. (Note that the same is not done on the test data.) A diverse set of 28 features is used in (Jiang and Ng, 2006) for argument classification. In this work, the number of features is pruned to 11, so that we can work with reasonably many auxiliary problems in later experiments with ASO. To find a smaller set of effective features, we start with all the features considered in (Jiang and Ng, 2006), in (Xue and Palmer, 2004), and various combinations of them, for a total of 52 features. These features are then pruned by the following algorithm: 1. For each feature in the current feature set, do step (2). 2. Remove the selected feature from the feature set. Obtain the F1 score of the remaining features when applied to the argument classification task, on development data section 24 with gold identification. 3. Select the highest of all the scores obtained in 1 2 3 4 5 6 7 8 9 10 11 position to the left/right of or overlaps with the predicate ptype syntactic category (phrase type) of the constituent C firstword fir"
P07-1027,N06-1055,0,0.549762,"on the PPI] is [arg−ext big]. The arguments of noun predicates can often be more easily omitted compared to the verb predicates: The [pred effect] [subj of auto prices] is [arg−ext big]. The [pred effect] [obj on the PPI] is [arg−ext big]. The [pred effect] is [arg−ext big]. With the recent release of NomBank (Meyers et al., 2004), it becomes possible to apply machine learning techniques to the task. So far we are aware of only one English NomBank-based SRL system (Jiang and Ng, 2006), which uses the maximum entropy classifier, although similar efforts are reported on the Chinese NomBank by (Xue, 2006) Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208–215, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and on FrameNet by (Pradhan et al., 2004) using a small set of hand-selected nominalizations. Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004). Semantic role labeling of NomBank is a multiclass classification problem by nature. Using the one-vs-all arrangement, that is, one binary classifier"
P07-1027,C98-1013,0,\N,Missing
P07-1087,P00-1037,0,0.0497676,"nly local information but also global information in a document in case restoration. Spelling error correction can be formalized as a classification problem. Golding and Roth (1996) propose using the Winnow algorithm to address the issue. The problem can also be formalized as that of data conversion using the source channel model. The source model can be built as an n-gram language model and the channel model can be constructed with confusing words measured by edit distance. Brill and Moore, Church and Gale, and Mayes et al. have developed different techniques for confusing words calculation (Brill and Moore, 2000; Church and Gale, 1991; Mays et al., 1991). Sproat et al. (1999) have investigated normalization of non-standard words in texts, including numbers, abbreviations, dates, currency amounts, and acronyms. They propose a taxonomy of nonstandard words and apply n-gram language models, decision trees, and weighted finite-state transducers to the normalization. 3 Text Normalization In this paper we define text normalization at three levels: paragraph, sentence, and word level. The subtasks at each level are listed in Table 1. For example, at the paragraph level, there are two subtasks: extra line-br"
P07-1087,P03-1020,0,0.337923,"ils using two machine-learning based methods: Conditional Random Fields and Perceptron for learning HMMs. See also (Carvalho and Cohen, 2004). Tang et al. (2005) propose a cascaded approach for email data cleaning by employing Support Vector Machines and rules. Their method can detect email headers, signatures, program codes, and extra line breaks in emails. See also (Wong et al., 2007). Palmer and Hearst (1997) propose using a Neural Network model to determine whether a period in a sentence is the ending mark of the sentence, an abbreviation, or both. See also (Mikheev, 2000; Mikheev, 2002). Lita et al. (2003) propose employing a language modeling approach to address the case restoration problem. They define four classes for word casing: all letters in lower case, first letter in uppercase, all letters in upper case, and mixed case, and formalize the problem as assigning class labels to words in natural language texts. Mikheev (2002) proposes using not only local information but also global information in a document in case restoration. Spelling error correction can be formalized as a classification problem. Golding and Roth (1996) propose using the Winnow algorithm to address the issue. The proble"
P07-1087,J02-3002,0,0.0262953,"cognition in emails using two machine-learning based methods: Conditional Random Fields and Perceptron for learning HMMs. See also (Carvalho and Cohen, 2004). Tang et al. (2005) propose a cascaded approach for email data cleaning by employing Support Vector Machines and rules. Their method can detect email headers, signatures, program codes, and extra line breaks in emails. See also (Wong et al., 2007). Palmer and Hearst (1997) propose using a Neural Network model to determine whether a period in a sentence is the ending mark of the sentence, an abbreviation, or both. See also (Mikheev, 2000; Mikheev, 2002). Lita et al. (2003) propose employing a language modeling approach to address the case restoration problem. They define four classes for word casing: all letters in lower case, first letter in uppercase, all letters in upper case, and mixed case, and formalize the problem as assigning class labels to words in natural language texts. Mikheev (2002) proposes using not only local information but also global information in a document in case restoration. Spelling error correction can be formalized as a classification problem. Golding and Roth (1996) propose using the Winnow algorithm to address t"
P07-1087,H05-1056,0,0.0346425,"zation is usually viewed as an engineering issue and is addressed in an ad-hoc manner. Much of the previous work focuses on processing texts in clean form, not texts in informal form. Also, prior work mostly focuses on processing one type or a small number of types of errors, whereas this paper deals with many different types of errors. Clark (2003) has investigated the problem of preprocessing noisy texts for natural language processing. He proposes identifying token boundaries and sentence boundaries, restoring cases of words, and correcting misspelled words by using a source channel model. Minkov et al. (2005) have investigated the problem of named entity recognition in informally in689 putted texts. They propose improving the performance of personal name recognition in emails using two machine-learning based methods: Conditional Random Fields and Perceptron for learning HMMs. See also (Carvalho and Cohen, 2004). Tang et al. (2005) propose a cascaded approach for email data cleaning by employing Support Vector Machines and rules. Their method can detect email headers, signatures, program codes, and extra line breaks in emails. See also (Wong et al., 2007). Palmer and Hearst (1997) propose using a N"
P07-1087,J97-2002,0,\N,Missing
P08-1007,W05-0909,0,0.189252,"l role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation. In this paper, we propose a new automatic MT evaluation metric, M AX S IM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations. Recognizing that different concepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other. Having weighted matches between items means that there could be many possible ways to match, or link items fr"
P08-1007,W07-0718,0,0.444735,"human evaluation of MT output is time consuming and expensive, having a robust and accurate automatic MT evaluation metric that correlates well with human judgement is invaluable. Among all the automatic MT evaluation metrics, BLEU (Papineni et al., 2002) is the most widely used. Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation. In this paper, we propose a new automatic MT evaluation metric, M AX S IM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations. Recognizing that different concepts can be expressed in a variety of ways, we allow m"
P08-1007,W07-0738,0,0.315981,"neni et al., 2002) is the most widely used. Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation. In this paper, we propose a new automatic MT evaluation metric, M AX S IM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations. Recognizing that different concepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other. Having weighted matches between i"
P08-1007,W07-0734,0,0.0826611,"sions, and there are four English reference translation texts. Since implementations of the BLEU and METEOR metrics are publicly available, we score the system submissions using BLEU (version 11b with its default settings), METEOR, and M AX S IM, showing the resulting correlations in Table 4. For METEOR, when used with its originally proposed parameter values of (α=0.9, β=3.0, γ=0.5), which the METEOR researchers mentioned were based on some early experimental work (Banerjee and Lavie, 2005), we obtain an average correlation value of 0.915, as shown in the row “METEOR”. In the recent work of (Lavie and Agarwal, 2007), the values of these parameters were tuned to be (α=0.81, β=0.83, γ=0.28), based on experiments on the NIST 2003 and 2004 Arabic-English evaluation datasets. When METEOR was run with these new parameter values, it returned an average correlation value of 0.972, as shown in the row “METEOR (optimized)”. M AX S IM using only n-gram information (M AX S IMn ) gives an average correlation value of 0.800, while adding dependency information (M AX S IMn+d ) improves the correlation value to 0.915. Note that so far, the parameters of M AX S IM are not optimized and we simply perform uniform averaging"
P08-1007,P05-1012,0,0.00651362,"l METEOR BLEU Adequacy 0.780 0.804 0.774 0.712 0.701 0.690 Fluency 0.827 0.845 0.839 0.742 0.719 0.722 Rank 0.875 0.893 0.804 0.769 0.746 0.672 Constituent 0.760 0.766 0.742 0.798 0.670 0.603 Average 0.811 0.827 0.790 0.755 0.709 0.672 Table 1: Overall correlations on the Europarl and News Commentary datasets. The “Semantic-role overlap” metric is abbreviated as “Semantic-role”. Note that each figure above represents 6 translation tasks: the Europarl and News Commentary datasets each with 3 language pairs (German-English, Spanish-English, French-English). In our work, we train the MSTParser4 (McDonald et al., 2005) on the Penn Treebank Wall Street Journal (WSJ) corpus, and use it to extract dependency relations from a sentence. Currently, we focus on extracting only two relations: subject and object. For each relation (ch, dp, pa) extracted, we note the child lemma ch of the relation (often a noun), the relation type dp (either subject or object), and the parent lemma pa of the relation (often a verb). Then, using the system relations and reference relations extracted from a system-reference sentence pair, we similarly construct a bipartite graph, where each node is a relation (ch, dp, pa). We define th"
P08-1007,N03-2021,0,0.0244134,"y of the alignment. Note that METEOR consists of three parameters that need to be optimized based on experimentation: α, β, and γ. 3 Metric Design Considerations We first review some aspects of existing metrics and highlight issues that should be considered when designing an MT evaluation metric. • Intuitive interpretation: To compensate for the lack of recall, BLEU incorporates a brevity penalty. This, however, prevents an intuitive interpretation of its scores. To address this, standard measures like precision and recall could be used, as in some previous research (Banerjee and Lavie, 2005; Melamed et al., 2003). • Allowing for variation: BLEU only counts exact word matches. Languages, however, often allow a great deal of variety in vocabulary and in the ways concepts are expressed. Hence, using information such as synonyms or dependency relations could potentially address the issue better. • Matches should be weighted: Current metrics either match, or don’t match a pair of items. We note, however, that matches between items (such as words, n-grams, etc.) should be weighted according to their degree of similarity. 4 The Maximum Similarity Metric We now describe our proposed metric, Maximum Similarity"
P08-1007,J05-1004,0,0.00309769,"(Gimenez and Marquez, 2007) proposed using deeper linguistic information to evaluate MT performance. For evaluation in the ACL-07 MT workshop, the authors used the metric which they termed as SR-Or -*1 . This metric first counts the number of lexical overlaps SR-Or -t for all the different semantic roles t that are found in the system and reference translation sentence. A uniform average of the counts is then taken as the score for the sentence pair. In their work, the different semantic roles t they considered include the various core and adjunct arguments as defined in the PropBank project (Palmer et al., 2005). For instance, SR-O r -A0 refers to the number of lexical overlaps between the A0 arguments. To extract semantic roles from a sentence, several processes such as lemmatization, partof-speech tagging, base phrase chunking, named entity tagging, and finally semantic role tagging need to be performed. 2.3 ParaEval The ParaEval metric (Zhou et al., 2006) uses a large collection of paraphrases, automatically extracted from parallel corpora, to evaluate MT performance. To compare a pair of sentences, ParaEval first locates paraphrase matches between the two 1 Verified through personal communication"
P08-1007,P02-1040,0,0.102976,"luated on data from the ACL-07 MT workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic MT evaluation metrics that were evaluated during the workshop. 1 Introduction In recent years, machine translation (MT) research has made much progress, which includes the introduction of automatic metrics for MT evaluation. Since human evaluation of MT output is time consuming and expensive, having a robust and accurate automatic MT evaluation metric that correlates well with human judgement is invaluable. Among all the automatic MT evaluation metrics, BLEU (Papineni et al., 2002) is the most widely used. Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marque"
P08-1007,rajman-hartley-2002-automatic,0,0.0159518,"similarly calculate their respective w(M ) and add to the corresponding matchuni and matchtri . Based on matchuni , matchbi , and matchtri , we calculate their corresponding precision P and recall R, from which we obtain their respective F mean scores via Equation 1. Using bigrams for illustration, we calculate its P and R as: P = R = 4.2 matchbi no. of bigrams in system translation matchbi no. of bigrams in reference translation Dependency Relations Besides matching a pair of system-reference sentences based on the surface form of words, previous work such as (Gimenez and Marquez, 2007) and (Rajman and Hartley, 2002) had shown that deeper linguistic knowledge such as semantic roles and syntax can be usefully exploited. In the previous subsection, we describe our method of using bipartite graphs for matching of ngrams found in a sentence pair. This use of bipartite graphs, however, is a very general framework to obtain an optimal alignment of the corresponding “information items” contained within a sentence pair. Hence, besides matching based on n-gram strings, we can also match other “information items”, such as dependency relations. Metric M AX S IMn+d M AX S IMn Semantic-role ParaEval-recall METEOR BLEU"
P08-1007,W96-0213,0,0.0265749,"ics either match, or don’t match a pair of items. We note, however, that matches between items (such as words, n-grams, etc.) should be weighted according to their degree of similarity. 4 The Maximum Similarity Metric We now describe our proposed metric, Maximum Similarity (M AX S IM), which is based on precision and recall, allows for synonyms, and weights the matches found. Given a pair of English sentences to be compared (a system translation against a reference translation), we perform tokenization 2 , lemmatization using WordNet3 , and part-of-speech (POS) tagging with the MXPOST tagger (Ratnaparkhi, 1996). Next, we remove all non-alphanumeric tokens. Then, we match the unigrams in the system translation to the unigrams in the reference translation. Based on the matches, we calculate the recall and precision, which we then combine into a single Fmean unigram score using Equation 1. Similarly, we also match the bigrams and trigrams of the sentence pair and calculate their corresponding F mean scores. To obtain a single similarity score score s for this sentence pair s, we simply average the three Fmean scores. Then, to obtain a single similarity score sim-score for the entire system corpus, we r"
P08-1007,H05-1010,0,0.0200348,"em to at most one reference item, we model the items in the sentence pair as nodes in a bipartite graph and use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to find a maximum weight matching (or alignment) between the items in polynomial time. The weights (from the edges) of the resulting graph will then be added to determine the final similarity score between the pair of sentences. 55 Proceedings of ACL-08: HLT, pages 55–62, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Although a maximum weight bipartite graph was also used in the recent work of (Taskar et al., 2005), their focus was on learning supervised models for single word alignment between sentences from a source and target language. The contributions of this paper are as follows. Current metrics (such as BLEU, METEOR, Semantic-role overlap, ParaEval-recall, etc.) do not assign different weights to their matches: either two items match, or they don’t. Also, metrics such as METEOR determine an alignment between the items of a sentence pair by using heuristics such as the least number of matching crosses. In contrast, we propose weighting different matches differently, and then obtain an optimal set"
P08-1007,W06-1610,0,0.22521,"lthough BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement. The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation. In this paper, we propose a new automatic MT evaluation metric, M AX S IM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations. Recognizing that different concepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other. Having weighted matches between items means that there could be many p"
P10-1113,P98-1013,0,0.00848289,"arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-b"
P10-1113,W04-2412,0,0.0830406,"ue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL,"
P10-1113,W05-0620,0,0.203606,"guments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more ser"
P10-1113,J05-1004,0,0.0735968,"s importance in natural language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both"
P10-1113,P01-1017,0,0.30443,"techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more seriously from syntactic parsing. Xue (2008) evaluated on Chinese PropBank and showed that the performance of Chinese verbal SRL drops by about 25 in F1-measure when replacing gold parse trees with automatic ones. Likewise, Xue (2008) and Li et al. (2009) reported a performance drop of about 12 in F1-measure in Chinese NomBank SRL. 1108 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108–1117, c Uppsala, Sw"
P10-1113,N09-1037,0,0.023537,"eir method was biased against these roles in general, thus lowering recall for them (e.g., precision of 87.6 and recall of 65.8). There have been other efforts in NLP on joint learning with various degrees of success. In particular, the recent shared tasks of CoNLL 2008 and 2009 (Surdeanu et al., 2008; Hajic et al., 2009) tackled joint parsing of syntactic and semantic dependencies. However, all the top 5 reported systems decoupled the tasks, rather than building joint models. Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al. (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. In addition, attempts on joint Chinese word segmentation and part-of-speech (POS) tagging (Ng and Low, 2004; Zhang and Clark, 2008) also illustrate the benefits of joint learning. 1109 TOP IP VP Arg1/Rel2 Arg0/Rel1 VP Arg0/Rel2 Arg2/Rel1 NP NN 中国 Chinese 政府 govt. P 向 to Arg1/Rel1 Sup/Rel2 Rel1 PP NR PU NP NR NP VV NN 朝鲜 政府 N. Korean govt. 。 . ArgM-MNR/Rel2 Rel2 NN NN 提供 provide 人民币 RMB Chinese government provides RMB loan to North Korean govern"
P10-1113,D09-1103,1,0.85366,"top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more seriously from syntactic parsing. Xue (2008) evaluated on Chinese PropBank and showed that the performance of Chinese verbal SRL drops by about 25 in F1-measure when replacing gold parse trees with automatic ones. Likewise, Xue (2008) and Li et al. (2009) reported a performance drop of about 12 in F1-measure in Chinese NomBank SRL. 1108 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108–1117, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics While it may be difficult to further improve syntactic parsing, a promising alternative is to perform both syntactic and semantic parsing in an integrated way. Given the close interaction between the two tasks, joint learning not only allows uncertainty about syntactic parsing to be carried forward to semantic parsing but als"
P10-1113,P07-1027,1,0.827603,"nt and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more seriously from syntactic parsing. Xue (2008)"
P10-1113,H05-1078,0,0.11609,"Missing"
P10-1113,W08-2101,0,0.0534059,"hey incorporated semantic role information into syntactic parse trees by extending syntactic constituent labels with their coarse-grained semantic roles (core argument or adjunct argument) in the sentence, and thus unified semantic parsing and syntactic parsing. The actual fine-grained semantic roles are assigned, as in other methods, by an ensemble classifier. However, the results obtained with this method were negative, and they concluded that semantic parsing on PropBank was too difficult due to the differences between chunk annotation and tree structure. Motivated by Yi and Palmer (2005), Merlo and Musillo (2008) first extended a statistical parser to produce a richly annotated tree that identifies and labels nodes with semantic role labels as well as syntactic labels. Then, they explored both rule-based and machine learning techniques to extract predicate-argument structures from this enriched output. Their experiments showed that their method was biased against these roles in general, thus lowering recall for them (e.g., precision of 87.6 and recall of 65.8). There have been other efforts in NLP on joint learning with various degrees of success. In particular, the recent shared tasks of CoNLL 2008 a"
P10-1113,meyers-etal-2004-annotating,0,0.0172328,"ion answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SR"
P10-1113,A00-2030,0,0.0214221,"xperiments showed that their method was biased against these roles in general, thus lowering recall for them (e.g., precision of 87.6 and recall of 65.8). There have been other efforts in NLP on joint learning with various degrees of success. In particular, the recent shared tasks of CoNLL 2008 and 2009 (Surdeanu et al., 2008; Hajic et al., 2009) tackled joint parsing of syntactic and semantic dependencies. However, all the top 5 reported systems decoupled the tasks, rather than building joint models. Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al. (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. In addition, attempts on joint Chinese word segmentation and part-of-speech (POS) tagging (Ng and Low, 2004; Zhang and Clark, 2008) also illustrate the benefits of joint learning. 1109 TOP IP VP Arg1/Rel2 Arg0/Rel1 VP Arg0/Rel2 Arg2/Rel1 NP NN 中国 Chinese 政府 govt. P 向 to Arg1/Rel1 Sup/Rel2 Rel1 PP NR PU NP NR NP VV NN 朝鲜 政府 N. Korean govt. 。 . ArgM-MNR/Rel2 Rel2 NN NN 提供 provide 人民币 RMB Chinese government provides R"
P10-1113,W06-1617,1,0.740505,"d nominal predicates in an integrated way. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 presents our baseline systems for syntactic and semantic parsing. Section 4 presents our proposed method of joint syntactic and semantic parsing for Chinese texts. Section 5 presents the experimental results. Finally, Section 6 concludes the paper. 2 Related Work Compared to the large body of work on either syntactic parsing (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001; Petrov and Klein, 2007), or SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Jiang and Ng, 2006), there is relatively less work on their joint learning. Koomen et al. (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. As an alternative to the above pseudo-joint learning methods (strictly speaking, they"
P10-1113,W05-0625,0,0.017597,"ed as follows. Section 2 reviews related work. Section 3 presents our baseline systems for syntactic and semantic parsing. Section 4 presents our proposed method of joint syntactic and semantic parsing for Chinese texts. Section 5 presents the experimental results. Finally, Section 6 concludes the paper. 2 Related Work Compared to the large body of work on either syntactic parsing (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001; Petrov and Klein, 2007), or SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Jiang and Ng, 2006), there is relatively less work on their joint learning. Koomen et al. (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. As an alternative to the above pseudo-joint learning methods (strictly speaking, they are still pipeline methods), one can augment the syntactic label of a constit"
P10-1113,N07-1051,0,0.0110215,"nowledge, this is the first research on exploring syntactic parsing and SRL for verbal and nominal predicates in an integrated way. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 presents our baseline systems for syntactic and semantic parsing. Section 4 presents our proposed method of joint syntactic and semantic parsing for Chinese texts. Section 5 presents the experimental results. Finally, Section 6 concludes the paper. 2 Related Work Compared to the large body of work on either syntactic parsing (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001; Petrov and Klein, 2007), or SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Jiang and Ng, 2006), there is relatively less work on their joint learning. Koomen et al. (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. A"
P10-1113,P03-1002,0,0.074885,"ion Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0"
P10-1113,W05-0636,0,0.0930155,"ic parsing for Chinese texts. Section 5 presents the experimental results. Finally, Section 6 concludes the paper. 2 Related Work Compared to the large body of work on either syntactic parsing (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001; Petrov and Klein, 2007), or SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Jiang and Ng, 2006), there is relatively less work on their joint learning. Koomen et al. (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. As an alternative to the above pseudo-joint learning methods (strictly speaking, they are still pipeline methods), one can augment the syntactic label of a constituent with semantic information, like what function parsing does (Merlo and Musillo, 2005). Yi and Palmer (2005) observed that the distributions of semantic labels could potentially interact with th"
P10-1113,W03-1707,0,0.0557221,"al language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kern"
P10-1113,xue-2006-annotating,0,0.0182294,"nan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras"
P10-1113,J08-2004,0,0.108807,"Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more seriously from syntactic parsing. Xue (2008) evaluated on Chinese PropBank and showed that the performance of Chinese verbal SRL drops by about 25 in F1-measure when replacing gold parse trees with automatic ones. Likewise, Xue (2008) and Li et al. (2009) reported a performance drop of about 12 in F1-measure in Chinese NomBank SRL. 1108 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108–1117, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics While it may be difficult to further improve syntactic parsing, a promising alternative is to perform both syntactic"
P10-1113,W05-0639,0,0.0182374,"m into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. As an alternative to the above pseudo-joint learning methods (strictly speaking, they are still pipeline methods), one can augment the syntactic label of a constituent with semantic information, like what function parsing does (Merlo and Musillo, 2005). Yi and Palmer (2005) observed that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefined the boundaries of constituents. Based on this observation, they incorporated semantic role information into syntactic parse trees by extending syntactic constituent labels with their coarse-grained semantic roles (core argument or adjunct argument) in the sentence, and thus unified semantic parsing and syntactic parsing. The actual fine-grained semantic roles are assigned, as in other methods, by an ensemble classifier. However, the results obtained with this"
P10-1113,P08-1101,0,0.0406815,"anu et al., 2008; Hajic et al., 2009) tackled joint parsing of syntactic and semantic dependencies. However, all the top 5 reported systems decoupled the tasks, rather than building joint models. Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al. (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. In addition, attempts on joint Chinese word segmentation and part-of-speech (POS) tagging (Ng and Low, 2004; Zhang and Clark, 2008) also illustrate the benefits of joint learning. 1109 TOP IP VP Arg1/Rel2 Arg0/Rel1 VP Arg0/Rel2 Arg2/Rel1 NP NN 中国 Chinese 政府 govt. P 向 to Arg1/Rel1 Sup/Rel2 Rel1 PP NR PU NP NR NP VV NN 朝鲜 政府 N. Korean govt. 。 . ArgM-MNR/Rel2 Rel2 NN NN 提供 provide 人民币 RMB Chinese government provides RMB loan to North Korean government. 贷款 loan Figure 1: Two predicates (Rel1 and Rel2) and their arguments in the style of Chinese PropBank and NomBank. 3 Baseline: Pipeline Top-Best Parse Tree Parsing on In this section, we briefly describe our approach to syntactic parsing and semantic role labeling, as well as"
P10-1113,C04-1100,0,0.0259772,"and nominal predicates in an integrated way. 1 Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semant"
P10-1113,W04-3236,1,\N,Missing
P10-1113,W08-2121,0,\N,Missing
P10-1113,J03-4003,0,\N,Missing
P10-1113,C98-1013,0,\N,Missing
P10-1113,D09-1133,1,\N,Missing
P10-1113,W09-1201,0,\N,Missing
P10-4014,D07-1007,0,0.00794707,"k of IMS allows users to integrate different preprocessing tools, additional features, and different classifiers. By default, we use linear support vector machines as the classifier with multiple knowledge-based features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. 78 Proceedings of the ACL 2010 System Demonstrations, pages 78–83, c Uppsala, Sweden, 13 July 2010. 2010"
P10-4014,P07-1005,1,0.592126,"eprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, S EM C OR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996). A previous implementation of the IMS system, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achieves competitive accuracies on several SensEval/SemEval English lexical-sample and all-words tasks. The remainder of this paper is organized as follows. Section 2 gives the system description, which introduces the system framework and the details of the implementation. In Section 3, we present the evaluation results of IMS on SensEWord sense disambiguation (WSD) systems based on supervised learning achieved the best pe"
P10-4014,W04-0827,0,0.0498239,"t outline the IMS system, and introduce the default preprocessing tools, the feature types, and the machine learning method used in our implementation. Then we briefly explain the collection of training data for content words. 2.1 2.1.2 Feature and Instance Extraction After gathering the formatted information in the preprocessing step, we use an instance extractor together with a list of feature extractors to extract the instances and their associated features. Previous research has found that combining multiple knowledge sources achieves high WSD accuracy (Ng and Lee, 1996; Lee and Ng, 2002; Decadt et al., 2004). In IMS, we follow Lee and Ng (2002) and combine three knowledge sources for all content word types4 : System Architecture Figure 1 shows the system architecture of IMS. The system accepts any input text. For each content word w (noun, verb, adjective, or adverb) in the input text, IMS disambiguates the sense of w and outputs a list of the senses of w, where each sense si is assigned a probability according to the likelihood of si appearing in that context. The sense inventory used is based on WordNet (Miller, 1990) version 1.7.1. IMS consists of three independent modules: preprocessing, feat"
P10-4014,P96-1006,1,0.282613,"in using a WSD component. Users can choose different tools to perform preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, S EM C OR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996). A previous implementation of the IMS system, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achieves competitive accuracies on several SensEval/SemEval English lexical-sample and all-words tasks. The remainder of this paper is organized as follows. Section 2 gives the system description, which introduces the system framework and the details of the implementation. In Section 3, we present the evaluation results of IMS on SensEWord sense disamb"
P10-4014,P00-1056,0,0.0299863,"Missing"
P10-4014,W02-1006,1,0.629767,"es or unsupervised methods. An open source supervised WSD system will promote the use of WSD in other applications. In this paper, we present an English all-words WSD system, IMS (It Makes Sense), built using a supervised learning approach. IMS is a Java implementation, which provides an extensible and flexible platform for researchers interested in using a WSD component. Users can choose different tools to perform preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, S EM C OR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996). A previous implementation of the IMS system, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achie"
P10-4014,S01-1005,0,0.838496,"multiple knowledge-based features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. 78 Proceedings of the ACL 2010 System Demonstrations, pages 78–83, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics val/SemEval English tasks. Finally, we conclude in Section 4. 2 By default, the sentence splitter and POS tagger in the OpenNLP toolkit1 are us"
P10-4014,I05-3025,1,0.417537,"Missing"
P10-4014,S07-1016,0,0.170126,"ementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. 78 Proceedings of the ACL 2010 System Demonstrations, pages 78–83, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics val/SemEval English tasks. Finally, we conclude in Section 4. 2 By default, the sentence splitter and POS tagger in the OpenNLP toolkit1 are used for sentence splitting and POS tagging. A Jav"
P10-4014,P05-3014,0,0.040412,"Missing"
P10-4014,W04-0811,0,0.85472,"sed features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. 78 Proceedings of the ACL 2010 System Demonstrations, pages 78–83, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics val/SemEval English tasks. Finally, we conclude in Section 4. 2 By default, the sentence splitter and POS tagger in the OpenNLP toolkit1 are used for sentence splitting"
P10-4014,S01-1031,0,0.0307736,"classification models trained with the senseannotated training examples from S EM C OR, DSO corpus, and 6 parallel corpora, for all content words. Evaluation on English all-words tasks shows that IMS with these models achieves stateof-the-art WSD accuracies compared to the top participating systems. As a Java-based system, IMS is platform independent. The source code of IMS and the classification models can be found on the homepage: http://nlp.comp.nus.edu. sg/software and are available for research, non-commercial use. two systems that participated in the above tasks (Yarowsky et al., 2001; Mihalcea and Moldovan, 2001; Mihalcea et al., 2004). Evaluation results show that IMS achieves significantly better accuracies than the MFS baseline. Comparing to the top participating systems, IMS achieves comparable results. 3.2 English All-Words Tasks In SensEval and SemEval English all-words tasks, no training data are provided. Therefore, the MFS baseline is no longer suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in S EM C OR, the WordNet first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will use it as the baseline in a"
P10-4014,W04-0807,0,0.024024,"d with the senseannotated training examples from S EM C OR, DSO corpus, and 6 parallel corpora, for all content words. Evaluation on English all-words tasks shows that IMS with these models achieves stateof-the-art WSD accuracies compared to the top participating systems. As a Java-based system, IMS is platform independent. The source code of IMS and the classification models can be found on the homepage: http://nlp.comp.nus.edu. sg/software and are available for research, non-commercial use. two systems that participated in the above tasks (Yarowsky et al., 2001; Mihalcea and Moldovan, 2001; Mihalcea et al., 2004). Evaluation results show that IMS achieves significantly better accuracies than the MFS baseline. Comparing to the top participating systems, IMS achieves comparable results. 3.2 English All-Words Tasks In SensEval and SemEval English all-words tasks, no training data are provided. Therefore, the MFS baseline is no longer suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in S EM C OR, the WordNet first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will use it as the baseline in all-words tasks. Using th"
P10-4014,H94-1046,0,0.870896,"xible platform for researchers interested in using a WSD component. Users can choose different tools to perform preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, S EM C OR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996). A previous implementation of the IMS system, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achieves competitive accuracies on several SensEval/SemEval English lexical-sample and all-words tasks. The remainder of this paper is organized as follows. Section 2 gives the system description, which introduces the system framework and the details of the implementation. In Section 3, we present the evaluation r"
P10-4014,S01-1040,0,\N,Missing
P10-4014,S07-1054,1,\N,Missing
P11-1092,W07-1604,0,0.0306729,"and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has"
P11-1092,J07-4004,0,0.0340842,"Missing"
P11-1092,I08-1059,0,0.199155,"r Computational Linguistics, pages 915–923, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work 3.1 In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth,"
P11-1092,N10-1019,0,0.482056,"2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, researchers have investigated GEC in connection with web-based models in NLP (Lapata and Keller, 2005; Bergsma et al., 2009; Yi et al., 2008). These methods do not use classifiers, but rely on simple n-gram counts or page hits from the Web. 3 Task Description In this work, we focus on article and preposition errors, as th"
P11-1092,han-etal-2010-using,0,0.339831,"memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work,"
P11-1092,P03-1054,0,0.00344484,"Missing"
P11-1092,P03-1004,0,0.247236,"Missing"
P11-1092,N04-2006,0,0.306252,"Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work 3.1 In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has"
P11-1092,J93-2004,0,0.0412043,"Missing"
P11-1092,W00-0708,0,0.070948,"r. 915 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 915–923, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work 3.1 In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed wor"
P11-1092,P06-1031,0,0.052864,"of the Association for Computational Linguistics, pages 915–923, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work 3.1 In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature"
P11-1092,D10-1094,0,0.273356,"Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, res"
P11-1092,N10-1018,0,0.450279,"Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, res"
P11-1092,C08-1109,0,0.663559,"ck et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. Th"
P11-1092,P10-2065,0,0.652037,"work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamo"
P11-1092,I08-2082,0,0.0449143,"cially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, researchers have investigated GEC in connection with web-based models in NLP (Lapata and Keller, 2005; Bergsma et al., 2009; Yi et al., 2008). These methods do not use classifiers, but rely on simple n-gram counts or page hits from the Web. 3 Task Description In this work, we focus on article and preposition errors, as they are among the most frequent types of errors made by EFL learners. 916 Selection vs. Correction Task There is an important difference between training on annotated learner text and training on non-learner text, namely whether the observed word can be used as a feature or not. When training on non-learner text, the observed word cannot be used as a feature. The word choice of the writer is “blanked out” from the t"
P11-1092,P03-2026,0,\N,Missing
P11-1100,P05-1018,0,0.806228,"implement and validate our model on three data sets, which show robust improvements over the current state-of-the-art for coherence assessment. We also provide the first assessment of the upper-bound of human performance on the standard task of distinguishing coherent from incoherent orderings. To the best our knowledge, this is also the first study in which we show output from an automatic discourse parser helps in coherence modeling. 2 Related Work The study of coherence in discourse has led to many linguistic theories, of which we only discuss algorithms that have been reduced to practice. Barzilay and Lapata (2005; 2008) proposed an entity-based model to represent and assess local textual coherence. The model is motivated by Centering Theory (Grosz et al., 1995), which states that subsequent sentences in a locally coherent text are likely to continue to focus on the same entities as in previous sentences. Barzilay and Lapata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HM"
P11-1100,J08-1001,0,0.827349,"Missing"
P11-1100,N04-1015,0,0.311771,"ve been reduced to practice. Barzilay and Lapata (2005; 2008) proposed an entity-based model to represent and assess local textual coherence. The model is motivated by Centering Theory (Grosz et al., 1995), which states that subsequent sentences in a locally coherent text are likely to continue to focus on the same entities as in previous sentences. Barzilay and Lapata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. The global coherence of a text can then be summarized by the overall probability of topic shift from the first sentence to the last. Following these two directions, Soricut and Marcu (2006) and Elsner et al. (2007) combined the entity-based and HMM-based models and demonstrated that these two models are complementary to each other in coherence assessment. Our approach differs from these models in that it introduces and operationalizes another in"
P11-1100,N07-1055,0,0.765675,"ata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. The global coherence of a text can then be summarized by the overall probability of topic shift from the first sentence to the last. Following these two directions, Soricut and Marcu (2006) and Elsner et al. (2007) combined the entity-based and HMM-based models and demonstrated that these two models are complementary to each other in coherence assessment. Our approach differs from these models in that it introduces and operationalizes another indicator of discourse coherence, by modeling a text’s discourse relation transitions. Karamanis (2007) has tried to integrate local discourse relations into the Centering-based coherence metrics for the task of information ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller data set and the way the dis"
P11-1100,J95-2003,0,0.847669,"de the first assessment of the upper-bound of human performance on the standard task of distinguishing coherent from incoherent orderings. To the best our knowledge, this is also the first study in which we show output from an automatic discourse parser helps in coherence modeling. 2 Related Work The study of coherence in discourse has led to many linguistic theories, of which we only discuss algorithms that have been reduced to practice. Barzilay and Lapata (2005; 2008) proposed an entity-based model to represent and assess local textual coherence. The model is motivated by Centering Theory (Grosz et al., 1995), which states that subsequent sentences in a locally coherent text are likely to continue to focus on the same entities as in previous sentences. Barzilay and Lapata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. The global coherence of a text"
P11-1100,D09-1036,1,0.530866,"Missing"
P11-1100,J91-1002,0,0.753052,"ananea” in S3 participates in the second, third, and fourth relations). Given these discourse relations, building the matrix is straightforward: we note down the relations that a term Ti from a sentence Sj participates in, and record its discourse roles in the respective cell. We hypothesize that the sequence of discourse role transitions in a coherent text provides clues that distinguish it from an incoherent text. The discourse role matrix thus provides the foundation for computing such role transitions, on a per term basis. In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. The key differences from the traditional lexical chains are that our chain nodes’ entities are simplified (they share the same stemmed form, instead being connected by WordNet relations), but are further enriched by being typed with discourse relations. We compile the set of sub-sequences of discourse role transitions for every term in the matrix. These transitions tell us how the discourse role of a term varies through the progression of the text. For instance, “cananea” functions as Comp.Arg1 in S1 and Comp.Arg2 in S3 , and plays the role of Exp."
P11-1100,P09-2004,0,0.0265124,"tion ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we first apply automatic discourse parsing on the input text. While any discourse framework, su"
P11-1100,C08-2022,0,0.0581557,"ewording (i.e., if the two sentences are swapped), it produces an incoherent text (Marcu, 1996). In addition to the intra-relation ordering, such preferences also extend to inter-relation ordering: (2) [ The Constitution does not expressly give the president such power. ]S1 [ However, the president does have a duty not to violate the Constitution. ]S2 [ The question is whether his only means of defense is the veto. ]S3 The second sentence above provides a contrast to the previous sentence and an explanation for the next one. This pattern of Contrast-followed-by-Cause is rather common in text (Pitler et al., 2008). Ordering the three sentences differently results in incoherent, cryptic text. Thus coherent text exhibits measurable preferences for specific intra- and inter-discourse relation ordering. Our key idea is to use the converse of this phenomenon to assess the coherence of a text. In this paper, we detail our model to capture the coherence of a text based on the statistical distribution of the discourse structure and relations. Our method specifically focuses on the discourse relation transitions between adjacent sentences, modeling them in a discourse role matrix. Proceedings of the 49th Annual"
P11-1100,P09-1077,0,0.0380552,"r the task of information ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we first apply automatic discourse parsing on the input text. While a"
P11-1100,prasad-etal-2008-penn,0,0.100595,"to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we first apply automatic discourse parsing on the input text. While any discourse framework, such as the Rhetorical Structure Theory (RST), could be applied in our work to encode discourse in"
P11-1100,P06-2103,0,0.768546,"s sentences. Barzilay and Lapata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. The global coherence of a text can then be summarized by the overall probability of topic shift from the first sentence to the last. Following these two directions, Soricut and Marcu (2006) and Elsner et al. (2007) combined the entity-based and HMM-based models and demonstrated that these two models are complementary to each other in coherence assessment. Our approach differs from these models in that it introduces and operationalizes another indicator of discourse coherence, by modeling a text’s discourse relation transitions. Karamanis (2007) has tried to integrate local discourse relations into the Centering-based coherence metrics for the task of information ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller dat"
P11-1100,P10-1073,0,0.0164132,"provement over the baseline method, which is partly due to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we first apply automatic discourse parsing on the input text. While any discourse framework, such as the Rhetorical Structure Theory"
P11-1100,D07-1010,0,0.0354183,"grate local discourse relations into the Centering-based coherence metrics for the task of information ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we"
P11-1130,baldwin-awab-2006-open,0,0.30741,",ganang)=gnng, and thus, the ratio is 4/6=0.67 (≥ 0.5) for gunung-ganang. However, for aceh-nias, it is 1/4=0.25, and thus (f) is applicable. As an illustration, here are the wordforms we generate for adik-beradiknya/‘his siblings’: adik, adik-beradiknya, adik-beradik nya, adik-beradik, beradiknya, beradik nya, adik nya, and beradik. And for berpelajaran/‘is educated’, we build the list: berpelajaran, pelajaran, pelajar, ajaran, and ajar. Note that the lists do include the original word. To generate the above wordforms, we used two morphological analyzers: a freely available Malay lemmatizer (Baldwin and Awab, 2006), and an inhouse re-implementation of the Indonesian stemmer described in (Adriani et al., 2007). Note that these tools’ objective is to return a single lemma/stem, e.g., they would return adik for adik-beradiknya, and ajar for berpelajaran. However, it was straightforward to modify them to also output the above intermediary wordforms, which the tools were generating internally anyway when looking for the final lemma/stem. Finally, since the two modified analyzers had different strengths and weaknesses, we combined their outputs to increase recall. 3.2 Word-Level Paraphrasing We perform word-l"
P11-1130,J93-2003,0,0.0711408,"ond the capabilities of statistical machine translation systems. Our experiments translating from Malay, whose morphology is mostly derivational, into English show significant improvements over rivaling approaches based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, w"
P11-1130,N06-1003,0,0.434639,"obability that w′ is a good paraphrase of w. Note that multi-word paraphrases, e.g., resulting from clitic segmentation, are encoded using a sequence of arcs; in such cases, we assign Pr(w′ |w) to the first arc, and 1 to each subsequent arc. We calculate the probability Pr(w′ |w) using the training Malay-English bi-text, which we align at the word level using IBM model 4 (Brown et al., 1993), and we observe which English words w and w′ are aligned to. More precisely, we use pivoting to estimate the probability Pr(w′ |w) as follows: P Pr(w′ |w) = i Pr(w′ |w, ei )Pr(ei |w) 1301 Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007), we make the simplifying assumption that w′ is conditionally independent of w given ei , thus obtaining the following expression: P Pr(w′ |w) = i Pr(w′ |ei )Pr(ei |w) We estimate the probability Pr(ei |w) directly from the word-aligned training bi-text as follows: Pr(ei |w) = P#(w,ei ) j #(w,ej ) where #(x, e) is the number of times the Malay word x is aligned to the English word e. Estimating Pr(w′ |ei ) cannot be done directly since w′ might not be present on the Malay side of the training bi-text, e.g., because it is a multi-token sequence generated by clitic segmentati"
P11-1130,P05-1033,0,0.0391683,"measures (for 320,000 sentence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, which we treat as potential paraphrases, and which we handle using paraphrasing techniques at various levels: word, phrase, and sentence level. An important advantage of this framework is that it can cope with various kinds of morphological wor"
P11-1130,P05-1066,0,0.132449,"Missing"
P11-1130,P08-1115,0,0.391255,"ically simpler than the original word. entries would be hard to estimate. For example, the clitics, and even many of the intermediate morphological forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them. Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignment"
P11-1130,P10-4002,0,0.0236514,"ove general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’ channel model.6 This is the model of Dyer (2007). It uses 0-1 weights in the lattice and only allows lemmata as alternative wordforms; it uses no sentence-level or phrase-level paraphrases. 6 We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al., 2010), which learns word segmentation lattices from raw text in an unsupervised manner. Unfortunately, it could not learn meaningful word segmentations for Malay, and thus we do not compare against it. We believe this may be due to its focus on word segmentation, which is of limited use for Malay. sent. 1k 2k 5k 10k 20k 40k 80k 160k 320k system baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases 1-gram 59.78 62.23 64.20 66.38 68.12 70.41 70.13 72.04 73.19 73.28 74"
P11-1130,W07-0729,0,0.0984925,"re morphologically simpler than the original word. entries would be hard to estimate. For example, the clitics, and even many of the intermediate morphological forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them. Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for"
P11-1130,N09-1046,0,0.124504,"the original word. entries would be hard to estimate. For example, the clitics, and even many of the intermediate morphological forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them. Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignments in both par"
P11-1130,N04-1035,0,0.0273084,"rd tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, which we treat as potential paraphrases, and which we handle using paraphrasing techniques at various levels: word, phrase, and sentence level. An important advantage of this framework is that it can cope with various kinds of morphological wordforms, including derivational ones. We demonstrate its potential o"
P11-1130,H05-1085,0,0.0282473,"dup (‘life/living’), respectively. Thus, in the paraphrasing system, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish a"
P11-1130,N06-2013,0,0.0562633,"to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with 1305 more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general lines of research suits Malay well, whose compounds are rarely concatenated, clitics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose s"
P11-1130,D07-1091,0,0.0246854,"T for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with 1305 more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)stem-suffix(es), which can be used instead"
P11-1130,E03-1076,0,0.039781,"ers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with 1305 more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general line"
P11-1130,N03-1017,0,0.0231554,"based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, which we treat as potential paraphrases, and which we handle using paraphrasing techniques at various levels: word, phrase, and sentence level. An important advantage of this framework is that it can cope with vario"
P11-1130,P07-2045,0,0.0131534,"ed with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. 1303 We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on the development dataset by optimizing BLEU (Papineni et al., 2002) using minimum error rate training (Och, 2003), and we plugged them in a beam search decoder (Koehn et al., 2007) to translate the Malay test sentences to English. Finally, we detokenized the output, and we evaluated it against the three reference translations. 4.3 Systems Using the above general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’ channel model.6 This is the model of Dyer (2007). It uses 0-1 weights in the lattice and only allows lemmata as"
P11-1130,N04-4015,0,0.0618989,"ne of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with 1305 more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general lines of research suits Malay well, whose compounds are rarely concatenated, clitics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose semantics substantially differs from the semantics of the original word. Therefore, we cannot expect the existence of equivalence classes: it is only occasionally that two derivationally related wordforms would share the same target language translati"
P11-1130,W10-1754,1,0.877222,"Missing"
P11-1130,D09-1141,1,0.870794,"the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignments in both parts of the bi-text (Nakov and Ng, 2009). We avoid the above issues by adopting a sentencelevel paraphrasing approach. Following the general framework proposed in (Nakov, 2008), we first create multiple paraphrased versions of the sourceside sentences of the training bi-text. Then, each paraphrased source sentence is paired with its original translation. This augmented bi-text is wordaligned and a phrase table T ′ is built from it, which is merged with a phrase table T for the original bitext. The merged table contains all phrase entries from T , and the entries for the phrase pairs from T ′ that are not in T . Following Nakov and N"
P11-1130,J03-1002,0,0.00563438,"used 1,420 sentences with 28.8K Malay word tokens, which were translated by three human translators, yielding translations of 32.8K, 32.4K, and 32.9K English word tokens, respectively. For development, we used 2,000 sentence pairs of 63.4K English and 58.5K Malay word tokens. 4.2 General Experimental Setup First, we tokenized and lowercased all datasets: training, development, and testing. We then built directed word-level alignments for the training bitext for English→Malay and for Malay→English using IBM model 4 (Brown et al., 1993), which we symmetrized using the intersect+grow heuristic (Och and Ney, 2003). Next, we extracted phraselevel translation pairs of maximum length seven, which we scored and used to build a phrase table where each phrase pair is associated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. 1303 We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on t"
P11-1130,P03-1021,0,0.15835,"i-text is wordaligned and a phrase table T ′ is built from it, which is merged with a phrase table T for the original bitext. The merged table contains all phrase entries from T , and the entries for the phrase pairs from T ′ that are not in T . Following Nakov and Ng (2009), we add up to three additional indicator features (taking the values 0.5 and 1) to each entry in the merged phrase table, showing whether the entry came from (1) T only, (2) T ′ only, or (3) both T and T ′ . We also try using the first one or two features only. We set all feature weights using minimum error rate training (Och, 2003), and we optimize their number (one, two, or three) on the development dataset.5 5 In theory, we should re-normalize the probabilities; in practice, this is not strictly required by the log-linear SMT model. 1302 Each of our paraphrased sentences differs from its original sentence by a single word, which prevents combinatorial explosions: on average, we generate 14 paraphrased versions per input sentence. It further ensures that the paraphrased parts of the sentences will not dominate the word alignments or the phrase pairs, and that there would be sufficient interaction at word alignment time"
P11-1130,P02-1040,0,0.09307,"n pairs of maximum length seven, which we scored and used to build a phrase table where each phrase pair is associated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. 1303 We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on the development dataset by optimizing BLEU (Papineni et al., 2002) using minimum error rate training (Och, 2003), and we plugged them in a beam search decoder (Koehn et al., 2007) to translate the Malay test sentences to English. Finally, we detokenized the output, and we evaluated it against the three reference translations. 4.3 Systems Using the above general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’"
P11-1130,P05-1034,0,0.024385,"ntence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, which we treat as potential paraphrases, and which we handle using paraphrasing techniques at various levels: word, phrase, and sentence level. An important advantage of this framework is that it can cope with various kinds of morphological wordforms, including derivational"
P11-1130,2006.amta-papers.25,0,0.110224,"Missing"
P11-1130,P06-1122,0,0.0142902,"em, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word ("
P11-1130,P07-1108,0,0.0137854,"araphrase of w. Note that multi-word paraphrases, e.g., resulting from clitic segmentation, are encoded using a sequence of arcs; in such cases, we assign Pr(w′ |w) to the first arc, and 1 to each subsequent arc. We calculate the probability Pr(w′ |w) using the training Malay-English bi-text, which we align at the word level using IBM model 4 (Brown et al., 1993), and we observe which English words w and w′ are aligned to. More precisely, we use pivoting to estimate the probability Pr(w′ |w) as follows: P Pr(w′ |w) = i Pr(w′ |w, ei )Pr(ei |w) 1301 Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007), we make the simplifying assumption that w′ is conditionally independent of w given ei , thus obtaining the following expression: P Pr(w′ |w) = i Pr(w′ |ei )Pr(ei |w) We estimate the probability Pr(ei |w) directly from the word-aligned training bi-text as follows: Pr(ei |w) = P#(w,ei ) j #(w,ej ) where #(x, e) is the number of times the Malay word x is aligned to the English word e. Estimating Pr(w′ |ei ) cannot be done directly since w′ might not be present on the Malay side of the training bi-text, e.g., because it is a multi-token sequence generated by clitic segmentation. Thus, we think o"
P11-1130,E06-1006,0,0.0585011,"an (‘life/existence’) are derivational forms of jalan (‘go’) and hidup (‘life/living’), respectively. Thus, in the paraphrasing system, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006)"
P11-2028,W05-0909,0,0.217552,"ord as the smallest unit when matching a system translation and a reference translation. On the other hand, to evaluate Chinese translation output, the smallest unit to use in matching can be a Chinese word or a Chinese character. As shown in Figure 1, given an English sentence “how much are the umbrellas?” a Chinese system translation (or a reference translation) can be segmented into characters (Figure 1(a)) or words (Figure 1(b)). A variety of automatic MT evaluation metrics have been developed over the years, including BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (exact) (Banerjee and Lavie, 2005), GTM (Melamed et al., 2003), and TER 160 Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005). The version we used was released on 2008-05-21 and the standard adopted is CTB. Urheen: Urheen is a CWS tool developed by (Wang et al., 2010a; Wang et al., 2010b), and it outperformed most of the state-of-the-art CWS systems in the CIPS-SIGHAN’2010 evaluation. This tool is trained on Chinese Treebank 6.0. 4 4.1 Experimental Results Data To compare the word-level automatic MT evaluation metrics with the character-level met"
P11-2028,W07-0718,0,0.0483886,"per due to length constraint. The NIST&apos;08 English-to-Chinese translation task evaluated 127 documents with 1,830 segments. Each segment has 4 reference translations and the system translations of 11 MT systems, released in the corpus LDC2010T01. We asked native speakers of Chinese to perform fluency and adequacy judgment on a five-point scale. Human assessment was done on the first 30 documents (355 segments) (document id “AFP_ENG_20070701.0026” to “AFP_ENG_20070731.0115”). The method of manually scoring the 11 submitted Chinese system translations of each segment is the same as that used in (Callison-Burch et al., 2007). The adequacy score indicates the overlap of the meaning expressed in the reference translations with a system translation, while the fluency score indicates how fluent a system translation is. 4.2 tion Segment-Level Consistency or CorrelaFor human fluency and adequacy judgments, the Pearson correlation coefficient is used to compute the segment-level correlation between human judgments and automatic metrics. Human rank judgment is not an absolute score and thus Pearson correlation coefficient cannot be used. We calculate segment-level consistency as follows:  The consistent number of pair"
P11-2028,W08-0336,0,0.141387,"Missing"
P11-2028,I05-3025,1,0.867134,"results in general, we experimented with four different CWS tools in this paper. ICTCLAS: ICTCLAS has been successfully used in a commercial product (Zhang et al., 2003). The version we adopt in this paper is ICTCLAS2009. Ref 1: 这_些_雨_伞_多_少_钱_？ …… Ref 7: 这_些_雨_伞_的_价_格_是_多_少_？ (a) Segmented into characters. NUS Chinese word segmenter (NUS): The NUS Chinese word segmenter uses a maximum entropy approach to Chinese word segmentation, which achieved the highest F-measure on three of the four corpora in the open track of the Second International Chinese Word Segmentation Bakeoff (Ng and Low, 2004; Low et al., 2005). The segmentation standard adopted in this paper is CTB (Chinese Treebank). Translation: 多少_钱_的_伞_吗_？ Ref 1: 这些_雨伞_多少_钱_？ …… Ref 7: 这些_雨伞_的_价格_是_多少_？ (b) Segmented into words by Urheen. Figure 1. An example to show an MT system translation and multiple reference translations being segmented into characters or words. To evaluate English translation output, automatic MT evaluation metrics take an English word as the smallest unit when matching a system translation and a reference translation. On the other hand, to evaluate Chinese translation output, the smallest unit to use in matching can be"
P11-2028,N03-2021,0,0.149985,"tching a system translation and a reference translation. On the other hand, to evaluate Chinese translation output, the smallest unit to use in matching can be a Chinese word or a Chinese character. As shown in Figure 1, given an English sentence “how much are the umbrellas?” a Chinese system translation (or a reference translation) can be segmented into characters (Figure 1(a)) or words (Figure 1(b)). A variety of automatic MT evaluation metrics have been developed over the years, including BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (exact) (Banerjee and Lavie, 2005), GTM (Melamed et al., 2003), and TER 160 Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005). The version we used was released on 2008-05-21 and the standard adopted is CTB. Urheen: Urheen is a CWS tool developed by (Wang et al., 2010a; Wang et al., 2010b), and it outperformed most of the state-of-the-art CWS systems in the CIPS-SIGHAN’2010 evaluation. This tool is trained on Chinese Treebank 6.0. 4 4.1 Experimental Results Data To compare the word-level automatic MT evaluation metrics with the character-level metrics, we conducted experimen"
P11-2028,W04-3236,1,0.794426,"rent segmentation results in general, we experimented with four different CWS tools in this paper. ICTCLAS: ICTCLAS has been successfully used in a commercial product (Zhang et al., 2003). The version we adopt in this paper is ICTCLAS2009. Ref 1: 这_些_雨_伞_多_少_钱_？ …… Ref 7: 这_些_雨_伞_的_价_格_是_多_少_？ (a) Segmented into characters. NUS Chinese word segmenter (NUS): The NUS Chinese word segmenter uses a maximum entropy approach to Chinese word segmentation, which achieved the highest F-measure on three of the four corpora in the open track of the Second International Chinese Word Segmentation Bakeoff (Ng and Low, 2004; Low et al., 2005). The segmentation standard adopted in this paper is CTB (Chinese Treebank). Translation: 多少_钱_的_伞_吗_？ Ref 1: 这些_雨伞_多少_钱_？ …… Ref 7: 这些_雨伞_的_价格_是_多少_？ (b) Segmented into words by Urheen. Figure 1. An example to show an MT system translation and multiple reference translations being segmented into characters or words. To evaluate English translation output, automatic MT evaluation metrics take an English word as the smallest unit when matching a system translation and a reference translation. On the other hand, to evaluate Chinese translation output, the smallest unit to use"
P11-2028,P02-1040,0,0.100113,"sentence into words, an alternative is to split a Chinese sentence into characters, which can be readily done with perfect accuracy. However, it has been reported that a Chinese-English phrase-based SMT system (Xu et al., 2004) that relied on characters (without CWS) performed slightly worse than when it used segmented words. It has been recognized that varying segmentation granularities are needed for SMT (Chang et al., 2008). To evaluate the quality of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 (IWSLT&apos;2005) used the word-level BLEU metric (Papineni et al., 2002). However, IWSLT&apos;08 and NIST&apos;08 adopted character-level evaluation metrics to rank the submitted systems. Although there is much work on automatic evaluation of machine translation (MT), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated. In this paper, we utilize various machine translation evaluation metrics to evaluate the quality of Chinese translation output, and compare their correlation with human assessment when the Chinese translation output is segmented into words versus characters. Since there ar"
P11-2028,2006.amta-papers.25,0,0.0912873,"Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics trics correlate with human assessment better than word-level metrics. That is, CWS is not essential for automatic evaluation of Chinese translation output. Our analysis suggests several key reasons behind this finding. 2 Chinese Translation Evaluation Automatic MT evaluation aims at formulating automatic metrics to measure the quality of MT output. Compared with human assessment, automatic evaluation metrics can assess the quality of MT output quickly and objectively without much human labor. Translation: 多_少_钱_的_伞_吗_？ (Snover et al., 2006). Some automatic MT evaluation metrics perform deeper linguistic analysis, such as part-of-speech tagging, synonym matching, semantic role labeling, etc. Since part-of-speech tags are only defined for Chinese words and not for Chinese characters, we restrict the automatic MT evaluation metrics explored in this paper to those metrics listed above which do not require part-ofspeech tagging. 3 CWS Tools Since there are a number of CWS tools and they give different segmentation results in general, we experimented with four different CWS tools in this paper. ICTCLAS: ICTCLAS has been successfully u"
P11-2028,I05-3027,0,0.289782,"Missing"
P11-2028,C10-1132,1,0.892928,"Missing"
P11-2028,W10-4133,1,0.895636,"Missing"
P11-2028,W04-1118,0,0.290667,"pore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg machine translation (SMT), it can happen that the most accurate word segmentation as judged by the human gold-standard segmentation may not produce the best translation output (Zhang et al., 2008). While state-of-the-art Chinese word segmenters achieve high accuracy, some errors still remain. Instead of segmenting a Chinese sentence into words, an alternative is to split a Chinese sentence into characters, which can be readily done with perfect accuracy. However, it has been reported that a Chinese-English phrase-based SMT system (Xu et al., 2004) that relied on characters (without CWS) performed slightly worse than when it used segmented words. It has been recognized that varying segmentation granularities are needed for SMT (Chang et al., 2008). To evaluate the quality of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 (IWSLT&apos;2005) used the word-level BLEU metric (Papineni et al., 2002). However, IWSLT&apos;08 and NIST&apos;08 adopted character-level evaluation metrics to rank the submitted systems. Although there is much work on automatic evaluation of machine translation (MT), whether word or cha"
P11-2028,W03-1709,0,0.337126,"n metrics perform deeper linguistic analysis, such as part-of-speech tagging, synonym matching, semantic role labeling, etc. Since part-of-speech tags are only defined for Chinese words and not for Chinese characters, we restrict the automatic MT evaluation metrics explored in this paper to those metrics listed above which do not require part-ofspeech tagging. 3 CWS Tools Since there are a number of CWS tools and they give different segmentation results in general, we experimented with four different CWS tools in this paper. ICTCLAS: ICTCLAS has been successfully used in a commercial product (Zhang et al., 2003). The version we adopt in this paper is ICTCLAS2009. Ref 1: 这_些_雨_伞_多_少_钱_？ …… Ref 7: 这_些_雨_伞_的_价_格_是_多_少_？ (a) Segmented into characters. NUS Chinese word segmenter (NUS): The NUS Chinese word segmenter uses a maximum entropy approach to Chinese word segmentation, which achieved the highest F-measure on three of the four corpora in the open track of the Second International Chinese Word Segmentation Bakeoff (Ng and Low, 2004; Low et al., 2005). The segmentation standard adopted in this paper is CTB (Chinese Treebank). Translation: 多少_钱_的_伞_吗_？ Ref 1: 这些_雨伞_多少_钱_？ …… Ref 7: 这些_雨伞_的_价格_是_多少_？ ("
P11-2028,2008.iwslt-evaluation.1,0,\N,Missing
P12-1029,C10-2002,0,0.0125156,"ering process of each word is a time consuming task. Because the sense inventory is collection dependent, it is also hard to expand the text collection without re-doing preprocessing. Many studies investigated the expansion effects by using knowledge sources from thesauri. Some researchers achieved improvements by expanding the disambiguated query words with synonyms and some other information from WordNet (Voorhees, 1994; Liu et al., 2004; Liu et al., 2005; Fang, 2008). The usage of knowledge sources from WordNet in document expansion also showed improvements in IR systems (Cao et al., 2005; Agirre et al., 2010). The previous work shows that the WSD errors can easily neutralize its positive effect. It is important to reduce the negative impact of erroneous disambiguation, and the integration of senses into traditional term index, such as stem-based index, is a possible solution. The utilization of semantic relations has proved to be helpful for IR. It is also interesting to investigate the utilization of semantic relations among senses in IR. 3 where µ is the prior parameter in the Dirichlet-prior smoothing method, and p(t|θC ) is the probability of t in C, which is often calculated with MLE: The Lan"
P12-1029,D07-1007,0,0.149067,"biguation (WSD) is the task of identifying the correct meaning of a word in context. As a basic semantic understanding task at the lexical level, WSD is a fundamental problem in natural language processing. It can be potentially used as a component in many applications, such as machine translation (MT) and information retrieval (IR). In recent years, driven by Senseval/Semeval workshops, WSD systems achieve promising performance. In the application of WSD to MT, research has shown that integrating WSD in appropriate ways significantly improves the performance of MT systems (Chan et al., 2007; Carpuat and Wu, 2007). In the application to IR, WSD can bring two kinds of benefits. First, queries may contain ambiguous words (terms), which have multiple meanings. The ambiguities of these query words can hurt retrieval precision. Identifying the correct meaning of the ambiguous words in both queries and documents can help improve retrieval precision. Second, query words may have tightly related meanings with other words not in the query. Making use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of words provided by WSD syste"
P12-1029,P07-1005,1,0.846289,"on Word sense disambiguation (WSD) is the task of identifying the correct meaning of a word in context. As a basic semantic understanding task at the lexical level, WSD is a fundamental problem in natural language processing. It can be potentially used as a component in many applications, such as machine translation (MT) and information retrieval (IR). In recent years, driven by Senseval/Semeval workshops, WSD systems achieve promising performance. In the application of WSD to MT, research has shown that integrating WSD in appropriate ways significantly improves the performance of MT systems (Chan et al., 2007; Carpuat and Wu, 2007). In the application to IR, WSD can bring two kinds of benefits. First, queries may contain ambiguous words (terms), which have multiple meanings. The ambiguities of these query words can hurt retrieval precision. Identifying the correct meaning of the ambiguous words in both queries and documents can help improve retrieval precision. Second, query words may have tightly related meanings with other words not in the query. Making use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of word"
P12-1029,P08-1017,0,0.0234687,"rieval performance, and the combination of word-based ranking and sense-based ranking can further improve performance. However, the clustering process of each word is a time consuming task. Because the sense inventory is collection dependent, it is also hard to expand the text collection without re-doing preprocessing. Many studies investigated the expansion effects by using knowledge sources from thesauri. Some researchers achieved improvements by expanding the disambiguated query words with synonyms and some other information from WordNet (Voorhees, 1994; Liu et al., 2004; Liu et al., 2005; Fang, 2008). The usage of knowledge sources from WordNet in document expansion also showed improvements in IR systems (Cao et al., 2005; Agirre et al., 2010). The previous work shows that the WSD errors can easily neutralize its positive effect. It is important to reduce the negative impact of erroneous disambiguation, and the integration of senses into traditional term index, such as stem-based index, is a possible solution. The utilization of semantic relations has proved to be helpful for IR. It is also interesting to investigate the utilization of semantic relations among senses in IR. 3 where µ is t"
P12-1029,W98-0705,0,0.0465232,"gs with other words not in the query. Making use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of words provided by WSD systems. However, in previous investigations of the usage of WSD in IR, different researchers arrived at conflicting observations and conclusions. Some of the early research showed a drop in retrieval performance by using word senses (Krovetz and Croft, 1992; Voorhees, 1993). Some other experiments observed improvements by integrating word senses in IR systems (Sch¨utze and Pedersen, 1995; Gonzalo et al., 1998; Stokoe et al., 2003; Kim et al., 2004). This paper proposes the use of word senses to improve the performance of IR. We propose an approach to annotate the senses for short queries. We incorporate word senses into the language modeling (LM) approach to IR (Ponte and Croft, 1998), and utilize sense synonym relations to further improve the performance. Our evaluation on standard TREC1 data sets shows that supervised WSD outperforms two other WSD baselines and significantly improves IR. The rest of this paper is organized as follows. In Section 2, we first review previous work using WSD in IR."
P12-1029,W99-0624,0,0.115473,"Missing"
P12-1029,J03-1002,0,0.00407806,"Missing"
P12-1029,P10-4014,1,0.297519,"Missing"
P12-1097,W05-0909,0,0.13099,"haracters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine translation system parameters are tuned by maximizing the automatic scores. Some recent research (Liu et al., 2011) has shown e"
P12-1097,W09-0401,0,0.0268064,"inear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different s"
P12-1097,W10-1703,0,0.02945,"TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine"
P12-1097,W11-2103,0,0.092536,"drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine translation system parameters"
P12-1097,P08-1007,1,0.853309,") ∈ [0, 1] ∀X ccand (X) ∈ [0, 1] ∀X However, the max(·) operator is not allowed in the linear programming formulation. We get around this by approximating max(·) with the sum instead. Hence, cref (买) ≤ wref (买) + wref (买雨)+ ∀X ccand (X) = 1 ∀X The Objective Function We now define our objective function in terms of the P c(·) variables. The recall is a function of PX cref (X), and the precision is a function of Y ccand (Y ), where X is the set of all n-grams of the reference, and Y is the set of all n-grams of the candidate translation. Many prior translation evaluation metrics such as MAXSIM (Chan and Ng, 2008) and TESLA (Liu et al., 2010; Dahlmeier et al., 2011) use the F-0.8 measure as the final score: wref (买雨伞) F0.8 = cref (雨) ≤ wref (雨) + wref (买雨)+ cref (X) = 1 Precision × Recall 0.8 × Precision + 0.2 × Recall wref (雨伞) + wref (买雨伞) ... We justify this approximation by the following observation. Consider the sub-problem consisting of just the w(·, ·), wref (·), wcand (·) variables and their associated constraints. This sub-problem can be seen as a maximum flow problem where all constants are integers, hence there exists an optimal solution where each of the w variables is assigned a value of e"
P12-1097,W04-3250,0,0.199172,"Missing"
P12-1097,P11-2028,1,0.677111,"s 买_雨 and 雨_伞 still have no match, and will be penalized accordingly, even though 买_雨_伞 and 买_伞 should match exactly. N-grams such as 买_雨 which cross natural word boundaries and are meaningless by themselves can be particularly tricky. 伞 umbrella Figure 1: Three forms of the same expression buy umbrella in Chinese al. (2011). The work compared various MT evaluation metrics (BLEU, NIST, METEOR, GTM, 1 − TER) with different segmentation schemes, and found that treating every single character as a token (character-level MT evaluation) gives the best correlation with human judgments. 2 Motivation Li et al. (2011) identify two reasons that characterbased metrics outperform word-based metrics. For illustrative purposes, we use Figure 1 as a running example in this paper. All three expressions are semantically identical (buy umbrella). The first two forms are identical because 雨伞1 and 伞 are synonyms. The last form is simply an (arguably wrong) alternative segmented form of the second expression. 1. Word-based metrics do not award partial matches, e.g., 买_雨 伞 and 买_伞 would be penalized for the mismatch between 雨伞 and 伞. Character-based metrics award the match between characters 伞 and 伞. 2. Character-based"
P12-1097,W10-1754,1,0.924211,"erforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine translation system parameters are tuned by maximizing the automatic scores. Some recent research (Liu et al., 2011) has shown evidence that replacing BLEU by a newer metric, TESLA, can"
P12-1097,D11-1035,1,0.904546,"Missing"
P12-1097,I05-3025,1,0.769932,"the human judged translation quality. Despite the importance and the research interest on automatic MT evaluation, almost all existing work has focused on European languages, in particular on English. Although many methods aim to be language neutral, languages with very different characteristics such as Chinese do present additional challenges. The most obvious challenge for Chinese is that of word segmentation. Unlike European languages, written Chinese is not split into words. Segmenting Chinese sentences into words is a natural language processing task in its own right (Zhao and Liu, 2010; Low et al., 2005). However, many different segmentation standards exist for different purposes, such as Microsoft Research Asia (MSRA) for Named Entity Recognition (NER), Chinese Treebank (CTB) for parsing and part-of-speech (POS) tagging, and City University of Hong Kong (CITYU) and Academia Sinica (AS) for general word segmentation and POS tagging. It is not clear which standard is the best in a given scenario. The only prior work attempting to address the problem of word segmentation in automatic MT evaluation for Chinese that we are aware of is Li et 921 Proceedings of the 50th Annual Meeting of the Associ"
P12-1097,P02-1040,0,0.0956913,"guages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translatio"
P12-1097,2008.iwslt-evaluation.1,0,0.0180127,"e N multiple references, we match the candidate translation against each of them and use the average of the N objective function values as the segment level score. System level score is the average of all the segment level scores. Z is a normalizing constant to scale the metric to the range [0, 1], chosen so that when all the c(·) variables have the value of one, our metric score attains the value of one. 4 Experiments In this section, we test the effectiveness of TESLACELAB on some real-world English-Chinese translation tasks. 4.1 IWSLT 2008 English-Chinese CT The test set of the IWSLT 2008 (Paul, 2008) English-Chinese ASR challenge task (CT) consists of 300 sentences of spoken language text. The average English source sentence is 5.8 words long and the average Chinese reference translation is 9.2 characters long. The domain is travel expressions. The test set was translated by seven MT systems, and each translation has been manually judged for adequacy and fluency. Adequacy measures whether the translation conveys the correct meaning, even if the translation is not fully fluent, whereas fluency measures whether a translation is fluent, regardless of whether the meaning is correct. Due to hi"
P12-1097,2006.amta-papers.25,0,0.0874511,"hat TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 1 Introduction Since the introduction of BLEU (Papineni et al., 2002), automatic machine translation (MT) evaluation has received a lot of research interest. The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). In the WMT shared tasks, many new generation metrics, such as METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), and TESLA (Liu et al., 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. The research on automatic machine translation evaluation is important for a number of reasons. Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different studies. In addition, machine translation system parameters are tuned by maximizing the automatic scores. Some recent research (Liu et al., 2011) has shown evidence that replacing BLEU"
P12-1097,N03-1033,0,0.00357686,"to character-level BLEU, we also present the correlations for the word-level metric TESLA. Compared to BLEU, TESLA allows more sophisticated weighting of n-grams and measures of word similarity including synonym relations. It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al., 2011). However, its use of POS tags and synonym dictionaries prevents its use at the character-level. We use TESLA as a representative of a competitive word-level metric. We use the Stanford Chinese word segmenter (Tseng et al., 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym 926 definition during matching. TESLA has several variants, and the simplest and often the most robust, TESLA-M, is used in this work. The various correlations are reported in the second row of Tables 2 and 3. The scores show that word-level TESLA-M has no clear advantage over character-level BLEU, despite its use of linguistic features. We consider this conclusion to be in line with that of Li et al. (2011). 4.4 TESLA-CELAB In all our experiments here we use TESLA-CELAB with n-grams for n up to four, since the vast majority of Chinese words, and theref"
P12-1097,I05-3027,0,0.0163638,"systems. 4.3.2 TESLA-M In addition to character-level BLEU, we also present the correlations for the word-level metric TESLA. Compared to BLEU, TESLA allows more sophisticated weighting of n-grams and measures of word similarity including synonym relations. It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al., 2011). However, its use of POS tags and synonym dictionaries prevents its use at the character-level. We use TESLA as a representative of a competitive word-level metric. We use the Stanford Chinese word segmenter (Tseng et al., 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym 926 definition during matching. TESLA has several variants, and the simplest and often the most robust, TESLA-M, is used in this work. The various correlations are reported in the second row of Tables 2 and 3. The scores show that word-level TESLA-M has no clear advantage over character-level BLEU, despite its use of linguistic features. We consider this conclusion to be in line with that of Li et al. (2011). 4.4 TESLA-CELAB In all our experiments here we use TESLA-CELAB with n-grams for n up to four, since the va"
P12-1097,W10-4126,0,\N,Missing
P12-1097,W11-2106,1,\N,Missing
P12-1106,J08-1001,0,0.104986,"course Coherence Model First, a free text in Figure 2 is parsed by a discourse parser to derive its discourse relations, which are shown in Figure 3. Lin et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using such patterns to measure the coherence of a text can result in feature sparseness. To solve this problem, they expand the relation sequence into a discourse role matrix, as shown in Table 2. The matrix essentially captures term occurrences in the sentence-to-sentence relation sequences. This model is motivated by the entity-based model (Barzilay and Lapata, 2008) which captures sentence-to-sentence entity transitions. Next, the discourse role transition probabilities of lengths 2 and 3 (e.g., Temp.Arg1→Exp.Arg2 and Comp.Arg1→nil→Temp.Arg1) are calculated with respect to the matrix. For example, the probability of Comp.Arg2→Exp.Arg2 is 2/25 = 0.08 in Table 2. Lin et al. applied their model on the task of discerning an original text from a permuted ordering of its sentences. They modeled it as a pairwise ranking model (i.e., original vs. permuted), and trained a SVM preference ranking model with discourse role 1010 S# S1 S2 copper nil Comp.Arg2 Comp.Arg"
P12-1106,C08-1019,0,0.0618063,"BE and Pyramid, DUC and TAC also asked human judges to score every candidate summary with regard to its content, readability, and overall responsiveness. DUC and TAC defined linguistic quality to cover several aspects: grammaticality, non-redundancy, referential clarity, focus, and structure/coherence. Recently, Pitler et al. (2010) conducted experiments on various metrics designed to capture these aspects. Their experimental results on DUC 2006 and 2007 show that grammaticality can be measured by a set of syntactic features, while the last three aspects are best evaluated by local coherence. Conroy and Dang (2008) combined two manual linguistic scores – grammaticality and focus – with various ROUGE/BE metrics, and showed this helps better predict the responsiveness of the summarizers. Since 2009, TAC introduced the task of Automatically Evaluating Summaries of Peers (AESOP). AESOP 2009 and 2010 focused on two summary qualities: content and overall responsiveness. Summary content is measured by comparing the output of an automatic metric with the manual Pyramid score. Overall responsiveness measures a combination of content and linguistic quality. In AESOP 2011 (Owczarzak and Dang, 2011), automatic metr"
P12-1106,hovy-etal-2006-automated,0,0.31698,"n evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics. 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al., 2006; Zhou et al., 2006). However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from source articles. If no post-processing is performe"
P12-1106,W04-3250,0,0.0407039,"listic similarity score may be problematic when evaluating abstract-based systems, the experimental results support our choice of the similarity function. This reflects a major difference between MT and summarization evaluation: while MT systems always generate new sentences, most summarization systems focus on locating existing salient sentences. Like in TESLA, function words (words in closed POS categories, such as prepositions and articles) have their weights reduced by a factor of 0.1, thus placing more emphasis on the content words. We found this useful empirically. 3.3 Significance Test Koehn (2004) introduced a bootstrap resampling method to compute statistical significance of the difference between two machine translation systems with regard to the BLEU score. We adapt this method to compute the difference between two evaluation metrics in summarization: 1. Randomly choose n topics from the n given topics with replacement. 2. Summarize the topics with the list of machine summarizers. 3. Evaluate the list of summaries from Step 2 with the two evaluation metrics under comparison. 4. Determine which metric gives a higher correlation score. 5. Repeat Step 1 – 4 for 1,000 times. As we have"
P12-1106,N03-1020,0,0.621348,"hole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics. 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al., 2006; Zhou et al., 2006). However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from s"
P12-1106,P11-1100,1,0.673499,", 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from source articles. If no post-processing is performed to the generated summaries, the presentation of the extracted sentences may confuse readers. Knott (1996) argued that when the sentences of a text are randomly ordered, the text becomes difficult to understand, as its discourse structure is disturbed. Lin et al. (2011) validated this argument by using a trained model to differentiate an original text from a randomlyordered permutation of its sentences by looking at their discourse structures. This prior work leads us to believe that we can apply such discourse models to evaluate the readability of extract-based summaries. We will discuss the application of Lin et al.’s discourse coherence model to evaluate readability of machine generated summaries. We also introduce two new feature sources to enhance the model with hierarchical and Explicit/Non-Explicit information, and demonstrate that they improve the or"
P12-1106,W10-1754,1,0.90386,"machine generated summaries. We also introduce two new feature sources to enhance the model with hierarchical and Explicit/Non-Explicit information, and demonstrate that they improve the original model. There are parallels between evaluations of machine translation (MT) and summarization with respect to textual content. For instance, the widely used ROUGE (Lin and Hovy, 2003) metrics are influenced by BLEU (Papineni et al., 2002): both look at surface n-gram overlap for content coverage. Motivated by this, we will adapt a state-of-theart, linear programming-based MT evaluation metric, TESLA (Liu et al., 2010), to evaluate the content coverage of summaries. TAC’s overall responsiveness metric evaluates the 1006 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1006–1014, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics quality of a summary with regard to both its content and readability. Given this, we combine our two component coherence and content models into an SVM-trained regression model as our surrogate to overall responsiveness. Our experiments show that the coherence model significantly outperforms all AES"
P12-1106,N04-1019,0,0.305873,"SOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics. 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al., 2006; Zhou et al., 2006). However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from source articles. If no post-pro"
P12-1106,P02-1040,0,0.0970871,"can apply such discourse models to evaluate the readability of extract-based summaries. We will discuss the application of Lin et al.’s discourse coherence model to evaluate readability of machine generated summaries. We also introduce two new feature sources to enhance the model with hierarchical and Explicit/Non-Explicit information, and demonstrate that they improve the original model. There are parallels between evaluations of machine translation (MT) and summarization with respect to textual content. For instance, the widely used ROUGE (Lin and Hovy, 2003) metrics are influenced by BLEU (Papineni et al., 2002): both look at surface n-gram overlap for content coverage. Motivated by this, we will adapt a state-of-theart, linear programming-based MT evaluation metric, TESLA (Liu et al., 2010), to evaluate the content coverage of summaries. TAC’s overall responsiveness metric evaluates the 1006 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1006–1014, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics quality of a summary with regard to both its content and readability. Given this, we combine our two component cohere"
P12-1106,P10-1056,0,0.0439133,"Missing"
P12-1106,N06-1057,0,0.0115881,"ies on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics. 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al., 2006; Zhou et al., 2006). However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from source articles. If no post-processing is performed to the generated s"
P12-1106,prasad-etal-2008-penn,0,\N,Missing
P13-1143,D09-1052,0,0.0915501,"The spelling correction candidates are given by a spell checker. We used GNU Aspell4 in our work. 1462 3 4 http://www.gnu.org/software/wdiff/ http://aspell.net 6.2.2 Weights As described in Section 3.2, the weight of each variable is a linear combination of the language model score, three classier condence scores, and three classier disagreement scores. We use the Web 1T 5-gram corpus (Brants and Franz, 2006) to compute the language model score for a sentence. Each of the three classiers (article, preposition, and noun number) is trained with the multi-class condence weighted algorithm (Crammer et al., 2009). The training data consists of all non-OCR papers in the ACL Anthology5 , minus the documents that overlap with the HOO 2011 data set. The features used for the classiers follow those in (Dahlmeier and Ng, 2012a), which include lexical and part-of-speech n-grams, lexical head words, web-scale n-gram counts, dependency heads and children, etc. Over 5 million training examples are extracted from the ACL Anthology for use as training data for the article and noun number classiers, and over 1 million training examples for the preposition classier. Finally, the language model score, classier c"
P13-1143,P11-1092,1,0.831643,"tical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Le"
P13-1143,D12-1052,1,0.928624,"used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to t"
P13-1143,N12-1067,1,0.913602,"used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to t"
P13-1143,W11-2838,0,0.542561,"erimental results on the Helping Our Own shared task show that our method is competitive with state-of-the-art systems. 1 Introduction Grammatical error correction is an important task of natural language processing (NLP). It has many potential applications and may help millions of people who learn English as a second language (ESL). As a research eld, it faces the challenge of processing ungrammatical language, which is different from other NLP tasks. The task has received much attention in recent years, and was the focus of two shared tasks on grammatical error correction in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012). To detect and correct grammatical errors, two different approaches are typically used  knowledge engineering or machine learning. The rst relies on handcrafting a set of rules. For example, the superlative adjective best is preceded by the article the. In contrast, the machine learning approach formulates the task as a classication problem based on learning from training data. For example, an article classier takes a noun phrase (NP) as input and predicts its article using class labels a/an, the, or ɛ (no article). Both approaches have their advantages and disadvantag"
P13-1143,1995.tmi-1.1,0,0.384786,"garriff, 2011). Experimental results show that the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surroundi"
P13-1143,N10-1019,0,0.169916,"ted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various error"
P13-1143,W11-1422,0,0.0493969,"Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search"
P13-1143,P10-2065,0,0.0583004,"have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error cor"
P13-1143,P98-1085,0,0.361985,"hat the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, languag"
P13-1143,D10-1104,0,0.0736214,"blem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively correc"
P13-1143,P09-1039,0,0.0213781,"s and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: Given an input sentence, choose a set of corrections which results in the best output sentence. In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inference objective as a l"
P13-1143,1993.tmi-1.18,0,0.102148,"hared task (Dale and Kilgarriff, 2011). Experimental results show that the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classicatio"
P13-1143,P11-1094,0,0.0790867,"d Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an a"
P13-1143,W06-1616,0,0.0338149,"produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: Given an input sentence, choose a set of corrections which results in the best output sentence. In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inf"
P13-1143,D11-1001,0,0.0269282,"the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: Given an input sentence, choose a set of corrections which results in the best output sentence. In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inference objective as a linear objective function; and 3. Introduce problem-specic constraints to rene the feasible output"
P13-1143,P11-1093,0,0.134979,"rammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlme"
P13-1143,W11-2843,0,0.0382812,"= 1 (language model); λt = 1 (classier condence); and µt = −1 (classier disagreement). 6.2.3 Constraints In Section 4, three sets of constraints are introduced: modication count (MC), article-noun agreement (ANA), and dependency relation (DR) constraints. The values for the modication count parameters are set as follows: NArt = 3, NPrep = 2, NNoun = 2, and NSpell = 1. 6.3 Experimental Results We compare our ILP approach with two other systems: the beam search decoder of (Dahlmeier and Ng, 2012a) which achieves the best published performance to date on the HOO 2011 data set, and UI Run1 (Rozovskaya et al., 2011) which achieves the best performance among all participating systems at the HOO 2011 shared task. The results are given in Table 4. The HOO 2011 shared task provides two sets of gold-standard edits: the original gold-standard edits produced by the annotator, and the ofcial gold5 http://aclweb.org/anthology-new/ System Original Ofcial P R F P R F UI Run1 40.86 11.21 17.59 54.61 14.57 23.00 Beam search 30.28 19.17 23.48 33.59 20.53 25.48 ILP 20.54 27.93 23.67 21.99 29.04 25.03 Table 4: Comparison of three grammatical error correction systems. standard edits which incorporated corrections propo"
P13-1143,P12-2039,0,0.0772544,"preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predened generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence t"
P13-1143,C08-1109,0,0.082898,"has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classication problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, na¨ve Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner"
P13-1143,W12-2006,0,\N,Missing
P15-1068,W14-1703,0,0.380629,"Missing"
P15-1068,J08-4004,0,0.146386,"ly discussed ways to minimize annotator bias effects in SMT (Snover et al., 2006; Madnani et al., 2008), IAA metrics such as κ still unhelpfully play a role in the field and have, for example, been reported almost every year in the Workshop on Machine Translation (WMT) conference. Inter-Annotator Agreement in GEC One important study that made use of κ as a measure of agreement between raters is by Tetrault and Chodorow (2008) (also in Tetreault et al. (2014)), who asked two native English speakers to insert a missing preposition into 200 randomly chosen, 2 See Hayes and Krippendorff (2007) or Artstein and Poesio (2008) for the pros and cons of different IAA metrics. 698 Source: A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 To put it in the nutshell, I believe that people should have the obligation to tell their relatives about the genetic testing result for the good of their health. To put it in a nutshell, I believe that people should be obliged to tell their relatives about their genetic test results for the good of their health. In a nutshell, I believe that people should have an obligation to tell their relatives about the genetic testing result for the good of their health. In summary, I believe that people should ha"
P15-1068,2008.amta-papers.13,0,0.0463943,"tatistics are only able to interpret the alternative answers as disagreements. 2.1 2.2 Inter-Annotator Agreement in SMT In fact, the issues regarding the reliability of IAA metrics are not unique to GEC and we can also draw a parallel with the field of statistical machine translation (SMT). In the same way that there is often more than one way to correct a sentence in GEC, it is also well known that there is often more than one way to translate a sentence in SMT. Nevertheless, while several papers have successfully discussed ways to minimize annotator bias effects in SMT (Snover et al., 2006; Madnani et al., 2008), IAA metrics such as κ still unhelpfully play a role in the field and have, for example, been reported almost every year in the Workshop on Machine Translation (WMT) conference. Inter-Annotator Agreement in GEC One important study that made use of κ as a measure of agreement between raters is by Tetrault and Chodorow (2008) (also in Tetreault et al. (2014)), who asked two native English speakers to insert a missing preposition into 200 randomly chosen, 2 See Hayes and Krippendorff (2007) or Artstein and Poesio (2008) for the pros and cons of different IAA metrics. 698 Source: A1 A2 A3 A4 A5 A"
P15-1068,C12-1038,0,0.465712,"e on certain error categories, and find that similar results can be obtained from a smaller subset of just 10 essays. 1 Introduction Interest in grammatical error correction (GEC) systems has grown considerably in the past few years, thanks mainly to the success of the recent Helping Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and Conference on Natural Language Learning (CoNLL) (Ng et al., 2013; Ng et al., 2014) shared tasks. Despite this increasing attention, however, one of the most significant challenges facing GEC today is the lack of a robust evaluation practice. In fact Chodorow et al. (2012) even go as far to say that it is sometimes “hard to draw meaningful comparisons between different approaches, even when they are evaluated on the same corpus.” One of the reasons for this is that, traditionally, system performance has only ever been evaluated against the gold standard annotations of a 1 http://www.comp.nus.edu.sg/˜nlp/sw/ 10gec_annotations.zip 697 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 697–707, c Beijing, China, July 26-31, 2015. 2015 Association f"
P15-1068,P11-2089,0,0.160613,"Missing"
P15-1068,N12-1067,1,0.696917,"or. See (Ng et al., 2014) for more details on how F0.5 is calculated. terms of category use, with almost half of all edits falling into the categories for article or determiner (ArtOrDet), spelling or punctuation (Mec), preposition (Prep), or word choice (Wci) errors. 5 Quantitative Analysis In the main phase of experimentation, we first investigated how different numbers of annotators affected the performance of various systems in the context of the CoNLL-2014 shared task. To do this, we downloaded the official system output of all the participating teams4 and then the MaxMatch (M2) Scorer5 (Dahlmeier and Ng, 2012), which was the official scorer of the previous CoNLL-2013 and CoNLL-2014 shared tasks. This scorer evaluates a system at the sentence level in terms of correct edits, proposed edits, and gold edits, and uses these to calculate an F-score for each team. When more than one set of gold standard annotations is available, the scorer will calculate F-scores for each alternative 5.1 Pairwise Evaluation In order to quantify how much the F-score can vary in a realistic grammar checking scenario when there is only one gold standard annotator, we first computed the scores for a participating system vs e"
P15-1068,W13-1703,1,0.761709,"rmed the basis of the CoNLL2014 test data (Ng et al., 2014). See Table 2 for some basic statistics on the resulting 50 essays. The 10 annotators who annotated all 50 essays include: the 2 official annotators of CoNLL-2014, the first author of this paper, and 7 freelancers who were recruited via online recruitment website, Elance.3 All annotators are native British English speakers, many of whom also have backgrounds in English language teaching, proofreading, and/or Linguistics. All annotations were made using an online annotation platform, WAMP, especially designed for annotating ESL errors (Dahlmeier et al., 2013). Using this platform, annotators were asked to 3 Total 252 1312 30144 4.1 Early Observations To investigate the extent to which different annotators have different biases, we first counted the total number of edits made by each annotator and sorted them by error category (Table 3). As can be seen, there is quite a difference between the annotator who made the most edits (A1) and the annotator who made the fewest edits (A7), with A1 making more than twice the number of edits as A7. This just goes to show how varied judgments on grammaticality can be. Incidentally, annotators A3 and A7, who are"
P15-1068,W13-3601,1,0.8235,"Missing"
P15-1068,W11-2838,0,0.45044,"Missing"
P15-1068,W14-1701,1,0.925871,"ut 45 minutes per essay. Data Collection The raw text data in our dataset was originally produced by 25 students at the National University of Singapore (NUS) who were non-native speakers of English. They were asked to write two essays on the topics of genetic testing and social media respectively. All essays were of similar length and quality. This was important because varying the skill level of the essays is likely to further affect the natural bias of the annotators, who may then consistently over- or under-correct essays. These raw essays also formed the basis of the CoNLL2014 test data (Ng et al., 2014). See Table 2 for some basic statistics on the resulting 50 essays. The 10 annotators who annotated all 50 essays include: the 2 official annotators of CoNLL-2014, the first author of this paper, and 7 freelancers who were recruited via online recruitment website, Elance.3 All annotators are native British English speakers, many of whom also have backgrounds in English language teaching, proofreading, and/or Linguistics. All annotations were made using an online annotation platform, WAMP, especially designed for annotating ESL errors (Dahlmeier et al., 2013). Using this platform, annotators we"
P15-1068,W12-2006,0,0.466054,"Missing"
P15-1068,2010.amta-papers.20,0,0.0236613,", “had go” → “had gone”, to be the minimal error. Similarly, the authors also noted that annotators sometimes had problems categorizing ambiguous errors which could be classified into more than one error category. In short, while annotators already vary as to what they consider an error, these observations show that even when they do apparently agree, there is no guarantee that every annotator will define the error in exactly the same terms. This poses a problem for IAA statistics, which rely on an exact match to measure agreement. Finally, it is also worth mentioning that a related study, by Denkowski and Lavie (2010), suggested that “annotators also have difficulty agreeing with 3 Annotator Bias In an effort to better understand how annotators’ judgments might differ, we first carried out a small-scale qualitative analysis on a handful of random sentences corrected by the 10 human annotators in our dataset. One such sentence, and all its various corrections, is shown in Table 1. It is interesting to note that, for even as short an idiom as “To put it in the nutshell’, there are still multiple alternative edits. Although 8 out of the 10 annotators elected to replace the article “the” with “a”, among them,"
P15-1068,W10-1004,0,0.0584274,"en the task is preposition error detection in ’noisy’ learner texts” and, by extension, imply that detection of all error types in ’noisy’ texts would show more disagreement still. The most important question to ask then, as a result of this study, is whether low κ-scores in ’noisy’ texts are truly indicative of real disagreement, or whether, as in this preposition test, the disagreement is actually the result of multiple correct answers, and therefore not disagreement at all. In a related study, and aware of the fact that there are often multiple ways to correct individual words in sentence, Rozovskaya and Roth (2010) instead chose to compute agreement at the sentence level. Specifically, three raters were asked simply to decide whether they thought 200 sentences were correct or not. This time, despite operating at the more general sentence level, the authors reported κ scores of just 0.16, 0.4 and 0.23, surmising that “the low numbers reflect the difficulty of the task and the variability of the native speakers’ judgments about acceptable usage.” If that is the case, then true disagreement may be indistinguishable from native variability, and we should be wary of using IAA statistics as a measure of agree"
P15-1068,W14-1702,0,0.201309,"Missing"
P15-1068,W14-1704,0,0.0853166,"Missing"
P15-1068,2006.amta-papers.25,0,0.0696707,"valid ways, but IAA statistics are only able to interpret the alternative answers as disagreements. 2.1 2.2 Inter-Annotator Agreement in SMT In fact, the issues regarding the reliability of IAA metrics are not unique to GEC and we can also draw a parallel with the field of statistical machine translation (SMT). In the same way that there is often more than one way to correct a sentence in GEC, it is also well known that there is often more than one way to translate a sentence in SMT. Nevertheless, while several papers have successfully discussed ways to minimize annotator bias effects in SMT (Snover et al., 2006; Madnani et al., 2008), IAA metrics such as κ still unhelpfully play a role in the field and have, for example, been reported almost every year in the Workshop on Machine Translation (WMT) conference. Inter-Annotator Agreement in GEC One important study that made use of κ as a measure of agreement between raters is by Tetrault and Chodorow (2008) (also in Tetreault et al. (2014)), who asked two native English speakers to insert a missing preposition into 200 randomly chosen, 2 See Hayes and Krippendorff (2007) or Artstein and Poesio (2008) for the pros and cons of different IAA metrics. 698 S"
P15-1068,W08-1205,0,0.0630126,"is often more than one way to correct a sentence in GEC, it is also well known that there is often more than one way to translate a sentence in SMT. Nevertheless, while several papers have successfully discussed ways to minimize annotator bias effects in SMT (Snover et al., 2006; Madnani et al., 2008), IAA metrics such as κ still unhelpfully play a role in the field and have, for example, been reported almost every year in the Workshop on Machine Translation (WMT) conference. Inter-Annotator Agreement in GEC One important study that made use of κ as a measure of agreement between raters is by Tetrault and Chodorow (2008) (also in Tetreault et al. (2014)), who asked two native English speakers to insert a missing preposition into 200 randomly chosen, 2 See Hayes and Krippendorff (2007) or Artstein and Poesio (2008) for the pros and cons of different IAA metrics. 698 Source: A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 To put it in the nutshell, I believe that people should have the obligation to tell their relatives about the genetic testing result for the good of their health. To put it in a nutshell, I believe that people should be obliged to tell their relatives about their genetic test results for the good of their heal"
P17-1036,N16-1180,0,0.0380638,"Missing"
P17-1036,C10-1074,0,0.0407358,"example, in the sentence “The beef was tender and melted in my mouth”, the aspect term is “beef”. Two sub-tasks are performed in aspect extraction: (1) extracting all aspect terms (e.g., “beef”) from a review corpus, (2) clustering aspect terms with similar meaning into categories where each category represents a single 388 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 388–397 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1036 labeling problem. Jin and Ho (2009) and Li et al. (2010) proposed to use hidden Markov models (HMM) and conditional random fields (CRF), respectively with a set of manually-extracted features. More recently, different neural models (Yin et al., 2016; Wang et al., 2016) were proposed to automatically learn features for CRF-based aspect extraction. Rule-based models are usually not refined enough to categorize the extracted aspect terms. On the other hand, supervised learning requires large amounts of labeled data for training purposes. ready map words that usually co-occur within the same context to nearby points in the embedding space (Mikolov et a"
P17-1036,D15-1166,0,0.0514934,"osed based on frequent item mining and dependency information to extract product aspects (Zhuang et al., 2006; Somasundaran and Wiebe, 2009; Qiu et al., 2011). These models heavily depend on predefined rules which work well only when the aspect terms are restricted to a small group of nouns. Supervised learning approaches generally model aspect extraction as a standard sequence Attention models (Mnih et al., 2014) have recently gained popularity in training neural networks and have been applied to various natural language processing tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), sentence summarization (Rush et al., 2015), sentiment classification (Chen et al., 2016; Tang et al., 2016), and question answering (Hermann et al., 2015). Rather than using all available information, attention mechanism aims to focus on the most pertinent information for a task. Unlike previous works, in this paper, we apply attention to an unsupervised neural model. Our experimental results demonstrate its effectiveness under an unsupervised setting for aspect extraction. 389 3 Model Description We describe the Attention-based Aspect Extraction (ABAE) model in this section. The ultimate go"
P17-1036,N10-1122,0,0.273857,"om Abstract aspect (e.g., cluster “beef”, “pork”, “pasta”, and “tomato” into one aspect food). Previous works for aspect extraction can be categorized into three approaches: rule-based, supervised, and unsupervised. Rule-based methods usually do not group extracted aspect terms into categories. Supervised learning requires data annotation and suffers from domain adaptation problems. Unsupervised methods are adopted to avoid reliance on labeled data needed for supervised learning. In recent years, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Titov and McDonald, 2008; Brody and Elhadad, 2010; Zhao et al., 2010; Mukherjee and Liu, 2012) have become the dominant unsupervised approach for aspect extraction. LDA models the corpus as a mixture of topics (aspects), and topics as distributions over word types. While the mixture of aspects discovered by LDA-based models may describe a corpus fairly well, we find that the individual aspects inferred are of poor quality – aspects often consist of unrelated or loosely-related concepts. This may substantially reduce users’ confidence in using such automated systems. There could be two primary reasons for the poor quality. Conventional LDA mo"
P17-1036,D16-1171,0,0.0420243,"huang et al., 2006; Somasundaran and Wiebe, 2009; Qiu et al., 2011). These models heavily depend on predefined rules which work well only when the aspect terms are restricted to a small group of nouns. Supervised learning approaches generally model aspect extraction as a standard sequence Attention models (Mnih et al., 2014) have recently gained popularity in training neural networks and have been applied to various natural language processing tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), sentence summarization (Rush et al., 2015), sentiment classification (Chen et al., 2016; Tang et al., 2016), and question answering (Hermann et al., 2015). Rather than using all available information, attention mechanism aims to focus on the most pertinent information for a task. Unlike previous works, in this paper, we apply attention to an unsupervised neural model. Our experimental results demonstrate its effectiveness under an unsupervised setting for aspect extraction. 389 3 Model Description We describe the Attention-based Aspect Extraction (ABAE) model in this section. The ultimate goal is to learn a set of aspect embeddings, where each aspect can be interpreted by lookin"
P17-1036,N13-1090,0,0.12421,"t al. (2010) proposed to use hidden Markov models (HMM) and conditional random fields (CRF), respectively with a set of manually-extracted features. More recently, different neural models (Yin et al., 2016; Wang et al., 2016) were proposed to automatically learn features for CRF-based aspect extraction. Rule-based models are usually not refined enough to categorize the extracted aspect terms. On the other hand, supervised learning requires large amounts of labeled data for training purposes. ready map words that usually co-occur within the same context to nearby points in the embedding space (Mikolov et al., 2013). We then filter the word embeddings within a sentence using an attention mechanism (Bahdanau et al., 2015) and use the filtered words to construct aspect embeddings. The training process for aspect embeddings is analogous to autoencoders, where we use dimension reduction to extract the common factors among embedded sentences and reconstruct each sentence through a linear combination of aspect embeddings. The attention mechanism deemphasizes words that are not part of any aspect, allowing the model to focus on aspect words. We call our proposed model Attention-based Aspect Extraction (ABAE). I"
P17-1036,P14-1033,0,0.157468,"tasks. In addition, ABAE is intuitive and structurally simple. It can also easily scale to a large amount of training data. Therefore, it is a promising alternative to LDA-based methods proposed previously. 2 Unsupervised approaches, especially topic models, have been proposed subsequently to avoid reliance on labeled data. Generally, the outputs of those models are word distributions or rankings for each aspect. Aspects are naturally obtained without separately performing extraction and categorization. Most existing works (Brody and Elhadad, 2010; Zhao et al., 2010; Mukherjee and Liu, 2012; Chen et al., 2014) are based on variants and extensions of LDA (Blei et al., 2003). Recently, Wang et al. (2015) proposed a restricted Boltzmann machine (RBM)-based model to simultaneously extract aspects and relevant sentiments of a given review sentence, treating aspects and sentiments as separate hidden variables in RBM. However, the RBM-based model proposed in (Wang et al., 2015) relies on a substantial amount of prior knowledge such as part-of-speech (POS) tagging and sentiment lexicons. A biterm topic model (BTM) that generates co-occurring word pairs was proposed in (Yan et al., 2013). We experimentally"
P17-1036,D11-1024,0,0.169029,"Missing"
P17-1036,P12-1036,0,0.659843,"ork”, “pasta”, and “tomato” into one aspect food). Previous works for aspect extraction can be categorized into three approaches: rule-based, supervised, and unsupervised. Rule-based methods usually do not group extracted aspect terms into categories. Supervised learning requires data annotation and suffers from domain adaptation problems. Unsupervised methods are adopted to avoid reliance on labeled data needed for supervised learning. In recent years, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Titov and McDonald, 2008; Brody and Elhadad, 2010; Zhao et al., 2010; Mukherjee and Liu, 2012) have become the dominant unsupervised approach for aspect extraction. LDA models the corpus as a mixture of topics (aspects), and topics as distributions over word types. While the mixture of aspects discovered by LDA-based models may describe a corpus fairly well, we find that the individual aspects inferred are of poor quality – aspects often consist of unrelated or loosely-related concepts. This may substantially reduce users’ confidence in using such automated systems. There could be two primary reasons for the poor quality. Conventional LDA models do not directly encode word co-occurrenc"
P17-1036,J11-1002,0,0.260128,"on multiple tasks in this paper. Related Work The problem of aspect extraction has been well studied in the past decade. Initially, methods were mainly based on manually defined rules. Hu and Liu (2004) proposed to extract different product features through finding frequent nouns and noun phrases. They also extracted opinion terms by finding the synonyms and antonyms of opinion seed words through WordNet. Following this, a number of methods have been proposed based on frequent item mining and dependency information to extract product aspects (Zhuang et al., 2006; Somasundaran and Wiebe, 2009; Qiu et al., 2011). These models heavily depend on predefined rules which work well only when the aspect terms are restricted to a small group of nouns. Supervised learning approaches generally model aspect extraction as a standard sequence Attention models (Mnih et al., 2014) have recently gained popularity in training neural networks and have been applied to various natural language processing tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), sentence summarization (Rush et al., 2015), sentiment classification (Chen et al., 2016; Tang et al., 2016), and question answering (Herm"
P17-1036,D10-1006,0,0.140268,"cluster “beef”, “pork”, “pasta”, and “tomato” into one aspect food). Previous works for aspect extraction can be categorized into three approaches: rule-based, supervised, and unsupervised. Rule-based methods usually do not group extracted aspect terms into categories. Supervised learning requires data annotation and suffers from domain adaptation problems. Unsupervised methods are adopted to avoid reliance on labeled data needed for supervised learning. In recent years, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Titov and McDonald, 2008; Brody and Elhadad, 2010; Zhao et al., 2010; Mukherjee and Liu, 2012) have become the dominant unsupervised approach for aspect extraction. LDA models the corpus as a mixture of topics (aspects), and topics as distributions over word types. While the mixture of aspects discovered by LDA-based models may describe a corpus fairly well, we find that the individual aspects inferred are of poor quality – aspects often consist of unrelated or loosely-related concepts. This may substantially reduce users’ confidence in using such automated systems. There could be two primary reasons for the poor quality. Conventional LDA models do not directl"
P17-1036,D15-1044,0,0.0248136,"ency information to extract product aspects (Zhuang et al., 2006; Somasundaran and Wiebe, 2009; Qiu et al., 2011). These models heavily depend on predefined rules which work well only when the aspect terms are restricted to a small group of nouns. Supervised learning approaches generally model aspect extraction as a standard sequence Attention models (Mnih et al., 2014) have recently gained popularity in training neural networks and have been applied to various natural language processing tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), sentence summarization (Rush et al., 2015), sentiment classification (Chen et al., 2016; Tang et al., 2016), and question answering (Hermann et al., 2015). Rather than using all available information, attention mechanism aims to focus on the most pertinent information for a task. Unlike previous works, in this paper, we apply attention to an unsupervised neural model. Our experimental results demonstrate its effectiveness under an unsupervised setting for aspect extraction. 389 3 Model Description We describe the Attention-based Aspect Extraction (ABAE) model in this section. The ultimate goal is to learn a set of aspect embeddings, w"
P17-1036,Q14-1017,0,0.0266325,"ruction with Aspect Embeddings Domain Restaurant Beer We have obtained the sentence embedding. Now we describe how to compute the reconstruction of the sentence embedding. As shown in Figure 1, the reconstruction process consists of two steps of transitions, which is similar to an autoencoder. Intuitively, we can think of the reconstruction as a linear combination of aspect embeddings from T: rs = T&gt; · pt 3.4 (5) U (θ) = kTn · T&gt; n − Ik (6) ABAE is trained to minimize the reconstruction error. We adopted the contrastive max-margin objective function used in previous work (Weston et al., 2011; Socher et al., 2014; Iyyer et al., 2016). For each input sentence, we randomly sample m sentences from our training data as negative samples. We represent each negative sample as ni which is computed by averaging its word embeddings. Our objective is to make the reconstructed embedding rs similar to the target sentence embedding zs while different from those negative samples. Therefore, the unregularized objective J is formulated as a hinge loss that maximize the inner product between rs and zs and simultaneously minimize the inner product between rs and the negative samples: m XX s∈D i=1 max(0, 1 − rs zs + rs n"
P17-1036,P09-1026,0,0.00772904,"mentally compare ABAE and BTM on multiple tasks in this paper. Related Work The problem of aspect extraction has been well studied in the past decade. Initially, methods were mainly based on manually defined rules. Hu and Liu (2004) proposed to extract different product features through finding frequent nouns and noun phrases. They also extracted opinion terms by finding the synonyms and antonyms of opinion seed words through WordNet. Following this, a number of methods have been proposed based on frequent item mining and dependency information to extract product aspects (Zhuang et al., 2006; Somasundaran and Wiebe, 2009; Qiu et al., 2011). These models heavily depend on predefined rules which work well only when the aspect terms are restricted to a small group of nouns. Supervised learning approaches generally model aspect extraction as a standard sequence Attention models (Mnih et al., 2014) have recently gained popularity in training neural networks and have been applied to various natural language processing tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), sentence summarization (Rush et al., 2015), sentiment classification (Chen et al., 2016; Tang et al., 2016), and quest"
P17-1036,D16-1021,0,0.0790435,"Somasundaran and Wiebe, 2009; Qiu et al., 2011). These models heavily depend on predefined rules which work well only when the aspect terms are restricted to a small group of nouns. Supervised learning approaches generally model aspect extraction as a standard sequence Attention models (Mnih et al., 2014) have recently gained popularity in training neural networks and have been applied to various natural language processing tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), sentence summarization (Rush et al., 2015), sentiment classification (Chen et al., 2016; Tang et al., 2016), and question answering (Hermann et al., 2015). Rather than using all available information, attention mechanism aims to focus on the most pertinent information for a task. Unlike previous works, in this paper, we apply attention to an unsupervised neural model. Our experimental results demonstrate its effectiveness under an unsupervised setting for aspect extraction. 389 3 Model Description We describe the Attention-based Aspect Extraction (ABAE) model in this section. The ultimate goal is to learn a set of aspect embeddings, where each aspect can be interpreted by looking at the nearest wor"
P17-1036,P15-1060,0,0.547922,"Missing"
P17-1036,D16-1059,1,0.394886,"rpus, (2) clustering aspect terms with similar meaning into categories where each category represents a single 388 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 388–397 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1036 labeling problem. Jin and Ho (2009) and Li et al. (2010) proposed to use hidden Markov models (HMM) and conditional random fields (CRF), respectively with a set of manually-extracted features. More recently, different neural models (Yin et al., 2016; Wang et al., 2016) were proposed to automatically learn features for CRF-based aspect extraction. Rule-based models are usually not refined enough to categorize the extracted aspect terms. On the other hand, supervised learning requires large amounts of labeled data for training purposes. ready map words that usually co-occur within the same context to nearby points in the embedding space (Mikolov et al., 2013). We then filter the word embeddings within a sentence using an attention mechanism (Bahdanau et al., 2015) and use the filtered words to construct aspect embeddings. The training process for aspect embed"
P18-2092,D17-1047,0,0.527773,"e sentence towards the opinion target. An opinion target or target for short refers to a word or a phrase describing an aspect of an entity. For example, in the sentence “This little place has a cute interior decor but the prices are quite expensive”, the targets are interior decor and prices, and they are associated with positive and negative sentiment respectively. A sentence may contain multiple sentimenttarget pairs, thus one challenge is to separate different opinion contexts for different targets. For this purpose, state-of-the-art neural methods (Wang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) adopt attention-based LSTM networks, where the LSTM aims to capture sequential patterns and the attention mechanism aims 2 Related Work Various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) have been proposed for aspect-level sentiment classification. The main idea behind these works is to develop neural architectures that are able to learn continuous features and capture the intricate relation between a target and context words. However, to sufficiently train"
P18-2092,P14-2009,0,0.192564,"ttle place has a cute interior decor but the prices are quite expensive”, the targets are interior decor and prices, and they are associated with positive and negative sentiment respectively. A sentence may contain multiple sentimenttarget pairs, thus one challenge is to separate different opinion contexts for different targets. For this purpose, state-of-the-art neural methods (Wang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) adopt attention-based LSTM networks, where the LSTM aims to capture sequential patterns and the attention mechanism aims 2 Related Work Various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) have been proposed for aspect-level sentiment classification. The main idea behind these works is to develop neural architectures that are able to learn continuous features and capture the intricate relation between a target and context words. However, to sufficiently train these models, substantial aspect579 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585 c Melbourne, Australia,"
P18-2092,S15-2082,0,0.314106,"Missing"
P18-2092,P17-1036,1,0.795077,"that aspect-level sentiment classification can be improved by employing knowledge gained from document-level sentiment classification, as these two tasks are highly related semantically. z= n X αi hi (2) i=1 And αi is computed as follows: exp(βi ) αi = Pn j=1 exp(βj ) βi = fscore (hi , t) = tanh(hTi Wa t) m 1 X t= exi m (3) (4) (5) i=1 where t is the target representation computed as the averaged word embedding of the target. fscore is a content-based function that captures the semantic association between a word and the target, for which we adopt the formulation used in (Luong et al., 2015b; He et al., 2017) with parameter matrix Wa ∈ Rd×d . The sentence representation z is fed into an output layer to predict the probability distribution p over sentiment labels on the target: p = sof tmax(Wo z + bo ) (6) We refer to this baseline model as LSTM+ATT. It is trained via cross entropy minimization: X J =− log pi (ci ) (7) i∈D 3 Models 3.1 where D denotes the overall training corpus, ci denotes the true label for sample i, and pi (ci ) denotes the probability of the true label. Attention-based LSTM We first describe a conventional implementation of an attention-based LSTM model for this task. We use it"
P18-2092,S14-2004,0,0.232704,"provements in macro-F1 scores are even more, especially on D3 and D4 where the labels are extremely unbalanced. MULT gives similar performance as LSTM+ATT on D1 and D2, but improvements can be clearly observed for D3 and D4. The combination (PRET+MULT) overall yields better results. There are two main reasons why the improvements of macro-F1 scores are more significant on D3 and D4 than on D1: (1) D1 has much more neutral examples in the training set. A classifier Experiments Datasets and Experimental Settings We run experiments on four benchmark aspectlevel datasets, taken from SemEval 2014 (Pontiki et al., 2014), SemEval 2015 (Pontiki et al., 2015), and SemEval 2016 (Pontiki et al., 2016). Following previous work (Tang et al., 2016b; Wang et al., 2016), we remove samples with conflicting polarities in all datasets1 . Statistics of the resulting datasets are presented in Table 1. We derived two document-level datasets from Yelp2014 (Tang et al., 2015) and the Amazon Electronics dataset (McAuley et al., 2015) respectively. The original reviews were rated on a 5point scale. We consider 3-class classification and 1 We remove samples in the 2015/6 datasets if an opinion target is associated with different"
P18-2092,D17-1039,0,0.0225502,"the sentence s is then given by: level annotated data is required, which is expensive to obtain in practice. We explore both pretraining and multi-task learning for transferring knowledge from document level to aspect level. Both methods are widely studied in the literature. Pretraining is a common technique used in computer vision where low-level neural layers can be usefully transferred to different tasks (Krizhevsky and Sutskever, 2012; Zeiler and Fergus, 2014). In natural language processing (NLP), some efforts have been initiated on pretraining LSTMs (Dai and Le, 2015; Zoph et al., 2016; Ramachandran et al., 2017) for sequence-to-sequence models in both supervised and unsupervised settings, where promising results have been obtained. On the other hand, multi-task learning simultaneously trains on samples in multiple tasks with a combined objective (Collobert and Weston, 2008; Luong et al., 2015a; Liu et al., 2016), which has improved model generalization ability in certain cases. In the work of Mou et al. (2016), the authors investigated the transferability of neural models in NLP applications with extensive experiments, showing that transferability largely depends on the semantic relatedness of the so"
P18-2092,D15-1166,0,0.124434,"Missing"
P18-2092,C16-1311,0,0.670216,"pensive”, the targets are interior decor and prices, and they are associated with positive and negative sentiment respectively. A sentence may contain multiple sentimenttarget pairs, thus one challenge is to separate different opinion contexts for different targets. For this purpose, state-of-the-art neural methods (Wang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) adopt attention-based LSTM networks, where the LSTM aims to capture sequential patterns and the attention mechanism aims 2 Related Work Various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) have been proposed for aspect-level sentiment classification. The main idea behind these works is to develop neural architectures that are able to learn continuous features and capture the intricate relation between a target and context words. However, to sufficiently train these models, substantial aspect579 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguist"
P18-2092,P15-1098,0,0.0293381,"es are more significant on D3 and D4 than on D1: (1) D1 has much more neutral examples in the training set. A classifier Experiments Datasets and Experimental Settings We run experiments on four benchmark aspectlevel datasets, taken from SemEval 2014 (Pontiki et al., 2014), SemEval 2015 (Pontiki et al., 2015), and SemEval 2016 (Pontiki et al., 2016). Following previous work (Tang et al., 2016b; Wang et al., 2016), we remove samples with conflicting polarities in all datasets1 . Statistics of the resulting datasets are presented in Table 1. We derived two document-level datasets from Yelp2014 (Tang et al., 2015) and the Amazon Electronics dataset (McAuley et al., 2015) respectively. The original reviews were rated on a 5point scale. We consider 3-class classification and 1 We remove samples in the 2015/6 datasets if an opinion target is associated with different sentiment polarities. 581 Methods Tang et al. (2016a) Wang et al. (2016) Tang et al. (2016b) Chen et al. (2017) LSTM LSTM+ATT Ours: PRET Ours: MULT Ours: PRET+MULT Acc. 75.37 78.60 76.87 78.48 75.23 76.83 78.28 77.41 79.11 D1 Macro-F1 64.51 67.02 66.40 68.54 64.21 66.48 68.55 66.68 69.73∗ D2 Macro-F1 65.96 63.93 62.79 68.43 64.02 64.82 68.53"
P18-2092,D16-1021,0,0.604629,"pensive”, the targets are interior decor and prices, and they are associated with positive and negative sentiment respectively. A sentence may contain multiple sentimenttarget pairs, thus one challenge is to separate different opinion contexts for different targets. For this purpose, state-of-the-art neural methods (Wang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) adopt attention-based LSTM networks, where the LSTM aims to capture sequential patterns and the attention mechanism aims 2 Related Work Various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) have been proposed for aspect-level sentiment classification. The main idea behind these works is to develop neural architectures that are able to learn continuous features and capture the intricate relation between a target and context words. However, to sufficiently train these models, substantial aspect579 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579–585 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguist"
P18-2092,D16-1046,0,0.0533108,"zhevsky and Sutskever, 2012; Zeiler and Fergus, 2014). In natural language processing (NLP), some efforts have been initiated on pretraining LSTMs (Dai and Le, 2015; Zoph et al., 2016; Ramachandran et al., 2017) for sequence-to-sequence models in both supervised and unsupervised settings, where promising results have been obtained. On the other hand, multi-task learning simultaneously trains on samples in multiple tasks with a combined objective (Collobert and Weston, 2008; Luong et al., 2015a; Liu et al., 2016), which has improved model generalization ability in certain cases. In the work of Mou et al. (2016), the authors investigated the transferability of neural models in NLP applications with extensive experiments, showing that transferability largely depends on the semantic relatedness of the source and target tasks. For our problem, we hypothesize that aspect-level sentiment classification can be improved by employing knowledge gained from document-level sentiment classification, as these two tasks are highly related semantically. z= n X αi hi (2) i=1 And αi is computed as follows: exp(βi ) αi = Pn j=1 exp(βj ) βi = fscore (hi , t) = tanh(hTi Wa t) m 1 X t= exi m (3) (4) (5) i=1 where t is th"
P18-2092,D16-1058,0,0.548223,"o determine the sentiment polarity in the sentence towards the opinion target. An opinion target or target for short refers to a word or a phrase describing an aspect of an entity. For example, in the sentence “This little place has a cute interior decor but the prices are quite expensive”, the targets are interior decor and prices, and they are associated with positive and negative sentiment respectively. A sentence may contain multiple sentimenttarget pairs, thus one challenge is to separate different opinion contexts for different targets. For this purpose, state-of-the-art neural methods (Wang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) adopt attention-based LSTM networks, where the LSTM aims to capture sequential patterns and the attention mechanism aims 2 Related Work Various neural models (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017) have been proposed for aspect-level sentiment classification. The main idea behind these works is to develop neural architectures that are able to learn continuous features and capture the intricate relation between a target and conte"
P18-2092,D15-1298,0,0.137813,"Missing"
P18-2092,D16-1163,0,0.0223307,"representation of the sentence s is then given by: level annotated data is required, which is expensive to obtain in practice. We explore both pretraining and multi-task learning for transferring knowledge from document level to aspect level. Both methods are widely studied in the literature. Pretraining is a common technique used in computer vision where low-level neural layers can be usefully transferred to different tasks (Krizhevsky and Sutskever, 2012; Zeiler and Fergus, 2014). In natural language processing (NLP), some efforts have been initiated on pretraining LSTMs (Dai and Le, 2015; Zoph et al., 2016; Ramachandran et al., 2017) for sequence-to-sequence models in both supervised and unsupervised settings, where promising results have been obtained. On the other hand, multi-task learning simultaneously trains on samples in multiple tasks with a combined objective (Collobert and Weston, 2008; Luong et al., 2015a; Liu et al., 2016), which has improved model generalization ability in certain cases. In the work of Mou et al. (2016), the authors investigated the transferability of neural models in NLP applications with extensive experiments, showing that transferability largely depends on the se"
P18-2092,D14-1162,0,\N,Missing
P19-1042,W13-1703,1,0.863381,". Our approach results in statistically significant improvements in overall GEC performance over strong baselines across multiple test sets. Analysis of our cross-sentence GEC model on a synthetic dataset shows high performance in verb tense corrections that require cross-sentence context. 1 Figure 1: A sentence from NUCLE appears correct out of context, but erroneous in context. is essential to identify and correct certain errors, mostly involving tense choice, use of the definite article ‘the’, and use of connectives. The example in Figure 1, from the NUS Corpus of Learner English or NUCLE (Dahlmeier et al., 2013), shows a learner-written sentence that seems correct in isolation, but actually involves a verb tense error in context. Sentence-level GEC systems fail to reliably correct such errors. Moreover, models may also benefit from the additional context by being able to disambiguate error corrections better. In this paper, we present the first approach to cross-sentence GEC1 . We build on a state-of-theart convolutional neural encoder-decoder model and incorporate cross-sentence context from previous sentences using an auxiliary encoder. The decoder attends to the representations generated by the au"
P19-1042,N18-1118,0,0.0555333,"Missing"
P19-1042,P17-1074,0,0.132772,"in these 795 sentences and replacing them by their present tense form6 , producing 1,090 synthetic verb tense errors. These errors require cross6 nc / np 91 / 398 227 / 509 Table 5: Performance on contextual verb tense errors on a synthetic test set. nc denotes the no. of correct changes and np denotes the no. of proposed changes. Figure 3: Output showing a cross-sentence correction. 5.2 P / R / F0.5 22.86 / 8.35 / 16.96 44.60 / 20.83 / 36.31 5.3 Overall Performance across Error Types We evaluate the overall performance on common error types (Figure 4) on the CoNLL-2013 dataset using ERRANT (Bryant et al., 2017). They include determiner (DET), noun number (NOUN:NUM), preposition (PREP), verb tense (VERB:TENSE), and errors that are not categorized (OTHERS). We find that the largest margins of improvements are observed on verb tense errors and determiner errors. This aligns with our expectation of cross-sentence models. 7 We keep the previous sentences in their actual form to analyze the model’s ability in a controlled way. However, in reality, previous sentences may also contain errors. Adapted from https://github.com/bendichter/tenseflow 441 POS BASELINE CROSENT 0.5 NOUN VERB ADJ DET PROPN F0.5 0.4 0"
P19-1042,N18-2046,0,0.0330248,"Missing"
P19-1042,D18-1274,1,0.663234,"by the auxiliary encoder via separate attention mechanisms and incorporates them via gating that controls the information that goes into the deIntroduction Grammatical error correction (GEC) is the task of correcting errors in input text and producing well-formed output text. GEC models are essential components of writing assistance and proof-reading tools that help both native and non-native speakers. Several adaptations of sophisticated sequence-to-sequence learning models with specialized techniques have been shown to achieve impressive performance (Ge et al., 2018; Lichtarge et al., 2018; Chollampatt and Ng, 2018b; Junczys-Dowmunt et al., 2018). All prior approaches to GEC consider one sentence at a time and ignore useful contextual information from the document in which it appears, unlike a human proofreader. Cross-sentence context 1 Our source code is publicly available at https:// github.com/nusnlp/crosentgec. 435 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 435–445 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics We make an assumption to simplify modeling and consider only two previous source sentences as"
P19-1042,N12-1067,1,0.843897,"context. Still, in our dataset, the first English sentence of an essay has an empty context and the second English sentence of an essay has only one previous sentence as its context. We evaluate on three test sets which have document-level contexts: CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014) shared task test sets, and Cambridge Learner Corpus-First Certificate Exam or FCE test set (Yannakoudakis et al., 2011). Another recent dataset, JFLEG (Napoles et al., 2017), does not have documentlevel contexts available and hence we do not use it for evaluation. We use the M2 scorer (Dahlmeier and Ng, 2012) for evaluation and perform significance tests using one-tailed sign test with bootstrap resampling on 100 samples. i=1 3 No. of essays 178,972 1,125 177,847 272 Data and Evaluation Similar to most published GEC models (Chollampatt and Ng, 2018b; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018), we rely on two datasets for training: Lang-8 Learner Corpora3 v2.0 (Mizumoto et al., 2011) and NUCLE (Dahlmeier et al., 2013). Both datasets have document context available which make them suitable for cross-sentence GEC. We split training and development datasets based on essay bo"
P19-1042,J82-2005,0,0.427202,"Missing"
P19-1042,C18-1050,0,0.050185,"Missing"
P19-1042,N19-1406,0,0.0145097,"urce sentence is separated by a special token (<CONCAT&gt;). This model does not have an auxiliary encoder. ‘aux (no gate)’ uses an auxiliary encoder similar to our C RO S ENT model except for gating. ‘aux (+gate)’ is our C RO S ENT model (Section 2.2) which employs the auxiliary encoder with the gating mechanism. The first two variants perform comparably to our sentence-level BASELINE and shows no notable gains from using cross-sentence context. When 5 After the submission of this paper, improved results on the CoNLL-2014 benchmark have been published (Zhao et al., 2019; Lichtarge et al., 2019; Stahlberg et al., 2019). 440 I agree that RFID technology shall not be made available to the public for easy abuse and distorted usage. First, our privacy is at threat. Though tracking devices such as the applications in our smartphones these days can add fun and entertainment to our fast-living paced livings, this can be a double-edged sword.We revealed our locations, for our friends to catch up with us, however we can never know who is watching us out there secretly. BASELINE : We revealed our locations, for our friends to catch up with us, ... C RO S ENT: We reveal our locations, for our friends to catch up with"
P19-1042,P17-2031,0,0.067771,"Missing"
P19-1042,N19-1333,0,0.076053,"ntext and the current source sentence is separated by a special token (<CONCAT&gt;). This model does not have an auxiliary encoder. ‘aux (no gate)’ uses an auxiliary encoder similar to our C RO S ENT model except for gating. ‘aux (+gate)’ is our C RO S ENT model (Section 2.2) which employs the auxiliary encoder with the gating mechanism. The first two variants perform comparably to our sentence-level BASELINE and shows no notable gains from using cross-sentence context. When 5 After the submission of this paper, improved results on the CoNLL-2014 benchmark have been published (Zhao et al., 2019; Lichtarge et al., 2019; Stahlberg et al., 2019). 440 I agree that RFID technology shall not be made available to the public for easy abuse and distorted usage. First, our privacy is at threat. Though tracking devices such as the applications in our smartphones these days can add fun and entertainment to our fast-living paced livings, this can be a double-edged sword.We revealed our locations, for our friends to catch up with us, however we can never know who is watching us out there secretly. BASELINE : We revealed our locations, for our friends to catch up with us, ... C RO S ENT: We reveal our locations, for our"
P19-1042,W17-4811,0,0.0427237,"(10) where σ represents element-wise sigmoid activation that restricts values to [0, 1]. Λl can be reˆ l. garded as probabilities of retaining values in C We also analyze our proposed approach by comparing against two other ways for modeling cross-sentence context (Section 5.1). One way is to integrate representations from the auxiliary encoder directly in the decoder layers via the attention mechanism, without gating. Another more straightforward way of incorporating crosssentence context is by simply passing the concatenation of the previous source sentences and the current source sentence (Tiedemann and Scherrer, 2017) to the main encoder itself, without employing an auxiliary encoder. Output of the lth decoder layer is computed as Gl = Yl + Cl + Gl−1 (9) Cross-Sentence GEC Model 2.3 Our proposed model (Figure 2) incorporates crosssentence context using an auxiliary encoder that is similar in structure to our source sentence encoder (Section 2.1). For encoding the context consisting of two previous sentences, we simply concatenate these two sentences and pass it to the auxiliary encoder. Let Sˆ represent the cross-sentence Other Techniques and BERT Rescoring We further improve our models (both sentencelevel"
P19-1042,Q18-1029,0,0.0165681,"e other hand, there are a number of studies recently on integrating cross-sentence context for neural machine translation (NMT). There are three major approaches for document-level NMT: (1) translating an extended source (context concatenated with the source) to a single or extended target (Tiedemann and Scherrer, 2017; Bawden et al., 2018); (2) using an additional encoder to capture document-wide context (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2018; Voita et al., 2018; Miculicich et al., 2018; Zhang et al., 2018); and (3) using discrete (Kuang et al., 2018) or continuous cache (Tu et al., 2018; Maruf and Haffari, 2018) mechanisms during translation for storing and retrieving document-level information. We investigated the first two approaches in this paper as we believe that most of the ambiguities can be resolved by considering a few previous sentences. Since GEC is a monolingual rewriting task, most of the disambiguating information is in the source sentence itself, unlike bilingual NMT. All approaches for document-level NMT extended recurrent models or Transformer models. There is no prior work that extends convolutional Related Work Sophisticated sequence-to-sequence architectu"
P19-1042,P12-3005,0,0.0142751,"ble 1: datasets. (11) where T−i is the target sentence where the ith target word ti is masked. Experiments 3.1 No. of sentences 1,306,108 16,284 1,289,824 5,006 No. of src tokens 18,080,501 423,503 17,656,998 128,116 Statistics of training and development one essay from every four) until we get over 5,000 annotated sentences. The remaining essays from NUCLE are used for training. From Lang-8, we extract essays written by learners whose native language is English and contain at least two English sentences with a minimum of one annotation. We identify the language of a sentence using langid.py (Lui and Baldwin, 2012). From the extracted essays, we select annotated English sentences (with at most 80 tokens) as our source sentences and their corresponding corrections as our target sentences. Statistics of the datasets are given in Table 1. For training our cross-sentence GEC models, we select two previous English sentences for each source sentence from its essay as the cross-sentence context. We found that using two previous sentences performed better than using only one previous sentence as the context. Still, in our dataset, the first English sentence of an essay has an empty context and the second Englis"
P19-1042,P18-1118,0,0.0231879,"re are a number of studies recently on integrating cross-sentence context for neural machine translation (NMT). There are three major approaches for document-level NMT: (1) translating an extended source (context concatenated with the source) to a single or extended target (Tiedemann and Scherrer, 2017; Bawden et al., 2018); (2) using an additional encoder to capture document-wide context (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2018; Voita et al., 2018; Miculicich et al., 2018; Zhang et al., 2018); and (3) using discrete (Kuang et al., 2018) or continuous cache (Tu et al., 2018; Maruf and Haffari, 2018) mechanisms during translation for storing and retrieving document-level information. We investigated the first two approaches in this paper as we believe that most of the ambiguities can be resolved by considering a few previous sentences. Since GEC is a monolingual rewriting task, most of the disambiguating information is in the source sentence itself, unlike bilingual NMT. All approaches for document-level NMT extended recurrent models or Transformer models. There is no prior work that extends convolutional Related Work Sophisticated sequence-to-sequence architectures (Gehring et al., 2017;"
P19-1042,P18-1117,0,0.0611891,"Missing"
P19-1042,D18-1325,0,0.0355897,"Missing"
P19-1042,D17-1301,0,0.0174632,"t in which it appears, unlike a human proofreader. Cross-sentence context 1 Our source code is publicly available at https:// github.com/nusnlp/crosentgec. 435 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 435–445 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics We make an assumption to simplify modeling and consider only two previous source sentences as the cross-sentence context (Sdoc = Sk−1 , Sk−2 ). We also do not explicitly consider previously corrected target sentences to avoid error propagation (Wang et al., 2017). We extend a sentencelevel encoder-decoder baseline model to build our cross-sentence GEC model. We describe our baseline encoder-decoder model and the proposed cross-sentence model below. coder. Auxiliary encoders have also been used in other sequence generation tasks such as automatic post editing (Libovick´y and Helcl, 2017), multilingual machine translation (Firat et al., 2016), and document-level neural machine translation (Jean et al., 2017; Wang et al., 2017). Our cross-sentence GEC model shows statistically significant improvements over a competitive sentence-level baseline across mul"
P19-1042,I11-1017,0,0.158505,"Missing"
P19-1042,N18-1057,0,0.0737577,"Missing"
P19-1042,E17-2037,0,0.0308491,"as the cross-sentence context. We found that using two previous sentences performed better than using only one previous sentence as the context. Still, in our dataset, the first English sentence of an essay has an empty context and the second English sentence of an essay has only one previous sentence as its context. We evaluate on three test sets which have document-level contexts: CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014) shared task test sets, and Cambridge Learner Corpus-First Certificate Exam or FCE test set (Yannakoudakis et al., 2011). Another recent dataset, JFLEG (Napoles et al., 2017), does not have documentlevel contexts available and hence we do not use it for evaluation. We use the M2 scorer (Dahlmeier and Ng, 2012) for evaluation and perform significance tests using one-tailed sign test with bootstrap resampling on 100 samples. i=1 3 No. of essays 178,972 1,125 177,847 272 Data and Evaluation Similar to most published GEC models (Chollampatt and Ng, 2018b; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018), we rely on two datasets for training: Lang-8 Learner Corpora3 v2.0 (Mizumoto et al., 2011) and NUCLE (Dahlmeier et al., 2013). Both datasets have"
P19-1042,P11-1019,0,0.050588,"s English sentences for each source sentence from its essay as the cross-sentence context. We found that using two previous sentences performed better than using only one previous sentence as the context. Still, in our dataset, the first English sentence of an essay has an empty context and the second English sentence of an essay has only one previous sentence as its context. We evaluate on three test sets which have document-level contexts: CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014) shared task test sets, and Cambridge Learner Corpus-First Certificate Exam or FCE test set (Yannakoudakis et al., 2011). Another recent dataset, JFLEG (Napoles et al., 2017), does not have documentlevel contexts available and hence we do not use it for evaluation. We use the M2 scorer (Dahlmeier and Ng, 2012) for evaluation and perform significance tests using one-tailed sign test with bootstrap resampling on 100 samples. i=1 3 No. of essays 178,972 1,125 177,847 272 Data and Evaluation Similar to most published GEC models (Chollampatt and Ng, 2018b; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018), we rely on two datasets for training: Lang-8 Learner Corpora3 v2.0 (Mizumoto et al., 2011)"
P19-1042,W14-1701,1,0.803849,"stics of the datasets are given in Table 1. For training our cross-sentence GEC models, we select two previous English sentences for each source sentence from its essay as the cross-sentence context. We found that using two previous sentences performed better than using only one previous sentence as the context. Still, in our dataset, the first English sentence of an essay has an empty context and the second English sentence of an essay has only one previous sentence as its context. We evaluate on three test sets which have document-level contexts: CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014) shared task test sets, and Cambridge Learner Corpus-First Certificate Exam or FCE test set (Yannakoudakis et al., 2011). Another recent dataset, JFLEG (Napoles et al., 2017), does not have documentlevel contexts available and hence we do not use it for evaluation. We use the M2 scorer (Dahlmeier and Ng, 2012) for evaluation and perform significance tests using one-tailed sign test with bootstrap resampling on 100 samples. i=1 3 No. of essays 178,972 1,125 177,847 272 Data and Evaluation Similar to most published GEC models (Chollampatt and Ng, 2018b; Junczys-Dowmunt et al., 2018; Grundkiewicz"
P19-1042,D18-1049,0,0.0303959,"Missing"
P19-1042,N19-1014,0,0.0453526,"ce sentence. The context and the current source sentence is separated by a special token (<CONCAT&gt;). This model does not have an auxiliary encoder. ‘aux (no gate)’ uses an auxiliary encoder similar to our C RO S ENT model except for gating. ‘aux (+gate)’ is our C RO S ENT model (Section 2.2) which employs the auxiliary encoder with the gating mechanism. The first two variants perform comparably to our sentence-level BASELINE and shows no notable gains from using cross-sentence context. When 5 After the submission of this paper, improved results on the CoNLL-2014 benchmark have been published (Zhao et al., 2019; Lichtarge et al., 2019; Stahlberg et al., 2019). 440 I agree that RFID technology shall not be made available to the public for easy abuse and distorted usage. First, our privacy is at threat. Though tracking devices such as the applications in our smartphones these days can add fun and entertainment to our fast-living paced livings, this can be a double-edged sword.We revealed our locations, for our friends to catch up with us, however we can never know who is watching us out there secretly. BASELINE : We revealed our locations, for our friends to catch up with us, ... C RO S ENT: We reveal"
P19-1042,W18-6105,0,0.037021,"Missing"
P19-1048,D18-1403,0,0.0272826,"not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this approach is unable to exploit the commonalities and associations"
P19-1048,D17-1310,0,0.101996,"e learned features and such implicit interactions are not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this a"
P19-1048,D17-1047,0,0.448278,"iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this approach is unable to exploit the commonalities and associations between tasks, which may help reduce the amount of training data required to train both tasks. Some previous works have attempted to develop integrated solutions. Zhang et"
P19-1048,P14-2009,0,0.0702707,"ethods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this approach is unable to exploit the commonalities and associations between tasks, which may h"
P19-1048,D15-1298,0,0.0236483,"d IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this approach is unable to exploit the commonalities and associations between tasks, which may help reduce the amount of"
P19-1048,Q15-1035,0,0.015497,"problem as a sequence labeling task with a unified tagging scheme. However, their results were discouraging. Recently, two works (Wang et al., 2018; Li et al., 2019) have shown some promising results in this direction with more sophisticated network structures. However, in their models, the two subtasks are still only linked through a unified tagging scheme, while the interactions between them are not explicitly modMessage Passing Architectures. Networked representations for message passing graphical model inference algorithms have been studied in computer vision (Arnab et al., 2018) and NLP (Gormley et al., 2015). Modeling the execution of these message passing algorithms as a network results in recurrent neural network architectures. We similarly propagate information in a network and learn the update operators, but the architecture is designed for solving multi-task learning problems. Our algorithm can similarly be viewed as a recurrent neural network since each iteration uses the same network to update the shared latent variables. 3 Proposed Method The IMN architecture is shown in Figure 1. It accepts a sequence of tokens {x1 , . . . , xn } as input into a feature extraction component fθs that is 2"
P19-1048,D14-1162,0,0.0823453,"ctronics dataset as Dds when Da is D2. Network details. We adopt the multi-layerCNN structure from (Xu et al., 2018) as the CNN-based encoders in our proposed network. See Appendix A for implementation details. For word embedding initialization, we concatenate a general-purpose embedding matrix and a domain-specific embedding matrix7 following (Xu et al., 2018). We adopt their released domainspecific embeddings for restaurant and laptop domains with 100 dimensions, which are trained on a large domain-specific corpus using fastText. The general-purpose embeddings are pre-trained Glove vectors (Pennington et al., 2014) with 300 dimensions. One set of important hyper-parameters are the number of CNN layers in the shared encoder and the task-specific encoders. To decide the values of ms , mae , mas , mds , mdd , we first investigate Evaluation metrics. During testing, we extract aspect (opinion) terms, and predict the sentiment ˆ ae(T ) and for each extracted aspect term based on y as(T ) ˆ y . Since the extracted aspect term may consist of multiple tokens and the sentiment predictions on them could be inconsistent in AS, we only output the sentiment label of the first token as the predicted sentiment for any"
P19-1048,P17-1036,1,0.846636,"and such implicit interactions are not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this approach is unable"
P19-1048,S15-2082,0,0.344074,"Missing"
P19-1048,C18-1096,1,0.736582,"taneously is multi-task learning, where one conventional framework is to employ a shared network and two task-specific network to derive a shared feature space and two task-specific feature spaces. Multitask learning frameworks have been employed successfully in various natural language processing (NLP) tasks (Collobert and Weston, 2008; Luong et al., 2015a; Liu et al., 2016). By learning semantically related tasks in parallel using a shared representation, multi-task learning could capture the correlations between tasks and improve the model generalization ability in certain cases. For ABSA, He et al. (2018b) have shown that aspectlevel sentiment classification can be significantly improved through joint training with documentlevel sentiment classification. However, conventional multi-task learning still does not explicitly model the interactions between tasks – the two tasks only interact with each other through error back-propoagation to contribute to the learned features and such implicit interactions are not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative"
P19-1048,P18-2092,1,0.562295,"taneously is multi-task learning, where one conventional framework is to employ a shared network and two task-specific network to derive a shared feature space and two task-specific feature spaces. Multitask learning frameworks have been employed successfully in various natural language processing (NLP) tasks (Collobert and Weston, 2008; Luong et al., 2015a; Liu et al., 2016). By learning semantically related tasks in parallel using a shared representation, multi-task learning could capture the correlations between tasks and improve the model generalization ability in certain cases. For ABSA, He et al. (2018b) have shown that aspectlevel sentiment classification can be significantly improved through joint training with documentlevel sentiment classification. However, conventional multi-task learning still does not explicitly model the interactions between tasks – the two tasks only interact with each other through error back-propoagation to contribute to the learned features and such implicit interactions are not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative"
P19-1048,S14-2004,0,0.256456,"the model for a fix number of epoches, and save the model at the epoch with the best F1-I score on the development set for evaluation. Test opinion 1008 674 510 Table 2: Dataset statistics with numbers of aspect terms and opinion terms they are semantically relevant. We fix θds and θdd when updating parameters for La , since we do not want them to be affected by the small number of aspect-level training instances. 4 4.1 Experiments Experimental Settings Datasets. Table 2 shows the statistics of the aspect-level datasets. We run experiments on three benchmark datasets, taken from SemEval2014 (Pontiki et al., 2014) and SemEval 2015 (Pontiki et al., 2015). The opinion terms are annotated by Wang et al. (2016a). We use two document-level datasets from (He et al., 2018b). One is from the Yelp restaurant domain, and the other is from the Amazon electronics domain. Each contains 30k instances with exactly balanced class labels of pos, neg, and neu. We use the concatenation of the two datasets with domain labels as Ddd . We use the Yelp dataset as Dds when Da is either D1 or D3, and use the electronics dataset as Dds when Da is D2. Network details. We adopt the multi-layerCNN structure from (Xu et al., 2018)"
P19-1048,P18-1087,0,0.20101,"t interactions are not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this approach is unable to exploit the c"
P19-1048,J11-1002,0,0.369327,"each other through error back-propoagation to contribute to the learned features and such implicit interactions are not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step"
P19-1048,C16-1311,0,0.136833,"be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this approach is unable to exploit the commonalities and associations between tasks, which may help reduce the amount of training data required to train both ta"
P19-1048,D16-1059,1,0.910814,"gation to contribute to the learned features and such implicit interactions are not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall perform"
P19-1048,D16-1058,0,0.840304,"gation to contribute to the learned features and such implicit interactions are not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA. Related Work Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE (Qiu et al., 2011; Yin et al., 2016; Wang et al., 2016a, 2017; Li and Lam, 2017; He et al., 2017; Li et al., 2018b; Angelidis and Lapata, 2018) and AS (Dong et al., 2014; Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a; Wang et al., 2016b; Zhang et al., 2016; Liu and Zhang, 2017; Chen et al., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall perform"
P19-1048,D15-1073,0,0.265176,"l., 2017; Cheng et al., 2017; Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b; Li et al., 2018a) have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this approach is unable to exploit the commonalities and associations between tasks, which may help reduce the amount of training data required to train both tasks. Some previous works have attempted to develop integrated solutions. Zhang et al. (2015) proposed to model the problem as a sequence labeling task with a unified tagging scheme. However, their results were discouraging. Recently, two works (Wang et al., 2018; Li et al., 2019) have shown some promising results in this direction with more sophisticated network structures. However, in their models, the two subtasks are still only linked through a unified tagging scheme, while the interactions between them are not explicitly modMessage Passing Architectures. Networked representations for message passing graphical model inference algorithms have been studied in computer vision (Arnab"
P19-1610,C18-1139,0,0.0158023,"ing is shown in Table 3. Even though the augmented training dataset is noisy (since not all generated questions are true paraphrases), all QA models still show improvement on the paraphrased test set after retraining. Furthermore, re-training causes only a negligible drop to the performance of QA models on the original development set, as shown in Table 4. 4.2.2 Adversarial Paraphrased Test Set In contrast to using PPDB to obtain paraphrase suggestions for the neural paraphrasing model, we now require the paraphrase suggestions to be from the context of the associated question. We use Flair6 (Akbik et al., 2018) trained on the Ontonotes dataset7 which contains 12 named entity classes to label which named entity class, if any, that the answer belongs to. Then, we extract 6 Pre-trained models available https://github.com/zalandoresearch/flair 7 https://catalog.ldc.upenn.edu/docs/LDC2013T19/ OntoNotes-Release-5.0.pdf 6071 at Model BERT DrQA BiDAF EM Score Before After 57.14 69.64 39.29 41.07 30.36 39.29 F1 Score Before After 63.18 73.85 48.94 49.86 38.30 47.49 5 We present related work in this section, divided into three sub-topics. 5.1 Table 5: Performance of QA models on the adversarial test set befor"
P19-1610,D18-2012,0,0.164057,"del from Vaswani et al. (2017) which is an encoder-decoder architecture that relies mainly on a self-attention mechanism. We extend the decoder using the copy mechanism of See et al. (2017) which allows tokens to be copied from the source question. This is achieved by augmenting the probability distribution of the output vocabulary to include tokens from the source question. The input to the encoder is the concatenation of a paraphrase suggestion and the source question separated by a special token: “&lt;suggestion&gt; &lt;sep&gt; &lt;source question&gt;”, tokenized using the subword tokenizer SentencePiece by Kudo and Richardson (2018). 2.2 Dataset Preparation We use a combination of the WikiAnswers paraphrase corpus (Fader et al., 2013) and the Quora Question Pairs dataset2 for training. The two questions in a question pair in the Quora dataset are typically very similar in meaning. In contrast, the WikiAnswers paraphrase corpus tends to be noisier but one source question is paired with multiple target questions. This allows the model to be trained to output different target questions depending on the paraphrase suggestion given. A combination of these two datasets thus provides a balance between good paraphrasing and usin"
P19-1610,E17-1083,0,0.051392,"e need to improve the robustness of current QA models. The creation of the adversarial paraphrased test set which aims to trick QA models intentionally also contrasts with the approach by Jia and Liang (2017), as the examples created in this work are natural and coherent. 5.2 Neural Paraphrasing Networks There are a number of neural architectures introduced to automatically generate a paraphrase given an input sentence (Prakash et al., 2016; 6072 Huang et al., 2018; Wang et al., 2018). One conceptually simple approach that does not require a paraphrase corpus is to carry out back translation (Lapata et al., 2017), by first translating the source sentence to a pivot foreign language and back. Besides single paraphrase generation, the value of generating multiple paraphrases for a given input sentence has also been explored. Gupta et al. (2018) achieved this by using a variational autoencoder (VAE) with a long short-term memory (LSTM) network. Xu et al. (2018) assumed that different paraphrasing styles used different rewriting patterns, which were represented as latent embeddings. These embeddings were used to augment the decoder’s hidden state to generate different paraphrases. In contrast to previous"
P19-1610,P17-1171,0,0.027582,"V ... Original Question: In what year did Doctor Who begin being shown in HDTV? Prediction: 2009 Paraphrased Question: Since what year has Doctor Who been televised in HDTV? Prediction: 1963 3,000 2,000 1,000 0 1 2 3 4 5 Figure 6: Semantic equivalence ratings 3,000 2,000 1,000 Figure 8: An example of paraphrasing question using context words (underlined) near a confusing answer candidate to generate a natural adversarial example. 0 1 2 3 4 5 Figure 7: Fluency ratings 4 3.2 Experiments on QA Models We conduct experiments on three state-of-the-art QA models: BERT (Devlin et al., 2018)3 , DrQA4 (Chen et al., 2017), and BiDAF5 (Seo et al., 2016). BERT, in particular, outperforms human on the SQuAD task. Adversarial Paraphrased Test Set Motivated by the observation that QA models trained on SQuAD tend to perform string matching to return an answer of an appropriate type near a region of significant word overlap between the context and the question (Jia and Liang, 2017; Rondeau and Hazen, 2018), we create a test set to exploit this weakness of the models. In the context of question paraphrasing, we can simply paraphrase the question by using words in the context near a wrong answer candidate of the same t"
P19-1610,W04-3252,0,0.0198177,"a training example (q, si , ti ). We constrain the selection such that all paraphrase suggestions chosen are unique, i.e., ∀i, j(i 6= j ⇒ si 6= sj ). This is to ensure that there are no duplicate (q, si ) input pairs in the training dataset which will result in the model being trained on different targets given the same input. Furthermore, to enable the model to paraphrase even without a suggestion given, some paraphrase suggestions are randomly selected to be replaced with a special empty token. Quora dataset: Since the Quora dataset does not come with word alignments, we first use TextRank (Mihalcea and Tarau, 2004) to obtain question keywords from both source and target questions. Then, the paraphrase suggestion is the highest ranked key phrase in the target question that is not in the source question. We do not allow stopwords to be selected as a paraphrase suggestion. Similarly, a random subset of the paraphrase suggestions is replaced with the special empty token. We show an example of obtaining paraphrase suggestions for this dataset in Figure 3. 2.3 Implementation We train our paraphrasing model using the implementation by OpenNMT (Klein et al., 2018), following the hyper-parameters of Vaswani et a"
P19-1610,D17-1091,0,0.0288351,"estion together with a source question to generate a paraphrased question. Given k suggestions, our model is thus able to generate up to k paraphrased questions. 5.3 Paraphrasing as an Intermediate Task to Question Answering Some previous work considers question reformulation as a subtask of question answering. The intuition for doing this is to reduce the space of question paraphrases that the QA model is required to understand. Models trained by this approach are expected to be more robust to various question paraphrases since the model can paraphrase a question to one which it understands. Dong et al. (2017) first generated multiple paraphrases for a given question and used a neural network to score the quality of each paraphrase. The probability distribution of the answer was then generated for each paraphrased question, which was subsequently weighted by the score of each paraphrased question to compute the overall conditional probability of the answer given the question. Buck et al. (2017) formulated QA as a reinforcement learning problem and introduced a paraphrasing agent trained to paraphrase a question to one that was able to get the best answer from the QA model. Similarly, multiple quest"
P19-1610,P13-1158,0,0.0282552,"mechanism. We extend the decoder using the copy mechanism of See et al. (2017) which allows tokens to be copied from the source question. This is achieved by augmenting the probability distribution of the output vocabulary to include tokens from the source question. The input to the encoder is the concatenation of a paraphrase suggestion and the source question separated by a special token: “&lt;suggestion&gt; &lt;sep&gt; &lt;source question&gt;”, tokenized using the subword tokenizer SentencePiece by Kudo and Richardson (2018). 2.2 Dataset Preparation We use a combination of the WikiAnswers paraphrase corpus (Fader et al., 2013) and the Quora Question Pairs dataset2 for training. The two questions in a question pair in the Quora dataset are typically very similar in meaning. In contrast, the WikiAnswers paraphrase corpus tends to be noisier but one source question is paired with multiple target questions. This allows the model to be trained to output different target questions depending on the paraphrase suggestion given. A combination of these two datasets thus provides a balance between good paraphrasing and using a paraphrase suggestion to generate a paraphrase. 2.2.1 Obtaining Source and Target Questions WikiAnsw"
P19-1610,D17-1215,0,0.512959,"ear a confusing answer candidate to generate a natural adversarial example. 0 1 2 3 4 5 Figure 7: Fluency ratings 4 3.2 Experiments on QA Models We conduct experiments on three state-of-the-art QA models: BERT (Devlin et al., 2018)3 , DrQA4 (Chen et al., 2017), and BiDAF5 (Seo et al., 2016). BERT, in particular, outperforms human on the SQuAD task. Adversarial Paraphrased Test Set Motivated by the observation that QA models trained on SQuAD tend to perform string matching to return an answer of an appropriate type near a region of significant word overlap between the context and the question (Jia and Liang, 2017; Rondeau and Hazen, 2018), we create a test set to exploit this weakness of the models. In the context of question paraphrasing, we can simply paraphrase the question by using words in the context near a wrong answer candidate of the same type to generate a natural adversarial example. We show in Figure 8 an example of producing such a paraphrased question. Since the correct answer “2009” is a year, we locate another year “1963” in the context and use the nearby context words “been televised” to paraphrase the original question. We perform such paraphrasing manually by going through question"
P19-1610,P17-1147,0,0.0402897,"odels. We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models. Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing. 1 Introduction With the release of large-scale, high-quality, and increasingly challenging question answering (QA) datasets (Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Reddy et al., 2018), the research community has made rapid progress on QA systems. On the popular SQuAD dataset (Rajpurkar et al., 2016), top QA models have achieved higher evaluation scores compared to human. However, since the test set is typically a randomly selected subset of the whole set of data collected, and thus follows the same distribution as the training and development sets, the performance of models on the test set tends to overestimate the models’ ability to generalize to other unseen test data. It is thus important for QA models to be evaluated on other unseen test data for a"
P19-1610,W18-1817,0,0.0295335,"h word alignments, we first use TextRank (Mihalcea and Tarau, 2004) to obtain question keywords from both source and target questions. Then, the paraphrase suggestion is the highest ranked key phrase in the target question that is not in the source question. We do not allow stopwords to be selected as a paraphrase suggestion. Similarly, a random subset of the paraphrase suggestions is replaced with the special empty token. We show an example of obtaining paraphrase suggestions for this dataset in Figure 3. 2.3 Implementation We train our paraphrasing model using the implementation by OpenNMT (Klein et al., 2018), following the hyper-parameters of Vaswani et al. (2017). We lowercase all data for training and create a tokenized vocabulary of size 8k from SentencePiece (Kudo and Richardson, 2018). 6067 Question Word Alignments Phrase Alignments Candidate Suggestions Source what nutrients do green peppers have in them ? Target what nutrients does a green pepper contain ? (what, what) (green, a green) (have in them, contain) ... a green, pepper, contain Figure 2: An example of finding possible paraphrase suggestions for a source and target question pair from the WikiAnswers dataset. Since there can be mul"
P19-1610,P15-2070,0,0.0445486,"Missing"
P19-1610,C16-1275,0,0.0367923,"rform any model querying when creating the test set. The ability of our generic approach to decrease the performance of all evaluated state-of-the-art QA models demonstrates the need to improve the robustness of current QA models. The creation of the adversarial paraphrased test set which aims to trick QA models intentionally also contrasts with the approach by Jia and Liang (2017), as the examples created in this work are natural and coherent. 5.2 Neural Paraphrasing Networks There are a number of neural architectures introduced to automatically generate a paraphrase given an input sentence (Prakash et al., 2016; 6072 Huang et al., 2018; Wang et al., 2018). One conceptually simple approach that does not require a paraphrase corpus is to carry out back translation (Lapata et al., 2017), by first translating the source sentence to a pivot foreign language and back. Besides single paraphrase generation, the value of generating multiple paraphrases for a given input sentence has also been explored. Gupta et al. (2018) achieved this by using a variational autoencoder (VAE) with a long short-term memory (LSTM) network. Xu et al. (2018) assumed that different paraphrasing styles used different rewriting pat"
P19-1610,P18-2124,0,0.0343318,"iability of QA models on unseen test questions. We focus on the SQuAD QA task in this paper. SQuAD was created by getting crowd workers to create questions and answers from Wikipedia paragraphs. SQuAD serves as a benchmark for QA systems, taking as input a question and a context to predict the correct answer. Two evaluation metrics are used: exact match (EM) and F1. Since an answer must be a span from the context, most models output a probability distribution for the start and end token separately, and constrain the end token to be after the start token. Despite the availability of SQuAD 2.0 (Rajpurkar et al., 2018) which requires models to additionally decide whether a question is unanswerable, we focus on the original version of SQuAD (Rajpurkar et al., 2016). This is due to the simpler task of the original SQuAD which allows us to concentrate on robustness of models to question paraphrasing. We created two paraphrased test sets by paraphrasing SQuAD questions so as to evaluate the robustness of models to question paraphrasing. Using a neural paraphrasing model trained to generate a paraphrased question given a source question and a paraphrase suggestion, we created a non-adversarial paraphrased test s"
P19-1610,D16-1264,0,0.438714,"nswer candidate in an attempt to confuse QA models. We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models. Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing. 1 Introduction With the release of large-scale, high-quality, and increasingly challenging question answering (QA) datasets (Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Reddy et al., 2018), the research community has made rapid progress on QA systems. On the popular SQuAD dataset (Rajpurkar et al., 2016), top QA models have achieved higher evaluation scores compared to human. However, since the test set is typically a randomly selected subset of the whole set of data collected, and thus follows the same distribution as the training and development sets, the performance of models on the test set tends to overestimate the models’ ability to generalize to other unseen test data. It is thus important for QA models to be"
P19-1610,P18-1079,0,0.0287419,"le 6). Adversarial Examples for Question Answering Jia and Liang (2017) showed that QA models can be confused by appending a distracting sentence to the end of a passage. While this highlighted an important weakness of trained models, the adversarial examples created are unnatural and not expected to be present in naturally occurring passages. In contrast, semantic preserving changes to an input question that lead to returning the wrong answers present more relevant failure cases that occur in practice. Some previous work used question paraphrasing to create more natural adversarial examples. Ribeiro et al. (2018) made use of back translation to obtain paraphrasing rules that were subsequently filtered by human annotators. Examples of rules obtained include “What VERB → So what VERB” and “What NOUN → Which NOUN”. Rychalska et al. (2018) replaced the most important question word identified using the LIME framework with a synonym from WordNet and ELMo embeddings, which was verified by human annotators. These replacements are expected to maintain the meaning of the questions but can sometimes change initially correct answers. In contrast, we do not restrict ourselves to specific types of paraphrasing when"
P19-1610,W18-2602,0,0.0186821,"r candidate to generate a natural adversarial example. 0 1 2 3 4 5 Figure 7: Fluency ratings 4 3.2 Experiments on QA Models We conduct experiments on three state-of-the-art QA models: BERT (Devlin et al., 2018)3 , DrQA4 (Chen et al., 2017), and BiDAF5 (Seo et al., 2016). BERT, in particular, outperforms human on the SQuAD task. Adversarial Paraphrased Test Set Motivated by the observation that QA models trained on SQuAD tend to perform string matching to return an answer of an appropriate type near a region of significant word overlap between the context and the question (Jia and Liang, 2017; Rondeau and Hazen, 2018), we create a test set to exploit this weakness of the models. In the context of question paraphrasing, we can simply paraphrase the question by using words in the context near a wrong answer candidate of the same type to generate a natural adversarial example. We show in Figure 8 an example of producing such a paraphrased question. Since the correct answer “2009” is a year, we locate another year “1963” in the context and use the nearby context words “been televised” to paraphrase the original question. We perform such paraphrasing manually by going through question and context pairs from the"
P19-1610,W00-0726,0,0.147818,"Missing"
P19-1610,P17-1099,0,0.060116,"(Section 3.1.1) to generate multiple paraphrases for a given SQuAD question. This is useful for the creation of the nonadversarial paraphrased test set (Section 3.1) and additional training data for improvement on this test set (Section 4.2.1). This model is also useful for training data augmentation for improvement on the adversarial paraphrased test set (Section 4.2.2). 6066 2.1 Model Architecture We use the transformer model from Vaswani et al. (2017) which is an encoder-decoder architecture that relies mainly on a self-attention mechanism. We extend the decoder using the copy mechanism of See et al. (2017) which allows tokens to be copied from the source question. This is achieved by augmenting the probability distribution of the output vocabulary to include tokens from the source question. The input to the encoder is the concatenation of a paraphrase suggestion and the source question separated by a special token: “&lt;suggestion&gt; &lt;sep&gt; &lt;source question&gt;”, tokenized using the subword tokenizer SentencePiece by Kudo and Richardson (2018). 2.2 Dataset Preparation We use a combination of the WikiAnswers paraphrase corpus (Fader et al., 2013) and the Quora Question Pairs dataset2 for training. The tw"
P19-1610,P18-1042,0,0.186294,"ides a balance between good paraphrasing and using a paraphrase suggestion to generate a paraphrase. 2.2.1 Obtaining Source and Target Questions WikiAnswers dataset: This paraphrase corpus contains over 22 million question pairs. We use only a small portion of this dataset so as not to overwhelm the Quora dataset. We only keep a question pair if each question is at least 7 tokens long, since training on longer sentences is more helpful. We also attempt to filter out erroneous question pairs by removing all question pairs with paraphrase similarity scores below 0.7 using a pre-trained model by Wieting and Gimpel (2018). Then, we randomly sample source questions to obtain about 350,000 question pairs. Quora dataset: For the Quora dataset, we use a pair of questions as two training examples by including both source question to target question and vice versa in the training set, i.e., we include QuestionA → QuestionB and QuestionB → QuestionA 2 https://data.quora.com/First-Quora-Dataset-ReleaseQuestion-Pairs in the training set. A total of about 280,000 training examples come from the Quora dataset. 2.2.2 Obtaining Paraphrase Suggestions WikiAnswers dataset: For each source and target question pair, we use wor"
P96-1006,H92-1022,0,0.00869019,"Missing"
P96-1006,P94-1020,0,0.786181,"st, 1987) used hand-coding of knowledge to perform WSD. The knowledge acquisition process is laborious. In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules. The recent emphasis on corpus based NLP has resulted in much work on WSD of unconstrained realworld texts. One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as (Wilks et al., 1990; Luk, 1995). In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. However, the POS used are abbreviated POS, and only in a window of -b2 words. No local collocation knowledge is used. A probabilistic classifier is used in (Bruce and W"
P96-1006,A92-1018,0,0.0333953,"Missing"
P96-1006,P92-1032,0,0.0690867,"Missing"
P96-1006,H93-1051,0,0.685962,"rials each. In each run, we utilized only one knowledge source and compute the average classification accuracy and the standard deviation. The results are given in Table 4. Local collocation knowledge yields the highest accuracy, followed by POS and morphological form. Surrounding words give lower accuracy, perhaps because in our work, only the current sentence forms the surrounding context, which averages about 20 words. Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al., 1993). Verb-object syntactic relation is the weakest knowledge source. Our experimental finding, t h a t local collocations are the most predictive, agrees with past observation that humans need a narrow window of only a few words to perform WSD (Choueka and Lusignan, 1985). The processing speed of LEXAS is satisfactory. Running on an SGI Unix workstation, LEXAS can process about 15 examples per second when tested on the ""interest"" data set. 4.2 The 70 verbs are: add appear ask become believe bring build call carry change come consider continue determine develop draw expect fall give go grow happen"
P96-1006,P95-1025,0,0.166188,"Missing"
P96-1006,J92-1001,0,0.516011,"Missing"
P96-1006,H94-1046,0,0.201651,"Missing"
P96-1006,C92-2070,0,0.761396,"e conducted 4 separate runs of 100 r a n d o m trials each. In each run, we utilized only one knowledge source and compute the average classification accuracy and the standard deviation. The results are given in Table 4. Local collocation knowledge yields the highest accuracy, followed by POS and morphological form. Surrounding words give lower accuracy, perhaps because in our work, only the current sentence forms the surrounding context, which averages about 20 words. Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al., 1993). Verb-object syntactic relation is the weakest knowledge source. Our experimental finding, t h a t local collocations are the most predictive, agrees with past observation that humans need a narrow window of only a few words to perform WSD (Choueka and Lusignan, 1985). The processing speed of LEXAS is satisfactory. Running on an SGI Unix workstation, LEXAS can process about 15 examples per second when tested on the ""interest"" data set. 4.2 The 70 verbs are: add appear ask become believe bring build call carry change come consider continue"
P96-1006,H93-1052,0,0.714573,"Missing"
P96-1006,P94-1013,0,0.689704,"Missing"
P96-1006,P95-1026,0,0.297533,"ristic. They attributed this to insufficient training data in SEMCOm In contrast, we adopt a different strategy of collecting the training data set. Instead of tagging every word in a running text, as is done in SEMCOR, we only concentrate on the set of 191 most frequently occurring and most ambiguous words, and collected large enough training data for these words only. This strategy yields better results, as indicated by a better performance of LEXAS compared with the most frequent heuristic on this set of words. Most recently, Yarowsky used an unsupervised learning procedure to perform WSD (Yarowsky, 1995), although this is only tested on disambiguating words into binary, coarse sense distinction. The effectiveness of unsupervised learning on disambiguating words into the refined sense distinction of WoRBNET needs to be further investigated. The work of (McRoy, 1992) pointed out that a diverse set of knowledge sources are important to achieve WSD, but no quantitative evaluation was given on the relative importance of each knowledge source. No previous work has reported any such evaluation either. The work of (Cardie, 1993) used a case-based approach that simultaneously learns part of speech, wo"
P99-1057,A97-1032,0,0.334616,"has been recognized in (Appelt and Israel, 1997). Recently, there is also a greater realization within the computational linguistics community that the layout and types of information (such as tables) contained in a document are important considerations in text processing (see the call for participation (Power and Scott, 1999) for the 1999 AAAI Fail Symposium Series). 443 However, despite the omnipresence of tables and their importance, there is surprisingly very little work in computational linguistics on algorithms to recognize tables. The only research that we are aware of is the work of (Hurst and Douglas, 1997; Douglas and Hurst, 1996; Douglas et al., 1995). Their method is essentially a deterministic algorithm that relies on spaces and special punctuation symbols to identify the presence and structure of tables. However, tables are notoriously idiosyncratic. The main difficulty in table recognition is that there axe so many different and varied ways in which tables can show up in real-world texts. Any deterministic algorithm based on a fixed set of conditions is bound to fail on tables with unforeseen layout and structure in some domains. In contrast, we present a new approach in this paper that l"
S07-1010,W02-0817,0,0.0299894,"rent corpus. In this paper, we describe our motivation for organizing the task, our task framework, and the results of participants. Past research has shown that supervised learning is one of the most successful approaches to WSD. However, this approach involves the collection of a large text corpus in which each ambiguous word has been annotated with the correct sense to serve as training data. Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottleneck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. Data gathered through the OMWE project were used in the SENSEVAL-3 English lexical sample task. In that task, WordNet-1.7.1 was used as the sense inventory for nouns and adjectives, while Wordsmyth 1 was used as the sense inventory for verbs. Another source of potential training data is parallel texts. Our past research in (Ng et al., 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD. Briefly, after manually assigning appropriate Chinese translations to each sense of an English word, the English s"
S07-1010,W04-0802,0,0.0433529,"Chinese translations chosen. The English side of these occurrences served as training data for w, as they were considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. The English half of the parallel texts (each ambiguous English word and its 3sentence context) were used as the training and test material to set up our English lexical sample task. Note that in our approach, the sense distinction is decided by the different Chinese translations assigned to each sense of a word. This is thus similar to the multilingual lexical sample task in SENSEVAL-3 (Chklovski et al., 2004), except that our training and test examples are collected without manually annotating each individual ambiguous word occurrence. The average time needed to assign Chinese translations for one noun and one adjective is 20 minutes and 25 minutes respectively. This is a relatively short time, compared to the effort otherwise needed to manually sense annotate individual word occurrences. Also, once the Chinese translations are assigned, more examples can be automatically gathered as more parallel texts become available. We note that frequently occurring words are usually highly polysemous and har"
S07-1010,I05-3025,1,0.75883,"the partici1 http://www.wordsmyth.net 54 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54–58, c Prague, June 2007. 2007 Association for Computational Linguistics pating systems. In Section 4, we present the results obtained by the participants, before concluding in Section 5. 2 Gathering Examples from Parallel Corpora To gather examples from parallel corpora, we followed the approach in (Ng et al., 2003). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al., 2005). We then made use of the GIZA++ software (Och and Ney, 2000) to perform word alignment on the parallel corpora. Then, we assigned some possible Chinese translations to each sense of an English word w. From the word alignment output of GIZA++, we selected those occurrences of w which were aligned to one of the Chinese translations chosen. The English side of these occurrences served as training data for w, as they were considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. The English half of the parallel texts (each ambiguous English word and its 3se"
S07-1010,P03-1058,1,0.960736,"the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottleneck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. Data gathered through the OMWE project were used in the SENSEVAL-3 English lexical sample task. In that task, WordNet-1.7.1 was used as the sense inventory for nouns and adjectives, while Wordsmyth 1 was used as the sense inventory for verbs. Another source of potential training data is parallel texts. Our past research in (Ng et al., 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD. Briefly, after manually assigning appropriate Chinese translations to each sense of an English word, the English side of a word-aligned parallel text can then serve as the training data, as they are considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. Using the above approach, we gathered the training and test examples for our task from parallel texts. Note that our examples are collected without manually annotating each individual ambiguous word occurrence"
S07-1010,P00-1056,0,0.0237246,"4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54–58, c Prague, June 2007. 2007 Association for Computational Linguistics pating systems. In Section 4, we present the results obtained by the participants, before concluding in Section 5. 2 Gathering Examples from Parallel Corpora To gather examples from parallel corpora, we followed the approach in (Ng et al., 2003). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al., 2005). We then made use of the GIZA++ software (Och and Ney, 2000) to perform word alignment on the parallel corpora. Then, we assigned some possible Chinese translations to each sense of an English word w. From the word alignment output of GIZA++, we selected those occurrences of w which were aligned to one of the Chinese translations chosen. The English side of these occurrences served as training data for w, as they were considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. The English half of the parallel texts (each ambiguous English word and its 3sentence context) were used as the training and test material t"
S07-1010,J03-3002,0,0.0469024,"e some statistics about the examples. For instance, each noun has an average of 197.6 training and 98.5 test examples and these examples represent an average of 5.2 senses per noun. 2 Participants taking part in this track need to have access to this LDC corpus in order to access the training and test material in this track. 2.2 Web Corpus Since not all interested participants may have access to the LDC corpus described in the previous subsection, the second track of this task makes use of English-Chinese documents gathered from the URL pairs given by the STRAND Bilingual Databases. 3 STRAND (Resnik and Smith, 2003) is a system that acquires document pairs in parallel translation automatically from the Web. Using this corpus, we gathered examples for 40 English words (20 nouns and 2 3 Only senses present in the examples are counted. http://www.umiacs.umd.edu/∼resnik/strand 20 adjectives). 3 Participating Systems The rows Web noun and Web adjective in Table 1 show that we selected an average of 182.0 training and 91.3 test examples for each noun and these examples represent an average of 3.5 senses per noun. We note that the average number of senses per word for the Web corpus is slightly lower than that"
S07-1054,P00-1064,0,0.0292477,"rpora and DSO corpus were randomly chosen but adhering to the sense distribution (proportion of each sense) of that word in the S EM C OR corpus. 2.5 Sense Inventory The test data of the two SemEval-2007 tasks we participated in are based on the WordNet-2.1 sense inventory. However, the examples we gathered from the parallel texts and the S EM C OR corpus are based on the WordNet-1.7.1 sense inventory. Hence, there is a need to map these examples from WordNet-1.7.1 to WordNet-2.1 sense inventory. For this, we rely primarily on the WordNet sense mappings automatically generated by the work of (Daude et al., 2000). To ensure the accuracy of the mappings, we performed some manual corrections of our own, focusing on the set of most frequently occurring nouns, adjectives, and verbs. For the verb examples from the DSO corpus which are based on the WordNet1.5 sense inventory, we manually mapped them to WordNet-2.1 senses. English all-words task Coarse-grained Fine-grained Training data SC+DSO SC+DSO+PT 0.817 0.825 0.578 0.587 Table 1: Scores for the coarse-grained English allwords task and fine-grained English all-words task, using different sets of training data. SC+DSO refers to using examples gathered fr"
S07-1054,W04-0827,0,0.11286,"Missing"
S07-1054,S01-1020,0,0.0392841,"Missing"
S07-1054,W02-1006,1,0.93885,"coarse-grained English all-words task1 and the second place for the fine-grained English all-words task. 1 Introduction In this paper, we describe the systems we developed for the coarse-grained English all-words task 1 A system developed by one of the task organizers of the coarse-grained English all-words task gave the highest overall score for the coarse-grained English all-words task, but this score is not considered part of the official scores. We developed 2 separate systems; one for each task. For both systems, we performed supervised word sense disambiguation based on the approach of (Lee and Ng, 2002) and using Support Vector Machines (SVM) as our learning algorithm. The knowledge sources used include local collocations, parts-of-speech (POS), and surrounding words. Our system employed for the coarse-grained English allwords task was trained with the coarse-grained sense inventory released by the task organizers, while our system employed for the fine-grained English allwords task was trained with the fine-grained sense 253 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 253–256, c Prague, June 2007. 2007 Association for Computational Linguistics"
S07-1054,I05-3025,1,0.616644,", 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD. In this evaluation, we gathered training data from 6 English-Chinese parallel corpora (Hong Kong Hansards, Hong Kong News, Hong Kong Laws, Sinorama, Xinhua News, and English translation of Chinese Treebank), available from the Linguistic Data Consortium (LDC). To gather examples from these parallel corpora, we followed the approach in (Ng et al., 2003). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al., 2005). We then made use of the GIZA++ software (Och and Ney, 2000) to perform word alignment on the parallel corpora. Then, we assigned some possible Chinese translations to each sense of an English word w. From the word alignment output of GIZA++, we selected those occurrences of w which were aligned to one of the Chinese translations chosen. The English side of these occurrences served as training data for w, as they were considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. We note that frequently occurring words are usually highly polysemous and hard"
S07-1054,H94-1046,0,0.209138,"Missing"
S07-1054,P96-1006,1,0.761961,"arguments occurring in three documents, according to the fine-grained sense inventory of WordNet. Results from previous SENSEVAL English allwords task have shown that supervised learning gives the best performance. Further, the best performing system in SENSEVAL-3 English all-words task (Decadt et al., 2004) used training data gathered from multiple sources, highlighting the importance of having a large amount of training data. Hence, besides gathering examples from the widely used S EM C OR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996). We participated in the SemEval-2007 coarse-grained English all-words task and fine-grained English all-words task. We used a supervised learning approach with SVM as the learning algorithm. The knowledge sources used include local collocations, parts-of-speech, and surrounding words. We gathered training examples from English-Chinese parallel corpora, S EM C OR, and DSO corpus. While the fine-grained sense inventory of WordNet was used to train our system employed for the fine-grained English all-words task, our system employed for the coarse-grained English all-words task was trained with t"
S07-1054,P03-1058,1,0.856687,"SENSEVAL allwords tasks. Hence, we also gathered examples from S EM C OR as part of our training data. We gathered training examples from parallel corpora, S EM C OR (Miller et al., 1994), and the DSO corpus. In this section, we describe these corpora and how examples gathered from them are combined to form the training data used by our systems. As these data sources use an earlier version of the WordNet sense inventory as compared to the test data of the two tasks we participated in, we also discuss the need to map between different versions of WordNet. 2.2 2.3 2.1 Parallel Text Research in (Ng et al., 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD. In this evaluation, we gathered training data from 6 English-Chinese parallel corpora (Hong Kong Hansards, Hong Kong News, Hong Kong Laws, Sinorama, Xinhua News, and English translation of Chinese Treebank), available from the Linguistic Data Consortium (LDC). To gather examples from these parallel corpora, we followed the approach in (Ng et al., 2003). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et"
S07-1054,P00-1056,0,0.00885508,"from parallel texts are useful for WSD. In this evaluation, we gathered training data from 6 English-Chinese parallel corpora (Hong Kong Hansards, Hong Kong News, Hong Kong Laws, Sinorama, Xinhua News, and English translation of Chinese Treebank), available from the Linguistic Data Consortium (LDC). To gather examples from these parallel corpora, we followed the approach in (Ng et al., 2003). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al., 2005). We then made use of the GIZA++ software (Och and Ney, 2000) to perform word alignment on the parallel corpora. Then, we assigned some possible Chinese translations to each sense of an English word w. From the word alignment output of GIZA++, we selected those occurrences of w which were aligned to one of the Chinese translations chosen. The English side of these occurrences served as training data for w, as they were considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. We note that frequently occurring words are usually highly polysemous and hard to disambiguate. To maximize the benefits of using parallel t"
W00-1316,P99-1042,0,0.123473,"e adaptable, robust, flexible, and maintainable. There is no need for a human to manually engineer a set of handcrafted rules and continuously improve or maintain the set of rules. For every question, our QA task requires the computer program to pick a sentence in the associated story as the answer to that question. In our approach, we represent each question-sentence pair as a feature vector. Our goal is to design a feature vector representation such that it provides useful information to a learning algorithm to automatically build five classifiers, one for each question type. In prior work (Hirschman et al., 1999; Charniak et al., 2000; Riloffand Thelen, 2000) the number and type of information sources used for computation is specific to and rlifFerent for each question type. In AQUAREAS, we use the same set of features for all five question types, leaving it to the learning algorithm to identify which are the useful features to test for in each question type. The machine learning approach comprises two steps. First, we design a set of features to capture the information that helps to distin126 guish answer sentences from non-answer sentences. Next, we use a learning algorithm to generate a classifier"
W00-1316,A00-1021,0,0.0258837,"Missing"
W00-1316,W00-0603,0,0.114503,"as an answer to q, and the question q itself. For negative training examples, all other sentences t h a t are not marked as answers to a question q can b e used. In AQUAREAS, we use all other sentences t h a t are marked as answers to the questions other t h a n q in the same story to generate the negative examples for question q. This also helps to keep the ratio of negative examples to positive examples from becoming too high. 3.1 Feature Representation Our feature representation was designed to capture the information sources that prior work (Hirschman et al., 1999; Cha_niak et al., 2000; Riloff and Thelen, 2000) used in their computations or rules. We hypothesize that given equivalent information sources, a machine learning approach can do as well as a system built using handcrafted rules. Our feature vector consists of 20 features. • Diff- f r o m - M a x - W o r d - M a t c h (DMWM) The possible values for this feature are 0, 1, 2, 3, . . . . For a given question q and a sentence s, the value for this feature is computed by first counting the number of matching words present in q and s, where two words match if they have the same morphological root. This gives the raw word m a t c h score m for the"
W00-1316,A00-1023,0,0.026974,"Missing"
W00-1316,voorhees-tice-2000-trec,0,0.052291,"Missing"
W00-1316,W00-0605,0,0.276332,"Missing"
W00-1316,W00-0601,0,\N,Missing
W02-1006,S01-1040,0,\N,Missing
W02-1006,A00-2018,0,\N,Missing
W02-1006,S01-1014,0,\N,Missing
W02-1006,S01-1031,0,\N,Missing
W02-1006,W96-0213,0,\N,Missing
W02-1006,W96-0208,0,\N,Missing
W02-1006,S01-1034,0,\N,Missing
W02-1006,S01-1036,0,\N,Missing
W02-1006,W00-1322,0,\N,Missing
W02-1006,A97-1004,0,\N,Missing
W02-1006,N01-1011,0,\N,Missing
W02-1006,J01-3001,0,\N,Missing
W02-1006,P96-1006,1,\N,Missing
W02-1006,S01-1001,0,\N,Missing
W03-0423,W02-2004,0,0.0201179,"Missing"
W03-0423,C02-1025,1,0.806336,"eering and machine learning approaches. At the last CoNLL in 2002, a common NER task was used to evaluate competing NER systems. In this year’s CoNLL, the NER task is to tag noun phrases with the following four classes: person (PER), organization (ORG), location (LOC), and miscellaneous (MISC). This paper presents a maximum entropy approach to the NER task, where NER not only made use of local context within a sentence, but also made use of other occurrences of each word within the same document to extract useful features (global features). Such global features enhance the performance of NER (Chieu and Ng, 2002b). where o refers to the outcome, h the history (or context), and Z(h) is a normalization function. The features used in the maximum entropy framework are binary. An example of a feature function is  1 if o = org-B, word = PETER fj (h, o) = 0 otherwise The parameters αj are estimated by a procedure called Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972). This is an iterative procedure that improves the estimation of the parameters at each iteration. The maximum entropy classifier is used to classify each word as one of the following: the beginning of a NE (B tag), a word insi"
W03-0423,P02-1060,0,0.0471824,"tial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified. For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1. Name Class of Previous Occurrences The name class of previous occurrences of w is used as a feature, similar to (Zhou and Su, 2002). We use the occurrence where w is part of the longest name class phrase (name class with the most number of tokens). For example, if w is the second token in a person name class phrase of 5 tokens, then a feature 2P erson5 is set to 1. During training, the name classes are known. During testing, the name classes are the ones already assigned to tokens in the sentences already processed. This last feature makes the order of processing important. As HL sentences usually contain less context, they are processed after the other sentences. 3.4 Name List In additional to the above features used by"
W03-0423,M98-1028,0,\N,Missing
W04-0834,A00-2018,0,0.00992332,"esented by one feature that can have many possible feature values (the local collocation strings), whereas each distinct surrounding word is represented by one feature that takes binary values (indicating presence or absence of that word). For example, if is the  word ), bars and suppose the set of collocations for is &apos; a chocolate, the wine, the0 iron ),- ( , then the feature value for collocation in the sentence “Reid saw me looking at the iron bars .” is the iron.                3.4 Syntactic Relations We first parse the sentence containing with a statistical parser (Charniak, 2000). The constituent tree structure generated by Charniak&apos;s parser is then converted into a dependency tree in which every word points to a parent headword. For example, in the sentence “Reid saw me looking at the iron bars .”, the word Reid points to the parent headword saw. Similarly, the word me also points to the parent headword saw. We use different types of syntactic relations, depending on the POS of . If is a noun, we use four features: its parent headword 1 , the POS of 1 , the voice of 1 (active, passive, or 2 if 1 is not a verb), 1(a) attention (noun) 1(b) He turned his attention to th"
W04-0834,W02-0811,0,0.0243423,"mentation of SVM in W EKA (Witten and Frank, 2000), where each nominal feature with possible values is converted into binary (0 or 1) features. If a nominal feature takes the th value, then the th binary feature is set to 1 and all the other binary features are set to 0. The default linear kernel is used. Since SVM only handles binary (2-class) classification, we built one binary classifier for each sense class.       Note that our supervised learning approach made use of a single learning algorithm, without combining multiple learning algorithms as adopted in other research (such as (Florian et al., 2002)). 3 Multiple Knowledge Sources To disambiguate a word occurrence , systems nusels and nusmlst used the first four knowledge sources listed below. System nusmlsts used the English sense given for the target ambiguous word as an additional knowledge source. Previous research (Ng and Lee, 1996; Stevenson and Wilks, 2001; Florian et al., 2002; Lee and Ng, 2002) has shown that a combination of knowledge sources improves WSD accuracy. Our experiments on the provided training data of the SENSEVAL-3 translation and sense subtask also indicated that the additional knowledge source of the English sense"
W04-0834,W02-1006,1,0.839799,"ce language) word . The multilingual lexical sample task is further subdivided into two subtasks: the translation subtask, as well as the translation and sense subtask. The distinction is that for the translation and sense subtask, the English sense of the target ambiguous word is also provided (for both training and test data). In all, we submitted 3 systems: system nusels for the English lexical sample task, system nusmlst for the translation subtask, and system nusmlsts for the translation and sense subtask. All systems were based on the supervised word sense disambiguation (WSD) system of Lee and Ng (2002), and used Support Vector Machines (SVM) learning. Only the training examples provided in the official training corpus were used to train the systems, and no other external resources were used. In particular, we did not use any external dictionary or the sample sentences in the provided dictionary. The knowledge sources used included part-ofspeech (POS) of neighboring words, single words in the surrounding context, local collocations, and syntactic relations, as described in Lee and Ng (2002). For the translation and sense subtask of the multilingual lexical sample task, the English sense give"
W04-0834,P96-1006,1,0.690756,"is used. Since SVM only handles binary (2-class) classification, we built one binary classifier for each sense class.       Note that our supervised learning approach made use of a single learning algorithm, without combining multiple learning algorithms as adopted in other research (such as (Florian et al., 2002)). 3 Multiple Knowledge Sources To disambiguate a word occurrence , systems nusels and nusmlst used the first four knowledge sources listed below. System nusmlsts used the English sense given for the target ambiguous word as an additional knowledge source. Previous research (Ng and Lee, 1996; Stevenson and Wilks, 2001; Florian et al., 2002; Lee and Ng, 2002) has shown that a combination of knowledge sources improves WSD accuracy. Our experiments on the provided training data of the SENSEVAL-3 translation and sense subtask also indicated that the additional knowledge source of the English sense of the target word further improved accuracy (See Section 4.3 for details). We did not attempt feature selection since our previous research (Lee and Ng, 2002) indicated that SVM performs better without feature selection. 3.1 Part-of-Speech (POS) of Neighboring Words We use  7 fe"
W04-0834,W97-0323,1,0.794992,"(positive) offset refers to a token to its left (right). For example, let be the word bars in the sentence “Reid saw me looking ),  at $) the iron bars# is the iron and is iron . , .” Then # where denotes a null token. Like POS, a collocation does not cross sentence boundary. To represent this knowledge source of local collocations, we extracted 11 features - .),-corresponding .),  ), to the ) following  ), collocations: , , , , ,  $), $) /),- 0 ),  $) $)  , , , , , and . This set of 11 features is the union of the collocation features used in Ng and Lee (1996) ) * and Ng (1997). Note that each collocation is represented by one feature that can have many possible feature values (the local collocation strings), whereas each distinct surrounding word is represented by one feature that takes binary values (indicating presence or absence of that word). For example, if is the  word ), bars and suppose the set of collocations for is &apos; a chocolate, the wine, the0 iron ),- ( , then the feature value for collocation in the sentence “Reid saw me looking at the iron bars .” is the iron.                3.4 Syntactic Relations We first parse the sentence co"
W04-0834,W96-0213,0,0.302985,"Missing"
W04-0834,A97-1004,0,0.0331038,"Missing"
W04-0834,J01-3001,0,0.00759345,"M only handles binary (2-class) classification, we built one binary classifier for each sense class.       Note that our supervised learning approach made use of a single learning algorithm, without combining multiple learning algorithms as adopted in other research (such as (Florian et al., 2002)). 3 Multiple Knowledge Sources To disambiguate a word occurrence , systems nusels and nusmlst used the first four knowledge sources listed below. System nusmlsts used the English sense given for the target ambiguous word as an additional knowledge source. Previous research (Ng and Lee, 1996; Stevenson and Wilks, 2001; Florian et al., 2002; Lee and Ng, 2002) has shown that a combination of knowledge sources improves WSD accuracy. Our experiments on the provided training data of the SENSEVAL-3 translation and sense subtask also indicated that the additional knowledge source of the English sense of the target word further improved accuracy (See Section 4.3 for details). We did not attempt feature selection since our previous research (Lee and Ng, 2002) indicated that SVM performs better without feature selection. 3.1 Part-of-Speech (POS) of Neighboring Words We use  7 features  to encod"
W04-0834,W00-0726,0,0.109143,"Missing"
W04-3236,E03-1081,0,0.00994881,"rs in the word to help predict the correct POS tag is a good heuristic. One-at-a-time or all-at-once? The all-at-once approach, which considers all aspects of available information in an integrated, unified framework, can make better informed decisions, but incurs a higher computational cost. compared with the all-at-once, character-based approach previously proposed. 7 9 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to systematically investigate issues of processing architecture and feature representation for Chinese POS tagging. Ou"
W04-3236,W03-1025,0,0.585398,"ly to Chinese in the maximum entropy framework. Language differences between Chinese and English have no doubt made the direct porting of an English POS tagging method to Chinese ineffective. One-at-a-Time, Tagger POS Since one-at-a-time, word-based POS tagging did not yield good accuracy, we proceeded to investigate other combinations of processing architecture and feature representation. We observed that character features were successfully used to build our word segmenter and that of (Xue and Shen, 2003). Similarly, character features were used to build a maximum entropy Chinese parser by (Luo, 2003), where his parser could perform word segmentation, POS tagging, and parsing in an integrated, unified approach. We hypothesized that assigning POS tags on a character-by-character basis, making use of character features in the surrounding context may yield good accuracy. So we next investigate such a one-at-a-time, character-based POS tagger. 4.1 89 88 87 86 85 84 83 82 81 80 79 Character-Based Features The features that were used for our word segmenter ((a) − (f)) in Section 2.1 were yet again applied, with two additional features (g) and (h) to aid POS tag prediction. 1 2 3 4 5 6 7 8 9 10 E"
W04-3236,W96-0213,0,0.230697,"rds (similar to feature (f) of the word segmenter in Section 2.1). Four type classes are defined: a word is of class 1 if it is a number; class 2 if the word is made up of only numeric characters followed by “日”, “月”，or “年”; class 3 when the word is made up of only English characters and optionally punctuation characters; class 4 otherwise. (a) W n ( n = −2 ,−1,0 ,1,2 ) (b) WnWn +1 ( n = −2 ,−1,0 ,1 ) (c) W−1W1 (d) Pu( W0 ) (e) T ( W− 2 )T ( W−1 )T ( W0 )T ( W1 )T ( W2 ) (f) POS ( W−1 ) (g) POS ( W− 2 )POS ( W−1 ) 3.2 4 Testing The testing procedure is similar to the beam search algorithm of (Ratnaparkhi, 1996), which tags each word one by one and maintains, as it sees a new word, the N most probable POS tag sequence candidates up to that point in the sentence. For our experiment, we have chosen N to be 3. 3.3 Experimental Results The 250K-word CTB corpus, tagged with 32 different POS tags (such as “NR”, “PU”, etc) was employed in our evaluation of POS taggers in this study. We ran 10-fold CV on the CTB corpus, using our word segmenter’s output for each of the 10 runs as the input sentences to the POS tagger. POS tagging accuracy is simply calculated as (number of characters assigned correct POS tag"
W04-3236,J96-3004,0,0.144582,"words is much higher than the OOV rate for Chinese characters, in the presence of an unknown word, using the component characters in the word to help predict the correct POS tag is a good heuristic. One-at-a-time or all-at-once? The all-at-once approach, which considers all aspects of available information in an integrated, unified framework, can make better informed decisions, but incurs a higher computational cost. compared with the all-at-once, character-based approach previously proposed. 7 9 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work i"
W04-3236,W03-1719,0,0.311037,"cters of these consecutive words is changed so that they are segmented as a single word. To illustrate, if the concatenation of 2 consecutive words “巴赛 罗纳” in the segmented output 97.0 96.5 96.0 95.5 95.0 94.5 94.0 93.5 1 2 3 4 5 6 7 8 9 10 Experim ent Num ber Figure 1: CTB 10-fold CV word segmentation Fmeasure for our word segmenter As further evaluation, we tested our word segmenter on all the 4 test corpora (CTB, Academia Sinica (AS), Hong Kong CityU (HK), and Peking University (PK)) of the closed track of the 2003 ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff (Sproat and Emerson, 2003). For each of the 4 corpora, we trained our word segmenter on only the official released training data of that corpus. Training was conducted with feature cutoff of 2 and 100 iterations (these parameters were obtained by cross validation on the training set), except for the AS corpus where we used cutoff 3 since the AS training corpus was too big to train with cutoff 2. Figure 2 shows our word segmenter’s Fmeasure (based on the official word segmentation scorer of 2003 SIGHAN bakeoff) compared to those reported by all the 2003 SIGHAN participants in the four closed tracks (ASc, HKc, PKc, CTBc)"
W04-3236,J00-3004,0,0.0080612,"than the OOV rate for Chinese characters, in the presence of an unknown word, using the component characters in the word to help predict the correct POS tag is a good heuristic. One-at-a-time or all-at-once? The all-at-once approach, which considers all aspects of available information in an integrated, unified framework, can make better informed decisions, but incurs a higher computational cost. compared with the all-at-once, character-based approach previously proposed. 7 9 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to system"
W04-3236,xia-etal-2000-developing,0,0.0359366,"possible that the classifier produces a sequence of invalid tags (e.g., “m” followed by “s”). To eliminate such possibilities, we implemented a dynamic programming algorithm which considers only valid boundary tag sequences given an input character sequence. At each character position i, the algorithm considers each last word candidate document matches another word “巴赛罗纳”, then “巴赛 罗纳” will be re-segmented as “巴赛罗纳”. 2.4 Word Segmenter Experimental Results To evaluate the accuracy of our word segmenter, we carried out 10-fold cross validation (CV) on the 250K-word Penn Chinese Treebank (CTB) (Xia et al., 2000) version 3.0. The Java opennlp maximum entropy package from sourceforge1 was used in our implementation, and training was done with a feature cutoff of 2 and 100 iterations. The accuracy of word segmentation is measured by recall (R), precision (P), and Fmeasure ( 2 RP /( R + P ) ). Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter’s output. Figure 1 gives the word segmentation Fmeasure of our word segmenter based on 10-fold CV on the 250K-word CTB. Our word segmenter achieves"
W04-3236,W03-1728,0,0.470001,"To our knowledge, our work is the first to systematically investigate such issues in Chinese POS tagging. 2 Word Segmentation As a first step in our investigation, we built a Chinese word segmenter capable of performing word segmentation without using POS tag information. Since errors in word segmentation will propagate to the subsequent POS tagging phase in the one-at-a-time approach, in order for our study to give relevant findings, it is important that the word segmenter we use gives state-ofthe-art accuracy. The word segmenter we built is similar to the maximum entropy word segmenter of (Xue and Shen, 2003). Our word segmenter uses a maximum entropy framework and is trained on manually segmented sentences. It classifies each Chinese character given the features derived from its surrounding context. Each character can be assigned one of 4 possible boundary tags: “b” for a character that begins a word and is followed by another character, “m” for a character that occurs in the middle of a word, “e” for a character that ends a word, and “s” for a character that occurs as a single-character word. 2.1 Word Segmenter Features Besides implementing a subset of the features described in (Xue and Shen, 20"
W04-3236,W03-1730,0,0.122342,"Missing"
W04-3236,W03-1711,0,0.0438378,"ble information in an integrated, unified framework, can make better informed decisions, but incurs a higher computational cost. compared with the all-at-once, character-based approach previously proposed. 7 9 Related Work Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003). Relatively less work has been done on Chinese POS tagging. Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks. Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model. Jing et al. (2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches. To our knowledge, our work is the first to systematically investigate issues of processing architecture and feature representation for Chinese POS tagging. Our maximum entropy word segmenter is similar to that of (Xue and Shen, 2003), but the additional features we used and the postprocessing step gave improved word se"
W04-3236,W03-1026,0,\N,Missing
W04-3236,W03-1726,0,\N,Missing
W06-1617,J05-1004,0,0.159775,"o our baseline system. Our implemented SRL system and experiments are based on the September 2005 release of NomBank (NomBank.0.8). The rest of this paper is organized as follows: Section 2 gives an overview of NomBank, Section 3 introduces the Maximum Entropy classification model, Section 4 introduces our features and feature selection strategy, Section 5 explains the experimental setup and presents the experimental results, Section 6 compares NomBank SRL to Introduction Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. 138 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138–145, c"
W06-1617,N04-4036,0,0.182296,"Missing"
W06-1617,J96-1002,0,0.00406253,"roblem and divide it into two phases: argument identification and argument classification. During the argument identification phase, each parse tree node is marked as either argument or non-argument. Each node marked as argument is then labeled with a specific class during the argument classification phase. The identification model is a binary classifier , while the classification model is a multi-class classifier. Opennlp maxent1 , an implementation of Maximum Entropy (ME) modeling, is used as the classification tool. Since its introduction to the Natural Language Processing (NLP) community (Berger et al., 1996), ME-based classifiers have been shown to be effective in various NLP tasks. ME modeling is based on the insight that the best model is consistent with the set of constraints imposed and otherwise as uniform as possible. ME models the probability of label l given input x as in Equation 1. fi (l, x) is a feature function that maps label l and input x to either 0 or 1, while the summation is over all n feature functions and with λi as the weight parameter for each feature function fi (l, x). Zx is a normalization factor. In the identification model, label l corresponds to either “argument” or “n"
W06-1617,W05-0620,0,0.0314278,"ts are based on the September 2005 release of NomBank (NomBank.0.8). The rest of this paper is organized as follows: Section 2 gives an overview of NomBank, Section 3 introduces the Maximum Entropy classification model, Section 4 introduces our features and feature selection strategy, Section 5 explains the experimental setup and presents the experimental results, Section 6 compares NomBank SRL to Introduction Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. 138 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138–145, c Sydney, July 2006. 2006 Association for Computational Linguistics  N"
W06-1617,W05-0302,0,0.065286,"Missing"
W06-1617,P05-1022,0,0.0952385,"Missing"
W06-1617,P05-1073,0,0.261166,"Missing"
W06-1617,W04-3212,0,0.583338,"by other arguments, so that when argument domination occurs, only the argument with the largest word span is kept. We do not perform similar pruning on the test data. 4 Features and feature selection Input p{syntactic parse tree} Input m{argument identification model, assigns each constituent in the parse tree log likelihood of being a semantic argument} Output score{maximum log likelihood of the parse tree p with arguments identified using model m} 4.1 Baseline NomBank SRL features Table 1 lists the baseline features we adapted from previous PropBank-based SRL systems (Pradhan et al., 2005; Xue and Palmer, 2004). For ease of description, related features are grouped, with MLParse(p, m) a specific individual feature given individual refif parse p is a leaf node then return max(Score(p, m, ARG), Score(p, m, N ON E)) erence name. For example, feature b11FW in else the group b11 denotes the first word spanned by M Lscore = 0 for each node ci in Children(p) do the constituent and b13LH denotes the left sisM Lscore += M LP arse(ci , m) ter’s head word. We also experimented with variend for ous feature combinations, inspired by the features N ON Escore = 0 for each node ci in Children(p) do used in (Xue and"
W06-1617,W04-0803,0,0.0306788,"r 2005 release of NomBank (NomBank.0.8). The rest of this paper is organized as follows: Section 2 gives an overview of NomBank, Section 3 introduces the Maximum Entropy classification model, Section 4 introduces our features and feature selection strategy, Section 5 explains the experimental setup and presents the experimental results, Section 6 compares NomBank SRL to Introduction Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that “Ben Bernanke” is the subject (labeled as ARG0 in PropBank) of predicate “replace”, and “Greenspan” is the object (labeled as ARG1): • Ben Bernanke replaced Greenspan as Fed chair. • Greenspan was replaced by Ben Bernanke as Fed chair. 138 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138–145, c Sydney, July 2006. 2006 Association for Computational Linguistics  NP   S H  H (A"
W06-1617,meyers-etal-2004-cross,0,0.0163779,"weight parameter for each feature function fi (l, x). Zx is a normalization factor. In the identification model, label l corresponds to either “argument” or “non-argument”, and in the classification model, label l corresponds to one of the specific NomBank argument classes. The classification output is the label l with the highest conditional probability p(l|x). H NN (ARG1) predicate PP  P Greenspan ’s replacement Figure 1: A sample sentence and its parse tree labeled in the style of NomBank PropBank SRL and discusses possible future research directions. 2 Overview of NomBank The NomBank (Meyers et al., 2004c; Meyers et al., 2004b) annotation project originated from the NOMLEX (Macleod et al., 1997; Macleod et al., 1998) nominalization lexicon developed under the New York University Proteus Project. NOMLEX lists 1,000 nominalizations and the correspondences between their arguments and the arguments of their verb counterparts. NomBank frames combine various lexical resources (Meyers et al., 2004a), including an extended NOMLEX and PropBank frames, and form the basis for annotating the argument structures of common nouns. Similar to PropBank, NomBank annotation is made on the Penn TreeBank II (PTB"
W06-1617,meyers-etal-2004-annotating,0,0.110528,"weight parameter for each feature function fi (l, x). Zx is a normalization factor. In the identification model, label l corresponds to either “argument” or “non-argument”, and in the classification model, label l corresponds to one of the specific NomBank argument classes. The classification output is the label l with the highest conditional probability p(l|x). H NN (ARG1) predicate PP  P Greenspan ’s replacement Figure 1: A sample sentence and its parse tree labeled in the style of NomBank PropBank SRL and discusses possible future research directions. 2 Overview of NomBank The NomBank (Meyers et al., 2004c; Meyers et al., 2004b) annotation project originated from the NOMLEX (Macleod et al., 1997; Macleod et al., 1998) nominalization lexicon developed under the New York University Proteus Project. NOMLEX lists 1,000 nominalizations and the correspondences between their arguments and the arguments of their verb counterparts. NomBank frames combine various lexical resources (Meyers et al., 2004a), including an extended NOMLEX and PropBank frames, and form the basis for annotating the argument structures of common nouns. Similar to PropBank, NomBank annotation is made on the Penn TreeBank II (PTB"
W06-1617,W04-2705,0,0.0886293,"weight parameter for each feature function fi (l, x). Zx is a normalization factor. In the identification model, label l corresponds to either “argument” or “non-argument”, and in the classification model, label l corresponds to one of the specific NomBank argument classes. The classification output is the label l with the highest conditional probability p(l|x). H NN (ARG1) predicate PP  P Greenspan ’s replacement Figure 1: A sample sentence and its parse tree labeled in the style of NomBank PropBank SRL and discusses possible future research directions. 2 Overview of NomBank The NomBank (Meyers et al., 2004c; Meyers et al., 2004b) annotation project originated from the NOMLEX (Macleod et al., 1997; Macleod et al., 1998) nominalization lexicon developed under the New York University Proteus Project. NOMLEX lists 1,000 nominalizations and the correspondences between their arguments and the arguments of their verb counterparts. NomBank frames combine various lexical resources (Meyers et al., 2004a), including an extended NOMLEX and PropBank frames, and form the basis for annotating the argument structures of common nouns. Similar to PropBank, NomBank annotation is made on the Penn TreeBank II (PTB"
W09-0412,N03-2016,0,0.464734,"up with seven parameters for each entry in the merged phrase table. Merging Two Lexicalized Reordering Tables. When building the two phrase tables, we also built two lexicalized reordering tables (Koehn et al., 2005) for them, Rnews and Reuro , which we merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Mela"
W09-0412,N01-1020,0,0.0624677,"st kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length"
W09-0412,P07-1083,0,0.0541762,"e merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments"
W09-0412,W95-0115,0,0.249911,"003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Oc"
W09-0412,J99-1003,0,0.0358046,"Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and"
W09-0412,J93-2003,0,0.0101214,"s that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distanc"
W09-0412,J00-2004,0,0.0976046,"ining News Commentary bi-text) and an out-ofdomain one (trained on the provided monolingual Spanish Europarl data). For both LMs, we used 5-gram models with Kneser-Ney smoothing. Merging Two Phrase Tables. Following Nakov (2008), we trained and merged two phrasebased SMT systems: a small in-domain one using the News Commentary bi-text, and a large out-ofLCSR (s1 , s2 ) = |LCS(s1 ,s2 )| max(|s1 |,|s2 |) where LCS(s1 , s2 ) is the longest common subsequence of s1 and s2 , and |s |is the length of s. Following Nakov et al. (2007), we combined the LCSR similarity measure with competitive linking (Melamed, 2000) in order to extract potential cog76 nates from the training bi-text. Competitive linking assumes that, given a source English sentence and its Spanish translation, a source word is either translated with a single target word or is not translated at all. Given an English-Spanish sentence pair, we calculated LCSR for all cross-lingual word pairs (excluding stopwords and words of length 3 or less), which induced a fully-connected weighted bipartite graph. Then, we performed a greedy approximation to the maximum weighted bipartite matching in that graph (competitive linking) as follows: First, we"
W09-0412,W07-0718,0,0.0553417,"Missing"
W09-0412,W08-0309,0,0.0509475,"Missing"
W09-0412,W06-3114,0,0.0532889,"on at the shared task. 1 Introduction Modern Statistical Machine Translation (SMT) systems are typically trained on sentence-aligned parallel texts (bi-texts) from a particular domain. When tested on text from that domain, they demonstrate state-of-the art performance, but on out-of-domain test data the results can deteriorate significantly. For example, on the WMT06 Shared Translation Task, the scores for French-to-English translation dropped from about 30 to about 20 Bleu points for nearly all systems when tested on News Commentary instead of the Europarl1 text, which was used for training (Koehn and Monz, 2006). 1 2 The NUS System Below we describe separately the standard and the nonstandard settings of our system. 2.1 Standard Settings In our baseline experiments, we used the following general setup: First, we tokenized the par2 The task organizers invited submissions translating forward and/or backward between English and five other European languages (French, Spanish, German, Czech and Hungarian), but we only participated in English→Spanish, due to time limitations. See (Koehn, 2005) for details about the Europarl corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pag"
W09-0412,W08-0320,1,0.884628,"Missing"
W09-0412,2005.iwslt-1.8,0,0.0516195,"tion probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We further added two new features, Fnews and Feuro , which show the source of each phrase. Their values are 1 and 0.5 when the phrase was extracted from the News Commentary bi-text, 0.5 and 1 when it was extracted from the Europarl bi-text, and 1 and 1 when it was extracted from both. As a result, we ended up with seven parameters for each entry in the merged phrase table. Merging Two Lexicalized Reordering Tables. When building the two phrase tables, we also built two lexicalized reordering tables (Koehn et al., 2005) for them, Rnews and Reuro , which we merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous resear"
W09-0412,J03-1002,0,0.00684256,"cted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from th"
W09-0412,J04-4002,0,0.0932008,"5), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate trai"
W09-0412,P03-1021,0,0.0163364,"ained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate training (MERT) (Och, 2003) on the tuning part of the development set (dev-test2009a). We used these weights in a beam search decoder (Koehn et al., 2007) to translate the test sentences (the English part of dev-test2009b, tokenized and lowercased). We then recased the output using a monotone model that translates from lowercase to uppercase Spanish, we post-cased it using a simple heuristic, de-tokenized the result, and compared it to the gold standard (the Spanish part of dev-test2009b) using Bleu and NIST. 2.2 Nonstandard Settings The nonstandard features of our system can be summarized as follows: Two Language Model"
W09-0412,P02-1040,0,0.082241,"h 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate training (MERT) (Och, 2003) on the tuning part of the development set (dev-test2009a). We used these weights in a beam search decoder (Koehn et al., 2007) to translate the test sentences (the English part of dev-test2009b, tokenized and lowercased). We then recased the output using a monotone model that translates from lowercase to uppercase Spanish, we post-cased it using a simple heuristic, de-tokenized the result, and compared it to the gold standard (the Spanish part of dev-test2009b) using Bleu and NIST. 2.2 Nonstandard Settings The nonstandard features"
W09-0412,2005.mtsummit-papers.11,0,0.0734173,"Missing"
W09-0412,W07-0730,1,\N,Missing
W09-0412,P07-2045,0,\N,Missing
W10-1754,P08-1007,1,0.461524,". 1 Introduction In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources. Heavyweight linguistic approaches including RTE (Pado et al., 2009) and ULC (Giménez and Màrquez, 2008) performed the best in the WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic approaches include BLEU (Papineni et al., 2002) and its variants, TER (Snover et al., 2006), among others. They operate purely at the surface word level and no linguistic resources are required. Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches. 2 Overview We consider the task of ev"
W10-1754,W08-0332,0,0.0191881,"tion metrics with state-of-the-art performances. TESLA-M builds on the success of METEOR and MaxSim, but employs a more expressive linear programming framework. TESLA further exploits parallel texts to build a shallow semantic representation. We evaluate both on the WMT 2009 shared evaluation task and show that they outperform all participating systems in most tasks. 1 Introduction In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources. Heavyweight linguistic approaches including RTE (Pado et al., 2009) and ULC (Giménez and Màrquez, 2008) performed the best in the WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic a"
W10-1754,P09-1104,0,0.0151676,"nd a system translation is the arithmetic average of the BTNG F-measures for unigrams, bigrams, and trigrams based on similarity functions sms and spos . We thus have 3 × 2 = 6 features for TESLA-M. We can compute a system-level score for a machine translation system by averaging its sentencelevel scores over the complete test set. 3.4 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree o"
W10-1754,N06-1014,0,0.00672045,"re for a reference and a system translation is the arithmetic average of the BTNG F-measures for unigrams, bigrams, and trigrams based on similarity functions sms and spos . We thus have 3 × 2 = 6 features for TESLA-M. We can compute a system-level score for a machine translation system by averaging its sentencelevel scores over the complete test set. 3.4 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is"
W10-1754,J03-1002,0,0.00211452,"l 4.1 Scoring The TESLA-M sentence-level score for a reference and a system translation is the arithmetic average of the BTNG F-measures for unigrams, bigrams, and trigrams based on similarity functions sms and spos . We thus have 3 × 2 = 6 features for TESLA-M. We can compute a system-level score for a machine translation system by averaging its sentencelevel scores over the complete test set. 3.4 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the w"
W10-1754,P09-1034,0,0.0279411,"c machine translation evaluation metrics with state-of-the-art performances. TESLA-M builds on the success of METEOR and MaxSim, but employs a more expressive linear programming framework. TESLA further exploits parallel texts to build a shallow semantic representation. We evaluate both on the WMT 2009 shared evaluation task and show that they outperform all participating systems in most tasks. 1 Introduction In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources. Heavyweight linguistic approaches including RTE (Pado et al., 2009) and ULC (Giménez and Màrquez, 2008) performed the best in the WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and sy"
W10-1754,P02-1040,0,0.104184,"he WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic approaches include BLEU (Papineni et al., 2002) and its variants, TER (Snover et al., 2006), among others. They operate purely at the surface word level and no linguistic resources are required. Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches. 2 Overview We consider the task of evaluating machine translation systems in the direction of translating the source language to the target language. Given a reference translation and a system translation, the 354 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 354–359, c Up"
W10-1754,atserias-etal-2006-freeling,0,0.00827873,"nt across all translation tracks. 5.2 TESLA TESLA-M ulc maxsim meteor-0.6 Table 1 compares the scores of TESLA and TESLA-M against three participants in WMT 2009 under identical settings6 : ULC (a heavyweight linguistic approach with the best performance in WMT 2009), MaxSim, and METEOR. The results show that TESLA outperforms all these systems by a substantial margin, and TESLA-M is very competitive too. We POS tag and lemmatize the texts using the following tools: for English, OpenNLP POS-tagger3 and WordNet lemmatizer; for French and German, TreeTagger4 ; for Spanish, the FreeLing toolkit (Atserias et al., 2006); and for Czech, the Morce morphological tagger5 . For German, we additionally perform noun compound splitting. For each noun, we choose the split that maximizes the geometric mean of the frequency counts of its parts, following the method in (Koehn and Knight, 2003): max n,p1 ,p2 ,...,pn n Y 5.4 Out-of-English task A synonym dictionary is required for target languages other than English. We use the freely available Wiktionary dictionary7 for each language. For Spanish, we additionally use the Spanish WordNet, a component of FreeLing. Only one pivot language (English) is used for the BPNG. For"
W10-1754,W09-0402,0,0.0566379,"Missing"
W10-1754,W05-0909,0,0.033013,"participating systems in most tasks. 1 Introduction In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources. Heavyweight linguistic approaches including RTE (Pado et al., 2009) and ULC (Giménez and Màrquez, 2008) performed the best in the WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic approaches include BLEU (Papineni et al., 2002) and its variants, TER (Snover et al., 2006), among others. They operate purely at the surface word level and no linguistic resources are required. Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches. 2 Overvie"
W10-1754,2006.amta-papers.25,0,0.124211,"it an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic approaches include BLEU (Papineni et al., 2002) and its variants, TER (Snover et al., 2006), among others. They operate purely at the surface word level and no linguistic resources are required. Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches. 2 Overview We consider the task of evaluating machine translation systems in the direction of translating the source language to the target language. Given a reference translation and a system translation, the 354 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 354–359, c Uppsala, Sweden, 15-16 July 2010. 2010 Associa"
W10-1754,W09-0441,0,0.0522378,"e and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. For example, at the word level, In this work, we set α = 0.8, following MaxSim. The value gives more importance to the recall than the precision. 3.3 TESLA Reduction W When every xW i and yj is 1, the linear programming problem proposed above reduces to weighted bipartite matching. This is a well known result; see for example, Cormen"
W10-1754,P05-1074,0,0.0303067,"tion Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. For example, at the word level, In this work, we set α = 0.8, following MaxSim. The value gives more importance to the recall than the precision. 3.3 TESLA Reduction W When every xW i and yj is 1, the linear programming problem proposed above reduces to weighted bipartite matching. T"
W10-1754,N06-1003,0,0.0194912,"xt between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. For example, at the word level, In this work, we set α = 0.8, following MaxSim. The value gives more importance to the recall than the precision. 3.3 TESLA Reduction W When every xW i and yj is 1, the linear programming problem proposed above reduces to weighted bipartite matching. This is a well known result; s"
W10-1754,E03-1076,0,\N,Missing
W10-1754,W09-0401,0,\N,Missing
W10-1754,W07-0734,0,\N,Missing
W10-1754,N03-1017,0,\N,Missing
W11-2106,P05-1074,0,0.0295896,"tion Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. 80 3.2 Segmenting a sentence into phrases To extend the concept of this semantic representation of phrases to sentences, we segment a sentence in the target language into phrases. Given a phrase table, we can approximate the probability of a phrase p by: N (p) (1) P r(p) = P 0 p0 N ("
W11-2106,N06-1003,0,0.0278032,"xt between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. 80 3.2 Segmenting a sentence into phrases To extend the concept of this semantic representation of phrases to sentences, we segment a sentence in the target language into phrases. Given a phrase table, we can approximate the probability of a phrase p by: N (p) (1) P r(p) = P 0 p0 N (p ) where N (·) is the count"
W11-2106,P08-1007,1,0.867284,"ust the counts of the respective N-grams. However, to emphasize the content words over the function words, we discount the weight of an N-gram by a factor of 0.1 for every function word in the N-gram. We decide whether a word is a function word based on its POS tag. In TESLA-M, the BNGs are extracted in the target language, so we call them bags of target language N-grams (BTNGs). 2.1 Similarity functions To match two BNGs, we first need a similarity measure between N-grams. In this section, we define the similarity measures used in our experiments. We adopt the similarity measure from MaxSim (Chan and Ng, 2008; Chan and Ng, 2009) as sms . For unigrams x and y, 78 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 78–84, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics • If lemma(x) = lemma(y), then sms = 1. Good morning morning , , sir sir . w=1.0 w=0.1 w=0.1 w=0.1 • Otherwise, let s=0.4 s=0.1 a = I(synsets(x) overlap with synsets(y)) b = I(POS(x) = POS(y)) where I(·) is the indicator function, then sms = (a + b)/2. The synsets are obtained by querying WordNet (Fellbaum, 1998). For languages other than English, a synonym dictionary i"
W11-2106,P09-1104,0,0.0245758,"f two types of Fmeasures: (1) BTNG F-measures as in TESLA-M and (2) F-measures between bags of N-grams in one or more pivot languages, called bags of pivot language N-grams (BPNGs), The rest of this section focuses on the generation of the BPNGs. Their matching is done in the same way as described for BTNGs in the previous section. 3.1 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree o"
W11-2106,N06-1014,0,0.043265,"B uses the average of two types of Fmeasures: (1) BTNG F-measures as in TESLA-M and (2) F-measures between bags of N-grams in one or more pivot languages, called bags of pivot language N-grams (BPNGs), The rest of this section focuses on the generation of the BPNGs. Their matching is done in the same way as described for BTNGs in the previous section. 3.1 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is"
W11-2106,W10-1754,1,0.907191,"Metric Daniel Dahlmeier1 and Chang Liu2 and Hwee Tou Ng1,2 1 NUS Graduate School for Integrative Sciences and Engineering 2 Department of Computer Science, National University of Singapore {danielhe,liuchan1,nght}@comp.nus.edu.sg Abstract 2 This paper describes the submission from the National University of Singapore to the WMT 2011 Shared Evaluation Task and the Tunable Metric Task. Our entry is TESLA in three different configurations: TESLA-M, TESLA-F, and the new TESLA-B. 1 Introduction TESLA (Translation Evaluation of Sentences with Linear-programming-based Analysis) was first proposed in Liu et al. (2010). The simplest variant, TESLA-M (M stands for minimal), is based on Ngram matching, and utilizes light-weight linguistic analysis including lemmatization, part-of-speech tagging, and WordNet synonym relations. TESLAB (B stands for basic) additionally takes advantage of bilingual phrase tables to model phrase synonyms. It is a new configuration proposed in this paper. The most sophisticated configuration TESLA-F (F stands for full) additionally uses language models and a ranking support vector machine instead of simple averaging. TESLA-F was called TESLA in Liu et al. (2010). In this paper, we"
W11-2106,D11-1035,1,0.894125,"Missing"
W11-2106,J03-1002,0,0.00516676,"over the complete test set. 3 TESLA-B TESLA-B uses the average of two types of Fmeasures: (1) BTNG F-measures as in TESLA-M and (2) F-measures between bags of N-grams in one or more pivot languages, called bags of pivot language N-grams (BPNGs), The rest of this section focuses on the generation of the BPNGs. Their matching is done in the same way as described for BTNGs in the previous section. 3.1 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the w"
W11-2106,P02-1040,0,0.100539,"F wpbleu en-fr 0.64 0.65 0.68 0.66 0.60 en-de 0.59 0.59 0.57 0.60 0.47 en-es 0.59 0.60 0.60 0.61 0.49 Overall 0.60 0.61 0.61 0.61 0.51 Table 3: Out-of-English sentence-level consistency on WMT 2009 data TESLA-M TESLA-B TESLA-F wpF wpbleu en-fr 0.93 0.91 0.85 0.90 0.92 en-de 0.86 0.05 0.78 -0.06 0.07 en-es 0.79 0.63 0.67 0.58 0.63 TuneTest BLEU TESLA-M TESLA-B TESLA-F Avg 0.86 0.53 0.77 0.47 0.54 Table 4: Out-of-English system-level Spearman’s rank correlation on WMT 2009 data 6.2 Tunable Metric Task The goal of the new tunable metric task is to explore MT tuning with metrics other than BLEU (Papineni et al., 2002). To allow for a fair comparison, the WMT organizers provided participants with a complete Joshua MT system for an Urdu-English translation task. We tuned models for each variant of TESLA, using Z-MERT in the default configuration provided by the organizers. There are four reference translations for each Urdu source sentence. The size of the N-best list is set to 300. For our own experiments, we randomly split the development set into a development portion (781 sentences) and a held-out test portion (200 sentences). We run the same Z-MERT tuning process for each TESLA variant on this reduced d"
W11-2106,W09-0441,0,0.0259126,"e and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. 80 3.2 Segmenting a sentence into phrases To extend the concept of this semantic representation of phrases to sentences, we segment a sentence in the target language into phrases. Given a phrase table, we can approximate the probability of a phrase p by: N (p) (1) P r(p) = P 0 p0 N (p ) where N (·) is the count of a phrase in the phr"
W11-2841,P11-1092,1,0.836563,"tion for Computational Linguistics Each input sentence is tagged with part-ofspeech (POS) tags and syntactic chunks. We use OpenNLP2 for POS tagging and YamCha (Kudo and Matsumoto, 2003) for chunking. For each noun phrase (NP), the system extracts a feature vector representation. We use the features proposed in (Han et al., 2006) which include the words before, in, and after the NP, the head word, POS tags, etc. A multiclass classifier then predicts the most likely article for the NP. We employ a linear classifier trained with empirical risk minimization on NP instances from well-edited text (Dahlmeier and Ng, 2011). The features are only extracted from the surrounding context of the article and do not include the article itself, which would be fully predictive of the class. During testing, a correction is proposed if the predicted article is not the same as the observed article used by the writer, and the difference between the confidence score for the predicted article and the confidence score for the observed article is larger than a threshold. Finally, we filter the corrections using a large language model and only keep corrections that strictly increase the normalized language model score of the sen"
W11-2841,W10-4236,0,0.310799,"Missing"
W11-2841,P03-1004,0,0.113264,"lity. 2.2 Article Errors Article error correction is treated as a multi-class classification problem. The possible classes are the articles a, the, and the empty article. The article an is normalized as a and restored later using a rule-based heuristic. 1 257 Spelling Correction http://aspell.net Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 257–259, c Nancy, France, September 2011. 2011 Association for Computational Linguistics Each input sentence is tagged with part-ofspeech (POS) tags and syntactic chunks. We use OpenNLP2 for POS tagging and YamCha (Kudo and Matsumoto, 2003) for chunking. For each noun phrase (NP), the system extracts a feature vector representation. We use the features proposed in (Han et al., 2006) which include the words before, in, and after the NP, the head word, POS tags, etc. A multiclass classifier then predicts the most likely article for the NP. We employ a linear classifier trained with empirical risk minimization on NP instances from well-edited text (Dahlmeier and Ng, 2011). The features are only extracted from the surrounding context of the article and do not include the article itself, which would be fully predictive of the class."
W11-2841,P07-1065,0,0.0714046,"journal. In both cases, we filter out section headings, references, tables, etc. The W EB 1T 5GRAM CORPUS (Brants and Franz, 2006) is used for language modeling. Table 1 gives an overview of the data sets. 4 Experiments and Results This section reports experimental results of our system on the HOO- HELDOUT and the HOO- TEST data set. The parameters of the system are as follows. The minimum length for spelling correction is four characters. The language model filter for article and preposition correction uses a 5-gram language model built from the complete W EB 1T 5- GRAM CORPUS using RandLM (Talbot and Osborne, 2007). For spelling correction, the language model filter is built from the ACL- ANTHOLOGY data set. The linear classifiers for article and preposition correction are trained on the CL- JOURNAL data set. Threshold parameters are tuned on HOOTUNE when testing on HOO- HELDOUT , and on the complete HOO development data when testing on HOO- TEST. 4.1 Evaluation We report micro-averaged detection, recognition, and correction F1 scores as defined in the HOO overview paper. The scores are computed over the entire test collection. For individual error categories, the HOO overview paper only reports the “pe"
W11-2841,C08-1109,0,0.0542311,"ly increase the normalized language model score of the sentence. 2.3 Preposition Errors Preposition error correction follows the same strategy of multi-class classification and language model filtering. The system only corrects preposition substitution errors, not preposition insertion or deletion errors. The possible classes are the prepositions about, among, at, by, for, in, into, of, on, to, and with. For each prepositional phrase (PP) which is headed by one of these prepositions, a linear classifier predicts the most likely preposition from the above list. We use the features proposed by (Tetreault and Chodorow, 2008). Again, we apply a threshold to bias the classifier towards the observed preposition and filter corrections with a large language model. 3 Data Sets We randomly split the files in the HOO development data into a tuning set HOO- TUNE (9 files) and a held-out test set HOO- HELDOUT (10 files). The official HOO test data HOO- TEST is completely unobserved during development. We cre2 http://opennlp.sourceforge.net 258 Data Set HOO- TUNE HOO- HELDOUT HOO- TEST ACL- ANTHOLOGY CL- JOURNAL Sentences 477 462 722 708,129 22,934 Tokens 12,115 10,691 18,789 18,020,431 611,334 Table 1: Overview of the data"
W12-2025,P10-1089,0,0.0242622,"eposition as an additional feature for determiner and replacement preposition correction. The features described so far are all binaryvalued, i.e., they indicate whether some feature is present in the input or not. Additionally, we can construct real-valued features by counting the log frequency of surface N-grams on the web or in a web-scale corpus (Bergsma et al., 2009). Web-scale N-gram count features can harness the power of the web in connection with supervised classification and have successfully been used for a number of NLP generation and disambiguation problems (Bergsma et al., 2009; Bergsma et al., 2010), although we are not aware of any previous application in grammatical error correction. Web-scale N-gram count features usually use N-grams of consecutive tokens. The release of web-scale parsed corpora like the WaCky project (Baroni et al., 2009) makes it possible to extend the idea to dependency N-grams of child-parent tuples over the dependency arcs in the dependency parse tree, e.g., {(child, node), (node, parent)} for bigrams, {(child’s child, child, node), (child, node, parent), (node, parent, parent’s parent)} for trigrams. We collect log frequency counts for dependency N-grams from a"
W12-2025,D09-1052,0,0.0579842,"ntences (part-of-speech (POS) tagging, chunking, and parsing) and identifies relevant instances for correction (e.g., all noun phrases (NP) for determiner correction). Each instance is mapped to a real-valued feature vector. Next, a classifier predicts the most likely correction for each feature vector. Finally, the proposed corrections are filtered using a language model and only corrections that strictly increase the language model score are kept. 2.1 Confidence-Weighted Learning As the learning algorithm for all classifiers, we choose confidence-weighted (CW) learning (Dredze et al., 2008; Crammer et al., 2009), which has been shown to perform well for natural language processing (NLP) problems with high dimensional and sparse feature spaces. Instead of keeping a single weight vector, CW learning maintains a distribution over weight vectors, parametrized by a multivariate normal distribution N (µ, Σ) with mean µ and covariance matrix Σ. In practice, Σ is often approximated by a diagonal matrix (Dredze et al., 2008). CW is an online learning algorithm that proceeds in rounds over a labeled training set ((y1 , x1 ), (y2 , x2 ), . . . , (yn , xn )), one example at a time. After the i-th round, CW learn"
W12-2025,P11-1092,1,0.95123,"ect). The surrounding context is represented as a real-valued feature vector x ∈ X . The features of our classifiers are described in Section 3. One challenge in training classifiers for grammatical error correction is that the data is highly skewed. Training examples without any error (i.e., the observed article equals the correct article) greatly outnumber those examples with an error (i.e., the observed article is different from the correct article). As the observed article is highly correlated with the correct article, the observed article is a valuable feature (Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011). However, the high correlation can have the undesirable effect that the classifier always predicts the observed article and never proposes any corrections. To mitigate this problem, we re-sample the training data, either by oversampling examples with an error or undersampling examples without an error. The sampling parameter is chosen through a grid search so as to maximize the F1 score on the development data. After training, the classifier can be used to predict the correct article for NPs from new unseen sentences. During testing, every NP in the test data generates one test example. If th"
W12-2025,W11-2841,1,0.849672,"8 F1 10.26 55.90 18.18 42.32 RT MT UT Prep 78.85 61.54 60.00 70.97 40.80 39.02 33.33 39.17 63.46 53.85 60.00 60.22 22.15 25.00 23.08 22.95 32.84 34.15 33.33 33.23 (a) Before revisions 27.52 28.57 23.08 27.05 (b) After revisions Table 9: Individual scores for each error type on the HOO- TEST data. for each of the six error types, and for determiners (Det: aggregate of RD, MD, UD) and prepositions (Prep: aggregate of RT, MT, UT) on the held-out HOO- DEVTEST set and the official test set HOOTEST , respectively. 5 Discussion The main differences between our submission to the HOO 2011 shared task (Dahlmeier et al., 2011) and to this year’s shared task are the use of the CW learning algorithm, the use of web-scale N-gram count features, and the use of the observed article or preposition as a feature. The CW learning algorithm performed slightly better than the empirical risk minimization batch learning algorithm that we have used previously while being significantly faster during training. Adding the web-scale N-gram count features showed significant improvements in initial experiments. Using the observed article or preposition feature allows the classifier to learn a bias against unnecessary corrections. We b"
W12-2025,W12-2006,0,0.0739767,"have evaluated on different data sets. The HOO 2012 shared task evaluates grammatical error correction systems for determiner and preposition errors. Participants are provided with a set of documents written by non-native speakers of English. The task is to automatically detect and correct determiner and preposition errors and produce a set of corrections (called edits). Evaluation is done by computing precision, recall, and F1 score between the system edits and a manually created set of gold-standard edits. The details of the HOO 2012 shared task are described in the official overview paper (Dale et al., 2012). In this paper, we describe the system submission from the National University of Singapore (NUS). Our system treats determiner and preposition correction as classification problems. We use confidenceweighted linear classifiers to predict the correct word from a confusion set of possible correction options. Separate classifiers are built for determiner errors, preposition replacement errors, and preposition insertion and deletion errors. The classifiers are combined into a pipeline of correction steps to form an end-to-end error correction system. Our system achieves the highest correction F1"
W12-2025,P07-2045,0,0.00564858,"ces We use the following NLP resources in our system. Sentence splitting is performed with the NLTK toolkit.4 For spelling correction, we use the free software Aspell.5 All words that appear at least ten times in the HOO 2012 training data are added to the spelling dictionary. We use the OpenNLP tools (version 1.5.2)6 for POS tagging, YamCha (version 0.33) (Kudo and Matsumoto, 2003) for chunking, and the MaltParser (version 1.6.1) (Nivre et al., 2007) for dependency parsing. We use RandLM (Talbot and Osborne, 2007) for language modeling. The re-casing model is built with the Moses SMT system (Koehn et al., 2007) from the Gigaword New York Times section and all normal-cased documents in the HOO 2012 training data. The CuVPlus English dictionary (Mitton, 1992) is used to determine the countability of nouns. The CW learning algorithm is implemented by our group. The source code is available from our website.7 All resources used in our system are publicly available. 4.3 Step Det + RT + MT/UT P 62.26 64.34 60.75 Recognition R F1 12.68 21.06 22.41 33.24 28.94 39.20 P 54.09 57.35 54.84 Correction R 11.01 19.97 26.12 F1 18.30 29.63 35.39 Table 6: Overall precision, recall, and F1 score on the HOO- DEVTEST da"
W12-2025,P03-1004,0,0.0449307,"ni et al., 2009) is used for collecting web-scale dependency N-gram counts, and the New York Times section of the Gigaword corpus3 is used for training the re-casing model. All data sets used in our system are publicly available. 4.2 Resources We use the following NLP resources in our system. Sentence splitting is performed with the NLTK toolkit.4 For spelling correction, we use the free software Aspell.5 All words that appear at least ten times in the HOO 2012 training data are added to the spelling dictionary. We use the OpenNLP tools (version 1.5.2)6 for POS tagging, YamCha (version 0.33) (Kudo and Matsumoto, 2003) for chunking, and the MaltParser (version 1.6.1) (Nivre et al., 2007) for dependency parsing. We use RandLM (Talbot and Osborne, 2007) for language modeling. The re-casing model is built with the Moses SMT system (Koehn et al., 2007) from the Gigaword New York Times section and all normal-cased documents in the HOO 2012 training data. The CuVPlus English dictionary (Mitton, 1992) is used to determine the countability of nouns. The CW learning algorithm is implemented by our group. The source code is available from our website.7 All resources used in our system are publicly available. 4.3 Step"
W12-2025,N10-1018,0,0.0740674,"(i.e., the article is correct). The surrounding context is represented as a real-valued feature vector x ∈ X . The features of our classifiers are described in Section 3. One challenge in training classifiers for grammatical error correction is that the data is highly skewed. Training examples without any error (i.e., the observed article equals the correct article) greatly outnumber those examples with an error (i.e., the observed article is different from the correct article). As the observed article is highly correlated with the correct article, the observed article is a valuable feature (Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011). However, the high correlation can have the undesirable effect that the classifier always predicts the observed article and never proposes any corrections. To mitigate this problem, we re-sample the training data, either by oversampling examples with an error or undersampling examples without an error. The sampling parameter is chosen through a grid search so as to maximize the F1 score on the development data. After training, the classifier can be used to predict the correct article for NPs from new unseen sentences. During testing, every NP in the test data generate"
W12-2025,W11-2843,0,0.0611877,"this section, we describe the features used in our system. The choice of features can have an important effect on classification performance. The exact features used for determiner, replacement preposition, and missing and unwanted preposition correction are listed in Tables 1, 2, 3, and 4, respectively. The features were chosen empirically through experiments on the development data. The most commonly used features for grammatical error correction are lexical and POS N-grams, and chunk features. We adopt the features from previous work by Han et al. (2006), Tetreault and Chodorow (2008), and Rozovskaya et al. (2011) for our system. Tetreault et al. (2010) show that parse features can further increase performance, and we use the dependency parse features based on their work. For all the above features, the observed article or preposition used by the writer is “blanked out” when computing the features. However, we add 219 the observed article or preposition as an additional feature for determiner and replacement preposition correction. The features described so far are all binaryvalued, i.e., they indicate whether some feature is present in the input or not. Additionally, we can construct real-valued featu"
W12-2025,P07-1065,0,0.0319469,"sed for training the re-casing model. All data sets used in our system are publicly available. 4.2 Resources We use the following NLP resources in our system. Sentence splitting is performed with the NLTK toolkit.4 For spelling correction, we use the free software Aspell.5 All words that appear at least ten times in the HOO 2012 training data are added to the spelling dictionary. We use the OpenNLP tools (version 1.5.2)6 for POS tagging, YamCha (version 0.33) (Kudo and Matsumoto, 2003) for chunking, and the MaltParser (version 1.6.1) (Nivre et al., 2007) for dependency parsing. We use RandLM (Talbot and Osborne, 2007) for language modeling. The re-casing model is built with the Moses SMT system (Koehn et al., 2007) from the Gigaword New York Times section and all normal-cased documents in the HOO 2012 training data. The CuVPlus English dictionary (Mitton, 1992) is used to determine the countability of nouns. The CW learning algorithm is implemented by our group. The source code is available from our website.7 All resources used in our system are publicly available. 4.3 Step Det + RT + MT/UT P 62.26 64.34 60.75 Recognition R F1 12.68 21.06 22.41 33.24 28.94 39.20 P 54.09 57.35 54.84 Correction R 11.01 19.97"
W12-2025,C08-1109,0,0.181181,"he next preposition. 3 Features In this section, we describe the features used in our system. The choice of features can have an important effect on classification performance. The exact features used for determiner, replacement preposition, and missing and unwanted preposition correction are listed in Tables 1, 2, 3, and 4, respectively. The features were chosen empirically through experiments on the development data. The most commonly used features for grammatical error correction are lexical and POS N-grams, and chunk features. We adopt the features from previous work by Han et al. (2006), Tetreault and Chodorow (2008), and Rozovskaya et al. (2011) for our system. Tetreault et al. (2010) show that parse features can further increase performance, and we use the dependency parse features based on their work. For all the above features, the observed article or preposition used by the writer is “blanked out” when computing the features. However, we add 219 the observed article or preposition as an additional feature for determiner and replacement preposition correction. The features described so far are all binaryvalued, i.e., they indicate whether some feature is present in the input or not. Additionally, we c"
W12-2025,P10-2065,0,0.0964798,"ed in our system. The choice of features can have an important effect on classification performance. The exact features used for determiner, replacement preposition, and missing and unwanted preposition correction are listed in Tables 1, 2, 3, and 4, respectively. The features were chosen empirically through experiments on the development data. The most commonly used features for grammatical error correction are lexical and POS N-grams, and chunk features. We adopt the features from previous work by Han et al. (2006), Tetreault and Chodorow (2008), and Rozovskaya et al. (2011) for our system. Tetreault et al. (2010) show that parse features can further increase performance, and we use the dependency parse features based on their work. For all the above features, the observed article or preposition used by the writer is “blanked out” when computing the features. However, we add 219 the observed article or preposition as an additional feature for determiner and replacement preposition correction. The features described so far are all binaryvalued, i.e., they indicate whether some feature is present in the input or not. Additionally, we can construct real-valued features by counting the log frequency of sur"
W12-2025,P11-1019,0,0.0416671,"cy count as a feature. We normalize all real-valued feature values to a unit interval [0, 1] to avoid features with larger values dominating features with smaller values. 4 Experiments In this section, we report experimental results of our system on two different data sets: a held-out test split of the HOO 2012 training data, and the official HOO 2012 test set. 4.1 Data Sets The HOO 2012 training data consists of 1,000 documents together with gold-standard annotation. The documents are a subset of the 1,244 documents in the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al., 2011). The HOO 2012 gold-standard annotation only contains edits for six determiner and preposition error types and discards all other gold edits Feature Lexical features Observed article† First word in NP† Word i before (i = 1, 2, 3)† Word i before NP (i = 1, 2) Word + POS i before (i = 1, 2, 3)† Word i after (i = 1, 2, 3)† Word after NP Word + POS i after (N = 1, 2)† Bag of words in NP† N-grams (N = 2, .., 5)‡ Word before + NP† NP + N-gram after NP (N = 1, 2, 3)† Noun compound (NC)† Adj + NC† Adj POS + NC† NP POS + NC† POS features First POS in NP POS i before (i = 1, 2, 3) POS i before NP (i = 1"
W13-1703,N12-1067,1,0.575078,"that it cannot be corrected. Unclear meaning Table 1: NUCLE error categories (continued) data set and are not included in the official NUCLE corpus. The essays were then annotated by our three annotators in a way that each essay was annotated independently by two annotators. Four essays had to be discarded as they were of very poor quality and did not allow for any meaningful correction. This left us with 96 essays with double annotation. Comparing two sets of annotation is complicated by the fact that the set of annotations that corrects an input text to a corrected output text is ambiguous (Dahlmeier and Ng, 2012). In other words, it is possible that two different sets of annotations produce the same correction. For example, one annotator could choose to select a whole phrase as one error, while the other annotator selects each word individually. Our annotation guidelines ask annotators to select the minimum span that is necessary to correct the error, but we do not enforce any hard constraints and different annotators can have a different perception of where an error starts or ends. An especially difficult case is the annotation of omission errors, for example missing articles. Selecting a range of wh"
W13-1703,W11-2838,0,0.539449,"ng errors) is most likely that the FCE data was not collected from take-home assignments where students have the chance to spell check their writing before submission. But it could also mean that the essays in FCE are from students with a lower proficiency in English compared to NUCLE. With regards to the annotation schema, the CLC annotations include both the type of error (missing, unnecessary, replacement, form) and the part of speech. As a result, the CLC tag set is large with 88 different error categories, far more than the 27 error categories in NUCLE. Finally, the HOO 2011 shared task (Dale and Kilgarriff, 2011) released an annotated corpus of fragments from academic papers written by non-native speakers and published in a conference or workshop of the Association for Computational Linguistics. The corpus uses the annotation schema from the CLC. Comparing the data set with NUCLE, the HOO 2011 data set is much smaller (about 20,000 words for training and testing, respectively) and represents a specific writing genre (NLP papers). The NUCLE corpus is much larger and covers a broader range of topics. 7 Conclusion We have presented the NUS Corpus of Learner English (NUCLE), a large, annotated corpus of l"
W13-1703,W12-2006,0,0.607788,"glish Corpus contains annotations for error types but does not include corrections for the errors. The Cambridge Learner Corpus (CLC) (Nicholls, 2003) is possibly the largest annotated English learner corpus. Unfortunately, to our knowledge, the corpus is not freely available for research purposes. A subset of the CLC was released in 2011 by Yannakoudakis et al. (2011). The released data set contains short essays written by students taking the First Certificate in English (FCE) examination. The data set was also used in the recent HOO 2012 shared task on preposition and determiner correction (Dale et al., 2012). Comparing the essays in the FCE data set and NUCLE, we observe that the essays in the FCE data set are shorter than the essays in NUCLE and show a higher density of grammatical errors. One reason for the higher number of errors (in particular spelling errors) is most likely that the FCE data was not collected from take-home assignments where students have the chance to spell check their writing before submission. But it could also mean that the essays in FCE are from students with a lower proficiency in English compared to NUCLE. With regards to the annotation schema, the CLC annotations inc"
W13-1703,W09-3010,0,0.0318914,"ly be considered fair and the Kappa scores for classification and exact agreement are moderate. Thus, an interesting result of the pilot study was that annotators find it harder to agree on whether a word is grammatically correct than agreeing on the type of error or how it should be corrected. The annotator agreement study shows that grammatical error correction, especially grammatical error identification, is a difficult problem. Our findings support previous research on annotator agreement that has shown that grammatical error correction is a challenging task (Tetreault and Chodorow, 2008; Lee et al., 2009). Tetreault and Chodorow (2008) report a Kappa score of 0.63 which in their words “shows the difficulty of this task and also show how two highly trained raters can produce very different judgments.” An interesting related work is (Lee et al., 2009) which investigates the annotation of article and noun number errors. The annotation is performed with either a single sentence context only or the five preceding sentences. The agreement between annotators increases when more context is given, from a Kappa score of 0.55 to a Kappa score of 0.60. Madnani et al. (2011) and Tetreault et al. (2010) pro"
W13-1703,P11-2089,0,0.191388,"task (Tetreault and Chodorow, 2008; Lee et al., 2009). Tetreault and Chodorow (2008) report a Kappa score of 0.63 which in their words “shows the difficulty of this task and also show how two highly trained raters can produce very different judgments.” An interesting related work is (Lee et al., 2009) which investigates the annotation of article and noun number errors. The annotation is performed with either a single sentence context only or the five preceding sentences. The agreement between annotators increases when more context is given, from a Kappa score of 0.55 to a Kappa score of 0.60. Madnani et al. (2011) and Tetreault et al. (2010) propose crowdsourcing to 27 overcome the problem of annotator variability. 4 Data Collection and Annotation The main data collection for the NUCLE corpus took place between August and December 2009. We collected a total of 2,249 student essays from 6 English courses at CELC. The courses are designed for students who need language support for their academic studies. The essays were written as course assignments on a wide range of topics, like technology innovation or health care. Some example question prompts are shown in Table 4. All students are at a similar acade"
W13-1703,W10-1004,0,0.228368,"Wcip errors, we observed that most Wcip errors are preposition errors. This confirms that articles and prepositions are the two most frequent error categories for EFL learners (Leacock et al., 2010). 29 6 Related Work In this section, we compare NUCLE with other learner corpora. While there were almost no annotated learner corpora available for research purposes until recently, non-annotated learner corpora have been available for a while. Two examples are the International Corpus of Learner English (ICLE) (Granger et al., 2002) and the Chinese Learner English Corpus (Gui and Yang., 2003)3 . Rozovskaya and Roth (2010) annotated a portion of each of these two learner corpora with error categories and corrections. However, with 63,000 words, the annotated data is small compared to NUCLE. 3 The Chinese Learner English Corpus contains annotations for error types but does not include corrections for the errors. The Cambridge Learner Corpus (CLC) (Nicholls, 2003) is possibly the largest annotated English learner corpus. Unfortunately, to our knowledge, the corpus is not freely available for research purposes. A subset of the CLC was released in 2011 by Yannakoudakis et al. (2011). The released data set contains"
W13-1703,W08-1205,0,0.128553,"dentification can therefore only be considered fair and the Kappa scores for classification and exact agreement are moderate. Thus, an interesting result of the pilot study was that annotators find it harder to agree on whether a word is grammatically correct than agreeing on the type of error or how it should be corrected. The annotator agreement study shows that grammatical error correction, especially grammatical error identification, is a difficult problem. Our findings support previous research on annotator agreement that has shown that grammatical error correction is a challenging task (Tetreault and Chodorow, 2008; Lee et al., 2009). Tetreault and Chodorow (2008) report a Kappa score of 0.63 which in their words “shows the difficulty of this task and also show how two highly trained raters can produce very different judgments.” An interesting related work is (Lee et al., 2009) which investigates the annotation of article and noun number errors. The annotation is performed with either a single sentence context only or the five preceding sentences. The agreement between annotators increases when more context is given, from a Kappa score of 0.55 to a Kappa score of 0.60. Madnani et al. (2011) and Tetreaul"
W13-1703,W10-1006,0,0.0285387,"ow, 2008; Lee et al., 2009). Tetreault and Chodorow (2008) report a Kappa score of 0.63 which in their words “shows the difficulty of this task and also show how two highly trained raters can produce very different judgments.” An interesting related work is (Lee et al., 2009) which investigates the annotation of article and noun number errors. The annotation is performed with either a single sentence context only or the five preceding sentences. The agreement between annotators increases when more context is given, from a Kappa score of 0.55 to a Kappa score of 0.60. Madnani et al. (2011) and Tetreault et al. (2010) propose crowdsourcing to 27 overcome the problem of annotator variability. 4 Data Collection and Annotation The main data collection for the NUCLE corpus took place between August and December 2009. We collected a total of 2,249 student essays from 6 English courses at CELC. The courses are designed for students who need language support for their academic studies. The essays were written as course assignments on a wide range of topics, like technology innovation or health care. Some example question prompts are shown in Table 4. All students are at a similar academic level, as they are all u"
W13-1703,P11-1019,0,0.406837,"h Corpus (Gui and Yang., 2003)3 . Rozovskaya and Roth (2010) annotated a portion of each of these two learner corpora with error categories and corrections. However, with 63,000 words, the annotated data is small compared to NUCLE. 3 The Chinese Learner English Corpus contains annotations for error types but does not include corrections for the errors. The Cambridge Learner Corpus (CLC) (Nicholls, 2003) is possibly the largest annotated English learner corpus. Unfortunately, to our knowledge, the corpus is not freely available for research purposes. A subset of the CLC was released in 2011 by Yannakoudakis et al. (2011). The released data set contains short essays written by students taking the First Certificate in English (FCE) examination. The data set was also used in the recent HOO 2012 shared task on preposition and determiner correction (Dale et al., 2012). Comparing the essays in the FCE data set and NUCLE, we observe that the essays in the FCE data set are shorter than the essays in NUCLE and show a higher density of grammatical errors. One reason for the higher number of errors (in particular spelling errors) is most likely that the FCE data was not collected from take-home assignments where student"
W13-1703,N10-1018,0,\N,Missing
W13-1703,W13-3601,1,\N,Missing
W13-3516,P05-1022,0,0.0426867,"umption. For the spoken genres – BC, BN and TC – we use the manual transcriptions rather than the output of a speech recognizer, as would be the case in real world. The performance on various layers for these genres would therefore be artificially inflated, and should be taken into account while analyzing results. Not many studies have previously reported on syntactic and semantic analysis for spoken genre. Favre et al. (2010) report the performance on the English subset of an earlier version of OntoNotes. 4.1 Syntax Predicted parse trees for English were produced using the Charniak parser11 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the nominal (NML) tag, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags to Penn-style part of speech tags. We used the mapping that is included with the Arabic Treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-folds for Arabic) cross-"
W13-3516,P08-1091,1,0.914176,"in the Treebank and (ii) we decided to exclude, we unfold the LINKs back to their original representation as in the PropBank 1.0 release. We used ASSERT15 (Pradhan et al., 2005) to predict the propositional structure for English. We made a small modification to ASSERT, and replaced the TinySVM classifier with a CRF16 to speed up training the model on all the data. The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on the OntoNotes v5.0 data. The Arabic propositional structure was predicted using the system described in Diab et al. (2008). (Diab et al., 2008) Table 5 shows the detailed per14 The Frame ID column indicates the F-score for English and Arabic, and accuracy for Chinese for the same reasons as word sense. 15 16 http://cemantix.org/assert.html http://leon.bottou.org/projects/sgd Frame Total ID Sent. English BC BN MZ NW TC WB PT Overall Chinese BC BN MZ NW TC WB Arabic Overall NW Total Prop. 93.2 1994 5806 92.7 1218 4166 90.8 740 2655 92.8 2122 6930 91.8 837 1718 90.7 1139 2751 96.6 1208 2849 92.8 9,261 26,882 87.7 885 2,323 93.3 929 4,419 92.3 451 2,620 96.6 481 2,210 82.2 968 1,622 87.8 758 1,761 90.9 4,472 14,955 8"
W13-3516,W00-1322,0,0.0291123,"Missing"
W13-3516,W06-0609,0,0.0675644,"and some noun instances, partial verb and noun word senses, coreference, and named entities. Table 1 gives an overview of the number of documents that have been annotated in the entire OntoNotes corpus. 2.1 Layers of Annotation This section provides a very concise overview of the various layers of annotations in OntoNotes. For a more detailed description, the reader is referred to (Weischedel et al., 2011) and the documentation accompanying the v5.04 release. 2.1.1 Syntax This represents the layer of syntactic annotation based on revised guidelines for the Penn Treebank (Marcus et al., 1993; Babko-Malaya et al., 2006), the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004). There were two updates made to the parse trees as part of the OntoNotes project: i) the introduction of NML phrases, in the English portion, to mark nominal sub-constituents of flat NPs that do not follow the default right-branching structure, and ii) re-tokenization of hyphenated tokens into multiple tokens in English and Chinese. The Arabic Treebank on the other hand was also significantly revised in an effort to increase consistency. 2.1.2 Word Sense Coarse-grained word senses are tagged for the mos"
W13-3516,2011.mtsummit-papers.22,1,0.659518,"reference decisions are made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities that were produced in the earlier sections. Each document part from the documents that were split into multiple parts during coreference annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic informatio"
W13-3516,P06-1005,0,0.0274234,"able. 4.5 Coreference The task is to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains. The coreference decisions are made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities that were produced in the earlier sections. Each document part from the documents that were split into multiple parts during coreference annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features i"
W13-3516,W12-4503,1,0.323243,"Missing"
W13-3516,W11-1905,1,0.8623,"Missing"
W13-3516,W10-4305,0,0.00957321,"thm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic information. Recently, it was discovered that there is possibly a bug in the official scorer used for the CoNLL 2011/2012 and the SemEval 2010 coreference tasks. This relates to the mis-implementation of the method proposed by (Cai and Strube, 2010) for scoring predicted mentions. This issue has also been recently reported in Recasens et al., (2013). As of this writing, the BCUBED metric has been fixed, and the correctness of the CEAFm , CEAFe and BLANC metrics is being verified. We will be updating the CoNLL shared task webpages22 with more detailed information and also release the patched scripts as soon as they are available. We will also re-generate the scores for previous shared tasks, and the coreference layer in this paper and make them available along with the models and system outputs for other layers. Table 7 shows the performa"
W13-3516,P11-2037,0,0.0435593,"tructure provided by the PropBank layer. Whereas in English, most traces represent syntactic phenomena such as movement and raising, in Chinese and Arabic, they can also represent dropped subjects/objects. These subset of traces directly affect the coreference layer, since, unlike English, traces in Chinese and Arabic (*pro* and * respectively) are legitimate targets of mentions and are considered for coreference annotation in OntoNotes. Recovering traces in text is a hard problem, and the most recently reported numbers in literature for Chinese are around a F-score of 50 (Yang and Xue, 2010; Cai et al., 2011). For Arabic there have not been much studies on recovering these. A study by Gabbard (2010) shows that these can be recovered with an F-score of 55 with automatic parses and roughly 65 using gold parses. Considering the low level of prediction accuracy of these tokens, and their relative low frequency, we decided to consider predicting traces in trees out of the scope of this study. In other words, we removed the manually identified traces and function tags from the Treebanks across all three languages, in all the three – training, development and test partitions. This meant removing any and"
W13-3516,W05-0620,0,0.0226073,"Missing"
W13-3516,P05-1045,0,0.0391503,"of the CEAFm , CEAFe and BLANC metrics is being verified. We will be updating the CoNLL shared task webpages22 with more detailed information and also release the patched scripts as soon as they are available. We will also re-generate the scores for previous shared tasks, and the coreference layer in this paper and make them available along with the models and system outputs for other layers. Table 7 shows the performance of the system on the Table 6: Performance of the named entity recognizer on the CoNLL-2012 test set. 4.4 Named Entities We retrained the Stanford named entity recognizer20 (Finkel et al., 2005) on the OntoNotes data. Table 6 shows the performance details for all the languages across all 18 name types broken down by genre. In English, BN has the highest performance followed by the NW genre. There is a significant drop from those and the TC and WB genre. Somewhat similar trend is observed in the Chinese data, with Arabic having the lowest scores. Since the Pivot Text portion (PT) of OntoNotes was not tagged with names, we could not compute the accuracy for that cross-section of the data. Previously Finkel and Manning (2009) performed 17 The number of sentences in this table are a subs"
W13-3516,W01-0521,0,0.0213903,"ver the sense inventories (and frame files) are defined per lemma – independent of the part of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, develo"
W13-3516,hockenmaier-steedman-2002-acquiring,0,0.0152391,"g5 1 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA 2 University of Trento, University of Trento, 38123 Povo (TN), Italy 3 QCRI, Qatar Foundation, 5825 Doha, Qatar 4 Brandeis University, Brandeis University, Waltham, MA 02453, USA 5 National University of Singapore, Singapore, 117417 6 University of Stuttgart, 70174 Stuttgart, Germany Abstract the Penn Discourse Treebank (Prasad et al., 2008), and many other annotation projects, all annotate the same underlying body of text. It was also converted to dependency structures and other syntactic formalisms such as CCG (Hockenmaier and Steedman, 2002) and LTAG (Shen et al., 2008), thereby creating an even bigger impact through these additional syntactic resources. The most recent one of these efforts is the OntoNotes corpus (Weischedel et al., 2011). However, unlike the previous extensions of the Treebank, in addition to using roughly a third of the same WSJ subcorpus, OntoNotes also added several other genres, and covers two other languages — Chinese and Arabic: portions of the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004) have been used to sample the genre of text that they represent. One of the cu"
W13-3516,W04-1602,0,0.0421028,"d to dependency structures and other syntactic formalisms such as CCG (Hockenmaier and Steedman, 2002) and LTAG (Shen et al., 2008), thereby creating an even bigger impact through these additional syntactic resources. The most recent one of these efforts is the OntoNotes corpus (Weischedel et al., 2011). However, unlike the previous extensions of the Treebank, in addition to using roughly a third of the same WSJ subcorpus, OntoNotes also added several other genres, and covers two other languages — Chinese and Arabic: portions of the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004) have been used to sample the genre of text that they represent. One of the current hurdles in language processing is the problem of domain, or genre adaptation. Although genre or domain are popular terms, their definitions are still vague. In OntoNotes, “genre” means a type of source – newswire (NW), broadcast news (BN), broadcast conversation (BC), magazine (MZ), telephone conversation (TC), web data (WB) or pivot text (PT). Changes in the entity and event profiles across source types, and even in the same source over a time duration, as explicitly expressed by surface lexical forms, usually"
W13-3516,J93-2004,0,0.0664902,"sitions for most verb and some noun instances, partial verb and noun word senses, coreference, and named entities. Table 1 gives an overview of the number of documents that have been annotated in the entire OntoNotes corpus. 2.1 Layers of Annotation This section provides a very concise overview of the various layers of annotations in OntoNotes. For a more detailed description, the reader is referred to (Weischedel et al., 2011) and the documentation accompanying the v5.04 release. 2.1.1 Syntax This represents the layer of syntactic annotation based on revised guidelines for the Penn Treebank (Marcus et al., 1993; Babko-Malaya et al., 2006), the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004). There were two updates made to the parse trees as part of the OntoNotes project: i) the introduction of NML phrases, in the English portion, to mark nominal sub-constituents of flat NPs that do not follow the default right-branching structure, and ii) re-tokenization of hyphenated tokens into multiple tokens in English and Chinese. The Arabic Treebank on the other hand was also significantly revised in an effort to increase consistency. 2.1.2 Word Sense Coarse-grained word s"
W13-3516,N06-1020,0,0.0154191,"inventories (and frame files) are defined per lemma – independent of the part of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, development and test set, so a"
W13-3516,P02-1014,0,0.0344201,"ce annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic information. Recently, it was discovered that there is possibly a bug in the official scorer used for the CoNLL 2011/2012 and the SemEval 2010 coreference tasks. This relates to the mis-implementation of the method proposed by (Cai and Strube, 2010) for scoring predicted mentions. This issue has also been recently reported"
W13-3516,J05-1004,0,0.0531944,"rformance. 1 Introduction Roughly a million words of text from the Wall Street Journal newswire (WSJ), circa 1989, has had a significant impact on research in the language processing community — especially those in the area of syntax and (shallow) semantics, the reason for this being the seminal impact of the Penn Treebank project which first selected this text for annotation. Taking advantage of a solid syntactic foundation, later researchers who wanted to annotate semantic phenomena on a relatively large scale, also used it as the basis of their annotation. For example the Proposition Bank (Palmer et al., 2005), BBN Name Entity and Pronoun coreference corpus (Weischedel and Brunstein, 2005), 1 A portion of the English data in the OntoNotes corpus is a selected set of sentences that were annotated for parse and word sense information. These sentences are present in a document of their own, and so the documents for parse layers for English are inflated by about 3655 documents and for the word sense are inflated by about 8797 documents. 143 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143–152, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Comput"
W13-3516,palmer-etal-2008-pilot,0,0.0620251,"Missing"
W13-3516,N07-1051,0,0.0519693,"ile analyzing results. Not many studies have previously reported on syntactic and semantic analysis for spoken genre. Favre et al. (2010) report the performance on the English subset of an earlier version of OntoNotes. 4.1 Syntax Predicted parse trees for English were produced using the Charniak parser11 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the nominal (NML) tag, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags to Penn-style part of speech tags. We used the mapping that is included with the Arabic Treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-folds for Arabic) cross-validation. For testing, we used a model trained on the entire training portion. Table 3 shows the precision, recall and F1 -scores of the re-trained parsers on the CoNLL-2012 test along with the part of speech accuracies (POS) using the standard evalb scorer. The performance on the PT ge"
W13-3516,J08-2006,1,0.738119,"of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, development and test set, so as to facilitate future studies. 2.1.3 Proposition The propositions in OntoNo"
W13-3516,W11-1901,1,0.408033,"Missing"
W13-3516,W12-4501,1,0.81236,"he exact same phenomena have been annotated on a broad cross-section of the same language before OntoNotes. The OntoNotes corpus thus provides an opportunity for studying the genre effect on different syntactic, semantic and discourse analyzers. Parts of the OntoNotes Corpus have been used for various shared tasks organized by the language processing community. The word sense layer was the subject of prediction in two SemEval-2007 tasks, and the coreference layer was the subject of prediction in the SemEval-20102 (Recasens et al., 2010), CoNLL-2011 and 2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). The CoNLL-2012 shared task provided predicted information to the participants, however, that did not include a few layers such as the named entities for Chinese and Arabic, propositions for Arabic, and for better comparison of the English data with the CoNLL-2011 task, a smaller OntoNotes v4.0 portion of the English parse and propositions was used for training. This paper is a first attempt at presenting a coherent high-level picture of the performance of various publicly available state-of-the-art tools on all the layers of OntoNotes in all three languages, so as to pave the way for further"
W13-3516,prasad-etal-2008-penn,0,0.0477485,"Missing"
W13-3516,S10-1001,0,0.0530331,"Missing"
W13-3516,N13-1071,0,0.0213813,"Missing"
W13-3516,J08-2004,1,0.227032,"nd LINK - PCR. Since the community is not used to the new PropBank representation which (i) relies heavily on the trace structure in the Treebank and (ii) we decided to exclude, we unfold the LINKs back to their original representation as in the PropBank 1.0 release. We used ASSERT15 (Pradhan et al., 2005) to predict the propositional structure for English. We made a small modification to ASSERT, and replaced the TinySVM classifier with a CRF16 to speed up training the model on all the data. The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on the OntoNotes v5.0 data. The Arabic propositional structure was predicted using the system described in Diab et al. (2008). (Diab et al., 2008) Table 5 shows the detailed per14 The Frame ID column indicates the F-score for English and Arabic, and accuracy for Chinese for the same reasons as word sense. 15 16 http://cemantix.org/assert.html http://leon.bottou.org/projects/sgd Frame Total ID Sent. English BC BN MZ NW TC WB PT Overall Chinese BC BN MZ NW TC WB Arabic Overall NW Total Prop. 93.2 1994 5806 92.7 1218 4166 90.8 740 2655 92.8 2122 6930 91.8 837 1718 90.7 1139 2751 96.6"
W13-3516,C10-2158,1,0.243983,"nd the proposition structure provided by the PropBank layer. Whereas in English, most traces represent syntactic phenomena such as movement and raising, in Chinese and Arabic, they can also represent dropped subjects/objects. These subset of traces directly affect the coreference layer, since, unlike English, traces in Chinese and Arabic (*pro* and * respectively) are legitimate targets of mentions and are considered for coreference annotation in OntoNotes. Recovering traces in text is a hard problem, and the most recently reported numbers in literature for Chinese are around a F-score of 50 (Yang and Xue, 2010; Cai et al., 2011). For Arabic there have not been much studies on recovering these. A study by Gabbard (2010) shows that these can be recovered with an F-score of 55 with automatic parses and roughly 65 using gold parses. Considering the low level of prediction accuracy of these tokens, and their relative low frequency, we decided to consider predicting traces in trees out of the scope of this study. In other words, we removed the manually identified traces and function tags from the Treebanks across all three languages, in all the three – training, development and test partitions. This mean"
W13-3516,W10-1836,1,0.361229,"Missing"
W13-3516,P10-4014,1,0.579474,"Missing"
W13-3516,D08-1105,1,0.881098,"Missing"
W13-3516,N01-1016,0,\N,Missing
W13-3516,N07-1070,1,\N,Missing
W13-3516,N09-1037,0,\N,Missing
W13-3601,de-marneffe-etal-2006-generating,0,0.067319,"Missing"
W13-3601,N10-1019,0,0.180097,"r determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb fo"
W13-3601,P03-1054,0,0.00544378,"h purposes since June 20111 . All instances of grammatical errors are annotated in NUCLE, and the errors are classified into 27 error types (Dahlmeier et al., 2013). To help participating teams in their preparation for the shared task, we also performed automatic preprocessing of the NUCLE corpus and released the preprocessed form of NUCLE. The preprocessing operations performed on the NUCLE essays include sentence segmentation and word tokenization using the NLTK toolkit (Bird et al., 2009), and part-of-speech (POS) tagging, constituency and dependency tree parsing using the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). The error annotations, which are originally at the character level, are then mapped to error annotations at the word token level. Error annotations at the word token 1 level also facilitate scoring, as we will see in Section 4, since our scorer operates by matching tokens. Note that although we released our own preprocessed version of NUCLE, the participating teams were however free to perform their own preprocessing if they so preferred. 3.1.1 Revised version of NUCLE NUCLE release version 2.3 was used in the CoNLL-2013 shared task. In this version, 17 essays were"
W13-3601,D11-1010,1,0.122995,"agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/determiners and prepositio"
W13-3601,P11-1092,1,0.860095,"agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/determiners and prepositio"
W13-3601,D12-1052,1,0.806821,"admit some bad effect which is brought by the new technology, still the advantages of the new technologies cannot be simply discarded. The noun number error effect needs to be corrected (effect → effects). This necessitates the correction of a subject-verb agreement error (is → are). A pipeline system in which corrections for subjectverb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of (Dahlmeier and Ng, 2012a), for example, is designed to deal with multiple, interacting errors. Note that the essays in the training data and the test essays naturally contain grammatical errors of all types, beyond the five error types focused in our shared task. In the automatically corrected essays returned by a participating system, only corrections necessary to correct errors of the five types From past to the present, many important innovations have surfaced. There is an article/determiner error (past → the past) in this sentence. The error annotation, also called correction or edit, in SGML format is shown in"
W13-3601,D10-1094,0,0.289959,", prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreem"
W13-3601,W13-3602,0,0.378187,"ion approach ANPSV AN ANPSV ANPSV ANPSV STEL SZEG TILB TOR UAB STAN SJT1 NTHU SAAR KOR NARA ANPSV R M R M T M L L M R M SV AN S ANP AP N SV ANPV A S ANPSV IITB M ANP HIT Approach T Error ANPSV Team CAMB and Manning, 2003). We also make use of the POS tags assigned in the preprocessed form of the test essays. We then assign an error type to a system edit based on the automatically determined POS tags, as follows: evaluated with alternative answers. Not surprisingly, the teams which submitted alternative answers tend to show the greatest improvements in their F1 measure. Overall, the UIUC team (Rozovskaya et al., 2013) achieves the best F1 measure, with a clear lead over the other teams in the shared task, under both evaluation settings (without and with alternative answers). For future research which uses the test data of the CoNLL-2013 shared task, we recommend that evaluation be carried out in the setting that does not use alternative answers, to ensure a fairer evaluation. This is because the scores of the teams which submitted alternative answers tend to be higher in a biased way when evaluated with alternative answers. Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Team UIUC NTHU UMC NARA HIT STEL CAM"
W13-3601,N12-1067,1,0.697277,"admit some bad effect which is brought by the new technology, still the advantages of the new technologies cannot be simply discarded. The noun number error effect needs to be corrected (effect → effects). This necessitates the correction of a subject-verb agreement error (is → are). A pipeline system in which corrections for subjectverb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of (Dahlmeier and Ng, 2012a), for example, is designed to deal with multiple, interacting errors. Note that the essays in the training data and the test essays naturally contain grammatical errors of all types, beyond the five error types focused in our shared task. In the automatically corrected essays returned by a participating system, only corrections necessary to correct errors of the five types From past to the present, many important innovations have surfaced. There is an article/determiner error (past → the past) in this sentence. The error annotation, also called correction or edit, in SGML format is shown in"
W13-3601,P10-2065,1,0.493158,"verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/de"
W13-3601,W13-1703,1,0.717953,"ince there are five error types in our shared task compared to two in HOO 2012, there is a greater chance of encountering multiple, interacting errors in a sentence in our shared task. This increases the complexity of our shared task relative to that of HOO 2012. To illustrate, consider the following sentence: are made. The other errors are to be left uncorrected. 3 Data This section describes the training and test data released to each participating team in our shared task. 3.1 Training Data The training data provided in our shared task is the NUCLE corpus, the NUS Corpus of Learner English (Dahlmeier et al., 2013). As noted by (Leacock et al., 2010), the lack of a manually annotated and corrected corpus of English learner texts has been an impediment to progress in grammatical error correction, since it prevents comparative evaluations on a common benchmark test data set. NUCLE was created precisely to fill this void. It is a collection of 1,414 essays written by students at the National University of Singapore (NUS) who are non-native speakers of English. The essays were written in response to some prompts, and they cover a wide range of topics, such as environmental pollution, health care, etc. The g"
W13-3601,P11-1019,0,0.412215,"Missing"
W13-3601,W11-2838,0,0.650871,"the various approaches adopted by the participating teams, and present the evaluation results. 1 Introduction Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013 (CoNLL2013). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years (Dale and Kilgarriff, 2011; Dale et al., 2012). In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application. This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed. Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwi"
W13-3601,W12-2006,0,0.751371,"ted by the participating teams, and present the evaluation results. 1 Introduction Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013 (CoNLL2013). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years (Dale and Kilgarriff, 2011; Dale et al., 2012). In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application. This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed. Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwide are learning Engl"
W13-3601,W13-3604,0,\N,Missing
W14-1104,P07-1007,1,0.378126,"adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation techniques whereas we apply and compare multiple state-of-the-art domain adaptation techniques. Rai et al. (2010) exploited a In this paper, we explore domain adaptation for coreference resolution from the resource-rich newswire domain to the biomedical domain. Our approach comprises domain adaptation, active learning, and target domain instance weighting to leverage the existing annotated corpora from the newswire domain, so as to reduce"
W14-1104,M95-1025,0,0.628812,"Missing"
W14-1104,P02-1014,0,0.0982735,"aining instances that we need to annotate. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing (NLP) tasks, such as information extraction and question answering. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly in21 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21–29, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics document with an average length of 6,155 words, while an annotator could annotate 3,000 words per hour in POS tag annotation (Marcus et al., 1993). tion 7. 2 The simplest approach to avoid the timeconsuming data annotation in a new domain is"
W14-1104,P07-1033,0,0.185691,"Missing"
W14-1104,W10-0104,0,0.0171252,"sfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation techniques whereas we apply and compare multiple state-of-the-art domain adaptation techniques. Rai et al. (2010) exploited a In this paper, we explore domain adaptation for coreference resolution from the resource-rich newswire domain to the biomedical domain. Our approach comprises domain adaptation, active learning, and target domain instance weighting to leverage the existing annotated corpora from the newswire domain, so as to reduce the cost of developing a coreference re"
W14-1104,C08-1033,0,0.024108,"ions in the biomedical domain. Their annotation only concerned the pronominal and nominal anaphoric expressions in 46 biomedical abstracts. Gasperin and Briscoe (2007) annotated coreferential relations on 5 full articles in the biomedical domain, but only on noun phrases referring to bio-entities. Yang et al. (2004) annotated full NP coreferential relations on biomedical abstracts of the GENIA corpus. The ongoing project of the CRAFT corpus is expected to annotate all coreferential relations on full text of biomedical articles (Cohen et al., 2010). Unlike the work of (Casta˜no et al., 2002), (Gasperin and Briscoe, 2008), and (Gasperin, 2009) that resolved coreferential relations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and"
W14-1104,J01-4004,1,0.589486,"of target domain training instances that we need to annotate. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing (NLP) tasks, such as information extraction and question answering. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly in21 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21–29, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics document with an average length of 6,155 words, while an annotator could annotate 3,000 words per hour in POS tag annotation (Marcus et al., 1993). tion 7. 2 The simplest approach to avoid the timeconsuming data annotati"
W14-1104,W09-1901,0,0.586077,"ir annotation only concerned the pronominal and nominal anaphoric expressions in 46 biomedical abstracts. Gasperin and Briscoe (2007) annotated coreferential relations on 5 full articles in the biomedical domain, but only on noun phrases referring to bio-entities. Yang et al. (2004) annotated full NP coreferential relations on biomedical abstracts of the GENIA corpus. The ongoing project of the CRAFT corpus is expected to annotate all coreferential relations on full text of biomedical articles (Cohen et al., 2010). Unlike the work of (Casta˜no et al., 2002), (Gasperin and Briscoe, 2008), and (Gasperin, 2009) that resolved coreferential relations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III,"
W14-1104,P09-1074,0,0.104025,"we need to annotate. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing (NLP) tasks, such as information extraction and question answering. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly in21 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21–29, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics document with an average length of 6,155 words, while an annotator could annotate 3,000 words per hour in POS tag annotation (Marcus et al., 1993). tion 7. 2 The simplest approach to avoid the timeconsuming data annotation in a new domain is to train a coreference"
W14-1104,P07-1034,0,0.608042,"coe, 2008), and (Gasperin, 2009) that resolved coreferential relations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count mergi"
W14-1104,P02-1016,0,0.0538352,"9 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation techniques whereas we apply and compare multiple state-of-the-art domain adaptation techniques. Rai et al. (2010) exploited a In this paper, we explor"
W14-1104,W11-1801,0,0.0830915,"Missing"
W14-1104,M95-1005,0,0.154467,"specific version. The augmented source domain data will contain only the general and the source-specific versions, while the augmented target domain data will contain only the general and the target-specific versions. In the above sentence, the same MTHC lines and the cells are referring to the same entity and hence are coreferential. It is possible that more than two markables are coreferential in a text. The task of coreference resolution is to determine these relations in a given text. To evaluate the performance of coreference resolution, we follow the MUC evaluation metric introduced by (Vilain et al., 1995). Let Si be an equivalence class generated by the key (i.e., Si 4.1.2 I NSTANCE W EIGHTING and I NSTANCE P RUNING Let x and y be the feature vector and the corresponding true label of an instance, respectively. 23 Jiang and Zhai (2007) pointed out that when applying a classifier trained on a source domain to a target domain, the joint probability Pt (x, y) in the target domain may be different from the joint probability Ps (x, y) in the source domain. They proposed a general framework to use Ps (x, y) to estimate Pt (x, y). The joint probability P (x, y) can be factored into P (x, y) = P (y|x)"
W14-1104,W11-1813,0,0.0196848,"View, CA 94043, USA szhao@elance.com Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg Abstract creasing number of biomedical texts, including research papers, patent documents, etc. This results in an increasing demand for applying natural language processing and information retrieval techniques to efficiently exploit information contained in these large amounts of texts. However, coreference resolution, one of the core tasks in NLP, has only a relatively small body of prior research in the biomedical domain (Kim et al., 2011a; Kim et al., 2011b). A large body of prior research on coreference resolution focuses on texts in the newswire domain. Standardized data sets, such as MUC (DARPA Message Understanding Conference, (MUC-6, 1995; MUC-7, 1998)) and ACE (NIST Automatic Content Extraction Entity Detection and Tracking task, (NIST, 2002)) data sets are widely used in the study of coreference resolution. Traditionally, in order to apply supervised machine learning approaches to an NLP task in a specific domain, one needs to collect a text corpus in the domain and annotate it to serve as training data. Compared to ot"
W14-1104,W12-2409,0,0.14636,"Missing"
W14-1104,D12-1068,0,0.192988,"lations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation tech"
W14-1104,C10-1147,1,0.842966,"Introduction Coreference resolution is the task of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing (NLP) tasks, such as information extraction and question answering. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly in21 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21–29, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics document with an average length of 6,155 words, while an annotator could annotate 3,000 words per hour in POS tag annotation (Marcus et al., 1993). tion 7. 2 The simplest approach to avoid the timeconsuming data annotation in a new domain is to train a coreference resolution system o"
W14-1104,J93-2004,0,0.0452612,"ne whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly in21 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21–29, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics document with an average length of 6,155 words, while an annotator could annotate 3,000 words per hour in POS tag annotation (Marcus et al., 1993). tion 7. 2 The simplest approach to avoid the timeconsuming data annotation in a new domain is to train a coreference resolution system on a resource-rich domain and apply it to a different target domain without any additional data annotation. Although coreference resolution systems work well on test texts in the same domain as the training texts, there is a huge performance drop when they are tested on a different domain. This motivates the usage of domain adaptation techniques for coreference resolution: adapting a coreference resolution system from one source domain in which we have a larg"
W14-1104,D08-1105,1,0.716428,"n studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation techniques whereas we apply and compare multiple state-of-the-art domain adaptation techniques. Rai et al. (2010) exploited a In this paper, we explore domain adaptation for coreference resolution from the resource-rich newswire domain to the biomedical domain. Our approach comprises domain adaptation, active learning, and target domain instance weighting to leverage the existing annotated corpora from the newswire domain, so as to reduce the cost of developi"
W14-1104,N12-1055,0,\N,Missing
W14-1701,D11-1010,1,0.82865,"bmit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word choice errors (Dahlmeier and Ng, 2011a) were not dealt with. In the CoNLL-2014"
W14-1701,P11-1092,1,0.545955,"bmit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word choice errors (Dahlmeier and Ng, 2011a) were not dealt with. In the CoNLL-2014"
W14-1701,D12-1052,1,0.571584,"systems for automatically detecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of Dahlmeier and Ng (2012a) and Wu and Ng (2013), for example, is designed to deal with multiple, interacting errors. present in English essays written by second language learners of English. Each participating team is given training data manually annotated with corrections of grammatical errors. The test data consists of new, blind test essays. Preprocessed test essays, which have been sentencesegmented and tokenized, are also made available to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical error"
W14-1701,N12-1067,1,0.528923,"systems for automatically detecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of Dahlmeier and Ng (2012a) and Wu and Ng (2013), for example, is designed to deal with multiple, interacting errors. present in English essays written by second language learners of English. Each participating team is given training data manually annotated with corrections of grammatical errors. The test data consists of new, blind test essays. Preprocessed test essays, which have been sentencesegmented and tokenized, are also made available to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical error"
W14-1701,W14-1705,0,0.0380739,"Missing"
W14-1701,W13-1703,1,0.793382,"examples of the 28 error types in the CoNLL-2014 shared task. Since there are 28 error types in our shared task compared to two in HOO 2012 and five in CoNLL2013, there is a greater chance of encountering multiple, interacting errors in a sentence in our shared task. This increases the complexity of our shared task. To illustrate, consider the following sentence: 3 Data This section describes the training and test data released to each participating team in our shared task. 3.1 Training Data The training data provided in our shared task is the NUCLE corpus, the NUS Corpus of Learner English (Dahlmeier et al., 2013). As noted by (Leacock et al., 2010), the lack of a manually annotated and corrected corpus of English learner texts has been an impediment to progress in grammatical error correction, since it prevents comparative evaluations on a common benchmark test data set. NUCLE was created precisely to fill this void. It is a collection of 1,414 essays written by students at the National University of Singapore (NUS) who are non-native speakers of English. The essays were written in response to some prompts, and they cover a wide range of topics, such as environmental pollution, health care, etc. The g"
W14-1701,W13-3601,1,0.494294,"ote that although we released our own preprocessed version of NUCLE, the participating teams were however free to perform their own preprocessing if they so preferred. NUCLE release version 3.2 was used in the CoNLL-2014 shared task. In this version, 17 essays were removed from the first release of NUCLE since these essays were duplicates with multiple annotations. In addition, in order to facilitate the detection and correction of article/determiner errors and preposition errors, we performed some automatic mapping of error types in the original NUCLE corpus to arrive at release version 3.2. Ng et al. (2013) gives more details of how the mapping was carried out. The statistics of the NUCLE corpus (release 3.2 version) are shown in Table 2. The distribution of errors among all error types is shown in Table 3. While the NUCLE corpus is provided in our shared task, participating teams are free to not use NUCLE, or to use additional resources and tools in building their grammatical error correction systems, as long as these resources and tools are pub1 # essays # sentences # word tokens Training data (NUCLE) 1,397 57,151 1,161,567 Test data 50 1,312 30,144 Table 2: Statistics of training and test dat"
W14-1701,W11-2838,0,0.158186,"o independently annotated the test essays, compared to just one human annotator in CoNLL-2013. 1 Introduction Grammatical error correction is the shared task of the Eighteenth Conference on Computational Natural Language Learning in 2014 (CoNLL-2014). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors of all error types present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) organized in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012), and a CoNLL 2 Task Definition The goal of the CoNLL-2014 shared task is to evaluate algorithms and systems for automatically detecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for th"
W14-1701,W12-2006,0,0.112084,"he test essays, compared to just one human annotator in CoNLL-2013. 1 Introduction Grammatical error correction is the shared task of the Eighteenth Conference on Computational Natural Language Learning in 2014 (CoNLL-2014). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors of all error types present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) organized in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012), and a CoNLL 2 Task Definition The goal of the CoNLL-2014 shared task is to evaluate algorithms and systems for automatically detecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The abil"
W14-1701,de-marneffe-etal-2006-generating,0,0.051769,"Missing"
W14-1701,D10-1094,0,0.00847095,"able to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word choice errors (Dahlmeier and N"
W14-1701,W14-1702,0,0.477963,"Bombay Instituto Polit´ecnico Nacional Nara Institute of Science and Technology National Tsing Hua University Peking University Pohang University of Science and Technology Research Institute for Artificial Intelligence, Romanian Academy Shanghai Jiao Tong University University of Franche-Comt´e University of Macau Table 5: The list of 13 participating teams. The teams that submitted their system output after the deadline have an asterisk affixed after their team names. NARA did not submit any system description paper. MT approach mainly differed in terms of their attitude toward tuning; CAMB (Felice et al., 2014) performed no tuning at all, IITB (Kunchukuttan et al., 2014) and UMC (Wang et al., 2014b) tuned F0.5 using MERT, while AMU (JunczysDowmunt and Grundkiewicz, 2014) explored a variety of tuning options, ultimately tuning F0.5 using a combination of kb-MIRA and MERT. No team used a syntax-based translation model, although UMC did include POS tags and morphology in a factored translation model. With regard to correcting single error types, rule-based (RB) approaches were also common in most teams’ systems. A possible reason for this is that some error types are more regular than others, and so in"
W14-1701,W14-1704,0,0.500627,"ypes are more regular than others, and so in order to boost accuracy, simple rules can be written to make sure that, for example, the number of a subject agrees with the number of a verb. In contrast, it is a lot harder to write a rule to consistently correct Wci (wrong collocation/idiom) errors. As such, RB methods were often, but not always, used as a preliminary or supplementary stage in a larger hybrid system. Finally, although there were fewer machinelearnt classifier (ML) approaches than last year, some teams still used various classifiers to correct specific error types. In fact, CUUI (Rozovskaya et al., 2014) only built classifiers for specific error types and did not attempt to tackle the whole range of errors. SJTU (Wang et al., 2014a) also preprocessed the training data into more precise error categories using rules (e.g., verb tense (Vt) team’s approach can be found in Table 6. While machine-learnt classifiers for specific error types proved popular in last year’s CoNLL-2013 shared task, since this year’s task required the correction of all 28 error types, teams tended to prefer methods that could deal with all error types simultaneously. In fact, most teams built hybrid systems that made use"
W14-1701,N10-1019,0,0.0148891,"so made available to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word cho"
W14-1701,W08-1205,0,0.00769315,"Missing"
W14-1701,P10-2065,0,0.0173654,"eams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many different types, including articles or determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. However, most prior published research on grammatical error correction only focuses on a small number of frequently occurring error types, such as article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL2013 shared task, the error types were extended to include five error types, comprising article or determiner, preposition, noun number, verb form, and subject-verb agreement. Other error types such as word choice errors (Dahlmeier and Ng, 2011a) were not dealt"
W14-1701,W14-1707,0,0.00737204,"s that made use of a combination of different approaches to identify and correct errors. One of the most popular approaches to nonspecific error type correction, incorporated to various extents in many teams’ systems, was the Language Model (LM) based approach. Specifically, the probability of a learner n-gram is compared with the probability of a candidate corrected ngram, and if the difference is greater than some threshold, an error was perceived to have been detected and a higher scoring replacement n-gram could be suggested. Some teams used this approach only to detect errors, e.g., IPN (Hernandez and Calvo, 2014), which could then be corrected by other methods, whilst other teams used other methods to detect errors first, and then made corrections based on the alternative highest n-gram probability score, e.g., RAC (Boros¸ et al., 2014). No single team used a uniquely LM-based solution and the LM approach was always a component in a hybrid system. An alternative solution to correcting all errors was to use a phrase-based statistical machine translation (MT) system to “translate” learner English into correct English. Teams that followed the 7 Team ID CAMB CUUI AMU POST NTHU RAC UMC PKU∗ NARA SJTU UFC∗"
W14-1701,W14-1710,0,0.061916,"ing Hua University Peking University Pohang University of Science and Technology Research Institute for Artificial Intelligence, Romanian Academy Shanghai Jiao Tong University University of Franche-Comt´e University of Macau Table 5: The list of 13 participating teams. The teams that submitted their system output after the deadline have an asterisk affixed after their team names. NARA did not submit any system description paper. MT approach mainly differed in terms of their attitude toward tuning; CAMB (Felice et al., 2014) performed no tuning at all, IITB (Kunchukuttan et al., 2014) and UMC (Wang et al., 2014b) tuned F0.5 using MERT, while AMU (JunczysDowmunt and Grundkiewicz, 2014) explored a variety of tuning options, ultimately tuning F0.5 using a combination of kb-MIRA and MERT. No team used a syntax-based translation model, although UMC did include POS tags and morphology in a factored translation model. With regard to correcting single error types, rule-based (RB) approaches were also common in most teams’ systems. A possible reason for this is that some error types are more regular than others, and so in order to boost accuracy, simple rules can be written to make sure that, for example, th"
W14-1701,W14-1711,0,0.0400407,"ing Hua University Peking University Pohang University of Science and Technology Research Institute for Artificial Intelligence, Romanian Academy Shanghai Jiao Tong University University of Franche-Comt´e University of Macau Table 5: The list of 13 participating teams. The teams that submitted their system output after the deadline have an asterisk affixed after their team names. NARA did not submit any system description paper. MT approach mainly differed in terms of their attitude toward tuning; CAMB (Felice et al., 2014) performed no tuning at all, IITB (Kunchukuttan et al., 2014) and UMC (Wang et al., 2014b) tuned F0.5 using MERT, while AMU (JunczysDowmunt and Grundkiewicz, 2014) explored a variety of tuning options, ultimately tuning F0.5 using a combination of kb-MIRA and MERT. No team used a syntax-based translation model, although UMC did include POS tags and morphology in a factored translation model. With regard to correcting single error types, rule-based (RB) approaches were also common in most teams’ systems. A possible reason for this is that some error types are more regular than others, and so in order to boost accuracy, simple rules can be written to make sure that, for example, th"
W14-1701,W14-1703,0,0.263527,"Missing"
W14-1701,P13-1143,1,0.816741,"ecting and correcting grammatical errors 1 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics (plays → play). A pipeline system in which corrections for subject-verb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of Dahlmeier and Ng (2012a) and Wu and Ng (2013), for example, is designed to deal with multiple, interacting errors. present in English essays written by second language learners of English. Each participating team is given training data manually annotated with corrections of grammatical errors. The test data consists of new, blind test essays. Preprocessed test essays, which have been sentencesegmented and tokenized, are also made available to the participating teams. Each team is to submit its system output consisting of the automatically corrected essays, in sentence-segmented and tokenized form. Grammatical errors consist of many diffe"
W14-1701,P03-1054,0,0.0268268,"ed in (Dahlmeier and Ng, 2011b), and has been publicly available for research purposes since June 20111 . All instances of grammatical errors are annotated in NUCLE. To help participating teams in their preparation for the shared task, we also performed automatic preprocessing of the NUCLE corpus and released the preprocessed form of NUCLE. The preprocessing operations performed on the NUCLE essays include sentence segmentation and word tokenization using the NLTK toolkit (Bird et al., 2009), and part-of-speech (POS) tagging, constituency and dependency tree parsing using the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). The error annotations, which are originally at the character level, are then mapped to error annotations at the word token level. Error annotations at the word token level also facilitate scoring, as we will see in Section 4, since our scorer operates by matching tokens. Note that although we released our own preprocessed version of NUCLE, the participating teams were however free to perform their own preprocessing if they so preferred. NUCLE release version 3.2 was used in the CoNLL-2014 shared task. In this version, 17 essays were removed from the first release o"
W14-1701,P11-1019,1,0.389773,"e shown in Table 2. The distribution of errors among all error types is shown in Table 3. While the NUCLE corpus is provided in our shared task, participating teams are free to not use NUCLE, or to use additional resources and tools in building their grammatical error correction systems, as long as these resources and tools are pub1 # essays # sentences # word tokens Training data (NUCLE) 1,397 57,151 1,161,567 Test data 50 1,312 30,144 Table 2: Statistics of training and test data. licly available and not proprietary. For example, participating teams are free to use the Cambridge FCE corpus (Yannakoudakis et al., 2011; Nicholls, 2003) (the training data provided in HOO 2012 (Dale et al., 2012)) as additional training data. 3.2 Test Data Similar to CoNLL-2013, 25 NUS students, who are non-native speakers of English, were recruited to write new essays to be used as blind test data in the shared task. Each student wrote two essays in response to the two prompts shown in Table 4, one essay per prompt. The first prompt was also used in the NUCLE training data, but the second prompt is entirely new and not used previously. As a result, 50 new test essays were collected. The statistics of the test essays are also"
W14-1701,W14-1708,0,0.0161278,"f Science and Technology National Tsing Hua University Peking University Pohang University of Science and Technology Research Institute for Artificial Intelligence, Romanian Academy Shanghai Jiao Tong University University of Franche-Comt´e University of Macau Table 5: The list of 13 participating teams. The teams that submitted their system output after the deadline have an asterisk affixed after their team names. NARA did not submit any system description paper. MT approach mainly differed in terms of their attitude toward tuning; CAMB (Felice et al., 2014) performed no tuning at all, IITB (Kunchukuttan et al., 2014) and UMC (Wang et al., 2014b) tuned F0.5 using MERT, while AMU (JunczysDowmunt and Grundkiewicz, 2014) explored a variety of tuning options, ultimately tuning F0.5 using a combination of kb-MIRA and MERT. No team used a syntax-based translation model, although UMC did include POS tags and morphology in a factored translation model. With regard to correcting single error types, rule-based (RB) approaches were also common in most teams’ systems. A possible reason for this is that some error types are more regular than others, and so in order to boost accuracy, simple rules can be written to make"
W17-5037,P14-1129,0,0.100611,"l SMT component to a word-level SMT-based GEC system, with the aim of correcting misspelled words. Our word-level SMT-based GEC system utilizes task-specific features described in (JunczysDowmunt and Grundkiewicz, 2016). We show in this paper that performance continues to improve further after adding neural network joint models (NNJMs), as introduced in (Chollampatt et al., 2016b). NNJMs can leverage the continuous space representation of words and phrases and can capture a larger context from the source sentence, which enables them to make better predictions than traditional language models (Devlin et al., 2014). The NNJM is further improved using the regularized adaptive training method described in (Chollampatt et al., 2016a) on a higher quality training dataset, which has a higher errorper-sentence ratio. In addition, we add a characterlevel SMT component to generate candidate corrections for misspelled words. These candidate corrections are rescored with n-gram language model features to prune away non-word candidates and select the candidate that best fits the context. Our final system outperforms the best prior published system when evaluated on the benchmark CoNLL-2014 test set. For better rep"
W17-5037,P06-1032,0,0.146359,"33 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics 2 Related Work Bouamor, 2015) and for pre-processing noisy input to an SMT system (Formiga and Fonollosa, 2012). GEC has gained popularity since the CoNLL-2014 (Ng et al., 2014) shared task was organized. Unlike previous shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013) that focused only on a few error types, the CoNLL-2014 shared task dealt with correction of all kinds of textual errors. The SMT approach, which was first used for correcting countability errors of mass nouns (Brockett et al., 2006), became popular during the CoNLL-2014 shared task. Two of the top three teams used this approach in their systems. It later became the most widely used approach and was used in state-of-the-art GEC systems (Susanto et al., 2014; Chollampatt et al., 2016b; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016). Neural machine translation approaches have also showed some promise (Xie et al., 2016; Yuan and Briscoe, 2016). A number of papers on GEC were published in 2016. Chollampatt et al. (2016b) showed that using neural network translation models in phrase-based SMT decoding improv"
W17-5037,P13-2071,0,0.0276671,"in transliteration and machine translation (Tiedemann, 2009; Nakov and Tiedemann, 2012; Durrani et al., 2014). It has been previously used for spelling correction in Arabic (Bougares and 3 Statistical Machine Translation We use the popular phrase-based SMT toolkit Moses (Koehn et al., 2007), which employs a loglinear model for combination of features. We use the task-specific tuning and features proposed in (Junczys-Dowmunt and Grundkiewicz, 2016) to further improve the system. The features include edit operation counts, a word class language model (WCLM), the Operation Sequence Model (OSM) (Durrani et al., 2013), and sparse edit operations. Moreover, Junczys-Dowmunt and Grundkiewicz (2016) trained a web-scale language model (LM) using large corpora from the Common Crawl data (Buck et al., 2014). We train an LM of similar size from the same corpora and use it to improve our GEC performance. 4 Neural Network Joint Models and Adaptation Following Chollampatt et al. (2016b), we add a neural network joint model (NNJM) feature to further improve the SMT component. We train the neural networks on GPUs using log-likelihood objective function with self-normalization, following (Devlin et al., 2014). Training"
W17-5037,P17-1074,0,0.0286243,"Missing"
W17-5037,E14-4029,0,0.0418076,"described a state-of-the-art SMT-based GEC system using task-specific features, better language models, and task-specific tuning of the SMT system. Their system achieved the best published score to date on the CoNLL2014 test set. We use the features proposed in their work to enhance the SMT component in our system as well. Additionally, we use neural network joint models (Devlin et al., 2014) introduced in (Chollampatt et al., 2016b) and a character-level SMT component. Character-level SMT systems are used in transliteration and machine translation (Tiedemann, 2009; Nakov and Tiedemann, 2012; Durrani et al., 2014). It has been previously used for spelling correction in Arabic (Bougares and 3 Statistical Machine Translation We use the popular phrase-based SMT toolkit Moses (Koehn et al., 2007), which employs a loglinear model for combination of features. We use the task-specific tuning and features proposed in (Junczys-Dowmunt and Grundkiewicz, 2016) to further improve the system. The features include edit operation counts, a word class language model (WCLM), the Operation Sequence Model (OSM) (Durrani et al., 2013), and sparse edit operations. Moreover, Junczys-Dowmunt and Grundkiewicz (2016) trained a"
W17-5037,W14-1702,0,0.0479913,"Missing"
W17-5037,buck-etal-2014-n,0,0.0534733,"Missing"
W17-5037,W12-2012,0,0.170521,"16. Chollampatt et al. (2016b) showed that using neural network translation models in phrase-based SMT decoding improves performance. Other works focused on re-ranking and combination of the n-best hypotheses produced by an SMT system using classifiers to generate better corrections (Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016). Rozovskaya and Roth (2016) compared the SMT and classifier approaches by performing error analysis of outputs and described a pipeline system using classifier-based error type-specific components, a context sensitive spelling correction system (Flor and Futagi, 2012), punctuation and casing correction systems, and SMT. Junczys-Dowmunt and Grundkiewicz (2016) described a state-of-the-art SMT-based GEC system using task-specific features, better language models, and task-specific tuning of the SMT system. Their system achieved the best published score to date on the CoNLL2014 test set. We use the features proposed in their work to enhance the SMT component in our system as well. Additionally, we use neural network joint models (Devlin et al., 2014) introduced in (Chollampatt et al., 2016b) and a character-level SMT component. Character-level SMT systems are"
W17-5037,D16-1195,1,0.90342,"Missing"
W17-5037,C12-2032,0,0.0162276,"ic tools and hence they can be trained for any language with adequate parallel data (i.e., erroneous and corrected sentence pairs). They are also capable of correcting complex errors which are difficult for classifier systems that target specific error types. The generalization of SMT-based GEC systems has been 327 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 327–333 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics 2 Related Work Bouamor, 2015) and for pre-processing noisy input to an SMT system (Formiga and Fonollosa, 2012). GEC has gained popularity since the CoNLL-2014 (Ng et al., 2014) shared task was organized. Unlike previous shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013) that focused only on a few error types, the CoNLL-2014 shared task dealt with correction of all kinds of textual errors. The SMT approach, which was first used for correcting countability errors of mass nouns (Brockett et al., 2006), became popular during the CoNLL-2014 shared task. Two of the top three teams used this approach in their systems. It later became the most widely used approach and was used in sta"
W17-5037,P14-2029,0,0.0254437,"To train the character-level SMT component, we obtain a corpus of misspelled words and their corrections2 , of which the misspellingcorrection pairs from Holbrook are used as the development set and the remaining pairs together with the unique words in the NUCLE training data (replicated on the source side to get parallel data) are used for training. We evaluate our system on the official CoNLL2014 test set, using the MaxMatch (Dahlmeier and Ng, 2012) scorer v3.2 which computes the F0.5 score, as well as on the JFLEG corpus (Napoles et al., 2017), an error-corrected subset of the GUG corpus (Heilman et al., 2014), using the F0.5 and GLEU (Napoles et al., 2015) metrics. 6.2 SMT-Based GEC System Our SMT-based GEC system uses a phrase table trained on the complete parallel data. In our word-level SMT system, we use two 5-gram LMs, one of them trained on the target side of the parallel training data and the other trained on Wikipedia texts (Wiki LM). We add all the dense features proposed in (Junczys-Dowmunt and Grundkiewicz, 2016) and sparse edit features on words (with one word context). We further improve the system by replacing Wiki LM with a 5gram LM trained on Common Crawl data (94BCC LM). NNJM is t"
W17-5037,N12-1067,1,0.215302,"s (i.e., a word) in the former is equivalent to a sequence of words (i.e., a sentence) in the latter. Input to our character-level SMT component is a sequence of characters that make up the unknown (misspelled) word and output is a list of correction candidates (words). Note that unknown words are words unseen in the source side of the parallel training data used to train the translation model. For training the character-level SMT component, alignments are computed based on a Levenshtein matrix, instead of using GIZA++ (Och and Ney, 2003). Our character-level SMT is tuned using the M2 metric (Dahlmeier and Ng, 2012) on characters, with character-level edit operation features and a 5-gram character LM. For each unknown word, character-level SMT produces 100 candidates that are then rescored to select the best candidate based on the context. This rescoring is done following Durrani et al. (2014) and uses word-level n-gram LM features: LM probability and the LM OOV (out-of-vocabulary) count denoting the number of words in the sentence that are not in the LM’s vocabulary. The architecture of our final system is shown in Figure 1. 6 6.1 (i.e., error-free sentences), are used for training. We extract the Engli"
W17-5037,W13-1703,1,0.367482,"(94BCC LM). NNJM is trained on the complete parallel data. We further adapt the NNJM following the adaptation method proposed by Chollampatt et al. (2016a) on sentences from the training portion of NUCLE that contain at least one error annotation (edit) in a sentence. We use the same hyper-parameters as (Chollampatt et al., 2016a). The SMT-based GEC system with all the features, 94BCC LM, and adapted NNJM, is referred to as “Word SMT-GEC”. Experiments Data and Evaluation The parallel data for training our word-level SMT system consist of two corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and Lang-8 Learner Corpora v2 (Lang-8) (Mizumoto et al., 2011). From NUCLE, we extract sentences with at least one annotation (edit) in a sentence. We use one-fourth of these sentences as our development data (5,458 sentences with 141,978 source tokens). The remainder of NUCLE, including sentences without annotations 2 329 http://www.dcs.bbk.ac.uk/∼ROGER/corpora.html System CoNLL-2014 Prec. Recall F0.5 55.96 22.54 43.16 58.24 24.84 45.90 61.02 27.80 49.25 61.65 29.11 50.39 62.14 30.92 51.70 SMT-GEC + dense + sparse features – Wiki LM + 94BCC LM + NNJM + adaptation [Word SMT-GEC] + Spelling SM"
W17-5037,D16-1161,0,0.660423,"models in phrase-based SMT decoding improves performance. Other works focused on re-ranking and combination of the n-best hypotheses produced by an SMT system using classifiers to generate better corrections (Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016). Rozovskaya and Roth (2016) compared the SMT and classifier approaches by performing error analysis of outputs and described a pipeline system using classifier-based error type-specific components, a context sensitive spelling correction system (Flor and Futagi, 2012), punctuation and casing correction systems, and SMT. Junczys-Dowmunt and Grundkiewicz (2016) described a state-of-the-art SMT-based GEC system using task-specific features, better language models, and task-specific tuning of the SMT system. Their system achieved the best published score to date on the CoNLL2014 test set. We use the features proposed in their work to enhance the SMT component in our system as well. Additionally, we use neural network joint models (Devlin et al., 2014) introduced in (Chollampatt et al., 2016b) and a character-level SMT component. Character-level SMT systems are used in transliteration and machine translation (Tiedemann, 2009; Nakov and Tiedemann, 2012;"
W17-5037,W12-2006,0,0.00858788,"ors which are difficult for classifier systems that target specific error types. The generalization of SMT-based GEC systems has been 327 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 327–333 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics 2 Related Work Bouamor, 2015) and for pre-processing noisy input to an SMT system (Formiga and Fonollosa, 2012). GEC has gained popularity since the CoNLL-2014 (Ng et al., 2014) shared task was organized. Unlike previous shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013) that focused only on a few error types, the CoNLL-2014 shared task dealt with correction of all kinds of textual errors. The SMT approach, which was first used for correcting countability errors of mass nouns (Brockett et al., 2006), became popular during the CoNLL-2014 shared task. Two of the top three teams used this approach in their systems. It later became the most widely used approach and was used in state-of-the-art GEC systems (Susanto et al., 2014; Chollampatt et al., 2016b; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016). Neural machine translatio"
W17-5037,P16-1208,0,0.0797015,"d tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013) that focused only on a few error types, the CoNLL-2014 shared task dealt with correction of all kinds of textual errors. The SMT approach, which was first used for correcting countability errors of mass nouns (Brockett et al., 2006), became popular during the CoNLL-2014 shared task. Two of the top three teams used this approach in their systems. It later became the most widely used approach and was used in state-of-the-art GEC systems (Susanto et al., 2014; Chollampatt et al., 2016b; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016). Neural machine translation approaches have also showed some promise (Xie et al., 2016; Yuan and Briscoe, 2016). A number of papers on GEC were published in 2016. Chollampatt et al. (2016b) showed that using neural network translation models in phrase-based SMT decoding improves performance. Other works focused on re-ranking and combination of the n-best hypotheses produced by an SMT system using classifiers to generate better corrections (Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016). Rozovskaya and Roth (2016) compared the SMT and classifier approaches by performing e"
W17-5037,P12-3005,0,0.0149878,"uces 100 candidates that are then rescored to select the best candidate based on the context. This rescoring is done following Durrani et al. (2014) and uses word-level n-gram LM features: LM probability and the LM OOV (out-of-vocabulary) count denoting the number of words in the sentence that are not in the LM’s vocabulary. The architecture of our final system is shown in Figure 1. 6 6.1 (i.e., error-free sentences), are used for training. We extract the English portion of Lang-8 by selecting sentences written by English learners via filtering using a language identification tool, langid.py (Lui and Baldwin, 2012). This filtered data set and the training portion of NUCLE are combined to form the training set, consisting of 2.21M sentences (26.77M source tokens and 30.87M target tokens). We use two corpora to train the LMs: Wikipedia texts (1.78B tokens) and a subset of the Common Crawl corpus (94B tokens). To train the character-level SMT component, we obtain a corpus of misspelled words and their corrections2 , of which the misspellingcorrection pairs from Holbrook are used as the development set and the remaining pairs together with the unique words in the NUCLE training data (replicated on the sourc"
W17-5037,D14-1102,1,0.667171,"since the CoNLL-2014 (Ng et al., 2014) shared task was organized. Unlike previous shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013) that focused only on a few error types, the CoNLL-2014 shared task dealt with correction of all kinds of textual errors. The SMT approach, which was first used for correcting countability errors of mass nouns (Brockett et al., 2006), became popular during the CoNLL-2014 shared task. Two of the top three teams used this approach in their systems. It later became the most widely used approach and was used in state-of-the-art GEC systems (Susanto et al., 2014; Chollampatt et al., 2016b; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016). Neural machine translation approaches have also showed some promise (Xie et al., 2016; Yuan and Briscoe, 2016). A number of papers on GEC were published in 2016. Chollampatt et al. (2016b) showed that using neural network translation models in phrase-based SMT decoding improves performance. Other works focused on re-ranking and combination of the n-best hypotheses produced by an SMT system using classifiers to generate better corrections (Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al."
W17-5037,I11-1017,0,0.0215374,"rther adapt the NNJM following the adaptation method proposed by Chollampatt et al. (2016a) on sentences from the training portion of NUCLE that contain at least one error annotation (edit) in a sentence. We use the same hyper-parameters as (Chollampatt et al., 2016a). The SMT-based GEC system with all the features, 94BCC LM, and adapted NNJM, is referred to as “Word SMT-GEC”. Experiments Data and Evaluation The parallel data for training our word-level SMT system consist of two corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and Lang-8 Learner Corpora v2 (Lang-8) (Mizumoto et al., 2011). From NUCLE, we extract sentences with at least one annotation (edit) in a sentence. We use one-fourth of these sentences as our development data (5,458 sentences with 141,978 source tokens). The remainder of NUCLE, including sentences without annotations 2 329 http://www.dcs.bbk.ac.uk/∼ROGER/corpora.html System CoNLL-2014 Prec. Recall F0.5 55.96 22.54 43.16 58.24 24.84 45.90 61.02 27.80 49.25 61.65 29.11 50.39 62.14 30.92 51.70 SMT-GEC + dense + sparse features – Wiki LM + 94BCC LM + NNJM + adaptation [Word SMT-GEC] + Spelling SMT 62.74 [Word&Char SMT-GEC] 32.96 Official Bryant and Ng (2015)"
W17-5037,2009.eamt-1.3,0,0.00819485,"MT. Junczys-Dowmunt and Grundkiewicz (2016) described a state-of-the-art SMT-based GEC system using task-specific features, better language models, and task-specific tuning of the SMT system. Their system achieved the best published score to date on the CoNLL2014 test set. We use the features proposed in their work to enhance the SMT component in our system as well. Additionally, we use neural network joint models (Devlin et al., 2014) introduced in (Chollampatt et al., 2016b) and a character-level SMT component. Character-level SMT systems are used in transliteration and machine translation (Tiedemann, 2009; Nakov and Tiedemann, 2012; Durrani et al., 2014). It has been previously used for spelling correction in Arabic (Bougares and 3 Statistical Machine Translation We use the popular phrase-based SMT toolkit Moses (Koehn et al., 2007), which employs a loglinear model for combination of features. We use the task-specific tuning and features proposed in (Junczys-Dowmunt and Grundkiewicz, 2016) to further improve the system. The features include edit operation counts, a word class language model (WCLM), the Operation Sequence Model (OSM) (Durrani et al., 2013), and sparse edit operations. Moreover,"
W17-5037,N16-1133,0,0.0315044,"as used in state-of-the-art GEC systems (Susanto et al., 2014; Chollampatt et al., 2016b; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016). Neural machine translation approaches have also showed some promise (Xie et al., 2016; Yuan and Briscoe, 2016). A number of papers on GEC were published in 2016. Chollampatt et al. (2016b) showed that using neural network translation models in phrase-based SMT decoding improves performance. Other works focused on re-ranking and combination of the n-best hypotheses produced by an SMT system using classifiers to generate better corrections (Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016). Rozovskaya and Roth (2016) compared the SMT and classifier approaches by performing error analysis of outputs and described a pipeline system using classifier-based error type-specific components, a context sensitive spelling correction system (Flor and Futagi, 2012), punctuation and casing correction systems, and SMT. Junczys-Dowmunt and Grundkiewicz (2016) described a state-of-the-art SMT-based GEC system using task-specific features, better language models, and task-specific tuning of the SMT system. Their system achieved the best published score to"
W17-5037,P12-2059,0,0.0305367,"nt and Grundkiewicz (2016) described a state-of-the-art SMT-based GEC system using task-specific features, better language models, and task-specific tuning of the SMT system. Their system achieved the best published score to date on the CoNLL2014 test set. We use the features proposed in their work to enhance the SMT component in our system as well. Additionally, we use neural network joint models (Devlin et al., 2014) introduced in (Chollampatt et al., 2016b) and a character-level SMT component. Character-level SMT systems are used in transliteration and machine translation (Tiedemann, 2009; Nakov and Tiedemann, 2012; Durrani et al., 2014). It has been previously used for spelling correction in Arabic (Bougares and 3 Statistical Machine Translation We use the popular phrase-based SMT toolkit Moses (Koehn et al., 2007), which employs a loglinear model for combination of features. We use the task-specific tuning and features proposed in (Junczys-Dowmunt and Grundkiewicz, 2016) to further improve the system. The features include edit operation counts, a word class language model (WCLM), the Operation Sequence Model (OSM) (Durrani et al., 2013), and sparse edit operations. Moreover, Junczys-Dowmunt and Grundk"
W17-5037,N16-1042,0,0.231929,"CoNLL-2014 shared task dealt with correction of all kinds of textual errors. The SMT approach, which was first used for correcting countability errors of mass nouns (Brockett et al., 2006), became popular during the CoNLL-2014 shared task. Two of the top three teams used this approach in their systems. It later became the most widely used approach and was used in state-of-the-art GEC systems (Susanto et al., 2014; Chollampatt et al., 2016b; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016). Neural machine translation approaches have also showed some promise (Xie et al., 2016; Yuan and Briscoe, 2016). A number of papers on GEC were published in 2016. Chollampatt et al. (2016b) showed that using neural network translation models in phrase-based SMT decoding improves performance. Other works focused on re-ranking and combination of the n-best hypotheses produced by an SMT system using classifiers to generate better corrections (Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016). Rozovskaya and Roth (2016) compared the SMT and classifier approaches by performing error analysis of outputs and described a pipeline system using classifier-based error type-specific components,"
W17-5037,W16-0530,0,0.0909801,"C systems (Susanto et al., 2014; Chollampatt et al., 2016b; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016). Neural machine translation approaches have also showed some promise (Xie et al., 2016; Yuan and Briscoe, 2016). A number of papers on GEC were published in 2016. Chollampatt et al. (2016b) showed that using neural network translation models in phrase-based SMT decoding improves performance. Other works focused on re-ranking and combination of the n-best hypotheses produced by an SMT system using classifiers to generate better corrections (Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016). Rozovskaya and Roth (2016) compared the SMT and classifier approaches by performing error analysis of outputs and described a pipeline system using classifier-based error type-specific components, a context sensitive spelling correction system (Flor and Futagi, 2012), punctuation and casing correction systems, and SMT. Junczys-Dowmunt and Grundkiewicz (2016) described a state-of-the-art SMT-based GEC system using task-specific features, better language models, and task-specific tuning of the SMT system. Their system achieved the best published score to date on the CoNLL2"
W17-5037,P15-2097,0,0.288432,"obtain a corpus of misspelled words and their corrections2 , of which the misspellingcorrection pairs from Holbrook are used as the development set and the remaining pairs together with the unique words in the NUCLE training data (replicated on the source side to get parallel data) are used for training. We evaluate our system on the official CoNLL2014 test set, using the MaxMatch (Dahlmeier and Ng, 2012) scorer v3.2 which computes the F0.5 score, as well as on the JFLEG corpus (Napoles et al., 2017), an error-corrected subset of the GUG corpus (Heilman et al., 2014), using the F0.5 and GLEU (Napoles et al., 2015) metrics. 6.2 SMT-Based GEC System Our SMT-based GEC system uses a phrase table trained on the complete parallel data. In our word-level SMT system, we use two 5-gram LMs, one of them trained on the target side of the parallel training data and the other trained on Wikipedia texts (Wiki LM). We add all the dense features proposed in (Junczys-Dowmunt and Grundkiewicz, 2016) and sparse edit features on words (with one word context). We further improve the system by replacing Wiki LM with a 5gram LM trained on Common Crawl data (94BCC LM). NNJM is trained on the complete parallel data. We further"
W17-5037,E17-2037,0,0.0788686,"(1.78B tokens) and a subset of the Common Crawl corpus (94B tokens). To train the character-level SMT component, we obtain a corpus of misspelled words and their corrections2 , of which the misspellingcorrection pairs from Holbrook are used as the development set and the remaining pairs together with the unique words in the NUCLE training data (replicated on the source side to get parallel data) are used for training. We evaluate our system on the official CoNLL2014 test set, using the MaxMatch (Dahlmeier and Ng, 2012) scorer v3.2 which computes the F0.5 score, as well as on the JFLEG corpus (Napoles et al., 2017), an error-corrected subset of the GUG corpus (Heilman et al., 2014), using the F0.5 and GLEU (Napoles et al., 2015) metrics. 6.2 SMT-Based GEC System Our SMT-based GEC system uses a phrase table trained on the complete parallel data. In our word-level SMT system, we use two 5-gram LMs, one of them trained on the target side of the parallel training data and the other trained on Wikipedia texts (Wiki LM). We add all the dense features proposed in (Junczys-Dowmunt and Grundkiewicz, 2016) and sparse edit features on words (with one word context). We further improve the system by replacing Wiki L"
W17-5037,W14-1701,1,0.274505,"data (i.e., erroneous and corrected sentence pairs). They are also capable of correcting complex errors which are difficult for classifier systems that target specific error types. The generalization of SMT-based GEC systems has been 327 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 327–333 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics 2 Related Work Bouamor, 2015) and for pre-processing noisy input to an SMT system (Formiga and Fonollosa, 2012). GEC has gained popularity since the CoNLL-2014 (Ng et al., 2014) shared task was organized. Unlike previous shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013) that focused only on a few error types, the CoNLL-2014 shared task dealt with correction of all kinds of textual errors. The SMT approach, which was first used for correcting countability errors of mass nouns (Brockett et al., 2006), became popular during the CoNLL-2014 shared task. Two of the top three teams used this approach in their systems. It later became the most widely used approach and was used in state-of-the-art GEC systems (Susanto et al., 2014; Chollampatt et al"
W17-5037,W13-3601,1,0.86837,"cult for classifier systems that target specific error types. The generalization of SMT-based GEC systems has been 327 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 327–333 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics 2 Related Work Bouamor, 2015) and for pre-processing noisy input to an SMT system (Formiga and Fonollosa, 2012). GEC has gained popularity since the CoNLL-2014 (Ng et al., 2014) shared task was organized. Unlike previous shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013) that focused only on a few error types, the CoNLL-2014 shared task dealt with correction of all kinds of textual errors. The SMT approach, which was first used for correcting countability errors of mass nouns (Brockett et al., 2006), became popular during the CoNLL-2014 shared task. Two of the top three teams used this approach in their systems. It later became the most widely used approach and was used in state-of-the-art GEC systems (Susanto et al., 2014; Chollampatt et al., 2016b; JunczysDowmunt and Grundkiewicz, 2016; Rozovskaya and Roth, 2016). Neural machine translation approaches have"
W17-5037,J03-1002,0,0.0210205,"nent is equivalent to a word in wordlevel SMT, and a sequence of characters (i.e., a word) in the former is equivalent to a sequence of words (i.e., a sentence) in the latter. Input to our character-level SMT component is a sequence of characters that make up the unknown (misspelled) word and output is a list of correction candidates (words). Note that unknown words are words unseen in the source side of the parallel training data used to train the translation model. For training the character-level SMT component, alignments are computed based on a Levenshtein matrix, instead of using GIZA++ (Och and Ney, 2003). Our character-level SMT is tuned using the M2 metric (Dahlmeier and Ng, 2012) on characters, with character-level edit operation features and a 5-gram character LM. For each unknown word, character-level SMT produces 100 candidates that are then rescored to select the best candidate based on the context. This rescoring is done following Durrani et al. (2014) and uses word-level n-gram LM features: LM probability and the LM OOV (out-of-vocabulary) count denoting the number of words in the sentence that are not in the LM’s vocabulary. The architecture of our final system is shown in Figure 1."
W17-5037,P07-2045,0,\N,Missing
W17-5037,W15-3221,0,\N,Missing
W17-5037,P15-1068,1,\N,Missing
W17-5037,W11-2838,0,\N,Missing
W19-5002,D15-1044,0,0.012091,"in Equation 8. (Sum(X, C)) as shown in Equation 6. R = Sum(H, X + C) = H + X + C (6) To be able to perform the sum operation as shown above, the dimension of the word embedding vectors (dLT ), output vectors of the convolution layer (dc ), and output vectors of the hidden RNN layer (dr ) have to be equal. Attention layer: Visualizing the learned model is of high importance in the medical domain. By using an attention mechanism, we can show the degree of importance of words and phrases. Attention mechanism has been successful in many recent studies (Bahdanau et al., 2015; Hermann et al., 2015; Rush et al., 2015). The outputs of the previous residual layer R = (r1 , r2 , ..., rM ) are used as inputs of the attention layer. In other words, this layer receives M vectors of size dr , where dr is the output dimension of the recurrent layer. R is a rich representation of the words in the ED note using a combination of word embeddings, CNN outputs, and RNN outputs. Each vector rt is multiplied by a learnable real-valued weight s0t between 0 and 1 before adding the elements of all M vectors into a single vector a as a form of weighted average. The functions of the attention layer are defined in Equation 7. s"
W19-5002,D16-1193,1,0.83602,"(LSTM) (Hochreiter and Schmidhuber, 1997). Based on our experimental results, LSTM outperforms the other two units and hence we only use LSTM as our RNN unit. LSTM is able to learn to preserve or forget inht = ot ◦ tanh(ct ) xt is the input vector at time t. LSTM produces one vector ht at each time step t (h0 is the zero vector). Wi , Wf , Wc , Wo , Ui , Uf , Uc , Uo are weight matrices and bi , bf , bc , bo are the bias vectors. The circle symbol ◦ denotes element-wise multiplication and σ denotes the sigmoid function. The output of the recurrent layer is H = (h1 , h2 , ..., hM ). Following (Taghipour and Ng, 2016), we use every output of the intermediate states of the RNN and perform summing (residual) and then pooling in the next layer to have a better representation of the entire ED note. Residual Layer: We perform the sum operation on the sequence of the output vectors from the recurrent layer (H = h1 , h2 , ..., hM ) and the output vectors of the previous residual layer 13 will be [a, l], the concatenation of a and l, where l contains the additional real-valued features which will be described in the next subsection. The linear layer maps the input vector into a single scalar value. This mapping is"
W19-5002,W16-4214,1,0.767653,"sists of about 180,000 ED notes and DS pairs. Each ED note contains 440 words on average. The ED notes are written in sentence fragments and point forms, and very often contain abbreviations, symbols, and misspelled words. This adds to the difficulty in diagnosing appendicitis. Moreover, the free-text ED notes contain patients’ personal health information (PHI) such as name, identification number, and contact number. The ED notes need to be anonymized (by removing the PHI) before they are used for research purposes. We have developed a simple and efficient algorithm to anonymize the ED notes (Yuwono et al., 2016) and it is used in this work. 2.2 precision × recall precision + recall precision × recall = (1 + 0.52 ) × 2 (0.5 × precision) + recall (1) F1 = 2 × 3 3.1 Neural Networks Model Architecture We have created a novel neural network architecture named convolutional residual recurrent neural network (CR2). Our architecture is illustrated in Figure 1. Lookup Table Layer: The first layer of our neural network projects each word into a dLT dimensional space. Given a sequence of words W represented by their one-hot representations (w1 , w2 , ..., wM ), the output of the lookup table layer (LT ) is give"
W19-5002,D18-1456,1,0.88975,"Missing"
W97-0201,J94-4003,0,0.0802862,"Missing"
W97-0201,P96-1042,0,0.060622,"Missing"
W97-0201,P92-1032,0,0.0561253,"39 occurrences of these 191 words that occur in 6 text files of the Wall Street Journal corpus. The performance figures of LEXAS in Table 1 are higher than those reported in (Ng and Lee, 1996). The classification accuracy of the nearest neighbor algorithm used by LEXAS (Cost and Salzberg, 1993) is quite sensitive to the number of nearest neighbors used to select the best matching example. By using 10-fold cross validation (Kohavi and John, 1995) to automatically pick the best number of nearest neighbors to use, the performance of LSXAShas improved. 4 Word Sense Disambiguation in the Large In (Gale et al., 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. The performance of LEXAS as indicated in Table 1 is significantly better than the most-frequent-sense classifier for the set of 191 words collected in our corpus. Figure 1 and 2 also confirm that all the training examples collected in our corpus are effectively utilized by LEXAS to improve its WSD performance. This is encouraging as it demonstrates the feasibility of building a wide coverage WSD program using a supervised"
W97-0201,P94-1020,0,0.0337675,"ontexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes. This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in (Ng and Lee, 1996). We can readily collapse the refined senses of WORDNET into a smaller set if only a coarse (ho3 The Effect of Training Corpus Size A number of past research work on WSD, such as (Leacock et al., 1993; Bruce and Wiebe, 1994; Mooney, 1996), were tested on a small number of words like ""line"" and ""interest"". Similarly, (Yarowsky, 1995) tested his WSD algorithm on a dozen words. The sense-tagged corpus SEMCOI~, prepared by (Miller et al., 1994), contains a substantial subset of the Brown corpus tagged with the refined senses of WORDNET. However, as reported in (Miller et al., 1994), there are not enough training examples per word in SP.MCOR to yield a broad coverage, high accuracy WSD program, due to the fact that sense tagging is done on every word in a t I will only focus on common noun in this paper and ignore pr"
W97-0201,H93-1051,0,0.03718,"ted, since homograph contexts are quite distinct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes. This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in (Ng and Lee, 1996). We can readily collapse the refined senses of WORDNET into a smaller set if only a coarse (ho3 The Effect of Training Corpus Size A number of past research work on WSD, such as (Leacock et al., 1993; Bruce and Wiebe, 1994; Mooney, 1996), were tested on a small number of words like ""line"" and ""interest"". Similarly, (Yarowsky, 1995) tested his WSD algorithm on a dozen words. The sense-tagged corpus SEMCOI~, prepared by (Miller et al., 1994), contains a substantial subset of the Brown corpus tagged with the refined senses of WORDNET. However, as reported in (Miller et al., 1994), there are not enough training examples per word in SP.MCOR to yield a broad coverage, high accuracy WSD program, due to the fact that sense tagging is done on every word in a t I will only focus on common noun in t"
W97-0201,J93-1001,0,0.0196097,"I also estimate the amount of human sense-tagged corpus and the manual annotation effort needed to build a largescale, broad coverage word sense disambiguation program which can significantly outperform the most-frequent-sense classifier. Finally, I suggest that intelligent example selection techniques may significantly reduce the amount of sense-tagged corpus needed and offer this research problem as a fruitful area for word sense disambiguation research. 1 Introduction Much recent research in the field of natural language processing (NLP) has focused on an empirical, corpus-based approach (Church and Mercer, 1993). The high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by (Church, 1988), has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation (WSD). The availability of large quantities of part-ofspeech tagged and syntactically parsed sentences like the Penn Treebank corpus (Marcus, Santorini, and Marcinkiewicz, 1993) has contributed greatly to the development of robust, broad coverage partof-speech taggers and syntactic parsers. The Penn Treebank corpus c"
W97-0201,J93-2004,0,0.0262247,"ambiguation research. 1 Introduction Much recent research in the field of natural language processing (NLP) has focused on an empirical, corpus-based approach (Church and Mercer, 1993). The high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by (Church, 1988), has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation (WSD). The availability of large quantities of part-ofspeech tagged and syntactically parsed sentences like the Penn Treebank corpus (Marcus, Santorini, and Marcinkiewicz, 1993) has contributed greatly to the development of robust, broad coverage partof-speech taggers and syntactic parsers. The Penn Treebank corpus contains a sufficient number of partof-speech tagged and syntactically parsed sentences to serve as adequate training material for building broad coverage part-of-speech taggers and parsers. Unfortunately, an analogous sense-tagged corpus large enough to achieve broad coverage, high accuracy word sense disambiguation is not available at present. In this paper, I argue that, given the current state-of-the-art capability of automated machine learning algori"
W97-0201,H94-1046,0,0.0143856,"tance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in (Ng and Lee, 1996). We can readily collapse the refined senses of WORDNET into a smaller set if only a coarse (ho3 The Effect of Training Corpus Size A number of past research work on WSD, such as (Leacock et al., 1993; Bruce and Wiebe, 1994; Mooney, 1996), were tested on a small number of words like ""line"" and ""interest"". Similarly, (Yarowsky, 1995) tested his WSD algorithm on a dozen words. The sense-tagged corpus SEMCOI~, prepared by (Miller et al., 1994), contains a substantial subset of the Brown corpus tagged with the refined senses of WORDNET. However, as reported in (Miller et al., 1994), there are not enough training examples per word in SP.MCOR to yield a broad coverage, high accuracy WSD program, due to the fact that sense tagging is done on every word in a t I will only focus on common noun in this paper and ignore proper noun. 2 Test set BC50 WSJ6 running text in SEMCOR. To overcome this data sparseness problem of W S D , I initiateda mini-project in sense tagging and collected a corpus in which 192,800 occurrences of 191 words have"
W97-0201,W96-0208,0,0.0225956,"nct and hence it is a much simpler task to disambiguate among a small number of coarse sense classes. This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in (Ng and Lee, 1996). We can readily collapse the refined senses of WORDNET into a smaller set if only a coarse (ho3 The Effect of Training Corpus Size A number of past research work on WSD, such as (Leacock et al., 1993; Bruce and Wiebe, 1994; Mooney, 1996), were tested on a small number of words like ""line"" and ""interest"". Similarly, (Yarowsky, 1995) tested his WSD algorithm on a dozen words. The sense-tagged corpus SEMCOI~, prepared by (Miller et al., 1994), contains a substantial subset of the Brown corpus tagged with the refined senses of WORDNET. However, as reported in (Miller et al., 1994), there are not enough training examples per word in SP.MCOR to yield a broad coverage, high accuracy WSD program, due to the fact that sense tagging is done on every word in a t I will only focus on common noun in this paper and ignore proper noun. 2 Te"
W97-0201,P96-1006,1,0.924095,"ore 118230 Republic of Singapore nhweet ou@dso, gov. sg Abstract Recent advances in large-scale, broad coverage part-of-speech tagging and syntactic parsing have been achieved in no small part due to the availability of large amounts of online, human-annotated corpora. In this paper, I argue that a large, human sensetagged corpus is also critical as well as necessary to achieve broad coverage, high accuracy word sense disambiguation, where the sense distinction is at the level of a good desk-top dictionary such as WORDNET. Using the sense-tagged corpus of 192,800 word occurrences reported in (Ng and Lee, 1996), I examine the effect of the number of training examples on the accuracy of an exemplar-based classifier versus the base-line, most-frequent-sense classitier. I also estimate the amount of human sense-tagged corpus and the manual annotation effort needed to build a largescale, broad coverage word sense disambiguation program which can significantly outperform the most-frequent-sense classifier. Finally, I suggest that intelligent example selection techniques may significantly reduce the amount of sense-tagged corpus needed and offer this research problem as a fruitful area for word sense disa"
W97-0201,P95-1026,0,0.198641,"Missing"
W97-0323,P94-1020,0,0.113971,"a large sense-tagged corpus first used in (Ng and Lee, 1996). The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. 1 Introduction Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word &quot;line&quot;. The seven algorithms that he evaluated are: a Naive-Bayes classifier (Duda and Hart, 1973), a perceptron (Rosenblatt, 1958), a decisiontree learner (Quinlan, 1993), a k nearest-neighbor classifier (exemplar-based learner) (Cover and Hart, 208 On the ot"
W97-0323,P92-1032,0,0.754364,"weetou@dso, o r g . s g Abstract 1967), logic-based DNF and CNF learners (Mooney, 1995), and a decision-list learner (Rivest, 1987). His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the &quot;line&quot; corpus tested. Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks (Clark and Niblett, 1989; Kohavi, 1996). This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation. In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus"
W97-0323,H93-1051,0,0.0744409,"ave obtained improved disambiguation accuracy on a large sense-tagged corpus first used in (Ng and Lee, 1996). The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. 1 Introduction Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word &quot;line&quot;. The seven algorithms that he evaluated are: a Naive-Bayes classifier (Duda and Hart, 1973), a perceptron (Rosenblatt, 1958), a decisiontree learner (Quinlan, 1993), a k nearest-neighbor classifier (exemp"
W97-0323,H94-1046,0,0.0589807,"Missing"
W97-0323,W96-0208,0,0.357624,"the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in (Ng and Lee, 1996). The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. 1 Introduction Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven"
W97-0323,P96-1006,1,0.306489,"1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation. In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in (Ng and Lee, 1996). The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. 1 Introduction Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995"
W97-0323,C92-2070,0,0.564148,"logic-based DNF and CNF learners (Mooney, 1995), and a decision-list learner (Rivest, 1987). His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the &quot;line&quot; corpus tested. Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks (Clark and Niblett, 1989; Kohavi, 1996). This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation. In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in (Ng and Lee, 1996). The"
W97-0323,H93-1052,0,0.0248563,"urrences of the 191 words appearing in 6 text files of the WSJ corpus. Both test sets are identical to the ones reported in (Ng and Lee, 1996). Since the primary aim of our present study is the comparative evaluation of learning algorithms, not feature representation, we have chosen, for simplicity, to use local collocations as the only features in the example representation. Local collocations have been found to be the single most informative set of features for WSD (Ng and Lee, 1996). T h a t local collocation knowledge provides important clues to WSD has also been pointed out previously by Yarowsky (1993). Let w be the word to be disambiguated, and let 12 ll w rl r2 be the sentence fragment containing w. In the present study, we used seven features in the representation of an example, which are the local collocations of the surrounding 4 words. These seven features are: 12-11, ll-rl, rl-r2, ll, rl, 12, and r2. The first three features are concatenation of two words. 2 The experimental results obtained are tabulated in Table 1. The first three rows of accuracy fig1This corpus is available from the Linguistic Data Consortium (LDC). Contact the LDC at ldc@unagi.cis.upenn.edu for details. 2The fir"
W97-0323,P94-1013,0,0.130386,"assifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. 1 Introduction Much recent research on word sense disambiguation (WSD) has adopted a corpus-based, learning approach. Many different learning approaches have been used, including neural networks (Leacock et al., 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al., 1992a; Gale et al., 1995; Leacock et al., 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word &quot;line&quot;. The seven algorithms that he evaluated are: a Naive-Bayes classifier (Duda and Hart, 1973), a perceptron (Rosenblatt, 1958), a decisiontree learner (Quinlan, 1993), a k nearest-neighbor classifier (exemplar-based learner) (Cover and Hart, 208 On the other hand, our past work on WSD (Ng and Lee, 1996) used an exemplar-based (or nearest neighbor) learning approac"
W99-0502,W98-1507,0,0.0760175,"ce contains the word w Assume that w has M senses Let 4 be the number of sentences which are assigned identical sense b~ two human annotators T h e n a simple measure to quantify the agreement rate between two human annotators Is Pc, where Pc, = A / N The drawback of this simple measure is that it does not take into account chance agreement between two annotators The Kappa statistic a (Cohen, 1960) is a better measure of rater-annotator agreement which takes into account the effect of chance agreement It has been used recently w~thm computatmnal hngu~stlcs to measure raterannotator agreement (Bruce and Wmbe, 1998, Carletta, 1996, Veroms, 1998) Let Cj be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences whmh have been assigned sense 3 by annotator 2 Then P~-P~ 1-P~ where M j=l and Pe measures the chance agreement between two annotators A Kappa ~alue of 0 indicates that the agreement is purely due to chance agreement, whereas a Kappa ~alue of 1 indicates perfect agreement A Kappa ~alue of 0 8 and above is considered as mdmatmg good agreement (Carletta, 1996) Table 1 summarizes the inter-annotator agreement on the mtersected corpus The first (b"
W99-0634,P95-1017,0,0.387386,"d entities ""USAir"" and ""Piedmont"" in the expression ""USAir and Piedmont"" but instead treat it as one single named entity. Also, some of the features such as number agreement, gender agreement and semantic class agreement are difficult to determine at times. For example, ""they"" is sometimes used to refer to ""the government"" even though superficially both do not seem to agree in number. All these problems hurt the performance of the coreference engine. 4 didate noun phrases and has to deal with the inevitable noisy data when mistakes occur in noun phrase identification. Also, the evaluation of (Aone and Bennett, 1995) and (McCarthy and Lehnert, 1995) only focused on specific types of noun phrases (organizations and business entities), and (Aone and Bennett, 1995) dealt only with Japanese texts. Our evaluation was done on all types of English noun phrases instead. None of the systems in MUC-7 adopted a learning approach to coreference resolution (Chinchor, 1998). Among the MUC-6 systems, the only one that we can directly compare to is the UMass system, which also used C4.5 for coreference resolution. The other MUC-6 systems were not based on a learning approach. The score of the UMass system is not high com"
W99-0634,W97-1306,0,0.0450906,"UMass system, and the difference is statistically significant at p = 0.05. Thus, the contribution of our work is in showing that a learning approach, when evaluated on a common coreference data set, is able to achieve accuracy competitive with state-of-theart systems using non-learning approaches. Related Work There is a long tradition of work on coreference resolution within computational linguistics, but most of them are not subjected to empirical evaluation until recently. Among the work that reported quantitative evaluation results, most are not based on learning from an annotated corpus (Baldwin, 1997; Kameyama, 1997; Lappin and Leass, 1994; Mitkov, 1997). To our knowledge, the work of (Aone and Bennett, 1995; Ge et al., 1998; McCarthy and Lehnert, 1995) are the only ones that are based on learning from an annotated corpus. Ge et al. (Ge et al., 1998) used a statistical model for resolving pronouns. In contrast, we used a decision tree learning algorithm and resolved general noun phrases, not just pronouns. Both the work of (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) employed decision tree learning. However, the features they used include domain-specific ones like DNP-F (definite"
W99-0634,A97-1029,0,0.116464,"Missing"
W99-0634,J94-4002,0,0.0547177,"e is statistically significant at p = 0.05. Thus, the contribution of our work is in showing that a learning approach, when evaluated on a common coreference data set, is able to achieve accuracy competitive with state-of-theart systems using non-learning approaches. Related Work There is a long tradition of work on coreference resolution within computational linguistics, but most of them are not subjected to empirical evaluation until recently. Among the work that reported quantitative evaluation results, most are not based on learning from an annotated corpus (Baldwin, 1997; Kameyama, 1997; Lappin and Leass, 1994; Mitkov, 1997). To our knowledge, the work of (Aone and Bennett, 1995; Ge et al., 1998; McCarthy and Lehnert, 1995) are the only ones that are based on learning from an annotated corpus. Ge et al. (Ge et al., 1998) used a statistical model for resolving pronouns. In contrast, we used a decision tree learning algorithm and resolved general noun phrases, not just pronouns. Both the work of (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) employed decision tree learning. However, the features they used include domain-specific ones like DNP-F (definite NP whose referent is a facility) (Aone a"
W99-0634,W97-1303,0,0.0217844,"ficant at p = 0.05. Thus, the contribution of our work is in showing that a learning approach, when evaluated on a common coreference data set, is able to achieve accuracy competitive with state-of-theart systems using non-learning approaches. Related Work There is a long tradition of work on coreference resolution within computational linguistics, but most of them are not subjected to empirical evaluation until recently. Among the work that reported quantitative evaluation results, most are not based on learning from an annotated corpus (Baldwin, 1997; Kameyama, 1997; Lappin and Leass, 1994; Mitkov, 1997). To our knowledge, the work of (Aone and Bennett, 1995; Ge et al., 1998; McCarthy and Lehnert, 1995) are the only ones that are based on learning from an annotated corpus. Ge et al. (Ge et al., 1998) used a statistical model for resolving pronouns. In contrast, we used a decision tree learning algorithm and resolved general noun phrases, not just pronouns. Both the work of (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) employed decision tree learning. However, the features they used include domain-specific ones like DNP-F (definite NP whose referent is a facility) (Aone and Bennett, 199"
W99-0634,M95-1002,0,0.0270958,"ch on a common data set, the MUC-6 coreference corpus. We obtained encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to non-learning approaches. 1 Introduction Coreference resolution refers to the process of determining if two expressions in natural language refer to the same entity in the world. It is an important subtask in natural language processing systems. In particular, information extraction (IE) systems like those built in the DAI:tPA Message Understanding Conferences (Chinchor, 1998; Sundheim, 1995) have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6 (Committee, 1995). In this paper, we focus on the task of determining coreference relations as defined in MUC6 (Committee, 1995). Specifically, a coreference relation denotes an identity of reference and holds between two textual elements known as markables, which are nouns, noun phrases, or pronouns. Thus, our coreference task resolves general noun phrases and not just pronouns, unlike in some previous work on anaphora resolution."
W99-0634,A88-1019,0,0.22083,"Missing"
W99-0634,M95-1025,0,0.139486,"ise and achieves accuracy comparable to non-learning approaches. 1 Introduction Coreference resolution refers to the process of determining if two expressions in natural language refer to the same entity in the world. It is an important subtask in natural language processing systems. In particular, information extraction (IE) systems like those built in the DAI:tPA Message Understanding Conferences (Chinchor, 1998; Sundheim, 1995) have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6 (Committee, 1995). In this paper, we focus on the task of determining coreference relations as defined in MUC6 (Committee, 1995). Specifically, a coreference relation denotes an identity of reference and holds between two textual elements known as markables, which are nouns, noun phrases, or pronouns. Thus, our coreference task resolves general noun phrases and not just pronouns, unlike in some previous work on anaphora resolution. The ability to link co-referring noun phrases both within and across sentences is critical to discourse analysis and language understanding in general. 2 A Learning Approach for Cor"
W99-0634,M95-1011,0,0.0254254,"tities), and (Aone and Bennett, 1995) dealt only with Japanese texts. Our evaluation was done on all types of English noun phrases instead. None of the systems in MUC-7 adopted a learning approach to coreference resolution (Chinchor, 1998). Among the MUC-6 systems, the only one that we can directly compare to is the UMass system, which also used C4.5 for coreference resolution. The other MUC-6 systems were not based on a learning approach. The score of the UMass system is not high compared to the rest of the MUC-6 systems. In particular, the system&apos;s recall is relatively low. As explained in (Fisher et al., 1995), the reason for this is that it only concentrated on coreference relationships among references to people and organizations. Our system, as opposed to the UMass system, considered all types of markables. The score of our system is higher than that of the UMass system, and the difference is statistically significant at p = 0.05. Thus, the contribution of our work is in showing that a learning approach, when evaluated on a common coreference data set, is able to achieve accuracy competitive with state-of-theart systems using non-learning approaches. Related Work There is a long tradition of wor"
W99-0634,W98-1119,0,0.0631888,"hat a learning approach, when evaluated on a common coreference data set, is able to achieve accuracy competitive with state-of-theart systems using non-learning approaches. Related Work There is a long tradition of work on coreference resolution within computational linguistics, but most of them are not subjected to empirical evaluation until recently. Among the work that reported quantitative evaluation results, most are not based on learning from an annotated corpus (Baldwin, 1997; Kameyama, 1997; Lappin and Leass, 1994; Mitkov, 1997). To our knowledge, the work of (Aone and Bennett, 1995; Ge et al., 1998; McCarthy and Lehnert, 1995) are the only ones that are based on learning from an annotated corpus. Ge et al. (Ge et al., 1998) used a statistical model for resolving pronouns. In contrast, we used a decision tree learning algorithm and resolved general noun phrases, not just pronouns. Both the work of (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) employed decision tree learning. However, the features they used include domain-specific ones like DNP-F (definite NP whose referent is a facility) (Aone and Bennett, 1995), JV-CHILD-i (does i refer to a joint venture formed as the result of"
W99-0634,M98-1001,0,\N,Missing
W99-0634,W97-1307,0,\N,Missing
W99-0634,M95-1006,0,\N,Missing
