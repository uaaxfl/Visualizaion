2021.scil-1.29,What{'}s in a Span? Evaluating the Creativity of a Span-Based Neural Constituency Parser,2021,-1,-1,2,1,2236,daniel dakota,Proceedings of the Society for Computation in Linguistics 2021,0,None
2021.latechclfl-1.19,Period Classification in {C}hinese Historical Texts,2021,-1,-1,2,1,5504,zuoyu tian,"Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"In this study, we study language change in Chinese Biji by using a classification task: classifying Ancient Chinese texts by time periods. Specifically, we focus on a unique genre in classical Chinese literature: Biji (literally {``}notebook{''} or {``}brush notes{''}), i.e., collections of anecdotes, quotations, etc., anything authors consider noteworthy, Biji span hundreds of years across many dynasties and conserve informal language in written form. For these reasons, they are regarded as a good resource for investigating language change in Chinese (Fang, 2010). In this paper, we create a new dataset of 108 Biji across four dynasties. Based on the dataset, we first introduce a time period classification task for Chinese. Then we investigate different feature representation methods for classification. The results show that models using contextualized embeddings perform best. An analysis of the top features chosen by the word n-gram model (after bleaching proper nouns) confirms that these features are informative and correspond to observations and assumptions made by historical linguists."
2021.iwpt-1.10,Bidirectional Domain Adaptation Using Weighted Multi-Task Learning,2021,-1,-1,3,1,2236,daniel dakota,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),0,"Domain adaption in syntactic parsing is still a significant challenge. We address the issue of data imbalance between the in-domain and out-of-domain treebank typically used for the problem. We define domain adaptation as a Multi-task learning (MTL) problem, which allows us to train two parsers, one for each do-main. Our results show that the MTL approach is beneficial for the smaller treebank. For the larger treebank, we need to use loss weighting in order to avoid a decrease in performance be-low the single task. In order to determine towhat degree the data imbalance between two domains and the domain differences affect results, we also carry out an experiment with two imbalanced in-domain treebanks and show that loss weighting also improves performance in an in-domain setting. Given loss weighting in MTL, we can improve results for both parsers."
2020.udw-1.23,{U}niversal {D}ependency Treebank for {X}ibe,2020,-1,-1,3,0,8274,he zhou,Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020),0,"We present our work of constructing the first treebank for the Xibe language following the Universal Dependencies (UD) annotation scheme. Xibe is a low-resourced and severely endangered Tungusic language spoken by the Xibe minority living in the Xinjiang Uygur Autonomous Region of China. We collected 810 sentences so far, including 544 sentences from a grammar book on written Xibe and 266 sentences from Cabcal News. We annotated those sentences manually from scratch. In this paper, we report the procedure of building this treebank and analyze several important annotation issues of our treebank. Finally, we propose our plans for future work."
2020.tlt-1.4,Fine-Grained Morpho-Syntactic Analysis for the Under-Resourced Language Chaghatay,2020,-1,-1,4,1,14345,kenneth steimel,Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories,0,None
2020.lrec-1.625,Offensive Language Detection Using Brown Clustering,2020,-1,-1,2,1,5504,zuoyu tian,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this study, we investigate the use of Brown clustering for offensive language detection. Brown clustering has been shown to be of little use when the task involves distinguishing word polarity in sentiment analysis tasks. In contrast to previous work, we train Brown clusters separately on positive and negative sentiment data, but then combine the information into a single complex feature per word. This way of representing words results in stable improvements in offensive language detection, when used as the only features or in combination with words or character n-grams. Brown clusters add important information, even when combined with words or character n-grams or with standard word embeddings in a convolutional neural network. However, we also found different trends between the two offensive language data sets we used."
2020.findings-emnlp.314,{OCNLI}: {O}riginal {C}hinese {N}atural {L}anguage {I}nference,2020,-1,-1,5,0.588235,8273,hai hu,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Despite the tremendous recent progress on natural language inference (NLI), driven largely by large-scale investment in new datasets (e.g.,SNLI, MNLI) and advances in modeling, most progress has been limited to English due to a lack of reliable datasets for most of the world{'}s languages. In this paper, we present the first large-scale NLI dataset (consisting of {\textasciitilde}56,000 annotated sentence pairs) for Chinese called the Original Chinese Natural Language Inference dataset (OCNLI). Unlike recent attempts at extending NLI to other languages, our dataset does not rely on any automatic translation or non-expert annotation. Instead, we elicit annotations from native speakers specializing in linguistics. We follow closely the annotation protocol used for MNLI, but create new strategies for eliciting diverse hypotheses. We establish several baseline results on our dataset using state-of-the-art pre-trained models for Chinese, and find even the best performing models to be far outpaced by human performance ({\textasciitilde}12{\%} absolute performance gap), making it a challenging new resource that we hope will help to accelerate progress in Chinese NLU. To the best of our knowledge, this is the first human-elicited MNLI-style corpus for a non-English language."
2020.alw-1.9,Investigating Sampling Bias in Abusive Language Detection,2020,-1,-1,2,0,22394,dante razo,Proceedings of the Fourth Workshop on Online Abuse and Harms,0,"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported here, we reproduce the investigation of Wiegand et al. (2019) to determine differences between different sampling strategies. They compared boosted random sampling, where abusive posts are upsampled, and biased topic sampling, which focuses on topics that are known to cause abusive language. Instead of comparing individual datasets created using these sampling strategies, we use the sampling strategies on a single, large dataset, thus eliminating the textual source of the dataset as a potential confounding factor. We show that differences in the textual source can have more effect than the chosen sampling strategy."
S19-2138,{UM}-{IU}@{LING} at {S}em{E}val-2019 Task 6: Identifying Offensive Tweets Using {BERT} and {SVM}s,2019,25,0,3,0,3808,jian zhu,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This paper describes the UM-IU@LING{'}s system for the SemEval 2019 Task 6: Offens-Eval. We take a mixed approach to identify and categorize hate speech in social media. In subtask A, we fine-tuned a BERT based classifier to detect abusive content in tweets, achieving a macro F1 score of 0.8136 on the test data, thus reaching the 3rd rank out of 103 submissions. In subtasks B and C, we used a linear SVM with selected character n-gram features. For subtask C, our system could identify the target of abuse with a macro F1 score of 0.5243, ranking it 27th out of 65 submissions."
R19-1132,Investigating Multilingual Abusive Language Detection: A Cautionary Tale,2019,0,0,4,1,14345,kenneth steimel,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Abusive language detection has received much attention in the last years, and recent approaches perform the task in a number of different languages. We investigate which factors have an effect on multilingual settings, focusing on the compatibility of data and annotations. In the current paper, we focus on English and German. Our findings show large differences in performance between the two languages. We find that the best performance is achieved by different classification algorithms. Sampling to address class imbalance issues is detrimental for German and beneficial for English. The only similarity that we find is that neither data set shows clear topics when we compare the results of topic modeling to the gold standard. Based on our findings, we can conclude that a multilingual optimization of classifiers is not possible even in settings where comparable data sets are used."
W18-1603,Detecting Syntactic Features of Translated {C}hinese,2018,13,0,3,1,8273,hai hu,Proceedings of the Second Workshop on Stylistic Variation,0,"We present a machine learning approach to distinguish texts translated to Chinese (by humans) from texts originally written in Chinese, with a focus on a wide range of syntactic features. Using Support Vector Machines (SVMs) as classifier on a genre-balanced corpus in translation studies of Chinese, we find that constituent parse trees and dependency triples as features without lexical information perform very well on the task, with an F-measure above 90{\%}, close to the results of lexical n-gram features, without the risk of learning topic information rather than translation features. Thus, we claim syntactic features alone can accurately distinguish translated from original Chinese. Translated Chinese exhibits an increased use of determiners, subject position pronouns, NP + {``}ç{''} as NP modifiers, multiple NPs or VPs conjoined by ''ã'', among other structures. We also interpret the syntactic features with reference to previous translation studies in Chinese, particularly the usage of pronouns."
L18-1293,{U}ni{M}orph 2.0: {U}niversal {M}orphology,2018,18,14,10,0.46875,6613,christo kirov,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"The Universal Morphology UniMorph project is a collaborative effort to improve how NLP handles complex morphology across the world's languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland and is sponsored by the DARPA LORELEI program. This paper details advances made to the collection, annotation, and dissemination of project resources since the initial UniMorph release described at LREC 2016. lexical resources} }"
C18-3002,Practical Parsing for Downstream Applications,2018,0,0,2,1,2236,daniel dakota,Proceedings of the 27th International Conference on Computational Linguistics: Tutorial Abstracts,0,None
W17-5046,Native Language Identification using Phonetic Algorithms,2017,0,1,2,0,9312,charese smiley,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper, we discuss the results of the IUCL system in the NLI Shared Task 2017. For our system, we explore a variety of phonetic algorithms to generate features for Native Language Identification. These features are contrasted with one of the most successful type of features in NLI, character n-grams. We find that although phonetic features do not perform as well as character n-grams alone, they do increase overall F1 score when used together with character n-grams."
dakota-kubler-2017-towards,Towards Replicability in Parsing,2017,0,0,2,1,2236,daniel dakota,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"We investigate parsing replicability across 7 languages (and 8 treebanks), showing that choices concerning the use of grammatical functions in parsing or evaluation, the influence of the rare word threshold, as well as choices in test sentences and evaluation script options have considerable and often unexpected effects on parsing accuracies. All of those choices need to be carefully documented if we want to ensure replicability."
hu-etal-2017-non,Non-Deterministic Segmentation for {C}hinese Lattice Parsing,2017,0,1,3,1,8273,hai hu,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Parsing Chinese critically depends on correct word segmentation for the parser since incorrect segmentation inevitably causes incorrect parses. We investigate a pipeline approach to segmentation and parsing using word lattices as parser input. We compare CRF-based and lexicon-based approaches to word segmentation. Our results show that the lattice parser is capable of selecting the correction segmentation from thousands of options, thus drastically reducing the number of unparsed sentence. Lexicon-based parsing models have a better coverage than the CRF-based approach, but the many options are more difficult to handle. We reach our best result by using a lexicon from the n-best CRF analyses, combined with highly probable words."
mukherjee-kubler-2017-similarity,Similarity Based Genre Identification for {POS} Tagging Experts {\\&} Dependency Parsing,2017,14,0,2,1,15543,atreyee mukherjee,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"POS tagging and dependency parsing achieve good results for homogeneous datasets. However, these tasks are much more difficult on heterogeneous datasets. In (Mukherjee et al. 2016, 2017), we address this issue by creating genre experts for both POS tagging and parsing. We use topic modeling to automatically separate training and test data into genres and to create annotation experts per genre by training separate models for each topic. However, this approach assumes that topic modeling is performed jointly on training and test sentences each time a new test sentence is encountered. We extend this work by assigning new test sentences to their genre expert by using similarity metrics. We investigate three different types of methods: 1) based on words highly associated with a genre by the topic modeler, 2) using a k-nearest neighbor classification approach, and 3) using perplexity to determine the closest topic. The results show that the choice of similarity metric has an effect on results and that we can reach comparable accuracies to the joint topic modeling in POS tagging and dependency parsing, thus providing a viable and efficient approach to POS tagging and parsing a sentence by its genre expert."
K17-2001,{C}o{NLL}-{SIGMORPHON} 2017 Shared Task: Universal Morphological Reinflection in 52 Languages,2017,30,3,8,0.0734874,1281,ryan cotterell,Proceedings of the {C}o{NLL} {SIGMORPHON} 2017 Shared Task: Universal Morphological Reinflection,0,"The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation required systems to be trained and tested in each of 52 typologically diverse languages. In sub-task 1, submitted systems were asked to predict a specific inflected form of a given lemma. In sub-task 2, systems were given a lemma and some of its specific inflected forms, and asked to complete the inflectional paradigm by predicting all of the remaining inflected forms. Both sub-tasks included high, medium, and low-resource conditions. Sub-task 1 received 24 system submissions, while sub-task 2 received 3 system submissions. Following the success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared task, all but one of the submissions included a neural component. The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in disjoint sets of inflected forms being predicted correctly, suggesting that there is room for future improvement."
E17-1033,Creating {POS} Tagging and Dependency Parsing Experts via Topic Modeling,2017,20,1,2,1,15543,atreyee mukherjee,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Part of speech (POS) taggers and dependency parsers tend to work well on homogeneous datasets but their performance suffers on datasets containing data from different genres. In our current work, we investigate how to create POS tagging and dependency parsing experts for heterogeneous data by employing topic modeling. We create topic models (using Latent Dirichlet Allocation) to determine genres from a heterogeneous dataset and then train an expert for each of the genres. Our results show that the topic modeling experts reach substantial improvements when compared to the general versions. For dependency parsing, the improvement reaches 2 percent points over the full training baseline when we use two topics."
W16-6316,{POS} Tagging Experts via Topic Modeling,2016,13,2,2,1,15543,atreyee mukherjee,Proceedings of the 13th International Conference on Natural Language Processing,0,None
S16-1064,{IUCL} at {S}em{E}val-2016 Task 6: An Ensemble Model for Stance Detection in {T}witter,2016,12,12,12,1,3108,can liu,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"We present the IUCL system, based on supervised learning, for the shared task on stance detection. Our official submission, the random forest model, reaches a score of 63.60, and is ranked 6th out of 19 teams. We also use gradient boosting decision trees and SVM and merge all classifiers into an ensemble method. Our analysis shows that random forest is good at retrieving minority classes and gradient boosting majority classes. The strengths of different classifiers wrt. precision and recall complement each other in the ensemble."
W15-0701,Tools for Digital Humanities: Enabling Access to the Old {O}ccitan {R}omance of Flamenca,2015,31,2,2,0,37106,olga scrivner,Proceedings of the Fourth Workshop on Computational Linguistics for Literature,0,"Accessing historical texts is often a challenge because readers either do not know the historical language, or they are challenged by the technological hurdle when such texts are available digitally. Merging corpus linguistic methods and digital technology can provide novel ways of representing historical texts digitally and providing a simpler access. In this paper, we describe a multi-dimensional parallel Old Occitan-English corpus, in which word alignment serves as the basis for search capabilities as well as for the transfer of annotations. We show how parallel alignment can help overcome some challenges of historical manuscripts. Furthermore, we apply a resource-light method of building an emotion annotation via parallel alignment, thus showing that such annotations are possible without speaking the historical language. Finally, using visualization tools, such as ANNIS and GoogleViz, we demonstrate how the emotion analysis can be queried and visualized dynamically in our parallel corpus, thus showing that such information can be made accessible with low technological barriers."
W14-6101,Parsing {G}erman: How Much Morphology Do We Need?,2014,34,4,2,0.645056,1564,wolfgang maier,Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages,0,"We investigate how the granularity of POS tags influences POS tagging, and furthermore, how POS tagging performance relates to parsing results. For this, we use the standard xe2x80x9cpipelinexe2x80x9d approach, in which a parser builds its output on previously tagged input. The experiments are performed on two German treebanks, using three POS tagsets of different granularity, and six different POS taggers, together with the Berkeley parser. Our findings show that less granularity of the POS tagset leads to better tagging results. However, both too coarse-grained and too fine-grained distinctions on POS level decrease parsing performance."
W14-6111,Introducing the {SPMRL} 2014 Shared Task on Parsing Morphologically-rich Languages,2014,31,43,2,0,167,djame seddah,Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages,0,"This first joint meeting on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical English (SPMRL-SANCL) featured a shared task on statistical parsing of morphologically rich languages (SPMRL). The goal of the shared task is to allow to train and test different participating systems on comparable data sets, thus providing an objective measure of comparison between state-of-the-art parsing systems on data data sets from a range of different languages. This 2014 SPMRL shared task is a continuation and extension of the SPMRL shared task, which was co-located with the SPMRL meeting at EMNLP 2013 (Seddah et al., 2013). This paper provides a short overview of the 2014 SPMRL shared task goals, data sets, and evaluation setup. Since the SPMRL 2014 largely builds on the infrastructure established for the SPMRL 2013 shared task, we start by reviewing the previous shared task (xc2xa72) and then proceed to the 2014 SPMRL evaluation settings (xc2xa73), data sets (xc2xa74), and a task summary (xc2xa75). Due to organizational constraints, this overview is published prior to the submission of all system test runs, and a more detailed overview including the description of participating systems and the analysis of their results will follow as part of (Seddah et al., 2014), once the shared task is completed."
W14-5902,Feature Selection for Highly Skewed Sentiment Analysis Tasks,2014,18,6,2,1,3108,can liu,Proceedings of the Second Workshop on Natural Language Processing for Social Media ({S}ocial{NLP}),0,"Sentiment analysis generally uses large feature sets based on a bag-of-words approach, which results in a situation where individual features are not very informative. In addition, many data sets tend to be heavily skewed. We approach this combination of challenges by investigating feature selection in order to reduce the large number of features to those that are discriminative. We examine the performance of five feature selection methods on two sentiment analysis data sets from different domains, each with different ratios of class imbalance. Our finding shows that feature selection is capable of improving the classification accuracy only in balanced or slightly skewed situations. However, it is difficult to mitigate high skewing ratios. We also conclude that there does not exist a single method that performs best across data sets and skewing ratios. However we found that TF IDF2 can help in identifying the minority class even in highly imbalanced cases."
W14-5903,"{``}My Curiosity was Satisfied, but not in a Good Way{''}: Predicting User Ratings for Online Recipes",2014,18,4,6,1,3108,can liu,Proceedings of the Second Workshop on Natural Language Processing for Social Media ({S}ocial{NLP}),0,"In this paper, we develop an approach to automatically predict user ratings for recipes at Epicurious.com, based on the recipesxe2x80x99 reviews. We investigate two distributional methods for feature selection, Information Gain and Bi-Normal Separation; we also compare distributionally selected features to linguistically motivated features and two types of frameworks: a one-layer system where we aggregate all reviews and predict the rating vs. a two-layer system where ratings of individual reviews are predicted and then aggregated. We obtain our best results by using the two-layer architecture, in combination with 5 000 features selected by Information Gain. This setup reaches an overall accuracy of 65.60%, given an upper bound of 82.57%."
W14-3912,The {IUCL}+ System: Word-Level Language Identification via Extended {M}arkov Models,2014,9,4,4,0.833333,28641,levi king,Proceedings of the First Workshop on Computational Approaches to Code Switching,0,"We describe the IUCL system for the shared task of the First Workshop on Computational Approaches to Code Switching (Solorio et al., 2014), in which participants were challenged to label each word in Twitter texts as a named entity or one of two candidate languages. Our system combines character n-gram probabilities, lexical probabilities, word label transition probabilities and existing named entity recognitiontools within a Markovmodel framework that weights these components and assigns a label. Our approach is language-independent, and we submitted results for all data sets (five test sets and three xe2x80x9csurprisexe2x80x9d sets, covering four language pairs), earning the highest accuracy score on the tweet level on two language pairs (Mandarin-English, Arabicdialects 1 & 2) and one of the surprise sets (Arabic-dialects)."
S14-2060,{IUCL}: Combining Information Sources for {S}em{E}val Task 5,2014,14,1,5,0,38986,alex rudnick,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We describe the Indiana University system for SemEval Task 5, the L2 writing assistant task, as well as some extensions to the system that were completed after the main evaluation. Our team submitted translations for all four language pairs in the evaluation, yielding the top scores for English-German. The system is based on combining several information sources to arrive at a final L2 translation for a given L1 text fragment, incorporating phrase tables extracted from bitexts, an L2 language model, a multilingual dictionary, and dependency-based collocational models derived from large samples of targetlanguage text."
maier-etal-2014-discosuite,Discosuite - A parser test suite for {G}erman discontinuous structures,2014,23,4,4,0.645056,1564,wolfgang maier,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Parser evaluation traditionally relies on evaluation metrics which deliver a single aggregate score over all sentences in the parser output, such as PARSEVAL. However, for the evaluation of parser performance concerning a particular phenomenon, a test suite of sentences is needed in which this phenomenon has been identified. In recent years, the parsing of discontinuous structures has received a rising interest. Therefore, in this paper, we present a test suite for testing the performance of dependency and constituency parsers on non-projective dependencies and discontinuous constituents for German. The test suite is based on the newly released TIGER treebank version 2.2. It provides a unique possibility of benchmarking parsers on non-local syntactic relationships in German, for constituents and dependencies. We include a linguistic analysis of the phenomena that cause discontinuity in the TIGER annotation, thereby closing gaps in previous literature. The linguistic phenomena we investigate include extraposition, a placeholder/repeated element construction, topicalization, scrambling, local movement, parentheticals, and fronting of pronouns."
gilmanov-etal-2014-swift,"{SWIFT} Aligner, A Multifunctional Tool for Parallel Corpora: Visualization, Word Alignment, and (Morpho)-Syntactic Cross-Language Transfer",2014,16,4,3,0,38515,timur gilmanov,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"It is well known that word aligned parallel corpora are valuable linguistic resources. Since many factors affect automatic alignment quality, manual post-editing may be required in some applications. While there are several state-of-the-art word-aligners, such as GIZA++ and Berkeley, there is no simple visual tool that would enable correcting and editing aligned corpora of different formats. We have developed SWIFT Aligner, a free, portable software that allows for visual representation and editing of aligned corpora from several most commonly used formats: TALP, GIZA, and NAACL. In addition, our tool has incorporated part-of-speech and syntactic dependency transfer from an annotated source language into an unannotated target language, by means of word-alignment."
W13-4917,Overview of the {SPMRL} 2013 Shared Task: A Cross-Framework Evaluation of Parsing Morphologically Rich Languages,2013,110,38,3,0,167,djame seddah,Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"This paper reports on the first shared task on statistical parsing of morphologically rich languages (MRLs). The task features data sets from nine languages, each available both in constituency and dependency annotation. We report on the preparation of the data sets, on the proposed parsing scenarios, and on the evaluation metrics for parsing MRLs given different representation types. We present and analyze parsing results obtained by the task participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios."
R13-1001,{ASMA}: A System for Automatic Segmentation and Morpho-Syntactic Disambiguation of {M}odern {S}tandard {A}rabic,2013,16,8,3,0,487,muhammad abdulmageed,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"In this paper, we present ASMA, a fast and efficient system for automatic segmentation and fine grained part of speech (POS) tagging of Modern Standard Arabic (MSA). ASMA performs segmentation both of agglutinative and of inflectional morphological boundaries within a word. In this work, we compare ASMA to two state of the art suites of MSA tools: AMIRA 2.1 (Diab et al., 2007; Diab, 2009) and MADATOKAN 3.2. (Habash et al., 2009). ASMA achieves comparable results to these two systemsxe2x80x99 state-of-theart performance. ASMA yields an accuracy of 98.34% for segmentation, and an accuracy of 96.26% for POS tagging with ar ich tagset and 97.59% accuracy with an extremely reduced tagset. 1I ntroduction"
R13-1008,Domain Adaptation for Parsing,2013,24,3,3,1,38514,eric baucom,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We compare two different methods in domain adaptation applied to constituent parsing: parser combination and cotraining, each used to transfer information from the source domain of news to the target domain of natural dialogs, in a setting without annotated data. Both methods outperform the baselines and reach similar results. Parser combination profits most from the large amounts of training data combined with a robust probability model. Co-training, in contrast, relies on a small set of higher quality data."
R13-1046,Towards Domain Adaptation for Parsing Web Data,2013,23,6,3,0,13028,mohammad khan,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We improve upon a previous line of work for parsing web data, by exploring the impact of different decisions regarding the training data. First, we compare training on automatically POS-tagged data vs. gold POS data. Secondly, we compare the effect of training and testing within sub-genres, i.e., whether a close match of the genre is more important than training set size. Finally, we examine different ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways (sentence length, perplexity). In general, we find that approximating the in-domain data has a positive impact on parsing."
R13-1097,Machine Learning for Mention Head Detection in Multilingual Coreference Resolution,2013,24,1,2,1,33863,desislava zhekova,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"This work introduces a machine learning approach to the identification of mention heads needed for multilingual coreference resolution (MCR). We evaluate the method and compare it to a heuristic baseline and a rule-based approach, which are widely used in coreference resolution systems. We use the CoNLL-2012 shared task data sets, which include data for Arabic, Chinese, and English. We show that for MCR, machine learning offers a competitive, flexible, and robust solution for mention head detection."
J13-1003,Parsing Morphologically Rich Languages: Introduction to the Special Issue,2013,32,40,3,0,5249,reut tsarfaty,Computational Linguistics,0,"Parsing is a key task in natural language processing. It involves predicting, for each natural language sentence, an abstract representation of the grammatical entities in the sentence and the relations between these entities. This representation provides an interface to compositional semantics and to the notions of who did what to whom. The last two decades have seen great advances in parsing English, leading to major leaps also in the performance of applications that use parsers as part of their backbone, such as systems for information extraction, sentiment analysis, text summarization, and machine translation. Attempts to replicate the success of parsing English for other languages have often yielded unsatisfactory results. In particular, parsing languages with complex word structure and flexible word order has been shown to require non-trivial adaptation. This special issue reports on methods that successfully address the challenges involved in parsing a range of morphologically rich languages MRLs. This introduction characterizes MRLs, describes the challenges in parsing MRLs, and outlines the contributions of the articles in the special issue. These contributions present up-to-date research efforts that address parsing in varied, cross-lingual settings. They show that parsing MRLs addresses challenges that transcend particular representational and algorithmic choices."
W12-4509,{UBIU} for Multilingual Coreference Resolution in {O}nto{N}otes,2012,18,4,2,1,33863,desislava zhekova,Joint Conference on {EMNLP} and {C}o{NLL} - Shared Task,0,"The current work presents the participation of UBIU (Zhekova and Kubler, 2010) in the CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes (Pradhan et al., 2012). Our system deals with all three languages: Arabic, Chinese and English. The system results show that UBIU works reliably across all three languages, reaching an average score of 40.57 for Arabic, 46.12 for Chinese, and 48.70 for English. For Arabic and Chinese, the system produces high precision, while for English, precision and recall are balanced, which leads to the highest results across languages."
W12-3624,Annotating Coordination in the {P}enn {T}reebank,2012,11,10,2,0.645056,1564,wolfgang maier,Proceedings of the Sixth Linguistic Annotation Workshop,0,"Finding coordinations provides useful information for many NLP endeavors. However, the task has not received much attention in the literature. A major reason for that is that the annotation of major treebanks does not reliably annotate coordination. This makes it virtually impossible to detect coordinations in which two conjuncts are separated by punctuation rather than by a coordinating conjunction. In this paper, we present an annotation scheme for the Penn Treebank which introduces a distinction between coordinating from non-coordinating punctuation. We discuss the general annotation guidelines as well as problematic cases. Eventually, we show that this additional annotation allows the retrieval of a considerable number of coordinate structures beyond the ones having a coordinating conjunction."
W12-2011,Predicting Learner Levels for Online Exercises of {H}ebrew,2012,21,6,2,0.0981777,28642,markus dickinson,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"We develop a system for predicting the level of language learners, using only a small amount of targeted language data. In particular, we focus on learners of Hebrew and predict level based on restricted placement exam exercises. As with many language teaching situations, a major problem is data sparsity, which we account for in our feature selection, learning algorithm, and in the setup. Specifically, we define a two-phase classification process, isolating individual errors and linguistic constructions which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features."
W11-1918,{UBIU}: A Robust System for Resolving Unrestricted Coreference,2011,5,3,2,0.930233,33863,desislava zhekova,Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,0,"In this paper, we discuss the application of UBIU to the CoNLL-2011 shared task on Modeling Unrestricted Coreference in OntoNotes. The shared task concentrates on the detection of coreference not only in noun phrases but also involving verbs. The information provided for the closed track included WordNet as well as corpus generated number and gender information. Our system shows no improvement when using WordNet information, and the number information proved less reliable than the information in the part of speech tags."
W11-0323,Filling the Gap: Semi-Supervised Learning for Opinion Detection Across Domains,2011,26,18,2,0,11687,ning yu,Proceedings of the Fifteenth Conference on Computational Natural Language Learning,0,"We investigate the use of Semi-Supervised Learning (SSL) in opinion detection both in sparse data situations and for domain adaptation. We show that co-training reaches the best results in an in-domain setting with small labeled data sets, with a maximum absolute gain of 33.5%. For domain transfer, we show that self-training gains an absolute improvement in labeling accuracy for blog data of 16% over the supervised approach with target domain training data."
R11-1006,Fast Domain Adaptation for Part of Speech Tagging for Dialogues,2011,17,13,1,1,2237,sandra kubler,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"Part of speech tagging accuracy deteriorates severely when a tagger is used out of domain. We investigate a fast method for domain adaptation, which provides additional in-domain training data from an unannotated data set by applying POS taggers with different biases to the unannotated data set and then choosing the set of sentences on which the taggers agree. We show that we improve the accuracy of a trigram tagger, TnT, from 85.77% to 86.10%. In order to improve performance on unknown words, we investigate using active learning for learning ambiguity classes of domain specific words, yielding an accuracy of 89.15% for TnT."
R11-1008,Actions Speak Louder than Words: Evaluating Parsers in the Context of Natural Language Understanding Systems for Human-Robot Interaction,2011,18,4,1,1,2237,sandra kubler,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,The standard ParsEval metrics alone are often not sufficient for evaluating parsers integrated in natural language understanding systems. We propose to augment intrinsic parser evaluations by extrinsic measures in the context of human-robot interaction using a corpus from a human cooperative search task. We compare a constituent with a dependency parser on both intrinsic and extrinsic measures and show that the conversion to semantics is feasible for different syntactic paradigms.
R11-1036,Singletons and Coreference Resolution Evaluation,2011,14,5,1,1,2237,sandra kubler,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"This paper presents an empirical study on the influence of singletons on the evaluation of coreference resolution systems. We present results on two English data sets used in the SEMEVAL 2010 shared task 1 and the CONLL 2011 shared task using the scorers of both shared tasks. We show that singletons, both in the gold standard and in the system output, have an immense impact on the overall evaluation xe2x80x90 in an experiment where the coreference resolution results remain unchanged over the different settings."
W10-1821,Chunking {G}erman: An Unsolved Problem,2010,12,4,1,1,2237,sandra kubler,Proceedings of the Fourth Linguistic Annotation Workshop,0,"This paper describes a CoNLL-style chunk representation for the Tubingen Treebank of Written German, which assumes a flat chunk structure so that each word belongs to at most one chunk. For German, such a chunk definition causes problems in cases of complex prenominal modification. We introduce a flat annotation that can handle these structures via a stranded noun chunk."
S10-1019,{UBIU}: A Language-Independent System for Coreference Resolution,2010,14,18,2,0.930233,33863,desislava zhekova,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"We present UBIU, a language independent system for detecting full coreference chains, composed of named entities, pronouns, and full noun phrases which makes use of memory based learning and a feature model following Rahman and Ng (2009). UBIU is evaluated on the task Coreference Resolution in Multiple Languages (SemEval Task 1 (Recasens et al., 2010)) in the context of the 5th International Workshop on Semantic Evaluation."
N10-1105,Is {A}rabic Part of Speech Tagging Feasible Without Word Segmentation?,2010,7,11,2,1,15693,emad mohamed,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In this paper, we compare two novel methods for part of speech tagging of Arabic without the use of gold standard word segmentation but with the full POS tagset of the Penn Arabic Treebank. The first approach uses complex tags without any word segmentation, the second approach is segmention-based, using a machine learning segmenter. Surprisingly, word-based POS tagging yields the best results, with a word accuracy of 94.74%."
mohamed-kubler-2010-arabic,{A}rabic Part of Speech Tagging,2010,8,15,2,1,15693,emad mohamed,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Arabic is a morphologically rich language, which presents a challenge for part of speech tagging. In this paper, we compare two novel methods for POS tagging of Arabic without the use of gold standard word segmentation but with the full POS tagset of the Penn Arabic Treebank. The first approach uses complex tags that describe full words and does not require any word segmentation. The second approach is segmentation-based, using a machine learning segmenter. In this approach, the words are first segmented, then the segments are annotated with POS tags. Because of the word-based approach, we evaluate full word accuracy rather than segment accuracy. Word-based POS tagging yields better results than segment-based tagging (93.93{\%} vs. 93.41{\%}). Word based tagging also gives the best results on known words, the segmentation-based approach gives better results on unknown words. Combining both methods results in a word accuracy of 94.37{\%}, which is very close to the result obtained by using gold standard segmentation (94.91{\%})."
eberhard-etal-2010-indiana,The {I}ndiana {``}Cooperative Remote Search Task{''} ({CR}e{ST}) Corpus,2010,12,34,3,0,35860,kathleen eberhard,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper introduces a novel corpus of natural language dialogues obtained from humans performing a cooperative, remote, search task (CReST) as it occurs naturally in a variety of scenarios (e.g., search and rescue missions in disaster areas). This corpus is unique in that it involves remote collaborations between two interlocutors who each have to perform tasks that require the other's assistance. In addition, one interlocutor's tasks require physical movement through an indoor environment as well as interactions with physical objects within the environment. The multi-modal corpus contains the speech signals as well as transcriptions of the dialogues, which are additionally annotated for dialog structure, disfluencies, and for constituent and dependency syntax. On the dialogue level, the corpus was annotated for separate dialogue moves, based on the classification developed by Carletta et al. (1997) for coding task-oriented dialogues. Disfluencies were annotated using the scheme developed by Lickley (1998). The syntactic annotation comprises POS annotation, Penn Treebank style constituent annotations as well as dependency annotations based on the dependencies of pennconverter."
R09-1037,Semi-Supervised Learning for Word Sense Disambiguation: Quality vs. Quantity,2009,18,6,1,1,2237,sandra kubler,Proceedings of the International Conference {RANLP}-2009,0,"In this paper, we discuss the importance of the quality against the quantity of automatically extracted examples for word sense disambiguation (WSD). We first show that we can build a competitive WSD system with a memory-based classifier and a feature set reduced to easily and eciently computable features. We then show that adding automatically annotated examples improves the performance of this system when the examples are carefully selected based on their quality."
R09-1047,Diacritization for Real-World {A}rabic Texts,2009,8,7,2,1,15693,emad mohamed,Proceedings of the International Conference {RANLP}-2009,0,"For Arabic, diacritizing written text is important for many NLP tasks. In the work presented here, we investigate the quality of a diacritization approach, with a high success rate for treebank data but with a more limited success on realworld data. One of the problems we encountered is the non-standard use of the hamza diacritic, which leads to a decrease in diacritization accuracy. If an automatic hamza restoration module precedes diacritization, the results improve from a word error rate of 9.20% to 7.38% in treebank data, and from 7.96% to 5.93% on selected real-world texts. This shows clearly that hamza restoration is a necessary step for improving diacritization quality for Arabic real-world texts."
R09-1085,Instance Sampling Methods for Pronoun Resolution,2009,12,13,2,0,46438,holger wunsch,Proceedings of the International Conference {RANLP}-2009,0,"Instance sampling is a method to balance extremely skewed training sets as they occur, for example, in machine learning settings for anaphora resolution. Here, the number of negative samples (i.e. non-anaphoric pairs) is usually substantially larger than the number of positive samples. This causes classifiers to be biased towards negative classification, leading to suboptimal performance. In this paper, we explore how dierent techniques of instance sampling influence the performance of an anaphora resolution system for German given dierent classifiers. All sampling methods prove to increase the F-score for all classifiers, but the most successful method is random sampling. In the best setting, the F-score improves from 0.541 to 0.608 for memory-based learning, from 0.561 to 0.611 for decision tree learning and from 0.511 to 0.584 for maximum entropy learning."
E09-1047,Parsing Coordinations,2009,17,10,1,1,2237,sandra kubler,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"The present paper is concerned with statistical parsing of constituent structures in German. The paper presents four experiments that aim at improving parsing performance of coordinate structure: 1) reranking the n-best parses of a PCFG parser, 2) enriching the input to a PCFG parser by gold scopes for any conjunct, 3) reranking the parser output for all possible scopes for conjuncts that are permissible with regard to clause structure. Experiment 4 reranks a combination of parses from experiments 1 and 3.n n The experiments presented show that n-best parsing combined with reranking improves results by a large margin. Providing the parser with different scope possibilities and reranking the resulting parses results in an increase in F-score from 69.76 for the baseline to 74.69. While the F-score is similar to the one of the first experiment (n-best parsing and reranking), the first experiment results in higher recall (75.48% vs. 73.69%) and the third one in higher precision (75.43% vs. 73.26%). Combining the two methods results in the best result with an F-score of 76.69."
W08-1008,The {P}a{G}e 2008 Shared Task on Parsing {G}erman,2008,13,29,1,1,2237,sandra kubler,Proceedings of the Workshop on Parsing {G}erman,0,"The ACL 2008 Workshop on Parsing German features a shared task on parsing German. The goal of the shared task was to find reasons for the radically different behavior of parsers on the different treebanks and between constituent and dependency representations. In this paper, we describe the task and the data sets. In addition, we provide an overview of the test results and a first analysis."
kubler-etal-2008-compare,How to Compare Treebanks,2008,20,14,1,1,2237,sandra kubler,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Recent years have seen an increasing interest in developing standards for linguistic annotation, with a focus on the interoperability of the resources. This effort, however, requires a profound knowledge of the advantages and disadvantages of linguistic annotation schemes in order to avoid importing the flaws and weaknesses of existing encoding schemes into the new standards. This paper addresses the question how to compare syntactically annotated corpora and gain insights into the usefulness of specific design decisions. We present an exhaustive evaluation of two German treebanks with crucially different encoding schemes. We evaluate three different parsers trained on the two treebanks and compare results using EvalB, the Leaf-Ancestor metric, and a dependency-based evaluation. Furthermore, we present TePaCoC, a new testsuite for the evaluation of parsers on complex German grammatical constructions. The testsuite provides a well thought-out error classification, which enables us to compare parser output for parsers trained on treebanks with different encoding schemes and provides interesting insights into the impact of treebank annotation schemes on specific constructions like PP attachment or non-constituent coordination."
D07-1096,The {C}o{NLL} 2007 Shared Task on Dependency Parsing,2007,51,513,3,0,10682,joakim nivre,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In thispaper, we definethe tasksof the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results."
W06-1614,Is it Really that Difficult to Parse {G}erman?,2006,13,39,1,1,2237,sandra kubler,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a comparative study of probabilistic treebank parsing of German, using the Negra and TuBa-D/Z tree-banks. Experiments with the Stanford parser, which uses a factored PCFG and dependency model, show that, contrary to previous claims for other parsers, lexicalization of PCFG models boosts parsing performance for both treebanks. The experiments also show that there is a big difference in parsing performance, when trained on the Negra and on the TuBa-D/Z treebanks. Parser performance for the models trained on TuBa-D/Z are comparable to parsing results for English with the Stanford parser, when trained on the Penn treebank. This comparison at least suggests that German is not harder to parse than its West-Germanic neighbor language English."
W06-1110,Towards Case-Based Parsing: Are Chunks Reliable Indicators for Syntax Trees?,2006,16,1,1,1,2237,sandra kubler,Proceedings of the Workshop on Linguistic Distances,0,"This paper presents an approach to the question whether it is possible to construct a parser based on ideas from case-based reasoning. Such a parser would employ a partial analysis of the input sentence to select a (nearly) complete syntax tree and then adapt this tree to the input sentence. The experiments performed on German data from the Tuba-D/Z treebank and the KaRoPars partial parser show that a wide range of levels of generality can be reached, depending on which types of information are used to determine the similarity between input sentence and training sentences. The results are such that it is possible to construct a case-based parser. The optimal setting out of those presented here need to be determined empirically."
J06-4007,"Book Review: Memory-Based Language Processing, by Walter Daelemans and Antal van den Bosch",2006,0,0,1,1,2237,sandra kubler,Computational Linguistics,0,None
W05-0303,"A Unified Representation for Morphological, Syntactic, Semantic, and Referential Annotations",2005,11,25,2,0.893712,17752,erhard hinrichs,Proceedings of the Workshop on Frontiers in Corpus Annotations {II}: Pie in the Sky,0,"This paper reports on the SYN-RA (SYNtax-based Reference Annotation) project, an on-going project of annotating German newspaper texts with referential relations. The project has developed an inventory of anaphoric and coreference relations for German in the context of a unified, XML-based annotation scheme for combining morphological, syntactic, semantic, and anaphoric information. The paper discusses how this unified annotation scheme relates to other formats currently discussed in the literature, in particular the annotation graph model of Bird and Liberman (2001) and the pie-in-the-sky scheme for semantic annotation."
telljohann-etal-2004-tuba,"The T{\\\u}ba-{D}/{Z} Treebank: Annotating {G}erman with a Context-Free Backbone""",2004,5,59,3,0,32150,heike telljohann,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Abstract The purpose of this paper is to describe the T uBa-D/Z treebank of written German and to compare it to the independently developed TIGER treebank (Brants et al., 2002). Both treebanks, TIGER and T uBa-D/Z, use an annotation framework that is based on phrase structure grammar and that is enhanced by a level of predicate-argument structure. The comparison between the annotation schemes of the two treebanks focuses on the different treatments of free word order and discontinuous constituents in German as well as on differences in phrase-internal annotation."
hinrichs-etal-2002-hybrid,A Hybrid Architecture for Robust Parsing of {G}erman,2002,27,4,2,0.893712,17752,erhard hinrichs,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper provides an overview of current research on a hybrid and robust parsing architecture for the morphological, syntactic and semantic annotation of German text corpora. The novel contribution of this research lies not in the individual parsing modules, each of which relies on state-of-the-art algorithms and techniques. Rather what is new about the present approach is the combination of these modules into a single architecture. This combination provides a means to significantly optimize the performance of each component, resulting in an increased accuracy of annotation."
P01-1045,From Chunks to Function-Argument Structure: A Similarity-Based Approach,2001,18,10,1,1,2237,sandra kubler,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"Chunk parsing has focused on the recognition of partial constituent structures at the level of individual chunks. Little attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances. Such larger structures are not only desirable for a deeper syntactic analysis. They also constitute a necessary prerequisite for assigning function-argument structure.The present paper offers a similarity-based algorithm for assigning functional labels such as subject, object, head, complement, etc. to complete syntactic structures on the basis of prechunked input.The evaluation of the algorithm has concentrated on measuring the quality of functional labels. It was performed on a German and an English treebank using two different annotation schemes at the level of function-argument structure. The results of 89.73 % correct functional labels for German and 90.40 % for English validate the general approach."
H01-1072,"{T}{\\\u}{SBL}: A Similarity-Based Chunk Parser for Robust Syntactic Processing""",2001,12,6,1,1,2237,sandra kubler,Proceedings of the First International Conference on Human Language Technology Research,0,"Chunk parsing has focused on the recognition of partial constituent structures at the level of individual chunks. Little attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances.The TuSBL parser extends current chunk parsing techniques by a tree-construction component that extends partial chunk parses to complete tree structures including recursive phrase structure as well as function-argument structure. TuSBL's tree construction algorithm relies on techniques from memory-based learning that allow similarity-based classification of a given input structure relative to a pre-stored set of tree instances from a fully annotated treebank.A quantitative evaluation of TuSBL has been conducted using a semi-automatically constructed treebank of German that consists of appr. 67,000 fully annotated sentences. The basic PARSEVAL measures were used although they were developed for parsers that have as their main goal a complete analysis that spans the entire input. This runs counter to the basic philosophy underlying TuSBL, which has as its main goal robustness of partially analyzed structures."
W98-1203,Learning a Lexicalized Grammar for {G}erman,1998,4,0,1,1,2237,sandra kubler,New Methods in Language Processing and Computational Natural Language Learning,0,"In syntax, the trend nowadays is towards lexiealized grammar formalisms. It is now widely accepted that dividing words into wordclasses may serve as a labor-saving mechanism - but at the same time, it discards all detailed information on the idiosyncratic behavior of words. And that is exactly the type of information that may be necessary in order to parse a sentence. For learning approaches, however, lexicalized grammars represent a challenge for the very reason that they include so much detailed and specific information, which is difficult to learn. This paper will present an algorithm for learning a , link grammar of German. The problem of data spar- seness is tackled by using all the available infor- mation from partial parses as well as from an ex- isting grammar fragment and a tagger. This is a re- port about work in progress so there are no repre- sentative results available yet."
