2020.acl-main.539,D13-1160,0,0.0230982,"-module is generated, which top is denoted as ym . After that, we make the final prediction by feedtop ing the combination of ym and the final hidden vector h(x)T from § 3.3 through an MLP (Multilayer Perceptron) layer. The motivation of this operation is to retain the complete semantic meaning of the whole contexts because some linguistic cues are discarded during the synthesizing process of the program. 3.5 Program Generation In this part, we describe our semantic parser for synthesizing a program for a textual statement. We tackle the semantic parsing problem in a weaklysupervised setting (Berant et al., 2013; Liang et al., 2017; Misra et al., 2018), since the ground-truth program is not provided. 6057 Model Val Test Human Performance Majority Guess BERT classifier w/o Table Table-BERT (Horizontal-S+T-Concatenate) Table-BERT (Vertical-S+T-Template) Table-BERT (Vertical-T+S-Template) Table-BERT (Horizontal-S+T-Template) Table-BERT (Horizontal-T+S-Template) LPA-Voting w/o Discriminator LPA-Weighted-Voting w/ Discriminator LPA-Ranking w/ Discriminator LogicalFactChecker (program from LPA) LogicalFactChecker (program from Seq2Action) 50.7 50.9 50.7 56.7 56.7 66.0 66.1 57.7 62.5 65.2 71.7 71.8 50.4 50."
2020.acl-main.539,P18-1071,0,0.0989351,"Missing"
2020.acl-main.539,D18-1192,0,0.0336961,"ate). In LPA settings, (Weighted) Voting means assigning each program with (score-weighted) equal weight to vote for the final result. Ranking means using the result generated by the top program ranked by the discriminator. As shown in Figure 3, a program in TABFACT is structural and follows a grammar with over 50 functions. To effectively capture the structure of the program and also generate legitimate programs following a grammar in the generation process, we develop a sequence-to-action approach, which is proven to be effective in solving many semantic parsing problems (Chen et al., 2018; Iyer et al., 2018; Guo et al., 2018). The basic idea is that the generation of a program tree is equivalent to the generation of a sequence of action, which is a traversal of the program tree following a particular order, like depth-first, left-to-right order. Specifically, our semantic parser works in a top-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig"
2020.acl-main.539,P17-1003,0,0.0467597,"which top is denoted as ym . After that, we make the final prediction by feedtop ing the combination of ym and the final hidden vector h(x)T from § 3.3 through an MLP (Multilayer Perceptron) layer. The motivation of this operation is to retain the complete semantic meaning of the whole contexts because some linguistic cues are discarded during the synthesizing process of the program. 3.5 Program Generation In this part, we describe our semantic parser for synthesizing a program for a textual statement. We tackle the semantic parsing problem in a weaklysupervised setting (Berant et al., 2013; Liang et al., 2017; Misra et al., 2018), since the ground-truth program is not provided. 6057 Model Val Test Human Performance Majority Guess BERT classifier w/o Table Table-BERT (Horizontal-S+T-Concatenate) Table-BERT (Vertical-S+T-Template) Table-BERT (Vertical-T+S-Template) Table-BERT (Horizontal-S+T-Template) Table-BERT (Horizontal-T+S-Template) LPA-Voting w/o Discriminator LPA-Weighted-Voting w/ Discriminator LPA-Ranking w/ Discriminator LogicalFactChecker (program from LPA) LogicalFactChecker (program from Seq2Action) 50.7 50.9 50.7 56.7 56.7 66.0 66.1 57.7 62.5 65.2 71.7 71.8 50.4 50.5 50.4 56.2 57.0 65."
2020.acl-main.539,D19-1603,0,0.11077,"Missing"
2020.acl-main.539,D18-2002,0,0.014426,"grams following a grammar in the generation process, we develop a sequence-to-action approach, which is proven to be effective in solving many semantic parsing problems (Chen et al., 2018; Iyer et al., 2018; Guo et al., 2018). The basic idea is that the generation of a program tree is equivalent to the generation of a sequence of action, which is a traversal of the program tree following a particular order, like depth-first, left-to-right order. Specifically, our semantic parser works in a top-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig, 2017) is used for directly passing information from parent actions. We further regard column names of the table as a part of the input (Zhong et al., 2017) to generate column names as program arguments. We implement the approach with the LSTMbased recurrent network and Glove word vectors (Pennington et al., 2014) in this work, and the framework could be easily implemented with Transformer-based framework. Foll"
2020.acl-main.539,D18-1010,0,0.0610233,"used for verification, including natural language (Thorne et al., 2018), semi-structured tables (Chen et al., 2019), and images (Zlatkova et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fa"
2020.acl-main.539,D14-1162,0,0.0848898,"-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig, 2017) is used for directly passing information from parent actions. We further regard column names of the table as a part of the input (Zhong et al., 2017) to generate column names as program arguments. We implement the approach with the LSTMbased recurrent network and Glove word vectors (Pennington et al., 2014) in this work, and the framework could be easily implemented with Transformer-based framework. Following Chen et al. (2019), we employ the label of veracity to guide the learning process of the semantic parser. We also employ programs produced by LPA (Latent Program Algorithm) for comparison, which is provided by Chen et al. (2019). In the training process, we train the semantic parser and the claim verification model separately. The training of semantic parser includes two steps: candidate search and sequence-to-action learning. For candidate search, we closely follow LPA by first collecting"
2020.acl-main.539,D17-1317,0,0.036168,"ost influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the factual accuracy of the generated summary in neural abstractive summarization systems (Goodrich et al., 2019; Kry´sci´nski et al., 2019), as well as the use of this factual accuracy as a reward"
2020.acl-main.539,P13-1045,0,0.0675626,"gical form, in a semantic parsing manner (Liang, 2016). Then, our system builds a heterogeneous graph to capture the connections among the statement, the table and the program. Such connections reflect the related context of each token in the graph, which are used to define attention masks in a Transformer-based (Vaswani et al., 2017) framework. The attention masks are used to learn graph-enhanced contextual representations of tokens1 . We further develop a program-guided neural module network to capture the structural and compositional semantics of the program for semantic compositionality. (Socher et al., 2013; Andreas et al., 2015). Graph nodes, whose representations are computed using the contextual representations of their constituents, are considered as arguments, and logical operations are considered as modules to recursively produce representations of higher level nodes along the program. Experiments show that our system outperforms previous systems and achieves the state-of-the-art verification accuracy. The contributions of this paper can be summarized as follows: • We propose LogicalFactChecker, a graphbased neural module network, which utilizes logical operations for fact-checking. 1 Here"
2020.acl-main.539,D19-1216,0,0.0218259,"nction of difference time, which is not covered by the current set. 5 Related Work There is a growing interest in fact checking in NLP with the rising importance of assessing the truthfulness of texts, especially when pre-trained language models (Radford et al., 2019; Zellers et al., 2019; Keskar et al., 2019) are more and more powerful in generating fluent and coherent texts. Previous studies in the field of fact checking differ in the genres of supporting evidence used for verification, including natural language (Thorne et al., 2018), semi-structured tables (Chen et al., 2019), and images (Zlatkova et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al.,"
2020.acl-main.539,N18-1074,0,0.15998,"Missing"
2020.acl-main.539,D19-6601,0,0.0280888,"a et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the factual accuracy of the generated summary in neural"
2020.acl-main.539,D19-5316,0,0.0844217,": An example of table-based fact checking. Given a statement and a table as the input, the task is to predict the label. Program reflects the underlying meaning of the statement, which should be considered for fact checking. Introduction Fact checking for textual statements has emerged as an essential research topic recently because of the unprecedented amount of false news and rumors spreading through the internet (Thorne et al., 2018; ∗ Work done while this author was an intern at Microsoft Research. Chen et al., 2019; Goodrich et al., 2019; Nakamura et al., 2019; Kry´sci´nski et al., 2019; Vaibhav et al., 2019). Online misinformation may manipulate people’s opinions and lead to significant influence on essential social events like political elections (Faris et al., 2017). In this work, we study fact checking, with the goal of automatically assessing the truthfulness of a textual statement. The majority of previous studies in fact checking mainly focused on making better use of the meaning of words, while rarely considered symbolic reasoning about logical operations (such as “count”, “superlative”, “aggregation”). However, modeling logical operations is an essential step towards the modeling of compl"
2020.acl-main.539,P17-1041,0,\N,Missing
2020.acl-main.539,D18-1266,0,\N,Missing
2020.acl-main.544,2020.acl-main.23,0,0.0747168,"Missing"
2020.acl-main.544,2021.ccl-1.108,0,0.0750586,"Missing"
2020.acl-main.544,P18-2119,0,0.0142462,"at focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recentl"
2020.acl-main.544,D16-1031,0,0.027836,"and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse responses. A recent work CWVAE (Du et al., 2019) on event-cen"
2020.acl-main.544,N16-1098,0,0.0228979,"ated text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP com"
2020.acl-main.544,P18-1043,0,0.387556,"two kinds of reasonable inferences for the event under different background knowledge that is absent in the dataset. Introduction Inferential text generation aims to understand dailylife events and generate texts about their underlying causes, effects, and mental states of event participants, which is crucial for automated commonsense reasoning. Taking Figure 1 as an example, given an event “PersonX reads PersonY’s diary”, the cause of the participant “PersonX” is to “obtain Person Y’s secrets” and the mental state of “PersonX” is “guilty”. Standard approaches for inferential text generation (Rashkin et al., 2018; Sap et al., 2019; Bosselut et al., 2019; Du et al., 2019) typically only ∗ Work done while this author was an intern at Microsoft Research. take the event as the input, while ignoring the background knowledge that provides crucial evidence to generate reasonable inferences. For example, if the background knowledge of this example is “PersonY invites PersonX to read his diary”, the outputs should be different. In this paper, we present an evidence-aware generative model, which ﬁrst retrieves relevant evidence from a large text corpus and then leverages retrieved evidence to guide the generati"
2020.acl-main.544,L16-1233,0,0.0270969,"y using better semantic-based retrieval model. The second problem is that the model cannot effectively leverage selected evidence. Although the selected evidence is closely related to the event and the inference can be obtained from the evidence, the model still generate incorrect texts since lacking of supervised information. A potential direction to mitigate the problem is to annotate background knowledge of events in the training dataset. 5 5.1 Related Work Event-Related Text Understanding Recently, event-related text understanding has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018; Rashkin et al., 2018; Sap et al., 2019; Guo et al., 2020), which is crucial to artiﬁcial intelligence systems for automated commonsense reasoning. There are a variety of tasks that focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and"
2020.acl-main.544,D17-1006,0,0.0182539,"ic-based retrieval model. The second problem is that the model cannot effectively leverage selected evidence. Although the selected evidence is closely related to the event and the inference can be obtained from the evidence, the model still generate incorrect texts since lacking of supervised information. A potential direction to mitigate the problem is to annotate background knowledge of events in the training dataset. 5 5.1 Related Work Event-Related Text Understanding Recently, event-related text understanding has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018; Rashkin et al., 2018; Sap et al., 2019; Guo et al., 2020), which is crucial to artiﬁcial intelligence systems for automated commonsense reasoning. There are a variety of tasks that focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Gr"
2020.acl-main.544,D18-1009,0,0.022466,"different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 20"
2020.acl-main.544,D16-1050,0,0.0283868,"erential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse"
2020.acl-main.544,P17-1061,0,0.0188185,"of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse responses. A recent work CWVAE (Du et al., 2019) on event-centered If-Then reasoning is the most related to our work, which introduces an additio"
2020.acl-main.549,D14-1059,0,0.0802266,"ly assessing the truthfulness of a textual claim by looking for textual evidence. Introduction Internet provides an efficient way for individuals and organizations to quickly spread information to massive audiences. However, malicious people spread false news, which may have significant influence on public opinions, stock prices, even presidential elections (Faris et al., 2017). Vosoughi et al. (2018) show that false news reaches more people ∗ Work done while this author was an intern at Microsoft Research. Previous works are dominated by natural language inference models (Dagan et al., 2013; Angeli and Manning, 2014) because the task requires reasoning of the claim and retrieved evidence sentences. They typically either concatenate evidence sentences into a single string, which is used in top systems in the FEVER challenge (Thorne et al., 2018b), or use feature fusion to aggregate the features of isolated evidence sentences (Zhou et al., 2019). However, both methods fail to capture rich semantic-level structures among multiple evidence, which also prevents the use of deeper reasoning model for fact checking. In Figure 1, we give a motivating example. Making the correct prediction requires a model to reaso"
2020.acl-main.549,W04-2412,0,0.194303,"Missing"
2020.acl-main.549,N16-1138,0,0.0557513,") Figure 5: A case study of our approach. Facts shared across the claim and the evidence are highlighted with different colors. a text or a subject-predicate-object triple (Nakashole and Mitchell, 2014). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Rashkin et al. (2017) who collect claims without meta-data while providing 74K news articles. We study FEVER (Thorne et al., 2018a), which requires aggregating information from multiple pieces of evidence from Wikipedia for making the conclusion. FEVER contains 185,445 annotated instances, which to the best of our knowledge is the largest benchmark dataset in this area. The majority of participating teams in the FEVER chall"
2020.acl-main.549,W18-5516,0,0.192352,"by training a classification model like Enhanced LSTM (Chen et al., 2016) in a supervised setting or using string similarity function like TFIDF without trainable parameters. Padia et al. (2018) utilizes semantic frames for evidence selection. In this work, our focus is the claim classification phase. Top-ranked three systems aggregate pieces of evidence through concatenating evidence sentences into a single string (Nie et al., 2019), classifying each evidence-claim pair separately, merging the results (Yoneda et al., 2018), and encoding each evidence-claim pair followed by pooling operation (Hanselowski et al., 2018). Zhou et al. (2019) are the first to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the information on top of BERT, regard-1 ing each evidence as a node in the graph. Our work differs from Zhou et al. (2019) in that (1) the construction of our graph requires understanding the syntax of each sentence, which could be viewed as a more fine-grained graph, and (2) both the contextual representation learning module and the reasoning module have model innovations of taking the graph information into consideration. Instead of trai"
2020.acl-main.549,2021.ccl-1.108,0,0.0443279,"Missing"
2020.acl-main.549,P14-1095,0,0.05237,"ce Medal of Honor , an astronaut must perform feats of extraordinary accomplishment while participating in space flight under the authority of NASA . Tuples: ('awarded', 'the Congressional Space Medal Evidence #2 of Honor’) ('To be awarded the Congressional Space Medal of Honor',’an astronaut','perform','feats of extraordinary accomplishment’) ('an astronaut', 'participating','in space flight','under the authority of NASA' ) Figure 5: A case study of our approach. Facts shared across the claim and the evidence are highlighted with different colors. a text or a subject-predicate-object triple (Nakashole and Mitchell, 2014). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Ra"
2020.acl-main.549,W18-5527,0,0.0209756,"EVER challenge (Thorne et al., 2018b) use the same pipeline consisting of three components, namely document selection, evidence sentence selection, and claim verification. In document selection phase, participants typically extract named entities from a claim as the query and use Wikipedia search API. In the evidence selection phase, participants measure the similarity between the claim and an evidence sentence candidate by training a classification model like Enhanced LSTM (Chen et al., 2016) in a supervised setting or using string similarity function like TFIDF without trainable parameters. Padia et al. (2018) utilizes semantic frames for evidence selection. In this work, our focus is the claim classification phase. Top-ranked three systems aggregate pieces of evidence through concatenating evidence sentences into a single string (Nie et al., 2019), classifying each evidence-claim pair separately, merging the results (Yoneda et al., 2018), and encoding each evidence-claim pair followed by pooling operation (Hanselowski et al., 2018). Zhou et al. (2019) are the first to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the informati"
2020.acl-main.549,D17-1317,0,0.0495327,"4). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Rashkin et al. (2017) who collect claims without meta-data while providing 74K news articles. We study FEVER (Thorne et al., 2018a), which requires aggregating information from multiple pieces of evidence from Wikipedia for making the conclusion. FEVER contains 185,445 annotated instances, which to the best of our knowledge is the largest benchmark dataset in this area. The majority of participating teams in the FEVER challenge (Thorne et al., 2018b) use the same pipeline consisting of three components, namely document selection, evidence sentence selection, and claim verification. In document selection phase, par"
2020.acl-main.549,N18-2074,0,0.154527,"rd w5j from s5 are connected on the graph, simply concatenating evidence sentences as a single string fails to capture their semantic-level structure, and would give a large distance to w1i and w5j , which is the number of words between them across other three sentences (i.e., s2 , s3 , and s4 ). An intuitive way to achieve our goal is to define an N × N matrix of distances of words along the graph, where N is the total number of words in the evidence. However, this is unacceptable in practice because the representation learning procedure will take huge memory space, which is also observed by Shaw et al. (2018). In this work, we adopt pre-trained model XLNet (Yang et al., 2019) as the backbone of our approach because it naturally involves the concept of relative position5 . Pre-trained models capture rich contextual representations of words, which is helpful for our task which requires sentence-level reasoning. Considering the aforementioned issues, we implement an approximate solution to trade off between the efficiency of implementation and the informativeness of the graph. Specifically, we reorder evidence sentences with a topology sort algorithm with the intuition that closely linked nodes shoul"
2020.acl-main.549,D18-1209,0,0.108334,"Missing"
2020.acl-main.549,N18-1074,0,0.293102,"Missing"
2020.acl-main.549,W18-5501,0,0.137843,"Missing"
2020.acl-main.549,W14-2508,0,0.231712,"al Evidence #2 of Honor’) ('To be awarded the Congressional Space Medal of Honor',’an astronaut','perform','feats of extraordinary accomplishment’) ('an astronaut', 'participating','in space flight','under the authority of NASA' ) Figure 5: A case study of our approach. Facts shared across the claim and the evidence are highlighted with different colors. a text or a subject-predicate-object triple (Nakashole and Mitchell, 2014). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Rashkin et al. (2017) who collect claims without meta-data while providing 74K news articles. We study FEVER (Thorne et al., 2018a), which requires aggregating information from multiple pieces of"
2020.acl-main.549,D18-1010,0,0.18542,"st to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the information on top of BERT, regard-1 ing each evidence as a node in the graph. Our work differs from Zhou et al. (2019) in that (1) the construction of our graph requires understanding the syntax of each sentence, which could be viewed as a more fine-grained graph, and (2) both the contextual representation learning module and the reasoning module have model innovations of taking the graph information into consideration. Instead of training each component separately, Yin and Roth (2018) show that joint learning could improve both claim verification and evidence selection. 7 Conclusion In this work, we present a graph-based approach for fact checking. When assessing the veracity of a claim giving multiple evidence sentences, our approach is built upon an automatically constructed graph, which is derived based on semantic role labeling. To better exploit the graph information, we propose two graph-based modules, one for calculating contextual word embeddings using graph-based distance in XLNet, and the other for learning representations of graph components and reasoning over t"
2020.acl-main.549,W18-5515,0,0.222915,", participants measure the similarity between the claim and an evidence sentence candidate by training a classification model like Enhanced LSTM (Chen et al., 2016) in a supervised setting or using string similarity function like TFIDF without trainable parameters. Padia et al. (2018) utilizes semantic frames for evidence selection. In this work, our focus is the claim classification phase. Top-ranked three systems aggregate pieces of evidence through concatenating evidence sentences into a single string (Nie et al., 2019), classifying each evidence-claim pair separately, merging the results (Yoneda et al., 2018), and encoding each evidence-claim pair followed by pooling operation (Hanselowski et al., 2018). Zhou et al. (2019) are the first to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the information on top of BERT, regard-1 ing each evidence as a node in the graph. Our work differs from Zhou et al. (2019) in that (1) the construction of our graph requires understanding the syntax of each sentence, which could be viewed as a more fine-grained graph, and (2) both the contextual representation learning module and the reasoning m"
2020.acl-main.549,P19-1085,0,0.40704,"sidential elections (Faris et al., 2017). Vosoughi et al. (2018) show that false news reaches more people ∗ Work done while this author was an intern at Microsoft Research. Previous works are dominated by natural language inference models (Dagan et al., 2013; Angeli and Manning, 2014) because the task requires reasoning of the claim and retrieved evidence sentences. They typically either concatenate evidence sentences into a single string, which is used in top systems in the FEVER challenge (Thorne et al., 2018b), or use feature fusion to aggregate the features of isolated evidence sentences (Zhou et al., 2019). However, both methods fail to capture rich semantic-level structures among multiple evidence, which also prevents the use of deeper reasoning model for fact checking. In Figure 1, we give a motivating example. Making the correct prediction requires a model to reason based on the understanding that “Rodney King riots” is occurred in “Los Angeles County” from the first evidence, and that “Los Angeles County” is “the most populous county 6170 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6170–6180 c July 5 - 10, 2020. 2020 Association for Computa"
2020.emnlp-main.193,I08-2115,0,0.0513539,"ies in the field of DeepFake detection of generated text are dominated by deep-learning based document classification models and studies about discriminating features of generated text. GROVER (Zellers et al., 2019) detects generated text by a fine-tuned model of the generative model itself. Ippolito et al. (2019) fine-tune the BERT model for discrimination and explore how sampling strategies and text excerpt length affect the detection. GLTR (Gehrmann et al., 2019) develops a statistical method of computing per-token likelihoods and visualizes histograms over them to help deepfake detection. Badaskar et al. (2008) and P´erez-Rosas et al. (2017) study language distributional features including n-gram frequencies, text coherence and syntax features. Vijayaraghavan et al. (2020) study the effectiveness of different numeric representations (e.g., TFIDF and Word2Vec) and different neural networks (e.g., ANNs, LSTMs) for detection. Bakhtin et al. (2019) tackle the problem as a ranking task and study about the cross-architecture and cross-corpus generalization of their scoring functions. Schuster et al. (2019) indicate that simple provenance-based detection methods are insufficient for solving the problem and"
2020.emnlp-main.193,2021.ccl-1.108,0,0.0824142,"Missing"
2020.emnlp-main.193,P17-1161,0,0.0592401,"Missing"
2020.emnlp-main.193,N18-1074,0,0.060846,"Missing"
2020.emnlp-main.193,P19-3019,0,0.0188039,"evaluate on datasets produced by both GPT-2 and GROVER. Advances in generative models have promoted the development of detection methods. Previous studies in the field of DeepFake detection of generated text are dominated by deep-learning based document classification models and studies about discriminating features of generated text. GROVER (Zellers et al., 2019) detects generated text by a fine-tuned model of the generative model itself. Ippolito et al. (2019) fine-tune the BERT model for discrimination and explore how sampling strategies and text excerpt length affect the detection. GLTR (Gehrmann et al., 2019) develops a statistical method of computing per-token likelihoods and visualizes histograms over them to help deepfake detection. Badaskar et al. (2008) and P´erez-Rosas et al. (2017) study language distributional features including n-gram frequencies, text coherence and syntax features. Vijayaraghavan et al. (2020) study the effectiveness of different numeric representations (e.g., TFIDF and Word2Vec) and different neural networks (e.g., ANNs, LSTMs) for detection. Bakhtin et al. (2019) tackle the problem as a ranking task and study about the cross-architecture and cross-corpus generalization"
2020.emnlp-main.320,2020.acl-main.499,0,0.0209775,"rounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification"
2020.emnlp-main.320,D19-1565,0,0.0399722,"Missing"
2020.emnlp-main.320,D16-1146,0,0.0382263,"Missing"
2020.emnlp-main.320,N19-1423,0,0.173838,"ency between sentencelevel and token-level predictions (§3.2) and textual knowledge from literal definitions of propaganda techniques (§3.3). At last, we describe the training and inference procedures (§3.4). 3.1 Base Model To better exploit the sentence-level information and further benefit token-level prediction, we develop a fine-grained multi-task method as our base model, which makes predictions for 18 propaganda techniques at both sentence level and token level. Inspired by the success of pre-trained language models on various natural language processing downstream tasks, we adopt BERT (Devlin et al., 2019) as the backbone model here. For each input sentence, the sequence is modified as “[CLS]sentence tokens[SEP ]”. Specifically, on top of BERT, we add 19 binary classifiers for finegrained sentence-level predictions, and one 19-way classifier for token-level predictions, where all classifiers are implemented as linear layers. At sentence level, we perform multiple binary classifications and this can further support leveraging declarative knowledge. The last representation of the special token [CLS] which is regarded as a summary of the semantic content of the input, is adopted to perform multipl"
2020.emnlp-main.320,Q15-1027,0,0.0123863,"ta. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural networ"
2020.emnlp-main.320,D19-1405,0,0.0184536,"h first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need a lot of training data and are not interpretable. On the other hand, logicbased expert systems are interpretable and require less or no training data. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number o"
2020.emnlp-main.320,P19-1028,0,0.154388,"ugh creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification modules to recursively construct model similar to the backward chaining algorithm of Prolog. Evans and Grefenstette (2018) develop a differentiable model of forward chaining inference, where weights represent a probability distribution over clauses. Li and Srikumar (2019) inject logic-driven neurons to existing neural networks by measuring the degree of the head being true measured by probabilistic soft logic (Kimmig et al., 2012). Our approach belongs to the first direction, and to the best of knowledge our work is the first one that augments neural network with logical knowledge for propaganda detection. 6 Conclusion In this paper, we propose a fine-grained multitask learning approach, which leverages declarative knowledge to detect propaganda techniques in news articles. Specifically, the declarative knowledge is expressed in both first-order logic and natu"
2020.emnlp-main.320,D17-1317,0,0.0842897,"fluence an audience. 4. Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, h"
2020.emnlp-main.320,N15-1118,0,0.421682,"Missing"
2020.emnlp-main.320,N18-1074,0,0.0442108,"njection of first-order logic into neural networks. We will describe related studies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of conven"
2020.emnlp-main.320,D18-1215,0,0.0256681,"tions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second gr"
2020.emnlp-main.320,P17-2067,0,0.193758,". Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, however, they"
2020.emnlp-main.320,D19-1216,0,0.0257362,"tudies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need"
2020.emnlp-main.320,P16-1228,0,\N,Missing
2020.emnlp-tutorials.1,2020.acl-tutorials.8,0,0.152569,"ns, which are interpretable to developers and users at concept level. The design of such symbolic functions in real applications are typically conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty i"
2020.emnlp-tutorials.1,P18-1043,0,0.0282424,"y conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represents types, relations, and the complex dependencies between them. We will use Bayesian Network (Pearl, 198"
2020.emnlp-tutorials.1,N16-3020,0,0.0292639,"natural language understanding challenges, including Grade 8 New York Regents science exam (Clark et al., 2019), discrete reasoning over natural language (Dua et al., 2019), reasoning over rules in natural language (Clark et al., 2020), and logical reasoning (Yu et al., 2020). Afterwards, we will review model interpretation methods, including post-hoc ones and intrinsic ones. Post-hoc methods aim to interpret what an existing model learned without making changes to the original model. We will cover saliency maps (Simonyan et al., 2013), local interpretable model-agnostic explanations (LIME) (Ribeiro et al., 2016), testing with concept activation vectors (TCAV) (Kim et al., 2018), and visual explanation generation (Hendricks et al., 2016). Intrinsic methods are that inherently interpretable (to some extent). We will cover attention (Bahdanau et al., 2014), interpretable CNN (Zhang et al., 2018), and neural 2 Dilemma: Interpretability vs. Performance (30 min.) will review post-hoc models and intrinsic models for interpretation, and discuss the dilemma of “interpretability versus performance”. module network (Andreas et al., 2016). We last summarize the content of this tutorial and discuss possible futur"
2020.emnlp-tutorials.1,N15-1118,0,0.0314425,"ng (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretability versus performance” by showing the empirical success of pretrained models on natural language understanding challenges, including Grade 8 New York Regents science e"
2020.emnlp-tutorials.1,2020.acl-tutorials.7,0,0.0323856,"ce; Outline Opening (15 min.) will describe the motivation and outline of this tutorial and give our definition on machine reasoning. • Du et al. (2020) - a survey on interpretable machine learning techniques; Symbolic Reasoning (20 min.) will review typical methods based on propositional logic and first order logic, respectively. • Chen and Yih (2020) - a tutorial on opendomain question answering, in which many work can be categorized as neural-evidence reasoning; Probabilistic Reasoning (20 min.) will review typical methods based on Bayesian Network and Markov Logic Network, respectively. • Sap et al. (2020) - a tutorial on commonsense reasoning for natural language processing. Neural-Symbolic Reasoning (40 min.) will review typical methods including knowledge graph reasoning, neural semantic parsing, neural module network and symbolic knowledge as constraints. 6 Tutorial Abstract Machine reasoning research aims to build interpretable AI systems that can solve problems or draw conclusions from what they are told (i.e. facts and observations) and already know (i.e. models, common sense and knowledge) under certain constraints. In this tutorial, we will (1) describe the Neural-Evidence Reasoning (4"
2020.emnlp-tutorials.1,P19-1028,0,0.0227561,"; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretability versus performance” by showing the empirical success of pretrained models on natural language understanding challenges, including Grade 8 New York Regents science exam (Clark et al., 2019), discrete reasoning over natural"
2020.emnlp-tutorials.1,P18-1034,1,0.817959,"erentiable modules, where each module represents a program with a specific function. By doing these, such systems are usually more interpretable than black-box neural networks. We will review knowledge graph reasoning (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box"
2020.emnlp-tutorials.1,N19-1421,0,0.0253981,"opers and users at concept level. The design of such symbolic functions in real applications are typically conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represe"
2020.emnlp-tutorials.1,N18-1074,0,0.0259547,"ymbolic functions in real applications are typically conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represents types, relations, and the complex dependencies bet"
2020.emnlp-tutorials.1,2020.emnlp-main.320,1,0.824035,"orot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretability versus performance” by showing the empirical success of pretrained models on natural language understanding challenges, including Grade 8 New York Regents science exam (Clark et al., 2019), discrete reasoning over natural language (Dua et al"
2020.emnlp-tutorials.1,D17-1060,0,0.0200163,"bolic reasoning system (1) integrates existing reasoning technologies with symbolic knowledge based on neural networks and (2) implements inference as a chain of differentiable modules, where each module represents a program with a specific function. By doing these, such systems are usually more interpretable than black-box neural networks. We will review knowledge graph reasoning (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned m"
2020.emnlp-tutorials.1,D18-1259,0,0.0285158,"o broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represents types, relations, and the complex dependencies between them. We will use Bayesian Network (Pearl, 1988) and Markov Logic Network (Richardson and Domingos, 2006) as two repre"
2020.emnlp-tutorials.1,2020.emnlp-main.558,0,0.0147533,"gram with a specific function. By doing these, such systems are usually more interpretable than black-box neural networks. We will review knowledge graph reasoning (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretabilit"
2020.findings-emnlp.139,D14-1181,0,0.0124414,"Missing"
2020.findings-emnlp.139,P07-2045,0,0.00976133,"Missing"
2020.findings-emnlp.139,2020.acl-main.703,0,0.111381,"Missing"
2020.findings-emnlp.139,P16-1195,0,0.0204779,"eration, evaluated with smoothed BLEU-4 score. language could improve code-to-NL generation. Besides, results in the Table 4 show that CodeBERT pre-trained with RTD and MLM objectives brings a gain of 1.3 BLEU score over RoBERTa overall and achieve the state-of-the-art performance8 . 4.4 Generalization to Programming Languages NOT in Pre-training We would like to evaluate CodeBERT on the programming language which is never seen in the pretraining step. To this end, we study the task of generating a natural language summary of a C# code snippet. We conduct experiments on the dataset of CodeNN (Iyer et al., 2016)9 , which consists of 66,015 pairs of questions and answers automatically collected from StackOverflow. This dataset is challenging since the scale of dataset is orders of magnitude smaller than CodeSearchNet Corpus. We evaluate models using smoothed BLEU-4 score and use the same evaluation scripts as Iyer et al. (2016). M ODEL BLEU MOSES (KOEHN ET AL ., 2007) IR SUM-NN (RUSH ET AL ., 2015) 2- LAYER B I LSTM T RANSFORMER (VASWANI ET AL ., 2017) T REE LSTM (TAI ET AL ., 2015) C ODE NN (I YER ET AL ., 2016) CODE 2 SEQ (A LON ET AL ., 2019) RO BERTA PRE - TRAIN W / CODE ONLY C ODE BERT (RTD) C OD"
2020.findings-emnlp.139,C04-1072,0,0.0190066,"ly applied. Predicted probabilities of RoBERTa and CodeBERT are given. Code Documentation Generation Although the pre-training objective of CodeBERT does not include generation-based objectives (Lewis et al., 2019), we would like to investigate to what extent does CodeBERT perform on generation tasks. Specifically, we study code-to-NL generation, and report results for the documentation generation task on CodeSearchNet Corpus in six programming languages. Since the generated documentations are short and higher order n-grams may not overlap, we remedy this problem by using smoothed BLEU score (Lin and Och, 2004). 7 The example comes from https:// github.com/peri-source/peri/blob/ 61beed5deaaf978ab31ed716e8470d86ba639867/ peri/comp/psfcalc.py#L994-L1002 Model Comparisons We compare our model with several baselines, including a RNN-based model with attention mechanism (Sutskever et al., 2014), the Transformer (Vaswani et al., 2017), RoBERTa and the model pre-trained on code only. To demonstrate the effectiveness of CodeBERT on code-to-NL generation tasks, we adopt various pre-trained models as encoders and keep the hyperparameters consistent. Detailed hyper-parameters are given in Appendix B.3. Table 4"
2020.findings-emnlp.139,2021.ccl-1.108,0,0.209986,"Missing"
2020.findings-emnlp.139,N18-1202,0,0.370575,"lps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.1 1 Introduction Large pre-trained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019) ∗ Work done while this author was an intern at Microsoft Research Asia. 1 All the codes and data are available at https:// github.com/microsoft/CodeBERT and RoBERTa (Liu et al., 2019) have dramatically improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual representations from massive unlabeled text optimized by self-supervised objectives, such as masked language modeling, which predicts the original masked word from an artifici"
2020.findings-emnlp.139,D19-1250,0,0.0585014,"Missing"
2020.findings-emnlp.139,P19-1493,0,0.0463596,"Missing"
2020.findings-emnlp.139,D15-1044,0,0.118151,"Missing"
2020.findings-emnlp.139,P15-1150,0,0.0976114,"Missing"
2020.findings-emnlp.139,2020.tacl-1.48,0,0.0794978,"Missing"
2020.findings-emnlp.139,P02-1040,0,\N,Missing
2021.acl-long.442,P17-1152,0,0.0331223,"enforcing similar objects to be closer while keeping dissimilar objects further apart. It is often accompanied with leveraging task-specific inductive bias to augment similar and dissimilar examples. In this work, given an example of query and code (qi , ci ), we define our contrastive learning task on example itself, inbatch augmented examples (qi , cj ), and augmented example with rewritten query (qi0 , ci ). Hence, the overall training objective can be formulated as: ci = CodeBERT(ci ). (1) L = Lb + Lib + Lqr . Next we perform query-code matching through a multi-layer perceptron. Following Chen et al. (2017) and Mou et al. (2016), we concatenate the query embedding qi and code embedding ci with the element-wiseJdifference qi − ci and elementwise product qi ci , followed by a 1-layer feedforward neural network, to obtain a relation embedding: r(i,i) = tanh(W1 · [qi , ci , qi − ci , qi K ci ]). (2) We expect such an operation can help sharpen the cross information between query and code to capture better matching relationships such as contradiction. Then we put the relation embedding r(i,i) into a final 1-layer perceptron classifier with a sigmoid output layer: s(i,i) = sigmoid(W2 · r(i,i) ). Score"
2021.acl-long.442,D17-1070,0,0.0259629,"logy In this section, we first describe the model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The r"
2021.acl-long.442,2020.findings-emnlp.139,1,0.917023,"x in fragments[1:]) Label: With the growing population of software developers, natural language code search, which improves the productivity of the development process via retrieving semantically relevant code given natural language queries, is increasingly important in both communities of software engineering and natural language processing (Allamanis et al., 2018; Liu et al., 2020a). The key challenge is how to effectively measure the semantic similarity between a natural language query and a code. There are recent attempts to utilize deep neural networks (Gu et al., 2018; Wan et al., 2019; Feng et al., 2020), which embed query and code as dense vectors to perform semantic matching in a unified vector space. However, these models are ∗ 0 Figure 1: Two examples in CoSQA. A pair of a web query and a Python function with documentation is annotated with “1” or “0”, representing whether the code answers the query or not. Introduction Work done during internship at Microsoft Research Asia. The CoSQA data and leaderboard are available at https://github.com/microsoft/CodeXGLUE/tree/main/TextCode/NL-code-search-WebQuery. The code is available at https://github.com/Jun-jie-Huang/CoCLR 1 1 mostly trained on"
2021.acl-long.442,2020.acl-main.758,0,0.0222518,"ring. 2.1 Datasets A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al., 2018; Yin et al., 2018; 3 We study on Python in this work, and we plan to extend to more programming languages in the future. 2.2 5691 Code Search Models et al., 2019; Yao et al., 2019; Liu et al., 2019a; Feng et al., 2020; Zhao and Sun, 2020). There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020). 3 3.1 Data Collection Query Curation We use the search logs from the Microsoft Bing search engine as the source of queries. Queries without the keyword “python” are removed. Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3) conceptual queries, (4) tools usage, (5) programming knowledge, (6) vague queries and (7) others. Basically, queries in (2)-(7) categories are not likely to be answered only by a code function, since they may nee"
2021.acl-long.442,2021.ccl-1.108,0,0.0812067,"Missing"
2021.acl-long.442,I17-2053,0,0.0462943,"Missing"
2021.acl-long.442,P16-2022,0,0.016286,"ts to be closer while keeping dissimilar objects further apart. It is often accompanied with leveraging task-specific inductive bias to augment similar and dissimilar examples. In this work, given an example of query and code (qi , ci ), we define our contrastive learning task on example itself, inbatch augmented examples (qi , cj ), and augmented example with rewritten query (qi0 , ci ). Hence, the overall training objective can be formulated as: ci = CodeBERT(ci ). (1) L = Lb + Lib + Lqr . Next we perform query-code matching through a multi-layer perceptron. Following Chen et al. (2017) and Mou et al. (2016), we concatenate the query embedding qi and code embedding ci with the element-wiseJdifference qi − ci and elementwise product qi ci , followed by a 1-layer feedforward neural network, to obtain a relation embedding: r(i,i) = tanh(W1 · [qi , ci , qi − ci , qi K ci ]). (2) We expect such an operation can help sharpen the cross information between query and code to capture better matching relationships such as contradiction. Then we put the relation embedding r(i,i) into a final 1-layer perceptron classifier with a sigmoid output layer: s(i,i) = sigmoid(W2 · r(i,i) ). Score s(i,i) can be viewed"
2021.acl-long.442,D19-1410,0,0.0129035,"he model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The red lines and dashed lines denote the augmented e"
2021.acl-long.442,W18-3022,0,0.0178495,"we first describe the model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The red lines and dashed"
2021.acl-long.442,2020.findings-emnlp.361,0,0.0315257,"Gu et al., 2018; Cambronero Related Work In this part, we describe existing datasets and methods on code search and code question answering. 2.1 Datasets A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al., 2018; Yin et al., 2018; 3 We study on Python in this work, and we plan to extend to more programming languages in the future. 2.2 5691 Code Search Models et al., 2019; Yao et al., 2019; Liu et al., 2019a; Feng et al., 2020; Zhao and Sun, 2020). There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020). 3 3.1 Data Collection Query Curation We use the search logs from the Microsoft Bing search engine as the source of queries. Queries without the keyword “python” are removed. Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3) conceptual queries, (4) tools usage, (5) programming knowledge, (6) vague que"
2021.acl-long.442,2020.emnlp-main.36,0,0.0559045,"Missing"
2021.acl-long.62,N19-1423,0,0.011306,"r model optimization, we adopt the gradient descent algorithm. 4 Experiments We conduct extensive experiments across various settings and datasets. Following the previous work (Vaibhav et al., 2019), we use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), and LUN: Labeled Unreliable News Dataset (Rashkin et al., 2017) for our experiments. Table 1 shows the statistics. Our baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al., 2019) (BERT for sentence encoder and then LSTM for document encoder) and BERT (Devlin et al., 2019) (directly for document encoder). We also compare our model with graph neural models: GCN and GAT based on an undirected fully-connected sentence graph, which use attention pooling or max pooling for learning news document representation. For fair comparison with the previous work (Vaibhav et al., 2019), we use LSTM to encode sentences with randomly initialized word embeddings, which is the same as all the graph neural baselines. We run our model 5 times and report the micro-averaged (Precision = Recall = F1) and macro-averaged scores (Precision, Recall, F1) in all the settings including 2-way"
2021.acl-long.62,D19-1488,1,0.846914,"ntity e, we build a one-way directed edge from a sentence to the entity e, in order to allow only information propagation from sentences to entities. In this way, we can avoid integrating true entity knowledge directly into news representation, which may mislead the detection of fake news. 3.2 Heterogeneous Graph Convolution Based on the above directed heterogeneous document graph G, we develop a heterogeneous graph attention network for learning the news representation as well as the contextual entity representations. It considers not only the weights of different nodes with different types (Hu et al., 2019) but also the edge directions in the heterogeneous graph. Formally, we have three types T = {τ1 , τ2 , τ3 } of nodes: sentences S, topics T and entities E with different feature spaces. We apply LSTM to encode a sentence s = {w1 , · · ·, wm } and get its feature vector xs ∈ RM . The entity e ∈ E is initialized with the entity representations eKB ∈ RM learned from the external KB (see Subsection 3.3.1). The topic t ∈ T is initialized with one-hot vector xt ∈ RK . Next, consider the graph G = (V, E) where V and E represent the set of nodes and edges respectively. Let X ∈ R|V|×M be a matrix conta"
2021.acl-long.62,D14-1181,0,0.00516264,"training, Y is the corresponding label indicator matrix, Θ is the model parameters, and η is regularization factor. For model optimization, we adopt the gradient descent algorithm. 4 Experiments We conduct extensive experiments across various settings and datasets. Following the previous work (Vaibhav et al., 2019), we use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), and LUN: Labeled Unreliable News Dataset (Rashkin et al., 2017) for our experiments. Table 1 shows the statistics. Our baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al., 2019) (BERT for sentence encoder and then LSTM for document encoder) and BERT (Devlin et al., 2019) (directly for document encoder). We also compare our model with graph neural models: GCN and GAT based on an undirected fully-connected sentence graph, which use attention pooling or max pooling for learning news document representation. For fair comparison with the previous work (Vaibhav et al., 2019), we use LSTM to encode sentences with randomly initialized word embeddings, which is the same as all the graph neural baselines. We run our model 5 times and report th"
2021.acl-long.62,2020.lrec-1.747,0,0.141975,"past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted. External KB such as Wikipedia contains a large amount of high-quality"
2021.acl-long.62,P18-1022,0,0.0202518,"representation for improving fake news detection. Some works (Wang, 2017; Khattar et al., 2019; Wang et al., 2020) also consider incorporating multi-modal features such as images for improving fake news detection. Content-based Fake News Detection On the other hand, news contents contain the clues to differentiate fake and trusted news. A lot of existing works extract specific writing styles such as lexical and syntactic features (Conroy et al., 2015; Rubin et al., 2016; Khurana and Intelligentie, 2017; Rashkin et al., 2017; Shu et al., 2020; Oshikawa et al., 2020) and sensational headlines (Potthast et al., 2018; Sitaula et al., 2019) for fake news classifier. To avoid hand-crafted feature engineering, neural models have been proposed (Wang, 2017; Rodr´ıguez and Iglesias, 2019). For example, Ibrain et al. applied deep neural networks, such as BiLSTM and convolutional neural networks (CNN) for fake news detection (Rodr´ıguez and Iglesias, 2019). However, these works fail to consider different sentence interaction patterns between trusted and fake news documents. Vaibhav et al. proposed to model a document as a sentence graph capturing the sentence interactions and applied graph attention networks for"
2021.acl-long.62,D17-1317,0,0.247703,"2017). Fake news can easily misguide public opinion, cause the crisis of confidence, and disturb the social order (Vosoughi et al., 2018). It is well known that fake news exerted an influence in the past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing ap"
2021.acl-long.62,W16-0802,0,0.658737,"llcott and Gentzkow, 2017). Fake news can easily misguide public opinion, cause the crisis of confidence, and disturb the social order (Vosoughi et al., 2018). It is well known that fake news exerted an influence in the past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Alth"
2021.acl-long.62,D18-1209,0,0.0282805,"ural information from the triplets and textual information from the entity descriptions in the KB. 3.3.2 Entity Comparison We then perform entity-to-entity comparison between the news document and the KB, to capture the semantic consistency between the news content and the KB. We calculate a comparison vector ai between each contextual entity representation ec ∈ RN and its corresponding KB-based entity embedding eKB ∈ RM . ai = fcmp (ec , We · eKB ) , (6) where fcmp () denotes the comparison function, and We ∈ RN ×M is a transformation matrix. To measure the embedding closeness and relevance (Shen et al., 2018), we design our comparison function as: fcmp (x, y) = Wa [x − y, x y], (7) where Wa ∈ RN ×2N is a transformation matrix and is hadamard product, i.e., element-wise product. The final output comparison feature vector C ∈ RN is obtained by the max pooling over the alignment vectors A = [a1 , a2 , ..., an ] of all the entities E = {e1 , e2 , ..., en } in the news document. 3.4 Model Training After obtaining the comparison vector C ∈ RN and the final news document representation vector Hd ∈ RN , we concatenate and feed them into a Softmax layer for fake news classification. Formally, Z = Softmax(W"
2021.acl-long.62,N18-1074,0,0.0329643,"Missing"
2021.acl-long.62,D19-5316,0,0.253593,"rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted. External KB such as Wikipedia contains a large amount of high-quality structured subjectpredicate-object triplets and unstructured entity descriptions, which could serve as evidence for detecting fake news. As shown in Figure 4, the news 754 Proceedings of the 59th Annual Meeting of the Association for Computation"
2021.acl-long.62,P17-2067,0,0.177697,"ial elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted. External KB such as Wikipedia contains a large amount of high-quality structured s"
2021.acl-long.62,D19-1471,0,0.0120727,"ou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based. 2.1 Social-based Fake News Detection Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019). Tacchini et al. constructed a bipartite network of user and posts with ‘like’ stance information, and proposed a semisupervised probabilistic model to predict the likelihood of posts being hoaxes (Tacchini et al., 2017). Propagation-based approaches for fake news detection are based on the basic assumption that the credibility of a news event is highly related to the credibilities of relevant social media posts. Both 755 Figure 1: An example of directed heterogeneous document graph incorporating topics and entities. homogeneous (Jin et al., 2016) and heterogeneous credibility networks (Gupta"
2021.acl-long.62,2020.acl-main.549,1,0.77437,"he KB. 3) Extensive experiments on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art models on fake news detection by effectively incorporating external knowledge and topic information. 2 Related Work Fake news detection has attracted much attention in recent years (Zhou and Zafarani, 2020; Oshikawa et al., 2020). A lot of works also focus on the related problem, i.e., fact checking, which aims to search evidence from external knowledge to verify the veracity of a claim (e.g., a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based. 2.1 Social-based Fake News Detection Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019). Tacchini et al. co"
2021.acl-long.62,P19-1085,0,0.0180519,"mpare the news to the KB. 3) Extensive experiments on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art models on fake news detection by effectively incorporating external knowledge and topic information. 2 Related Work Fake news detection has attracted much attention in recent years (Zhou and Zafarani, 2020; Oshikawa et al., 2020). A lot of works also focus on the related problem, i.e., fact checking, which aims to search evidence from external knowledge to verify the veracity of a claim (e.g., a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based. 2.1 Social-based Fake News Detection Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019"
2021.findings-acl.121,P19-1279,0,0.0206704,"dopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In particular, (1) K-A DAPTER models outperform RoBERTa, which proves the effectiveness of infusing knowledge in"
2021.findings-acl.121,P18-1009,0,0.0216531,"d analyses with the case study and probing experiments to explore the effectiveness and ability of models for learning factual knowledge. The notations of K-A DAPTER (F+L), K-A DAPTER (F), and K-A DAPTER (L) denote our model which consists of both factual adapter and linguistic adapter, only factual adapter and only linguistic adapter, respectively. Implementation details, and statistics of datasets are in the Appendix. 4.1 Entity Typing We conduct experiments on fine-grained entity typing which aims to predict the types of a given entity and its context. We evaluate our models on OpenEntity (Choi et al., 2018) and FIGER (Ling et al., 2015) following the same split setting as Zhang et al. (2019). To fine-tune our models for entity typing, we modify the input token sequence by adding the special token “@” before and after a certain entity, then the first “@” special token representation is adopted to perform classification. As for OpenEntity, we adopt micro F1 score as the final metric to represent the model performance. As for FIGER, we adopt strict accuracy, loose macro, loose micro F1 scores (Ling and Weld, 2012) for evaluation following the same evaluation criteria used in previous works. Baselin"
2021.findings-acl.121,P19-1285,0,0.0618623,"Missing"
2021.findings-acl.121,N19-1423,0,0.0429486,"texttriplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained via dependency parsing. Results on three knowledge-driven tasks, including relation classification, entity typing, and question answering, demonstrate that each adapter improves the performance and the combination of both adapters brings further improvements. Further analysis indicates that K-A DAPTER captures versatile knowledge than RoBERTa. 1 1 Introduction Language representation models, which are pretrained on large-scale text corpus through unsupervised objectives like (masked) language modeling, such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018, 2019), XLNet (Yang et al., ∗ Work is done during internship at Microsoft. Zhongyu Wei and Duyu Tang are corresponding authors. 1 Codes are publicly available at https://github. com/microsoft/K-Adapter 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), have established state-of-the-art performances on various NLP downstream tasks. Despite the huge success of these pre-trained models in empirical studies, recent studies suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) sugges"
2021.findings-acl.121,L18-1544,0,0.0249766,"ayer, we denote the number of transformer layer as N , the hidden dimension of transformer layer as HA , the number of self-attention heads as AA , the hidden dimension of down-projection and up-projection layers as Hd and Hu . In detail, we have the following adapter size: N = 2, HA = 768, AA = 12, Hu = 1024 and Hd = 768. The RoBERTa lay2 3.3 Factual Adapter Factual knowledge can be described as the basic information that is concerned with facts. In this work, we acquire factual knowledge from the relationships among entities in natural language. We extract a sub-dataset T-REx-rc from T-REx (ElSahar et al., 2018) which is a large scale alignment dataset between Wikipedia abstracts and Wikidata triples. We discard all relations having less than 50 entity pairs, collecting 430 relations and 5.5M sentences. In order to inject factual knowledge, we propose to pre-train a knowledge-specific adapter called facAdapter on the relation classification task. This task requires a model to classify relation labels of given entity pairs based on context. Specifically, the last hidden features of RoBERTa and facAdapter are concatenated as the input representation, and the pooling layer is applied to the input repres"
2021.findings-acl.121,2020.findings-emnlp.71,0,0.198207,"Missing"
2021.findings-acl.121,W16-1313,0,0.118964,"-the-shell dependency parser from Stanford Parser3 on a part of Book Corpus (Zhu et al., 2015). To inject linguistic knowledge, we pre-train another knowledge-specific adapter called linAdapter on the task of dependency relation prediction. This task aims to predict the head index of each token in the given sentence. We concatenate the last hidden features of RoBERTa and linAdapter as the input representation, and then apply a linear layer 3 https://github.com/huggingface/transformers 1408 http://nlp.stanford.edu/software/lex-parser.html OpenEntity Model FIGER P R Mi-F1 Acc Ma-F1 Mi-F1 NFGEC (Shimaoka et al., 2016) BERT-base (Zhang et al., 2019) ERNIE (Zhang et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2021) WKLM (Xiong et al., 2020) 68.80 76.37 78.42 78.60 77.20 - 53.30 70.96 72.90 73.70 74.20 - 60.10 73.56 75.56 76.10 75.70 - 55.60 52.04 57.19 60.21 75.15 75.16 75.61 81.99 71.73 71.63 73.39 77.00 RoBERTa RoBERTa + multitask K-A DAPTER (w/o knowledge) K-A DAPTER (F) K-A DAPTER (L) K-A DAPTER (F+L) 77.55 77.96 74.47 79.30 80.01 78.99 74.95 76.00 74.91 75.84 74.00 76.27 76.23 76.97 76.17 77.53 76.89 77.61 56.31 59.86 56.93 59.50 61.10 61.81 82.43 84.45 82.56 84.52 83.61 84.87 77.83 7"
2021.findings-acl.121,D18-1244,0,0.0240646,"ion classification aims to determine the correct relation between two entities in a given sentence. We adopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In part"
2021.findings-acl.121,D17-1004,0,0.069357,"Missing"
2021.findings-acl.121,P19-1139,0,0.231713,"s suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) suggest that although language models do well in reasoning about the surface form of entity names, they fail in capturing rich factual knowledge. Kassner and Sch¨utze (2020) observe that BERT mostly did not learn the meaning of negation (e.g. “not”). These observations motivate us to study the injection of knowledge into pre-trained models like BERT and RoBERTa. Recently, some efforts have been made to exploit injecting knowledge into pre-trained language models (Zhang et al., 2019; Lauscher et al., 2019; Levine et al., 2020; Peters et al., 2019; He et al., 2020; Xiong et al., 2020). Most previous works (as shown in Table 1) augment the standard language modeling objective with knowledge-driven objectives and update the entire model parameters. Although these methods obtain better performance on downstream tasks, they struggle at supporting the development of versatile models with multiple kinds of knowledge injected (Kirkpatrick et al., 2017). When new kinds of knowledge are injected, model parameters need to be retrained so that previously injected knowledge would fad"
2021.findings-acl.121,P17-1018,1,0.80079,"elected. We report accuracy scores obtained from the leaderboard. Open-domain QA aims to answer questions using external resources such as collections of documents and webpages. We evaluate our modes on two public datasets, i.e., Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). Specifically, we first retrieve paragraphs corresponding to the question using the information retrieval system and then extract the answer from these retrieved paragraphs through the reading comprehension technique. Following previous work(Lin et al., 2018), we use the retrieved paragraphs provided by Wang et al. (2017) for these two datasets. To fine-tune our models for this task, the input token sequence is modified as “<SEP>question </SEP>paragraph</SEP>”. We apply linear layers over the last hidden features of our model to predict the start and end position of the answer span. We adopt two metrics including ExactMatch (EM) and loose F1 (Ling and Weld, 2012) scores to evaluate our models. Baselines BERT-FTRACE+SW AG (Huang et al., 2019) is the BERT model sequentially fine-tuned on both RACE and SWAG datasets. BiDAF (Seo et al., 2017) adopts a bi-directional attention network. AQA (Buck et al., 2018) propo"
2021.findings-acl.129,2021.acl-long.353,0,0.0712387,"Missing"
2021.findings-acl.129,2021.ccl-1.108,0,0.0436956,"Missing"
2021.findings-acl.129,D15-1167,1,0.706881,"redict the sentiment label of the text. More specifically, we study sentiment analysis in a few-shot learning scenario, where (1) instances in the test set are written by users never seen in the training set and (2) each user in the test set is also paired with a dozen of text-label pairs used for few-shot learning. To the best of our knowledge, there is no existing datasets meeting our demands, so we create two datasets by ourselves. One dataset comes from Diao et al. (2014), where each text is a movie review on IMDB and the sentiment label (rating) is from 1 to 10. The other dataset is from Tang et al. (2015), where each text is a restaurant review from Yelp and the sentiment label is from 1 to 5. Each dataset includes two parts: (1) part A consisting of massive user data for training a general classification model; (2) part B used for few-shot learning. To ensure that each user in part B is never seen in the training set of A, we separate these datasets based on users. To support few-shot learning, we have a constraint on the users in part B that they only write no more than 50 reviews. The data statistics are shown in Table 1. # of users 783 229 3,247 1,213 Index Input 1 2 3 4 5 6 7 8 9 ?? ?1 ?2"
2021.findings-emnlp.23,S15-2045,0,0.0289652,"Missing"
2021.findings-emnlp.23,S14-2010,0,0.060457,"Missing"
2021.findings-emnlp.23,S16-1081,0,0.0638689,"Missing"
2021.findings-emnlp.23,S12-1051,0,0.0224819,"trix of eigenvectors, satisfying Cov(E) = U DU T . 4 Experiment We evaluate sentence embeddings on the task of unsupervised semantic textual similarity. We show experimental results and report the best way to derive unsupervised sentence embedding from PLMs. 4.1 Experiment Settings Task and Datasets The task of unsupervised semantic textual similarity (STS) aims to predict the similarity of two sentences without direct supervision. We experiment on seven STS datasets, namely the STS-Benchmark (STS-B) (Cer et al., 2017), the SICK-Relatedness (Marelli et al., 2014), and the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). These datasets consist of sentence pairs with labeled semantic similarity scores ranging from 0 to 5. Evaluation Procedure Following the procedures in SBERT (Reimers and Gurevych, 2019), we first derive sentence embeddings for each sentence pair and compute their cosine similarity score as the predicted similarity. Then we compute the Spearman’s rank correlation coefficient between the predicted similarity and gold standard similarity scores as the evaluation metric. We average the Spearman’s coefficients among the seven datasets as the final correlation score. White"
2021.findings-emnlp.23,S17-2001,0,0.0115647,"e covariance matrix Cov(E) = (E − m)T (E − m) ∈ Rd×d and U is the corresponding orthogonal matrix of eigenvectors, satisfying Cov(E) = U DU T . 4 Experiment We evaluate sentence embeddings on the task of unsupervised semantic textual similarity. We show experimental results and report the best way to derive unsupervised sentence embedding from PLMs. 4.1 Experiment Settings Task and Datasets The task of unsupervised semantic textual similarity (STS) aims to predict the similarity of two sentences without direct supervision. We experiment on seven STS datasets, namely the STS-Benchmark (STS-B) (Cer et al., 2017), the SICK-Relatedness (Marelli et al., 2014), and the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). These datasets consist of sentence pairs with labeled semantic similarity scores ranging from 0 to 5. Evaluation Procedure Following the procedures in SBERT (Reimers and Gurevych, 2019), we first derive sentence embeddings for each sentence pair and compute their cosine similarity score as the predicted similarity. Then we compute the Spearman’s rank correlation coefficient between the predicted similarity and gold standard similarity scores as the evaluation metric. We ave"
2021.findings-emnlp.23,D14-1162,0,0.103087,"Missing"
2021.findings-emnlp.23,D19-1410,0,0.402486,"We study on four pretrained models and conduct massive experiments on seven datasets regarding sentence semantics. We have three main findings. First, averaging all tokens is better than only using [CLS] vector. Second, combining both top and bottom layers is better than only using top layers. Lastly, an easy whitening-based vector normalization strategy with less than 10 lines of code consistently boosts the performance. 1 1 Introduction Pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019) perform well on learning sentence semantics when fine-tuned with supervised data (Reimers and Gurevych, 2019; Thakur et al., 2020). However, in practice, especially when a large amount of supervised data is unavailable, an approach that provides sentence embeddings in an unsupervised way is of great value in scenarios like sentence matching and retrieval. While there are attempts on unsupervised sentence embeddings (Arora et al., 2017; Zhang et al., 2020), to the best of our knowledge, there is no comprehensive study on various PLMs with regard to multiple factors. Meanwhile, we aim to provide an easy-touse toolkit that can be used to produce sentence embeddings upon various PLMs. In this paper, we"
2021.findings-emnlp.23,D19-1059,0,0.0503764,"Missing"
2021.findings-emnlp.23,2020.emnlp-main.124,0,0.136199,"ith less than 10 lines of code consistently boosts the performance. 1 1 Introduction Pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019) perform well on learning sentence semantics when fine-tuned with supervised data (Reimers and Gurevych, 2019; Thakur et al., 2020). However, in practice, especially when a large amount of supervised data is unavailable, an approach that provides sentence embeddings in an unsupervised way is of great value in scenarios like sentence matching and retrieval. While there are attempts on unsupervised sentence embeddings (Arora et al., 2017; Zhang et al., 2020), to the best of our knowledge, there is no comprehensive study on various PLMs with regard to multiple factors. Meanwhile, we aim to provide an easy-touse toolkit that can be used to produce sentence embeddings upon various PLMs. In this paper, we investigate PLMs-based unsupervised sentence embeddings from three aspects. First, a standard way of obtaining sentence embedding is to pick the vector of [CLS] token. We explore whether using the hidden vectors of other tokens is beneficial. Second, some works suggest producing sentence embedding from the last layer or the combination of the last t"
C14-1018,baccianella-etal-2010-sentiwordnet,0,0.227345,"or negative score reflecting its sentiment polarity and strength. Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale se"
C14-1018,J81-4005,0,0.756294,"Missing"
C14-1018,P07-1054,0,0.0130165,"component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym"
C14-1018,P12-1092,0,0.137724,"ons between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment"
C14-1018,C04-1200,0,0.143042,"ng the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are"
C14-1018,P13-2087,0,0.0738275,"hrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words an"
C14-1018,P12-1043,0,0.0177108,"with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item."
C14-1018,C94-1079,0,0.0290494,"nt lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentim"
C14-1018,P11-1015,0,0.623804,"ntation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase em"
C14-1018,S13-2053,0,0.722061,"(2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentiment lexicon, Mohammad et al. (2013) use pointwise mutual information (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and :(. Chen et al. (2012) utilize the Urban Dictionary and extract the target-dependent sentiment expressions from Twitter. Unlike Mohammad et al. (2013) that only capture the relations between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of word"
C14-1018,S13-2052,0,0.0305225,"Missing"
C14-1018,W02-1011,0,0.0230923,"m tweets, leveraging massive tweets containing positive and negative emoticons as training set without any manual annotation. To obtain more training data for building the phrase-level sentiment classifier, we exploit the similar words from Urban Dictionary 2 , which is a crowd-sourcing resource, to expand a small list of sentiment seeds. Finally, we utilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting in the sentiment lexicon. We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. T"
C14-1018,J11-1002,0,0.0117166,"timent information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approach, as illustrated in Figure 1. We cast sentiment lexicon learning as a phrase-level cla"
C14-1018,E09-1077,0,0.0168758,"uilt manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resource"
C14-1018,D11-1014,0,0.0867976,"an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodology In this section, we describe our method for building large-scale sentiment lexicon from Twitter within a classification framework, as illustrated in Figure 1. We leverage the continuous representation of phrases as features, without parsers or hand-crafted rules, and automatically obtain the training data by seed expansion from Urban Dictionary. After the classifier is built, we employ it to predict the sentiment distribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon. To encode the sentimen"
C14-1018,D13-1170,0,0.0317488,"Missing"
C14-1018,P14-1146,1,0.621391,"d in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodolo"
C14-1018,P02-1053,0,0.0288544,"eets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment sco"
C14-1018,N10-1119,0,0.0358758,"entiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approac"
C14-1018,H05-1044,0,0.940027,"eness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. The main contributions of this work are as follows: • To our best knowledge, this is the first work that leverages the continuous representation of phrases for building large-scale sentiment lexicon from Twitter; • We propose a tailored neural architecture for learning the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework fo"
C14-1018,P13-1173,0,0.0128539,"negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this directi"
C14-1018,P10-1040,0,\N,Missing
C16-1276,W09-1604,0,0.076015,"Missing"
C16-1276,P16-2011,1,0.871694,"Missing"
C16-1276,P16-1212,0,0.0613039,"Missing"
C16-1276,D11-1014,0,0.087629,"previously obtained entity representation and relational representation, both of which play important roles for representing the meaning of a triple. Furthermore, a better approach should benefit from both aspects, and integrate them in triple semantic with an automatic method. To this end, we introduce a gated neural network in this part. It takes entity and relational vectors of a triple as input, and adaptively produces the composed continuous representation of them. Given ve and vr as inputs, a traditional compositional function is to concatenate ve and vr and feed them to a linear layer (Socher et al., 2011), which is calculated as Equation 2. Despite its computational efficiency, tied parameters cannot easily capture the complex linguistic phenomena in natural language expressions. v˜ = tanh(We ve + Wr vr + b) (2) α = σ(Weg ve + Wrg vr + bg ) (3) v(t) = α · vr + (1 − α) · v˜ (4) Therefore, we add a neural gate to change parameter values for different input vectors ve and vr , which is partly inspired by the recent success of gated recurrent neural network (Cho et al., 2014; Chung et al., 2015) and Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Tai et al., 2015). And our gated neural n"
C16-1276,P13-1045,0,0.111394,"c calculating process based on different triples rather than a fixed model. In this way, source triple and target triple are naturally encoded in the same semantic vector space. We design a ranking-type hinge loss function to effectively train the parameters of neural networks. We evaluate the effectiveness of our method on a manually created corpus. We conduct experiments in two settings. Empirical results show that the proposed method consistently outperforms baseline methods. We also show that the use of gated neural network improves strong composition models such as neural tensor network (Socher et al., 2013b) in terms of translation accuracy. The main contributions of this work are as follows: • We introduce an approach based on representation learning for English-Chinese KB translation in this paper. • We present a gated neural network to adaptively integrate entity and relational level evidences in triple representation. • We build a dataset for English-Chinese KB translation, and report the superior performance of our method over baseline methods on it. 2 The Approach In this section, we present our neural network method for KB Translation in detail. Figure 2 displays a high-level overview of"
C16-1276,D13-1170,0,0.158804,"c calculating process based on different triples rather than a fixed model. In this way, source triple and target triple are naturally encoded in the same semantic vector space. We design a ranking-type hinge loss function to effectively train the parameters of neural networks. We evaluate the effectiveness of our method on a manually created corpus. We conduct experiments in two settings. Empirical results show that the proposed method consistently outperforms baseline methods. We also show that the use of gated neural network improves strong composition models such as neural tensor network (Socher et al., 2013b) in terms of translation accuracy. The main contributions of this work are as follows: • We introduce an approach based on representation learning for English-Chinese KB translation in this paper. • We present a gated neural network to adaptively integrate entity and relational level evidences in triple representation. • We build a dataset for English-Chinese KB translation, and report the superior performance of our method over baseline methods on it. 2 The Approach In this section, we present our neural network method for KB Translation in detail. Figure 2 displays a high-level overview of"
C16-1276,P15-1150,0,0.152104,"m to a linear layer (Socher et al., 2011), which is calculated as Equation 2. Despite its computational efficiency, tied parameters cannot easily capture the complex linguistic phenomena in natural language expressions. v˜ = tanh(We ve + Wr vr + b) (2) α = σ(Weg ve + Wrg vr + bg ) (3) v(t) = α · vr + (1 − α) · v˜ (4) Therefore, we add a neural gate to change parameter values for different input vectors ve and vr , which is partly inspired by the recent success of gated recurrent neural network (Cho et al., 2014; Chung et al., 2015) and Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Tai et al., 2015). And our gated neural network is inspired by highway network, which allow the model to suffer less from the vanishing gradient problem (Srivastava et al., 2015a; Srivastava et al., 2015b). The gate takes ve and vr as inputs, and outputs as a weight α ∈ [0, 1], which linearly weights the two parts. Specifically, the gate is calculated as Equation 3, where σ is standard sigmoid function, Weg , Wrg and bg are parameters. Triple representation v(t) is calculated as given in Equation 4, which linearly weights the candidate composed representation v˜ and relational representation vr . In this way,"
C16-1276,D13-1141,0,0.02453,"ing bilingual parallel corpus, these word vectors are mapped into different semantic spaces. This is not desirable for comparing the semantic relatedness between English triple and Chinese triple. We use linear layers to transform English and Chinese word vectors in a same semantic vector space. A simple linear layer is calculated as ve = W e + b, where W and b are the parameters. One could also learn bilingual 3 code.google.com/p/word2vec/ https://dumps.wikimedia.org/ 5 http://baike.baidu.com/ 4 2937 word vectors simultaneously from bilingual parallel corpus with tailored learning algorithm (Zou et al., 2013). We leave this as a future work and we believe our method could benefit from the bilingual word embeddings. 2.2.2 Relational Representation We model the semantic relatedness between entities in this part. The basic idea is that the semantic relatedness between entities is determined by the semantics of entities and their relations. Based on this, we utilize neural tensor network, which is one of state-of-the-art semantic composition approach for natural language processing tasks (Mitchell and Lapata, 2010; Socher et al., 2013a; Jenatton et al., 2012). A standard neural tensor with rank 3 is e"
C16-1311,D15-1263,0,0.00993495,"compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that 3305 incorporating target information could boost the performance of a long short-term memory model. The target-dependent LSTM model obtains state-of-the-art classification accuracy. Acknowledgements We greatly thank Yaming Sun for tremendously helpful discus"
C16-1311,P14-2009,1,0.714033,"ccuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.1 1 Introduction Sentiment analysis, also known as opinion mining (Pang and Lee, 2008; Liu, 2012), is a fundamental task in natural language processing and computational linguistics. Sentiment analysis is crucial to understanding user generated text in social networks or product reviews, and has drawn a lot of attentions from both industry and academic communities. In this paper, we focus on target-dependent sentiment classification (Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015), which is a fundamental and extensively studied task in the field of sentiment analysis. Given a sentence and a target mention, the task calls for inferring the sentiment polarity (e.g. positive, negative, neutral) of the sentence towards the target. For example, let us consider the sentence: “I bought a new camera. The picture quality is amazing but the battery life is too short”. If the target string is picture quality, the expected sentiment polarity is “positive” as the sentence expresses a positive opinion towards picture quality. If we consider the target as battery"
C16-1311,P11-1016,0,0.0890004,"the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.1 1 Introduction Sentiment analysis, also known as opinion mining (Pang and Lee, 2008; Liu, 2012), is a fundamental task in natural language processing and computational linguistics. Sentiment analysis is crucial to understanding user generated text in social networks or product reviews, and has drawn a lot of attentions from both industry and academic communities. In this paper, we focus on target-dependent sentiment classification (Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015), which is a fundamental and extensively studied task in the field of sentiment analysis. Given a sentence and a target mention, the task calls for inferring the sentiment polarity (e.g. positive, negative, neutral) of the sentence towards the target. For example, let us consider the sentence: “I bought a new camera. The picture quality is amazing but the battery life is too short”. If the target string is picture quality, the expected sentiment polarity is “positive” as the sentence expresses a positive opinion towards picture quality. If we consider th"
C16-1311,P14-1062,0,0.0547208,"entence/document level sentiment classification, previous studies mostly have two steps. They first learn continuous word vector embeddings from data (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, an"
C16-1311,D14-1181,0,0.00785348,"iment classification, previous studies mostly have two steps. They first learn continuous word vector embeddings from data (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that"
C16-1311,D15-1278,0,0.108959,"r semantic composition in the area of sentiment 3299 Softmax Target-Connection Long Short-Term Memory ℎ1 ℎ? LSTML LSTML ?? ?1 …… ℎ?+1 LSTML LSTML ??+1 ℎ?−1 ℎ?+1 LSTMR LSTMR ??+1 ??−1 ℎ? ℎ?−1 …… ??−1 …… ℎ? LSTMR LSTMR ?? ?? …… ??????? Figure 2: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {wl+1 , wl+2 , ..., wr−1 } are target words, vtarget is target representation, {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is"
C16-1311,P15-1107,0,0.565302,"r semantic composition in the area of sentiment 3299 Softmax Target-Connection Long Short-Term Memory ℎ1 ℎ? LSTML LSTML ?? ?1 …… ℎ?+1 LSTML LSTML ??+1 ℎ?−1 ℎ?+1 LSTMR LSTMR ??+1 ??−1 ℎ? ℎ?−1 …… ??−1 …… ℎ? LSTMR LSTMR ?? ?? …… ??????? Figure 2: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {wl+1 , wl+2 , ..., wr−1 } are target words, vtarget is target representation, {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is"
C16-1311,W02-1011,0,0.0501051,"e model. A potential reason might be that the attention based LSTM has larger number of parameters, which cannot be easily optimized with the small number of corpus. 4 Related Work We briefly review existing studies on target-dependent sentiment classification and neural network approaches for sentiment classification in this section. 4.1 Target-Dependent Sentiment Classification Target-dependent sentiment classification is typically regarded as a kind of text classification problem in literature. Therefore, standard text classification approach such as feature-based Supported Vector Machine (Pang et al., 2002; Jiang et al., 2011) can be naturally employed to build a sentiment classifier. Despite the effectiveness of feature engineering, it is labor intensive and unable to discover the discriminative or explanatory factors of data. To handle this problem, some recent studies (Dong et al., 2014; Vo and Zhang, 2015) use neural network methods and encode each sentence in continuous and low-dimensional vector space without feature engineering. Dong et al. (2014) transfer a dependency tree of a sentence into a target-specific recursive structure, and get higher level representation based on that structu"
C16-1311,D14-1162,0,0.120323,", {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. In this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms (Pennington et al., 2014; Tang et al., 2014) to make better use of semantic and grammatical associations of words. We use LSTM to compute the vector of a sentence from the vectors of words it contains, an illustration of the model is shown in Figure 1. LSTM is a kind of recurrent neural network (RNN), which is capable of mapping"
C16-1311,D13-1170,0,0.0826717,"ew of TC-LSTM is illustrated in Figure 2. The input of TC-LSTM is a sentence consisting of n words {w1 , w2 , ...wn } and a target string t occurs in the sentence. We represent target t as {wl+1 , wl+2 ...wr−1 } because a target could be a word sequence of variable length, such as “google” or “harry potter”. When processing a sentence, we split it into three components: target words, preceding context words and following context words. We obtain target vector vtarget by averaging the vectors of words it contains, which has been proven to be simple and effective in representing named entities (Socher et al., 2013a; Sun et al., 2015). When compute the hidden vectors of preceding and following context words, we use two separate long short-term memory models, which are similar with the strategy used in TD-LSTM. The difference is that in TC-LSTM the input at each position is the concatenation of word embedding and target vector vtarget , while in TD-LSTM the input at each position only includes 3301 the embedding of current word. We believe that TC-LSTM could make better use of the connection between target and each context word when building the representation of a sentence. 2.4 Model Training We train L"
C16-1311,P15-1150,0,0.0526433,"ontinuous word vector embeddings from data (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that 3305 incorporating target information could boost the performance of a long short-term"
C16-1311,P14-1146,1,0.913339,"e preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. In this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms (Pennington et al., 2014; Tang et al., 2014) to make better use of semantic and grammatical associations of words. We use LSTM to compute the vector of a sentence from the vectors of words it contains, an illustration of the model is shown in Figure 1. LSTM is a kind of recurrent neural network (RNN), which is capable of mapping vectors of words wi"
C16-1311,D15-1167,1,0.590323,"tion in the area of sentiment 3299 Softmax Target-Connection Long Short-Term Memory ℎ1 ℎ? LSTML LSTML ?? ?1 …… ℎ?+1 LSTML LSTML ??+1 ℎ?−1 ℎ?+1 LSTMR LSTMR ??+1 ??−1 ℎ? ℎ?−1 …… ??−1 …… ℎ? LSTMR LSTMR ?? ?? …… ??????? Figure 2: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {wl+1 , wl+2 , ..., wr−1 } are target words, vtarget is target representation, {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of wor"
C16-1311,N16-1174,0,0.0298263,"f a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that 3305 incorporating target information could boost the performance of a long short-term memory model. The target-dependent LSTM model obtains state-of-the-art classification accuracy. Acknowledgements We greatly thank Yaming Sun for tremendously helpful discussions. This work was"
D14-1054,D08-1083,0,0.0209071,"Sentiment Analysis Duyu Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contai"
D14-1054,P14-2009,1,0.766966,"hou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does"
D14-1054,P13-2087,0,0.0212728,"using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is"
D14-1054,P11-2008,0,0.131527,"Missing"
D14-1054,P11-1015,0,0.3622,"cial Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic ch"
D14-1054,D13-1171,0,0.046832,"Missing"
D14-1054,J11-2001,0,0.0176143,"notated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like e"
D14-1054,S13-2053,0,0.183839,"such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does not capture the phrasal information. The segmentations based on syntactic chunkers typically aim to identify noun groups, verb groups or named entities from a sentence. However, many sentiment indicators are phrases constituted of adjectives, negations, adverbs or idioms (Liu, 2012; Mohammad et al., 2013a), which are splitted by syntactic chunkers. Besides, a better approach would be to utilize the sentiment information to improve the segmentor. Accordingly, the sentiment-specific segmentor will enhance the performance of sentiment classification in turn. In this paper, we propose a joint segmentation and classification framework (JSC) for sentiment analysis, which simultaneous conducts sentence segmentation and sentence-level sentiment classification. The framework is illustrated in FigIn this paper, we propose a joint segmentation and classification framework for sentiment analysis. Existin"
D14-1054,C14-1018,1,0.854768,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,N10-1120,0,0.161612,"Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, b"
D14-1054,P14-1146,1,0.670131,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,pak-paroubek-2010-twitter,0,0.0427636,"pically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like emoticons (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Zhao et al., 2012). Majority of existing approaches follow Pang et al. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learnin"
D14-1054,P10-1141,0,0.148379,"Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-w"
D14-1054,P02-1053,0,0.0058165,"the joint model from sentences annotated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corp"
D14-1054,W02-1011,0,0.0553792,"fication performance. The joint model is trained only based on the annotated sentiment polarity of sentences, without any segmentation annotations. Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that, our joint model performs comparably with the state-of-the-art methods. 1 Introduction Sentiment classification, which classifies the sentiment polarity of a sentence (or document) as positive or negative, is a major research direction in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Majority of existing approaches follow Pang et al. (2002) and treat sen∗ This work was partly done when the first and fourth authors were visiting Microsoft Research. 477 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477–487, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics CG SC SEG SEG SC that is not bad -1 <+1,-1> NO 0.6 0.6 that is not bad that is not bad -1 <+1,-1> NO 0.4 0.4 Polarity: +1 that is not bad +1 <+1,+1> YES 2.3 2.3 that is not bad +1 <+1,+1> YES 1.6 1.6 Segmentations Polarity Update Rank Update Input Top K Figure 1: The joint segmentation and c"
D14-1054,P12-2018,0,0.141338,"l. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learning effective features to obtain better classification performance. On movie or product reviews, Wang and Manning (2012) present NBSVM, which trades-off • To our knowledge, this is the first work that automatically produces sentence segmentation for sentiment classification within a joint framework. • We show that the joint model yields comparable performance with the state-of-the-art methods on the benchmark Twitter sentiment classification datasets in SemEval 2013. 478 overview of the proposed joint segmentation and classification model (JSC) for sentiment analysis. The segmentation candidate generation model and the segmentation ranking model are described in Section 4. The details of the sentiment classific"
D14-1054,H05-1044,0,0.0828802,"Missing"
D14-1054,D11-1014,0,0.256554,"ytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a sepa"
D14-1054,D13-1016,0,0.0546336,"Missing"
D14-1054,D12-1110,0,0.0455462,"Missing"
D14-1054,D11-1016,0,0.0130106,"the document feature. On Twitter, Mohammad et al. (2013b) develop a state-of-the-art Twitter sentiment classifier in SemEval 2013, using a variety of sentiment lexicons and hand-crafted features. With the revival of deep learning (representation learning (Hinton and Salakhutdinov, 2006; Bengio et al., 2013; Jones, 2014)), more recent studies focus on learning the low-dimensional, dense and real-valued vector as text features for sentiment classification. Glorot et al. (2011) investigate Stacked Denoising Autoencoders to learn document vector for domain adaptation in sentiment classification. Yessenalina and Cardie (2011) represent each word as a matrix and compose words using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabili"
D14-1054,P14-1011,1,0.830375,"ords or phrases of variable length. Under this scenario, phrase embedding is highly suitable as it is capable to represent phrases with different length into a consistent distributed vector space (Mikolov et al., 2013). For each phrase, phrase embedding is a dense, real-valued and continuous vector. After the phrase embedding is trained, the nearest neighbors in the embedding space are favored to have similar grammatical usages and semantic meanings. The effectiveness of phrase embedding has been verified for building large-scale sentiment lexicon (Tang et al., 2014a) and machine translation (Zhang et al., 2014). We learn phrase embedding with Skip-Gram model (Mikolov et al., 2013), which is the state-of(2) k where φij is the segmentation score of Ωij ; sf eijk is the k-th segmentation feature of Ωij ; w and b are the parameters of the segmentation ranking model. During training, given a sentence si and its gold sentiment polarity polig , the optimization objec2 j∈Hi Segmentation-Specific Feature We empirically design four segmentation-specific features to reflect the information of each segmentation, as listed in Table 3. The objective of the segmentation ranking model is to assign a scalar to each"
D14-1054,D13-1170,0,0.203308,"ng Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal"
D14-1054,J13-3004,0,\N,Missing
D14-1054,P11-1016,1,\N,Missing
D15-1167,P14-1023,0,0.00747045,"xt generation task. Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks f"
D15-1167,W06-3808,0,0.0107953,"edNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). sition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local"
D15-1167,P13-1088,0,0.0400688,"er line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic compo5 Conclusion We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives. We conduct extensive experiments on four review datasets with two evaluation metrics. Empirical results show that our"
D15-1167,N15-1011,0,0.619314,"can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order t"
D15-1167,P14-1062,0,0.857869,"y size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolu"
D15-1167,W02-1011,0,0.120139,"gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1 1 Introduction Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn∗ Corresponding author. Codes and datasets are publicly available at http://ir.hit.edu.cn/˜dytang. 1 ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in t"
D15-1167,P13-2087,0,0.00447156,"Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Car"
D15-1167,D14-1162,0,0.131068,"scribe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or consti"
D15-1167,D15-1278,0,0.409141,"Missing"
D15-1167,C10-1103,0,0.0117788,"d is crucial to understand user generated content in social networks or product reviews (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn∗ Corresponding author. Codes and datasets are publicly available at http://ir.hit.edu.cn/˜dytang. 1 ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typ"
D15-1167,P15-1107,0,0.501227,"Missing"
D15-1167,P13-1045,0,0.0387591,"ection 2.3). 2.1 Average Tanh Pooling Filter 1 Filter 2 Filter 3 Convolution Lookup Sentence Composition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors"
D15-1167,P11-1015,0,0.552207,"ngio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recu"
D15-1167,P14-5010,0,0.019412,"Missing"
D15-1167,P10-1141,0,0.0122869,"nderstand user generated content in social networks or product reviews (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn∗ Corresponding author. Codes and datasets are publicly available at http://ir.hit.edu.cn/˜dytang. 1 ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capt"
D15-1167,P05-1015,0,0.42979,"re our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification. (1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set. (2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5 . (3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al. 5 We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). However, its performance is obviously worse than SVM classifier. 1426 Majority SVM + Unigrams SVM + Bigrams SVM + TextFeatures SVM + AverageSG SVM + SSWE JMARS Paragraph Vector Convolutional NN Conv-GRNN LSTM-GRNN Yelp 2013 Accuracy MSE 0.356 3.06 0.589 0.79 0.576 0.75 0.598 0.68 0.543 1.11 0.535 1.12 N/A – 0.577 0.86 0.597 0.76 0.637 0.56 0.651 0.50 Yelp 2014 Accuracy MSE 0.361 3.28 0.600 0.78 0.616 0.65 0.618 0.63 0.557 1.08 0.543 1.13 N/A – 0.592 0.70 0.610 0.68 0.655 0.51 0.671 0.48 Yelp 2015 Accuracy MSE 0.369 3.30 0.611 0.75 0.62"
D15-1167,D13-1170,0,0.490494,"ection 2.3). 2.1 Average Tanh Pooling Filter 1 Filter 2 Filter 3 Convolution Lookup Sentence Composition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors"
D15-1167,P15-1150,0,0.772158,"NN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of uni"
D15-1167,P14-1146,1,0.843593,"entation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results"
D15-1167,P15-1098,1,0.437565,"ateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of unigrams, bigramw1 w2 w3 w4 …… w?−1 w? Figure 2: Sentence composition with convolutional neural network. s and trigrams in a sentence. Each"
D15-1167,P02-1053,0,0.0607185,"es, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings. The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word"
D15-1167,P12-2018,0,0.86286,"rformance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capture such information. For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that. Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its “sparse” and “discrete” characteristics make it clumsy in taking into account of side information like relations between sentences. Recently, Le and Mikolov (2014) exploit neural networks to learn continuous document representation from data. Essentially, they use local ngram information and do not capture semantic relations between sentences. Furthermore, a person asked to do this task will naturally carry it o"
D15-1167,C10-2153,0,0.0144539,"ed learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). sition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of s"
D15-1167,C14-1064,0,0.0121979,"d Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic compo5 Conclusion We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives. We conduct extensive experiments on four review datasets with two evaluation metrics. Empirical results show that our approaches achieve state-of-the-art performances on all the"
D15-1167,D11-1016,0,0.00442251,"on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use sta"
D15-1167,D11-1015,0,0.00575452,"ument composition, while adding neural gates dramatically boosts the performance, (2) LSTM performs better than a multi-filtered CNN in modeling sentence representation. We briefly discuss some future plans. How to effectively compose sentence meanings to document meaning is a central problem in natural language processing. In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results. From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as “contrast”, “condition”, “cause”, etc. Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a). However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document 1429 representation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms"
D15-1167,D14-1181,0,\N,Missing
D15-1167,D14-1179,0,\N,Missing
D16-1021,D15-1263,0,0.00556126,"use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are r"
D16-1021,P10-2050,0,0.00966793,"n and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, research"
D16-1021,P14-2009,1,0.264452,"finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014). Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in liter221 ature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether i"
D16-1021,D14-1080,0,0.0185064,"g et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising au"
D16-1021,P14-1062,0,0.00487534,"part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts"
D16-1021,D14-1181,0,0.00616185,"positionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et"
D16-1021,S14-2076,0,0.468058,"ontext word and then utilizes this information to calculate continuous text representation. The text representation in the last layer is regarded as the feature for sentiment classification. As every component is differentiable, the entire model could be efficiently trained end-toend with gradient descent, where the loss function is the cross-entropy error of sentiment classification. We apply the proposed approach to laptop and restaurant datasets from SemEval 2014 (Pontiki et al., 2014). Experimental results show that our approach performs comparable to a top system using feature-based SVM (Kiritchenko et al., 2014). On both datasets, our approach outperforms both LSTM and attention-based LSTM models (Tang et al., 2015a) in terms of classification accuracy and running speed. Lastly, we show that using multiple computational layers over external memory could achieve improved performance. put representation given a new input and the current memory state, R outputs a response based on the output representation. Let us take question answering as an example to explain the work flow of memory network. Given a list of sentences and a question, the task aims to find evidences from these sentences and generate an"
D16-1021,P15-1107,0,0.0116738,"phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded a"
D16-1021,D15-1278,0,0.007366,"phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded a"
D16-1021,D15-1168,0,0.0594919,"growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot e"
D16-1021,D15-1166,0,0.0247409,"014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “where to look” by assigning a weight/importance to each lower position when computing an upper level representation. Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015). 6 Conclusion We develop deep memory networks that capture importances of context words for aspect level sentiment classification. Compared with recurrent neural models like LSTM, this approach is simpler and faster. Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures. We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text represen"
D16-1021,D15-1298,0,0.234316,"t analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014). Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in liter221 ature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2"
D16-1021,D14-1162,0,0.11812,"classification aims at determining the sentiment polarity of 1 In practice, an aspect might be a multi word expression such as “battery life”. For simplicity we still consider aspect as a single word in this definition. sentence s towards the aspect wi . For example, the sentiment polarity of sentence “great food but the service was dreadful!” towards aspect “food” is positive, while the polarity towards aspect “service” is negative. When dealing with a text corpus, we map each word into a low dimensional, continuous and real-valued vector, also known as word embedding (Mikolov et al., 2013; Pennington et al., 2014). All the word vectors are stacked in a word embedding matrix L ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. The word embedding of wi is notated as ei ∈ Rd×1 , which is a column in the embedding matrix L. 3.2 An Overview of the Approach softmax hop 3 Attention ∑ Linear word embedding hop 2 Attention ∑ Linear Linear hop 1 context words context words Attention ∑ Tanh Linear Linear sentence: ?1 , ?2 … ??−1 , ?? , ??+1 … ??−1 , ?? aspect word ??1 wi Figure 1: An illustration of our deep memory network with We present an overview of the deep memory network for aspe"
D16-1021,S14-2004,0,0.648481,"er an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation. 1 Introduction Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014). Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect. For example, in sentence “great food but the service was dreadful!”, the sentiment polarity of aspect “food” is positive while the polarity of aspect “service” is ∗ Corresponding author. Despite these advantages, conventional neural models like long short-term memory (LSTM) (Tang et al., 2015a) capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect. We believe tha"
D16-1021,D15-1044,0,0.037867,"t al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “where to look” by assigning a weight/importance to each lower position when computing an upper level representation. Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015). 6 Conclusion We develop deep memory networks that capture importances of context words for aspect level sentiment classification. Compared with recurrent neural models like LSTM, this approach is simpler and faster. Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures. We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text representation. We also demonstrate that using m"
D16-1021,D13-1170,0,0.00729358,"ts constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g"
D16-1021,P15-1150,0,0.0288958,"e, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writ"
D16-1021,D15-1167,1,0.358709,"1 Introduction Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014). Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect. For example, in sentence “great food but the service was dreadful!”, the sentiment polarity of aspect “food” is positive while the polarity of aspect “service” is ∗ Corresponding author. Despite these advantages, conventional neural models like long short-term memory (LSTM) (Tang et al., 2015a) capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect. We believe that only some subset of context words are needed to infer the sentiment towards an aspect. For example, in sentence “great food but the service was dreadful!”, “dreadful” is an important clue for the aspect “service” but “great” is not needed. Standard LSTM works in a sequential way and manipulates each context word with the same operation, so that it cannot explicitly reveal the importance of each context word. A desirable solution should be capable o"
D16-1021,S14-2036,0,0.0874602,"ntiment Classification Aspect level sentiment classification is a finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014). Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in liter221 ature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, w"
D16-1021,N16-1174,0,0.012777,"pute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “wher"
D16-1021,D11-1016,0,0.0129842,"on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014;"
D16-1021,K15-1021,0,0.00778988,"Missing"
D17-1090,N16-1152,0,0.0232147,"shows significant improvements. Question Generation for QA This section describes how question generation can improve existing QA systems. There are several types of QA systems, i.e. knowledge-based QA, community-based QA, text-based QA, etc, and in this paper, we focus on text-based QA task (a.k.a. answer sentence selection), which aims to select one or multiple answer sentences from a text given an input question. We select this task as it can be considered as a dual task of QG. A typical answer sentence selection method, such as (Yin et al., 2016; Santos et al., 2016; Miller et al., 2016; Tymoshenko et al., 2016), computes the relevance score between input question Q and each answer candidate A, and selects the one with the highest relevance score as the final answer: Ab = arg max P (A|Q) A Motivated by Dual Learning (He et al., 2016), we integrate question generation into answer ranking procedure, by changing the above formula to: Ab = arg max{P (A|Q) + λ · QQ(Q, Qgen max )} A λ is hyper-parameter, and in order to compute QQ(Q, Qgen max ), we generate top-10 questions gen gen {Q1 , ..., Q10 } for current answer candidate A, and then compute the question-to-generated question matching score QQ(Q, Qgen"
D17-1090,D13-1160,0,0.0746789,"sing recurrent neural network (RNN); Second, we show how to leverage the generated questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved. 1 Introduction Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation ("
D17-1090,P16-1049,1,0.835962,"assage, and [Gen] denotes the generated question of the passage. WikiQA QA QA+QG ly wrong question topic, or a partially right question topic. In the future, we plan to develop an independent question topic selection model for the question generation task. 9.3 As described in Section 7, we combine question generation into QA system for answer sentence selection task, and do evaluation on SQuAD, MS MARCO, and WikiQA. Evaluation results are shown in Table 5, 6, and 7, where QA denotes the result of our in-house implementation of a retrieval-based answer selection approach (DocChat) proposed by (Yan et al., 2016), QA+QG denotes result by combining question-to-generated question matching score with DocChat score. MAP 0.8843 0.8887 MRR 0.8915 0.8963 MAP 0.5131 0.5230 MRR 0.5195 0.5291 ACC@1 0.6540 0.6624 for CQA websites; while questions from the other datasets are labeled by crowd-sourcing. In order to explain these improvements, two datasets, WikiQG+ and WikiQG-, are built from WikiQA test set: given each document and its labeled question, we pair the question with its CORRECT answer sentence as a QA pair and add it to WikiQG+; we also pair the same question with a randomly selected WRONG answer sente"
D17-1090,J15-1001,0,0.0853699,"e input sentence into its corresponding Minimal Recursion Semantics (MRS) representation, and then generates a question guided by the English Resource Grammar that includes a large scale handcrafted lexicon and grammar rules. Labutov et al. (2015) proposed an ’ontologycrowd-relevance’ method for question generation. First, Freebase types and Wikipedia session names are used as semantic tags to understand texts. Question are then generated based on question templates that are aligned with types/session names and labeled by crowdsourcing. All generated questions are ranked by a relevance model. Chali and Hasan (2015) proposed a topic-toquestion method, which uses about 350 generalpurpose rules to transform the semantic-role labeled sentences into corresponding questions. Serban et al. (2016) used the encoder-decoder framework to generate 30M QA pairs, but their inputs are knowledge triples, instead of passages. Song and Zhao (2016) proposed a question generation method using question template seeds and used search engine to do question expansion. Du et al. (2017) proposed a neural question generation method using a vanilla sequence-tosequence RNN model, which is most-related to our work. But this method i"
D17-1090,D15-1237,0,0.0201733,"is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation (QG) engine generates a set of questions {Qi }, 866 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866–874 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics where each generated Qi can be answered by S. There are four components in our QG engine: query “what is the population of nyc” is issued to YahooAnswers2 , the returned page contains a list of related questions"
D17-1090,P17-1123,0,0.430684,"templates that are aligned with types/session names and labeled by crowdsourcing. All generated questions are ranked by a relevance model. Chali and Hasan (2015) proposed a topic-toquestion method, which uses about 350 generalpurpose rules to transform the semantic-role labeled sentences into corresponding questions. Serban et al. (2016) used the encoder-decoder framework to generate 30M QA pairs, but their inputs are knowledge triples, instead of passages. Song and Zhao (2016) proposed a question generation method using question template seeds and used search engine to do question expansion. Du et al. (2017) proposed a neural question generation method using a vanilla sequence-tosequence RNN model, which is most-related to our work. But this method is still based on labeled dataset, and tried RNN only. Comparing to all these related work mentioned above, our question generation approach has two uniqueness: (1) all question patterns, that are used as training data for question generation, are automatically extracted from a large scale CQA question set without any crowdsourcing effort. Such question patterns reflect the most common user intentions, and therefore are useful to search, QA, and chatbo"
D17-1090,E17-1066,0,0.0152031,"ction task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved. 1 Introduction Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation (QG) engine generates a set of questions {Qi }, 866 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866–874 c Copenhagen, Denmark, September 7–11, 2017. 2017 Associatio"
D17-1090,P03-1054,0,0.0495266,"ct question topic candidates from S, including: 6 Given a predicted question pattern Qp and a selected question topic Qt of an input passage S, a complete question Q can be simply generated by replacing # in Qp with Qt . We use a set of features to rank generated question candidates: • question pattern prediction score, which is the prediction score by either retrieval-based approach or generation-based approach; • Entities as question topic candidates, which are detected based on Freebase4 entities; • Noun phrases as question topic candidates, which are detected based on the Stanford parser (Klein and Manning, 2003). • question topic selection score, for retrievalbased approach, this score is computed as s(Qt , Qp ), while for generation-based approach, this score is the attention score; Once a question topic candidate Qt is extracted from S, we then measure how Qt can fit Qp by: • QA matching score, which measures relevance between generated question Q and S. 1 X s(Qt , Qp ) = · #(Qtpk ) · dist(vQt , vQtk ) p N k • word overlap between Q and S, which counts number of words that co-occur in Q and S; s(Qt , Qp ) denotes the confidence that Qt can be filled into Qp to generate a reasonable question. Qtpk d"
D17-1090,Q16-1019,0,0.11248,"egrated and evaluated in an end-to-end QA task directly, and shows significant improvements. Question Generation for QA This section describes how question generation can improve existing QA systems. There are several types of QA systems, i.e. knowledge-based QA, community-based QA, text-based QA, etc, and in this paper, we focus on text-based QA task (a.k.a. answer sentence selection), which aims to select one or multiple answer sentences from a text given an input question. We select this task as it can be considered as a dual task of QG. A typical answer sentence selection method, such as (Yin et al., 2016; Santos et al., 2016; Miller et al., 2016; Tymoshenko et al., 2016), computes the relevance score between input question Q and each answer candidate A, and selects the one with the highest relevance score as the final answer: Ab = arg max P (A|Q) A Motivated by Dual Learning (He et al., 2016), we integrate question generation into answer ranking procedure, by changing the above formula to: Ab = arg max{P (A|Q) + λ · QQ(Q, Qgen max )} A λ is hyper-parameter, and in order to compute QQ(Q, Qgen max ), we generate top-10 questions gen gen {Q1 , ..., Q10 } for current answer candidate A, and then"
D17-1090,P15-1086,0,0.265209,"ched by Qp , and 0 otherwise. All features are combined by a linear model as: p(Q|S) = https://developers.google.com/freebase/ X i 870 λi · hi (Q, S, Qp , Qt ) 8 where hi (Q, S, Qp , Qt ) is one of the features described above, and λi is the corresponding weight. 7 Related Work Yao et al. (2012) proposed a semantic-based question generation approach, which first parses the input sentence into its corresponding Minimal Recursion Semantics (MRS) representation, and then generates a question guided by the English Resource Grammar that includes a large scale handcrafted lexicon and grammar rules. Labutov et al. (2015) proposed an ’ontologycrowd-relevance’ method for question generation. First, Freebase types and Wikipedia session names are used as semantic tags to understand texts. Question are then generated based on question templates that are aligned with types/session names and labeled by crowdsourcing. All generated questions are ranked by a relevance model. Chali and Hasan (2015) proposed a topic-toquestion method, which uses about 350 generalpurpose rules to transform the semantic-role labeled sentences into corresponding questions. Serban et al. (2016) used the encoder-decoder framework to generate"
D17-1090,D16-1147,0,0.0176953,"QA task directly, and shows significant improvements. Question Generation for QA This section describes how question generation can improve existing QA systems. There are several types of QA systems, i.e. knowledge-based QA, community-based QA, text-based QA, etc, and in this paper, we focus on text-based QA task (a.k.a. answer sentence selection), which aims to select one or multiple answer sentences from a text given an input question. We select this task as it can be considered as a dual task of QG. A typical answer sentence selection method, such as (Yin et al., 2016; Santos et al., 2016; Miller et al., 2016; Tymoshenko et al., 2016), computes the relevance score between input question Q and each answer candidate A, and selects the one with the highest relevance score as the final answer: Ab = arg max P (A|Q) A Motivated by Dual Learning (He et al., 2016), we integrate question generation into answer ranking procedure, by changing the above formula to: Ab = arg max{P (A|Q) + λ · QQ(Q, Qgen max )} A λ is hyper-parameter, and in order to compute QQ(Q, Qgen max ), we generate top-10 questions gen gen {Q1 , ..., Q10 } for current answer candidate A, and then compute the question-to-generated question"
D17-1090,P02-1040,0,0.114478,"QA) website. The motivation of using CQA website for training data collection is that, such websites (e.g., YahooAnswers, Quora, etc.) contain large scale QA pairs generated by real users, and these questions reflect the most common user intentions, and therefore are useful to search, QA, and chatbot scenarios. To achieve the 2nd goal, we explore two ways to generate questions for a given passage, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN). We evaluate the generation quality by BLEU score (Papineni et al., 2002) and human annotations, and discuss their pros and cons in Section 9. To achieve the 3rd goal, we integrate our question generation approach into an end-to-end QA task, i.e., answer sentence selection, and evaluate its impact on three popular benchmark datasets, SQuAD, MS MARCO, and WikiQA. Experimental results show that, the generated questions can improve the QA quality on all these three datasets. This paper presents how to generate questions from given passages using neural networks, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as t"
D17-1090,D16-1264,0,0.0988632,"questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved. 1 Introduction Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation (QG) engine generates a set of questions {Qi }, 866 Proceedings of the 2017 Conference"
D17-1090,P14-1133,0,\N,Missing
D18-1188,D13-1160,0,0.082852,"without knowing that cells under the column name “built” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table"
D18-1188,E17-2029,0,0.0206229,"ence. ψg (yi ) = viT Wg st ψc (yi ) = tanh(hi T Wc )st Encoder-Decoder Encoder: A bidirectional RNN with gated recurrent unit (GRU) (Cho et al., 2014) is used as the encoder to read a SQL query x = (x1 , ..., xT ). The forward RNN reads a SQL query (5) 4.3 (7) Incorporating Latent Variable Increasing the diversity of generated questions is very important to improve accuracy, generalization, and stability of the semantic parser, since this 1599 increases the mount of training data and produces more diverse questions for the same intent. In this work, we incorporate stochastic latent variables (Cao and Clark, 2017; Serban et al., 2017) to the sequence-to-sequence model in order to increase question diversity. Specifically, we introduce a latent variable z ∼ p(z), which is a standard Gaussian distribution N (0, In ) in our case, and calculate the likelihood of a target sentence y as follows: Z (8) p(y|x) = p(y|z, x)p(z) dz z We maximize the evidence lower bound (ELBO), which decomposes the loss into two parts, including (1) the KL divergence between the posterior distribution and the prior distribution, and (2) a cross-entropy loss between the generated question and the ground truth. logp(y|x) ≥ −DKL (Q"
D18-1188,D17-1090,1,0.851999,"uage questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2017; Dong et al., 2"
D18-1188,N13-1092,0,0.044976,"Missing"
D18-1188,D17-1087,0,0.0434135,"the area of question generation, which has drawn plenty of attention recently partly influenced by the remarkable success of neural networks in text generation. Studies in this area are classified based on the definition of the answer, including a sentence (Heilman, 2011), a topic word (Chali and Hasan, 2015), a fact (including a subject, a relation phrase and an object) from knowledge bases (Serban et al., 2016), an image (Mostafazadeh et al., 2016), etc. Recent studies in machine reading comprehension generate questions from an answer span and its context from the document (Du et al., 2017; Golub et al., 2017). Wang et al. (2015) first generate logical forms, and then use AMTurkers to paraphrase them to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that"
D18-1188,P16-1154,0,0.0599355,"ding of the previously predicted word yt−1 , and the last hidden state st−1 is fed to the next step. st = GRU (st−1 , yt−1 , ct ) After obtaining hidden states st , we adopt the copying mechanism that predicts a word from the target vocabulary or from the source sentence (detailed in Subsection 4.2). 4.2 Incorporating Copying Mechanism In our task, the generated question utterances typically include informative yet low-frequency words such as named entities or numbers. Usually, these words are not included in the target vocabulary but come from SQL queries. To address this, we follow CopyNet (Gu et al., 2016) and incorporate a copying mechanism to select whether to generate from the vocabulary or copy from SQL queries. The probability distribution of generating the t-th word is calculated as Equation 6, where ψg (·) and ψc (·) are scoring functions for generating from the vocabulary ν and copying from the source sentence x, respectively. eψg (yt ) + eψc (yt ) P ψg (v) + ψc (w) v∈ν e w∈x e p(yt |y<t , x) = P (6) The two scoring functions are calculated as follows, where Wg and Wc are model parameters, vi is the one-hot indicator vector for yi and hi is the hidden state of word yi in the source sent"
D18-1188,P17-1097,0,0.0762159,"Missing"
D18-1188,P16-1004,0,0.187589,"Missing"
D18-1188,P17-1089,0,0.15759,"s, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table as the knowledge base, which is critical for accessing relational databases w"
D18-1188,D17-1091,0,0.0405482,"ns from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2017; Dong et al., 2017); and (3) using the QG results as additional constraints in the training objectives (Tang et al., 2017). This work belongs to the first direct"
D18-1188,P18-1069,0,0.0366913,"Missing"
D18-1188,P17-1123,0,0.0222525,"also relates to the area of question generation, which has drawn plenty of attention recently partly influenced by the remarkable success of neural networks in text generation. Studies in this area are classified based on the definition of the answer, including a sentence (Heilman, 2011), a topic word (Chali and Hasan, 2015), a fact (including a subject, a relation phrase and an object) from knowledge bases (Serban et al., 2016), an image (Mostafazadeh et al., 2016), etc. Recent studies in machine reading comprehension generate questions from an answer span and its context from the document (Du et al., 2017; Golub et al., 2017). Wang et al. (2015) first generate logical forms, and then use AMTurkers to paraphrase them to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates t"
D18-1188,P16-1002,0,0.0909215,"Missing"
D18-1188,P17-1014,0,0.0279021,"r et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2017; Dong et al., 2017); and (3) using the"
D18-1188,D17-1160,0,0.0464092,"xtensional experiment on WikiTableQuestions2 (Pasupat and Liang, 2015) in a transfer learning scenario to verify the effectiveness of our approach. WikiTableQuestions contains 22,033 complex questions on 2,108 Wikipedia tables. Each instance consists of a natural language question, a table and an answer. Following Pasupat and Liang (2015), we report development accuracy which is averaged over the first three 80-20 training data splits. Test accuracy is reported on the train-test data. In this experiment, we apply the QG model learnt from WikiSQL to improve the state-of-theart semantic parser (Krishnamurthy et al., 2017) on this dataset. Different from WikiSQL, this dataset requires question-answer pairs for training. Thus, we generate question-answer pairs by follow steps. We first sample SQL queries on the tables from WikiTableQuestions, and then use our QG model to generate question-SQL pairs. After2 https://nlp.stanford.edu/software/ sempre/wikitable/ wards, we obtain question-answer pairs by executing SQL queries. The generated question-answer pairs will be combined with the original WikiTableQuestions training data to train the model. Pasupat and Liang (2015) Neelakantan et al. (2016) Haug et al. (2017)"
D18-1188,Q13-1016,0,0.0298687,"cells under the column name “built” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table as the knowledge base, which is"
D18-1188,P11-1060,0,0.0403622,"ions. For instance, without knowing that cells under the column name “built” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work,"
D18-1188,P16-1170,0,0.035962,"e QA model and train the QG model, and incorporate more generated question-logical form pairs to further improve the QA model. Question Generation Our work also relates to the area of question generation, which has drawn plenty of attention recently partly influenced by the remarkable success of neural networks in text generation. Studies in this area are classified based on the definition of the answer, including a sentence (Heilman, 2011), a topic word (Chali and Hasan, 2015), a fact (including a subject, a relation phrase and an object) from knowledge bases (Serban et al., 2016), an image (Mostafazadeh et al., 2016), etc. Recent studies in machine reading comprehension generate questions from an answer span and its context from the document (Du et al., 2017; Golub et al., 2017). Wang et al. (2015) first generate logical forms, and then use AMTurkers to paraphrase them to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority o"
D18-1188,P02-1040,0,0.10339,"Missing"
D18-1188,P15-1142,0,0.0229864,"imit-supervision scenarios. This is consistent with our intuition that the performance of the QG model is improved by incorporating the copying mechanism, since rare words of great importance mainly come from the input sequence. To better understand the impact of incorporating a latent variable, we show examples generated by different QG variations in Table 7. We can see that incorporating a latent variable empowers the model to generate diverse questions for the same intent. 5.5 Transfer Learning on WikiTableQuestions In this part, we conduct an extensional experiment on WikiTableQuestions2 (Pasupat and Liang, 2015) in a transfer learning scenario to verify the effectiveness of our approach. WikiTableQuestions contains 22,033 complex questions on 2,108 Wikipedia tables. Each instance consists of a natural language question, a table and an answer. Following Pasupat and Liang (2015), we report development accuracy which is averaged over the first three 80-20 training data splits. Test accuracy is reported on the train-test data. In this experiment, we apply the QG model learnt from WikiSQL to improve the state-of-theart semantic parser (Krishnamurthy et al., 2017) on this dataset. Different from WikiSQL, t"
D18-1188,P16-1003,0,0.0372546,"lt” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table as the knowledge base, which is critical for accessing re"
D18-1188,D14-1162,0,0.0812641,"pectively. Similar to hx , hy is obtained by encoding the target sentence. µ = Wµ [hx ; hy ] + bµ log(σ 2 ) = Wσ [hx ; hy ] + bσ 4.4 the initial state of the decoder, and then decoding deterministically for each sample. Here, we list our training details. We set the dimension of the encoder hidden state as 300, and the dimension of the latent variable z as 64. We use dropout with a rate of 0.5, which is applied to the inputs of RNN. Model parameters are initialized with uniform distribution, and updated with stochastic gradient decent. Word embedding values are initialized with Glove vectors (Pennington et al., 2014). We set the learning rate as 0.1 and the batch size as 32. We tune hyper parameters on the development, and use beam search in the inference process. 5 Experiment We conduct experiments on the WikiSQL dataset1 (Zhong et al., 2017). WikiSQL is the largest handannotated semantic parsing dataset which is an order of magnitude larger than other datasets in terms of both the number of logical forms and the number of schemata (tables). WikiSQL is built by crowd-sourcing on Amazon Mechanical Turk, including 61,297 examples for training, and 9,145/17,284 examples for development/testing. Each instanc"
D18-1188,P18-1034,1,0.877859,"ll-scale supervised training data that consists of SQL-question pairs. Lastly, the generated question-SQL pairs are viewed as the pseudo-labeled data, which are combined with the supervised training data to train the semantic parser. Since we conduct the experiment on WikiSQL dataset, we follow Zhong et al. (2017) and use the same template-based SQL sampler, as summarized in Table 1. The details about the semantic parser and the question generation model will be introduced in Sections 3 and Section 4, respectively. 3 Semantic Parsing Model We use a state-of-the-art end-to-end semantic parser (Sun et al., 2018) that takes a natural language question as the input and outputs a SQL query, which is executed on a table to obtain the answer. To make the paper self-contained, we briefly describe the approach in this section. The semantic parser is abbreviated as STAMP, which is short for Syntax- and Table- Aware seMantic Parser. Based on the encoder-decoder framework, STAMP takes a question as the input and generates a SQL query. It extends pointer networks (Zhong et al., 2017; Vinyals et al., 2015) by incorporating three “channels” in the decoder, in which the column channel predicts column names, the va"
D18-1188,P15-1129,0,0.431991,"Missing"
D18-1188,P07-1121,0,0.419448,"Missing"
D18-1188,P16-1127,0,0.207322,"Missing"
D18-1188,P17-1096,0,0.0226953,"to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2"
D18-1188,D07-1071,0,0.232712,"Missing"
D18-1188,D17-1125,0,0.0685018,"Missing"
D19-1172,D14-1181,0,0.00514974,"For simplification, we use a special symbol, [S], to concatenate all the entity information together. These two parts can be regarded as different source information. Based on whether the inter-relation between different source information is explicitly explored, the classification models can be classified into two categories, unstructured models and structured models. Unstructured models concatenate different source inputs into a long sequence with a special separation symbol [SEL]. We implement several widely-used sequence classification models, including Convolutional Neural Network (CNN) (Kim, 2014), Long-Short Term Memory Network (LSTM) (Schuster and Paliwal, 1997), and Recurrent Convolutional Neural Network (RCNN) (Lai et al., 2015), Transformer. The details of the models are shown in Supplementary Materials. 1622 [A] web browser Midori operating system Midori Decoder Decoder Encoder Entity Info [B] Entity Rendering Module Ambiguous Context Template What are the languages used to create the source code of Midori? Encoder Template Generating Module Decoder When you say the source code language used in the program Midori, are you referring to [A] or [B]? Hidden vector of [A] Hidden vecto"
D19-1172,J81-4005,0,0.674542,"Missing"
D19-1172,N03-1007,0,0.0346386,"Missing"
D19-1172,D16-1076,0,0.0350947,"Missing"
D19-1172,P17-1045,0,0.0494573,"Missing"
D19-1172,P18-1068,0,0.018531,"es (e.g., “web browser Midori ”) and pattern phrases ( e.g. “When you say the source code language used in the program Midori, are you referring to [A] or [B]?” ). The entity phrase is summarized from the given entity information for distinguishing between two entities. The pattern phrase is used to locate the position of ambiguity, which is closely related with the context. In summary, two kinds of phrases refer to different source information. Based on this feature, we propose a new coarse-to-fine model, as shown in Figure 6. Similar ideas have been successfully applied to semantic parsing (Dong and Lapata, 2018). The proposed model consists of a template generating module Tθ and an entity rendering module Rφ . Tθ first generates a template containing pattern phrases and the symbolic representation of the entity phrases. Then, the symbolized entities contained in the generated template are further properly rendered by the entity rendering module Rφ to reconstruct complete entity information. Since the annotated clarification questions explicitly separate entity phrases and pattern phrases, we can easily build training data for these two modules. For clarity, the template is constructed by replacing en"
D19-1172,D18-1188,1,0.829182,"ll dataset size makes it hard to help downstream tasks. Following this work, Guo et al. (2017) provide a larger synthetic dataset QRAQ by replacing some of entities with variables. Li et al. (2017) use misQuestion Generation For different purposes, there are various question generation tasks. Hu et al. (2018) aim to ask questions to play the 20 question game. Dhingra et al. (2017) teach models to ask questions to limit the number of answer candidates in task-oriented dialogues. Ke et al. (2018) train models to ask questions in open-domain conversational systems to better interact with people. Guo et al. (2018) develop a sequence-to-sequence model to generate natural language questions. 7 Conclusion and Future Work In this work, we construct a clarification question dataset for KBQA. The dataset supports three tasks, clarification identification, clarification question generation, and clarification-based question answering. We implement representative neural networks as baselines for three tasks and propose a new generation model. The detailed analysis shows that our dataset brings new challenges. More powerful models and reasonable evaluation metrics need further explored. In the future, we plan to"
D19-1172,D18-1361,0,0.0212818,"rification Question in Other Tasks There are several studies on asking clarification questions. Stoyanchev et al. (2014) randomly drop one phrase from a question and require annotators to ask a clarification question toward the dropped information, e.g.“Do you know the birth data of XXX”. However, the small dataset size makes it hard to help downstream tasks. Following this work, Guo et al. (2017) provide a larger synthetic dataset QRAQ by replacing some of entities with variables. Li et al. (2017) use misQuestion Generation For different purposes, there are various question generation tasks. Hu et al. (2018) aim to ask questions to play the 20 question game. Dhingra et al. (2017) teach models to ask questions to limit the number of answer candidates in task-oriented dialogues. Ke et al. (2018) train models to ask questions in open-domain conversational systems to better interact with people. Guo et al. (2018) develop a sequence-to-sequence model to generate natural language questions. 7 Conclusion and Future Work In this work, we construct a clarification question dataset for KBQA. The dataset supports three tasks, clarification identification, clarification question generation, and clarification"
D19-1172,P18-1255,0,0.125162,"Missing"
D19-1172,N16-1174,0,0.0245396,"late generating module and an entity rendering module. The former is used to generate a clarification template based on ambiguous question, e.g., “When you say the source code language used in the program Midori, are you referring to [A] or [B]?”. The latter is used to fill up the generated template with detailed entity information. Structured models use separate structures to encode different source information and adopt an additional structure to model the inter-relation of the source information. Specifically, we use two representative neural networks, Hierarchical Attention Network (HAN) (Yang et al., 2016) and Dynamic Memory Network (DMN) (Kumar et al., 2016), as our structured baselines. The details of the models are shown in Supplementary Materials. 4.2 Clarification Question Generation Models The input of the generation model is the ambiguous context and entity information. In single-turn cases, the ambiguous context is current question Qa . In multi-cases, the input is current question and the previous conversation turn {Qp , Rp , Qa }. We use [S] to concatenate all entity information together, and use [SEL] to concatenate entity information and context information into a long sequence. We"
D19-1248,P13-2009,0,0.0266995,"cal form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and"
D19-1248,Q13-1005,0,0.0265942,"s the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example,"
D19-1248,P17-1005,0,0.0132889,"2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a question to a logical form based on the candidates. D"
D19-1248,P16-1004,0,0.197334,"n intern at Microsoft, Beijing, China. Lapata, 2016; Liang et al., 2016; Dong and Lapata, 2018; Guo et al., 2018) is gaining rising attention for knowledge-based question answer (KBQA) in recent years since it does not rely on handcrafted features and is easy to adapt across domains. Traditional approaches usually retrieve answers from a small KB (e.g., small table) (Jia and Liang, 2016; Xiao et al., 2016) and are difficult to handle large-scale KBs. Many recent neural semantic parsing based approaches for KB-QA take a stepwise framework to handle this issue. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first use an entity linking system to find entities in a question, and then learn a model to map the question to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the question and the sketch. However, these stepwise approaches have two issues. First, errors in upstream subtasks (e.g., entity detection and linking, relation classification) are propagated to downstream ones (e.g., sema"
D19-1248,P18-1068,0,0.079013,"seen the development of AIdriven personal assistants (e.g., Siri, Alexa, Cortana, and Google Now) that often need to answer factorial questions. Meanwhile, large-scale knowledge base (KB) like DBPedia (Auer et al., 2007) or Freebase (Bollacker et al., 2008) has been built to store world’s facts in a structure database, which is used to support open-domain question answering (QA) in those assistants. Neural semantic parsing based approach (Jia and Liang, 2016; Reddy et al., 2014; Dong and ∗ Work done while the author was an intern at Microsoft, Beijing, China. Lapata, 2016; Liang et al., 2016; Dong and Lapata, 2018; Guo et al., 2018) is gaining rising attention for knowledge-based question answer (KBQA) in recent years since it does not rely on handcrafted features and is easy to adapt across domains. Traditional approaches usually retrieve answers from a small KB (e.g., small table) (Jia and Liang, 2016; Xiao et al., 2016) and are difficult to handle large-scale KBs. Many recent neural semantic parsing based approaches for KB-QA take a stepwise framework to handle this issue. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first use an entity linking system to find entit"
D19-1248,P16-1002,0,0.121012,"distribution over V to score candidates2 . Then, a FFN(·) or a pointer network (Vinyals et al., 2015) is utilized to predict instantiation for entry semantic category (i.e., e, p, tp or u num in V(vec) ) if it is necessary. (n) pj = softmax(sTj W (n) H:,1:n−1 ), (5) where H:,1:n−1 is contextual embedding of tokens in the question except [CTX], W (e) and W (n) are weights of pointer-network for (e) (n) entity and number, pj , pj ∈ Rn−1 are the resulting distributions over positions of input question, and n is the length of the question. The pointer network is also used for semantic parsing in (Jia and Liang, 2016), where the pointer aims at copying out-of-vocabulary words from a question over small-scale KB. Different from that, the pointer used here aims at locating the targeted entity and number in a question, which has two advantages. First, it handles the coreference problem by considering the context of entity mentions in the question. Second, it solves the problem caused by huge entity vocabulary, which reduces the size of decoding vocabulary from several million (i.e., the number of entities in KB) to several dozen (i.e., the length of the question). 3.2.3 Entity Detection and Linking • For pred"
D19-1248,D17-1160,0,0.0176575,"s, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a question to a logical form based"
D19-1248,D11-1140,0,0.031991,"n example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng"
D19-1248,P16-1057,0,0.0379427,"Missing"
D19-1248,P16-1138,0,0.0291545,"while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis a"
D19-1248,D16-1147,0,0.0443322,"Missing"
D19-1248,P17-1105,0,0.0137173,"7; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a quest"
D19-1248,Q14-1030,0,0.103666,"Missing"
D19-1248,P07-1121,0,0.0543536,"rold, Queen Lillian and Arthur Pendragon possess” as an example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (R"
D19-1248,1983.tc-1.13,0,0.382158,"Missing"
D19-1248,P16-1127,0,0.19754,"e, which is used to support open-domain question answering (QA) in those assistants. Neural semantic parsing based approach (Jia and Liang, 2016; Reddy et al., 2014; Dong and ∗ Work done while the author was an intern at Microsoft, Beijing, China. Lapata, 2016; Liang et al., 2016; Dong and Lapata, 2018; Guo et al., 2018) is gaining rising attention for knowledge-based question answer (KBQA) in recent years since it does not rely on handcrafted features and is easy to adapt across domains. Traditional approaches usually retrieve answers from a small KB (e.g., small table) (Jia and Liang, 2016; Xiao et al., 2016) and are difficult to handle large-scale KBs. Many recent neural semantic parsing based approaches for KB-QA take a stepwise framework to handle this issue. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first use an entity linking system to find entities in a question, and then learn a model to map the question to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the questi"
D19-1248,P16-1220,0,0.030023,"to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the question and the sketch. However, these stepwise approaches have two issues. First, errors in upstream subtasks (e.g., entity detection and linking, relation classification) are propagated to downstream ones (e.g., semantic parsing), resulting in accumulated errors. For example, case studies in previous works (Yih et al., 2015; Dong and Lapata, 2016; Xu et al., 2016; Guo et al., 2018) show that entity linking error is one of the major errors leading to wrong predictions in KB-QA. Second, since models for the subtasks are learned independently, the supervision signals cannot be shared among the models for mutual benefits. To tackle issues mentioned above, we propose a novel multi-task semantic parsing framework for KB-QA. Specifically, an innovative pointerequipped semantic parsing model is first designed for two purposes: 1) built-in pointer network toward positions of entity mentions in the question 2442 Proceedings of the 2019 Conference on Empirical M"
D19-1248,P15-1128,0,0.0730713,"d then learn a model to map the question to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the question and the sketch. However, these stepwise approaches have two issues. First, errors in upstream subtasks (e.g., entity detection and linking, relation classification) are propagated to downstream ones (e.g., semantic parsing), resulting in accumulated errors. For example, case studies in previous works (Yih et al., 2015; Dong and Lapata, 2016; Xu et al., 2016; Guo et al., 2018) show that entity linking error is one of the major errors leading to wrong predictions in KB-QA. Second, since models for the subtasks are learned independently, the supervision signals cannot be shared among the models for mutual benefits. To tackle issues mentioned above, we propose a novel multi-task semantic parsing framework for KB-QA. Specifically, an innovative pointerequipped semantic parsing model is first designed for two purposes: 1) built-in pointer network toward positions of entity mentions in the question 2442 Proceedin"
D19-1248,P17-1041,0,0.014099,"., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a question to a logical form based on the candidates. Dong and Lapata (2018) d"
D19-1248,D07-1071,0,0.0686871,"uestion “Which sexes do King Harold, Queen Lillian and Arthur Pendragon possess” as an example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syn"
D19-1248,P09-1110,0,0.0855188,"Arthur Pendragon possess” as an example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishna"
N18-1141,D13-1160,0,0.066233,"ning when to regard generated questions as positive instances (collaborative) could improve the accuracy of the QA model. 2 Related Work Our work connects to existing works on question answering (QA), question generation (QG), and the use of generative adversarial nets in question answering and text generation. We consider two kinds of answer selection tasks in this work, one is table as the answer (Balakrishnan et al., 2015) and another is sentence as the answer (Yang et al., 2015). In natural language processing community, there are also other types of QA tasks including knowledge based QA (Berant et al., 2013), community based QA (Qiu and Huang, 2015) and reading comprehension (Rajpurkar et al., 2016). We believe that our algorithm is generic and could also be applied to these tasks with dedicated QA and QG model architectures. Despite the use of sophisticated features could learn a more accurate QA model, in this work we implement a simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks"
N18-1141,D17-1091,0,0.0227649,", question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary task and use auxiliary task, such as question generation and question paraphrasing, to improve the primary task. This is one part of our goal and our another goal is to improve the QG model with the QA system and further to increasingly improve both tasks in a loop. In terms of assigning category label to the generated question, Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 1565 2017) and Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) could be viewed as special cases of our algorithm. Our algor"
N18-1141,P17-1123,0,0.0582539,"arning of question answering and question generation. Question answering (QA) and question generation (QG) are closely related natural language processing tasks. The goal of QA is to obtain an answer given a question. The goal of QG is almost reverse which is to generate a question from the answer. In this work, we consider answer selection (Yang et al., 2015; Balakrishnan et al., 2015) as the QA task, which assigns a numeric score to each candidate answer, and selects the top ranked one as the answer. We consider QG as a generation problem and exploit sequence-to-sequence learning (Seq2Seq) (Du et al., 2017; Zhou et al., 2017) as the backbone of the QG model. The key idea of this work is that QA and QG are two closely tasks and we seek to leverage the connection between these two tasks to improve both QA and QG. Our primary motivations are twofolds. On one hand, the Seq2Seq based QG model is trained by maximizing the literal similarity between the generated sentence and the ground truth sentence with maximum-likelihood estimation objective function (Du et al., 2017). However, there is no signal indicating whether or not the generated sentence could be correctly answered by the input. This proble"
N18-1141,D17-1090,1,0.740288,"g (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary task and use auxiliary task, such as question generation and question paraphrasing, to improve the primary task. This is one part of our goal and our another goal is to improve the QG model with the QA system and further to increasingly improve both tasks in a loop. In terms of assigning category label to the generated question, Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 1565 2017) and Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) could be viewed as special cases of our algorithm. Our algorithm learns when to"
N18-1141,P16-1154,0,0.0765019,"Missing"
N18-1141,P16-1014,0,0.026262,"vector, header vector, and the cell vector. The backbone of the decoder is an attention based GRU RNN, which generates a word at each time step and repeats the process until generating the end-of-sentence symbol. We made two modifications to adapt the decoder to the table structure. The first modification is that the attention model is calculated over the headers, cells and the caption of a table. Ideally, the decoder should learn to focus on a region of the table when generating a word. The second modification is a table based copying mechanism. It has been proven that the copying mechanism (Gulcehre et al., 2016; Gu 1568 et al., 2016) is an effective way to replicate lowfrequent words from the source to the target sequence in sequence-to-sequence learning. In the decoding process, a word is generated either from the target vocabulary via standard sof tmax or from a table via the copy mechanism. A neural gate gt is used to trade-off between generating from the target vocabulary and copying from the table. The probability of generating a word y calculated as follows, where αt (y) is the attention probability of the word y from the table at time step t and βt (y) is the probability of predicting the wor"
N18-1141,P17-1019,0,0.0168286,"ion-answer pairs are correct and some are wrong. However, this kind of dataset is hard to obtain in most situations because of the lack of manual annotation efforts. From this perspective, the QA model could exactly benefit from the QG model through incorporating additional questionanswer pairs whose questions are automatically generated by the QG model1 . To achieve this goal, we present a training algorithm that improves the QA model and the 1 An alternative way is to automatically generate answers for each question. Solving the problem in this condition requires an answer generation model (He et al., 2017), which is out of the focus of this work. Our algorithm could also be adapted to this scenario. 1564 Proceedings of NAACL-HLT 2018, pages 1564–1574 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics QG model in a loop. The QA model improves QG through introducing an additional QA-specific loss function, the objective of which is to maximize the expectation of the QA scores of the generated question-answer pairs. Policy gradient method (Williams, 1992; Yu et al., 2017) is used to update the QG model. In turn, the QG model improves QA through incorporating"
N18-1141,N03-1017,0,0.0990867,"A and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity between a query and a table is calculated with the BM25 algorithm. WordCnt uses the number of co-occurred words in query-caption pair, query-header pair, and query-cell pair, respectively. MT based PP is a phrase-level feature. The features come from a phrase table which is extracted from bilingual corpus via statistical machine translation approach (Koehn et al., 2003). LambdaMART (Burges, 2010) is used to train the ranker. CNN uses convolutional neural network to measure the similarity between the query and table caption, table headers, and table cells, respectively. TQNN is the table-based QA model implemented in this work, which is regard as the baseline for the joint learning algorithm. Results of single systems are given in Table 1. We can see that BM25 is a simple yet very effective baseline method. Our basic model performs better than all the single models in terms of MAP. Method BM25 WordCnt MT based PP CNN TQNN (baseline) Seq2SeqPara GCN (competiti"
N18-1141,D16-1127,0,0.264999,"rks on generating questions from different resources, including a sentence(Heilman, 2011), a topic (Chali and Hasan, 2015), a fact from knowledge base (Serban et al., 2016), etc. In computer vision community, there are also recent studies on generating questions from an image (Mostafazadeh et al., 2016). Our QG model belongs to sentence-based question generation. GAN has been successfully applied in computer vision tasks (Goodfellow et al., 2014). There are also some recent trials that adapt GAN to text generation (Yu et al., 2017), question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary tas"
N18-1141,P17-1103,0,0.0185062,"49M query-table pairs. An example of the data is given in Figure 2. We randomly select 1.29M as the training set, 0.1M as the dev set and 0.1M as the test set. We evaluate the performance on table-based QA with Mean Average Precision (MAP) and Precision@1 (P@1) (Manning et al., 2008). We use the same candidate retrieval adopted in (Yan et al., 2017), namely representing a table as bag-ofwords, to guarantee the efficiency of the approach. Each query has 50 candidate tables on average. It is still an open problem to automatically evaluate the performance of a natural language generation system (Lowe et al., 2017). In this work, we use BLEU-4 (Papineni et al., 2002) score as the evaluation metric, which measures the overlap between the generated question and the referenced question. The hyper parameters are tuned on the validation set and the performance is reported on the test set. Results and Analysis We report the results and our analysis on table-based QA and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity be"
N18-1141,P16-1170,0,0.0518732,"simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks in natural language generation. There are several works on generating questions from different resources, including a sentence(Heilman, 2011), a topic (Chali and Hasan, 2015), a fact from knowledge base (Serban et al., 2016), etc. In computer vision community, there are also recent studies on generating questions from an image (Mostafazadeh et al., 2016). Our QG model belongs to sentence-based question generation. GAN has been successfully applied in computer vision tasks (Goodfellow et al., 2014). There are also some recent trials that adapt GAN to text generation (Yu et al., 2017), question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when t"
N18-1141,P02-1040,0,0.10243,"given in Figure 2. We randomly select 1.29M as the training set, 0.1M as the dev set and 0.1M as the test set. We evaluate the performance on table-based QA with Mean Average Precision (MAP) and Precision@1 (P@1) (Manning et al., 2008). We use the same candidate retrieval adopted in (Yan et al., 2017), namely representing a table as bag-ofwords, to guarantee the efficiency of the approach. Each query has 50 candidate tables on average. It is still an open problem to automatically evaluate the performance of a natural language generation system (Lowe et al., 2017). In this work, we use BLEU-4 (Papineni et al., 2002) score as the evaluation metric, which measures the overlap between the generated question and the referenced question. The hyper parameters are tuned on the validation set and the performance is reported on the test set. Results and Analysis We report the results and our analysis on table-based QA and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity between a query and a table is calculated with the BM25"
N18-1141,D16-1264,0,0.0851505,"e the accuracy of the QA model. 2 Related Work Our work connects to existing works on question answering (QA), question generation (QG), and the use of generative adversarial nets in question answering and text generation. We consider two kinds of answer selection tasks in this work, one is table as the answer (Balakrishnan et al., 2015) and another is sentence as the answer (Yang et al., 2015). In natural language processing community, there are also other types of QA tasks including knowledge based QA (Berant et al., 2013), community based QA (Qiu and Huang, 2015) and reading comprehension (Rajpurkar et al., 2016). We believe that our algorithm is generic and could also be applied to these tasks with dedicated QA and QG model architectures. Despite the use of sophisticated features could learn a more accurate QA model, in this work we implement a simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks in natural language generation. There are several works on generating questions from differe"
N18-1141,P17-1096,0,0.474587,"objective of which is to maximize the expectation of the QA scores of the generated question-answer pairs. Policy gradient method (Williams, 1992; Yu et al., 2017) is used to update the QG model. In turn, the QG model improves QA through incorporating additional training instances. Here the key problem is how to label the generated question-answer pair. The application of Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 2017) in this scenario regards every generated question-answer pair as a negative instance. On the contrary, Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) regards every generated question-answer pair appended with special domain tag as a positive instance. However, it is non-trivial to label the generated question-answer pairs because some of which are good paraphrases of the ground truth yet some might be negative instances with similar utterances. To address this, we bring in a collaboration detector, which takes two question-answer pairs as the input and determines their relation as collaborative or competitive. The output of the collaboration detector is regarded as the label of the generated questionanswer pair. We conduct experiments on b"
P14-1146,C10-2005,0,0.00563043,"aches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focu"
P14-1146,P07-1056,0,0.114896,"plement this system because the codes are not publicly available 3 . NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model. The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases. Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007). Results and Analysis. Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets. Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier. The results of bagof-ngram (uni/bi/tri-gram) features are not satisfied because the one-hot word representation cannot capture the latent connections between words. NBSVM and RAE perform comparably and have 3 For 3-class sentiment classification in SemEval 2013, our re-implementation"
P14-1146,C10-2028,0,0.00814251,"t classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter senti"
P14-1146,P11-2008,0,0.0123158,"Missing"
P14-1146,P13-1088,0,0.0113325,"Missing"
P14-1146,P11-1016,1,0.370167,"their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment cl"
P14-1146,P13-2087,0,0.792293,"utoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and"
P14-1146,P11-1015,0,0.646787,"(2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from sc"
P14-1146,D12-1110,0,0.368522,"Missing"
P14-1146,P13-1045,0,0.149339,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,S13-2053,0,0.231338,"on has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases"
P14-1146,S13-2052,0,0.0193943,"to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the w"
P14-1146,pak-paroubek-2010-twitter,0,0.047259,"low traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based me"
P14-1146,W02-1011,0,0.132502,"dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the lea"
P14-1146,D11-1014,0,0.926164,"Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We propose incorporat"
P14-1146,D13-1170,0,0.34155,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,J11-2001,0,0.172252,"ion, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy"
P14-1146,P02-1053,0,0.0615851,"asks. 2 Related Work In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011;"
P14-1146,P12-2018,0,0.321669,"ve/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013. Baseline Methods. We compare our method with the following sentiment classification algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available"
P14-1146,H05-1044,0,0.100893,"he sentiment lexicon, P#Lex PN j=1 β(wi , cij ) i=1 Accuracy = (10) #Lex × N where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, β(wi , cij ) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case. The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon. We set N as 100 in our experiment. Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding. For each lexicon, we remove the words that do not appear in the lookup table of word embedding. We only use unigram embedding in this section because these sentiment lexicons do not contain phrases. The distribution of the lexicons used in this paper is listed in Table 4. Lexicon HL MPQA Joint Positive 1,331 1,932 1,051 Negative 2,647 2,817 2,024 Total 3,978 4,749 3,075 Table 4: Statistics of the sentiment lexicons. Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity. Results. Table 5"
P14-1146,D11-1016,0,0.0249497,"the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors."
P14-1146,D13-1061,0,0.152584,"representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors. It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity. In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis. We encode the sentiment information in1555 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics to the continuous representation of words, so that it is able to separate good and bad to opposite ends"
P14-1146,P10-1040,0,\N,Missing
P14-2009,W02-1011,0,0.0546421,"Missing"
P14-2009,D12-1110,0,0.0247666,"Missing"
P14-2009,P11-2008,0,0.0271014,"Missing"
P14-2009,D13-1170,0,0.118265,"Missing"
P14-2009,J11-2001,0,0.042909,"respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗ Contribution during internship at Microsoft Research. 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 3 determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better performances than the baseline methods. 2 We use the dependency parsing results to find the words syntactically connected with the interested target. Adaptive Recursive Neural Network is proposed to propagate the sentiments of words to the target node. We model the adaptive sentiment propagations as semantic compositions. The computation process is conducted in a bottom-up manner, and the vector representations"
P14-2009,P11-1016,1,0.747726,"is to classify their sentiments for a given target as positive, negative, and neutral. People may mention several entities (or targets) in one tweet, which affects the availabilities for most of existing methods. For example, the tweet “@ballmer: windows phone is better than ios!” has three targets (@ballmer, windows phone, and ios). The user expresses neutral, positive, and negative sentiments for them, respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗ Contribution during internship at Microsoft Research. 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 3 determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better perfo"
P14-2009,P03-1054,0,0.0549664,"Missing"
P14-2009,N10-1120,0,0.0373418,"Missing"
P15-1098,D10-1115,0,0.00705842,"rder to model user-text consistency, we represent each user as a continuous matrix Uk ∈ RdU ×d , which acts as an operator to modify the semantic meaning of a word. This is on the basis of vector based semantic composition (Mitchell and Lapata, 2010). They regard compositional modifier as a matrix X1 to modify another component x2 , and use matrix-vector multiplication y = X1 × x2 as the composition function. Multiplicative semantic composition is suitable for our need of user modifying word meaning, and it has been successfully utilized to model adjectivenoun composition (Clark et al., 2008; Baroni and Zamparelli, 2010) and adverb-adjective composition (Socher et al., 2012). Similarly, we model product-text consistency by encoding each product as a matrix Pj ∈ RdP ×d , where d is the dimension of word vector, dP is the output length of product-word multiplicative composition. After conducting user-word multiplication and productword multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product enhanced document representation. Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning fr"
P15-1098,I13-1156,0,0.266074,"lassifier and achieve better performance2 . In this paper, we propose a new model dubbed User Product Neural Network (UPNN) to capture user- and product-level information for sentiment classification of documents (e.g. reviews). UPNN takes as input a variable-sized document as well as the user who writes the review and the product which is evaluated. It outputs sentiment polarity label of a document. Users and products are encoded in continuous vector spaces, the representations of which capture important global clues such 2 One can manually design a small number of user and product features (Gao et al., 2013). However, we argue that they are not effective enough to capture sophisticated semantics of users and products. 1014 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1014–1023, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as user preferences and product qualities. These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification. We apply UPNN to three datasets der"
P15-1098,W06-3808,0,0.0209607,"earns tailored representation for each user and product. We evaluate classification accuracy on the extracted OOV test set. Experimental results are given in Figure 5. We can find that these two strategies perform slightly better than UPNN (no UP), but still worse than the full model. 5 5.1 Related Work Sentiment Classification Sentiment classification is a fundamental problem in sentiment analysis, which targets at inferring the sentiment label of a document. Pang and Lee (2002; 2005) cast this problem a classification task, and use machine learning method in a supervised learning framework. Goldberg and Zhu (2006) use unlabelled reviews in a graphbased semi-supervised learning method. Many studies design effective features, such as text topic (Ganu et al., 2009), bag-of-opinion (Qu et al., 2010) and sentiment lexicon features (Kiritchenko et al., 2014). User information is also used for sentiment classification. Gao et al. (2013) design user-specific features to capture user leniency. Li et al. (2014) incorporate textual topic and user-word factors with supervised topic modeling. Tan et al. (2011) and Hu et al. (2013) utilize usertext and user-user relations for Twitter sentiment analysis. Unlike most"
P15-1098,P14-1062,0,0.13981,"rd this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Socher et al., 2013; Kalchbrenner et al., 2014; Le and Mikolov, 2014). Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review “it works great” and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sentiment rating of a review. Product quality also has an impact on review sentiment rating. Reviews towards high-quality products (e.g. Macbook)"
P15-1098,D14-1181,0,0.0387917,"m distribution, regarded as a parameter and jointly trained with other parameters of neural networks. Alternatively, they can be pretrained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b), and applied as initial values of word embedding matrix. We adopt the latter strategy which better exploits the semantic and grammatical associations of words. To model semantic representations of sentences, convolutional neural network (CNN) and recursive neural network (Socher et al., 2013) are two state-of-the-art methods. We use CNN (Kim, 2014; Kalchbrenner et al., 2014) in this work as it does not rely on external parse tree. Specifically, we use multiple convolutional filters with different widths to produce sentence representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for sentiment classification. The convolutional filter with a width of 3 essentially captures the semantics of trigrams in a sentence. Accordingly, multiple convolutional filters with widths of 1, 2 and 3 encode the semantics of unigrams, bigrams and trigrams in a sentence. A"
P15-1098,P13-2087,0,0.0199465,"n that we encode four kinds of consistencies and use neural network approach. User representation is also leveraged for recommendation (Weston et al., 2013), web search (Song et al., 2014) and social media analytics (Perozzi et al., 2014). 5.2 Neural Network for Sentiment Classification Neural networks have achieved promising results for sentiment classification. Existing neural network methods can be divided into two groups: word embedding and semantic composition. For learning word embeddings, (Mikolov et al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent neural network on five NLP t"
P15-1098,D14-1162,0,0.111996,"sentations of user k and product j for capturing user-text and product-text consistencies. ous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is the size of word vocabulary. These word vectors can be randomly initialized from a uniform distribution, regarded as a parameter and jointly trained with other parameters of neural networks. Alternatively, they can be pretrained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b), and applied as initial values of word embedding matrix. We adopt the latter strategy which better exploits the semantic and grammatical associations of words. To model semantic representations of sentences, convolutional neural network (CNN) and recursive neural network (Socher et al., 2013) are two state-of-the-art methods. We use CNN (Kim, 2014; Kalchbrenner et al., 2014) in this work as it does not rely on external parse tree. Specifically, we use multiple convolutional filters with different widths to produce sentence representation. The reason is that they are capab"
P15-1098,C10-1103,0,0.102162,"ity or intensity (e.g. 1-5 or 1-10 stars on review sites) of a document. Dominating studies follow Pang et al. (2002; 2005) and regard this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Socher et al., 2013; Kalchbrenner et al., 2014; Le and Mikolov, 2014). Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review “it works great” and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sen"
P15-1098,D15-1278,0,0.0128866,"t al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent neural network on five NLP tasks including sentiment classification. (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2014) use convolutional neural networks. Le and Mikolov (2014) introduce 1021 Paragraph Vector. Unlike existing neural network approaches that only use the semantics of texts, we take consideration of user and product representations and leverage their connections with text semantics for sentiment classification. This work is an extension of our previous work (Tang et al., 2015), which only takes cons"
P15-1098,D12-1110,0,0.0623837,"a continuous matrix Uk ∈ RdU ×d , which acts as an operator to modify the semantic meaning of a word. This is on the basis of vector based semantic composition (Mitchell and Lapata, 2010). They regard compositional modifier as a matrix X1 to modify another component x2 , and use matrix-vector multiplication y = X1 × x2 as the composition function. Multiplicative semantic composition is suitable for our need of user modifying word meaning, and it has been successfully utilized to model adjectivenoun composition (Clark et al., 2008; Baroni and Zamparelli, 2010) and adverb-adjective composition (Socher et al., 2012). Similarly, we model product-text consistency by encoding each product as a matrix Pj ∈ RdP ×d , where d is the dimension of word vector, dP is the output length of product-word multiplicative composition. After conducting user-word multiplication and productword multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product enhanced document representation. Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning framework (Pang and Lee, 2005). Instead of using handcraf"
P15-1098,P11-1015,0,0.0913889,"Li et al. (2014) in that we encode four kinds of consistencies and use neural network approach. User representation is also leveraged for recommendation (Weston et al., 2013), web search (Song et al., 2014) and social media analytics (Perozzi et al., 2014). 5.2 Neural Network for Sentiment Classification Neural networks have achieved promising results for sentiment classification. Existing neural network methods can be divided into two groups: word embedding and semantic composition. For learning word embeddings, (Mikolov et al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent ne"
P15-1098,P14-5010,0,0.0030416,"atasets used for sentiment classification. The rating scale of IMDB dataset is 1-10. The rating scale of Yelp 2014 and Yelp 2013 datasets is 1-5. |V |is the vocabulary size of words in each dataset. #users is the number of users, #docs/user means the average number of documents per user posts in the corpus. et al., 2014) and Yelp Dataset Challenge4 in 2013 and 2014. Statistical information of the generated datasets are given in Table 1. We split each corpus into training, development and testing sets with a 80/10/10 split, and conduct tokenization and sentence splitting with Stanford CoreNLP (Manning et al., 2014). We use standard accuracy (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000) to measure the overall sentiment classification performance, and use M AE and RM SE to measure the divergences between predicted sentiment ratings (pr) and ground truth ratings (gd). P |gdi − pri | M AE = i (3) N rP 2 i (gdi − pri ) RM SE = (4) N 4.2 Baseline Methods We compare UPNN with the following baseline methods for document-level sentiment classification. (1) Majority is a heuristic baseline method, which assigns the majority sentiment category in training set to each review in the test dataset. (2) In Tr"
P15-1098,P05-1015,0,0.0327322,"rb-adjective composition (Socher et al., 2012). Similarly, we model product-text consistency by encoding each product as a matrix Pj ∈ RdP ×d , where d is the dimension of word vector, dP is the output length of product-word multiplicative composition. After conducting user-word multiplication and productword multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product enhanced document representation. Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning framework (Pang and Lee, 2005). Instead of using handcrafted features, we use continuous representation of documents, users and products as discriminative features. The sentiment classifier is built from documents with gold standard sentiment labels. As is shown in Figure 2, the feature representation for building rating predictor is the concatenation of three parts: continuous user representation uk , continuous product representation pj and continuous document representation vd , where vd encodes user-text consistency, product-text consistency and document level semantic composition. We use sof tmax to build the classifi"
P15-1098,W02-1011,0,0.0331478,"l evidence in turn facilitates embedding learning procedure at document level, yielding better text representations. By combining evidence at user-, product- and documentlevel in a unified neural framework, the proposed model achieves state-of-the-art performances on IMDB and Yelp datasets1 . 1 Introduction Document-level sentiment classification is a fundamental problem in the field of sentiment analysis and opinion mining (Pang and Lee, 2008; Liu, 2012). The task is to infer the sentiment polarity or intensity (e.g. 1-5 or 1-10 stars on review sites) of a document. Dominating studies follow Pang et al. (2002; 2005) and regard this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Soche"
P15-1098,D13-1170,0,0.476576,"(2002; 2005) and regard this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Socher et al., 2013; Kalchbrenner et al., 2014; Le and Mikolov, 2014). Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review “it works great” and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sentiment rating of a review. Product quality also has an impact on review sentiment rating. Reviews towards high-qual"
P15-1098,C14-1018,1,0.783389,"ernational Joint Conference on Natural Language Processing, pages 1014–1023, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as user preferences and product qualities. These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification. We apply UPNN to three datasets derived from IMDB and Yelp Dataset Challenge. We compare to several neural network models including recursive neural networks (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), sentimentspecific word embedding (Tang et al., 2014b), and a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) UPNN outperforms baseline methods for sentiment classification of documents; (2) incorporating representations of users and products significantly improves classification accuracy. The main contributions of this work are as follows: • We present a new neural network method (UPNN) by leveraging users and products for document-level sentiment classification. • We validate the influences of users and products in terms of sentiment and text on massive IMDB and Yelp reviews. • We repor"
P15-1098,P14-1146,1,0.652646,"ernational Joint Conference on Natural Language Processing, pages 1014–1023, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as user preferences and product qualities. These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification. We apply UPNN to three datasets derived from IMDB and Yelp Dataset Challenge. We compare to several neural network models including recursive neural networks (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), sentimentspecific word embedding (Tang et al., 2014b), and a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) UPNN outperforms baseline methods for sentiment classification of documents; (2) incorporating representations of users and products significantly improves classification accuracy. The main contributions of this work are as follows: • We present a new neural network method (UPNN) by leveraging users and products for document-level sentiment classification. • We validate the influences of users and products in terms of sentiment and text on massive IMDB and Yelp reviews. • We repor"
P15-1098,C14-1064,0,0.0191227,"ddings, (Mikolov et al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent neural network on five NLP tasks including sentiment classification. (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2014) use convolutional neural networks. Le and Mikolov (2014) introduce 1021 Paragraph Vector. Unlike existing neural network approaches that only use the semantics of texts, we take consideration of user and product representations and leverage their connections with text semantics for sentiment classification. This work is an extension of our previous work (Tang et al., 2015), whi"
P16-2011,P15-2047,1,0.12748,"as alternatives. 2.2 Output 2.4 Training In our model, the loss function is the cross-entropy error of event trigger identification and trigger classification. We initialize all parameters to form a uniform distribution U (−0.01, 0.01). We set the widths of convolutional filters as 2 and 3. The number of feature maps is 300 and the dimension of the PF is 5. Table 1 illustrates the setting parameters used for three languages in our experiments (Zeiler, 2012). Convolution Neural Network 3 As the convolutional neural network (CNN) is good at capturing salient features from a sequence of objects (Liu et al., 2015), we design a CNN to capture some local chunks. This approach has been used for event detection in previous studies (Nguyen and Grishman, 2015; Chen et al., 2015). Specifically, we use multiple convolutional filters with different widths to produce local context representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for event detection. In our work, multiple convolutional filters with widths of 2 and 3 encode the semantics of bigrams and trigrams in a sentence. This local information can also help our mode"
P16-2011,R15-1010,0,0.0363436,"and syntactic features, thus the precision is lower than neural network based methods. (4) RNN and LSTM perform slightly worse than Bi-LSTM. An obvious reason is that RNN and LSTM only consider the preceding sequence information of the trigger, which may miss some important following clues. Considering S1 again, when extracting the trigger “releases”, both models will miss the following sequence “20 million euros to Iraq”. This may seriously hinder the performance of RNN and LSTM for event detection. Table 2: Comparison of different methods on English event detection. (5) Pattern Recognition (Miao and Grishman, 2015), using a pattern expansion technique to extract event triggers. (6) Convolutional Neural Network (Chen et al., 2015), which exploits a dynamic multi-pooling convolutional neural network for event trigger detection. 3.2 Comparison On English Table 2 shows the overall performance of all methods on the ACE2005 English corpus. We can see that our approach significantly outperforms all previous methods. The better performance of HNN can be further explained by the following reasons: (1) Compared with feature based methods, such as MaxEnt, Cross-Event, Cross-Entity, and Joint Model, neural network"
P16-2011,C12-1033,0,0.0193323,"Missing"
P16-2011,P15-1017,0,0.563445,"t trigger or not. Specifically, we first use a Bi-LSTM to encode semantics of each word with its preceding and following information. Then, we add a convolutional neural network to capture structure information from local contexts. 2.1 Bi-LSTM In this section we describe a Bidirectional LSTM model for event detection. Bi-LSTM is a type of bidirectional recurrent neural networks (RNN), which can simultaneously model word representation with its preceding and following information. Word representations can be naturally considered as features to detect triggers and their event types. As show in (Chen et al., 2015), we take all the words of the whole sentence as the input and each token is transformed by looking up word embeddings. Specifically, we use the SkipGram model to pre-train the word embeddings to represent each word (Mikolov et al., 2013; Bahdanau et al., 2014). We present the details of Bi-LSTM for event trigger extraction in Figure 2. We can see that Bi-LSTM is composed of two LSTM neural networks, a forward LSTMF to model the preced67 vector with fixed length. C3 Max-Pooling Feature Map 1 Feature Map 2 Lookup 2.3 Feature Map n ... Convolution ... At the end, we concatenate the bidirectional"
P16-2011,P15-2060,0,0.548592,"d trigger classification. We initialize all parameters to form a uniform distribution U (−0.01, 0.01). We set the widths of convolutional filters as 2 and 3. The number of feature maps is 300 and the dimension of the PF is 5. Table 1 illustrates the setting parameters used for three languages in our experiments (Zeiler, 2012). Convolution Neural Network 3 As the convolutional neural network (CNN) is good at capturing salient features from a sequence of objects (Liu et al., 2015), we design a CNN to capture some local chunks. This approach has been used for event detection in previous studies (Nguyen and Grishman, 2015; Chen et al., 2015). Specifically, we use multiple convolutional filters with different widths to produce local context representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for event detection. In our work, multiple convolutional filters with widths of 2 and 3 encode the semantics of bigrams and trigrams in a sentence. This local information can also help our model fix some errors due to lexical ambiguity. An illustration of CNN with three convolutional filters is given in Figure 3. Let us denote a sent"
P16-2011,P11-1113,0,0.595906,"ection aims to extract event triggers (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”)"
P16-2011,P08-1030,1,0.898397,"Missing"
P16-2011,D15-1167,1,0.218601,"it will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are the output of Bi-LSTM and C2 , C3 are the output of CNN with convolutional filters with widths of 2 and 3. use of word embeddings to induce a more general representation for trigger candidates. Recently, deep learning techniques have been widely used in modeling complex structures and proven effective for many NLP tasks, such as machine translation (Bahdanau et al., 2014), relation extraction (Zeng et al., 2014) and sentiment analysis (Tang et al., 2015a). Bi-directional long short-term memory (Bi-LSTM) model (Schuster et al., 1997) is a two-way recurrent neural network (RNN) (Mikolov et al., 2010) which can capture both the preceding and following context information of each word. Convolutional neural network (CNN) (LeCun et al., 1995) is another effective model for extracting semantic representations and capturing salient features in a flat structure (Liu et al., 2015), such as chunks. In this work, we develop a hybrid neural network incorporating two types of neural networks: Bi-LSTM and CNN, to model both sequence and chunk information f"
P16-2011,P14-1038,1,0.0783646,"e (F). Table 1 shows the detailed description of the data sets used in our experiments. We abbreviate our model as HNN (Hybrid Neural Networks). 3.1 Baseline Methods We compare our approach with the following baseline methods. (1) MaxEnt, a basesline feature-based method, which trains a Maximum Entropy classifier with some lexical and syntactic features (Ji et al., 2008). (2) Cross-Event (Liao et al., 2010), using document-level information to improve the performance of ACE event extraction. (3) Cross-Entity (Hong et al., 2011), extracting events using cross-entity inference. (4) Joint Model (Li and Ji, 2014), a joint structured perception approach, incorporating multilevel linguistic features to extract event triggers and arguments at the same time so that local predictions can be mutually improved. 68 Language English Chinese Spanish Word Embedding corpus dim NYT 300 Gigaword 300 Gigaword 300 Gradient Learning Method method parameters SGD learning rate r = 0.03 Adadelta p = 0.95, δ = 1e−6 Adadelta p = 0.95, δ = 1e−6 Corpus ACE2005 ACE2005 ERE Data Sets Train Dev 529 30 513 60 93 12 Test 40 60 12 Table 1: Hyperparameters and # of documents used in our experiments on three languages. Model MaxEnt"
P16-2011,P13-1008,1,0.945252,"act event triggers (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are t"
P16-2011,D15-1278,0,0.0128362,"Missing"
P16-2011,C14-1220,0,0.0106635,"Missing"
P16-2011,P15-1107,0,0.0217978,"s (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are the output of Bi-L"
P16-2011,P10-1081,0,0.353297,"Missing"
P18-1034,Q13-1005,0,0.0484716,"udies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic parsing over tables, which is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model paramet"
P18-1034,D13-1160,0,0.522838,"Missing"
P18-1034,D14-1179,0,0.0149909,"Missing"
P18-1034,H94-1010,0,0.425333,"he input, and outputs a SQL query y. We do not consider the join operation over multiple relational tables, which we leave in the future work. We use WikiSQL (Zhong et al., 2017), the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of S"
P18-1034,P17-1174,1,0.889119,"Missing"
P18-1034,N16-1024,0,0.0300555,"imum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect t"
P18-1034,P17-1089,0,0.106833,"mn names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti"
P18-1034,C12-2040,0,0.0387214,", the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL que"
P18-1034,P17-1167,0,0.0492084,"Missing"
P18-1034,P18-1168,0,0.060629,"Missing"
P18-1034,P16-1154,0,0.051477,"etworks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One advantage of this architecture is that it"
P18-1034,P16-1002,0,0.0341733,"al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. questi"
P18-1034,P16-1014,0,0.0177017,"y We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One"
P18-1034,P16-1086,0,0.028017,"s a probability distribution over the tokens from one of the three channels. X p(yt |y&lt;t , x) = pw (yt |zt , y&lt;t , x)pz (zt |y&lt;t , x) Methodology We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and ge"
P18-1034,P17-1097,0,0.143076,"is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu"
P18-1034,D16-1116,0,0.0360868,"Missing"
P18-1034,D17-1160,0,0.26447,"sign of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4.2 STAMP: Syntax- and Table- Aware seMantic Parser Figure 2 illustrates an overview of the prop"
P18-1034,Q13-1016,0,0.0505279,"tional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) th"
P18-1034,D11-1140,0,0.0801846,"Missing"
P18-1034,P15-1142,0,0.163748,"et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Z"
P18-1034,P16-1003,0,0.0875917,"s in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for le"
P18-1034,P17-1003,0,0.0189993,"tional databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus base"
P18-1034,D14-1162,0,0.0809332,"ells, and weighted average two cell distributions, which is calculated as follows. cell pcell pcell w (j) = λˆ w (j) + (1 − λ)αj 4.5 As the WikiSQL data contains rich supervision of question-SQL pairs, we use them to train model parameters. The model has two cross-entropy loss functions, as given below. One is for the switching gate classifier (pz ) and another is for the attentional probability distribution of a channel (pw ). l=− X X logpw (yt |zt , y&lt;t , x) logpz (zt |y&lt;t , x)− t t (6) Our parameter setting strictly follows Zhong et al. (2017). We represent each word using word embedding2 (Pennington et al., 2014) and the mean of the sub-word embeddings of all the n-grams in the word (Hashimoto et al., 2016)3 . The dimension of the concatenated word embedding is 400. We clamp the embedding values to avoid over-fitting. We set the dimension of encoder and decoder hidden state as 200. During training, we randomize model parameters from a uniform distribution with fan-in and fan-out, set batch size as 64, set the learning rate of SGD as 0.5, and update the model with stochastic gradient descent. Greedy search is used in the inference process. We use the model trained from question-SQL pairs as initializat"
P18-1034,P13-1092,0,0.0220016,"ses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic exec"
P18-1034,P11-1060,0,0.149695,"set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coup"
P18-1034,P17-1105,0,0.048093,"ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has n"
P18-1034,P17-2034,0,0.0222559,"to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic p"
P18-1034,D07-1071,0,0.600848,"Missing"
P18-1034,P07-1121,0,0.306045,"Missing"
P18-1034,D17-1125,0,0.635741,"Missing"
P18-1034,P17-1065,1,0.837395,"17; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4"
P18-1034,P16-1127,0,0.111172,"Missing"
P18-1034,P17-1041,0,0.227409,"the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input se"
P18-1034,N18-2093,0,0.393921,"Missing"
P18-1034,P16-1004,0,\N,Missing
P18-1034,P16-1138,0,\N,Missing
S14-2033,S13-2053,0,0.595629,"utomatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants o"
S14-2033,P14-2009,1,0.80825,"afted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creat"
S14-2033,S13-2052,0,0.118118,"Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants owning larger training data than us. The performance of only using SSWE as features is comparable to th"
S14-2033,W02-1011,0,0.0200027,"nction 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 http://alt.qcri.org/semeval20"
S14-2033,P11-2008,0,0.291901,"Missing"
S14-2033,P14-1146,1,0.727439,"uru Wei‡ , Bing Qin† , Ting Liu† , Ming Zhou‡ Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment c"
S14-2033,P11-1016,1,0.797284,"res with the state-of-the-art hand-crafted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the orga"
S14-2033,H05-1044,0,0.204982,"Missing"
S14-2033,P14-1062,0,0.00858127,"omputing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To le"
