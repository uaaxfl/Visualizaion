1993.iwpt-1.11,C90-2049,0,0.0653994,"Missing"
2001.mtsummit-papers.5,C00-2131,1,0.878601,"sh translation dictionary consultation. Then, the system finds correspondences in remaining phrases by using sentences dependency structures and the balance of all correspondences. The method is based on an assumption that in parallel corpus most fragments in a source sentence have corresponding fragments in a target sentence. Keywords Example-based Translation, Finding Phrasal Correspondence, Phrasal Alignment, Parallel Corpus, Dependency Structure Figure 1: System Image and an Example of System Output Introduction Example-based translation system requires a large set of translation patterns [1]. Over the last decade, the sentence alignment and word alignment have been explored and achieved numerous successes by using statistical approach. In contrast, a fewer results are reported in phrasal alignment. In statistical phrasal alignment acquires the bilingual-correspondences appear with high frequency. However the coverage is low [2] [3]. In parallel corpus, we think that fragments in a source sentence usually have corresponding fragments in a target sentence. So, this paper proposes a system finds correspondences by using dependency structures and a balance of correspondences. This pa"
2001.mtsummit-papers.5,P95-1033,0,0.0752679,"le-based Translation, Finding Phrasal Correspondence, Phrasal Alignment, Parallel Corpus, Dependency Structure Figure 1: System Image and an Example of System Output Introduction Example-based translation system requires a large set of translation patterns [1]. Over the last decade, the sentence alignment and word alignment have been explored and achieved numerous successes by using statistical approach. In contrast, a fewer results are reported in phrasal alignment. In statistical phrasal alignment acquires the bilingual-correspondences appear with high frequency. However the coverage is low [2] [3]. In parallel corpus, we think that fragments in a source sentence usually have corresponding fragments in a target sentence. So, this paper proposes a system finds correspondences by using dependency structures and a balance of correspondences. This paper is organized as follows: In the next section, we present the overview of our approach. In section 3, we describe our methods in detail. In section 4, experiments and results are given. In section 5, we describe a conclusion. Overview of our approach We have developed a system finds phrasal correspondences in parallel parsed corpus that a"
2001.mtsummit-papers.5,C00-2135,0,0.0566837,"ased Translation, Finding Phrasal Correspondence, Phrasal Alignment, Parallel Corpus, Dependency Structure Figure 1: System Image and an Example of System Output Introduction Example-based translation system requires a large set of translation patterns [1]. Over the last decade, the sentence alignment and word alignment have been explored and achieved numerous successes by using statistical approach. In contrast, a fewer results are reported in phrasal alignment. In statistical phrasal alignment acquires the bilingual-correspondences appear with high frequency. However the coverage is low [2] [3]. In parallel corpus, we think that fragments in a source sentence usually have corresponding fragments in a target sentence. So, this paper proposes a system finds correspondences by using dependency structures and a balance of correspondences. This paper is organized as follows: In the next section, we present the overview of our approach. In section 3, we describe our methods in detail. In section 4, experiments and results are given. In section 5, we describe a conclusion. Overview of our approach We have developed a system finds phrasal correspondences in parallel parsed corpus that are c"
2001.mtsummit-papers.5,J94-4001,1,0.914151,"es in a dictionary (2817 sentences) Figure 9: E(k) and J(K) E(k) and J(k) are distances as shown in figure 9. E(k) is the distance between an English remaining phrase and an Short sentences We acquired test-set 200 sentences by extracting 100 sentences form each corpora under follow 3 conditions. Condition 1: A pair of sentences has one-to-one sentence correspondence. Condition 2: The number of both English and Japanese phrases differed by less than 2:1 ratio. Condition 3: The number of both English and Japanese phrases is less than 20 phrases We made a parsed bilingual corpus by using the KNP[4] Japanese parser (developed by Kyoto University) for Japanese Sentences and ESG[5] English parser (developed by IBM Watson Research Center) for English sentences. We evaluated the system output as follows. We tagged on correct target phrases every phrase in 200 testset sentences. If a system output correspondence exactly equal with a pre-aligned correspondence, we regard it as correct. If a correspondence which system output partly matches with a pre-aligned correspondence, we regard it as near-correct. Else we regard it as wrong. In extend-correspondences selection, the system has the thresho"
2001.mtsummit-papers.5,C90-3044,1,0.895338,"Missing"
2001.mtsummit-papers.5,C92-2115,1,0.863046,"Missing"
2001.mtsummit-papers.5,J80-1003,0,\N,Missing
2004.iwslt-evaluation.15,2001.mtsummit-papers.5,1,0.918544,"Missing"
2004.iwslt-evaluation.15,A00-2018,0,0.111904,"Missing"
2004.iwslt-evaluation.15,W03-0312,1,0.846103,"Missing"
2004.iwslt-evaluation.15,P02-1040,0,0.0768205,"0.49 per 0.45 0.42 gtm 0.66 0.67 3. Experiments 3.1. Experimental Condition We built translation examples from a training-set for the IWSLT04. The training-set consists of 20,000 Japanese and English sentence pairs. The evaluation was conducted using a dev-set and a test-set for the IWSLT04, which consist of about 500 Japanese sentences with 16 references. 3.2. Result The following five automatic evaluation results are shown in Table 1 and some translation samples are shown in Table 2. BLEU: The geometric mean of n-gram precision by the system output with respect to the reference translations[6]. NIST: A variant of BLEU using the arithmetic mean of weighted n-gram precision values. WER (word error rate): The edit distance between the system output and the closest reference translation. PER (position-independent WER): A variant of mWER which disregards word ordering. Figure 6: Corpus Size and Performance (BLEU). * The system without a corpus can generate translations using only the translation dictionaries. GTM (general text matcher): Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid. 93 A target language model may be helpful for th"
2004.iwslt-evaluation.15,J94-4001,1,\N,Missing
2005.iwslt-1.27,W99-0604,0,0.0401063,"common and something different. The important common feature is to use bilingual corpus, or translation examples, for the translation of new inputs. Both methods exploit translation knowledge implicitly embedded in translation examples, and make MT system maintenance and improvement much easier compared with Rule-Based Machine Translation. The difference is that SMT supposes bilingual corpus is the only available resource (but not a bilingual lexicon and parsers); EBMT does not consider such a constraint. SMT basically combines words or phrases (relatively small pieces) with high probability [2]; EBMT tries to use larger translation examples. When EBMT tries to use larger examples, it had better handle examples which are discontinuous as a word-string, but continuous structurally. Accordingly, though it is not inevitable, EBMT naturally seeks syntactic information. The difference in the problem setting is important. SMT is a natural approach when linguistic resources such as parsers and a bilingual lexicon are not available. On the other hand, in case of such linguistic resources are available, it is also natural to see how accurate MT can be achieved using all the available resource"
2005.iwslt-1.27,J94-4001,1,0.547121,"Missing"
2005.iwslt-1.27,A00-2018,0,0.0468239,"Missing"
2005.iwslt-1.27,J03-1002,0,0.00380044,"other remaining nodes are merged into correspondences of their parent (or ancestor) nodes. In the case of Figure 1, “あの (that)” is merged into the correspondence “ 車 (car) ↔ the car”, since it is within an NP. Then, “突然 (suddenly)”, “at me” and “from the side” are merged into their parent correspondence, “ 飛び出して来 たのです (rush out) ↔ came”. We call the correspondences constructed so far as basic correspondences. 2.5. Comparison with EM based Alignment Here, let us compare our alignment method with an EM based alignment. We tested an EM based tool, giza++ for the alignment of 20,000 training data [6]. We found many inappropriate word alignments in the giza++ results, and concluded that this size of training data might be too small for EM based alignment. On the other hand, our method using a 0.9M-entry bilingual dictionary and a transliteration module could find correspondences quite accurately. For the given training set, we could conclude that our proposed method is superior to the EM based method. However, the correspondence statistics in the whole training data must be an important information, and it is our future target to use a flat bilingual dictionary and the statistical informat"
2005.iwslt-1.27,2005.mtsummit-papers.29,1,0.795144,"Missing"
2005.iwslt-1.27,P93-1004,0,0.0606899,"that parsing might cause sideeffects and lower translation performance. As we mentioned above, parsing errors are not a principal cause of translation errors, but these are not a few. One of the possible countermeasures is to reconsider the learning process of an English parser. The English parser used here is learned from Penn Treebank, and seems to be vulnerable to conversational sentences in travel domain. Furthermore, it is quite possible to improve parsing accuracies of both languages complementarily by taking advantage of the difference of syntactic ambiguities between the two languages [9]. This approach may not substantially improve the parsing accuracy of the travel domain sentences, because of their short length, but is promising for translating longer general sentences. 7. Conclusion As we stated in Introduction, we not only aim at the development of machine translation through some evaluation measure, but also tackle this task from the comprehensive viewpoint including the development of structural NLP. The examination of translation errors revealed the problems, such as problems in parsing and inflexible matching of a Japanese input and Japanese translation examples. Reso"
2005.mtsummit-papers.29,2004.iwslt-evaluation.1,0,0.0123601,"ure. ᶛ VTCPUNCVKQPGZCORNG UQWTEG U K 6&apos; K K VCTIGV U 6&apos; U QWVRWV V V V V V V Figure 5: Output Sentence Generation. 4 Experiments 2. The relation between translation examples is equal to the relation between their corresponding input phrases. For example, TE2 has a corresponding input phrase i2 , and it has a relation to i3 in the input structure. In this case, TE2 has a relation to an example phrase t( 3), which corresponds to i3 ( as shown in a dotted line). 4.1 Experimental setting For evaluation, we used corpora (trainingset and test-set) which are provided in the IWSLT04(Akiba et al., 2004). The training-set consists of 20K English-Japanese sentence pairs in a travel conversation domain. We built translation examples from the training-set by using the proposed alignment method mentioned in Section 3. The test-set consists of 500 Japanese sentences and their English references (500 × 16). The experiments are conducted using the following ﬁve systems: Finally, the output word-order is decided based on the n-gram language model (n = 3). proposed: The system which selects translation examples based on the proposed 223 method . Table 1: Evaluation Metrics. basic: The system (Aramaki"
2005.mtsummit-papers.29,2004.iwslt-evaluation.15,1,0.799506,"., 2004). The training-set consists of 20K English-Japanese sentence pairs in a travel conversation domain. We built translation examples from the training-set by using the proposed alignment method mentioned in Section 3. The test-set consists of 500 Japanese sentences and their English references (500 × 16). The experiments are conducted using the following ﬁve systems: Finally, the output word-order is decided based on the n-gram language model (n = 3). proposed: The system which selects translation examples based on the proposed 223 method . Table 1: Evaluation Metrics. basic: The system (Aramaki and Kurohashi, 2004) which selects translation examples based on the heuristic criterion. This system submitted translation evaluation workshop on IWSLT04(Akiba et al., 2004), and showed its basic feasibility. Note that basic uses the same alignment result as proposed. BLEU NIST baseline: EBMT baseline, which searches the most similar translation examples by using a character-based DP matching method, and outputs its target parts as is. WER C1, C2: Commercial machine translation systems under rule base approach. PER 4.2 Evaluation Evaluation is conducted based on the following conditions and by using the ﬁve eval"
2005.mtsummit-papers.29,2001.mtsummit-papers.5,1,0.784603,"rs are parsed by the Japanese parser KNP (Kurohashi and Nagao, 1994) and the English nl-parser (Charniak, 2000). The Japanese parser outputs a dependency structure, and we use it as is. The English parser outputs a phrase structure. Then, it is converted into a dependency structure by rules which decide on a head word in a phrase. A Japanese phrase unit consists of sequential content words and their following function words. An English phrase unit is a base-NP or a base-VP. Step 2: Alignment based on translation dictionary Then, correspondences are estimated by using translation dictionaries (Aramaki et al., 2001). We used four dictionaries: EDR, EDICT, ENAMDICT, and EIJIRO. These dictionaries have about two million entries in total. Step 3: Building Translation Example Database The system generate possible combinations of correspondences from aligned sentence pairs as shown in Figure 4. We regard these combinations of correspondences as translation examples, and store them in the database. In this operation, the system stores also their surrounding phrases, which are used for calculating context sim. Translation module Step 1: Input sentence analysis First, an input sentence is analyzed by the Japanes"
2005.mtsummit-papers.29,W03-0312,1,0.781548,"ith larger corpora. 㪇㪅㪋㪌 㪇㪅㪋 5 Related Work 㪇㪅㪊㪌 㪹㫃㪼㫌 㪇㪅㪊 㪇㪅㪉㪌 㪇㪅㪉 㪇㪅㪈㪌 㪇㪅㪈 㪇㪅㪇㪌 㪇 㪇 㪌㪇㪇㪇 㪈㪇㪇㪇㪇 㪈㪌㪇㪇㪇 㪉㪇㪇㪇㪇 㪺㫆㫉㫇㫌㫊㩷㫊㫀㫑㪼 㪧㪩㪦㪧㪦㪪㪜㪛 Figure 6: (BLEU). 㪙㪘㪪㪜㪣㪠㪥㪜 Corpus Size and Performance To our knowledge, there has been no work realizing EBMT based on the translation probability, and previous EBMT systems handle their translation examples using heuristic measures/criterion. For instance, MSR-MT (Richardson et al., 2001) retrieves translation examples by using only the example size. HPAT(Imamura, 2002) and TDMT(Furuse and Iida, 1994) are EBMT systems based on size and context similarity. UTOKYOMT(Aramaki et al., 2003) used alignment conﬁdence in addition to these metrics. Such a combination of multiple metrics leads to a problem of how to estimate the weight of each metric. 6 OTHERS: OTHERS is a case that multiple errors occur ,and we could not classify it into the above error types. As shown in Table 3, DATA-SPARSENESS is the most outstanding problem. Therefore, we can believe that the system will achieve a higher performance if we obtain more corpora. 4.6 Corpus size and accuracy Finally, we investigated the relation between the corpus size (the number of training sentence pairs) and its performance (BLE"
2005.mtsummit-papers.29,J93-2003,0,0.00677215,"odel, the system searches the translation example combination which has the highest probability. The proposed model clearly formalizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental results demonstrate that the proposed model has a slightly better translation quality than stateof-the-art EBMT systems. 1 Introduction Nowadays, much attention has been given to data-driven (or corpus-based) machine translation, such as example-based machine translation or EBMT(Nagao, 1984) and statistical machine translation or SMT (Brown et al., 1993). This paper focuses on EBMT approach. The idea of EBMT is that translation examples similar to a part of an input sentence are retrieved and combined to produce a translation. EBMT basically prefers larger translation examples, because the larger the translation is, the wider context is taken into account. So, most EBMT systems retrieve large examples as possible as they can, and the retrieving is based on some heuristic criterion/measures which prefer larger examples. On the other hand, SMT approach basically breaks down translation examples into small word/phrases in order to calculate tran"
2005.mtsummit-papers.29,A00-2018,0,0.013935,"able, i.e., P (play |kakeru) = 23 , P (set |kakeru) = 1 3. Thus, a translation example with the similar context can naturally get a higher translation probability. 3 Algorithm The Algorithm of proposed method consists of the following two modules: (1) an alignment module, which builds translation example from corpus, and (2) a translation module, which generates a translation. Alignment module Step 1: Conversion into phrasal dependency structures saurus(Ikehara et al., 1997). 222 First, sentence pairs are parsed by the Japanese parser KNP (Kurohashi and Nagao, 1994) and the English nl-parser (Charniak, 2000). The Japanese parser outputs a dependency structure, and we use it as is. The English parser outputs a phrase structure. Then, it is converted into a dependency structure by rules which decide on a head word in a phrase. A Japanese phrase unit consists of sequential content words and their following function words. An English phrase unit is a base-NP or a base-VP. Step 2: Alignment based on translation dictionary Then, correspondences are estimated by using translation dictionaries (Aramaki et al., 2001). We used four dictionaries: EDR, EDICT, ENAMDICT, and EIJIRO. These dictionaries have abo"
2005.mtsummit-papers.29,C94-1015,0,0.0476541,"HERS the fact that, as mentioned before, the system will achieve a higher performance with larger corpora. 㪇㪅㪋㪌 㪇㪅㪋 5 Related Work 㪇㪅㪊㪌 㪹㫃㪼㫌 㪇㪅㪊 㪇㪅㪉㪌 㪇㪅㪉 㪇㪅㪈㪌 㪇㪅㪈 㪇㪅㪇㪌 㪇 㪇 㪌㪇㪇㪇 㪈㪇㪇㪇㪇 㪈㪌㪇㪇㪇 㪉㪇㪇㪇㪇 㪺㫆㫉㫇㫌㫊㩷㫊㫀㫑㪼 㪧㪩㪦㪧㪦㪪㪜㪛 Figure 6: (BLEU). 㪙㪘㪪㪜㪣㪠㪥㪜 Corpus Size and Performance To our knowledge, there has been no work realizing EBMT based on the translation probability, and previous EBMT systems handle their translation examples using heuristic measures/criterion. For instance, MSR-MT (Richardson et al., 2001) retrieves translation examples by using only the example size. HPAT(Imamura, 2002) and TDMT(Furuse and Iida, 1994) are EBMT systems based on size and context similarity. UTOKYOMT(Aramaki et al., 2003) used alignment conﬁdence in addition to these metrics. Such a combination of multiple metrics leads to a problem of how to estimate the weight of each metric. 6 OTHERS: OTHERS is a case that multiple errors occur ,and we could not classify it into the above error types. As shown in Table 3, DATA-SPARSENESS is the most outstanding problem. Therefore, we can believe that the system will achieve a higher performance if we obtain more corpora. 4.6 Corpus size and accuracy Finally, we investigated the relation be"
2005.mtsummit-papers.29,2002.tmi-papers.9,0,0.0180469,"D-ORDER SELECTION-ERR OTHERS the fact that, as mentioned before, the system will achieve a higher performance with larger corpora. 㪇㪅㪋㪌 㪇㪅㪋 5 Related Work 㪇㪅㪊㪌 㪹㫃㪼㫌 㪇㪅㪊 㪇㪅㪉㪌 㪇㪅㪉 㪇㪅㪈㪌 㪇㪅㪈 㪇㪅㪇㪌 㪇 㪇 㪌㪇㪇㪇 㪈㪇㪇㪇㪇 㪈㪌㪇㪇㪇 㪉㪇㪇㪇㪇 㪺㫆㫉㫇㫌㫊㩷㫊㫀㫑㪼 㪧㪩㪦㪧㪦㪪㪜㪛 Figure 6: (BLEU). 㪙㪘㪪㪜㪣㪠㪥㪜 Corpus Size and Performance To our knowledge, there has been no work realizing EBMT based on the translation probability, and previous EBMT systems handle their translation examples using heuristic measures/criterion. For instance, MSR-MT (Richardson et al., 2001) retrieves translation examples by using only the example size. HPAT(Imamura, 2002) and TDMT(Furuse and Iida, 1994) are EBMT systems based on size and context similarity. UTOKYOMT(Aramaki et al., 2003) used alignment conﬁdence in addition to these metrics. Such a combination of multiple metrics leads to a problem of how to estimate the weight of each metric. 6 OTHERS: OTHERS is a case that multiple errors occur ,and we could not classify it into the above error types. As shown in Table 3, DATA-SPARSENESS is the most outstanding problem. Therefore, we can believe that the system will achieve a higher performance if we obtain more corpora. 4.6 Corpus size and accuracy Finally,"
2005.mtsummit-papers.29,J94-4001,1,0.691537,"only three, but its target translation becomes more stable, i.e., P (play |kakeru) = 23 , P (set |kakeru) = 1 3. Thus, a translation example with the similar context can naturally get a higher translation probability. 3 Algorithm The Algorithm of proposed method consists of the following two modules: (1) an alignment module, which builds translation example from corpus, and (2) a translation module, which generates a translation. Alignment module Step 1: Conversion into phrasal dependency structures saurus(Ikehara et al., 1997). 222 First, sentence pairs are parsed by the Japanese parser KNP (Kurohashi and Nagao, 1994) and the English nl-parser (Charniak, 2000). The Japanese parser outputs a dependency structure, and we use it as is. The English parser outputs a phrase structure. Then, it is converted into a dependency structure by rules which decide on a head word in a phrase. A Japanese phrase unit consists of sequential content words and their following function words. An English phrase unit is a base-NP or a base-VP. Step 2: Alignment based on translation dictionary Then, correspondences are estimated by using translation dictionaries (Aramaki et al., 2001). We used four dictionaries: EDR, EDICT, ENAMDI"
2005.mtsummit-papers.29,niessen-etal-2000-evaluation,0,0.0324007,"ems under rule base approach. PER 4.2 Evaluation Evaluation is conducted based on the following conditions and by using the ﬁve evaluation metrics in Table 1. (1) (2) (3) (4) case insensitive (lower case only) no punctuation marks (.,?!”) no hyphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Table 3. 4.3 Results The result is shown in Table 2. Because proposed accuracy is slightly higher than basic, the result demonstrates validity of the proposed translation model. 4.4 Contribution of context similarity We inv"
2005.mtsummit-papers.29,W99-0604,0,0.105445,"input sentence are retrieved and combined to produce a translation. EBMT basically prefers larger translation examples, because the larger the translation is, the wider context is taken into account. So, most EBMT systems retrieve large examples as possible as they can, and the retrieving is based on some heuristic criterion/measures which prefer larger examples. On the other hand, SMT approach basically breaks down translation examples into small word/phrases in order to calculate translation probability reliably. Of course, recent SMT studies incorporate larger phrase unit, for example, Och(Och et al., 1999) used alignment template to handle phrase chunks. However, SMT translation unit is smaller than EBMT, which has no limitation in its unit size. 219 Simply speaking, EBMT and SMT have two diﬀerences: 1. EBMT pays more attention to the size; SMT to the frequency. 2. EBMT relies rion/measures; formalized. on heuristic criteSMT is statistically For the formalization of EBMT, this paper proposes a probabilistic translation model, which deals not only with the example size but with the context similarity. In the experiments, the proposed model has a slightly better translation quality than stateof-t"
2005.mtsummit-papers.29,W01-1408,0,0.0234891,"nditions and by using the ﬁve evaluation metrics in Table 1. (1) (2) (3) (4) case insensitive (lower case only) no punctuation marks (.,?!”) no hyphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Table 3. 4.3 Results The result is shown in Table 2. Because proposed accuracy is slightly higher than basic, the result demonstrates validity of the proposed translation model. 4.4 Contribution of context similarity We investigated the contribution of context similarity to the translation performance. This is condu"
2005.mtsummit-papers.29,P02-1040,0,0.0788812,"ed. BLEU NIST baseline: EBMT baseline, which searches the most similar translation examples by using a character-based DP matching method, and outputs its target parts as is. WER C1, C2: Commercial machine translation systems under rule base approach. PER 4.2 Evaluation Evaluation is conducted based on the following conditions and by using the ﬁve evaluation metrics in Table 1. (1) (2) (3) (4) case insensitive (lower case only) no punctuation marks (.,?!”) no hyphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Ta"
2005.mtsummit-papers.29,W01-1402,0,0.174752,"Missing"
2005.mtsummit-papers.29,2003.mtsummit-papers.51,0,0.0610725,"yphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Table 3. 4.3 Results The result is shown in Table 2. Because proposed accuracy is slightly higher than basic, the result demonstrates validity of the proposed translation model. 4.4 Contribution of context similarity We investigated the contribution of context similarity to the translation performance. This is conducted by performance comparison between the proposed system and without sim, which does not use the thesaurus (Table 2). As shown in Table 2, propose"
2006.iwslt-evaluation.9,P93-1004,0,0.0526775,"parsing might cause sideeffects and lower translation performance. As we mentioned above, parsing errors are not a principal cause of translation errors, but these are not a few. One of the possible countermeasures is to reconsider the learning process of an English parser. The English parser used here is learned from the penn Treebank, and seems to be vulnerable to conversational sentences in travel domain. Furthermore, it is quite possible to improve parsing accuracies of both languages complementarily by taking advantage of the difference of syntactic ambiguities between the two languages [8]. This approach may not substantially improve the parsing accuracy of the travel domain sentences, because of their short length, but is promising for translating longer general sentences. Other main points are as follows: ࡎ࠹࡞1. 0 ߦ &lt;hotel> ߦ 0.99 ৻⇟ (best) 1. 0 &lt;most>0.99 &lt;nearest> ㄭ (near ) 1. 0 0.99 㚞 (station) 1. 0 ߪ ߤߎ (where) 1. 0 ߢߔ ߆ Figure 3: Example of SYNGRAPH. Table 1: JP to EN Evaluation results. Dev 1 Dev 2 Dev 3 Dev 4 Dev 4 ASR Test Test ASR BLEU 0.5087 0.4881 0.4468 0.1921 0.1590 0.1655 0.1418 NIST 9.6803 9.4918 9.1883 5.7880 5.0107 5.4325 4.8804 • Punctuation marks are remove"
2006.iwslt-evaluation.9,E06-1031,0,0.0225558,"des are basic nodes and the other nodes are SYN nodes 2 . Then, if the expression conjoining two or more nodes corresponds to one synonymous group, a SYN node will be added there. For example, in Figure 3,  nearest  is such a SYN node. Furthermore, if one SYN node has a hyper synonymous group in the synonymy database, the SYN node with the hyper SYNID is also added. In this SYNGRAPH data structure, each node has a score, NS (Node Score), which reflects how much the expression of the node is shifted from the original expression. • Automatic evaluation methods are a little advantageous to SMT [9],[10]. • The soundness of dictionaries heavily affects on the accuracy of alignment. • The extension rules of remaining nodes should be revised. • The constraint of selecting translation examples should be more robust. It is currently impossible to use ’almost equal’ exanples to the input sentence, such as those that differ perhaps only with respect to whether or not it contains a negation adverb such as ’not’. 5. Results and Discussion 6. Related Work Our Japanese-English translation system tried two tasks: manual manuscript translation and ASR output translation (for ASR output we just trans"
2006.iwslt-evaluation.9,E06-1032,0,0.0178145,"are basic nodes and the other nodes are SYN nodes 2 . Then, if the expression conjoining two or more nodes corresponds to one synonymous group, a SYN node will be added there. For example, in Figure 3,  nearest  is such a SYN node. Furthermore, if one SYN node has a hyper synonymous group in the synonymy database, the SYN node with the hyper SYNID is also added. In this SYNGRAPH data structure, each node has a score, NS (Node Score), which reflects how much the expression of the node is shifted from the original expression. • Automatic evaluation methods are a little advantageous to SMT [9],[10]. • The soundness of dictionaries heavily affects on the accuracy of alignment. • The extension rules of remaining nodes should be revised. • The constraint of selecting translation examples should be more robust. It is currently impossible to use ’almost equal’ exanples to the input sentence, such as those that differ perhaps only with respect to whether or not it contains a negation adverb such as ’not’. 5. Results and Discussion 6. Related Work Our Japanese-English translation system tried two tasks: manual manuscript translation and ASR output translation (for ASR output we just translated"
2006.iwslt-evaluation.9,W01-1402,0,0.157807,"Missing"
2006.iwslt-evaluation.9,W01-1406,0,0.0234825,"Missing"
2006.iwslt-evaluation.9,W99-0604,0,0.0355217,"tant common feature between SMT and EBMT is to use a bilingual corpus, or translation examples, for the translation of new inputs. Both methods exploit translation knowledge implicitly embedded in translation examples, and make MT system maintenance and improvement much easier compared with Rule-Based Machine Translation. On the other hand, EBMT is different from SMT in that SMT hesitates to exploit rich linguistic resources such as a bilingual lexicon and parsers; EBMT does not consider such a constraint. SMT basically combines words or phrases (relatively small pieces) with high probability [2]; EBMT tries to use larger translation examples. When EBMT tries to use larger examples, it can better handle examples which are discontinuous as a word-string, but continuous structurally. Accordingly, though it is not inevitable, EBMT can quite naturally handle syntactic information. Besides that, the difference in the problem setting between EBMT and SMT is also important. SMT is a natural approach when linguistic resources such as parsers and a bilingual lexicon are not available. On the other hand, in case that such linguistic resources are available, it is also natural to see how accurat"
2006.iwslt-evaluation.9,J94-4001,1,0.562861,"ching, we need to recognize the synonymous relations among this expressive divergence. However, the combination of synonymous expressions will cause combinatorial explosion, which makes both pre-unfolding and dynamic search are infeasible. To handle this problem, we introduce SYNGRAPH data structure which packs all synonymous expressions and can generate all the possible paraphrase sentences. Figure 3 shows an example SYNGRAPH which can generate the above paraphrases. The basis of SYNGRAPH is the dependency structure of the original sentence (so, in this paper we always employ a robust parser [3]). In the dependency structure, each 2. Extracted synonymous expressions are effectively handled by SYNGRAPH data structure, which can pack expressive divergence. A thesaurus is a knowledge source to provide synonym and hypernym-hyponym relations. However, existing thesauri are not appropriate for flexible matching. One reason is that the number of words assigned to one unit is often too large, and it is difficult to distinguish synonyms in a narrow sense from similar words. Such distinction is important for 68 there were variety of problems, such as parsing errors of both languages, excess an"
2006.iwslt-evaluation.9,A00-2018,0,0.061125,"Missing"
2006.iwslt-evaluation.9,P06-1043,0,0.0425398,"Missing"
2006.iwslt-evaluation.9,2005.mtsummit-papers.29,1,0.740268,"t) the intersection” is attached on “家に ”, which means “house” is replaced with “the intersection”. On the other hand, a parent bond tells that the translation 3.4. Handling of Numerals Numerals in Japanese are translated into English in several ways. • cardinal : 124 → one hundred twenty four • ordinal (e.g., day) : 2 日 → second • two-figure (e.g., room number, year) : 124 → one twenty four • one-figure (e.g., flight number, phone number) : 124 便 → one two four • non-numeral (e.g., month) : 8 月 → August 1 We proposed a method of selecting translation examples based on translation probability [6]. Though we used size- and similarity-based criteria for IWSLT06 because of time constraints, we are planning to use probabilitybased criteria from now on. At the time of parallel sentence alignment, it is checked in which type Japanese numerals are translated. 67 Translation examples of non-numeral type are used only if the numerals match exactly (“8 月 → August” cannot be used to translate “7 月”). However, translation examples of the other types can be used by generalizing numerals, and the input numeral is transformed according to the type. For example, “2 日 → second” can be used to translat"
2006.iwslt-evaluation.9,2001.mtsummit-ebmt.4,0,\N,Missing
2006.iwslt-evaluation.9,P06-1085,0,\N,Missing
2007.mtsummit-papers.45,P05-1022,0,0.01835,"achieved by the following steps, using a Japanese parser, an English parser, and a bilingual dictionary. 2.1 Dependency Analysis of Sentences Japanese sentences are converted into dependency structures using the morphological analyzer, JUMAN (Kurohashi et al., 1994), and the dependency analyzer, KNP (Kurohashi and Nagao, 1994). Japanese dependency structure consists of nodes which correspond to content words. Function words such as post-positions, affixes, and auxiliary verbs are included in the nodes. For English sentences, Charniak’s nlparser is used to convert them into phrase structures (Charniak and Johnson, 2005), and then they are transformed into dependency structures by rules defining head words for phrases. In the same way as Japanese, each node in this dependency tree consists of a content word and related function words. Figure 1 shows an example of tree structure. The root of a tree is placed at the extreme left and phrases are placed from top to bottom. ᣣᧄ 䈪 2.2 Detection of Word/Phrase Correspondence Candidates 新宿 → Shinjuku ↔ Shinjuku (similarity:1.0) ローズワイン → rosuwain ↔ rose wine (similarity:0.78) In Figure 1, the correspondence candidates “ 日本 (Japan) ↔ Japan”, “請求 (claim) ↔ claim”, “申し立て"
2007.mtsummit-papers.45,P03-1011,0,0.0315403,"ost meaningful elements of a sentence. With this structure, they proposed a “best-first” alignment method. This method starts from the nodes with the tightest lexical correspondence and then goes to close nodes from the first node. (Groves et al., 2004) used parsed tree structure of an original sentence, and then aligned the trees with some heuristic rules that constrain the order of alignment. Although these structural methods utilize profound knowledge of NLP and achieve high accuracy, the manner of alignment is still heuristic, which is often not in general purpose. To resolve this issue, (Gildea, 2003) proposed a probabilistic tree-based alignment between Korean and English. They use some cloning operations to calculate the probability, so they make the structure more complicated. Moreover, it is not apparent that the same operations are effective and suitable for different language pairs. In this paper, we propose an alignment method applying dependency type distance and distance score function into the structural alignment. Our motivation is to measure the alignment consistency based on distance, which is not only keeping the simple sentence structure but also language independent. Experi"
2007.mtsummit-papers.45,J94-4001,1,0.58696,"and model 2. We performed some experiments to evaluate our proposal, and it is reported in Section 4. At last, we give a short conclusion and introduce our future work. 2 Procedure of Structural Phrase Alignment Our machine translation system works mainly for JapaneseEnglish, and the alignment is achieved by the following steps, using a Japanese parser, an English parser, and a bilingual dictionary. 2.1 Dependency Analysis of Sentences Japanese sentences are converted into dependency structures using the morphological analyzer, JUMAN (Kurohashi et al., 1994), and the dependency analyzer, KNP (Kurohashi and Nagao, 1994). Japanese dependency structure consists of nodes which correspond to content words. Function words such as post-positions, affixes, and auxiliary verbs are included in the nodes. For English sentences, Charniak’s nlparser is used to convert them into phrase structures (Charniak and Johnson, 2005), and then they are transformed into dependency structures by rules defining head words for phrases. In the same way as Japanese, each node in this dependency tree consists of a content word and related function words. Figure 1 shows an example of tree structure. The root of a tree is placed at the ex"
2007.mtsummit-papers.45,W01-1406,0,0.0144398,"word but a larger block which is usually a multiple word or a phrase. However, even if these methods are oriented to use larger block or structure, data sparseness is still a big problem on its way. For this reason, it is not easy to achieve high performance for the language pair whose linguistic structure is quite different from each other. While, by using heuristic rules in alignment procedure, structural methods can easily use NLP resources, such as a morphological analyzer and a syntactic analyzer, to grasp characteristics of language pairs with large difference in linguistic structure. (Menezes and Richardson, 2001) proposed a kind of tree structure called “Logical Form”, which is a disordered graph representing the relations among the most meaningful elements of a sentence. With this structure, they proposed a “best-first” alignment method. This method starts from the nodes with the tightest lexical correspondence and then goes to close nodes from the first node. (Groves et al., 2004) used parsed tree structure of an original sentence, and then aligned the trees with some heuristic rules that constrain the order of alignment. Although these structural methods utilize profound knowledge of NLP and achiev"
2007.mtsummit-papers.45,J03-1002,0,0.0109488,"simple statistical method (GIZA++), and 3.0 points over a baseline system. We also conducted a translation experiment and achieved a BLEU score improvement of 0.4 points over a baseline system. 1 Introduction In machine translation task, how to align the training parallel corpus with high accuracy is a big problem, and thus a number of studies have been done. The alignment methods can be categorized into two groups: one is probabilistic method and the other is heuristic method with structural information. Probabilistic methods are mainly used in Statistical Machine Translation (SMT) systems (Och and Ney, 2003a). The main issue is how to decompose the alignment probabilities P r(A|S, T) reasonably to make good use of some approximations. The simplest statistical method is based on word level alignment, in which the IBM Model (Brown et al., 1993) is mostly used as the baseline method. Recently, more sophisticated methods have been proposed by (Watanabe et al., 2002) and (Zhang and Vogel, 2005), which handle not only a word but a larger block which is usually a multiple word or a phrase. However, even if these methods are oriented to use larger block or structure, data sparseness is still a big probl"
2007.mtsummit-papers.45,W04-2208,0,0.149941,". At the same time, the conflicting correspondence “ 申し 立 て (allegation) ↔ claim” is rejected. After that, the correspondence “ 請求 (claim) ↔ claim” is unambiguous, so it is adopted. 3.4 Proposed Model Here we would like to refine the heuristic definition of distance and distance-score function. We proposed two models. 3.3 Baseline Method Proposed Model 1 First, we refine distance-score function in order to reject unambiguous and incorrect correspondences. Here, the definition of dS (ai , aj ) and dT (ai , aj ) is the same as the baseline model. Using the gold standard alignment data from NICT(Uchimoto et al., 2004), which includes about 40,000 sentence pairs, we learned the frequency distribution of distance pair. Figure 4 shows the result of automatic learning form gold standard data. Based on the observation of gold standard data, we design f (dS , dT ) as follows: Criteria 1: f (dS , dT ) is positive if both dS and dT are small, which means the relation between the two correspondences is appropriate; Criteria 2: f (dS , dT ) is 0 if both dS and dT are large, for the relation is not so important if they are far from each other; Criteria 3: f (dS , dT ) is negative is dS is large but dT is small, or dT"
2007.mtsummit-papers.45,P03-1010,0,0.0330636,"s difference is that Japanese sentences consist of SOV word order, but English word order is SVO. For such language pair as Japanese and English, deeper sentence analysis using NLP resources is necessary, like our method. Sample alignments are shown in Figure 12 and Figure 13. Wrongly aligned parts in the baseline method are modified in the model 2. 4.2 Translation Experiment We also conducted translation experiment. For this purpose, we utilized around 218K parallel sentences for trainFigure 12: Sample Alignment 1. ing, and 500 sentences for testing. All the sentences are on newspaper domain(Utiyama and Isahara, 2003). The translation results are summarized in Table 2 and Table 3. Results were evaluated by n-gram precision based metrics, BLEU and NIST, with only one reference. We show 3, 4, 5-gram evaluation results in the tables. From the result, it is able to be said that the translation quality is improved by our proposed method. The improvement of alignment accuracy leads to the improvement of the quality of translation examples used in translation step. Sample translations are shown in Table 4. The numerals following the method name represent the 4-gram BLEU score of the output. 5 Conclusion We have p"
2007.mtsummit-papers.45,2002.tmi-papers.20,0,0.0171049,"done. The alignment methods can be categorized into two groups: one is probabilistic method and the other is heuristic method with structural information. Probabilistic methods are mainly used in Statistical Machine Translation (SMT) systems (Och and Ney, 2003a). The main issue is how to decompose the alignment probabilities P r(A|S, T) reasonably to make good use of some approximations. The simplest statistical method is based on word level alignment, in which the IBM Model (Brown et al., 1993) is mostly used as the baseline method. Recently, more sophisticated methods have been proposed by (Watanabe et al., 2002) and (Zhang and Vogel, 2005), which handle not only a word but a larger block which is usually a multiple word or a phrase. However, even if these methods are oriented to use larger block or structure, data sparseness is still a big problem on its way. For this reason, it is not easy to achieve high performance for the language pair whose linguistic structure is quite different from each other. While, by using heuristic rules in alignment procedure, structural methods can easily use NLP resources, such as a morphological analyzer and a syntactic analyzer, to grasp characteristics of language p"
2007.mtsummit-papers.45,2005.eamt-1.39,0,0.0223359,"can be categorized into two groups: one is probabilistic method and the other is heuristic method with structural information. Probabilistic methods are mainly used in Statistical Machine Translation (SMT) systems (Och and Ney, 2003a). The main issue is how to decompose the alignment probabilities P r(A|S, T) reasonably to make good use of some approximations. The simplest statistical method is based on word level alignment, in which the IBM Model (Brown et al., 1993) is mostly used as the baseline method. Recently, more sophisticated methods have been proposed by (Watanabe et al., 2002) and (Zhang and Vogel, 2005), which handle not only a word but a larger block which is usually a multiple word or a phrase. However, even if these methods are oriented to use larger block or structure, data sparseness is still a big problem on its way. For this reason, it is not easy to achieve high performance for the language pair whose linguistic structure is quite different from each other. While, by using heuristic rules in alignment procedure, structural methods can easily use NLP resources, such as a morphological analyzer and a syntactic analyzer, to grasp characteristics of language pairs with large difference i"
2008.amta-papers.15,P05-1022,0,0.011735,"2.1 Dependency Analysis of Sentences Since our model utilizes dependency tree structures, both source and target sentences are parsed at first. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kurohashi and Nagao, 1994). Japanese dependency structure consists of nodes which correspond to content words. Function words such as post-positions, affixes, and auxiliary verbs are included in the nodes. For English sentences, Charniak’s nlparser is used to convert them into phrase structures (Charniak and Johnson, 2005), and then they are transformed into dependency structures by handmade rules defining head words for phrases. As is the case with Japanese, each node in this dependency tree consists of a content word and related function words1 . We define function words as the words with tags of “IN”(preposition or subordinating conjunction), “TO”, “MD”(modal), “CC”(coordinating conjunction) by nlparser. 1 There would be some special cases that a phrase has no content word. See a phrase “and” in figure 1. Also, there would be a phrase which has more than one content words. 164 In IBM models (Brown et al., 19"
2008.amta-papers.15,P03-1012,0,0.162161,"about alignment quality (Fraser and Marcu, 2007), if the gold standard data only contains sure alignments, precision is much important than recall. Modified F-measure calculated by the equation 10 shows higher correlation to translation quality if α is set to be more than 0.5 (means emphasizing on precision). Modified F-measure = 1 α Precision + (1−α) Recall (10) However, our results disagree with this argument. There seems no relation between precision and translation quality, indeed, F-measure has good correlation with BLEU score. 7 Related Work Our proposed model is similar to the work of Cherry and Lin (2003). They use dependency tree structure for source side and construct a probabilistic model. 169 Figure 5: Result of phrase-base alignment. The differences between their model and ours are following. • Unit of alignment: Their model aligns words to words. Ours aligns syntactic phrases to phrases. • Parser: They use a parser for source side only, and reproduce the target side dependency structure introducing a cohesion constraint. We use parsers for both source and target side, and do not require any constraints. • Alignment constraint: They can only make one-to-one links. We can make even many-to"
2008.amta-papers.15,P05-1033,0,0.0521376,"and achieve reasonably high quality of alignment compared with wordbased alignment model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing"
2008.amta-papers.15,W06-1628,0,0.0878667,"Missing"
2008.amta-papers.15,J07-3002,0,0.0191839,"ulin 䕔  جand 䕔 decreases 䕔 ؒ  NEFA 䕔  ؤin concentra!on 䕔  ؤin the blood 䕔 䕔 䕔 䕔 䕔 䕔䕔 ج ؒؤ ؒ ؤ ج ؒؤ ؒ ؤ ؒ ؤ ؤ ؤ Figure 4: Result of word-base alignment (grow-diagfinal-and). is more prominence in case only small amount of parallel corpus can be used. What is more, most of these works are done on similar language pairs, such as English v.s. Chinese, French. We are now investigating the contribution of alignment quality improvement to translation quality in JapaneseEnglish, language pair whose structure is very different. According to the study about alignment quality (Fraser and Marcu, 2007), if the gold standard data only contains sure alignments, precision is much important than recall. Modified F-measure calculated by the equation 10 shows higher correlation to translation quality if α is set to be more than 0.5 (means emphasizing on precision). Modified F-measure = 1 α Precision + (1−α) Recall (10) However, our results disagree with this argument. There seems no relation between precision and translation quality, indeed, F-measure has good correlation with BLEU score. 7 Related Work Our proposed model is similar to the work of Cherry and Lin (2003). They use dependency tree s"
2008.amta-papers.15,P06-1121,0,0.0296798,"t experiments on a JapaneseEnglish corpus, and achieve reasonably high quality of alignment compared with wordbased alignment model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knigh"
2008.amta-papers.15,P03-1011,0,0.174341,"ct phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes, Gildea cloned the subtrees to deal with the problem. Our method proposed in this paper does not require any operations for controlling tree structures, just align phrase-to-phrase on dependency structure. Though our model is more simple than well-known IBM Model3 or greater, our model can achieve high accuracy of alignment an"
2008.amta-papers.15,N03-1017,0,0.0614156,"Missing"
2008.amta-papers.15,P07-2045,0,0.00534894,") for Japanese sentences and created alignments using freely available word alignment tool GIZA++ (Och and Ney, 2003). We conducted word alignment bidirectionally with its default parameters and merged them using seven types of symmetrization heuristics (Koehn et al., 2003) shown in table 2. Training are run on original forms of words for both proposed model and GIZA++. For translation evaluation, we use 500 paper abstract sentences which are parts of JST corpus. Note that test sentences are not included in training corpus. As a decoder, we used state-of-the-art phrasebased SMT toolkit Moses (Koehn et al., 2007) with its default options except for phrase table limit (20 → 10) and distortion limit (6 → -1 means infinite). Evaluation was done with all the punctuations being deleted and case-insensitively. The BLEU scores of each alignment methods are shown in table 2, in the last column. Actually, it is hard to integrate proposed alignment results into Moses decoder because our model is based on “linguistic phrase”. If we align all words to all words in a corresponding two phrases, Moses would fail to translate a content word with different function words from the learned phrase pair. To avoid this pro"
2008.amta-papers.15,J94-4001,1,0.297584,"ponds to a linguistic phrase. Underlined words are handled as function words, others are content words. Our model uses the linguistic phrase as an unit of alignment rather than a word. The root of a tree is placed at the extreme left and phrases are placed from top to bottom. 2.2 Tree-based Model 2.1 Dependency Analysis of Sentences Since our model utilizes dependency tree structures, both source and target sentences are parsed at first. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kurohashi and Nagao, 1994). Japanese dependency structure consists of nodes which correspond to content words. Function words such as post-positions, affixes, and auxiliary verbs are included in the nodes. For English sentences, Charniak’s nlparser is used to convert them into phrase structures (Charniak and Johnson, 2005), and then they are transformed into dependency structures by handmade rules defining head words for phrases. As is the case with Japanese, each node in this dependency tree consists of a content word and related function words1 . We define function words as the words with tags of “IN”(preposition or"
2008.amta-papers.15,2006.amta-papers.11,0,0.0183626,"e BLEU score, BLEU score of our proposed model is worse than that of word-base models. One reason of this is that, as mentioned above, the infelicity of integrating our alignment results into Moses decoder. Another reason is that BLEU is essentially insensitive to syntactic structure. The translation result may indeed better from the point of dependency structure. We need to try parsing base line output and the output of the realigned system and see if the parsing results improve. Some of recent studies suggest that there is less relationship between alignment quality and translation results (Lopez and Resnik, 2006; Ayan and Dorr, 2006). Even if the contribution to translation quality is small, there is no doubt that better alignment quality leads to better translation, which [8th AMTA conference, Hawaii, 21-25 October 2008] Propylene 䕔 glycol 䕔 increases 䕔 in 䕔䕔 blood 䕔 glucose 䕔 and insulin 䕔 and decreases in NEFA concentra!on in the blood 䕔  Propylene 䕔  glycol 䕔  جincreases 䕔 ؒ ؒ  blood 䕔 ؒ ؒ  glucose and 䕔 ؒ  ؤin insulin 䕔  جand 䕔 decreases 䕔 ؒ  NEFA 䕔  ؤin concentra!on 䕔  ؤin the blood 䕔 䕔 䕔 䕔 䕔 䕔䕔 ج ؒؤ ؒ ؤ ج ؒؤ ؒ ؤ ؒ ؤ ؤ ؤ Figure 4: Result of word-base a"
2008.amta-papers.15,W01-1406,0,0.0414531,"ent model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes, Gildea cloned the subtrees to deal with the problem. Our method proposed in this paper do"
2008.amta-papers.15,J03-1002,0,0.0157923,"and Results 2 3 http://www.jst.go.jp/ http://www.nict.go.jp/ 167 ؒؤ ؒ ؤ ؒ ؤ ؤ ج ؤ ج ؤ We conducted both alignment and translation evaluation experiments. JST 2 Japanese-English paper abstract corpus consists of 1M parallel sentences were used for model training. This corpus was constructed from 2M Japanese-English paper abstract corpus belongs to JST by NICT 3 using the method of Uchiyama and Isahara (2007). We made gold-standard alignment for 100 sentence pairs among the 1M parallel sentences by hand. The annotations are only sure (S) alignments (no possible (P ) alignments (Och and Ney, 2003)). The unit of evaluation was morpheme-base for Japanese and word-base for English. We used precision, recall, and F-measure as evaluation criterion. The evaluation results are shown in table 2. We used “3-best-grow” and “5-best-grow” 5 2  invesgaon 10 ؒ ؤof the hint 10 ؒ  ؤfor challenge 10 ؒ  ؤfor future was made 5 3 5  ؤthrough review 1 5  ؤof the history 7 3 ؒ technology and 10 ؒ جtypical 5 9  ؤof invenons 7 ؒ tweneth  ؤin the century 7 1 Figure 3: Example of growing the alignment points. [8th AMTA conference, Hawaii, 21-25 October 2008] symmetrization heuristic"
2008.amta-papers.15,P05-1034,0,0.0489519,"alignment. We conduct experiments on a JapaneseEnglish corpus, and achieve reasonably high quality of alignment compared with wordbased alignment model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too ri"
2008.amta-papers.15,2007.mtsummit-papers.63,0,0.186669,"Missing"
2008.amta-papers.15,C00-2131,1,0.883089,"pared with wordbased alignment model. 1 Introduction Most of statistical machine translation (SMT) systems are based on “word-based” alignment method starting with IBM models (Brown et al., 1993). Based on the word alignment results, some enhanced and successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes, Gildea cloned the subtrees to deal with the problem. Ou"
2008.amta-papers.15,P01-1067,0,0.135001,"successful models which extract phrases have been proposed and established the state-of-theart Phrase-Based SMT models (Koehn et al., 2003). Another approaches incorporate syntactic information by parsing source or target sentences (Quirk et al., 2005; Galley et al., 2006; Cowan et al., 2006). Chiang (2005) proposed hierarchical phrase-based 163 Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities, not considering the consistency between two dependency structure as a whole. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees of one side to reproduce the other side. The constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes, Gildea cloned the subtrees to deal with the problem. Our method proposed in this paper does not require any operations for controlling tree structures, just align phrase-to-phrase on dependency structure. Though our model is more simple than well-known IBM Model3 or greater, our model can achieve high accura"
2008.amta-papers.15,J93-2003,0,\N,Missing
2008.amta-papers.15,2001.mtsummit-ebmt.4,0,\N,Missing
2008.amta-papers.15,P06-1002,0,\N,Missing
2011.mtsummit-papers.53,I08-1012,0,0.0164577,". We set the weight w to 6000 for both Word and Phrase Full Match, 3000 for both Word and Phrase Part Match and 1 for both Word and Phrase None Match. These weights showed the best performance in the preliminary experiments for tuning the weights. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM models. We conducted word alignment bidirectionally with its default parameters and merged them using grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Also, we used BerkelyAligner7 (DeNero and Klein, 2007) with its default settings for unsupervised training. Experimental results are shown in Table 8. Common Chinese characters information is detected by our proposed detecting method and Kanconvit. The alignment accuracy of the alignment model we used with"
2011.mtsummit-papers.53,P07-1003,0,0.022513,"JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM models. We conducted word alignment bidirectionally with its default parameters and merged them using grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Also, we used BerkelyAligner7 (DeNero and Klein, 2007) with its default settings for unsupervised training. Experimental results are shown in Table 8. Common Chinese characters information is detected by our proposed detecting method and Kanconvit. The alignment accuracy of the alignment model we used without incorporating the information is indicated as ”Baseline”, the alignment accuracy after adjusting the base distribution to reﬂect the information is indicated as ”BD”, and the alignment accuracy after exploiting the information directly into the 7 469 Settings http://code.google.com/p/berkeleyaligner/ grow-diag-ﬁnal-and BerkelyAligner Baselin"
2011.mtsummit-papers.53,D08-1033,0,0.0424857,"Missing"
2011.mtsummit-papers.53,I05-1059,0,0.254738,"rk Common Chinese characters information have been employed for a number of Japanese-Chinese related tasks. Tan et al. (1995) availed the occurrence of common Chinese characters as a feature of Japanese-Traditional Chinese sentence pair to ﬁnd a Meaning Kanji TC SC snow    country    love    begin hair      Table 1: Examples of Chinese characters (TC denotes Traditional Chinese and SC denotes Simpliﬁed Chinese). SC TC      , ,     , ,  ... ... Table 2: Hanzi converter version 3.0 standard conversion table. direct correspondence in automatic sentence alignment task. Goh et al. (2005) built a JapaneseSimpliﬁed Chinese dictionary partly using direct conversion of Japanese into Chinese for the Japanese words that all the characters in the word are made up of Kanji only, namely Kanji words. They did the conversion using a Chinese encoding converter1 which can convert Traditional Chinese into Simpliﬁed Chinese. It works because most Kanji are identical to Traditional Chinese. And for the Kanji with visual variations that cannot be automatically converted using the converter, they manually converted them by hand. In the context of machine translation, Kondrak et al. (2003) inco"
2011.mtsummit-papers.53,N06-1023,1,0.781188,"ts (Och and Ney, 2003). The unit of evaluation was word. We used precision, recall and alignment error rate (AER) as evaluation criteria. All the experiments were run on original forms of words. We set the weight w to 6000 for both Word and Phrase Full Match, 3000 for both Word and Phrase Part Match and 1 for both Word and Phrase None Match. These weights showed the best performance in the preliminary experiments for tuning the weights. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM models. We conducted word alignment bidirectionally with its default parameters and merged them using grow-diag-ﬁnal-and heuristic (Koehn et al., 2003). Also, we used BerkelyAligner7 (DeNero and Klein, 2007) with its default settings for unsupervised training. Experimental resu"
2011.mtsummit-papers.53,N03-1017,0,0.0328458,"Missing"
2011.mtsummit-papers.53,N03-2016,0,0.187254,"nt task. Goh et al. (2005) built a JapaneseSimpliﬁed Chinese dictionary partly using direct conversion of Japanese into Chinese for the Japanese words that all the characters in the word are made up of Kanji only, namely Kanji words. They did the conversion using a Chinese encoding converter1 which can convert Traditional Chinese into Simpliﬁed Chinese. It works because most Kanji are identical to Traditional Chinese. And for the Kanji with visual variations that cannot be automatically converted using the converter, they manually converted them by hand. In the context of machine translation, Kondrak et al. (2003) incorporated cognates (words or languages which have the same origin) information in European languages into the translation models of Brown et al. (1993). They arbitrarily selected a subset from the Europarl corpus as training data and extracted a list of likely cognate word pairs from the training corpus on the basis of orthographic similarity, and appended to the corpus itself in order to reinforce the co-occurrence count between cognates. The results of experiments conducted on a variety of bitexts showed that cognate identiﬁcation can improve word alignments without modifying the statist"
2011.mtsummit-papers.53,P09-1058,0,0.0307693,"Missing"
2011.mtsummit-papers.53,W02-1018,0,0.0346189,"y of JUMAN (Kurohashi et al., 1994). Table 5 gives some examples of Kana-Kanji conversion results. We only do Kana-Kanji conversion for content words because it is proved that do Kana-Kanji conversion for function words may lead to wrong alignment in the alignment experiments we did. 4 Alignment Model We used an alignment model proposed by Nakazawa and Kurohashi (2011) which is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences as a sequence of words works adequately. This does not hold for distant language pairs such as Japanese-English or Japanese-Chinese, in which word orders differ greatly. The model we used incorporates dependency relations of words into the alignment model (Nakazawa and Kurohashi, 2009) and deﬁnes the reorderings on the word dependency trees. 4.1 Generative Story Description Similar to the previous works (Marcu and Wong, 2002; DeNero et al., 2008), the model we used ﬁrst describes the generative story for the"
2011.mtsummit-papers.53,W09-2302,1,0.84403,"(2011) which is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences as a sequence of words works adequately. This does not hold for distant language pairs such as Japanese-English or Japanese-Chinese, in which word orders differ greatly. The model we used incorporates dependency relations of words into the alignment model (Nakazawa and Kurohashi, 2009) and deﬁnes the reorderings on the word dependency trees. 4.1 Generative Story Description Similar to the previous works (Marcu and Wong, 2002; DeNero et al., 2008), the model we used ﬁrst describes the generative story for the joint alignment model. 1. Generate  concepts from which subtree pairs are generated independently. 2. Combine the subtrees in each language so as to create parallel sentences. Here, subtrees are equivalent to phrases in the previous works. One subtree in a concept can be NULL, which represents an unaligned subtree. The model restricts the unaligned subtrees to be compo"
2011.mtsummit-papers.53,I11-1089,1,0.725191,"to be used. The Chinese characters in Kanji expressions are again useful as clues to ﬁnd word-to-word matchings. We can use Kana-Kanji conversion techniques to get the Kanji expressions from Kana expressions, but here, we simply consult a Japanese dictionary of JUMAN (Kurohashi et al., 1994). Table 5 gives some examples of Kana-Kanji conversion results. We only do Kana-Kanji conversion for content words because it is proved that do Kana-Kanji conversion for function words may lead to wrong alignment in the alignment experiments we did. 4 Alignment Model We used an alignment model proposed by Nakazawa and Kurohashi (2011) which is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences as a sequence of words works adequately. This does not hold for distant language pairs such as Japanese-English or Japanese-Chinese, in which word orders differ greatly. The model we used incorporates dependency relations of words into the alignment model (Nakazaw"
2011.mtsummit-papers.53,J03-1002,0,0.00494741,"shown in Table 6) • Kanconvit: Kanconvit Kanji to Hanzi conversion and Kana-Kanji conversion The results shown in Table 7 veriﬁed the effectiveness of our proposed detecting method. Also, there is complementation between Kanconvit and our proposed detecting method. 4 http://www.jst.go.jp http://www.nict.go.jp/ 6 http://kanconvit.ta2o.net/ 5 The training corpus we used is the same one we used in Subsection 5.1. As gold-standard data, we used 510 sentence pairs for Japanese-Chinese which were annotated by hand. There are two types of annotations, sure (S) alignments and possible (P) alignments (Och and Ney, 2003). The unit of evaluation was word. We used precision, recall and alignment error rate (AER) as evaluation criteria. All the experiments were run on original forms of words. We set the weight w to 6000 for both Word and Phrase Full Match, 3000 for both Word and Phrase Part Match and 1 for both Word and Phrase None Match. These weights showed the best performance in the preliminary experiments for tuning the weights. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi"
2012.eamt-1.7,I05-3025,0,0.0307599,"nd “» ‰↔» T(anesthesia)” are not identical, because “ ↔u(create)”, “4↔è(arrive)” and “‰↔T(drunk)” are common Chinese characters, “uË(found)” is converted into “ Ë(found)”, “èŠ(clinical)” is converted into “4Š(clinical)” and “»T(anesthesia)” is converted into “»‰(anesthesia)” in Step 2. In preliminary experiments, we extracted 14,359 lexicons using Strategy 1, and 18,584 lexicons using Strategy 2 from a paper abstract parallel corpus containing 680K sentence pairs. 3.2 Chinese Lexicons Incorporation Several studies showed that using a system dictionary is helpful for Chinese word segmentation (Low et al., 2005; Wang et al., 2011). Therefore, we use a corpus-based Chinese word segmentation and POS tagging tool with a system dictionary. We incorporate the extracted lexicons into the system dictionary. The extracted lexicons are not only effective for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Ja"
2012.eamt-1.7,E09-1063,0,0.316822,"Missing"
2012.eamt-1.7,I08-1033,0,0.17868,"ctive for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the POS tagset defined in the morphological analyzer JUMAN (Kurohashi et al., 1994). JUMAN adapts a POS tagset containing sub POS tags. For example, the POS tag “ ^(noun)” contains sub POS 3.3 Short Unit Transformation Bai et al. (2008) showed that adjusting Chinese word segmentation to make tokens 1-to-1 mapping as many as possible between a parallel sentences can improve alignment accuracy which is crucial for corpus-based MT. Wang et al. (2010) proposed a short unit standard for Chinese word segmentation that is more similar to the Japanese word segmentation standard, which can reduce the number of 1-to-n alignments and improve MT performance. Here, we propose a method to transform the annotated training data of Chinese segmenter into Japanese word segmentation standard using the extracted Chinese lexicons, and use the tr"
2012.eamt-1.7,C04-1081,0,0.0241624,"Missing"
2012.eamt-1.7,W08-0336,0,0.0350343,"orter unit standard. Therefore, the segmentation unit in Chinese may be longer than Japanese even for the same concept. This can increase the number of 1-to-n alignments which makes the word alignment task more difficult. Taking “founder” Introduction As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important first step in MT. Studies showed that a MT system with Chinese word segmentation outperforms the one treating each Chinese character as a single word, and the quality of Chinese word segmentation affects the MT performance (Xu et al., 2004; Chang et al., 2008). It has been found that besides segmentation accuracy, segmentation consistency and granularity of Chinese words are also important for MT (Chang et al., 2008). Moreover, optimal Chinese word segmentation for MT is dependent on the other language, therefore, a bilingual approach is necessary (Ma and Way, 2009). c 2012 European Association for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence"
2012.eamt-1.7,2011.mtsummit-papers.53,1,0.899763,"iation for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence alignment task. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese but different from Simplified Chinese (e.g. “love” in Table 1). They used Chinese encoding converter1 which can convert Traditional Chinese into Simplified Chinese, and built a Japanese-Simplified Chinese dictionary. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chinese-Japanese phrase alignment. In this paper, we focus on Simplified Chinese-Japanese MT and exploit common Chinese characters in Chinese word segmentation optimization. Table 1: Examples of common Chinese characters (TC denotes Traditional Chinese and SC denotes Simplified Chinese). in Figure 1 as an example, the Chinese segmenter recognizes it as one token, while the Japanese segmenter splits"
2012.eamt-1.7,wang-etal-2010-adapting,0,0.0166521,"s mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the POS tagset defined in the morphological analyzer JUMAN (Kurohashi et al., 1994). JUMAN adapts a POS tagset containing sub POS tags. For example, the POS tag “ ^(noun)” contains sub POS 3.3 Short Unit Transformation Bai et al. (2008) showed that adjusting Chinese word segmentation to make tokens 1-to-1 mapping as many as possible between a parallel sentences can improve alignment accuracy which is crucial for corpus-based MT. Wang et al. (2010) proposed a short unit standard for Chinese word segmentation that is more similar to the Japanese word segmentation standard, which can reduce the number of 1-to-n alignments and improve MT performance. Here, we propose a method to transform the annotated training data of Chinese segmenter into Japanese word segmentation standard using the extracted Chinese lexicons, and use the transformed data for training the Chinese segmenter. Because the extracted lexicons are derived from Japanese word segmentation results, they follow Japanese 37 从_P/ 有效性_NN /高_VA/的_DEC/ 格要素_NN /… CTB: Lexicon: Lexicon"
2012.eamt-1.7,W02-1001,0,0.0137644,"e same domain as the parallel training corpus. The statistics of the test sets are shown in Table 4. Note that all sentences in the test sets are not included in the parallel training corpus. Settings Parallel Training Corpus 4.2 The parallel training corpus we used is a paper abstract corpus provided by JST4 and NICT5 . This 4 Chinese and Japanese Segmenters For Chinese, we used a corpus-based word segmentation and POS tagging tool with a system dictionary, weights for the lexicons in the system dictionary are automatically learned from the training data using averaged structured perceptron (Collins, 2002). For Japanese, we used JUMAN (Kurohashi et al., 1994). We conducted Chinese-Japanese translation experiments to show the effectiveness of exploiting common Chinese characters in Chinese word segmentation optimization. 4.1.1 Chinese Annotated Corpus We used two types of manually annotated Chinese corpus for training the Chinese segmenter. One is NICT Chinese Treebank, which is from the same domain as the parallel training corpus and contains 9,792 sentences. Note that the annotated sentences in this corpus are not included in the parallel training corpus. The other corpus is CTB 7 (LDC2010T07)"
2012.eamt-1.7,I11-1035,0,0.118655,"esia)” are not identical, because “ ↔u(create)”, “4↔è(arrive)” and “‰↔T(drunk)” are common Chinese characters, “uË(found)” is converted into “ Ë(found)”, “èŠ(clinical)” is converted into “4Š(clinical)” and “»T(anesthesia)” is converted into “»‰(anesthesia)” in Step 2. In preliminary experiments, we extracted 14,359 lexicons using Strategy 1, and 18,584 lexicons using Strategy 2 from a paper abstract parallel corpus containing 680K sentence pairs. 3.2 Chinese Lexicons Incorporation Several studies showed that using a system dictionary is helpful for Chinese word segmentation (Low et al., 2005; Wang et al., 2011). Therefore, we use a corpus-based Chinese word segmentation and POS tagging tool with a system dictionary. We incorporate the extracted lexicons into the system dictionary. The extracted lexicons are not only effective for the unknown word problem, but also helpful to solve the word segmentation granularity problem. However, setting POS tags for the extracted lexicons is problematic. To solve this problem, we made a POS tags mapping table between Chinese and Japanese by hand. For Chinese, we use the POS tagset used in CTB which is also used in our Chinese segmenter. For Japanese, we use the P"
2012.eamt-1.7,I05-1059,0,0.173956,"und that besides segmentation accuracy, segmentation consistency and granularity of Chinese words are also important for MT (Chang et al., 2008). Moreover, optimal Chinese word segmentation for MT is dependent on the other language, therefore, a bilingual approach is necessary (Ma and Way, 2009). c 2012 European Association for Machine Translation. 35 Meaning TC SC Kanji snow ê(U+96EA) ê(U+96EA) ê(U+96EA) love (U+611B) 1(U+7231) (U+611B) begin |(U+767C) Ñ(U+53D1) z(U+767A) used the occurrence of identical common Chinese characters (e.g. “snow” in Table 1) in automatic sentence alignment task. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese but different from Simplified Chinese (e.g. “love” in Table 1). They used Chinese encoding converter1 which can convert Traditional Chinese into Simplified Chinese, and built a Japanese-Simplified Chinese dictionary. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chinese-Japanese phrase alignment. In this paper, we focus on Simplified Chinese-Ja"
2012.eamt-1.7,P07-2045,0,0.00948082,"racters, such as “L‚(praise)”, “×L(poem)” etc. Obviously, splitting “L ‚(praise)” into “L(song)” and “‚(eulogy)”, or splitting “× L(poem)” into “×(poem)” and “L(song)” is undesirable. Also, there are few consecutive tokens in the training data that can be combined to one extracted lexicon, we do not consider this pattern. 4 corpus was created by the Japanese project “Development and Research of Chinese-Japanese Natural Language Processing Technology”. The statistics of this corpora are shown in Table 3. 4.1.2 4.1.3 4.1.4 5 SMT Model We used the state-of-the-art phrase-based SMT toolkit Moses (Koehn et al., 2007) with default options, except for the distortion limit (6→20). It was tuned by MERT using another 500 development sentence pairs. 4.1.5 Test Sets We translated 5 test sets of Chinese sentences from the same domain as the parallel training corpus. The statistics of the test sets are shown in Table 4. Note that all sentences in the test sets are not included in the parallel training corpus. Settings Parallel Training Corpus 4.2 The parallel training corpus we used is a paper abstract corpus provided by JST4 and NICT5 . This 4 Chinese and Japanese Segmenters For Chinese, we used a corpus-based wo"
2012.eamt-1.7,xia-etal-2000-developing,0,0.106997,"Missing"
2012.eamt-1.7,W04-1118,0,0.085468,"Missing"
2012.eamt-1.7,W04-3230,0,0.0697192,"Missing"
2012.iwslt-evaluation.12,D11-1047,1,0.920432,"Figure 1 shows the overview of our EBMT system on Chinese-English translation. The translation example database is automatically constructed from a parallel training corpus by means of a Bayesian subtree alignment model. Note that both source and target sides of all the examples are stored in dependency tree structures. An input sentence is also parsed and transformed into a dependency structure. For all the sub-trees in the input dependency structure, matching examples are searched in the example database. This step is the most time consuming part, and we exploit a fast tree retrieval method [2]. There are many available examples for one sub-tree, and also, there are many possible sub-tree combinations. The best combination is detected by a log-linear decoding model with features described in Section 3. In the example in Figure 1, ﬁve examples are used. They are combined and produce an output dependency tree. We call nodes surrounding those of the example, “bond” nodes. The bond nodes of one example are replaced by other examples, and thus examples can be combined. We attended the IWSLT 2012 OLYMPICS task which is a Chinese-to-English text translation task. Based on the characteristi"
2012.iwslt-evaluation.12,I11-1089,1,0.840723,"equential model is prone to many such errors even for short simple sentences of a distant language pair. Even if the word order differs greatly between languages, phrase dependencies tend to hold between languages. This can be seen in Figure 2. Therefore, incorporating dependency analysis into the alignment model is useful for distant lanFigure 3: Alignment results from bi-directional GIZA++. Black boxes depict the system output, while dark (Sure) and light (Possible) gray cells denote gold-standard alignments. guage pairs. We exploit Bayesian subtree alignment model based on dependency trees [3]. This model incorporates dependency relations of words into the alignment model and deﬁne the reorderings on the word dependency trees. Figure 2 shows an example of the dependency trees for Japanese and English. 3. Tree-based Translation As a tree-based translation method, we adopt an examplebased machine translation system [1]. In this section, we brieﬂy introduce the translation procedure in our EBMT system. 3.1. Retrieval of Translation Examples The input sentence is converted into the dependency structure as in the alignment step. Then, for each sub-tree, avail97 The 9th International Wor"
2012.iwslt-evaluation.12,J05-4003,0,0.118733,"Missing"
2012.iwslt-evaluation.12,E09-1063,0,0.0208048,"ﬁer trained on the BTEC corpus does not work well on the HIT corpus because of the difference between these two corpora, thus some parallel sentences are also ﬁltered in this process. BLEU 0.1162 0.1209 0.1271 Table 1: Results of preliminary translation experiments. 4.4. Optimized Chinese Segmenter As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important ﬁrst step in machine translation. Research shows that optimal Chinese word segmentation for machine translation is dependent on the other language, therefore, a bilingual approach is necessary [6]. In this task, we adopted a Chinese segmenter optimized based on a bilingual perspective, which exploits common Chinese characters shared between Chinese and Japanese for Chinese word segmentation optimization [7]. The BLEU scores with and without Chinese segmenter optimization are given in Table 1, indicated as “Optimized” and “Baseline” respectively. Although the Chinese segmenter we used is optimized for Chinese-Japanese machine translation, it shows better translation performance compared to the Chinese segmenter without optimization. We think the reason is that the optimized segmentation"
2012.iwslt-evaluation.12,2012.eamt-1.7,1,0.576691,"e 1: Results of preliminary translation experiments. 4.4. Optimized Chinese Segmenter As there are no explicit word boundary markers in Chinese, word segmentation is considered as an important ﬁrst step in machine translation. Research shows that optimal Chinese word segmentation for machine translation is dependent on the other language, therefore, a bilingual approach is necessary [6]. In this task, we adopted a Chinese segmenter optimized based on a bilingual perspective, which exploits common Chinese characters shared between Chinese and Japanese for Chinese word segmentation optimization [7]. The BLEU scores with and without Chinese segmenter optimization are given in Table 1, indicated as “Optimized” and “Baseline” respectively. Although the Chinese segmenter we used is optimized for Chinese-Japanese machine translation, it shows better translation performance compared to the Chinese segmenter without optimization. We think the reason is that the optimized segmentation results are much more similar to English in number, which can reduce the number of 1-to-n alignments and improve the alignment accuracy. 4.5. Rule-based Decoding Constraints Translating long and complex sentences"
2012.iwslt-evaluation.12,2011.iwslt-evaluation.5,0,0.0312388,"d” and “Baseline” respectively. Although the Chinese segmenter we used is optimized for Chinese-Japanese machine translation, it shows better translation performance compared to the Chinese segmenter without optimization. We think the reason is that the optimized segmentation results are much more similar to English in number, which can reduce the number of 1-to-n alignments and improve the alignment accuracy. 4.5. Rule-based Decoding Constraints Translating long and complex sentences is a critical problem in machine translation, because it increases the computational complexity. Finch et al. [8] presented a simple yet efﬁcient method to solve this problem. They split a sentence into smaller units based on part-of-speech (POS) tags and commas, and translate the split units separately. Following their method, we also split a sentence into smaller units during decoding. Our EBMT system tends to choose large examples. Since the development data of this task also has the sub-sentence problem (described in Section 4.3), our system may use examples across punctuation boundaries which can generate translations with unnatural word order. Therefore, we split a source sentence based on comma, p"
2012.iwslt-evaluation.12,2011.iwslt-papers.6,0,0.0390629,"Missing"
2012.iwslt-evaluation.12,I08-1012,0,0.0274568,"Missing"
2012.iwslt-evaluation.12,N10-1015,0,0.069588,"Missing"
2012.iwslt-evaluation.12,federico-etal-2012-iwslt,0,\N,Missing
2012.iwslt-evaluation.12,E09-1000,0,\N,Missing
2014.amta-wptp.15,aziz-etal-2012-pet,0,0.0300338,"shiaki Nakazawa‡ Daisuke Kawahara† Sadao Kurohashi† † Graduate School of Informatics, Kyoto University, Kyoto 606-8501 ‡ Japan Science and Technology Agency, Kawaguchi-shi, Saitama 332-0012 † {kishimoto,dk,kuro}@nlp.ist.i.kyoto-u.ac.jp ‡ nakazawa@pa.jst.jp 1 Purpose and characteristics Translation has become increasingly important by virtue of globalization. To reduce the cost of translation, it is necessary to use machine translation and further to take advantage of post-editing based on the result of a machine translation for accurate information dissemination. Such post-editing (e.g., PET [Aziz et al., 2012]) can be used practically for translation between European languages, which has a high performance in statistical machine translation. However, due to the low accuracy of machine translation between languages with diﬀerent word order, such as Japanese-English and Japanese-Chinese, post-editing has not been used actively. We propose a post-editing system based on syntaxbased machine translation to deal with diﬀerent word order. For language pairs with diﬀerent word order, it is time-consuming for a translator to understand what a machine translation system did. To solve this problem, our syste"
2015.mtsummit-papers.20,chu-etal-2012-chinese,1,0.92817,"α)Scontext The specified value of α (0 ≤ α ≤1) is determined using a method described in next chapter. The character combination with the highest score is regarded as the final translation result. If there are more than 2 words that cannot be translated by Wikipedia or Wiktionary, we process the procedure in sequence (from beginning to end of the sentence) and use the translation result as context feature to the remaining unknown words. 4 Experiment 4.1 Settings Chu et al. had produced a Chinese character mapping table for Japanese (Kanji), Traditional Chinese (TC) and Simplified Chinese (SC) Chu et al. (2012). Thus, for constructing a table that contains mapping relationship between Korean Hangul and Chinese Hanzi, we need to construct rather Kanji-Hangul or Hanzi-Hangul tables and merge them. We collected HangulHanja mapping information from the web. • We acquired 1365 Hanja characters with their aligned Hangul from a freely accessible webpage4 . These characters are contained by words whose frequency is higher than 5965 times in some Sino-Korean corpus (there is no specific information about the mentioned Sino-Korean corpus). • There are also some materials that contain Hanja characters that are"
2015.mtsummit-papers.20,W00-0803,0,0.0966074,"2 Related Work There are some previous work on character conversion, both within language or between two languages (Chen and Lee (2000), Huang et al. (2004)). During character conversion, a bilingual dictionary is needed for candidate selection. For a low resource language like Korean, a bilingual dictionary containing enough data, including polysemous words, is often hard to obtain. Moreover, unlike sentence translation, character translation often ignores the context information of the input source sentence. Chinese character knowledge is widely used in cross-language information retrieval (Hasan and Matsumoto (2000)), or translation of names of people (Wang et al. (2007, 2008, 2009)). During translation, they select named entities by removing the postpositions or the endings, by applying the maximum matching algorithm. For Sino-Korean words that are written using same Hangul word but expressing different meanings according to various context environments (ambiguous words), they adopt some mutual information score to evaluate the co-relation between the query term and the candidates. In languages such as Japanese and Korean, there is more ambiguity about where word boundaries should be, wherein some parti"
2015.mtsummit-papers.20,W11-2123,0,0.0601652,"Missing"
2015.mtsummit-papers.20,W04-1111,0,0.0559045,"p. 257 ually decreasing but these kinds of words are still frequently used in formal writings, such as newspapers and dissertations. Most of the Sino-Korean words are not written in Hanja directly, but in Hangul. However, we can convert them into Hanja and further to Hanzi because there is a correspondence among them. Actually, some papers are published with combinations of Hangul and Hanzi in order to specify definitions of vocabularies or emphasize them. 2.2 Related Work There are some previous work on character conversion, both within language or between two languages (Chen and Lee (2000), Huang et al. (2004)). During character conversion, a bilingual dictionary is needed for candidate selection. For a low resource language like Korean, a bilingual dictionary containing enough data, including polysemous words, is often hard to obtain. Moreover, unlike sentence translation, character translation often ignores the context information of the input source sentence. Chinese character knowledge is widely used in cross-language information retrieval (Hasan and Matsumoto (2000)), or translation of names of people (Wang et al. (2007, 2008, 2009)). During translation, they select named entities by removing"
2015.mtsummit-papers.20,P00-1050,0,0.0900092,"rds are often not efficient. Moreover, unlike information retrieval, the machine translation method also has to ensure the meaning of the sentence in order to be fluent. In other words, we should also consider context features of the sentences. Since Korean characters are phonograms, we can find corresponding Hangul characters for given Hanzi characters. Actually, almost all of the Hanzi can be converted to one (or in some rare cases, many) Korean characters. Huang et al. constructed a Chinese-Korean Character Transfer Table (CKCT Table) to reflect the correspondence between Hanzi and Hangul (Huang and Choi (2000)). It is reported that the table contains 436 Hangul with corresponding 6763 Hanzi. Practically, there are 4888 common Hanja used in Korean (KATS (Korean Agency for Technology and Standards) (1997), Hanyang Systems (1992)). Moreover, the number of dailyused Hanzi in Korea numbers around 18001 , while 3500 Hanzi are required to learn for practical Chinese character level test2 . Obviously, most of the Hanzi in their table cannot be considered as practical ones. After all, the table is non-public to ordinary users. 3 Proposed Method Figure 1 gives an overview of our Korean-to-Chinese terminologi"
2015.mtsummit-papers.20,I08-7004,0,0.0617686,"Missing"
2015.mtsummit-papers.20,I13-1020,1,0.891531,"Missing"
2015.mtsummit-papers.20,Y07-1051,0,0.0244467,"ion, both within language or between two languages (Chen and Lee (2000), Huang et al. (2004)). During character conversion, a bilingual dictionary is needed for candidate selection. For a low resource language like Korean, a bilingual dictionary containing enough data, including polysemous words, is often hard to obtain. Moreover, unlike sentence translation, character translation often ignores the context information of the input source sentence. Chinese character knowledge is widely used in cross-language information retrieval (Hasan and Matsumoto (2000)), or translation of names of people (Wang et al. (2007, 2008, 2009)). During translation, they select named entities by removing the postpositions or the endings, by applying the maximum matching algorithm. For Sino-Korean words that are written using same Hangul word but expressing different meanings according to various context environments (ambiguous words), they adopt some mutual information score to evaluate the co-relation between the query term and the candidates. In languages such as Japanese and Korean, there is more ambiguity about where word boundaries should be, wherein some particles may also be a part of a noun, or a verb, their met"
2015.mtsummit-papers.20,I08-1037,0,0.0760323,"Missing"
2015.mtsummit-wpslt.7,P05-1022,0,0.0247598,"el only requires monolingual data, for comparison we also trained a separate model on a larger (30M sentences) in-house monolingual corpus (Mono) of technical/scientiﬁc documents. For the baseline SMT system we used KyotoEBMT (Richardson et al., 2014), a state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks) and did not accurately measure the function word accuracy"
2015.mtsummit-wpslt.7,2006.eamt-1.27,0,0.0997249,"n word translation for statistical machine translation systems, despite this having been looked at in rule-based systems (Arnold and Sadler, 1991). While we were unable to ﬁnd any previous work on function word statistical post-editing, function words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often unclear from string output. In this case we cannot tell easily that ‘translate documents’ is a relative clause (missing the relative pronoun ‘which’ or ‘tha"
2015.mtsummit-wpslt.7,W11-2123,0,0.0174646,"state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks) and did not accurately measure the function word accuracy. In human evaluation we asked two native speakers of the target language (English) with knowledge of the source language (Japanese) to decide if the system output was 1 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ Proceedings of 6th Workshp on Pa"
2015.mtsummit-wpslt.7,O10-5004,0,0.0165403,"tion for statistical machine translation systems, despite this having been looked at in rule-based systems (Arnold and Sadler, 1991). While we were unable to ﬁnd any previous work on function word statistical post-editing, function words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often unclear from string output. In this case we cannot tell easily that ‘translate documents’ is a relative clause (missing the relative pronoun ‘which’ or ‘that’) and that ‘the pap"
2015.mtsummit-wpslt.7,N06-1023,1,0.759196,"trained on the training fold of the ASPEC data. Since our model only requires monolingual data, for comparison we also trained a separate model on a larger (30M sentences) in-house monolingual corpus (Mono) of technical/scientiﬁc documents. For the baseline SMT system we used KyotoEBMT (Richardson et al., 2014), a state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks"
2015.mtsummit-wpslt.7,D08-1077,0,0.0255988,"ﬀect on preserving sentence meaning, and that badly formed punctuation impedes understanding. Surprisingly few studies have been made speciﬁcally on improving function word translation for statistical machine translation systems, despite this having been looked at in rule-based systems (Arnold and Sadler, 1991). While we were unable to ﬁnd any previous work on function word statistical post-editing, function words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often un"
2015.mtsummit-wpslt.7,P02-1040,0,0.0925772,"-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks) and did not accurately measure the function word accuracy. In human evaluation we asked two native speakers of the target language (English) with knowledge of the source language (Japanese) to decide if the system output was 1 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 47 better, worse, or neutral compared to the baseline. A random sample of 20 edited sentences were selected for each exper"
2015.mtsummit-wpslt.7,P14-5014,1,0.831472,"tences, 1790 development sentences and 1812 test sentences. We deﬁned English function words as those tokens with POS tags of functional types such as determinants and prepositions, and treated Japanese particles as function words for the purposes of alignment-based ﬁltering. The primary post-editing model was trained on the training fold of the ASPEC data. Since our model only requires monolingual data, for comparison we also trained a separate model on a larger (30M sentences) in-house monolingual corpus (Mono) of technical/scientiﬁc documents. For the baseline SMT system we used KyotoEBMT (Richardson et al., 2014), a state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenL"
2015.mtsummit-wpslt.7,D11-1046,0,0.0263646,"e monolingual corpus (Mono) of technical/scientiﬁc documents. For the baseline SMT system we used KyotoEBMT (Richardson et al., 2014), a state-of-the-art dependency tree-to-tree translation system that can keep track of the input–output word alignments. Post-editing was performed on the top-1 translation produced by the tree-to-tree baseline system. Japanese segmentation and parsing were performed with Juman and KNP (Kawahara and Kurohashi, 2006). For English we used NLParser (Charniak and Johnson, 2005), converted to dependency parses with an in-house tool. Alignment was performed with Nile (Riesa et al., 2011) and an in-house alignment tool. We used a 5-gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 3.2 Evaluation Human evaluation was conducted to evaluate directly the change in translation quality of function words. We found that automatic evaluation metrics such as BLEU (Papineni et al., 2002) were not suﬃciently sensitive to changes (the change rate is relatively low for post-editing tasks) and did not accurately measure the function word accuracy. In human evaluation we asked two native speakers of the target language (English) with knowledge of the sour"
2015.mtsummit-wpslt.7,P13-3025,0,0.01308,"ction words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often unclear from string output. In this case we cannot tell easily that ‘translate documents’ is a relative clause (missing the relative pronoun ‘which’ or ‘that’) and that ‘the paper’ is a prepositional phrase (missing the preposition ‘in’) rather than the direct object of ‘described’. We believe that the intended meaning of a sentence is often unclear from ﬂat MT output. For example, in Figure 1, the"
2015.mtsummit-wpslt.7,N07-1064,0,0.0203737,"ing. Surprisingly few studies have been made speciﬁcally on improving function word translation for statistical machine translation systems, despite this having been looked at in rule-based systems (Arnold and Sadler, 1991). While we were unable to ﬁnd any previous work on function word statistical post-editing, function words have been used to generate translation rules (Wu et al., 2011). The most similar approach to our method of editing function words used structural templates and was proposed for SMT (Menezes and Quirk, 2008). Statistical post-editing of MT output in a more general sense (Simard et al., 2007) and learning post-editing rules based on common errors (Elming, 2006; Huang et al., 2010) have shown promising results. The majority of statistical postediting methods work directly with string output, however a syntactically motivated approach has been tried for post-editing verb-noun valency (Rosa et al., 2013). Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6) Miami, October 30, 2015 |p. 44 Figure 1: String vs Tree Output: The intended meaning of the translation is often unclear from string output. In this case we cannot tell easily that ‘translate document"
2020.acl-srw.31,D11-1052,0,0.0203729,"annot simply apply a spelling checker, and (2) the way people inputting kanji logographs results in typos with drastically different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typ"
2020.acl-srw.31,N19-1348,0,0.0398173,"Missing"
2020.acl-srw.31,D18-1395,0,0.0532572,"Missing"
2020.acl-srw.31,2020.lrec-1.835,0,0.0368508,"king it difficult to identify the word affected. Although state-of-the-art word segmenters provide reasonable accuracy for clean texts, word segmentation on texts with typos remains a challenging problem. In addition, languages with complex writing systems such as Japanese and Chinese have typos not found in French and English. These languages use logographs, kanji in Japanese, and they are ∗ Current affiliation is Waseda University 1 In the present study, typos may cover some grammatical errors in addition to spelling errors. 2 Although the publicly available multilingual GitHub Typo Corpus (Hagiwara and Mita, 2020) covers Japanese, it contains only about 1,000 instances and ignores erroneous kanjiconversion, an important class of typos in Japanese. 230 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 230–236 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics typically entered using input methods, with which people enter phonetic symbols, kana in the case of Japanese, and then select a correct logograph from a list of logographs matching the reading. Typos occurring during this process can be drastically differ"
2020.acl-srw.31,P17-4012,0,0.0103195,"because i-omission is considered inappropriate in formal writing, but crowdworkers turned out to be tolerant of colloquialism. “Other” of kanji-conversion was more frequent than those of other categories. This means that the answers of crowdworkers were diverse. We conjecture that judging whether kanji is correct or not needs higher-level knowledge of kanji. Some pairs that should have been classified as “Correct revision” were classified as “Other” or “Bad revision”. These imply that the quality of deletion and kanji-conversion was better than the scores indicate. 6 Settings We used OpenNMT (Klein et al., 2017)12 , a Python toolkit of encoder-decoder-based machine translation, as a typo correction system. We trained the model separately for each category of typos. For training and validation, we used sentence pairs not used in the crowdsourced evaluation. The training set contained 79,714 substitutions, 82,227 deletions, 102,897 insertions, and 230,490 kanjiconversions and the validation set contained 5,000 sentence pairs of each category. The test set contained 1,689 substitutions13 , 1,665 deletion, 2,127 insertion, and 3,061 kanji-conversion sentence pairs classified as “Correct revision” as the"
2020.acl-srw.31,W04-3230,0,0.422318,"Missing"
2020.acl-srw.31,L18-1363,0,0.0386803,"combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typo correction system because they have demonstrated their success in a closely related task, spelling correction (Sakaguchi et al., 2017). Since neural networks are known to"
2020.acl-srw.31,max-wisniewski-2010-mining,0,0.042826,"ce neural networks are known to be data-hungry, the first step to develop a neural typo correction system is to prepare a large number of typos and their corrections. However, to our best knowledge, no such dataset is available for Japanese.2 This motivated us to build a large Japanese typo dataset. Typos are usually collected using data mining techniques because thorough corpus annotation is inefficient for infrequently occurring typos. Previous studies on building typo datasets have exploited Wikipedia because it is large, and more importantly, keeps track of all changes made to an article (Max and Wisniewski, 2010; Zesch, 2012). In these studies, to collect typo–correction pairs, the first step is to identify words changed in revisions and the second step is to apply a spell checker to them or calculate the edit distance between them. While these methods work well on the target languages of the previous studies, namely French and English, they cannot be applied directly to languages such as Japanese and Chinese, where words are not delimited by white space. This is because a typo may cause a word segmentation error and can be misinterpreted as a multiple-word change, making it difficult to identify the"
2020.acl-srw.31,I11-1017,0,0.0402207,"nes. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typo correction system because they have demonstrated their success in a closely related task, spelling correction (Sakaguchi et al., 2017). Since neural net"
2020.acl-srw.31,I17-2044,0,0.0211667,"ay people inputting kanji logographs results in typos with drastically different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typo correction system because they have demo"
2020.acl-srw.31,I13-1019,1,0.822011,"lly different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. 1 Introduction For over a decade, user generated content (UGC) has been an important target of NLP technology. It is characterized by phenomena not found in standard texts, such as word lengthening (Brody and Diakopoulos, 2011), dialectal variations (Saito et al., 2017; Blodgett et al., 2016), unknown onomatopoeias (Sasano et al., 2013), grammatical errors (Mizumoto et al., 2011; Lee et al., 2018), and mother tongue interference in non-native writing (Goldin et al., 2018). Typographical errors (typos) also occur often in UGC.1 Typos prevent machines from analyzing texts properly (Belinkov and Bisk, 2018). Typo correction systems are important because applying them before analysis would reduce analysis errors and lead to improved accuracy in various NLP tasks. Neural networks are a promising choice for building a typo correction system because they have demonstrated their success in a closely related task, spelling correction"
2020.acl-srw.31,D18-2010,1,0.8337,"ble decision because each category has its own characteristics. However, this mining strategy prevents us from obtaining a balanced dataset. We leave it for future work. 3.2 Filtering Sentence pairs obtained according to the above procedure contain pairs that do not seem to be typo corrections. We use the following three methods to remove them. losspost − losspre the number of characters changed in the pairs 10 We use the Python3 regex library (https://pypi. org/project/regex/) to determine character types, hiragana, katakana, kanji, or alphabet. 11 We use the morphological analyzers Juman++ (Tolmachev et al., 2018) (http://nlp.ist.i.kyoto-u. ac.jp/index.php?JUMAN++) and MeCab to get readings of kanji. If at least one of the analyses of reading matches, we regard the pairs as having the same reading. > α, where α is determined heuristically. It is set to −4 for substitution, −5 for deletion, and −6 for insertion. We do not apply this filter to kanji-conversion. We found that a change from high-frequency kanji to low-frequency kanji often yielded a large value even if the change was correct. The second filter focused on the loss of the postrevision sentence. This filters out sentence pairs 232 Typo Substi"
2020.acl-srw.31,Q16-1029,0,0.017311,"stem using JWTD. 234 Model Typo Subst. Deletion Morph Insertion Kanji-conv. Subst Deletion Char Insertion Kanji-conv P 11.9 23.2 16.8 30.8 4.5 6.5 6.2 10.0 R 39.8 69.9 79.7 57.0 37.2 59.4 76.0 43.5 F0.5 Match SARI 13.8 31.6 61.2 26.7 60.9 80.2 19.9 69.3 83.8 33.9 48.7 71.0 5.5 25.2 54.2 8.0 44.7 69.6 7.6 51.8 72.5 11.8 33.7 60.9 7 Table 6: Results of the typo correction experiment. and the hidden size were 500. 6.2 Evaluation metrics Our evaluation metrics were precision, recall and F0.5 score in typo correction, the percentage of exact matches between system outputs and references, and SARI (Xu et al., 2016). We defined precision and recall as follows. For each sentence, we calculate the character-level minimum edits from the input to the gold G, the character-level minimum edits from the input to the system output O, and G ∩ O. Let NG , NO , and NG∩O be the sums of |G|, |O|, and |G ∩ O |in all sentences, respectively. We calculated Precision = NG∩O /NO and Recall = NG∩O /NG . We used the Python3 python-Levenshtein library14 for calculating minimum edits. SARI is a metric for text editing. This calculates the averaged F1 scores of the added, kept, and deleted n-grams. We used character-level 4gra"
2020.acl-srw.31,E12-1054,0,0.0269591,"wn to be data-hungry, the first step to develop a neural typo correction system is to prepare a large number of typos and their corrections. However, to our best knowledge, no such dataset is available for Japanese.2 This motivated us to build a large Japanese typo dataset. Typos are usually collected using data mining techniques because thorough corpus annotation is inefficient for infrequently occurring typos. Previous studies on building typo datasets have exploited Wikipedia because it is large, and more importantly, keeps track of all changes made to an article (Max and Wisniewski, 2010; Zesch, 2012). In these studies, to collect typo–correction pairs, the first step is to identify words changed in revisions and the second step is to apply a spell checker to them or calculate the edit distance between them. While these methods work well on the target languages of the previous studies, namely French and English, they cannot be applied directly to languages such as Japanese and Chinese, where words are not delimited by white space. This is because a typo may cause a word segmentation error and can be misinterpreted as a multiple-word change, making it difficult to identify the word affected"
2020.acl-srw.37,D11-1033,0,0.301577,"a selection, pre-training and fine-tuning. 3.1 Data Pre-processing Blindly pre-training a NMT model on vast amounts of monolingual data belonging to the assisting languages and LOI might improve translation quality slightly. However, divergences between the languages, especially their scripts (Hermjakob et al., 2018) and also the distributions of data between different training phases is known to impact the final result. Motivated by past works on using related languages (Dabre et al., 2017), orthography mapping/unification (Hermjakob et al., 2018; Chu et al., 2012) and data selection for MT (Axelrod et al., 2011), we propose to improve the efficacy of pre-training by reducing data and language divergence. 3.1.1 Script Mapping Previous research has shown that enforcing shared orthography (Sennrich et al., 2016b; Dabre et al., 2015) has a strong positive impact on translation. Following this, we propose to leverage existing script mapping rules1 or script unification mechanisms to, at the very least, maximize the possibility of cognate sharing and thereby bringing the assisting language closer to the LOI. This should strongly impact languages such as Hindi, Punjabi and Bengali belonging to the same fami"
2020.acl-srw.37,chu-etal-2012-chinese,1,0.614354,"ASS, but focuses on complementing the potential scarcity of monolingual corpora for the languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target languages Fine-tune NMT model Figure 1: An overview of our proposed method consisting of script mapping, data selection, pre-training and fine-tuning 2. Empirical evaluation: We make a comparison of existing and proposed techniques in a v"
2020.acl-srw.37,Y15-1033,1,0.921893,"languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target languages Fine-tune NMT model Figure 1: An overview of our proposed method consisting of script mapping, data selection, pre-training and fine-tuning 2. Empirical evaluation: We make a comparison of existing and proposed techniques in a variety of corpora settings to verify our hypotheses. 2 Assisting languages Proposed Metho"
2020.acl-srw.37,Y17-1038,1,0.933618,"velopment of methods like BERT (Devlin et al., 2018). Song et al. (2019) recently proposed MASS, a new state-of-the-art NMT pre-training task that jointly trains the encoder and the decoder. Our approach builds on the initial idea of MASS, but focuses on complementing the potential scarcity of monolingual corpora for the languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target langu"
2020.acl-srw.37,W11-2123,0,0.0396903,"g a denoised auto-encoder model. The NMT model is pre-trained with the MASS task, until convergence, jointly for both the source and target languages. Thereafter training is resumed on the parallel corpus, a step known as fine-tuning (Zoph et al., 2016). 4 Experimental Settings We conducted experiments on Japanese–English (Ja–En) translation in a variety of simulated lowresource settings using the “similar” assisting lan281 guage pairs Chinese (Zh) and French (Fr) and the “distant” assisting language pairs Russian (Ru) and Arabic (Ar). 4.1 as linguistically). 3. Data selection: We used KenLM (Heafield, 2011) to train 5-gram LMs on in-domain data for LM scoring based data selection and use ASPEC dev set for length distribution based data selection. Datasets We used the official ASPEC Ja–En parallel corpus (Nakazawa et al., 2016) provided by WAT 20192 . The official split consists of 3M, 1790 and 1872 train, dev and test sentences respectively. We sampled parallel corpora from the top 1M sentences for fine-tuning. Out of the remaining 2M sentences, we used the En side of the first 1M and the Ja side of the next 1M sentences as monolingual data for language modeling for data selection. We used Commo"
2020.acl-srw.37,P18-4003,0,0.115289,"er (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target languages Fine-tune NMT model Figure 1: An overview of our proposed method consisting of script mapping, data selection, pre-training and fine-tuning 2. Empirical evaluation: We make a comparison of existing and proposed techniques in a variety of corpora settings to verify our hypotheses. 2 Assisting languages Proposed Method: Using Assisting Languages We propose a novel monolingual pre-training meth"
2020.acl-srw.37,D18-2012,0,0.0403888,"t languages (script-wise as well 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/index. html#task.html 3 http://data.statmt.org/ngrams/ 4 http://data.statmt.org/news-commentary/v14/ 5 https://github.com/fxsjy/jieba 6 https://www.nltk.org 5 Results and Analysis 5.1 Training and Evaluation Settings We used the tensor2tensor framework (Vaswani et al., 2018) 7 , version 1.14.0., with its default “transformer big” setting. We created a shared sub-word vocabulary using Japanese and English data from ASPEC mixing with Japanese, English, Chinese and French data from Common Crawl. We used SentencePiece (Kudo and Richardson, 2018) and obtained a vocabulary with the size of roughly 64k . We used this vocabulary in all experiments except unrelated language experiment where Arabic and Russian were used instead of Chinese and French data. We combined monolingual data of assisting languages and languages of interest (LOI; Japanese and English) for pre-training. When mixing datasets of different sizes, we always oversampled the smaller datasets to match the size of the largest. For all pre-training models, we saved checkpoints every 1000 steps and for all fine-tuning models, we saved checkpoints every 200 steps. We used earl"
2020.acl-srw.37,N18-2084,0,0.0301046,"ever, most language pairs are resource poor (Russian–Japanese, Marathi–English) as they lack large parallel corpora and the lack of bilingual training data can be compensated by by monolingual corpora. Although it is possible to utilise the popular back-translation method (Sennrich et al., 2016a), it is time-consuming to backtranslate a large amount of monolingual data. Furthermore, poor quality backtranslated data tends to be of little help. Recently, another approach has gained popularity where the NMT model is pre-trained through tasks that only require monolingual data (Song et al., 2019; Qi et al., 2018). Pre-training using models like BERT (Devlin et al., 2018) have led to new state-of-the-art results in text understanding. However, BERT-like sequence models were not designed to be used for NMT which is sequence to sequence (S2S). Song et al. (2019) recently proposed MASS, a S2S specific pre-training task for NMT and obtained new state-of-the-art results in low-resource settings. MASS assumes that a large amount of monolingual data is available for the languages involved but some language pairs may lack both parallel and monolingual corpora and are “truly low-resource” and challenging. Fortu"
2020.acl-srw.37,P16-1009,0,0.233853,"corpora, we were able to improve Japanese–English translation quality by up to 8.5 BLEU in lowresource scenarios. 1 Introduction Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is known to give state-of-the-art (SOTA) translations for language pairs with an abundance of parallel corpora. However, most language pairs are resource poor (Russian–Japanese, Marathi–English) as they lack large parallel corpora and the lack of bilingual training data can be compensated by by monolingual corpora. Although it is possible to utilise the popular back-translation method (Sennrich et al., 2016a), it is time-consuming to backtranslate a large amount of monolingual data. Furthermore, poor quality backtranslated data tends to be of little help. Recently, another approach has gained popularity where the NMT model is pre-trained through tasks that only require monolingual data (Song et al., 2019; Qi et al., 2018). Pre-training using models like BERT (Devlin et al., 2018) have led to new state-of-the-art results in text understanding. However, BERT-like sequence models were not designed to be used for NMT which is sequence to sequence (S2S). Song et al. (2019) recently proposed MASS, a S"
2020.acl-srw.37,P16-1162,0,0.411738,"corpora, we were able to improve Japanese–English translation quality by up to 8.5 BLEU in lowresource scenarios. 1 Introduction Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is known to give state-of-the-art (SOTA) translations for language pairs with an abundance of parallel corpora. However, most language pairs are resource poor (Russian–Japanese, Marathi–English) as they lack large parallel corpora and the lack of bilingual training data can be compensated by by monolingual corpora. Although it is possible to utilise the popular back-translation method (Sennrich et al., 2016a), it is time-consuming to backtranslate a large amount of monolingual data. Furthermore, poor quality backtranslated data tends to be of little help. Recently, another approach has gained popularity where the NMT model is pre-trained through tasks that only require monolingual data (Song et al., 2019; Qi et al., 2018). Pre-training using models like BERT (Devlin et al., 2018) have led to new state-of-the-art results in text understanding. However, BERT-like sequence models were not designed to be used for NMT which is sequence to sequence (S2S). Song et al. (2019) recently proposed MASS, a S"
2020.acl-srw.37,D18-2010,1,0.834981,"selection. We used Common Crawl3 monolingual corpora for pre-training. To train LMs for data-selection of the assisting languages corpora, we used news commentary datasets 4 . While this data selection step for the assisting languages won’t minimize the domain difference from the parallel corpus, it can help in filtering noisy sentences. In this paper we consider the ASPEC and news commentary data as in-domain and the rest of the pre-training data as out-of-domain. 4.2 Data Pre-processing 1. Normalization and Initial Filtering: We applied NFKC normalization to data of all languages. Juman++ (Tolmachev et al., 2018) for Ja tokenization, jieba5 for Zh tokenization and NLTK6 tokenization for other languages. We filtered out all sentences from the pre-training data that contain fewer than 3 and equal or more than 80 tokens. For Chinese data, we filtered out sentences containing fewer than 30 percent Chinese words or more than 30 percent English words. 2. Script Mapping: Chinese is the only assisting language that can be mapped to Japanese reliably. We converted Chinese to Japanese script to make them more similar by using the mapping table from (Chu et al., 2012) and the mapping approaches mentioned in the"
2020.acl-srw.37,W18-1819,0,0.0277583,"om (Chu et al., 2012) and the mapping approaches mentioned in the previous section. French and English are written using the Roman alphabet and do not need any script mapping. We did not perform script mapping for Arabic and Russian to show the impact of using distant languages (script-wise as well 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/index. html#task.html 3 http://data.statmt.org/ngrams/ 4 http://data.statmt.org/news-commentary/v14/ 5 https://github.com/fxsjy/jieba 6 https://www.nltk.org 5 Results and Analysis 5.1 Training and Evaluation Settings We used the tensor2tensor framework (Vaswani et al., 2018) 7 , version 1.14.0., with its default “transformer big” setting. We created a shared sub-word vocabulary using Japanese and English data from ASPEC mixing with Japanese, English, Chinese and French data from Common Crawl. We used SentencePiece (Kudo and Richardson, 2018) and obtained a vocabulary with the size of roughly 64k . We used this vocabulary in all experiments except unrelated language experiment where Arabic and Russian were used instead of Chinese and French data. We combined monolingual data of assisting languages and languages of interest (LOI; Japanese and English) for pre-train"
2020.acl-srw.37,P19-1583,0,0.0136871,"ine ∈ TargetFile do 6 TargetD[len(Line)]+ = 1 ; 9 10 ically the monolingual corpus). When selecting monolingual data of languages of interest, we can first calculate the length distribution of parallel data as target distribution (the ratio of all lengths in T argetF ile) and we fill the length distribution by selecting sentences from monolingual data of same language. As a result, the monolingual data and parallel data have similar length distribution. Data Selection Often, the pre-training monolingual data and the fine-tuning parallel data belong to different domains. (Axelrod et al., 2011; Wang and Neubig, 2019) have shown that proper data selection can reduce the differences between the natures of data between different training domains and phases. In this paper we experiment with (a) Scoring monolingual sentences using a language model (LM) and selecting the highest scoring ones and (b) Selecting monolingual sentences to match the sentence length distribution of the development set sentences in the parallel corpus. 1. LM based data selection: We use a language model trained on corpora belonging to the domain that the fine-tuning data belongs to. We use this sort monolingual sentences according to L"
2020.acl-srw.37,D16-1163,0,0.199846,"resource language translation. Pre-training has enjoyed great success in other NLP tasks with the development of methods like BERT (Devlin et al., 2018). Song et al. (2019) recently proposed MASS, a new state-of-the-art NMT pre-training task that jointly trains the encoder and the decoder. Our approach builds on the initial idea of MASS, but focuses on complementing the potential scarcity of monolingual corpora for the languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Targ"
2020.acl-srw.37,D19-1632,0,\N,Missing
2020.acl-srw.37,D19-1146,1,\N,Missing
2020.coling-main.114,Y12-1058,1,0.853219,"Experimental Settings In our experiments, we used CAModel and CorefCAModel. CAModel was trained on various combinations of tasks. We ﬁne-tuned both the models for 4 epochs using cross entropy loss following Devlin et al. (2019). Since CorefCAModel cannot perform CR with sufﬁciently high accuracy in the early stage of training, we mixed gold coreference data with the ﬁrst stage prediction and gradually reduced the gold ratio inspired by Scheduled Sampling (Bengio et al., 2015)． We used two kinds of datasets for our experiment. One is the Kyoto University Web Document Leads Corpus (Web corpus) (Hangyo et al., 2012), and the other is the Kyoto University Text Corpus (News corpus) (Kawahara et al., 2002). Verbal predicate-argument relations, nominal predicate-argument relations, coreference relations, and bridging anaphora relations are manually annotated in both the corpora. Table 1 lists the number of sentences in each corpus. In our experiment, training was performed on a mixture of both corpora2 and the evaluation was done on each corpus. We used the NICT BERT Japanese pre-trained model (with BPE).3 This model was trained after morphological and subword segmentation using the full text of Japanese Wik"
2020.coling-main.114,2020.acl-main.132,0,0.0334042,"Missing"
2020.coling-main.114,kawahara-etal-2002-construction,1,0.276561,"trained on various combinations of tasks. We ﬁne-tuned both the models for 4 epochs using cross entropy loss following Devlin et al. (2019). Since CorefCAModel cannot perform CR with sufﬁciently high accuracy in the early stage of training, we mixed gold coreference data with the ﬁrst stage prediction and gradually reduced the gold ratio inspired by Scheduled Sampling (Bengio et al., 2015)． We used two kinds of datasets for our experiment. One is the Kyoto University Web Document Leads Corpus (Web corpus) (Hangyo et al., 2012), and the other is the Kyoto University Text Corpus (News corpus) (Kawahara et al., 2002). Verbal predicate-argument relations, nominal predicate-argument relations, coreference relations, and bridging anaphora relations are manually annotated in both the corpora. Table 1 lists the number of sentences in each corpus. In our experiment, training was performed on a mixture of both corpora2 and the evaluation was done on each corpus. We used the NICT BERT Japanese pre-trained model (with BPE).3 This model was trained after morphological and subword segmentation using the full text of Japanese Wikipedia for approximately 1 million steps. At the ﬁne-tuning stage, we set the maximum seq"
2020.coling-main.114,P18-1044,1,0.752299,"AR, and CR all together using BERT. This model is called Cohesion Analysis Model (CAModel). In this section, we ﬁrst describe our Base Model for performing only VPA, and then describe the CAModel for multi-task learning of all four tasks. Finally, we describe the Coreference-aware Cohesion Analysis Model (CorefCAModel), which deals with CR specially. 3.1 Base Model Our Base Model using BERT is shown in Figure 2. This ﬁgure shows the analysis of nominative (NOM) of the predicate ti in VPA. The predictions are performed by an argument selection method, following Shibata and Kurohashi (2018) and Kurita et al. (2018). When the predicate ti is the target, the model calculates the probability that a word is the nominative argument of ti for all other words in the 1325 Figure 2: Our Base Model (in the case of analyzing the nominative (NOM) of verbal predicate ti ). documents. The one with the highest probability among them is adopted as the nominative argument of ti . This calculation is done for all the other cases, such as accusative (ACC) and dative (DAT), and for all predicates in the document. 3.1.1 Input Representation The segmentation of a document consists of three steps: sentence, word, and subword"
2020.coling-main.114,J94-4001,1,0.528332,"ng the full text of Japanese Wikipedia for approximately 1 million steps. At the ﬁne-tuning stage, we set the maximum sequence length to 128. The maximum sequence length of the Web corpus was shorter than 128. In the News corpus, there are many documents with sequence lengths exceeding 128, and one document is divided into multiple parts for training. To do this, we divided a document so that it had as many preceding contexts as possible. For VPA, we extracted all predicates in a document, and analyzed them in terms of four cases of NOM, ACC, DAT and NOM2.4 The Japanese dependency parser KNP (Kurohashi and Nagao, 1994) was used for predicate extraction. For NPA, we analyzed nouns that KNP judged to have arguments. For both of VPA and NPA, we used arguments that have case or zero relation for training and evaluation.5 BAR and CR were performed on nouns. Following Shibata and Kurohashi (2018), we consider author, reader, and unspeciﬁed person as targets of exophora, and the evaluation of VPA, NPA, BAR, and CR was relaxed using a gold coreference chain.6 2 In our preliminary experiments, we have veriﬁed that mixing the corpora leads to better performance than using them alone. 3 https://alaginrc.nict.go.jp/nic"
2020.coling-main.114,2020.acl-main.744,0,0.0203185,"Missing"
2020.coling-main.114,I17-2022,0,0.0574946,"Missing"
2020.coling-main.114,C18-1009,0,0.0411379,"Missing"
2020.coling-main.114,N19-1344,0,0.0328333,"Missing"
2020.coling-main.114,P17-1146,0,0.278134,"Missing"
2020.coling-main.114,C04-1174,1,0.560255,"Missing"
2020.coling-main.114,P16-1162,0,0.0279555,"nt of ti for all other words in the 1325 Figure 2: Our Base Model (in the case of analyzing the nominative (NOM) of verbal predicate ti ). documents. The one with the highest probability among them is adopted as the nominative argument of ti . This calculation is done for all the other cases, such as accusative (ACC) and dative (DAT), and for all predicates in the document. 3.1.1 Input Representation The segmentation of a document consists of three steps: sentence, word, and subword segmentation. Sentence and word segmentation is annotated in the corpora. For subword segmentation, we use BPE (Sennrich et al., 2016) following the segmentation method used at the pre-training stage. Following Devlin et al. (2019), we insert [CLS]and [SEP]tokens at the beginning and end of a document, respectively. In addition, we insert ﬁve special tokens at the end of input sequence: [author], [reader], [unspecified person], [NULL], and [NA]. [author], [reader], and [unspecified person]are used in exophora resolution. In anaphora resolution, an anaphor sometimes refers to an entity that does not appear in the document. This phenomenon is referred to as exophora. In this study, author, reader, and unspeciﬁed:person are tak"
2020.coling-main.114,P18-1054,1,0.806091,"his study, we perform VPA, NPA, BAR, and CR all together using BERT. This model is called Cohesion Analysis Model (CAModel). In this section, we ﬁrst describe our Base Model for performing only VPA, and then describe the CAModel for multi-task learning of all four tasks. Finally, we describe the Coreference-aware Cohesion Analysis Model (CorefCAModel), which deals with CR specially. 3.1 Base Model Our Base Model using BERT is shown in Figure 2. This ﬁgure shows the analysis of nominative (NOM) of the predicate ti in VPA. The predictions are performed by an argument selection method, following Shibata and Kurohashi (2018) and Kurita et al. (2018). When the predicate ti is the target, the model calculates the probability that a word is the nominative argument of ti for all other words in the 1325 Figure 2: Our Base Model (in the case of analyzing the nominative (NOM) of verbal predicate ti ). documents. The one with the highest probability among them is adopted as the nominative argument of ti . This calculation is done for all the other cases, such as accusative (ACC) and dative (DAT), and for all predicates in the document. 3.1.1 Input Representation The segmentation of a document consists of three steps: sen"
2020.coling-main.114,2020.acl-main.622,0,0.0334479,"Missing"
2020.coling-main.114,2020.coling-main.315,0,0.0499898,"Missing"
2020.coling-main.514,2020.cl-1.4,0,0.0129369,"licity of SEW has been pointed out by Xu et al. (2015). Another study concludes that in practice the vocabulary richness of SEW is the same as in regular English Wikipedia (EW), and that the decrease in complexity is mostly due to usage of shorter sentences, while syntax itself is not drastically simplified (Yasseri et al., 2012). Finally, most of the editors seem to be native speakers of English, and the SEW is apparently failing to reach its target audience of L2 speakers, students, and developmentally disabled people. SEW, aligned with EW, is often used for the task of text simplification (Alva-Manchego et al., 2020). A popular subtask of text simplification is lexical simplification, where difficult words are substituted with simple words. In fact, CWI is often treated as a prerequisite of lexical simplification. Although we limit ourselves to detection, paraphrasing is an interesting direction to explore. It should be noted again that our focus on native speakers and longer expressions makes our task unique and distinctive. 2.3 Native Language Identification From a technological point of view, the proposed method has a close connection to native language identification (NLI) (Koppel et al., 2005; Tetrea"
2020.coling-main.514,W11-2838,0,0.013974,"cing for quantitative evaluation, which was followed by in-depth manual analysis of the detected expressions. We found that the scores given by the proposed method weakly correlated with aggregated ratings provided by L2 crowdworkers. Remarkably, the proposed method often identified expressions that consisted of words so basic that the traditional word-based models would have deemed them easy for L2 speakers. 2 2.1 Related Work Word-based Models for Second Language Learners A growing body of research adopts NLP techniques to assist second language learners. While grammatical error correction (Dale and Kilgarriff, 2011; Ng et al., 2013; Ng et al., 2014) has arguably been the most actively studied, a number of researchers have also worked on identifying words that are difficult for learners. The existing approaches to modeling words can be grouped into type-based and token-based ones. 5844 The goal of type-based approaches is to estimate learners’ vocabulary proficiency. To predict whether a learner knows a given word, logistic models based on item response theory are often used (Ehara et al., 2012; Ehara et al., 2013). Token-based approaches are formalized as complex word identification (CWI) (Paetzold and"
2020.coling-main.514,N19-1423,0,0.0237182,"Missing"
2020.coling-main.514,C12-1049,0,0.0319646,"f research adopts NLP techniques to assist second language learners. While grammatical error correction (Dale and Kilgarriff, 2011; Ng et al., 2013; Ng et al., 2014) has arguably been the most actively studied, a number of researchers have also worked on identifying words that are difficult for learners. The existing approaches to modeling words can be grouped into type-based and token-based ones. 5844 The goal of type-based approaches is to estimate learners’ vocabulary proficiency. To predict whether a learner knows a given word, logistic models based on item response theory are often used (Ehara et al., 2012; Ehara et al., 2013). Token-based approaches are formalized as complex word identification (CWI) (Paetzold and Specia, 2016; Yimam et al., 2018). CWI is a task that aims to identify words in texts that might present a challenge for target readers, who are often but not always second language learners. Our task is closer to CWI in that both tasks are designed to handle context sensitivity: depending on surrounding context, a given expression can convey different meanings and hence can be easy or difficult. As the name suggests, however, the focus of CWI is on words, although phrases are not en"
2020.coling-main.514,D18-1395,0,0.0230657,"on is lexical simplification, where difficult words are substituted with simple words. In fact, CWI is often treated as a prerequisite of lexical simplification. Although we limit ourselves to detection, paraphrasing is an interesting direction to explore. It should be noted again that our focus on native speakers and longer expressions makes our task unique and distinctive. 2.3 Native Language Identification From a technological point of view, the proposed method has a close connection to native language identification (NLI) (Koppel et al., 2005; Tetreault et al., 2013; Malmasi et al., 2017; Goldin et al., 2018). In its simplest form, NLI is formalized as a binary classification task where the goal is to determine whether the writer is a native or L2 speaker. Goldin et al. (2018) worked on an English corpus in which L2 speakers were highly advanced, almost at the level of native speakers (Rabinovich et al., 2018). As we see in Section 5.1, we use this dataset in our task. Although we also train a classifier, an important difference from NLI is that classification is not our goal but an intermediate task. While Goldin et al. (2018) used as the input a text chunk large enough to reveal the writer’s ide"
2020.coling-main.514,W17-5007,0,0.0145035,"k of text simplification is lexical simplification, where difficult words are substituted with simple words. In fact, CWI is often treated as a prerequisite of lexical simplification. Although we limit ourselves to detection, paraphrasing is an interesting direction to explore. It should be noted again that our focus on native speakers and longer expressions makes our task unique and distinctive. 2.3 Native Language Identification From a technological point of view, the proposed method has a close connection to native language identification (NLI) (Koppel et al., 2005; Tetreault et al., 2013; Malmasi et al., 2017; Goldin et al., 2018). In its simplest form, NLI is formalized as a binary classification task where the goal is to determine whether the writer is a native or L2 speaker. Goldin et al. (2018) worked on an English corpus in which L2 speakers were highly advanced, almost at the level of native speakers (Rabinovich et al., 2018). As we see in Section 5.1, we use this dataset in our task. Although we also train a classifier, an important difference from NLI is that classification is not our goal but an intermediate task. While Goldin et al. (2018) used as the input a text chunk large enough to r"
2020.coling-main.514,P14-5010,0,0.00442703,"Missing"
2020.coling-main.514,W13-3601,0,0.0240514,"ation, which was followed by in-depth manual analysis of the detected expressions. We found that the scores given by the proposed method weakly correlated with aggregated ratings provided by L2 crowdworkers. Remarkably, the proposed method often identified expressions that consisted of words so basic that the traditional word-based models would have deemed them easy for L2 speakers. 2 2.1 Related Work Word-based Models for Second Language Learners A growing body of research adopts NLP techniques to assist second language learners. While grammatical error correction (Dale and Kilgarriff, 2011; Ng et al., 2013; Ng et al., 2014) has arguably been the most actively studied, a number of researchers have also worked on identifying words that are difficult for learners. The existing approaches to modeling words can be grouped into type-based and token-based ones. 5844 The goal of type-based approaches is to estimate learners’ vocabulary proficiency. To predict whether a learner knows a given word, logistic models based on item response theory are often used (Ehara et al., 2012; Ehara et al., 2013). Token-based approaches are formalized as complex word identification (CWI) (Paetzold and Specia, 2016; Yim"
2020.coling-main.514,W14-1701,0,0.0158237,"followed by in-depth manual analysis of the detected expressions. We found that the scores given by the proposed method weakly correlated with aggregated ratings provided by L2 crowdworkers. Remarkably, the proposed method often identified expressions that consisted of words so basic that the traditional word-based models would have deemed them easy for L2 speakers. 2 2.1 Related Work Word-based Models for Second Language Learners A growing body of research adopts NLP techniques to assist second language learners. While grammatical error correction (Dale and Kilgarriff, 2011; Ng et al., 2013; Ng et al., 2014) has arguably been the most actively studied, a number of researchers have also worked on identifying words that are difficult for learners. The existing approaches to modeling words can be grouped into type-based and token-based ones. 5844 The goal of type-based approaches is to estimate learners’ vocabulary proficiency. To predict whether a learner knows a given word, logistic models based on item response theory are often used (Ehara et al., 2012; Ehara et al., 2013). Token-based approaches are formalized as complex word identification (CWI) (Paetzold and Specia, 2016; Yimam et al., 2018)."
2020.coling-main.514,Q18-1024,0,0.079152,"ive speakers and longer expressions makes our task unique and distinctive. 2.3 Native Language Identification From a technological point of view, the proposed method has a close connection to native language identification (NLI) (Koppel et al., 2005; Tetreault et al., 2013; Malmasi et al., 2017; Goldin et al., 2018). In its simplest form, NLI is formalized as a binary classification task where the goal is to determine whether the writer is a native or L2 speaker. Goldin et al. (2018) worked on an English corpus in which L2 speakers were highly advanced, almost at the level of native speakers (Rabinovich et al., 2018). As we see in Section 5.1, we use this dataset in our task. Although we also train a classifier, an important difference from NLI is that classification is not our goal but an intermediate task. While Goldin et al. (2018) used as the input a text chunk large enough to reveal the writer’s identity, we classify single sentences in order to narrow down potential occurrences of native-like expressions. To this end, we are eager to eliminate the impact of extralinguistic patterns as much as possible while Goldin et al. (2018) used them to improve the classification performance. For 1 https://simpl"
2020.coling-main.514,W13-1706,0,0.0324005,"2020). A popular subtask of text simplification is lexical simplification, where difficult words are substituted with simple words. In fact, CWI is often treated as a prerequisite of lexical simplification. Although we limit ourselves to detection, paraphrasing is an interesting direction to explore. It should be noted again that our focus on native speakers and longer expressions makes our task unique and distinctive. 2.3 Native Language Identification From a technological point of view, the proposed method has a close connection to native language identification (NLI) (Koppel et al., 2005; Tetreault et al., 2013; Malmasi et al., 2017; Goldin et al., 2018). In its simplest form, NLI is formalized as a binary classification task where the goal is to determine whether the writer is a native or L2 speaker. Goldin et al. (2018) worked on an English corpus in which L2 speakers were highly advanced, almost at the level of native speakers (Rabinovich et al., 2018). As we see in Section 5.1, we use this dataset in our task. Although we also train a classifier, an important difference from NLI is that classification is not our goal but an intermediate task. While Goldin et al. (2018) used as the input a text c"
2020.coling-main.514,Q15-1021,0,0.0146012,"sh (Ogden, 1930) — one of the oldest and most influential controlled languages. The vocabulary of Basic English is stripped down from regular English to 850 word forms, with verbs being especially restricted to just 18. The largest collection of texts that is claimed to use Basic English is the Simple English Wikipedia (SEW). SEW contributors purport to adhere to the principle of using “simpler” vocabulary and avoiding idioms,1 but this is not strict, and the case-by-case judgments are largely left to writers’ and editors’ discretion. The surprising unsimplicity of SEW has been pointed out by Xu et al. (2015). Another study concludes that in practice the vocabulary richness of SEW is the same as in regular English Wikipedia (EW), and that the decrease in complexity is mostly due to usage of shorter sentences, while syntax itself is not drastically simplified (Yasseri et al., 2012). Finally, most of the editors seem to be native speakers of English, and the SEW is apparently failing to reach its target audience of L2 speakers, students, and developmentally disabled people. SEW, aligned with EW, is often used for the task of text simplification (Alva-Manchego et al., 2020). A popular subtask of text"
2020.coling-main.514,W18-0507,0,0.0183936,"013; Ng et al., 2014) has arguably been the most actively studied, a number of researchers have also worked on identifying words that are difficult for learners. The existing approaches to modeling words can be grouped into type-based and token-based ones. 5844 The goal of type-based approaches is to estimate learners’ vocabulary proficiency. To predict whether a learner knows a given word, logistic models based on item response theory are often used (Ehara et al., 2012; Ehara et al., 2013). Token-based approaches are formalized as complex word identification (CWI) (Paetzold and Specia, 2016; Yimam et al., 2018). CWI is a task that aims to identify words in texts that might present a challenge for target readers, who are often but not always second language learners. Our task is closer to CWI in that both tasks are designed to handle context sensitivity: depending on surrounding context, a given expression can convey different meanings and hence can be easy or difficult. As the name suggests, however, the focus of CWI is on words, although phrases are not entirely absent from the data. Our departure from word-based models is motivated by skepticism about the idea that simpler words result in better c"
2020.emnlp-main.192,N19-1423,0,0.0456148,"k problems from the whole data of ConceptNet. Introduction Along with the progress of deep learning, there have been many studies that consider task settings and build their datasets for training/evaluating language understanding ability by computers (Wang et al., 2019b,a). Language understanding by computers requires two types of knowledge: knowledge of language (meaning of words, syntax, and so forth) and knowledge of our world and society beyond language. The former problem of acquiring linguistic knowledge has been solved to a large extent by general-purpose language models, such as BERT (Devlin et al., 2019), which are pre-trained using a large corpus. It is now possible to represent the meaning of a word as a vector according to its context. Fine-tuning based on these vectors has made natural language inference, paraphrase recognition, and question answering without requiring deep inference as accurate as humans. On the other hand, there are still many problems with acquiring knowledge beyond language. Actu∗ 1 Current affiliation is Waseda University. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JCID Another important point is that biases in building datasets must be reduced as much as possible."
2020.emnlp-main.192,P84-1044,0,0.381226,"Missing"
2020.emnlp-main.192,N18-2017,0,0.0679104,"Missing"
2020.emnlp-main.192,E14-1007,1,0.879084,"Missing"
2020.emnlp-main.192,P19-1441,0,0.014713,"porating such knowledge bases into an NLP model have been studied but have not been established yet. Many QA datasets for commonsense inference have been built. They include COPA (Choice of Plausible Alternatives) (Roemmele et al., 2011), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), and CommonsenseQA (Talmor et al., 2019). These datasets can be solved to some extent by machine comprehension models (Devlin et al., 2019) that have been rapidly improved. There have been also some approaches that transfer knowledge in such a dataset to downstream tasks using multi-task learning (Liu et al., 2019). We briefly introduce these datasets below. COPA consists of 1,000 two-choice questions that ask a causal relation between two sentences. Each question provides a premise sentence and requires to choose its cause or ending sentence from two alternatives. This dataset was manually created for the purpose of evaluation and is too small to learn commonsense by computers. SWAG is a commonsense inference dataset consisting of 113k multiple-choice questions that ask the most appropriate verb phrase following a given context. To guarantee generality as commonsense, questions were created from video"
2020.emnlp-main.192,prasad-etal-2008-penn,0,0.0604091,"ied in a straightforward way, but we need to take care of the case that an argument in the latter event is pronominalized or omitted. If the latter event does not have an explicit argument, we recover it with any of the arguments in the former event and examine whether the recovered latter event is composed of a basic event. Table 3: Examples of Japanese basic events. The contingency relation between events should be expressed by an explicit discourse marker and be a causal or conditional relation, corresponding to “CONTINGENCY:Cause” or “CONTINGENCY:Condition” in the Penn Discourse Treebank (Prasad et al., 2008). To select highly reliable parts from analysis results and to extract only general event pairs as commonsense, we keep event pairs satisfying the following conditions. Here, we call the first event that represents a cause or reason former event and the second event latter event. For example, consider the event pair “the glass breaks on impact → I replace it”. In this case, we generate recovered latter events “I replace the glass” and “I replace impact” by substituting an argument in the former event for “it”. Then, we examine whether either of them is composed of a basic event and extract thi"
2020.emnlp-main.192,N19-1421,0,0.0602062,"is comprised of 877k if-then pairs of basic events. They collected these pairs using crowdsourcing based on frequent basic events extracted from several corpora. These fully manual or crowdsourcing approaches are costly and have a problem of scalability. Also, methods for incorporating such knowledge bases into an NLP model have been studied but have not been established yet. Many QA datasets for commonsense inference have been built. They include COPA (Choice of Plausible Alternatives) (Roemmele et al., 2011), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), and CommonsenseQA (Talmor et al., 2019). These datasets can be solved to some extent by machine comprehension models (Devlin et al., 2019) that have been rapidly improved. There have been also some approaches that transfer knowledge in such a dataset to downstream tasks using multi-task learning (Liu et al., 2019). We briefly introduce these datasets below. COPA consists of 1,000 two-choice questions that ask a causal relation between two sentences. Each question provides a premise sentence and requires to choose its cause or ending sentence from two alternatives. This dataset was manually created for the purpose of evaluation and"
2020.emnlp-main.192,D18-1009,0,0.0252109,"se relations is not high. ATOMIC (Sap et al., 2019) is a knowledge base that is comprised of 877k if-then pairs of basic events. They collected these pairs using crowdsourcing based on frequent basic events extracted from several corpora. These fully manual or crowdsourcing approaches are costly and have a problem of scalability. Also, methods for incorporating such knowledge bases into an NLP model have been studied but have not been established yet. Many QA datasets for commonsense inference have been built. They include COPA (Choice of Plausible Alternatives) (Roemmele et al., 2011), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), and CommonsenseQA (Talmor et al., 2019). These datasets can be solved to some extent by machine comprehension models (Devlin et al., 2019) that have been rapidly improved. There have been also some approaches that transfer knowledge in such a dataset to downstream tasks using multi-task learning (Liu et al., 2019). We briefly introduce these datasets below. COPA consists of 1,000 two-choice questions that ask a causal relation between two sentences. Each question provides a premise sentence and requires to choose its cause or ending sentence from two alterna"
2020.emnlp-main.192,P19-1472,0,0.0625795,"Sap et al., 2019) is a knowledge base that is comprised of 877k if-then pairs of basic events. They collected these pairs using crowdsourcing based on frequent basic events extracted from several corpora. These fully manual or crowdsourcing approaches are costly and have a problem of scalability. Also, methods for incorporating such knowledge bases into an NLP model have been studied but have not been established yet. Many QA datasets for commonsense inference have been built. They include COPA (Choice of Plausible Alternatives) (Roemmele et al., 2011), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), and CommonsenseQA (Talmor et al., 2019). These datasets can be solved to some extent by machine comprehension models (Devlin et al., 2019) that have been rapidly improved. There have been also some approaches that transfer knowledge in such a dataset to downstream tasks using multi-task learning (Liu et al., 2019). We briefly introduce these datasets below. COPA consists of 1,000 two-choice questions that ask a causal relation between two sentences. Each question provides a premise sentence and requires to choose its cause or ending sentence from two alternatives. This dataset was manually c"
2020.findings-emnlp.121,O14-4001,1,0.877406,"Missing"
2020.findings-emnlp.121,Q14-1022,0,0.153812,"pus, a series of temporal competitions (TempEval-1,2,3) (Verhagen et al., 2009, 2010; UzZaman et al., 2012) are attracting growing research efforts. Temporal relation classification (TRC) is the task to predict a temporal relation (after, before, includes, etc.) of a TLINK from a source mention to a target mention. Less effort has been paid to explore the sharing information across ‘local’ pairs and TLINK categories. In recent years, a variety of dense annotation schemas are proposed to overcome the ‘sparse’ annotation in the original Timebank. A typical one is the Timebank-Dense (TD) corpus (Chambers et al., 2014), which performs We demonstrate our proposal with the above adjacent-sentence excerpt in Timebank-Dense. ‘(es , et )’ denotes a directed TLINK from the source es to target et in this paper. Considering the ‘manhunt (e1 )’ centric chain: {(e1 , DCT ), (e1 , e2 ), (e1 , t1 ), (e1 , e3 )}2 , ‘manhunt’ holds a ‘includes’ relation to ‘continues’. 1 Time-to-Time (T2T) is not included in this paper, as we focus on event centric representations. 2 As DCT is not explicitly mentioned in documents, we always place (ei , DCT ) on the top of a SECT chain DCT : 1998-02-27 An intense manhunt (e1 ) conducted"
2020.findings-emnlp.121,P17-2001,1,0.932839,"TLINKs. 2) Our model exploits a multi-task learning framework with two common layers trained by a combined category-specific loss to overcome the data isolation among TLINK categories. The experimental results suggest the effectiveness of our proposal on two datasets. All the codes of our model and two baselines is released. 3 2 2.1 Related Work Temporal Relation Classification Most existing temporal relation classification approaches focus on extracting various features from the textual sentence in the local pair-wise setting. Inspired by the success of neural networks in various NLP tasks, Cheng and Miyao (2017); Meng et al. (2017); Vashishtha et al. (2019); Han et al. (2019b,a) propose a series of neural networks to achieve accuracy with less feature engineering. However, these neural models still drop in the pairwise setting. Meng and Rumshisky (2018) propose a global context layer (GCL) to store/read the solved TLINK history upon a pre-trained pair-wise classifier. However, they find slow converge when training the GCL and pair-wise classifier simultaneously. Minor improvement is observed compared to their pair-wise classifier. Our model is distinguished from their work in three focuses: 1) We con"
2020.findings-emnlp.121,N19-1423,0,0.0726018,"ings of the Association for Computational Linguistics: EMNLP 2020, pages 1352–1357 c November 16 - 20, 2020. 2020 Association for Computational Linguistics We assume that dynamically updating the representation of ‘manhunt’ in the early step ‘(e1 , e2 )’ will benefit the prediction for the later step (e1 , e3 ) to ‘search’. ‘manhunt’ is supposed to hold the same ‘includes’ relation to ‘search’, as the search should be included in the continuing manhunt. Our model further exploits a multi-task learning framework to leverage all three categories of TLINKs in the SECT chain scope. A common BERT (Devlin et al., 2019) encoder layer is applied to retrieve token embeddings. The global RNN layer manages the dynamic event and TLINK presentations in the chain. Finally, our system feeds the TLINK representations into their corresponding category-specific (E2D, E2T and E2E) classifiers to calculate a combined loss. The contribution of this work is listed as follows: 1) We present a novel source event centric model to dynamically manage event representations across TLINKs. 2) Our model exploits a multi-task learning framework with two common layers trained by a combined category-specific loss to overcome the data"
2020.findings-emnlp.121,K19-1062,0,0.0164696,"o common layers trained by a combined category-specific loss to overcome the data isolation among TLINK categories. The experimental results suggest the effectiveness of our proposal on two datasets. All the codes of our model and two baselines is released. 3 2 2.1 Related Work Temporal Relation Classification Most existing temporal relation classification approaches focus on extracting various features from the textual sentence in the local pair-wise setting. Inspired by the success of neural networks in various NLP tasks, Cheng and Miyao (2017); Meng et al. (2017); Vashishtha et al. (2019); Han et al. (2019b,a) propose a series of neural networks to achieve accuracy with less feature engineering. However, these neural models still drop in the pairwise setting. Meng and Rumshisky (2018) propose a global context layer (GCL) to store/read the solved TLINK history upon a pre-trained pair-wise classifier. However, they find slow converge when training the GCL and pair-wise classifier simultaneously. Minor improvement is observed compared to their pair-wise classifier. Our model is distinguished from their work in three focuses: 1) We constrains the model in a reasonable scope, i.e. 3 https://github.c"
2020.findings-emnlp.121,D19-1041,0,0.119089,"Missing"
2020.findings-emnlp.121,P19-1441,0,0.105091,"random/ NeuralTime Figure 1: The overview of the proposed model. SECT chain. 2) We manages dynamic event representations, while their model stores/reads pair history 3) Our model integrates category-specific classifiers by multi-task learning, while they use the categories as the features in one single classifier. 2.2 Multi-task Transfer Learning For the past three years, several successful transfer learning models (ELMO, GPT and BERT) (Peters et al., 2018; Radford et al.; Devlin et al., 2019) have been proposed, which significantly improved the state-of-the-art on a wide range of NLP tasks. (Liu et al., 2019) propose a single-task batch multitask learning approach over a common BERT to leverage a large mount of cross-task data in the fine-tuning stage. In this work, our model deals with various categories of TLINKs (E2E, E2T and E2D) in a batch of SECT chains to calculate the combined loss with the category-specific classifiers. 2.3 Non-English Temporal Corpora Less attention has been paid for non-English temporal corpora. Until 2014, Asahara et al. starts the first corpus-based study BCCWJ-Timebank (BT) on Japanese temporal information annotation. We explore the feasibility of our model on this J"
2020.findings-emnlp.121,P18-1049,0,0.620441,"proposal on two datasets. All the codes of our model and two baselines is released. 3 2 2.1 Related Work Temporal Relation Classification Most existing temporal relation classification approaches focus on extracting various features from the textual sentence in the local pair-wise setting. Inspired by the success of neural networks in various NLP tasks, Cheng and Miyao (2017); Meng et al. (2017); Vashishtha et al. (2019); Han et al. (2019b,a) propose a series of neural networks to achieve accuracy with less feature engineering. However, these neural models still drop in the pairwise setting. Meng and Rumshisky (2018) propose a global context layer (GCL) to store/read the solved TLINK history upon a pre-trained pair-wise classifier. However, they find slow converge when training the GCL and pair-wise classifier simultaneously. Minor improvement is observed compared to their pair-wise classifier. Our model is distinguished from their work in three focuses: 1) We constrains the model in a reasonable scope, i.e. 3 https://github.com/racerandom/ NeuralTime Figure 1: The overview of the proposed model. SECT chain. 2) We manages dynamic event representations, while their model stores/reads pair history 3) Our mo"
2020.findings-emnlp.121,D17-1092,0,0.279835,"ploits a multi-task learning framework with two common layers trained by a combined category-specific loss to overcome the data isolation among TLINK categories. The experimental results suggest the effectiveness of our proposal on two datasets. All the codes of our model and two baselines is released. 3 2 2.1 Related Work Temporal Relation Classification Most existing temporal relation classification approaches focus on extracting various features from the textual sentence in the local pair-wise setting. Inspired by the success of neural networks in various NLP tasks, Cheng and Miyao (2017); Meng et al. (2017); Vashishtha et al. (2019); Han et al. (2019b,a) propose a series of neural networks to achieve accuracy with less feature engineering. However, these neural models still drop in the pairwise setting. Meng and Rumshisky (2018) propose a global context layer (GCL) to store/read the solved TLINK history upon a pre-trained pair-wise classifier. However, they find slow converge when training the GCL and pair-wise classifier simultaneously. Minor improvement is observed compared to their pair-wise classifier. Our model is distinguished from their work in three focuses: 1) We constrains the model in"
2020.findings-emnlp.121,C16-1265,0,0.0242799,"ation micro F1 of all TLINKs against the training epochs of the above asynchronous training strategies. no freeze shows the evidence of our concern that the curve undulate after the initial 3 epochs. freeze performs a stable learning phase with the lowest initialization. freeze after k epochs achieves the balance of the stability and high F1. Therefore, we perform the third strategy for all the following experiments. The number k is selected from {3, 4, 5} based on the validation scores. 4.2 Main Timebank-Dense Results Table 2 shows the experimental results on the English TD corpus. ‘CATENA’ (Mirza and Tonelli, 2016) is the feature-based model combined with dense word embeddings. ‘SDP-RNN’ (Cheng and Miyao, 2017) is the dependency tree enhanced RNN model.‘GCL’ (Meng and Rumshisky, 2018) is the global context layer model introduced in § 2.1. ‘Fine-grained TRC’ Vashishtha et al. (2019) is the ELMO based fine-grained TRC model with only the E2E results reported. It’s not surprising that the proposed model substantially outperforms state-of-the-art systems, as the existing SOTA didn’t exploit BERT yet. Therefore, we offer the ablation test with ‘LocalBERT’(w/o multi-categories learning and global SEC RNN) and"
2020.findings-emnlp.121,N18-1202,0,0.0494105,"-wise classifier. Our model is distinguished from their work in three focuses: 1) We constrains the model in a reasonable scope, i.e. 3 https://github.com/racerandom/ NeuralTime Figure 1: The overview of the proposed model. SECT chain. 2) We manages dynamic event representations, while their model stores/reads pair history 3) Our model integrates category-specific classifiers by multi-task learning, while they use the categories as the features in one single classifier. 2.2 Multi-task Transfer Learning For the past three years, several successful transfer learning models (ELMO, GPT and BERT) (Peters et al., 2018; Radford et al.; Devlin et al., 2019) have been proposed, which significantly improved the state-of-the-art on a wide range of NLP tasks. (Liu et al., 2019) propose a single-task batch multitask learning approach over a common BERT to leverage a large mount of cross-task data in the fine-tuning stage. In this work, our model deals with various categories of TLINKs (E2E, E2T and E2D) in a batch of SECT chains to calculate the combined loss with the category-specific classifiers. 2.3 Non-English Temporal Corpora Less attention has been paid for non-English temporal corpora. Until 2014, Asahara"
2020.findings-emnlp.121,D18-2010,1,0.731125,"Missing"
2020.findings-emnlp.121,P19-1280,0,0.0435049,"Missing"
2020.findings-emnlp.23,Q14-1037,0,0.0296602,"en these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models ("
2020.findings-emnlp.23,D17-1181,0,0.0426636,"Missing"
2020.findings-emnlp.23,P19-1136,0,0.0256531,"Missing"
2020.findings-emnlp.23,C16-1239,0,0.0720788,"Missing"
2020.findings-emnlp.23,P19-1407,0,0.0251793,"th Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) E E [sE 0 , s1 , . . . , sn ] = Encoder([x0 , x1 , . . . , xn ]) (1) Then we pass the output s sequence to Conven : E E o0 = Conven ([sE 0 , s1 , . . . , sn ]) (2) where Conven is the encoder convolutional layer. Conven maps sE to o0 , which is also a sequence and has the identical dimension as the s sequence. The output is denoted as o0 ∈ Rn×h , where h is the hidden size, n is the length of the input sentence. o0 is the auxiliary representation of the sentence, which is used for decoding with scratchpad attention mechanism (Benmalek et al., 2019): on−1 is used to calculate attention score, and on−1 will be updated to on at every decoding step. During decoding, we use different input embeddings and output layers for relation and entity ex238 SOS sentence Encoder Obama Decoder Obama HLR Decoder Encoder Decoder Obama HLR sentence Encoder Obama Decoder graduate Decoder graduate president CU SOS sentence graduate SOS HLS sentence president HLR Decoder Decoder Obama HLR afﬁliation Encoder Decoder Decoder graduate president Obama HLR Decoder graduate president SOS Obama HLR Decoder HLR graduate afﬁliation Decoder CU HLS president Decoder HLR"
2020.findings-emnlp.23,P11-1056,0,0.0387397,"d time step, thus the model is prone to feed entity pairs to the classification layer with an low odds (low recall) but high confidence (high precision). In contrast, for the order h-r-t, given the predicted h, the corresponding r can be easily identified according to the context. Subsequently, the predicted h-r pair gives strong hint to the last time step prediction, hence the model will not collapse from the no-relation. This also applies to any other order with r in the first two time steps. 5 Related Work Previous work uses P IPELINE to extract triplets from text (Nadeau and Sekine, 2007; Chan and Roth, 2011). They first recognize all entities in the input sentence then classify relations for each entity pair exhaustively. Li and Ji (2014) point out that the classification errors may propagate across subtasks. Instead of treating these two subtasks separately, for joint entities and relations extraction (JERE), TABLE methods calculate the similarity score of all token pairs and relations by exhaustive enumeration and the extracted triplets are found by the position of the output in the table (Miwa and Bansal, 2016; Gupta et al., 2016). However, as a triplet may contain entities with different leng"
2020.findings-emnlp.23,K19-1055,0,0.0136371,"es auto-regressive decoding strategy. The decoder predicts the nodes layer by layer, where the prediction results of the previous layer are used as the input of the next time step separately, as shown in Fig. 3b. 3 3.1 Experiments Settings Dataset We evaluate our model on two datasets, NYT and DuIE1 . NYT (Riedel et al., 2010) is a English news dataset that is generated by distant supervision without manual annotation, which is widely used in JERE studies (Zheng et al., 2017; Zeng et al., 2018; Takanobu et al., 2018; Dai et al., 2019; Fu et al., 2019; Nayak and Ng, 2019; Zeng et al., 2019a,b; Chen et al., 2019; Wei et al., 2019). We use the same data split as CopyRE (Zeng et al., 2018). DuIE (Li et al., 2019) is a large-scale Chinese JERE dataset where sentences are from Baidu 1 https://ai.baidu.com/broad/introduction? dataset=dureader Baselines We compare the proposed model, Seq2UMTree, with strong baselines under the same hyperparameters, as follows: 1) CopyMTL (Zeng et al., 2019a) is a Seq2Seq model with copy mechanism, and the entities are found by multi-task learning. 2) WDec (Nayak and Ng, 2019) is a standard Seq2Seq model with dynamic masking, and decode the entity token by token. 3) MHS (Be"
2020.findings-emnlp.23,D14-1179,0,0.0369231,"Missing"
2020.findings-emnlp.23,P82-1020,0,0.803956,"Missing"
2020.findings-emnlp.23,P05-1051,1,0.612724,"iversity), Obama and Columbia University are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belong"
2020.findings-emnlp.23,H05-1003,1,0.628363,"lumbia University are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple rel"
2020.findings-emnlp.23,P17-1085,0,0.0853559,"classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one entity, which solves the problem naturally (Zeng et"
2020.findings-emnlp.23,P16-4011,0,0.0283357,"ethods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can"
2020.findings-emnlp.23,P14-1038,1,0.924787,"he relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSe"
2020.findings-emnlp.23,D14-1198,1,0.921782,"Missing"
2020.findings-emnlp.23,2020.acl-main.713,1,0.797784,"the effects of exposure bias. Our method differs from previous solution on exposure bias that we remove the order by structure decoding rather than random sampling (Tsai and Lee, 2019). CASREL (Wei et al., 2020) is a recently proposed two-step tagging method, which first finds all the head entities in the sentence then labels a relation-tail table for each head entity, which can also be seen as a UMTree decoder with a decoding length two. However, they overlook the data bias problem in NYT, which causing model unreliability and possible model bias. Note that our task is different from ONEIE (Lin et al., 2020), which models event extraction, entity span detection, entity type recognition and relation extraction in a Seq2Graph way. In contrast to ONEIE, JERE aims to extract only relation-entity triplets, which can be modeled by our UMTree structure naturally. The simplicity of the tree enables the model to conduct global extraction. 243 6 Conclusions In this paper, we thoroughly analyze the effects of exposure bias of Seq2Seq models on joint entity and relation extraction. Exposure bias causes overfitting that hurts the reliability of the performance scores. To solve the problem of exposure bias, we"
2020.findings-emnlp.23,D19-1241,1,0.820988,") and propose a novel model Seq2UMTree. The Seq2UMTree model is based on an Encoder-Decoder framework, which is composed of a conventional encoder and a UMTree decoder. The UMTree decoder models entities and relations jointly and structurally, using a copy mechanism with unordered multi-label classification as the output layer. This multi-label classification model ensures the nodes in the same layer are unordered and discards the predefined triplet order so that the prediction deviation will not aggregate and affect other triplets. Different from the standard Seq2Tree (Dong and Lapata, 2016; Liu et al., 2019), the decoding length is limited to three (one triplet), which is the shortest feasible length for JERE task. In this way, the exposure bias is minimized under the triplet-level F1 metrics. In conclusion, our contributions are listed as follows: To mitigate the exposure bias problem while keeping the simplicity of Seq2Seq, we recast the one-dimension triplet sequence to two-dimension 237 • We point out the redundancy of the predefined triplet order of the Seq2Seq model, and propose a novel Seq2UMTree model to minimize exposure bias by recasting the ordered triplet sequence to an Unordered-Mult"
2020.findings-emnlp.23,D15-1102,0,0.027607,"xtraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an e"
2020.findings-emnlp.23,D15-1166,0,0.0236156,"sidered as depth 0, (b) relation embedding: wtr ∈ Rh , e2 h (c) entity embedding: wte = oe1 t−1 + ot−1 ∈ R , where e1 and e2 are the beginning position and the end position of the predicted entity respectively. t ∈ {1, 2, 3}, which is the decoding time step. The decoding order can be predefined arbitrarily, such as h-r-t or t-r-h. Given the input embedding wt and the output of the previous time step sD t−1 , a unary LSTM decoder is used to generate decoder hidden state: D sD t = Decoder(wt , st−1 ) (3) D where sD t is the decoder hidden states; s0 is iniE tialized by sn . Attention mechanism (Luong et al., 2015) is used to generate context-aware embedding: at = Attention(ot−1 , sD t ) (5) where Convde maps dimension 2h to h and at is replicated n times before concatenation. The output layer of the relation prediction is a linear transformation followed by a max-pooling over sequence: probr = σ(Max(ot Wr + br )) The output layers of the entity prediction are two binary classification layers over the whole sequence, predicting the positions of the beginning and the end of the entities respectively: probeb = σ(WeTb ot + beb ) probee = σ(WeTe ot + bee ) (7) where We ∈ Rh×1 , be is a scalar and probe ∈ Rn"
2020.findings-emnlp.23,P16-1105,0,0.174523,"ty extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one enti"
2020.findings-emnlp.23,D14-1200,0,0.0636813,"r supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are a"
2020.findings-emnlp.23,W04-2401,0,0.330979,"mple, in the triplet (Obama, graduate from, Columbia University), Obama and Columbia University are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a sin"
2020.findings-emnlp.23,D18-1249,0,0.0156993,"oint out that the classification errors may propagate across subtasks. Instead of treating these two subtasks separately, for joint entities and relations extraction (JERE), TABLE methods calculate the similarity score of all token pairs and relations by exhaustive enumeration and the extracted triplets are found by the position of the output in the table (Miwa and Bansal, 2016; Gupta et al., 2016). However, as a triplet may contain entities with different lengths, the table methods either suffer from exponential computational burden (Adel and Sch¨utze, 2017) or roll back to pipeline methods (Sun et al., 2018; Bekoulis et al., 2018; Fu et al., 2019). Furthermore, such table enumeration dilutes the positive labels quadratically, thus aggravating the classimbalanced problem. To model the task in a more concise way, Zheng et al. (2017) propose a N OVELTAGGING scheme, which represents relation and entity in one tag, so that the joint extraction can be solved by the well-studied sequence labeling approach. However, this tagging scheme cannot assign multiple tags to one token thus fail on overlapping triplets. The follow-on methods revise the tagging scheme to enable multi-pass sequence labeling (Takano"
2020.findings-emnlp.23,2020.acl-main.136,0,0.0997078,". As the exposure bias problem stems from the ordered left-to-right triplet decoding, we block the decoding of them from each other by removing the order of the triplet generation, thus the possible prediction error cannot propagate from triplet to triplet. Furthermore, because each triplet is generated by an independent decoding process, the decoding length has been extremely shortened, thus minimizes the effects of exposure bias. Our method differs from previous solution on exposure bias that we remove the order by structure decoding rather than random sampling (Tsai and Lee, 2019). CASREL (Wei et al., 2020) is a recently proposed two-step tagging method, which first finds all the head entities in the sentence then labels a relation-tail table for each head entity, which can also be seen as a UMTree decoder with a decoding length two. However, they overlook the data bias problem in NYT, which causing model unreliability and possible model bias. Note that our task is different from ONEIE (Lin et al., 2020), which models event extraction, entity span detection, entity type recognition and relation extraction in a Seq2Graph way. In contrast to ONEIE, JERE aims to extract only relation-entity triplet"
2020.findings-emnlp.23,P17-1113,0,0.504717,"asks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one entity, which solves the problem naturally (Zeng et al., 2018, 2019a,b; Nayak and Ng, 2019). Specifically, all existing Seq2Seq models pre-define a sequential order for the target triplets, e.g. triplet alphab"
2020.findings-emnlp.23,N16-1033,0,0.0501813,"udies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thu"
2020.findings-emnlp.23,P19-1518,0,0.0496315,"Missing"
2020.findings-emnlp.23,C18-1330,0,0.0233643,"lapping triplets problem. Although this paper introduces a problem that multi-token entities cannot be predicted, this problem has been solved by multiple follow-up papers (Zeng et al., 2019a; Nayak and Ng, 2019). However, there still remains a weakness in Seq2Seq models, i.e., the exposure bias, which has been overlooked. Exposure bias originates from the discrepancy between training and testing: Seq2Seq models use data distribution for training and model distribution for testing (Ranzato et al., 2015). Existing work mainly focuses on how to mitigate the information loss of arg max sampling (Yang et al., 2018, 2019; Zhang et al., 2019). Nam et al. (2017) notice that different orders affect the performance of the Seq2Seq models in Multi-Class Classification (MCC), and conduct thoroughly experiments on frequency order and topology order. In JERE, Zeng et al. (2019b) study additional rule-based triplet prediction orders, including alphabetical, shuffle and fix-unsort, and then propose a reinforcement learning framework to generate triplets in adaptive orders dynamically. Tsai and Lee (2019) first point out the unnecessary order causes exposure bias altering the performance in MCC, and they find that"
2020.findings-emnlp.23,C10-2160,0,0.0361426,"are the head and tail entities appearing in the text, and graduate from is the relation between these two entities. For supervised relation extraction, early studies focus on pipeline methods, which use an entity extractor to extract entities, and then classify the relations of entity pairs. These methods ignore the intrinsic interactions between these two subtasks and propagate classification errors through the tasks. Jointly entity and relation extraction (JERE) considers the subtask interaction (Roth and Yih, 2004; ∗ This denotes equal contribution. Ji and Grishman, 2005; Ji et al., 2005; Yu and Lam, 2010; Riedel et al., 2010; Sil and Yates, 2013; Li et al., 2014; Li and Ji, 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016; Miwa and Bansal, 2016; Gupta et al., 2016; Katiyar and Cardie, 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the predic"
2020.findings-emnlp.23,D19-1035,1,0.90365,"g phase, the UMTree uses auto-regressive decoding strategy. The decoder predicts the nodes layer by layer, where the prediction results of the previous layer are used as the input of the next time step separately, as shown in Fig. 3b. 3 3.1 Experiments Settings Dataset We evaluate our model on two datasets, NYT and DuIE1 . NYT (Riedel et al., 2010) is a English news dataset that is generated by distant supervision without manual annotation, which is widely used in JERE studies (Zheng et al., 2017; Zeng et al., 2018; Takanobu et al., 2018; Dai et al., 2019; Fu et al., 2019; Nayak and Ng, 2019; Zeng et al., 2019a,b; Chen et al., 2019; Wei et al., 2019). We use the same data split as CopyRE (Zeng et al., 2018). DuIE (Li et al., 2019) is a large-scale Chinese JERE dataset where sentences are from Baidu 1 https://ai.baidu.com/broad/introduction? dataset=dureader Baselines We compare the proposed model, Seq2UMTree, with strong baselines under the same hyperparameters, as follows: 1) CopyMTL (Zeng et al., 2019a) is a Seq2Seq model with copy mechanism, and the entities are found by multi-task learning. 2) WDec (Nayak and Ng, 2019) is a standard Seq2Seq model with dynamic masking, and decode the entity toke"
2020.findings-emnlp.23,P18-1047,1,0.937366,", 2017) , but they mainly exploit featurebased system or multi-task neural network, which can not capture inter-triplet dependency. NovelTagging (Zheng et al., 2017) integrates these two subtasks into one sequence labeling process, which assigns a single entity-relation tag to each token; when a token belongs to multiple relations, the prediction results will be incomplete. Instead of sequence labeling, Sequence-toSequence (Seq2Seq) models (Cho et al., 2014) are able to extract an entity multiple times, thus multiple relations can be assigned to one entity, which solves the problem naturally (Zeng et al., 2018, 2019a,b; Nayak and Ng, 2019). Specifically, all existing Seq2Seq models pre-define a sequential order for the target triplets, e.g. triplet alphabetical order, and then decode the triplet sequence according to the order autoregressively, which means the current triplet prediction relies on the previous output. For exmaple, in Figure 1, the triplet list is flattened to [Obama]-[graduate from][Columbia University]-[Obama]-[graduate from][Harvard Law School]... However, the autoregressive decoding of the Seq2Seq models introduces exposure bias problem which may severely reduce the performance."
2020.lrec-1.145,P19-1206,0,0.0266205,"e demonstrate that these three techniques can be combined straightforwardly in a single training pipeline. Through comprehensive experiments, we found that the first and second techniques provide additional gain while the last one did not. Keywords: discourse relation classification, BERT, explicit connective, implicit connective 1. Introduction Discourse relation classification, a task of recognizeing the semantic relations between two text spans, is beneficial for many NLP tasks including machine translation (Meyer et al., 2015), natural language inference (Pan et al., 2018), summarization (Isonuma et al., 2019), and sentiment analysis (Saito et al., 2019). In the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations have strong cues named discourse connectives such as “because” and “however”, while implicit relations lack these cues. For this reason, the recognition of implicit relations is the bottleneck of discourse relation classification (Xue et al., 2016; Dai and Huang, 2019). In this paper, we investigate how to improve the performance of implicit discourse relation classification by applyi"
2020.lrec-1.145,Q15-1024,0,0.070388,", 2008), which is the most popular and largest corpus of discourse relations in English. The annotation is done as another layer on the Wall Street Journal sections of the Penn Treebank. Each discourse relation consists of two text spans (arguments), a relation label and a discourse connective. Relation labels are organized as a 3-level hierarchy in the PDTB. Popular experimental settings are top-level one-versus-all binary classification (Pitler et al., 2009), top-level 4-way classification (Pitler et al., 2009; Rutherford and Xue, 2015), second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015), and modified second-level classification for the CoNLL 2015 Shared Task (Xue et al., 2015). We used top-level one-versus-all binary classification, top-level 4-way classification, and second-level 11-way classification in this experiment. In top-level one-versus-all binary classification and toplevel 4-way classification, we followed previous studies (Pitler et al., 2009; Rutherford and Xue, 2015); sections 2–20 as the training set, sections 0–1 as the development set, and sections 21–22 as the test set. Note that we used equal numbers of positive and negative examples in top-level one-versu"
2020.lrec-1.145,C18-1049,1,0.87664,"use Expansion.Alternative Expansion.Conjunction Expansion.Instantiation Expansion.List Expansion.Restatement Temporal.Asynchronous Temporal.Synchrony Micro Ave. Precision 0.00 50.21 ± 2.66 54.48 ± 1.58 0.00 34.64 ± 8.17 51.49 ± 2.08 59.96 ± 4.03 47.20 ± 14.41 48.08 ± 3.43 48.54 ± 4.87 0.00 51.86 ± 1.27 BERT+ECP+ICP Recall 0.00 48.76 ± 2.66 60.56 ± 1.58 0.00 28.33 ± 8.17 57.69 ± 2.08 58.21 ± 4.03 26.20 ± 14.41 46.51 ± 3.43 46.46 ± 4.87 0.00 51.86 ± 1.27 F-measure 0.00 49.43 ± 2.66 57.32 ± 1.58 0.00 30.64 ± 8.17 54.39 ± 2.08 58.97 ± 4.03 32.53 ± 14.41 47.17 ± 3.43 47.41 ± 4.87 0.00 51.86 ± 1.27 Kishimoto et al. (2018) (F1 ) 0.00 22.87 ± 2.9 48.13 ± 1.5 0.00 0.87 ± 2.6 44.66 ± 2.3 44.98 ± 3.7 22.03 ± 9.1 33.41 ± 2.7 21.40 ± 7.7 0.00 39.80 ± 0.9 Table 4: The performance of BERT in the Cross Validation dataset. • Bai and Zhao (2018) The state-of-the-art model in Comparison-versus-all classification. It is a deeper neural network model augmented by different grained text representations like character, sentence and sentence pair levels. • Shi and Demberg (2019a) The state-of-the-art model in Contingency-versus-all classification and 4-way classification in terms of accuracy. It is a sequence-to-sequence neural"
2020.lrec-1.145,D09-1036,0,0.139489,"2.0 (Prasad et al., 2008), which is the most popular and largest corpus of discourse relations in English. The annotation is done as another layer on the Wall Street Journal sections of the Penn Treebank. Each discourse relation consists of two text spans (arguments), a relation label and a discourse connective. Relation labels are organized as a 3-level hierarchy in the PDTB. Popular experimental settings are top-level one-versus-all binary classification (Pitler et al., 2009), top-level 4-way classification (Pitler et al., 2009; Rutherford and Xue, 2015), second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015), and modified second-level classification for the CoNLL 2015 Shared Task (Xue et al., 2015). We used top-level one-versus-all binary classification, top-level 4-way classification, and second-level 11-way classification in this experiment. In top-level one-versus-all binary classification and toplevel 4-way classification, we followed previous studies (Pitler et al., 2009; Rutherford and Xue, 2015); sections 2–20 as the training set, sections 0–1 as the development set, and sections 21–22 as the test set. Note that we used equal numbers of positive and negative examp"
2020.lrec-1.145,P19-1442,0,0.285253,"., 2018). BERT is a Transformer-based neural network architecture with specialized training procedures. It is pre-trained on large corpora like Wikipedia and BooksCorpus, and fine-tuned using task-specific datasets to transfer the pre-trained representations to downstream tasks. Although BERT is conceptually simple, it significantly outperforms previous stateof-the-art models in many natural language processing tasks such as reading comprehension (Devlin et al., 2018), syntactic analysis (Goldberg, 2019), and sentiment analysis (Xu et al., 2019). In implicit discourse relation classification, Nie et al. (2019) and Shi and Demberg (2019b) reported that BERT significantly outperformed previous state-ofthe-art models. Considering the broader context of research in this area, however, we expect additional improvement to be achieved with task-specific adaptation. Table 1 summarizes recent studies and techniques employed by them. It is evident that a comprehensive investigation is needed to answer the question: How is BERT best adapted to the task? We examine three adaptation methods in this paper. The first one is to exploit a large amount of unlabeled data from same domain text (referred to as Domain t"
2020.lrec-1.145,P18-1091,0,0.0200635,"diction at the fine-tuning step. We demonstrate that these three techniques can be combined straightforwardly in a single training pipeline. Through comprehensive experiments, we found that the first and second techniques provide additional gain while the last one did not. Keywords: discourse relation classification, BERT, explicit connective, implicit connective 1. Introduction Discourse relation classification, a task of recognizeing the semantic relations between two text spans, is beneficial for many NLP tasks including machine translation (Meyer et al., 2015), natural language inference (Pan et al., 2018), summarization (Isonuma et al., 2019), and sentiment analysis (Saito et al., 2019). In the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations have strong cues named discourse connectives such as “because” and “however”, while implicit relations lack these cues. For this reason, the recognition of implicit relations is the bottleneck of discourse relation classification (Xue et al., 2016; Dai and Huang, 2019). In this paper, we investigate how to improve the performance of implicit disco"
2020.lrec-1.145,P09-1077,0,0.0602679,"4. Experiments 4.1. Setup 4.1.1. Penn Discourse TreeBank We evaluated the performance of our models on the Penn Discourse TreeBank (PDTB) 2.0 (Prasad et al., 2008), which is the most popular and largest corpus of discourse relations in English. The annotation is done as another layer on the Wall Street Journal sections of the Penn Treebank. Each discourse relation consists of two text spans (arguments), a relation label and a discourse connective. Relation labels are organized as a 3-level hierarchy in the PDTB. Popular experimental settings are top-level one-versus-all binary classification (Pitler et al., 2009), top-level 4-way classification (Pitler et al., 2009; Rutherford and Xue, 2015), second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015), and modified second-level classification for the CoNLL 2015 Shared Task (Xue et al., 2015). We used top-level one-versus-all binary classification, top-level 4-way classification, and second-level 11-way classification in this experiment. In top-level one-versus-all binary classification and toplevel 4-way classification, we followed previous studies (Pitler et al., 2009; Rutherford and Xue, 2015); sections 2–20 as the training set, s"
2020.lrec-1.145,prasad-etal-2008-penn,0,0.365525,"Through comprehensive experiments, we found that the first and second techniques provide additional gain while the last one did not. Keywords: discourse relation classification, BERT, explicit connective, implicit connective 1. Introduction Discourse relation classification, a task of recognizeing the semantic relations between two text spans, is beneficial for many NLP tasks including machine translation (Meyer et al., 2015), natural language inference (Pan et al., 2018), summarization (Isonuma et al., 2019), and sentiment analysis (Saito et al., 2019). In the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations have strong cues named discourse connectives such as “because” and “however”, while implicit relations lack these cues. For this reason, the recognition of implicit relations is the bottleneck of discourse relation classification (Xue et al., 2016; Dai and Huang, 2019). In this paper, we investigate how to improve the performance of implicit discourse relation classification by applying BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018). BERT is a Transfo"
2020.lrec-1.145,P17-1093,0,0.448548,"ining and fine-tuning (additional pre-training). While their additional pre-training only covers explicit connective prediction (referred to as single-task pre-training in Table 1), but a more straightforward way is to do this together with BERT’s ordinary pre-training tasks (multi-task pre-training). The third technique is to exploit implicit connectives (referred to as implicit connective prediction in Table 1). In PDTB, annotators inserted an implicit connective between an implicit argument pair to facilitate consistent annotation. Implicit connectives were exploited by Zhou et al. (2010), Qin et al. (2017), and Shi and Demberg (2019a) for implicit discourse relation classification. A BERT-friendly formalization of the task is a multi-task learning at the finetuning step: BERT is trained to predict the implicit connective as well as the discourse relation. 1152 BERT BERT+ICP BERT+DT BERT+DT+ICP BERT+ECP BERT+ECP+ICP Rutherford and Xue (2015) Wu et al. (2017) Lei et al. (2018) Bai and Zhao (2018) Shi and Demberg (2019a) Nie et al. (2019) Shi and Demberg (2019b) Dai and Huang (2019) Using BERT ✓ ✓ ✓ ✓ ✓ ✓ Domain text Explicit connective prediction Implicit connective prediction fine-tuning DC DC f"
2020.lrec-1.145,N15-1081,0,0.168195,"to be achieved with task-specific adaptation. Table 1 summarizes recent studies and techniques employed by them. It is evident that a comprehensive investigation is needed to answer the question: How is BERT best adapted to the task? We examine three adaptation methods in this paper. The first one is to exploit a large amount of unlabeled data from same domain text (referred to as Domain text in Table 1). In the context of BERT, a simple way to do this is to perform additional pre-training on the domain text (Shi and Demberg, 2019b), but the domain can be more specific to this task. In fact, Rutherford and Xue (2015) automatically collected explicit argument pairs from an unlabeled corpus. In the pre-BERT era, they had no choice but to forcibly convert them into pseudo-training data for implicit discourse relation classification. Now, such explicit argument pairs can be used for BERT’s additional pre-training. The second technique is a more direct use of explicit argument pairs (referred to as explicit connective prediction in Table 1). Explicit argument pairs have, by definition, (explicit) discourse connectives. Training a model to predict explicit connectives may help it learn the discourse relations ("
2020.lrec-1.145,D19-1581,1,0.83503,"e combined straightforwardly in a single training pipeline. Through comprehensive experiments, we found that the first and second techniques provide additional gain while the last one did not. Keywords: discourse relation classification, BERT, explicit connective, implicit connective 1. Introduction Discourse relation classification, a task of recognizeing the semantic relations between two text spans, is beneficial for many NLP tasks including machine translation (Meyer et al., 2015), natural language inference (Pan et al., 2018), summarization (Isonuma et al., 2019), and sentiment analysis (Saito et al., 2019). In the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations have strong cues named discourse connectives such as “because” and “however”, while implicit relations lack these cues. For this reason, the recognition of implicit relations is the bottleneck of discourse relation classification (Xue et al., 2016; Dai and Huang, 2019). In this paper, we investigate how to improve the performance of implicit discourse relation classification by applying BERT (Bidirectional Encoder Representation"
2020.lrec-1.145,E17-2024,0,0.0125961,"as the test set. Note that we used equal numbers of positive and negative examples in top-level one-versus-all binary classification. In second-level 11-way classification, we report results in three different settings. The first setting, PDTB-Lin, is based on Lin et al. (2009); sections 2–21 as the training set, section 22 as the development set, and section 23 as the test set. The second one, PDTB-Ji, is following Ji and Eisenstein (2015); sections 2–20 as the training set, sections 0–1 as the development set, and sections 21–22 as the test set. The last one, Cross Validation, is following Shi and Demberg (2017); 10-fold cross validation using the whole corpus of sections 0–24. Table 2 shows a distribution of relation labels in the Cross Validation dataset. Note that although we tried to replicate the procedures described by Shi and Demberg (2017) as closely as possible, there remained slight differences in the discourse relation distribution. Dev 22 206 412 7 18 344 139 38 311 65 18 1,580 Test 22 207 411 7 18 344 140 39 310 65 17 1,5805 Table 2: The distribution of relation labels in the Cross Validation dataset. as the number of training epochs. Accordingly, we finetuned for 3 epochs. In this exper"
2020.lrec-1.145,W19-0416,0,0.125902,"ansformer-based neural network architecture with specialized training procedures. It is pre-trained on large corpora like Wikipedia and BooksCorpus, and fine-tuned using task-specific datasets to transfer the pre-trained representations to downstream tasks. Although BERT is conceptually simple, it significantly outperforms previous stateof-the-art models in many natural language processing tasks such as reading comprehension (Devlin et al., 2018), syntactic analysis (Goldberg, 2019), and sentiment analysis (Xu et al., 2019). In implicit discourse relation classification, Nie et al. (2019) and Shi and Demberg (2019b) reported that BERT significantly outperformed previous state-ofthe-art models. Considering the broader context of research in this area, however, we expect additional improvement to be achieved with task-specific adaptation. Table 1 summarizes recent studies and techniques employed by them. It is evident that a comprehensive investigation is needed to answer the question: How is BERT best adapted to the task? We examine three adaptation methods in this paper. The first one is to exploit a large amount of unlabeled data from same domain text (referred to as Domain text in Table 1). In the co"
2020.lrec-1.145,D19-1586,0,0.283032,"ansformer-based neural network architecture with specialized training procedures. It is pre-trained on large corpora like Wikipedia and BooksCorpus, and fine-tuned using task-specific datasets to transfer the pre-trained representations to downstream tasks. Although BERT is conceptually simple, it significantly outperforms previous stateof-the-art models in many natural language processing tasks such as reading comprehension (Devlin et al., 2018), syntactic analysis (Goldberg, 2019), and sentiment analysis (Xu et al., 2019). In implicit discourse relation classification, Nie et al. (2019) and Shi and Demberg (2019b) reported that BERT significantly outperformed previous state-ofthe-art models. Considering the broader context of research in this area, however, we expect additional improvement to be achieved with task-specific adaptation. Table 1 summarizes recent studies and techniques employed by them. It is evident that a comprehensive investigation is needed to answer the question: How is BERT best adapted to the task? We examine three adaptation methods in this paper. The first one is to exploit a large amount of unlabeled data from same domain text (referred to as Domain text in Table 1). In the co"
2020.lrec-1.145,speer-havasi-2012-representing,0,0.0117721,"le to revisit the notion of freely omissible discourse connective (Rutherford and Xue, 2015) to focus on explicit argument pairs from which knowledge can be straightforwardly transferred to implicit discourse relation classification. We plan to modify freely omissible discourse connectives to fit second-level 11-way classification. Another future direction is to adapt the BERT to incorporate external knowledge. Kishimoto et al. (2018) and Dai and Huang (2019) argued that the model for discourse classification could be further improved by incorporating external event knowledge like ConceptNet (Speer and Havasi, 2012) and temporal event knowledge. We plan to combine BERT with knowledge representation learning. 7. Bibliographical References Bai, H. and Zhao, H. (2018). Deep enhanced representation for implicit discourse relation recognition. In Proceedings of the 27th International Conference on Computational Linguistics, pages 571–583. Beltagy, I., Lo, K., and Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJC"
2020.lrec-1.145,P17-2042,0,0.211543,"automatically collected explicit argument pairs from an unlabeled corpus. In the pre-BERT era, they had no choice but to forcibly convert them into pseudo-training data for implicit discourse relation classification. Now, such explicit argument pairs can be used for BERT’s additional pre-training. The second technique is a more direct use of explicit argument pairs (referred to as explicit connective prediction in Table 1). Explicit argument pairs have, by definition, (explicit) discourse connectives. Training a model to predict explicit connectives may help it learn the discourse relations (Wu et al., 2017). We call this task explicit connective prediction. For BERT, Nie et al. (2019) inserted this task between pre-training and fine-tuning (additional pre-training). While their additional pre-training only covers explicit connective prediction (referred to as single-task pre-training in Table 1), but a more straightforward way is to do this together with BERT’s ordinary pre-training tasks (multi-task pre-training). The third technique is to exploit implicit connectives (referred to as implicit connective prediction in Table 1). In PDTB, annotators inserted an implicit connective between an impli"
2020.lrec-1.145,N19-1242,0,0.120172,"ectional Encoder Representations from Transformers) (Devlin et al., 2018). BERT is a Transformer-based neural network architecture with specialized training procedures. It is pre-trained on large corpora like Wikipedia and BooksCorpus, and fine-tuned using task-specific datasets to transfer the pre-trained representations to downstream tasks. Although BERT is conceptually simple, it significantly outperforms previous stateof-the-art models in many natural language processing tasks such as reading comprehension (Devlin et al., 2018), syntactic analysis (Goldberg, 2019), and sentiment analysis (Xu et al., 2019). In implicit discourse relation classification, Nie et al. (2019) and Shi and Demberg (2019b) reported that BERT significantly outperformed previous state-ofthe-art models. Considering the broader context of research in this area, however, we expect additional improvement to be achieved with task-specific adaptation. Table 1 summarizes recent studies and techniques employed by them. It is evident that a comprehensive investigation is needed to answer the question: How is BERT best adapted to the task? We examine three adaptation methods in this paper. The first one is to exploit a large amoun"
2020.lrec-1.145,K15-2001,0,0.186902,"tion is done as another layer on the Wall Street Journal sections of the Penn Treebank. Each discourse relation consists of two text spans (arguments), a relation label and a discourse connective. Relation labels are organized as a 3-level hierarchy in the PDTB. Popular experimental settings are top-level one-versus-all binary classification (Pitler et al., 2009), top-level 4-way classification (Pitler et al., 2009; Rutherford and Xue, 2015), second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015), and modified second-level classification for the CoNLL 2015 Shared Task (Xue et al., 2015). We used top-level one-versus-all binary classification, top-level 4-way classification, and second-level 11-way classification in this experiment. In top-level one-versus-all binary classification and toplevel 4-way classification, we followed previous studies (Pitler et al., 2009; Rutherford and Xue, 2015); sections 2–20 as the training set, sections 0–1 as the development set, and sections 21–22 as the test set. Note that we used equal numbers of positive and negative examples in top-level one-versus-all binary classification. In second-level 11-way classification, we report results in thr"
2020.lrec-1.145,K16-2001,0,0.172138,"l for many NLP tasks including machine translation (Meyer et al., 2015), natural language inference (Pan et al., 2018), summarization (Isonuma et al., 2019), and sentiment analysis (Saito et al., 2019). In the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations have strong cues named discourse connectives such as “because” and “however”, while implicit relations lack these cues. For this reason, the recognition of implicit relations is the bottleneck of discourse relation classification (Xue et al., 2016; Dai and Huang, 2019). In this paper, we investigate how to improve the performance of implicit discourse relation classification by applying BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018). BERT is a Transformer-based neural network architecture with specialized training procedures. It is pre-trained on large corpora like Wikipedia and BooksCorpus, and fine-tuned using task-specific datasets to transfer the pre-trained representations to downstream tasks. Although BERT is conceptually simple, it significantly outperforms previous stateof-the-art models in"
2020.lrec-1.145,C10-2172,0,0.174408,"task between pre-training and fine-tuning (additional pre-training). While their additional pre-training only covers explicit connective prediction (referred to as single-task pre-training in Table 1), but a more straightforward way is to do this together with BERT’s ordinary pre-training tasks (multi-task pre-training). The third technique is to exploit implicit connectives (referred to as implicit connective prediction in Table 1). In PDTB, annotators inserted an implicit connective between an implicit argument pair to facilitate consistent annotation. Implicit connectives were exploited by Zhou et al. (2010), Qin et al. (2017), and Shi and Demberg (2019a) for implicit discourse relation classification. A BERT-friendly formalization of the task is a multi-task learning at the finetuning step: BERT is trained to predict the implicit connective as well as the discourse relation. 1152 BERT BERT+ICP BERT+DT BERT+DT+ICP BERT+ECP BERT+ECP+ICP Rutherford and Xue (2015) Wu et al. (2017) Lei et al. (2018) Bai and Zhao (2018) Shi and Demberg (2019a) Nie et al. (2019) Shi and Demberg (2019b) Dai and Huang (2019) Using BERT ✓ ✓ ✓ ✓ ✓ ✓ Domain text Explicit connective prediction Implicit connective prediction"
2020.lrec-1.281,Y18-1026,1,0.819791,"Missing"
2020.lrec-1.281,W19-6704,1,0.241475,"texts from social media to infer author personality (e.g., Golbeck et al., 2011; Park et al., 2015; Plank and Hovy, 2015; Schwartz et al., 2013). This NLP approach requires texts for personality inference. We, however, need what personality a person is described with a personality descriptor. Accordingly, we developed a personality dictionary where word entries have weights that can be used to infer the five traits from each entry (Iwai et al., 2020). First, we developed a 20-item Big Five questionnaire, Trait Descriptors Personality Inventory (TDPI), based on the responses of 17,591 people (Iwai et al., 2019b). Each item contains a personality word obtained from English personality adjectives, using word embeddings and phrase-based statistical machine translation. We collected 527 personality words from the seeds of 116 personality descriptors obtained in the development process (Iwai et al., 2017; Iwai, Kumada et al., 2018; Iwai, Kawahara et al., 2019b), using word embeddings trained with 200 million Japanese sentences. Furthermore, we conducted a websurvey on 1,938 participants to evaluate their personality based on each personality descriptor in addition to TDPI (Iwai, Kawahara et al., 2019b),"
2020.lrec-1.281,2020.lrec-1.379,1,0.872345,"nfirmed in Japanese (Kashiwagi et al., 2005; Oshio et al., 2014). Meanwhile, NLP researchers have used Big Five to develop language models from a certain volume of texts from social media to infer author personality (e.g., Golbeck et al., 2011; Park et al., 2015; Plank and Hovy, 2015; Schwartz et al., 2013). This NLP approach requires texts for personality inference. We, however, need what personality a person is described with a personality descriptor. Accordingly, we developed a personality dictionary where word entries have weights that can be used to infer the five traits from each entry (Iwai et al., 2020). First, we developed a 20-item Big Five questionnaire, Trait Descriptors Personality Inventory (TDPI), based on the responses of 17,591 people (Iwai et al., 2019b). Each item contains a personality word obtained from English personality adjectives, using word embeddings and phrase-based statistical machine translation. We collected 527 personality words from the seeds of 116 personality descriptors obtained in the development process (Iwai et al., 2017; Iwai, Kumada et al., 2018; Iwai, Kawahara et al., 2019b), using word embeddings trained with 200 million Japanese sentences. Furthermore, we"
2020.lrec-1.281,C08-1111,0,0.0753523,"Missing"
2020.lrec-1.379,W19-6704,1,0.356478,"e users’ personalities by using a certain amount of texts (e.g., Golbeck et al., 2011; Nasukawa et al., 2016; Nasukawa and Kamijo, 2017; Park et al., 2015; Plank and Hovy, 2011; Schwartz et al., 2013). They developed models to infer the self-evaluated personalities. However, 3103 this approach does not provide information on what words represent the personality or the weights for personality inference. Our goal was to acquire personality words that allow computers to infer an individual’s personality from a single personality word. 2.3 An Incorporative Approach Recently, Iwai, Kawahara et al. (2019) introduced a new Big Five questionnaire in Japanese named as TraitDescriptors Personality Inventory (TDPI). To the best of our knowledge, it is the only personality measurement developed by NLP techniques such as word embeddings and phrase-based statistical machine translation. It was constructed based on the responses of more than 40,000 Japanese people (Iwai et al., 2017, 2018; Ueda et al., 2016). In addition, Iwai, Kawahara et al. (2019) demonstrated reliability and the five-factor structure replications among different samples. However, it only included 20 items, and one personality word"
2020.lrec-1.379,2020.lrec-1.281,1,0.720122,"). …車が、…直進車を妨害してる。…なぜ かこういう自分勝手な運転手が増えるん だよなぁ。… a car, …., is obstructing another straight traveling car. Somehow such selfish drivers increase. If computers know that a car obstructing another straight traveling car is selfish and disconscientious, computers can predict that the selfish driver causes similar disconscientious driving behaviors such as sudden turns at the corner of an intersection or lane changes without any signals. By using the personality dictionary and the Driving Behavior and Subjectivity corpus, we acquired social knowledge about personality and driving-related behavior (Iwai et al., 2020). 6. Conclusions In this study, we developed a Japanese personality dictionary that comprises two sub-dictionaries, using psychological methods and statistical analyses. To the best of our knowledge, this is the first Japanese language resource developed based on theories and methodologies of personality psychology and has such weights. Furthermore, it is the only Japanese personality dictionary that is available for NLP researchers. The interests in human-machine interactions such as virtual agents, chat bots, and social robots are growing. Our dictionary and methodology will inspire those st"
2020.lrec-1.449,abdelali-etal-2014-amara,0,0.066042,", 2014) focused on collecting data from AMARA platform (Jansen et al., 2014). They usually aim at European and BRIC languages, such as German, Polish, and Russian. Using automatic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel"
2020.lrec-1.449,P17-1042,0,0.0121265,"necessarily hold for every language pair. We thus encourage researchers to try iterative refinement of training data in their own experimental settings. 5.6. Indirect Assessment of the Created In-Domain Data In this section, we evaluate the superiority of our sentence alignment method, presented in Section 3.2. (henceforth, MT+CS), over other methods, extrinsically, through MT performance. The following two similarity measures were additionally implemented and tested. Unsupervised: Cosine similarity over the cross-lingual sentence embeddings, learned by an unsupervised method, called VecMap (Artetxe et al., 2017).19 18 Compare the pairs (A1, A2), (A2, A3), (A1, A10), (B2, B3), (C3, C4), (C10, C11), (D3, D4), (D10, D11), and (A1, E14). 19 3646 https://github.com/artetxem/vecmap # of aligned lines Unsupervised MT+BLEU MT+CS (B16 in Table 6) 40,452 42,672 40,770 BLEU Ja→En En→Ja 4.0 2.8 6.2 4.3 3.4 6.4 Table 9: BLEU scores achieved with only Coursera parallel data extracted by different similarity measures. Acknowledgments This work was carried out when Haiyue Song was taking up an internship at NICT, Japan. A part of this work was conducted under the program “Research and Development of Enhanced Multili"
2020.lrec-1.449,L18-1528,0,0.0299661,"Missing"
2020.lrec-1.449,2012.eamt-1.60,0,0.254145,"lecture subtitles into other languages, including English, is also an important challenge. The TraMOOC project (Kordoni et al., 2015) aims at improving the accessibility of European languages through MT. They focus on collecting translations of lecture subtitles and constructing MT systems for eleven European and BRIC languages. However, the amount of parallel resources involving other languages, such as Chinese and Japanese, are still quite low. Subtitle translation falls under spoken language translation. Past studies in spoken language translation mainly focused on subtitles for TED talks (Cettolo et al., 2012). Even though the parallel data in this domain should be exploitable for lectures translation to some degree, university lectures are devoted mainly for educational purposes, and the subtle differences in domains may hinder translation quality. To obtain high-quality parallel data, professional translators are typically employed to translate. However, the cost is often very high and thus using this way to produce large quantities of parallel data is economically infeasible, especially for universities and non-profit organizations. In the case of online lectures and talks, subtitles are often t"
2020.lrec-1.449,W19-5435,0,0.0614599,"Missing"
2020.lrec-1.449,P17-2061,1,0.798899,"l., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translations, one can use crowdsourcing services ("
2020.lrec-1.449,D19-1146,1,0.865738,"TC→C flow. Bold indicates the initial training, and red-, blue-, and grey-colored cells mean inflation , deflation , and replacement of training data, respectively. into multiple stages where each stage uses data from different domains to maximize the impact of the domain-specific training data. As we have larger parallel corpora from other domains, such as TED (0.2M pairs; non-educational spoken domain) and ASPEC (3.0M pairs; scientific domain), we can leverage domain adaptation techniques, such as finetuning and mixed fine-tuning (Chu et al., 2017). Furthermore, Imankulova et al. (2019) and Dabre et al. (2019) showed that training in multiple stages where each stage contains different proportions of various types of training data leads to the best results. Following them, we decided to conduct an extensive experiment with multistage training with different proportions of training data from different domains at each stage. 5.2. Datasets As in the previous section, we performed Juman++ and NLTK tokenization for Japanese and English, respectively. Henceforth, we refer to the ASPEC training data of 1.0 million lines as “A,” the TED training data of 0.2 million lines as “T,” and the Coursera training da"
2020.lrec-1.449,L18-1545,0,0.0167994,"he best possible sentence alignments for Coursera data were already found, owing to our algorithm, similarity measure, and/or the initial MT system trained only on ASPEC and TED. • Leveraging out-of-domain data through multistage training is invaluable. • Gradually inflating the data starting from out-ofdomain corpus and adding the in-domain corpus at the end should give the best possible translation quality. One peculiarity of our results is that the BLEU scores for the Ja→En task were significantly higher than the En→Ja task, which is a reversal of a general tendency for this language pair (Imamura and Sumita, 2018; Nakazawa et al., 2019), even though the BLEU scores in different languages are not directly comparable. Table 7 gives a comparison of three different translation tasks. Upon manual investigation, we identified that the En→Ja translations in TED and Coursera tasks tend to be much shorter than the reference translation, receiving around 0.7 brevity penalty. When we tuned the length penalty for decoding on the development set, we observed 0.5 to 1.0 point BLEU gains on the test set Iterative Refinement of Aligned Data Having obtained a better MT system than the initial one, we can iterate the w"
2020.lrec-1.449,W19-6613,1,0.842963,"1) sub-paths of the A→AT→ATC→TC→C flow. Bold indicates the initial training, and red-, blue-, and grey-colored cells mean inflation , deflation , and replacement of training data, respectively. into multiple stages where each stage uses data from different domains to maximize the impact of the domain-specific training data. As we have larger parallel corpora from other domains, such as TED (0.2M pairs; non-educational spoken domain) and ASPEC (3.0M pairs; scientific domain), we can leverage domain adaptation techniques, such as finetuning and mixed fine-tuning (Chu et al., 2017). Furthermore, Imankulova et al. (2019) and Dabre et al. (2019) showed that training in multiple stages where each stage contains different proportions of various types of training data leads to the best results. Following them, we decided to conduct an extensive experiment with multistage training with different proportions of training data from different domains at each stage. 5.2. Datasets As in the previous section, we performed Juman++ and NLTK tokenization for Japanese and English, respectively. Henceforth, we refer to the ASPEC training data of 1.0 million lines as “A,” the TED training data of 0.2 million lines as “T,” and"
2020.lrec-1.449,W17-3204,0,0.0132128,"https://github.com/shyyhs/CourseraParallelCorpusMining http://mooc.org 4 https://www.coursera.org 5 https://iversity.org 6 https://wit3.fbk.eu/mt.php?release=2017-01-ted-test 7 https://www.opensubtitles.org 3 2.3. Domain Adaptation for Neural Machine Translation At present, neural machine translation (NMT) is known to give higher quality of translation. To train a sequence-tosequence model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addr"
2020.lrec-1.449,W15-4935,0,0.065899,"Missing"
2020.lrec-1.449,L16-1003,0,0.0411224,"Missing"
2020.lrec-1.449,L18-1236,0,0.0116466,"ic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domain parallel document"
2020.lrec-1.449,2015.iwslt-evaluation.11,0,0.0224968,"model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpor"
2020.lrec-1.449,D17-1156,0,0.0170107,"ing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translations, one can use crowdsourcing services (Behnke et al., 2018). However, this can be extremely timeconsuming if not expensive. Previous research (Abdelali et 2 al., 2014) focused on collecting data from AMARA platform (Jansen et al., 2014). They usually aim at European and BRIC languages, such as German, Polish, and R"
2020.lrec-1.449,L16-1350,1,0.819808,"data. Dataset English Mean / Median / s.d. Japanese Mean / Median / s.d. ASPEC TED Coursera 25.4 / 23 / 11.4 20.4 / 17 / 13.9 21.1 / 19 / 11.1 27.5 / 20 / 12.0 19.8 / 16 / 14.1 22.2 / 20 / 11.8 Table 2: BLEU score for Ja→En on TED test set. A, T, and AT respectively stand for ASPEC training set, TED training set, and their balanced mixture. “→” indicates that the model trained on the left-hand side data is fine-tuned on the right-hand side data. domain. However, given its small size, it can lead to only an unreliable MT system. Therefore, we decided to use a larger out-of-domain ASPEC corpus (Nakazawa et al., 2016)13 to build a better MT system. Table 1 gives the statistics of the ASPEC and TED corpora that we used to train our initial MT system. We compared fine-tuning and mixed fine-tuning approaches proposed by Chu et al. (2017). When performing mixed fine-tuning on the concatenation of both two corpora, the TED corpus was oversampled to match the size of the ASPEC corpus. We trained our NMT models using tensor2tensor with its default hyper-parameters. Refer to Section 5.3. for further details on training configurations. So far, we do not have a test set for the target domain, i.e., Coursera. We ther"
2020.lrec-1.449,P02-1040,0,0.109811,"o translate one side into the other language using an MT system (Sennrich and Volk, 2010). To train such a system, we can leverage any existing parallel data in related or even distant domains. The MT system should generate translations as accurately as possible. In practice, domain adaptation techniques (Chu et al., 2017) are most useful in training an accurate MT system. 3.2.2. Similarity Measure The key component in the DP algorithm is the matching function, i.e., similarity measure in our context. Existing methods, such as that in Sennrich and Volk (2010), used sentence-level BLEU scores (Papineni et al., 2002) of machine-translated source sentence against the actual target language sentence as their similarity score: formally, Sim BLEU (fi , ej ) = BLEU (MT (fi ), ej ), (1) where fi and ej are the i-th sentence in the source document and the j-th sentence in the target document, respectively. However, due to the lack of in-domain data, MT system can give only translations of low quality and thus the BLEU scores can be misleading, especially for distant language pairs, such as Japanese and English. An alternative way is to directly compute cosine similarity of a given sentence pair (Bouamor and Sajj"
2020.lrec-1.449,N16-1110,0,0.0201253,"Missing"
2020.lrec-1.449,2010.amta-papers.14,0,0.10192,"such as German, Polish, and Russian. Using automatic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the"
2020.lrec-1.449,W11-4624,0,0.0319982,"nd Russian. Using automatic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domai"
2020.lrec-1.449,P16-1009,0,0.220059,"2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translation"
2020.lrec-1.449,D19-1168,0,0.0189494,"erage similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection procedure for the Japanese–English setting."
2020.lrec-1.449,N19-1209,0,0.0109303,"arallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translations, one can use crowdsourcing services (Behnke et al., 2018). However, this can be extremely timeconsuming if not expensive. Previous research (Abdelali et 2 al., 2014) focused on collecting data from AMARA platform (Jansen et al., 2014). They usually aim at European and BRIC languages, such as German, Polish, and Russian. Using automatic"
2020.lrec-1.449,tiedemann-2012-parallel,0,0.0507603,"://iversity.org 6 https://wit3.fbk.eu/mt.php?release=2017-01-ted-test 7 https://www.opensubtitles.org 3 2.3. Domain Adaptation for Neural Machine Translation At present, neural machine translation (NMT) is known to give higher quality of translation. To train a sequence-tosequence model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006"
2020.lrec-1.449,L16-1559,0,0.333293,"data, professional translators are typically employed to translate. However, the cost is often very high and thus using this way to produce large quantities of parallel data is economically infeasible, especially for universities and non-profit organizations. In the case of online lectures and talks, subtitles are often translated by crowdsourcing (Behnke et al., 2018) which involves non-professional translators. The resulting translation can thus be often inaccurate and quality control is indispensable. There are many automatic ways to find parallel sentences from roughly parallel documents (Tiedemann, 2016).1 In particular, MT-based approaches are quite desirable because of their simplicity and it is possible to use existing translation models to extract additional parallel data. However, using an MT system trained on data from another domain can give unreliable translations which can lead to parallel data of low quality. In this paper, we propose a new method which combines machine translation and similarities of sentence vector representations to automatically align sentences between roughly aligned document pairs. As we are interested in educational lectures translation, we focus on extractin"
2020.lrec-1.449,C73-2019,0,0.51231,"Missing"
2020.lrec-1.449,C94-2175,0,0.762098,"at can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domain parallel documents (top-left), such as those available from Coursera, and out-of-domain parallel sentences (bottom-right). We give details about the way we pre"
2020.lrec-1.449,W18-1819,0,0.0295138,"ection 4.4., we decided to focus on the training schedule A→AT→ATC→TC→C, where in each stage we use datasets more similar with indomain dataset (Wang et al., 2019a), and thoroughly evaluated all of its sub-paths. We also used T and AC for some contrastive experiments. 5.3. Settings for MT We created a shared sub-word vocabulary for Japanese and English from ASPEC and TED training set using BPE (Sennrich et al., 2016b) with roughly 32k merge operations. This vocabulary was used for all experiments, even when a model is trained only on C. We trained NMT models using the tensor2tensor framework (Vaswani et al., 2018)16 with its default “transformer big” setting, such as dropout=0.2, attention dropout=0.1, optimizer=adam with beta1=0.9, beta2=0.997. We used eight Tesla V100 32GB GPUs with batch size of 4,096 sub-word tokens, and early-stopping on approximate BLEU score computed on the development set: the training process stops when the score shows no gain larger than 0.1 for 10,000 steps. When fine-tuning the model on a different dataset, we resumed the training process from the last checkpoint in the previous stage. In the decoding step, we used the average of the last 10 checkpoints, and decoded the tes"
2020.lrec-1.449,P19-1116,0,0.0265469,"pairs in the descending order of the average similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection proc"
2020.lrec-1.449,P19-1583,0,0.0198191,"allel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domain parallel documents (top-left), such as those available from Coursera, and out-of-domain parallel"
2020.lrec-1.449,P19-1123,0,0.0830708,"ding order of the average similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection procedure for the Japan"
2020.lrec-1.449,P19-1624,0,0.094315,"ding order of the average similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection procedure for the Japan"
2020.lrec-1.449,D16-1163,0,0.0201952,"eral domains. 2.2. https://github.com/shyyhs/CourseraParallelCorpusMining http://mooc.org 4 https://www.coursera.org 5 https://iversity.org 6 https://wit3.fbk.eu/mt.php?release=2017-01-ted-test 7 https://www.opensubtitles.org 3 2.3. Domain Adaptation for Neural Machine Translation At present, neural machine translation (NMT) is known to give higher quality of translation. To train a sequence-tosequence model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over"
2020.lrec-1.454,P05-1066,0,0.299925,"Missing"
2020.lrec-1.454,D19-1146,1,0.794451,"dels, code and annotated data as resources for reproducibility and public use. 2. Related Work Pre-training based approaches are essentially transfer learning approaches where we leverage an external source of data to train a model whose components can be used for NLP tasks which do not have abundant data. In the context of NMT, cross-lingual transfer (Zoph et al., 2016) was shown to be most effective to improve Hausa-English translation when a pre-trained French-English NMT model was fine-tuned on Hausa-English data. While this work focused on strongly pre-training the English side decoder, (Dabre et al., 2019) showed that pre-training the encoder is also useful through experiments on fine-tuning an English–Chinese model on a small multi-parallel English–X (7 Asian languages) data. All these works rely on bilingual corpora but our focus is on leveraging monolingual corpora that are orders of magnitude larger than bilingual corpora. Pre-trained models such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018), XLNET (Yang et al., 2019) and GPT (Radford, 2018) have proved very useful for 2 https://github.com/Mao-KU/JASS tasks such as Text Understanding, but have a limited application to NMT, as th"
2020.lrec-1.454,N19-1423,0,0.182544,"monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-sp"
2020.lrec-1.454,W17-4123,0,0.030112,"show that linguistically motivated pre-training can be complementary to MASS. Our research is motivated by previous research (Kawahara et al., 2017) for Japanese NLP which showed that linguistic annotations from the syntactic analyzers such as Juman (Morita et al., 2015) and KNP (Kurohashi et al., 1994) are extremely important. Pre-ordering consists of pre-processing a sentence so that its word-order is more similar to that of its expected translation. It has been a popular technique for Statistical Machine Translation since the early work of (Collins et al., 2005). Although initial research (Du and Way, 2017) had concluded that pre-ordering had limited usefulness for NMT, it has been shown more recently that it can improve translation quality, especially in the case of low-resource languages. (Murthy et al., 2019) showed that pre-ordering English to Indic language word order is beneficial when performing transfer learning via fine-tuning. (Zhou et al., 2019) showed that leveraging structural knowledge for creating the psuedo Japanese-ordered English by pre-ordering English from SVO to SOV improves Japanese–English translation. Our work will try to incorporate similar ideas directly in the pre-trai"
2020.lrec-1.454,D18-1045,0,0.0356188,"Missing"
2020.lrec-1.454,W18-2703,0,0.0338408,"the quality of automatic translation over previous approaches such as Statistical Machine Translation (Koehn, 2004). One of the drawbacks of NMT is that it requires large parallel corpora for training robust and high quality translation models. This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentia"
2020.lrec-1.454,W14-7008,0,0.0272325,"We expect that this will let the system learn the structure of Japanese language, as well as prepare it for the reordering operation it will have to perform when translating to a language with different grammar. 4.2.1. Bunsetsu-based Reordering We first define here a simple process for re-ordering a (typically SOV) Japanese sentence into a “SVO-ordered Japanese” pseudo-sentence. We will then use this reordered sentence in section 4.2.2. for our BRSS pre-training. There exist several previous works about reordering a SOVordered sentence to a SVO-ordered sentence (Katz-Brown and Collins, 2008; Hoshino et al., 2014). In our case, 3685 : Bunsetsu (a) Origin (b) MASS (c) BMASS : Chunking Signal Token : Token ラブライブ は 、 三 つ の プロジェクト に よって 構成 さ れて いる 。 Src. ラブライブ は 、 三 [M] [M] [M] [M] [M] [M] [M] れて いる 。 Tgt. [M] [M] [M] [M] つ の プロジェクト に よって 構成 [M] さ [M] [M] [M] Src. ラブライブ は [M] [M] [M] プロジェクト に よって [M] [M] [M] [M] [M] Tgt. [M] [M] [M] 三 つ の [M] [M] [M] 構成 さ れて いる 。 Src. ラブライブ は 、 構成 いる よって 三 つ の 。 Tgt. ラブライブ は 、 三 つ の Src. LoveLive (theme) , make Tgt. LoveLive (theme) , three _ 、 さ れて プロジェクト に (d) BRSS English version of BRSS プロジェクト _ (passive) of _ project に よって based on 構成 project based make さ れて on three"
2020.lrec-1.454,W19-6613,1,0.911441,"ives, which we dub MASS+JASS in the following sections. 5. Experimental Settings In this section, we evaluate our pre-training methods on 4 translation directions: Japanese-to-English (Ja-En), English-to-Japanese (En-Ja), Japanese-to-Russian (Ja-Ru) and Russian-to-Japanese (Ru-Ja). Specifically, we monitor the performance of our pre-training methods on both simulated low-resource and high-resource scenarios involving ASPEC Japanese–English translation (Nakazawa et al., 2015). We also test our methods on a realistic low-resource scenario involving News Commentary Japanese–Russian translation3 (Imankulova et al., 2019). 5.1. Datasets and Pre-processing from the official WMT monolingual training data5 for pretraining. Each side of the parallel data used in fine-tuning is also incorporated into the monolingual data for pretraining. Specifically, for Japanese and English, 3M sentences from each side of the parallel data is added to the monolingual data while for Japanese and Russian, 10K sentences from each side of the parallel data is also used in pre-training. This results in 50M monolingual sentences for Japanese and English, and 45M monolingual sentences for Japanese and Russian. Given that our pre-trainin"
2020.lrec-1.454,W17-6301,1,0.851346,"er. Pre-training schemes more suitable to NMT have been proposed by (Lample and Conneau, 2019), (Ren et al., 2019) and (Song et al., 2019). In particular, (Song et al., 2019) obtained state-of-the-art results with their “MASS” pre-training scheme. MASS allows for the simultaneous pre-training of the encoder and decoder and hence is the most useful for NMT. However, MASS does not consider the linguistic properties of language when pretraining whereas our objective is to show that linguistically motivated pre-training can be complementary to MASS. Our research is motivated by previous research (Kawahara et al., 2017) for Japanese NLP which showed that linguistic annotations from the syntactic analyzers such as Juman (Morita et al., 2015) and KNP (Kurohashi et al., 1994) are extremely important. Pre-ordering consists of pre-processing a sentence so that its word-order is more similar to that of its expected translation. It has been a popular technique for Statistical Machine Translation since the early work of (Collins et al., 2005). Although initial research (Du and Way, 2017) had concluded that pre-ordering had limited usefulness for NMT, it has been shown more recently that it can improve translation qu"
2020.lrec-1.454,W04-3250,0,0.359057,"cantly surpass the individual methods indicating their complementary nature. We will release our code, pre-trained models and bunsetsu annotated data as resources for researchers to use in their own NLP tasks. Keywords: pre-training, neural machine translation, bunsetsu, low resource 1. Introduction Encoder-decoder based neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015), and in particular, the Transformer model (Vaswani et al., 2017) have led to a large jump in the quality of automatic translation over previous approaches such as Statistical Machine Translation (Koehn, 2004). One of the drawbacks of NMT is that it requires large parallel corpora for training robust and high quality translation models. This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of th"
2020.lrec-1.454,D15-1276,1,0.94624,"al., 2019; Zhou et al., 2019) have shown that such information could improve results. However, because NMT models are end-to-end sequence to sequence models, the manner in which such linguistic hints should be provided is not always clear. In this paper, we argue that pre-training provides an ideal framework both for leveraging monolingual data and improving NMT models with linguistic information. Our setting focuses on the translation between language pairs involving Japanese. Japanese is a language for which very high quality syntactic analyzers have been developed (Kurohashi et al., 1994; Morita et al., 2015). On the other hand, large parallel corpora involving Japanese exist only for a few language pairs and domains. As such it is critical to leverage both monolingual data and the syntactic analyses of Japanese for optimal translation quality. Our pre-training approach is inspired by MASS, but with more linguistically motivated tasks. In particular, we add syntactic constraints to the sentence-masking process of MASS and dub the resulting task BMASS1 . We also add a linguistically-motivated reordering task that we dub BRSS (Bunsetsu Reordering Sequence to Sequence). We combine these two tasks to"
2020.lrec-1.454,N19-1387,0,0.0964144,"P tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-specific information. Works such as (Sennrich and Haddow, 2016; Murthy et al., 2019; Zhou et al., 2019) have shown that such information could improve results. However, because NMT models are end-to-end sequence to sequence models, the manner in which such linguistic hints should be provided is not always clear. In this paper, we argue that pre-training provides an ideal framework both for leveraging monolingual data and improving NMT models with linguistic information. Our setting focuses on the translation between language pairs involving Japanese. Japanese is a language for which very high quality syntactic analyzers have been developed (Kurohashi et al., 1994; Morita et"
2020.lrec-1.454,W15-5001,1,0.762132,"e” monolingual data with MASS objective. We can also consider using Japanese monolingual data with a combination of BMASS, BRSS and MASS objectives, which we dub MASS+JASS in the following sections. 5. Experimental Settings In this section, we evaluate our pre-training methods on 4 translation directions: Japanese-to-English (Ja-En), English-to-Japanese (En-Ja), Japanese-to-Russian (Ja-Ru) and Russian-to-Japanese (Ru-Ja). Specifically, we monitor the performance of our pre-training methods on both simulated low-resource and high-resource scenarios involving ASPEC Japanese–English translation (Nakazawa et al., 2015). We also test our methods on a realistic low-resource scenario involving News Commentary Japanese–Russian translation3 (Imankulova et al., 2019). 5.1. Datasets and Pre-processing from the official WMT monolingual training data5 for pretraining. Each side of the parallel data used in fine-tuning is also incorporated into the monolingual data for pretraining. Specifically, for Japanese and English, 3M sentences from each side of the parallel data is added to the monolingual data while for Japanese and Russian, 10K sentences from each side of the parallel data is also used in pre-training. This"
2020.lrec-1.454,L16-1350,1,0.853945,"[BM ASS] or [RSS] and a language token [Ja], [En], or [Ru]. This ensures that the model learns to distinguish between different pre-training objectives and languages. 5.2. For the NMT model, we experiment with a Transformer (Vaswani et al., 2017) having 6 layers for both the encoder and the decoder. We implement our approaches on top of the OpenNMT9 transformer implementation. We use both the monolingual data and parallel data for pretraining and the parallel data for fine-tuning. Refer to Table 1 for an overview. 5.1.1. Parallel Data We use scientific abstracts domain ASPEC parallel corpus (Nakazawa et al., 2016) for Japanese–English translation and the news commentary domain JaRuNC parallel corpus (Imankulova et al., 2019) for Japanese–Russian translation. 5.1.2. Monolingual data We use monolingual data containing 22M Japanese, 22M English and 22M Russian sentences randomly sub-sampled from Common Crawl dataset and News crawl4 dataset 3 Neither Japanese nor Russian are low-resource languages, but Ja-Ru can be regarded as a low-resource language pair because of the limited amount of the parallel data. 4 The pre-training will be very effective if the domains of the pre-training and fine-tuning dataset"
2020.lrec-1.454,P02-1040,0,0.1071,"ent our pre-training methods and fine-tuning using the Transformer-big setting, which consists of a 6-layer encoder and a 6-layer decoder, with the length of 1024 for hidden size, the length of 4096 for feedforward size, dropout rate of 0.3 and attention heads of 16. A learning-rate of 10−4 is used both for pre-training and fine-tuning, and all the pre-training tasks are implemented on 8 TITAN X (Pascal) GPU cards until convergence with a batch-size of 2048 for each GPU while single GPU is used for fine-tuning. The checkpoint with the highest accuracy is selected for fine-tuning. We use BLEU (Papineni et al., 2002) to implement the evaluation. We do early stopping if no improvement on development-set within 5 checkpoints, and the checkpoint with the best BLEU performance on development-set is selected for evaluation. For multi-task pre-training, data is randomly shuffled so that even in each mini-batch, different pre-training objectives will appear, corresponding to a real joint pre-training. We evaluate the statistical significance of our BLEU scores by bootstrap resampling (Koehn, 2004). 5.3. We compare these models with baselines which do not use pre-training. 6. Results & Analysis We now give the re"
2020.lrec-1.454,N18-1202,0,0.0704212,"re much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-specific information. Works su"
2020.lrec-1.454,N18-2084,0,0.0259756,". This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence mo"
2020.lrec-1.454,D19-1071,0,0.118197,"Missing"
2020.lrec-1.454,W16-2209,0,0.153824,"(Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-specific information. Works such as (Sennrich and Haddow, 2016; Murthy et al., 2019; Zhou et al., 2019) have shown that such information could improve results. However, because NMT models are end-to-end sequence to sequence models, the manner in which such linguistic hints should be provided is not always clear. In this paper, we argue that pre-training provides an ideal framework both for leveraging monolingual data and improving NMT models with linguistic information. Our setting focuses on the translation between language pairs involving Japanese. Japanese is a language for which very high quality syntactic analyzers have been developed (Kurohashi et"
2020.lrec-1.454,P16-1009,0,0.464907,"e led to a large jump in the quality of automatic translation over previous approaches such as Statistical Machine Translation (Koehn, 2004). One of the drawbacks of NMT is that it requires large parallel corpora for training robust and high quality translation models. This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-lik"
2020.lrec-1.454,P16-1162,0,0.49472,"e led to a large jump in the quality of automatic translation over previous approaches such as Statistical Machine Translation (Koehn, 2004). One of the drawbacks of NMT is that it requires large parallel corpora for training robust and high quality translation models. This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-lik"
2020.lrec-1.454,D16-1160,0,0.0258231,"1 ≤ len(x) and len(x) is the number of tokens in sentence x. We denote by xC the masked sequence where tokens in positions from p1 to p2 , p3 to p4 and so on until pn to pn+1 in x are replaced by a special token [M ]. x!C is the invert masked sequence where tokens in positions other than the aforementioned fragments are replaced by the mask token [M ]. MASS is a pre-training objective that predicts the masked fragments in x using an encoder-decoder model where xC is the input to the encoder and x!C is the reference for the decoder. The log likelihood objective function is: based pre-training (Zhang and Zong, 2016). It is a combination of two sub-methods, BMASS (Bunsetsu-based MAsked Sequence to Sequence pre-training) and BRSS (Bunsetsu Reordering Sequence to Sequence pre-training). 4.1. In MASS, a NMT model is trained by making it predict random parts of a sentence given their context. Instead of random parts we are interested in making the model predict a set of bunsetsus given the contextual bunsetsus. We expect this will let the model learn about the important concept of bunsetsu, as well as focus its training on predicting meaningful subsequences instead of random ones. More precisely, we propose B"
2020.lrec-1.454,D19-1143,0,0.0909798,"se methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-specific information. Works such as (Sennrich and Haddow, 2016; Murthy et al., 2019; Zhou et al., 2019) have shown that such information could improve results. However, because NMT models are end-to-end sequence to sequence models, the manner in which such linguistic hints should be provided is not always clear. In this paper, we argue that pre-training provides an ideal framework both for leveraging monolingual data and improving NMT models with linguistic information. Our setting focuses on the translation between language pairs involving Japanese. Japanese is a language for which very high quality syntactic analyzers have been developed (Kurohashi et al., 1994; Morita et al., 2015). On the o"
2020.lrec-1.454,D16-1163,0,0.430581,"language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Anothe"
2020.lrec-1.454,W14-7001,1,\N,Missing
2020.lrec-1.561,W09-1324,1,0.780113,"numerous variations. In medical NE recognition (NER) tasks, for instance, disease names are a popular NE category. The definition of ‘diseases,’ however, varies substantially along with downstream applications. On the one hand, disease names refer strictly to diagnoses of patients (Wakamiya et al., 2019; Patel et al., 2018). On the other hand, disease names encompass not only diagnoses, but also symptoms and even complaints (Morita et al., 2013; Aramaki et al., 2014). Several corpora have been designed for general purposes covering multiple categories. An example is a Japanese medical corpus (Aramaki et al., 2009) consisting of datetime information (TIMEX), diseases, medication (drug names & dosages), test names, and test values, as well as the relation between drug names and their adverse effects. Because this complex corpus covers multiple NE categories, the annotation cost for 435 discharge summaries was extremely high (over 10,000 USD; during two years). In addition, a French medical corpus, QUAERO (Suominen et al., 2013) uses UMLS3 semantic categories consisting of anatomy, chemical and drugs, devices, disorders, geographic areas, living organisms, objects, phenomena, physiology, and procedures. S"
2020.lrec-1.561,N19-1423,0,0.0234069,"Missing"
2020.lrec-1.561,L18-1375,1,0.71619,"by one if they find an entity that matches our definitions presented above. The following guidelines also support annotators to label entities without extensive medical knowledge, while maintaining coherent annotation: 1. nested structures in which another tag is labeled inside one tag are not allowed; in other words, technical medical terms should be annotated as a single entity 2. most informative entity types such as “diseases and symptoms <D&gt;” and “time expressions <TIMEX3&gt;” are assigned priority over others 3. an easy-to-use reference dictionary for diseases and symptoms such as J-MeDic (Ito et al., 2018) is used when annotators are not confident about the exact span of the entity 4. annotators can take a longer span of a single entity if unsure, especially under the case of complex compound words Although these guidelines might reduce the granularity of annotated entities, it can be controlled after the annotation is completed. It is noteworthy that the granularity, range, and definition of entities depends on the downstream application. We instead assign importance to the ease of labeling for non-medical professionals. Overly finer-grained annotation might impose burdens, especially for such"
2020.lrec-1.561,N16-1030,0,0.0341966,"Missing"
2020.lrec-1.561,D18-1228,0,0.312735,"portrays a summary of existing corpora. As the table shows, the corpora are designed for an idiosyncratic task. Only task-specific annotation was done for each task. For example, the i2b2 Deid corpus includes only personal health information (PHI). The definition (range) of tags also presents numerous variations. In medical NE recognition (NER) tasks, for instance, disease names are a popular NE category. The definition of ‘diseases,’ however, varies substantially along with downstream applications. On the one hand, disease names refer strictly to diagnoses of patients (Wakamiya et al., 2019; Patel et al., 2018). On the other hand, disease names encompass not only diagnoses, but also symptoms and even complaints (Morita et al., 2013; Aramaki et al., 2014). Several corpora have been designed for general purposes covering multiple categories. An example is a Japanese medical corpus (Aramaki et al., 2009) consisting of datetime information (TIMEX), diseases, medication (drug names & dosages), test names, and test values, as well as the relation between drug names and their adverse effects. Because this complex corpus covers multiple NE categories, the annotation cost for 435 discharge summaries was extr"
2020.lrec-1.561,N18-1202,0,0.0685803,"Missing"
2020.lrec-1.561,C73-2019,0,0.510227,"Missing"
2020.wat-1.1,L18-1548,1,0.762787,"emselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training, development and test data. Par ticipants of this taks must get a copy of BSD corpus by themselves. 2.12 IITB Hindi–English task In this task we use IIT Bombay EnglishHindi Cor pus (Kunchukuttan et al., 2018) which contains EnglishHindi parallel corpus as well as mono lingual Hindi corpus collected from a variety of sources and corpora (Bojar et al., 2014). This cor pus had been developed at the Center for Indian Language Technology, IIT Bombay over the years. The corpus is used for mixed domain tasks hi↔en. The statistics for the corpus are shown in Table 13. 3 4.1 Tokenization We used the following tools for tokenization. 4.1.1 For ASPEC, JPC, TDDC, JIJI, ALT, UCSY, ECCC, and IITB • Juman version 7.024 for Japanese segmenta tion. • Stanford Word Segmenter version 201401 0425 (Chinese Penn"
2020.wat-1.1,E06-1031,0,0.0814841,"ipants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcomes. The scores are thus comparable only wi"
2020.wat-1.1,W17-5701,1,0.728933,"on to documentlevel evaluation. 2.11 Documentlevel Translation Task In WAT2020, we set up 2 documentlevel transla tion tasks: ParaNatCom and BSD. 20 21 http://www2.nict.go.jp/astrec-att/member/ mutiyama/paranatcom/ https://github.com/nlabmpg/Flickr30kEntJP 7 Lang hien hi Train 1,609,682 – Dev 520 – Test 2,507 – Mono – 45,075,279 systems were published on the WAT web page.23 We also have SMT baseline systems for the tasks that started at WAT 2017 or before 2017. The base line systems are shown in Tables 16, 17, and 18. SMT baseline systems are described in the WAT 2017 overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online transla tion systems were operated by the organizers. We note that these RBMT companies and online trans lation companies did not submit themselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training"
2020.wat-1.1,Y18-3001,1,0.753826,"raining, develop ment, and test data of the KhmerEnglish transla tion tasks are listed in Table 6. 1 4 http://opus.nlpl.eu/ Lang.pair Ja↔Ru Ja↔En Ru↔En Table 8: task. Partition train development test train development test train development test #sent. 12,356 486 600 47,082 589 600 82,072 313 600 #tokens 341k / 229k 16k / 11k 22k / 15k 1.27M / 1.01M 21k / 16k 22k / 17k 1.61M / 1.83M 7.8k / 8.4k 15k / 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k 2.7 Indic Multilingual Task In 2018, we had organized an Indic languages task (Nakazawa et al., 2018) but due to lack of reli able evaluation corpora we discontinued it in WAT 2019. However, in 2020, high quality publicly available evaluation (and training) corpora became available which motivated us to relaunch the task. The Indic task involves mixed domain corpora for evaluation consisting of various articles composed by Indian Prime Minister. The languages involved are Hindi (Hi), Marathi (Mr), Tamil (Ta), Telugu (Te), Gujarati (Gu), Malayalam (Ml), Bengali (Bg) and English (En). English is either the source or the target language during evaluation leading to a total of 14 translation dir"
2020.wat-1.1,W12-5611,1,0.853557,"Missing"
2020.wat-1.1,P11-2093,0,0.0137734,"eq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 30 https://github.com/tensorflow/ tensor2tensor https://taku910.github.io/mecab/ 12 (separated by 1000 batches) and performed decod ing with a beam of size 4 and a length penalty of 0.6. 4.2.3 Before the calculation of the automatic evalua tion scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmenta tion, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model33 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.34 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 20140616 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.35 For Korean segmentation, we used mecabko.36 For Myanmar and Khmer segmen tations, we used myseg.py37 and kmseg.py38 . For English and Russian tokenizations, we used tokenizer.perl39 in the Moses toolkit. For Indonesian and Malay tokenizations, we used tokenizer.perl as same as the Engl"
2020.wat-1.1,2020.lrec-1.518,1,0.770784,") which are not publicly available at the time of WAT 2020. Note that phrasetoregion an notation is not included in the test data. There are two settings of submission: with and without resource constraints. In the constrained setting, external resources such as additional data and pretrained models (with external data) are not allowed to use, except for pretrained convo lutional neural networks (for visual analysis) and basic linguistic tools such as taggers, parsers, and morphological analyzers. As the baseline system to compute the Pairwise score, we implement the textonly model in (Nishihara et al., 2020) under the constrained setting. 2.11.1 Documentlevel Scientific Paper Translation Traditional ASPEC translation tasks are sentence level and the translation quality of them seem to be saturated. We think it’s high time to move on to documentlevel evaluation. For the first year, we use ParaNatCom 21 (Parallel EnglishJapanese ab stract corpus made from Nature Communications articles) for the development and test sets of the Documentlevel Scientific Paper Translation sub task. We cannot provide documentlevel training corpus, but you can use ASPEC and any other ex tra resources. 2.11.2 Do"
2020.wat-1.1,P02-1040,0,0.120093,"ase tophrase and phrasetoregion annotations avail able in the training dataset. 5 5.2 Automatic Evaluation System The automatic evaluation system receives transla tion results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each sub mission: Automatic Evaluation 5.1 Procedure for Calculating Automatic Evaluation Score • Human Evaluation: whether or not they sub mit the results for human evaluation; We evaluated translation results by three met rics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.31 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2020 web page.32 All scores for each task were calculated using the corresponding reference translations. 33 http://www.phontron.com/kytea/model.html http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 35 http://nlp.stanford.edu/software/segmenter"
2020.wat-1.1,2020.lrec-1.462,0,0.0440733,"Missing"
2020.wat-1.1,2006.amta-papers.25,0,0.152126,"conducted pairwise evaluation for participants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcome"
2020.wat-1.1,J82-2005,0,0.692397,"Missing"
2020.wat-1.1,L16-1249,1,0.823126,"pur pose of this task was to test the feasibility of multi domain multilingual solutions for extremely low resource language pairs and domains. Naturally the solutions could be onetomany, manytoone or manytomany NMT models. The domains in question are Wikinews and IT (specifically, Soft ware Documentation). The total number of evalu ation directions are 16 (8 for each domain). There is very little clean and publicly available data for these domains and language pairs and thus we en couraged participants to not only utilize the small Asian Language Treebank (ALT) parallel corpora (Thu et al., 2016) but also the parallel corpora from OPUS1 . The ALT dataset contains 18,088, 1,000 and 1,018 training, development and test ing sentences. As for corpora for the IT domain we only provided evaluation (dev and test sets) 2016), consisting of twenty thousand Khmer English parallel sentences from news articles. • The ECCC corpus consists of 100 thousand KhmerEnglish parallel sentences extracted from document pairs of KhmerEnglish bi lingual records in Extraordinary Chambers in the Court of Cambodia, collected by National Institute of Posts, Telecoms & ICT, Cambo dia. The ALT corpus has been"
2020.wat-1.5,Q19-1038,0,0.0164061,"f-domain datasets used are shown in Table 2 including Ubuntu corpora from OPUS (Tiedemann, 2012), Global Voices, and News Commentary; OpenSubtitles (Lison and Tiedemann, 2016); TED talks (Dabre and Kurohashi, 2017); Wikipedia (Chu et al., 2014, 2016); Wikitionary.org;2 Tatoeba.org under CC-BY License;34 and WikiMatrix (Schwenk et al., 2019). In total, over 1.9M out-of-domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task. However, we observe some sentence alignments are of low accuracy. Thus, we also conduct a filtering by using LASER (Artetxe and Schwenk, 2019). Specifically, we remove sentence pairs with the cossimilarity score less than a fixed threshold where similarity scores are calculated by LASER embeddings of two sentences. We observe that alignment quality tends to be high if the similarity score is over 0.6, so we use this value as the filtering threshConvS2S (Gehring et al., 2017). Compared with RNNs that maintain a hidden state of the entire past, convolution operations can be fully parallelized during training. ConvS2S integrates the convolution operations into the sequence-to-sequence framework, which not only improves computation effi"
2020.wat-1.5,P17-2061,1,0.890242,"longer sequences due to the quadratic complexity. Lightconv builds dynamic convolutions to predict a different kernel at each time-step rather than the entire sequence, which drastically reduces the number of parameters. 1 https://github.com/didi/iwslt2020_ open_domain_translation 2 https://dumps.wikimedia.org/ 3 https://tatoeba.org/eng/ 4 https://creativecommons.org/licenses/ by/2.0/fr/ 65 old for most experiments. This results in 1.5M filtered out-of-domain parallel sentences which we leverage to train the NMT system jointly with ASPEC-JC dataset. We also revisit the domain adaption method (Chu et al., 2017) by adding tags of h2indomaini and h2outof domaini during the training phase. 2020) is a multilingual sequence-to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese. Specifically, we fine-tune mBART255 by JapaneseChinese parallel sentences and compare this kind of multilingual pre-training with other fully (semi-) supervised baselines. 3.3 3.6 Data Augmentation by Back Translation Besides using out-of-domain parallel data, we also implement back translation (Sennrich et al., 2016a; Edunov et al., 2018), another effective data augmentation tech"
2020.wat-1.5,D18-2012,0,0.018691,"t al., 1994; Morita et al., 2015) for Japanese and stanford parser pku7 for Chinese. Sentences over 175 tokens are removed for training. We build a joint vocabulary with 30k merge operations by Byte-Pair Encoding (Sennrich et al., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT"
2020.wat-1.5,chu-etal-2012-chinese,1,0.724409,"requently being used in Japanese-Chinese translation tasks (Song et al., 2020; Chen et al., 2020) because there are a large number of shared Chinese characters in Chinese and Japanese. Usually they not only share the character but also share the semantic function within a sentence, so pre-mapping Chinese characters to the target-side can help amplify the cross-lingual supervision. More precisely, for Kana characters in Japanese, we remain them with the same tokenization granularity whereas for Chinese characters, we first tokenize them into by characters, then use the character mapping table (Chu et al., 2012) to pre-map them to target-side. 3.5 Meta Ensemble 4 Preprocessing and Training Details We conduct tokenization by using Juman6 (Kurohashi et al., 1994; Morita et al., 2015) for Japanese and stanford parser pku7 for Chinese. Sentences over 175 tokens are removed for training. We build a joint vocabulary with 30k merge operations by Byte-Pair Encoding (Sennrich et al., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model trainin"
2020.wat-1.5,chu-etal-2014-constructing,1,0.795141,"on the size of the training data. Thus, high quality augmented training dataset help ameliorate NMT. There exist just around 0.68M parallel sentences in ASPEC-JC, so we expect that extra parallel data can significantly improve the translation quality. We use Japanese-Chinese parallel datasets on other domains collected by IWSLT 2020 (Ansari et al., 2020).1 All the out-of-domain datasets used are shown in Table 2 including Ubuntu corpora from OPUS (Tiedemann, 2012), Global Voices, and News Commentary; OpenSubtitles (Lison and Tiedemann, 2016); TED talks (Dabre and Kurohashi, 2017); Wikipedia (Chu et al., 2014, 2016); Wikitionary.org;2 Tatoeba.org under CC-BY License;34 and WikiMatrix (Schwenk et al., 2019). In total, over 1.9M out-of-domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task. However, we observe some sentence alignments are of low accuracy. Thus, we also conduct a filtering by using LASER (Artetxe and Schwenk, 2019). Specifically, we remove sentence pairs with the cossimilarity score less than a fixed threshold where similarity scores are calculated by LASER embeddings of two sentences. We observe that alignment quality tends to"
2020.wat-1.5,2020.acl-main.703,0,0.0257025,"vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT (2) 7 out-of-domain + BT / FT (1) Different S2S frameworks 8 BT / FT (1) 9 BT / FT (1) 10 BT / FT (1) Different model capacities 11 BT / FT (1) 12 BT / FT (1) 13 BT / FT (1) Character mapping 14 Fine-tune pre-trained models 15 mBART25 16 mBART25 + BT / FT"
2020.wat-1.5,L16-1147,0,0.0234976,"gmentation by Filtering Out-of-domain Parallel Data NMT quality depends highly on the size of the training data. Thus, high quality augmented training dataset help ameliorate NMT. There exist just around 0.68M parallel sentences in ASPEC-JC, so we expect that extra parallel data can significantly improve the translation quality. We use Japanese-Chinese parallel datasets on other domains collected by IWSLT 2020 (Ansari et al., 2020).1 All the out-of-domain datasets used are shown in Table 2 including Ubuntu corpora from OPUS (Tiedemann, 2012), Global Voices, and News Commentary; OpenSubtitles (Lison and Tiedemann, 2016); TED talks (Dabre and Kurohashi, 2017); Wikipedia (Chu et al., 2014, 2016); Wikitionary.org;2 Tatoeba.org under CC-BY License;34 and WikiMatrix (Schwenk et al., 2019). In total, over 1.9M out-of-domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task. However, we observe some sentence alignments are of low accuracy. Thus, we also conduct a filtering by using LASER (Artetxe and Schwenk, 2019). Specifically, we remove sentence pairs with the cossimilarity score less than a fixed threshold where similarity scores are calculated by LASER emb"
2020.wat-1.5,2020.tacl-1.47,0,0.0329397,"Missing"
2020.wat-1.5,N19-1423,0,0.0108897,"175 tokens are removed for training. We build a joint vocabulary with 30k merge operations by Byte-Pair Encoding (Sennrich et al., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT (2) 7 out-of-domain + BT / FT (1) Different S2S frameworks 8 BT / FT (1) 9 BT / FT (1) 10 BT"
2020.wat-1.5,2020.lrec-1.454,1,0.840481,"esults in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT (2) 7 out-of-domain + BT / FT (1) Different S2S frameworks 8 BT / FT (1) 9 BT / FT (1) 10 BT / FT (1) Different model capacities 11 BT / FT (1) 12 BT / FT (1) 13 BT / FT (1) Character mapping 14 Fine-tune pre-trained models 15 mBART25"
2020.wat-1.5,D18-1045,0,0.0160507,"We also revisit the domain adaption method (Chu et al., 2017) by adding tags of h2indomaini and h2outof domaini during the training phase. 2020) is a multilingual sequence-to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese. Specifically, we fine-tune mBART255 by JapaneseChinese parallel sentences and compare this kind of multilingual pre-training with other fully (semi-) supervised baselines. 3.3 3.6 Data Augmentation by Back Translation Besides using out-of-domain parallel data, we also implement back translation (Sennrich et al., 2016a; Edunov et al., 2018), another effective data augmentation technique for NMT. For monolingual corpora, we use the Japanese sentences in ASPECJE as in-domain Japanese monolingual data, where 3M Japanese sentences are used to augment the ASPEC-JC corpus. We do not perform the back translation by using Chinese monolingual data because no in-domain Chinese sentences are available. We neither do not consider using other outof-domain monolingual data. For the translation direction of ja→zh, we name it forward translation because of the absence of the target-side monolingual data, which means we use the pre-trained syste"
2020.wat-1.5,D15-1276,1,0.756555,"and Japanese. Usually they not only share the character but also share the semantic function within a sentence, so pre-mapping Chinese characters to the target-side can help amplify the cross-lingual supervision. More precisely, for Kana characters in Japanese, we remain them with the same tokenization granularity whereas for Chinese characters, we first tokenize them into by characters, then use the character mapping table (Chu et al., 2012) to pre-map them to target-side. 3.5 Meta Ensemble 4 Preprocessing and Training Details We conduct tokenization by using Juman6 (Kurohashi et al., 1994; Morita et al., 2015) for Japanese and stanford parser pku7 for Chinese. Sentences over 175 tokens are removed for training. We build a joint vocabulary with 30k merge operations by Byte-Pair Encoding (Sennrich et al., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART:"
2020.wat-1.5,N19-4009,0,0.0162351,"ayers and 1 decoder layer. “ls”, “dp” and “ffhd” represents label smoothing, dropout and feed-forward hidden dimension, respectively. Model settings without declarations of “ls”, “dp” and “ffhd” are set to “ls(0.1)”, “dp(0.3)” and “ffhd(4,096)” for transformer-big and default for other S2S frameworks. “Threshold” means the filtering threshold value for LASER embedding. for fine-tuning mBART25. For parts of the outof-domain Chinese sentences that are in traditional Chinese, we transfer Traditional Chinese characters to Simplified Chinese ones.8 We conduct all the experiments by using Fairseq9 (Ott et al., 2019), an open source sequenceto-sequence framework implementation. Most systems are set to the Transformer-big setting except those built up by other architectures or different model capacities. In particular, our model has a 6layer encoder and decoder, a hidden size of 1,024, a feed-forward hidden layer size of 4,096, batch-size of 2,048, dropout rate of 0.3 and 16 attention heads. For LSTM, we also use a 6-layer encoder and decoder architecture. For ConvS2S and Lightconv, we use the default settings in Fairseq. All the systems are early stopped if BLEU does not improve for continuous 50,000 step"
2020.wat-1.5,tiedemann-2012-parallel,0,0.0107677,"g information for longer periods of time on the sequence. 3.2 Data Augmentation by Filtering Out-of-domain Parallel Data NMT quality depends highly on the size of the training data. Thus, high quality augmented training dataset help ameliorate NMT. There exist just around 0.68M parallel sentences in ASPEC-JC, so we expect that extra parallel data can significantly improve the translation quality. We use Japanese-Chinese parallel datasets on other domains collected by IWSLT 2020 (Ansari et al., 2020).1 All the out-of-domain datasets used are shown in Table 2 including Ubuntu corpora from OPUS (Tiedemann, 2012), Global Voices, and News Commentary; OpenSubtitles (Lison and Tiedemann, 2016); TED talks (Dabre and Kurohashi, 2017); Wikipedia (Chu et al., 2014, 2016); Wikitionary.org;2 Tatoeba.org under CC-BY License;34 and WikiMatrix (Schwenk et al., 2019). In total, over 1.9M out-of-domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task. However, we observe some sentence alignments are of low accuracy. Thus, we also conduct a filtering by using LASER (Artetxe and Schwenk, 2019). Specifically, we remove sentence pairs with the cossimilarity score"
2020.wat-1.5,P02-1040,0,0.106558,"Missing"
2020.wat-1.5,D19-1071,0,0.0181312,"l., 2016b). This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. We oversample ASPEC-JC for BPE codes learning and NMT model training to balance the in-domain and out-of-domain tokens during the model training. We use the provided vocabulary constructed by SentencePiece (Kudo and Richardson, 2018) mBART: Multilingual Denoising Pre-training After the apperance of BERT (Devlin et al., 2019), several pre-training methods are proposed for enhancing NMT (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; Mao et al., 2020; Lewis et al., 2020). Recently, mBART (Liu et al., 5 https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md 6 https://github.com/ku-nlp/jumanpp 7 https://nlp.stanford.edu/software/ lex-parser.shtml 66 # Augmentation Vanilla transformer 1 Different training data 2 out-of-domain w/o tag 3 out-of-domain w tag 4 BT / FT (1) 5 BT / FT (1) 6 BT / FT (2) 7 out-of-domain + BT / FT (1) Different S2S frameworks 8 BT / FT (1) 9 BT / FT (1) 10 BT / FT (1) Different model capacities 11 BT / FT (1) 12 BT / FT (1) 13 BT / FT (1) Character mapping 14 Fine-tune pre-trained"
2020.wat-1.5,2021.eacl-main.115,0,0.0555276,"Missing"
2020.wat-1.5,P16-1009,0,0.239855,"y with ASPEC-JC dataset. We also revisit the domain adaption method (Chu et al., 2017) by adding tags of h2indomaini and h2outof domaini during the training phase. 2020) is a multilingual sequence-to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese. Specifically, we fine-tune mBART255 by JapaneseChinese parallel sentences and compare this kind of multilingual pre-training with other fully (semi-) supervised baselines. 3.3 3.6 Data Augmentation by Back Translation Besides using out-of-domain parallel data, we also implement back translation (Sennrich et al., 2016a; Edunov et al., 2018), another effective data augmentation technique for NMT. For monolingual corpora, we use the Japanese sentences in ASPECJE as in-domain Japanese monolingual data, where 3M Japanese sentences are used to augment the ASPEC-JC corpus. We do not perform the back translation by using Chinese monolingual data because no in-domain Chinese sentences are available. We neither do not consider using other outof-domain monolingual data. For the translation direction of ja→zh, we name it forward translation because of the absence of the target-side monolingual data, which means we us"
2020.wat-1.5,P16-1162,0,0.334612,"y with ASPEC-JC dataset. We also revisit the domain adaption method (Chu et al., 2017) by adding tags of h2indomaini and h2outof domaini during the training phase. 2020) is a multilingual sequence-to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese. Specifically, we fine-tune mBART255 by JapaneseChinese parallel sentences and compare this kind of multilingual pre-training with other fully (semi-) supervised baselines. 3.3 3.6 Data Augmentation by Back Translation Besides using out-of-domain parallel data, we also implement back translation (Sennrich et al., 2016a; Edunov et al., 2018), another effective data augmentation technique for NMT. For monolingual corpora, we use the Japanese sentences in ASPECJE as in-domain Japanese monolingual data, where 3M Japanese sentences are used to augment the ASPEC-JC corpus. We do not perform the back translation by using Chinese monolingual data because no in-domain Chinese sentences are available. We neither do not consider using other outof-domain monolingual data. For the translation direction of ja→zh, we name it forward translation because of the absence of the target-side monolingual data, which means we us"
2020.wat-1.5,2020.acl-srw.37,1,0.912193,"ormal University (KyotoU+ECNU) to WAT 2020 (Nakazawa et al., 2020). We participate in APSEC JapaneseChinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the models into a single model. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation. 1 • We revisit and explore the trick of character mapping (Song et al., 2020; Chen et al., 2020) between Chinese and Japanese on ASPEC translation task. Although only our team participated in the ASPEC Japanese-Chinese translation task this year, BLEU results we report on the WAT official leader-board rank 1st both on ja→zh and zh→ja compared with all the previous submitted systems. 2 Introduction Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has led to large improvements in machine translation quality when large parallel corpora are available for training. In this work, we revisit several existing NMT based techniques on ASPEC Japan"
2021.acl-long.226,P19-1309,0,0.114523,"y-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages. The above-mentioned models can be categorized into tw"
2021.acl-long.226,Q19-1038,0,0.157321,"y-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages. The above-mentioned models can be categorized into tw"
2021.acl-long.226,W19-4330,0,0.091064,"tion of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages. The above-mentioned models"
2021.acl-long.226,2020.acl-main.747,0,0.0967538,"Missing"
2021.acl-long.226,2020.acl-main.536,0,0.19767,"tive. In this section, we revisit the previous work in these 2 categories, which is crucial for designing a cross-lingual representation model. Generative Tasks. Generative tasks measure a generative probability between predicted tokens and real tokens by training a language model. BERT-style MLM (Devlin et al., 2019) masks and predicts contextualized tokens within a given sentence. For the cross-lingual scenario, cross-lingual supervision is implemented by shared cognates and joint training (Devlin et al., 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al., 2020a) or explicitly predicting the translated token (Ren et al., 2019). The [CLS] embedding or pooled embedding of all the tokens is introduced as the classifier embedding, which can be used as sentence embedding for sentence-level tasks (Reimers and Gurevych, 2019). Sequence to sequence methods (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence. The intermediate state between the encoder and the decoder are extracted as sentence representations. Particularly, the cross-lingual sen"
2021.acl-long.226,D15-1131,0,0.174526,"(Schwenk and Li, 2018) and BiSent2Vec (Sanh et al., 2019), the representative fixed-dimensional sentence representation methods (Yu et al., 2018), LASER (Artetxe and Schwenk, 2019b), and T-LASER (Li and Mak, 2020) as baselines. In addition, as reference only, we present the results of the global finetuning methods, mBERT (Devlin et al., 2019) and the state-of-the-art BERT-based variant, MultiFit (Eisenschlos et al., 2019). For the XSR task, bilingual fixed-dimensional methods, Bi-Vec (Luong et al., 2015) & BiSent2Vec (Sabet et al., 2019), and multilingual fixed-dimensional methods, TransGram (Coulmance et al., 2015) & LASER (Artetxe and Schwenk, 2019b) are used as baselines. Note that T-LASER and LASER are trained on 223M parallel sentences on 93 languages, which uses significantly more training data than ours. We also show the results by comparing with (Reimers and Gurevych, 2020) in Appendix A, which is a recent work using global fine-tuning methods to generate multilingual sentence representations. 4.3 MLDoc: Zero-shot Cross-lingual Document Classification The MLDoc task, which consists of news documents given in 8 different languages, is a benchmark to evaluate cross-lingual sentence representations."
2021.acl-long.226,N19-1423,0,0.334077,"task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages."
2021.acl-long.226,D19-1572,0,0.278223,"-lingual sentence representation learning methods.10 4.2 Baselines For evaluation on the MLDoc benchmark, we use the state-of-the-art fixed-dimensional word representation methods MultiCCA+CNN method (Schwenk and Li, 2018) and BiSent2Vec (Sanh et al., 2019), the representative fixed-dimensional sentence representation methods (Yu et al., 2018), LASER (Artetxe and Schwenk, 2019b), and T-LASER (Li and Mak, 2020) as baselines. In addition, as reference only, we present the results of the global finetuning methods, mBERT (Devlin et al., 2019) and the state-of-the-art BERT-based variant, MultiFit (Eisenschlos et al., 2019). For the XSR task, bilingual fixed-dimensional methods, Bi-Vec (Luong et al., 2015) & BiSent2Vec (Sabet et al., 2019), and multilingual fixed-dimensional methods, TransGram (Coulmance et al., 2015) & LASER (Artetxe and Schwenk, 2019b) are used as baselines. Note that T-LASER and LASER are trained on 223M parallel sentences on 93 languages, which uses significantly more training data than ours. We also show the results by comparing with (Reimers and Gurevych, 2020) in Appendix A, which is a recent work using global fine-tuning methods to generate multilingual sentence representations. 4.3 MLDo"
2021.acl-long.226,W18-6317,0,0.15151,"ion learning (Mikolov et al., 2013). Subsequently, contrastive tasks gradually emerged in many NLP tasks in various ways: negative sampling in knowledge graph embedding learning (Bordes et al., 2013; Wang et al., 2014), next sentence prediction in BERT (Devlin et al., 2019), token-level discrimination in ELECTRA (Clark et al., 2020), sentence-level discrimination in DeCLUTR (Giorgi et al., 2020), and hierarchical contrastive learning in HICTL (Wei et al., 2020). For the cross-lingual sentence representation training, typical ones include using correct and wrong translation pairs introduced by Guo et al. (2018); Yang et al. (2019); Chidambaram et al. (2019); Feng et al. (2020) or utilizing similarities between sentence pairs by introducing a regularization term (Yu et al., 2018). As another advantage, contrastive methods have proven to be more efficient than generative methods (Clark et al., 2020). Inspired by previous work, for our lightweight model, we propose a robust sentence-level contrastive task by leveraging similarity relationships arising from translation pairs. 3 Methodology We perform cross-lingual sentence representation learning by a lightweight dual-transformer framework. Concerning t"
2021.acl-long.226,2020.findings-emnlp.372,0,0.0359469,"., 2019) and XLM (Conneau and Lample, 2019) require being fine-tuned globally which results in a significant overhead of its own. On the other hand, fixeddimensional methods like LASER (Artetxe and Schwenk, 2019b) fix the sentence representations during the pre-training phase, and subsequently the fine-tuning for specific downstream tasks without back-propagating to the pre-trained model will be extremely computationally-lite. Lightweight models have been sufficiently explored for the former group by either shrinking the model (Lan et al., 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020). However, the lightweight models for the latter group have not been explored before, which may have a more promising future for deploying task-specific fine-tuning onto edge devices. In this work, we propose a variety of training tasks for a lightweight cross-lingual sentence model while retaining the robustness. To improve the computational efficiency, we utilize a lightweight dual-transformer architecture with just 2 layers, significantly decreasing the memory consumption and accelerating the training to further improve the efficiency. Our mode"
2021.acl-long.226,W19-4309,0,0.0206556,"stive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low-resource languages. The above-mentioned models can be categorized into two classes. On one h"
2021.acl-long.226,P18-1007,0,0.0235862,"build our PyTorch implementation on top of HuggingFace’s Transformers library (Wolf et al., 2020). Training data is composed of the ParaCrawl8 (Ba˜no´ n et al., 2020) v5.0 datasets for each language pair. We experiment on English– French, English–German, English–Spanish and English–Italian. We filter the parallel corpus for each language pair by removing sentences that cover tokens out of 2 languages. Raw and filtered number of the parallel sentences for each pair are shown in Table 2. 10,000 sentences are selected for validation on each language pair. We tokenize sentences by SentencePiece9 (Kudo, 2018) and build a shared vocabulary with the size of 50k for each language pair. For each encoder, we use the transformer architecture with 2 hidden layers, 8 attention heads, hidden size of 512 and filter size of 1,024, and the parameters of two encoders are shared with each other. The sentence representations generated are 512 dimensional. For the training phase, it minimizes the weighted losses for our proposed crosslingual language model jointly with 2 auxiliary tasks. We train 12 epochs for each language pair (30 epochs for English-Italian because of nearly half number of parallel sentences) w"
2021.acl-long.226,W15-1521,0,0.213113,"LDoc benchmark, we use the state-of-the-art fixed-dimensional word representation methods MultiCCA+CNN method (Schwenk and Li, 2018) and BiSent2Vec (Sanh et al., 2019), the representative fixed-dimensional sentence representation methods (Yu et al., 2018), LASER (Artetxe and Schwenk, 2019b), and T-LASER (Li and Mak, 2020) as baselines. In addition, as reference only, we present the results of the global finetuning methods, mBERT (Devlin et al., 2019) and the state-of-the-art BERT-based variant, MultiFit (Eisenschlos et al., 2019). For the XSR task, bilingual fixed-dimensional methods, Bi-Vec (Luong et al., 2015) & BiSent2Vec (Sabet et al., 2019), and multilingual fixed-dimensional methods, TransGram (Coulmance et al., 2015) & LASER (Artetxe and Schwenk, 2019b) are used as baselines. Note that T-LASER and LASER are trained on 223M parallel sentences on 93 languages, which uses significantly more training data than ours. We also show the results by comparing with (Reimers and Gurevych, 2020) in Appendix A, which is a recent work using global fine-tuning methods to generate multilingual sentence representations. 4.3 MLDoc: Zero-shot Cross-lingual Document Classification The MLDoc task, which consists of"
2021.acl-long.226,D19-1410,0,0.14781,"training a language model. BERT-style MLM (Devlin et al., 2019) masks and predicts contextualized tokens within a given sentence. For the cross-lingual scenario, cross-lingual supervision is implemented by shared cognates and joint training (Devlin et al., 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al., 2020a) or explicitly predicting the translated token (Ren et al., 2019). The [CLS] embedding or pooled embedding of all the tokens is introduced as the classifier embedding, which can be used as sentence embedding for sentence-level tasks (Reimers and Gurevych, 2019). Sequence to sequence methods (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence. The intermediate state between the encoder and the decoder are extracted as sentence representations. Particularly, the cross-lingual sentence representation quality of LASER (Artetxe and Schwenk, 2019b) benefits from a massively multilingual machine translation task covering 93 languages. In our work, we revisit the BERT-style training tasks and introduce a novel 2903 generative loss enhanced by"
2021.acl-long.226,2020.emnlp-main.365,0,0.102774,"onneau and Lample, 2019) require being fine-tuned globally which results in a significant overhead of its own. On the other hand, fixeddimensional methods like LASER (Artetxe and Schwenk, 2019b) fix the sentence representations during the pre-training phase, and subsequently the fine-tuning for specific downstream tasks without back-propagating to the pre-trained model will be extremely computationally-lite. Lightweight models have been sufficiently explored for the former group by either shrinking the model (Lan et al., 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020). However, the lightweight models for the latter group have not been explored before, which may have a more promising future for deploying task-specific fine-tuning onto edge devices. In this work, we propose a variety of training tasks for a lightweight cross-lingual sentence model while retaining the robustness. To improve the computational efficiency, we utilize a lightweight dual-transformer architecture with just 2 layers, significantly decreasing the memory consumption and accelerating the training to further improve the efficiency. Our model uses significantly less nu"
2021.acl-long.226,D19-1071,0,0.0277913,"ies, which is crucial for designing a cross-lingual representation model. Generative Tasks. Generative tasks measure a generative probability between predicted tokens and real tokens by training a language model. BERT-style MLM (Devlin et al., 2019) masks and predicts contextualized tokens within a given sentence. For the cross-lingual scenario, cross-lingual supervision is implemented by shared cognates and joint training (Devlin et al., 2019), concatenating source sentences in multiple languages (Conneau and Lample, 2019; Conneau et al., 2020a) or explicitly predicting the translated token (Ren et al., 2019). The [CLS] embedding or pooled embedding of all the tokens is introduced as the classifier embedding, which can be used as sentence embedding for sentence-level tasks (Reimers and Gurevych, 2019). Sequence to sequence methods (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Artetxe and Schwenk, 2019b; Li and Mak, 2020) autoregressively reconstruct the translation of the source sentence. The intermediate state between the encoder and the decoder are extracted as sentence representations. Particularly, the cross-lingual sentence representation quality of LASER (Artetxe and Schwenk, 2019b)"
2021.acl-long.226,W17-2619,0,0.135555,"ngual token-level reconstruction task. We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during train"
2021.acl-long.226,W18-3023,0,0.281122,"ment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model. 1 1 Introduction Cross-lingual sentence representation models (Schwenk and Douze, 2017; Espa˜na-Bonet et al., 2017; Yu et al., 2018; Devlin et al., 2019; Chidambaram et al., 2019; Artetxe and Schwenk, 2019b; Kim et al., 2019; Sabet et al., 2019; Conneau and Lample, 2019; Feng et al., 2020; Li 1 https://github.com/Mao-KU/ lightweight-crosslingual-sent2vec and Mak, 2020) learn language-agnostic representations facilitating tasks like cross-lingual sentence retrieval (XSR) and cross-lingual knowledge transfer on downstream tasks without the need for training a new monolingual representation model from scratch. Thus, such models benefit from an increased amount of data during training and lead to improved performances for low"
2021.acl-long.226,L18-1560,0,0.21183,"uxiliary tasks to compensate for the learning bottleneck of lightweight transformer for generative tasks. Following the state-of-the-art fixeddimensional model LASER, we proceed to learn cross-lingual sentence representations from parallel sentences, where we employ 2-layer dualtransformer encoders to shrink the model architecture. By introducing the above-stated training tasks, we establish a computationally-lite framework for training cross-lingual sentence models. We evaluate the learned sentence representations on cross-lingual tasks including multilingual document classification (MLDoc) (Schwenk and Li, 2018) and XSR. Our results confirm the ability of our lightweight model to yield robust sentence representations. We also do a systematic study on the performance of our model in an ablative manner. The contributions of this work can be summarized as follows: • We implement fixed-dimensional crosslingual sentence representation learning in a lightweight model, achieving improved training efficiency and competitive performance of the learned sentence representations. • Our proposed novel generative and contrastive tasks allow cross-lingual sentence representation efficiently trainable by the lightwe"
2021.acl-long.226,2020.acl-main.195,0,0.0200492,"uire being fine-tuned globally which results in a significant overhead of its own. On the other hand, fixeddimensional methods like LASER (Artetxe and Schwenk, 2019b) fix the sentence representations during the pre-training phase, and subsequently the fine-tuning for specific downstream tasks without back-propagating to the pre-trained model will be extremely computationally-lite. Lightweight models have been sufficiently explored for the former group by either shrinking the model (Lan et al., 2020) or training a student model (Sanh et al., 2019; Jiao et al., 2020; Reimers and Gurevych, 2020; Sun et al., 2020). However, the lightweight models for the latter group have not been explored before, which may have a more promising future for deploying task-specific fine-tuning onto edge devices. In this work, we propose a variety of training tasks for a lightweight cross-lingual sentence model while retaining the robustness. To improve the computational efficiency, we utilize a lightweight dual-transformer architecture with just 2 layers, significantly decreasing the memory consumption and accelerating the training to further improve the efficiency. Our model uses significantly less number of parameters"
2021.acl-srw.9,W16-3210,0,0.0342243,"Missing"
2021.acl-srw.9,D15-1166,0,0.0247773,"dle layers between the object-to-frame layer and frameto-video layer in the original HAN. Experiments on the VATEX dataset (Wang et al., 2019) show that our VMT system achieves 35.86 corpus-level BLEU-4 score on the VATEX test set, yielding a 0.51 score improvement over the single model of the SOTA method (Hirasawa et al., 2020). 2 features and obtain ordered motion representations M ∗ , represented as: M ∗ = PE(M ) Target Decoder. The sentence embedding U from the source language encoder and the ordered motion embedding M∗ from the motion encoder are processed using two attention mechanisms (Luong et al., 2015): ru,t = Attentionu,t (ht−1 , U ) (2) ∗ rm,t = Attentionm,t (ht−1 , M ) (3) where Attention denotes a standard attention block, ht−1 denotes the hidden state at the previous decoding time step. Text representations ru,t and motion representations rm,t are allocated by another attention layer to obtain a contextual vector rc,t at decoding time step t. The contextual vector is fed into a GRU layer for decoding: VMT with Spatial HAN The overview of the proposed model is presented in Figure 2, which consists of components in the VMT baseline model (Hirasawa et al., 2020) and our proposed spatial H"
2021.acl-srw.9,W16-2346,0,0.0599618,"Missing"
2021.acl-srw.9,W18-6402,0,0.0197146,"5; Wu et al., 2016) have achieved high performance for domains where there is less ambiguity in data such as the newspaper domain. For some other domains, especially real-time domains such as spoken language or sports commentary, the verb and the noun sense ambiguity largely affects the translation quality. To solve the ambiguity problem, multimodal machine translation (MMT) (Specia et al., 2016) focuses on incorporating visual data as auxiliary information, where the spatiotemporal contextual information in the visual data helps reduce the ambiguity of nouns or verbs in the source text data (Barrault et al., 2018). Previous MMT studies mainly focus on imageguided machine translation (IMT) task (Zhao et al., 2020; Elliott et al., 2016). However, videos are better information sources than images because one 87 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 87–92 August 5–6, 2021. ©2021 Association for Computational Linguistics One person… teasing cats… Word Embeddings Text Encoder Text Attention 3D ConvNet Motion Encoder Motion Atten"
2021.acl-srw.9,D18-1325,0,0.0285582,"anguage Processing: Student Research Workshop, pages 87–92 August 5–6, 2021. ©2021 Association for Computational Linguistics One person… teasing cats… Word Embeddings Text Encoder Text Attention 3D ConvNet Motion Encoder Motion Attention Faster R-CNN Target Decoder ⼀个⼈… 逗两只猫… Spatial HAN Figure 2: The proposed model with spatial HAN. The text encoder and the motion encoder are the same as those in the VMT baseline model. problems by using both motion and spatial representations in a video. To obtain spatial representations efficiently, we propose to use a hierarchical attention network (HAN) (Werlen et al., 2018) to model the spatial information from object-level to video-level, thus we call it the spatial HAN module. Additionally, to obtain a better contextual spatial information, we add several kinds of middle layers between the object-to-frame layer and frameto-video layer in the original HAN. Experiments on the VATEX dataset (Wang et al., 2019) show that our VMT system achieves 35.86 corpus-level BLEU-4 score on the VATEX test set, yielding a 0.51 score improvement over the single model of the SOTA method (Hirasawa et al., 2020). 2 features and obtain ordered motion representations M ∗ , represent"
2021.acl-srw.9,2020.eamt-1.12,1,0.786724,"as the newspaper domain. For some other domains, especially real-time domains such as spoken language or sports commentary, the verb and the noun sense ambiguity largely affects the translation quality. To solve the ambiguity problem, multimodal machine translation (MMT) (Specia et al., 2016) focuses on incorporating visual data as auxiliary information, where the spatiotemporal contextual information in the visual data helps reduce the ambiguity of nouns or verbs in the source text data (Barrault et al., 2018). Previous MMT studies mainly focus on imageguided machine translation (IMT) task (Zhao et al., 2020; Elliott et al., 2016). However, videos are better information sources than images because one 87 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 87–92 August 5–6, 2021. ©2021 Association for Computational Linguistics One person… teasing cats… Word Embeddings Text Encoder Text Attention 3D ConvNet Motion Encoder Motion Attention Faster R-CNN Target Decoder ⼀个⼈… 逗两只猫… Spatial HAN Figure 2: The proposed model with spatial HA"
2021.eacl-main.265,2020.tacl-1.5,0,0.0376709,"ntropy loss against the oracle extraction labels. 3 3.1 Experiment Dataset We evaluated our proposed model on the benchmark CNN/DailyMail dataset (non-anonymized version) (Hermann et al., 2015). We used the standard dataset split, which contains 287,227 / 13,368 / 11,490 documents for training, validation, and test split, respectively. We used the Stanford CoreNLP (Manning et al., 2014) to split sentences. Further, we used the RST discourse parser proposed by Ji and Eisenstein (2014) for both discourse segmentation and discourse parsing. For coreference resolution, we used the spanBERT-based (Joshi et al., 2020) version of the end-to-end coreference resolver proposed by Lee et al. (2017). Since the CNN/DailyMail dataset only contains abstractive gold summaries, we have to construct oracle labels heuristically. We obtained the oracle labels on EDU-level with the heuristic algorithm based on ROUGE (Lin, 2004), similar to the one in Liu and Lapata (2019). For each document, we selected up to 5 EDUs. 3.2 Experimental Settings We used the base model of Longformer (Beltagy et al., 2020) to encode the input document. The length of each document is truncated to 1024 BPEs. The hidden size of the EDU encoder a"
2021.eacl-main.265,P16-1046,0,0.0374301,"ent. Between text spans of different granularity, there exist many different kinds of relations (Figure 1). For example, coreference relations exist between mention phrases of the same entity, and discourse relations exist between Elementary Discourse Units (EDUs) within a document. Due to its complex nature, modeling the various relations among text spans of a document remains an open challenge. To capture inter-sentential relations, some recent works utilize recurrent neural networks (RNNs) or Transformer (Vaswani et al., 2017) based encoders on top of the acquired sentence representations (Cheng and Lapata, 2016; Nallapati et al., 2016; Liu and Lapata, 2019). However, empirical observations show that these sentence-level encoders do not bring much performance gain (Liu and Lapata, 2019). Graph structure is an intuitive way to model long-range dependencies among text spans throughout a document. Early works build connectivity graphs based on content similarity between sentences (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Some recent works incorporate discourse or coreference relations into the graph structure and utilize graph neural networks (GNNs) to obtain a high-level representation of text"
2021.eacl-main.265,N13-1136,0,0.0515271,"Missing"
2021.eacl-main.265,D17-1018,0,0.049086,"valuated our proposed model on the benchmark CNN/DailyMail dataset (non-anonymized version) (Hermann et al., 2015). We used the standard dataset split, which contains 287,227 / 13,368 / 11,490 documents for training, validation, and test split, respectively. We used the Stanford CoreNLP (Manning et al., 2014) to split sentences. Further, we used the RST discourse parser proposed by Ji and Eisenstein (2014) for both discourse segmentation and discourse parsing. For coreference resolution, we used the spanBERT-based (Joshi et al., 2020) version of the end-to-end coreference resolver proposed by Lee et al. (2017). Since the CNN/DailyMail dataset only contains abstractive gold summaries, we have to construct oracle labels heuristically. We obtained the oracle labels on EDU-level with the heuristic algorithm based on ROUGE (Lin, 2004), similar to the one in Liu and Lapata (2019). For each document, we selected up to 5 EDUs. 3.2 Experimental Settings We used the base model of Longformer (Beltagy et al., 2020) to encode the input document. The length of each document is truncated to 1024 BPEs. The hidden size of the EDU encoder and the entity encoder is 128. Based on the evaluation losses on the validatio"
2021.eacl-main.265,N19-1423,0,0.00785687,"des and entity nodes to embed the coreference relations. If EDU di contains a mention of entity ej , then we add an undirected edge (di , ej ) to E. In this way, each entity indirectly connects all EDUs with mentions of the entity. Third, we also link each sentence node to its constituent EDU nodes undirectionally. The proposed heterogeneous document graph enables us to simultaneously model various relations between different sizes of text spans: sentence, EDU, entity phrase efficiently. 2.2 Graph Node Initialization Following the settings in Liu and Lapata (2019), we utilize pretrained BERT (Devlin et al., 2019) to encode the input document D. We insert the 〈CLS〉 and 〈SEP〉 special tokens to the beginning and the end of each sentence si , respectively. With the BERT output vectors, we acquire the initial representations of each node in V as follows: vectors {hji } of the tokens: αij = v2 ReLU (W1 hji + b1 ) (1) aij = sof tmaxj (αi1 , αi2 , ...) (2) EDUi = X aij hji Entity Representations The structure of the entity encoder is identical to the EDU encoder. For each entity ei , we consider all mentions of it. By taking self-attention among the BERT output vectors which correspond to tokens of these ment"
2021.eacl-main.265,D18-1409,0,0.0354221,"Missing"
2021.eacl-main.265,P14-1002,0,0.0324302,"n layer with sigmoid activation to predict binary labels: yˆi = σ(Wp EDUi + bp ) (7) The training loss of the model is the binary crossentropy loss against the oracle extraction labels. 3 3.1 Experiment Dataset We evaluated our proposed model on the benchmark CNN/DailyMail dataset (non-anonymized version) (Hermann et al., 2015). We used the standard dataset split, which contains 287,227 / 13,368 / 11,490 documents for training, validation, and test split, respectively. We used the Stanford CoreNLP (Manning et al., 2014) to split sentences. Further, we used the RST discourse parser proposed by Ji and Eisenstein (2014) for both discourse segmentation and discourse parsing. For coreference resolution, we used the spanBERT-based (Joshi et al., 2020) version of the end-to-end coreference resolver proposed by Lee et al. (2017). Since the CNN/DailyMail dataset only contains abstractive gold summaries, we have to construct oracle labels heuristically. We obtained the oracle labels on EDU-level with the heuristic algorithm based on ROUGE (Lin, 2004), similar to the one in Liu and Lapata (2019). For each document, we selected up to 5 EDUs. 3.2 Experimental Settings We used the base model of Longformer (Beltagy et a"
2021.eacl-main.265,W16-3617,0,0.0261123,"homogeneous graphs with only one type of node. One of the major disadvantages of homogeneous graphs is that they can only embed one relation type in a single graph, since there is only one type of node and one type of edge. Fewer summarization models operate on heterogeneous graphs with different types of nodes. Wei (2012) introduces a heterogeneous graph of sentence, word, and topic nodes, and Wang et al. (2020) also utilizes a heterogeneous graph of sentence and word nodes. However, neither of the above works incorporates external knowledge into the graph. EDU based Extractive Summarization Li et al. (2016) illustrates the potential of using EDU as the extraction unit for summarization. Xu et al. (2020) also introduces an end-to-end EDU based extractive summarization model. By using a heuristic based on RST dependency structure, they enhanced the grammaticality and discourse consistency of the extracted summary. 5 Conclusion In this paper, we proposed a novel heterogeneous graph based model for extractive summarization. By introducing nodes of different granularity, the heterogeneous graph has the capacity to embed various types of relations between text spans. Experiments on CNN/DailyMail bench"
2021.eacl-main.265,W04-1013,0,0.104921,"d test split, respectively. We used the Stanford CoreNLP (Manning et al., 2014) to split sentences. Further, we used the RST discourse parser proposed by Ji and Eisenstein (2014) for both discourse segmentation and discourse parsing. For coreference resolution, we used the spanBERT-based (Joshi et al., 2020) version of the end-to-end coreference resolver proposed by Lee et al. (2017). Since the CNN/DailyMail dataset only contains abstractive gold summaries, we have to construct oracle labels heuristically. We obtained the oracle labels on EDU-level with the heuristic algorithm based on ROUGE (Lin, 2004), similar to the one in Liu and Lapata (2019). For each document, we selected up to 5 EDUs. 3.2 Experimental Settings We used the base model of Longformer (Beltagy et al., 2020) to encode the input document. The length of each document is truncated to 1024 BPEs. The hidden size of the EDU encoder and the entity encoder is 128. Based on the evaluation losses on the validation set, we set the number of iterations of the GAT layer to 3. Also, the number of attention heads is set to 8, with each head having a hidden size 64. R-1 40.34 52.59 55.96 41.50 41.59 42.37 42.95 43.25 42.73 43.77 43.61 R-2"
2021.eacl-main.265,D19-1387,0,0.203221,", there exist many different kinds of relations (Figure 1). For example, coreference relations exist between mention phrases of the same entity, and discourse relations exist between Elementary Discourse Units (EDUs) within a document. Due to its complex nature, modeling the various relations among text spans of a document remains an open challenge. To capture inter-sentential relations, some recent works utilize recurrent neural networks (RNNs) or Transformer (Vaswani et al., 2017) based encoders on top of the acquired sentence representations (Cheng and Lapata, 2016; Nallapati et al., 2016; Liu and Lapata, 2019). However, empirical observations show that these sentence-level encoders do not bring much performance gain (Liu and Lapata, 2019). Graph structure is an intuitive way to model long-range dependencies among text spans throughout a document. Early works build connectivity graphs based on content similarity between sentences (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Some recent works incorporate discourse or coreference relations into the graph structure and utilize graph neural networks (GNNs) to obtain a high-level representation of text spans (Yasunaga et al., 2017; Xu and Durrett,"
2021.eacl-main.265,P14-5010,0,0.00271527,"2.4 Prediction Layer We feed the final representation of the EDU nodes (EDUi ) to the prediction layer with sigmoid activation to predict binary labels: yˆi = σ(Wp EDUi + bp ) (7) The training loss of the model is the binary crossentropy loss against the oracle extraction labels. 3 3.1 Experiment Dataset We evaluated our proposed model on the benchmark CNN/DailyMail dataset (non-anonymized version) (Hermann et al., 2015). We used the standard dataset split, which contains 287,227 / 13,368 / 11,490 documents for training, validation, and test split, respectively. We used the Stanford CoreNLP (Manning et al., 2014) to split sentences. Further, we used the RST discourse parser proposed by Ji and Eisenstein (2014) for both discourse segmentation and discourse parsing. For coreference resolution, we used the spanBERT-based (Joshi et al., 2020) version of the end-to-end coreference resolver proposed by Lee et al. (2017). Since the CNN/DailyMail dataset only contains abstractive gold summaries, we have to construct oracle labels heuristically. We obtained the oracle labels on EDU-level with the heuristic algorithm based on ROUGE (Lin, 2004), similar to the one in Liu and Lapata (2019). For each document, we"
2021.eacl-main.265,P17-1099,0,0.0254205,"nularity. Experimental results on a benchmark summarization dataset verify the effectiveness of our proposed method. 1 Figure 1: Relations among text spans of different granularity. Introduction Automatic summarization aims to condense the information of the input document into a shorter summary. The task has two main paradigms: extractive summarization and abstractive summarization. Generating summary sentences from scratch, abstractive summarizers can generate concise and flexible summaries. However, they also suffer from the problem of not being able to reproduce factual details correctly (See et al., 2017). On the other hand, extractive summarization aims to select salient text spans (mostly sentences) from the input document. Compared to abstractive summarizers, extractive summarizers have the advantage of being efficient and factually reliable. In this paper, we will focus on extractive summarization. For extractive summarization, it is crucial to model the relations between text spans throughout the document. Between text spans of different granularity, there exist many different kinds of relations (Figure 1). For example, coreference relations exist between mention phrases of the same entit"
2021.eacl-main.265,2020.acl-main.553,0,0.0157141,"oreference relations. Some works convert the RST tree of the input document into dependency form in either sentence or EDU level (Xu and Durrett, 2019; Xu et al., 2020). Most of these models operate on homogeneous graphs with only one type of node. One of the major disadvantages of homogeneous graphs is that they can only embed one relation type in a single graph, since there is only one type of node and one type of edge. Fewer summarization models operate on heterogeneous graphs with different types of nodes. Wei (2012) introduces a heterogeneous graph of sentence, word, and topic nodes, and Wang et al. (2020) also utilizes a heterogeneous graph of sentence and word nodes. However, neither of the above works incorporates external knowledge into the graph. EDU based Extractive Summarization Li et al. (2016) illustrates the potential of using EDU as the extraction unit for summarization. Xu et al. (2020) also introduces an end-to-end EDU based extractive summarization model. By using a heuristic based on RST dependency structure, they enhanced the grammaticality and discourse consistency of the extracted summary. 5 Conclusion In this paper, we proposed a novel heterogeneous graph based model for extr"
2021.eacl-main.265,D19-1324,0,0.0603222,"nd Lapata, 2019). However, empirical observations show that these sentence-level encoders do not bring much performance gain (Liu and Lapata, 2019). Graph structure is an intuitive way to model long-range dependencies among text spans throughout a document. Early works build connectivity graphs based on content similarity between sentences (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Some recent works incorporate discourse or coreference relations into the graph structure and utilize graph neural networks (GNNs) to obtain a high-level representation of text spans (Yasunaga et al., 2017; Xu and Durrett, 2019; Xu et al., 2020). Most of these works operate on homogeneous graphs with only one type of nodes, such as Approximate Discourse Graph (ADG) (Christensen et al., 2013) or Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) dependency graph. As illustrated in Figure 1, the various types of relations exist between text spans of different 3046 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3046–3052 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Figure 2: System overview. granularity. Thus, homogeneo"
2021.eacl-main.265,2020.acl-main.451,0,0.39347,"ver, empirical observations show that these sentence-level encoders do not bring much performance gain (Liu and Lapata, 2019). Graph structure is an intuitive way to model long-range dependencies among text spans throughout a document. Early works build connectivity graphs based on content similarity between sentences (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Some recent works incorporate discourse or coreference relations into the graph structure and utilize graph neural networks (GNNs) to obtain a high-level representation of text spans (Yasunaga et al., 2017; Xu and Durrett, 2019; Xu et al., 2020). Most of these works operate on homogeneous graphs with only one type of nodes, such as Approximate Discourse Graph (ADG) (Christensen et al., 2013) or Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) dependency graph. As illustrated in Figure 1, the various types of relations exist between text spans of different 3046 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3046–3052 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Figure 2: System overview. granularity. Thus, homogeneous graphs may not"
2021.eacl-main.265,K17-1045,0,0.0821687,"ati et al., 2016; Liu and Lapata, 2019). However, empirical observations show that these sentence-level encoders do not bring much performance gain (Liu and Lapata, 2019). Graph structure is an intuitive way to model long-range dependencies among text spans throughout a document. Early works build connectivity graphs based on content similarity between sentences (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Some recent works incorporate discourse or coreference relations into the graph structure and utilize graph neural networks (GNNs) to obtain a high-level representation of text spans (Yasunaga et al., 2017; Xu and Durrett, 2019; Xu et al., 2020). Most of these works operate on homogeneous graphs with only one type of nodes, such as Approximate Discourse Graph (ADG) (Christensen et al., 2013) or Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) dependency graph. As illustrated in Figure 1, the various types of relations exist between text spans of different 3046 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3046–3052 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Figure 2: System overview. granul"
2021.eacl-main.265,P19-1499,0,0.0304753,"Missing"
2021.eacl-main.265,P18-1061,0,0.040233,"Missing"
2021.findings-emnlp.165,D16-1132,0,0.0238579,"n languages, Chinese and Korean as well as Nakaiwa (1999) and Furukawa et al. (2017), who Japanese, which not only omit contextually infer- identify a multi-hop link from a Japanese zero proable pronouns but also show no verbal agreement noun to its Japanese antecedent via English counfor person, number, or gender (Park et al., 2015; terparts. Their rule-based methods suffer from acYin et al., 2017; Song et al., 2020; Kim et al., 2021). cumulated errors and syntactically non-transparent While supervised learning is the standard approach correspondences. In addition, they do not handle to ZAR (Iida et al., 2016; Ouchi et al., 2017; Shi- inter-sentential anaphora, a non-negligible subtype bata and Kurohashi, 2018), training data are so of anaphora we cover in this paper. small that additional resources are clearly needed. While we exploit MT to improve the perforEarly studies work on case frame construction from mance of ZAR, the exploitation in the reverse direca large raw corpus (Sasano et al., 2008; Sasano and tion has been studied. A line of research has been Kurohashi, 2011; Yamashiro et al., 2018), pseudo done on Chinese zero pronoun prediction (ZPP) training data generation (Liu et al., 2017),"
2021.findings-emnlp.165,D19-5603,0,0.220868,". (c) A Japanese-English pair (Nabeshima and Brooks, 2020, p. 74) whose correspondences are too obscure for rule-based transfer. Because Japanese generally avoids having inanimate agents with animate patients, the English inanimate-subject sentence corresponds to two animate-subject clauses in Japanese, with two exophoric references to the reader (i.e., you). it is non-trivial to combine the two distinct architectures, with the goal to help the former. We use a pretrained BERT model to initialize the encoder part of the encoder-decoder model for MT. While this technique was previously used by Imamura and Sumita (2019) and Clinchant et al. (2019), they both aimed at improving MT performance. We show that by ejecting the encoder part for use in fine-tuning (Figure 2), we can achieve performance improvements in ZAR. We also demonstrate further improvements can be brought by incorporating encoder-side masked language model (MLM) training into the intermediate training on MT. data augmentation (Konno et al., 2020), and an intermediate task tailored to ZAR (Konno et al., 2021). The multi-task learning approach of Ueda et al. (2020) covers verbal predicate analysis (which subsumes ZAR), and nominal predicate anal"
2021.findings-emnlp.165,Y12-1058,1,0.667191,"called prodrop languages like Japanese and Chinese because they usually omit pronouns that are inferable from context. The task of identifying the referent of such a dropped element, as illustrated in Figure 1(a), is referred to as zero anaphora resolution (ZAR). Although Japanese ZAR saw a performance boost with the introduction of BERT (Ueda et al., 2020; Konno et al., 2020), there is still a good amount of room for improvement. A major barrier to improvement is the scarcity of training data. The number of annotated sentences is the order of tens of thousands or less (Kawahara et al., 2002; Hangyo et al., 2012; Iida et al., 2017), and the considerable linguistic expertise required 1 In our preliminary experiments, we tested encoderfor annotation makes drastic corpus expansion im- decoder pretrained models with no success. We briefly revisit practical. this in Section 4.7. 1920 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1920–1934 November 7–11, 2021. ©2021 Association for Computational Linguistics (a) 妻が wife=NOM 息子に いくつか son=DAT several おもちゃを 買ってあげた 。 toy=ACC buy.GER=give.PST ? ! =NOM 赤い 車を 特に red car=ACC especially ? !=NOM 気に入っている 。 like.GER=be.NPST (b) My wife go"
2021.findings-emnlp.165,kawahara-etal-2002-construction,1,0.253936,"lly challenging for so-called prodrop languages like Japanese and Chinese because they usually omit pronouns that are inferable from context. The task of identifying the referent of such a dropped element, as illustrated in Figure 1(a), is referred to as zero anaphora resolution (ZAR). Although Japanese ZAR saw a performance boost with the introduction of BERT (Ueda et al., 2020; Konno et al., 2020), there is still a good amount of room for improvement. A major barrier to improvement is the scarcity of training data. The number of annotated sentences is the order of tens of thousands or less (Kawahara et al., 2002; Hangyo et al., 2012; Iida et al., 2017), and the considerable linguistic expertise required 1 In our preliminary experiments, we tested encoderfor annotation makes drastic corpus expansion im- decoder pretrained models with no success. We briefly revisit practical. this in Section 4.7. 1920 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1920–1934 November 7–11, 2021. ©2021 Association for Computational Linguistics (a) 妻が wife=NOM 息子に いくつか son=DAT several おもちゃを 買ってあげた 。 toy=ACC buy.GER=give.PST ? ! =NOM 赤い 車を 特に red car=ACC especially ? !=NOM 気に入っている 。 like.GER=b"
2021.findings-emnlp.165,P17-1146,0,0.0215165,"e and Korean as well as Nakaiwa (1999) and Furukawa et al. (2017), who Japanese, which not only omit contextually infer- identify a multi-hop link from a Japanese zero proable pronouns but also show no verbal agreement noun to its Japanese antecedent via English counfor person, number, or gender (Park et al., 2015; terparts. Their rule-based methods suffer from acYin et al., 2017; Song et al., 2020; Kim et al., 2021). cumulated errors and syntactically non-transparent While supervised learning is the standard approach correspondences. In addition, they do not handle to ZAR (Iida et al., 2016; Ouchi et al., 2017; Shi- inter-sentential anaphora, a non-negligible subtype bata and Kurohashi, 2018), training data are so of anaphora we cover in this paper. small that additional resources are clearly needed. While we exploit MT to improve the perforEarly studies work on case frame construction from mance of ZAR, the exploitation in the reverse direca large raw corpus (Sasano et al., 2008; Sasano and tion has been studied. A line of research has been Kurohashi, 2011; Yamashiro et al., 2018), pseudo done on Chinese zero pronoun prediction (ZPP) training data generation (Liu et al., 2017), and ad- with a prim"
2021.findings-emnlp.165,Y15-1050,0,0.0246566,"task They spot multi2 Related Work ple occurrences of the same noun phrase, mask one of them, and force the model to identify the 2.1 Zero Anaphora Resolution (ZAR) pseudo-antecedent. ZAR has been extensively studied in major East Our use of parallel texts in ZAR is inspired by Asian languages, Chinese and Korean as well as Nakaiwa (1999) and Furukawa et al. (2017), who Japanese, which not only omit contextually infer- identify a multi-hop link from a Japanese zero proable pronouns but also show no verbal agreement noun to its Japanese antecedent via English counfor person, number, or gender (Park et al., 2015; terparts. Their rule-based methods suffer from acYin et al., 2017; Song et al., 2020; Kim et al., 2021). cumulated errors and syntactically non-transparent While supervised learning is the standard approach correspondences. In addition, they do not handle to ZAR (Iida et al., 2016; Ouchi et al., 2017; Shi- inter-sentential anaphora, a non-negligible subtype bata and Kurohashi, 2018), training data are so of anaphora we cover in this paper. small that additional resources are clearly needed. While we exploit MT to improve the perforEarly studies work on case frame construction from mance of Z"
2021.findings-emnlp.165,P18-1044,1,0.933399,"ng to fine-tuning on ZAR, we inject MT as an intermediate task (Section 3.2). In addition, we introduce the 2 MLM training objective during the intermediate We suspect that the poor performance resulted in part from their excessively simple decoder, a single-layer LSTM. training (Section 3.3). 1922 3.1 BERT-based Model for ZAR ZAR as argument selection As illustrated in Figure 3, the basic idea behind BERT-based ZAR is that given the powerful neural encoder, the joint task of omission detection and antecedent identification can be formalized as argument selection (Shibata and Kurohashi, 2018; Kurita et al., 2018; Ueda et al., 2020). Omission detection concerns whether a given predicate has an argument for a given case (relation). If not, the model must point to the special token [NULL]. Otherwise the model must identify the antecedent of the zero pronoun by pointing either to a token in the given text or to a special token reserved for exophora. Note that by getting the entire document as the input, the model can handle inter-sentential anaphora as well as intra-sentential anaphora. In practice, the input length limitation of BERT forces us to implement a sliding window approach. Also note that in th"
2021.findings-emnlp.165,N18-1202,0,0.0106477,") Figure 2: Overview of the proposed method. Left: The model is pretrained with the masked language model (MLM) objective (known as BERT). Center: The pretrained BERT is used to initialize the encoder part of the encoder-decoder, which is trained on MT with the MLM objective. Right: The encoder is extracted from the MT model and is fine-tuned on ZAR and related tasks. Note that some special tokens are omitted for simplicity. open question whether MT helps ZAR as well. 2.2 MT as an Intermediate Task Inspired by the great success of the pretraining/finetuning paradigm on a broad range of tasks (Peters et al., 2018; Devlin et al., 2019), a line of research inserts an intermediate task between pretraining and fine-tuning on a target task (Phang et al., 2018; Wang et al., 2019a; Pruksachatkun et al., 2020). However, Wang et al. (2019a) found that MT used as an intermediate task led to performance degeneration in various target tasks, such as natural language inference and sentiment classification.2 They argue that the considerable difference between MLM pretraining and MT causes catastrophic forgetting (CF). Pruksachatkun et al. (2020) suggest injecting the MLM objective during intermediate training as a"
2021.findings-emnlp.165,2020.acl-main.703,0,0.0368818,"native argument of the predicate, “で” (be), points to “定吉” (Sadakichi, the father of Jutaro). Although the model correctly translated the zero pronoun as “He”, it failed in ZAR. This is probably because not only “定吉 (Sadakichi)” but also “龍馬” (Ryoma) and “重太郎” (Jutaro) can be referred to as “He”. When disambiguation is not required to generate an overt pronoun, MT is not very helpful. 4.7 Note on Other Pretrained Models Conclusion Due to space limitation, we have limited our focus to BERT, but for the sake of future practitioners, we would like to briefly note that we extensively tested BART (Lewis et al., 2020) and its variants before switching to BERT. Unlike BERT, BART is an encoder-decoder model pretrained on a monolin- Acknowledgments gual corpus (original) or a non-parallel multilingual We thank the Yomiuri Shimbun for providing corpus (mBART) (Liu et al., 2020). Because MT Japanese-English parallel texts. We are grateful requires the encoder-decoder architecture, main- for Nobuhiro Ueda’ help in setting up the baseline taining the model architecture between pretraining model. We thank the anonymous reviewers for and intermediate training looked promising to us. their insightful comments. We sp"
2021.findings-emnlp.165,P17-1010,0,0.0249078,"(Iida et al., 2016; Ouchi et al., 2017; Shi- inter-sentential anaphora, a non-negligible subtype bata and Kurohashi, 2018), training data are so of anaphora we cover in this paper. small that additional resources are clearly needed. While we exploit MT to improve the perforEarly studies work on case frame construction from mance of ZAR, the exploitation in the reverse direca large raw corpus (Sasano et al., 2008; Sasano and tion has been studied. A line of research has been Kurohashi, 2011; Yamashiro et al., 2018), pseudo done on Chinese zero pronoun prediction (ZPP) training data generation (Liu et al., 2017), and ad- with a primary aim of improving Chinese-English versarial training (Kurita et al., 2018). These efforts translation (Wang et al., 2016, 2018, 2019b). ZPP are, however, overshadowed by the surprising effec- is different from ZAR in that it does not identify tiveness of BERT’s pretraining (Ueda et al., 2020; antecedents. This is understandable given that clasKonno et al., 2020). sification of zero pronouns into overt ones suffices Adopting BERT, recent studies seek gains for MT. Although Wang et al. (2019b) report muthrough multi-task learning (Ueda et al., 2020), tual benefits between"
2021.findings-emnlp.165,2020.acl-main.467,0,0.0432031,"Missing"
2021.findings-emnlp.165,C08-1097,1,0.418963,", 2017; Song et al., 2020; Kim et al., 2021). cumulated errors and syntactically non-transparent While supervised learning is the standard approach correspondences. In addition, they do not handle to ZAR (Iida et al., 2016; Ouchi et al., 2017; Shi- inter-sentential anaphora, a non-negligible subtype bata and Kurohashi, 2018), training data are so of anaphora we cover in this paper. small that additional resources are clearly needed. While we exploit MT to improve the perforEarly studies work on case frame construction from mance of ZAR, the exploitation in the reverse direca large raw corpus (Sasano et al., 2008; Sasano and tion has been studied. A line of research has been Kurohashi, 2011; Yamashiro et al., 2018), pseudo done on Chinese zero pronoun prediction (ZPP) training data generation (Liu et al., 2017), and ad- with a primary aim of improving Chinese-English versarial training (Kurita et al., 2018). These efforts translation (Wang et al., 2016, 2018, 2019b). ZPP are, however, overshadowed by the surprising effec- is different from ZAR in that it does not identify tiveness of BERT’s pretraining (Ueda et al., 2020; antecedents. This is understandable given that clasKonno et al., 2020). sificati"
2021.findings-emnlp.165,I11-1085,1,0.809451,"Missing"
2021.findings-emnlp.165,P19-1439,0,0.0455075,"Missing"
2021.findings-emnlp.165,P16-1162,0,0.0593725,"inter-sentential reasoning, consecutive sentences were concatenated into chunks, with the maximum number of tokens equal to that of ZAR. As a result, we obtained around 373,000, 21,000, and 21,000 chunks for the training, validation, and test data, respectively. Japanese sentences were split into words using the morphological analyzer MeCab with the Juman 3 https://database.yomiuri.co.jp/about/ glossary/ 4 We counted the Japanese sentences since there were oneto-many mappings. dictionary (Kudo et al., 2004).5 Both Japanese and English texts underwent subword tokenization. We used Subword-NMT (Sennrich et al., 2016) for Japanese and SentencePiece (Kudo and Richardson, 2018) for English. We used separate vocabularies for Japanese and English, with the vocabulary sizes of around 32,000 and 16,000, respectively. 4.2 Model Settings BERT We employed a Japanese BERT model with BPE segmentation distributed by NICT.6 It had the same architecture as Google’s BERTBase (Devlin et al., 2019): 12 layers, 768 hidden units, and 12 attention heads. It was trained on the full text of Japanese Wikipedia for approximately 1 million steps. MT We used the Transformer encoder-decoder architecture (Vaswani et al., 2017). The e"
2021.findings-emnlp.165,D19-1085,0,0.0862499,"lel sentences without transparent syntactic correspondences (Figure 1(c)). In this paper, we propose neural transfer learning from machine translation (MT). By generating English translations, a neural MT model should be able to implicitly recover omitted Japanese pronouns, thanks to its expressiveness and large training data. We expect the knowledge gained during MT training to be transferred to ZAR. Given that state-of-the-art ZAR models are based on BERT (Ueda et al., 2020; Konno et al., 2020, 2021), it is a natural choice to explore intermediate task transfer learning (Phang et al., 2018; Wang et al., 2019a; Pruksachatkun et al., 2020; Vu et al., 2020): A pretrained BERT model is first trained on MT and the resultant model is then fine-tuned on ZAR.1 A key challenge to this approach is a mismatch in model architectures. While BERT is an encoder, the dominant paradigm of neural MT is the encoder-decoder. Although both share Transformer (Vaswani et al., 2017) as the building block, Figuring out who did what to whom is an essential part of natural language understanding. This is, however, especially challenging for so-called prodrop languages like Japanese and Chinese because they usually omit pro"
2021.findings-emnlp.165,P18-1054,1,0.828204,"tly moving from MLM pretraining to fine-tuning on ZAR, we inject MT as an intermediate task (Section 3.2). In addition, we introduce the 2 MLM training objective during the intermediate We suspect that the poor performance resulted in part from their excessively simple decoder, a single-layer LSTM. training (Section 3.3). 1922 3.1 BERT-based Model for ZAR ZAR as argument selection As illustrated in Figure 3, the basic idea behind BERT-based ZAR is that given the powerful neural encoder, the joint task of omission detection and antecedent identification can be formalized as argument selection (Shibata and Kurohashi, 2018; Kurita et al., 2018; Ueda et al., 2020). Omission detection concerns whether a given predicate has an argument for a given case (relation). If not, the model must point to the special token [NULL]. Otherwise the model must identify the antecedent of the zero pronoun by pointing either to a token in the given text or to a special token reserved for exophora. Note that by getting the entire document as the input, the model can handle inter-sentential anaphora as well as intra-sentential anaphora. In practice, the input length limitation of BERT forces us to implement a sliding window approach."
2021.findings-emnlp.165,2020.lrec-1.447,0,0.019267,"mixed results. It worked for the Web but did not for the News. What is worse, its combination with MLM led to performance degeneration on both datasets. MLM achieved superior performance as it worked well in all settings. The gains were larger with one-stage optimization than with two-stage optimization (1.4 vs. 0.3 on the Web). 4.4 Translation of Zero Pronouns The experimental results demonstrate that MT helps ZAR, but why does it work? Unfortunately, conventional evaluation metrics for MT (e.g., BLEU) reveal little about the model’s ability to handle zero anaphora. To address this problem, Shimazu et al. (2020) and Nagata and Morishita (2020) constructed Japanese-English parallel datasets that were designed to automatically evaluate MT models with regard to the translation of Japanese zero pronouns (ZPT). We used Shimazu et al.’s dataset for its larger data size.10 To facilitate automatic evaluation of ZPT, this dataset paired a correct English sentence with an incorrect one. All we had to do was to calculate the ratio of instances for which the model assigned higher translation scores to the correct candidates. The only difference between the two sentences involved the translation of a Japanese zer"
2021.findings-emnlp.165,2020.acl-main.482,0,0.0134301,"f them, and force the model to identify the 2.1 Zero Anaphora Resolution (ZAR) pseudo-antecedent. ZAR has been extensively studied in major East Our use of parallel texts in ZAR is inspired by Asian languages, Chinese and Korean as well as Nakaiwa (1999) and Furukawa et al. (2017), who Japanese, which not only omit contextually infer- identify a multi-hop link from a Japanese zero proable pronouns but also show no verbal agreement noun to its Japanese antecedent via English counfor person, number, or gender (Park et al., 2015; terparts. Their rule-based methods suffer from acYin et al., 2017; Song et al., 2020; Kim et al., 2021). cumulated errors and syntactically non-transparent While supervised learning is the standard approach correspondences. In addition, they do not handle to ZAR (Iida et al., 2016; Ouchi et al., 2017; Shi- inter-sentential anaphora, a non-negligible subtype bata and Kurohashi, 2018), training data are so of anaphora we cover in this paper. small that additional resources are clearly needed. While we exploit MT to improve the perforEarly studies work on case frame construction from mance of ZAR, the exploitation in the reverse direca large raw corpus (Sasano et al., 2008; Sasa"
2021.findings-emnlp.165,2020.coling-main.114,1,0.339559,"sing, and English coreference resolution. More importantly, the great linguistic differences between the two language often lead to parallel sentences without transparent syntactic correspondences (Figure 1(c)). In this paper, we propose neural transfer learning from machine translation (MT). By generating English translations, a neural MT model should be able to implicitly recover omitted Japanese pronouns, thanks to its expressiveness and large training data. We expect the knowledge gained during MT training to be transferred to ZAR. Given that state-of-the-art ZAR models are based on BERT (Ueda et al., 2020; Konno et al., 2020, 2021), it is a natural choice to explore intermediate task transfer learning (Phang et al., 2018; Wang et al., 2019a; Pruksachatkun et al., 2020; Vu et al., 2020): A pretrained BERT model is first trained on MT and the resultant model is then fine-tuned on ZAR.1 A key challenge to this approach is a mismatch in model architectures. While BERT is an encoder, the dominant paradigm of neural MT is the encoder-decoder. Although both share Transformer (Vaswani et al., 2017) as the building block, Figuring out who did what to whom is an essential part of natural language unders"
2021.findings-emnlp.165,D18-1333,0,0.056094,"Missing"
2021.findings-emnlp.165,N16-1113,0,0.0687318,"Missing"
2021.findings-emnlp.165,Y18-1089,0,0.014547,"le supervised learning is the standard approach correspondences. In addition, they do not handle to ZAR (Iida et al., 2016; Ouchi et al., 2017; Shi- inter-sentential anaphora, a non-negligible subtype bata and Kurohashi, 2018), training data are so of anaphora we cover in this paper. small that additional resources are clearly needed. While we exploit MT to improve the perforEarly studies work on case frame construction from mance of ZAR, the exploitation in the reverse direca large raw corpus (Sasano et al., 2008; Sasano and tion has been studied. A line of research has been Kurohashi, 2011; Yamashiro et al., 2018), pseudo done on Chinese zero pronoun prediction (ZPP) training data generation (Liu et al., 2017), and ad- with a primary aim of improving Chinese-English versarial training (Kurita et al., 2018). These efforts translation (Wang et al., 2016, 2018, 2019b). ZPP are, however, overshadowed by the surprising effec- is different from ZAR in that it does not identify tiveness of BERT’s pretraining (Ueda et al., 2020; antecedents. This is understandable given that clasKonno et al., 2020). sification of zero pronouns into overt ones suffices Adopting BERT, recent studies seek gains for MT. Although W"
2021.findings-emnlp.165,D17-1135,0,0.0162277,"phrase, mask one of them, and force the model to identify the 2.1 Zero Anaphora Resolution (ZAR) pseudo-antecedent. ZAR has been extensively studied in major East Our use of parallel texts in ZAR is inspired by Asian languages, Chinese and Korean as well as Nakaiwa (1999) and Furukawa et al. (2017), who Japanese, which not only omit contextually infer- identify a multi-hop link from a Japanese zero proable pronouns but also show no verbal agreement noun to its Japanese antecedent via English counfor person, number, or gender (Park et al., 2015; terparts. Their rule-based methods suffer from acYin et al., 2017; Song et al., 2020; Kim et al., 2021). cumulated errors and syntactically non-transparent While supervised learning is the standard approach correspondences. In addition, they do not handle to ZAR (Iida et al., 2016; Ouchi et al., 2017; Shi- inter-sentential anaphora, a non-negligible subtype bata and Kurohashi, 2018), training data are so of anaphora we cover in this paper. small that additional resources are clearly needed. While we exploit MT to improve the perforEarly studies work on case frame construction from mance of ZAR, the exploitation in the reverse direca large raw corpus (Sasano"
2021.naacl-main.433,P19-1422,0,0.2474,"based stegosystems suffered las, 1998). While images, videos, and audio have been dominant cover media (Fridrich, 2009), nat- from low payload capacity, for example, 2 bits per sentence (Chang and Clark, 2014). ural language is a promising choice, thanks to the With advances in neural language models (LMs), omnipresence of text (Bennett, 2004). edit-based approaches have been replaced by Formally, the goal of linguistic steganography is to create a steganographic system (stegosystem) generation-based ones (Fang et al., 2017; Yang with which the sender Alice encodes a secret mes- et al., 2019; Dai and Cai, 2019; Ziegler et al., 2019; Shen et al., 2020). In these approaches, bit chunks sage, usually in the form of a bit sequence, into a text and the receiver Bob decodes the message, are directly assigned to the conditional probability distribution over the next word estimated by the with the requirement that the text is so natural that LM, yielding impressive payload capacities of 1–5 even if transmitted in a public channel, it does not bits per word (Shen et al., 2020). arouse the suspicion of the eavesdropper Eve. For a stegosystem that creates the text through transforHowever, it remains challengi"
2021.naacl-main.433,N19-1423,0,0.124384,"ution for each masked token. Bit chunks are assigned to some high-probability subwords in the distribution from which one is chosen according to the secret message. The stego text is then transmitted in a public channel Eve (eavesdropper) monitors. Receiving the stego text, Bob performs mostly the same procedure to decode the secret message. conditioned generation on human-written introductory sentences to ensure the stego text quality. In this paper, we revisit edit-based linguistic steganography. Our key idea is that a masked language model (masked LM), which was first introduced with BERT (Devlin et al., 2019), offers an off-the-shelf solution. Usually treated as an intermediate model with no direct application, the masked LM drastically simplifies an edit-based stegosystem. It eliminates painstaking rule construction because it readily offers a list of words applicable in the context. As illustrated in Figure 1, all Alice and Bob have to share is the masking and encoding strategies in addition to the masked LM. In our experiments, we showed that the proposed method had a high payload capacity for an editbased model. As expected, the amount was far smaller than those of generation-based models, but"
2021.naacl-main.433,P17-3017,0,0.0459841,"Missing"
2021.naacl-main.433,2020.acl-main.164,0,0.0430412,"the conditional probability distribution over the next word estimated by the with the requirement that the text is so natural that LM, yielding impressive payload capacities of 1–5 even if transmitted in a public channel, it does not bits per word (Shen et al., 2020). arouse the suspicion of the eavesdropper Eve. For a stegosystem that creates the text through transforHowever, it remains challenging for an LM to mation, we refer to the original text as the cover text generate so genuine-looking texts that they fool and the modified text as the stego text. A stegosys- both humans and machines (Ippolito et al., 2020) tem has two objectives, security and payload ca- even if they do not encode secret messages. It is pacity. Security is the degree of how unsuspicious also worth noting that generation-based stegosysthe stego text is while payload capacity is the size tems do not necessarily cut out the need for cover of the secret message relative to the size of the texts, as Ziegler et al. (2019) and Shen et al. (2020) 5486 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5486–5492 June 6–11, 2021. ©2021 Asso"
2021.naacl-main.433,K16-1028,0,0.0716802,"Missing"
2021.naacl-main.433,2020.emnlp-main.22,0,0.283168,"hile images, videos, and audio have been dominant cover media (Fridrich, 2009), nat- from low payload capacity, for example, 2 bits per sentence (Chang and Clark, 2014). ural language is a promising choice, thanks to the With advances in neural language models (LMs), omnipresence of text (Bennett, 2004). edit-based approaches have been replaced by Formally, the goal of linguistic steganography is to create a steganographic system (stegosystem) generation-based ones (Fang et al., 2017; Yang with which the sender Alice encodes a secret mes- et al., 2019; Dai and Cai, 2019; Ziegler et al., 2019; Shen et al., 2020). In these approaches, bit chunks sage, usually in the form of a bit sequence, into a text and the receiver Bob decodes the message, are directly assigned to the conditional probability distribution over the next word estimated by the with the requirement that the text is so natural that LM, yielding impressive payload capacities of 1–5 even if transmitted in a public channel, it does not bits per word (Shen et al., 2020). arouse the suspicion of the eavesdropper Eve. For a stegosystem that creates the text through transforHowever, it remains challenging for an LM to mation, we refer to the or"
2021.naacl-main.433,2020.lrec-1.494,0,0.0120072,"eded the limit, and the remainder was filled with zeros. GPT-2 Ziegler et al. (2019) built a state-of-theart generation-based model on top of the GPT-2 neural LM (Radford et al., 2019). We used their original implementation1 to encode random bit sequences. We set the option finish_sent to true to avoid terminating generation at the middle of a sentence. We tested the temperature parameter τ = {0.4, 0.7, 1.0}. Since the generation was conditioned on context sentences, we supplied the first three sentences of a paragraph. Data We extracted paragraphs from the English part of the CC-100 dataset (Wenzek et al., 2020) and used them as the cover texts for BERT and as the contexts for GPT-2.2 For each stegosystem, we also extracted texts that were comparable to the corresponding stego texts in terms of length. We refer to them as real texts. 3.2 We trained discriminators to distinguish stego texts from real texts. This corresponds to a situation unusually favorable to Eve as she has access to labeled data, though not to secret messages. A practical reason for this is that after all, we cannot build discriminators without training data. Besides, a stegosystem’s performance is deemed satisfactory if it manages"
2021.naacl-main.433,D19-1115,0,0.212676,"suffered las, 1998). While images, videos, and audio have been dominant cover media (Fridrich, 2009), nat- from low payload capacity, for example, 2 bits per sentence (Chang and Clark, 2014). ural language is a promising choice, thanks to the With advances in neural language models (LMs), omnipresence of text (Bennett, 2004). edit-based approaches have been replaced by Formally, the goal of linguistic steganography is to create a steganographic system (stegosystem) generation-based ones (Fang et al., 2017; Yang with which the sender Alice encodes a secret mes- et al., 2019; Dai and Cai, 2019; Ziegler et al., 2019; Shen et al., 2020). In these approaches, bit chunks sage, usually in the form of a bit sequence, into a text and the receiver Bob decodes the message, are directly assigned to the conditional probability distribution over the next word estimated by the with the requirement that the text is so natural that LM, yielding impressive payload capacities of 1–5 even if transmitted in a public channel, it does not bits per word (Shen et al., 2020). arouse the suspicion of the eavesdropper Eve. For a stegosystem that creates the text through transforHowever, it remains challenging for an LM to mation"
2021.naacl-main.442,D18-2029,0,0.0367268,"Missing"
2021.naacl-main.442,D17-1070,0,0.0895526,"Missing"
2021.naacl-main.442,N19-1423,0,0.076897,"Missing"
2021.naacl-main.442,2020.acl-main.226,0,0.0650738,"Missing"
2021.naacl-main.442,N06-1023,1,0.675746,"Missing"
2021.naacl-main.442,C14-1027,1,0.793601,"Missing"
2021.naacl-main.442,2020.acl-main.480,0,0.0939572,"Missing"
2021.naacl-main.442,L18-1637,1,0.877779,"Missing"
2021.naacl-main.442,D19-1387,0,0.0404159,"Missing"
2021.naacl-main.442,2021.ccl-1.108,0,0.0463121,"Missing"
2021.naacl-main.442,N18-1202,0,0.141677,"Missing"
2021.naacl-main.442,D19-1410,0,0.026144,"Missing"
2021.naacl-main.442,P18-1042,0,0.0440141,"Missing"
2021.naacl-main.442,N18-1101,0,0.0610043,"Missing"
2021.naacl-main.442,P19-1499,0,0.0547091,"Missing"
2021.wat-1.1,2020.acl-main.703,0,0.187967,"news-commentary corpus.14 This year we also encouraged participants to use any corpora from WMT 202015 and WMT 202116 involving Japanese, Russian, and English as long as it did not belong to the news commentary domain to prevent any test set sentences from being unintentionally seen during training. 2,049 2,050 Table 5: The NICT-SAP task corpora splits. The corpora belong to two domains: wikinews (ALT) and software documentation (IT). The Wikinews corpora are Nway parallel. also encouraged the use of monolingual corpora expecting that it would be for pre-trained NMT models such as BART/MBART (Lewis et al., 2020; Liu et al., 2020). In Table 5 we give statistics of the aforementioned corpora which we used for the organizer’s baselines. Note that the evaluation corpora for both domains are created from documents and thus contain document level meta-data. Participants were encouraged to use document level approaches. Note that we do not exhaustively list6 all available corpora here and participants were not restricted from using any corpora as long as they are freely available. 2.7 Partition train development test train development test train development test 8 http://www.phontron.com/kftt/ https://data"
2021.wat-1.1,C18-2019,0,0.0200002,"task and 4 systems for the Japanese→ English.74 On the whole, all the submitted systems are basically lexical-constraint-aware NMT models with lexically constrained decoding method, where the restricted target vocabulary is concatenated into source sentences and, during the beam search at inference time, the models generate translation outputs containing the target vocabulary. We observed that these techniques boost the final translation performance of the NMT models in the restricted translation task. For human evaluation, we conducted the sourcebased direct assessment (Cettolo et al., 2017; Federmann, 2018) and source-based contrastive assessment (Sakaguchi and Van Durme, 2018; Federmann, 2018), to have the top-ranked systems of each team appraised by bilingual human annotators. In the human evaluation campaign, we also include the human reference data. Table 20 reports the final automatic evaluation score and the human evaluation results. In both tasks, the systems from the team “NTT” are the most highly evaluated in all the submitted systems in the final score and the human evaluation, consistently. We also note that our designed automation metric is well correlated Flickr30kEnt-JP Japanese↔En"
2021.wat-1.1,W18-1819,0,0.0552279,"Missing"
2021.wat-1.1,2007.mtsummit-papers.63,0,0.0610664,"om OPUS. We Test set II : A pair of test and reference sentences and context data that are articles including test sentences. The references were automatically extracted from English newswire sentences and manually selected. Therefore, the quality of the references of test set II is better than that of test set I. The statistics of JIJI Corpus are shown in Table 2. The definition of data use is shown in Table 3. Participants submit the translation results of one or more of the test data. The sentence pairs in each data are identified in the same manner as that for ASPEC using the method from (Utiyama and Isahara, 2007). 2.5 ALT and UCSY Corpus The parallel data for Myanmar-English translation tasks at WAT2021 consists of two corpora, the ALT corpus and UCSY corpus. • The ALT corpus is one part from the Asian Language Treebank (ALT) project (Riza et al., 2016), consisting of twenty thousand Myanmar-English parallel sentences from news articles. • The UCSY corpus (Yi Mon Shwe Sin and Khin Mar Soe, 2018) is constructed by the NLP Lab, University of Computer Studies, 3 http://opus.nlpl.eu/ http://www.statmt.org/wmt20/ 5 Software Domain Evaluation Splits 4 3 Task Use Training Test set I Japanese to English Test"
C00-1063,A97-1052,0,0.0619708,"Missing"
C00-1063,J94-4001,1,0.802931,"s of about 40,000 analyzed sentences of newspaper articles, very basic verbs like tetsudau ‘help’ or uketsukeru ‘accept’ appear only 10 times or 15 times respectively. It is obvious that such small data are insufficient for automatic case frame learning. That is, case frame learning must be done from enormous unanalyzed corpora, in unsupervised way2 . We can collect pairs of verbs and case components from the automatic analyses of large corpora by KNP. 3.1 Good parser NLP research group at Kyoto University has been developing a robust and accurate parsing system, KNP, over the last ten years (Kurohashi and Nagao, 1994; Kurohashi and Nagao, 1998). This parser has the following advantages: The following sections explain how to solve these problems. 3.2.1 Word sense ambiguity If a verb has two or more meanings and their case frame patterns differ, we have to disambiguate the sense of each occurrence of the verb in a corpus first, and collect case components for each sense respectively. However, unsupervised word sense disambiguation of free texts is one of the most difficult problems in NLP. At the very beginning, even the definition of word senses is open to question. To cope with this problem, we made a ver"
C00-1063,P98-2214,0,0.0188322,"such as scrambling, omission of case components, and disappearance of case markers. Therefore, in Japanese sentence analysis, case structure analysis is an important issue, and a case frame dictionary is necessary for the analysis. Some research institutes have constructed Japanese case frame dictionaries manually (Ikehara et al., 1997; Information-Technology Promotion Agency, Japan, 1987). However, it is quite expensive, or almost impossible to construct a wide-coverage case frame dictionary by hand. Others have tried to construct a case frame dictionary automatically from analyzed corpora (Utsuro et al., 1998). However, existing syntactically analyzed corpora are too small to learn a dictionary, since case frame information consists of relations between nouns and verbs, which multiplies to millions of combinations. Based on such a consideration, we took the following unsupervised learning strategy to the Japanese case structure analysis: 1. At first, a robust and accurate parser is developed, which does not utilize a case frame dictionary, 2. a very large corpus is parsed by the parser, 3. reliable noun-verb relations are extracted from the parse results, and a case frame dictionary is constructed"
C00-1063,P93-1032,0,\N,Missing
C00-2131,C92-2101,0,0.896063,"Missing"
C00-2131,J94-4001,1,0.829914,", we get nally four phrasal correspondences P1 , P2 , P3 , and P4 . 3 Experiments 3.1 Corpus and Dictionary We used documents from White Papers on Science and Technology (1994 to 1996) published by the Science and Technology Agency (STA) of the Japanese government. STA published these White Papers in both Japanese and English. The Communications Research Laboratory of the Ministry of Posts and Telecommunication of the Japanese government supplied us with the bilingual corpus which is already roughly aligned. We made a bilingual corpus consisting of parsed dependency structures by using the KNP[2] Japanese parser (developed by Kyoto University) for Japanese sentences and the ESG[5] English parser (developed by IBM Watson Research Center) for English sentences. We made about 500 sentence pairs, each of which has a one-to-one sentence correspondence, from the raw data of the White Papers, and selected randomly about 130 sentence pairs for experiments. However, since a parser does not always produce correct parse trees, we excluded some sentence pairs which have severe parse errors, and nally got 115 sentence pairs as a test set. As a translation word dictionary between Japanese and Engli"
C00-2131,P93-1004,0,0.242877,"Missing"
C00-2131,J80-1003,0,0.104143,"Corpus and Dictionary We used documents from White Papers on Science and Technology (1994 to 1996) published by the Science and Technology Agency (STA) of the Japanese government. STA published these White Papers in both Japanese and English. The Communications Research Laboratory of the Ministry of Posts and Telecommunication of the Japanese government supplied us with the bilingual corpus which is already roughly aligned. We made a bilingual corpus consisting of parsed dependency structures by using the KNP[2] Japanese parser (developed by Kyoto University) for Japanese sentences and the ESG[5] English parser (developed by IBM Watson Research Center) for English sentences. We made about 500 sentence pairs, each of which has a one-to-one sentence correspondence, from the raw data of the White Papers, and selected randomly about 130 sentence pairs for experiments. However, since a parser does not always produce correct parse trees, we excluded some sentence pairs which have severe parse errors, and nally got 115 sentence pairs as a test set. As a translation word dictionary between Japanese and English, we rst used J-to-E translation dictionary which has more than 100,000 entries, but"
C00-2131,C96-1078,0,0.54206,"Missing"
C00-2131,C90-3044,0,0.0781638,"These structural correspondences are used as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system"
C00-2131,P96-1020,0,0.0185665,"d as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system, it is very important to collect a lar"
C00-2131,C96-2211,0,0.0374233,"d as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system, it is very important to collect a lar"
C00-2131,C92-2115,1,0.832893,"These structural correspondences are used as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system"
C00-2131,1993.tmi-1.25,1,0.870946,"Missing"
C00-2131,P98-2223,1,0.862324,"d as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system, it is very important to collect a lar"
C00-2131,C98-2218,1,\N,Missing
C02-1084,J94-4001,1,\N,Missing
C02-1084,W00-1016,1,\N,Missing
C02-1084,P01-1037,0,\N,Missing
C02-1122,H01-1043,1,0.878382,"ame dictionary by incorporating newly acquired information. 1 Introduction To understand a text, it is necessary to find out relations between words in the text. What is required to do so is a case frame dictionary. It describes what kinds of cases each verb has and what kinds of nouns can fill a case slot. Since these relations have millions of combinations, it is difficult to construct a case frame dictionary by hand. We proposed a method to construct a Japanese case frame dictionary automatically by arranging large volumes of parse results by coupling a verb and its closest case component (Kawahara and Kurohashi, 2001). This case frame dictionary, however, could not handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change. This paper proposes a method of fertilizing the case frame dictionary to handle these complicated expressions. We take an iterative method which consists of two stages. This means gradual learning of what is understood by an analyzer in each stage. In the first stage, we parse a large raw corpus and construct a Japanese case frame dictionary automatically from the parse results. This is the method proposed by (Kawahara and Kuro"
C02-1122,kawahara-etal-2002-construction,1,0.780805,"Missing"
C04-1050,W03-2604,0,0.0537192,"Missing"
C04-1050,W03-1024,0,0.161186,"Missing"
C04-1050,C02-1122,1,0.946217,"ined in the NTT thesaurus, respectively. This section describes the NTT thesaurus and the case frames briefly. 2.1 NTT thesaurus NTT Communication Science Laboratories constructed a semantic feature tree, whose 3,000 nodes are semantic features, and a nominal dictionary containing about 300,000 nouns, each of which is given one or more appropriate semantic features. Figure 1 shows the upper levels of the semantic feature tree. The similarity between two words is defined by formula (1) in Appendix A. 2.2 Automatically constructed case frames We employ the automatically constructed case frames (Kawahara and Kurohashi, 2002) as the basic resource for zero pronoun resolution and word sense disambiguation. This section outlines the method of constructing the case frames. The biggest problem in automatic case frame construction is verb sense ambiguity. Verbs which have different meanings should have different case frames, but it is hard to disambiguate verb senses precisely. To deal with this problem, predicate-argument examples which are collected from a large corpus are distinguished by coupling a verb and its closest case component. That is, examples are not distinguished by verbs (e.g. “tsumu” (load/accumulate))"
C04-1050,kawahara-etal-2002-construction,1,0.788048,"method 526/911 (0.577) 526/1087 (0.484) F 0.512 0.527 Table 3: Accuracy (cooking). precision recall 696/1092 (0.637) 696/1482 (0.470) 713/1081 (0.660) 713/1482 (0.481) F 0.541 0.556 baseline our method case frames have <agent> in their “ga” case slots). In our approach, each of the semantic features are matched against the case example “soup”, and only the best matched semantic feature <soup> is given to “osumashi ”. 5 Experimental Results and Discussion We conducted experiments of zero pronoun resolution on two different domain corpora. One is newspaper articles of “Relevance-tagged corpus” (Kawahara et al., 2002), and the other is utterances of cooking TV programs. These cooking utterances were handled by (Shibata et al., 2003). They annotated various relations to closed captions of the cooking utterances based on the specification of the “Relevance-tagged corpus” (Kawahara et al., 2002). For newspaper domain, the antecedent preference and the classifier were trained with 1,841 sentences in the newspaper corpus, and the newspaper case frames were used. The experiment was performed on 633 sentences. For cooking domain, we used 813 sentences (5 TV programs), and conducted 5-fold cross validation using t"
C04-1050,P02-1014,0,0.0998525,"Missing"
C04-1050,P03-1023,0,0.0291639,"Missing"
C04-1050,J94-4001,1,\N,Missing
C04-1174,A97-1052,0,0.0397582,"en a noun and its indispensable entity is parallel to that between a verb and its arguments or obligatory cases. In this paper, we call indispensable entities of nouns obligatory cases. Indirect anaphora resolution needs a comprehensive information or dictionary of obligatory cases of nouns. In case of verbs, syntactic structures such as subject/object/PP in English or case markers such as ga, wo, ni in Japanese can be utilized as a strong clue to distinguish several obligatory cases and adjuncts (and adverbs), which makes it feasible to construct case frames from large corpora automatically (Briscoe and Carroll, 1997; Kawahara and Kurohashi, 2002). (Kawahara and Kurohashi, 2004) then utilized the automatically constructed case frames to Japanese zero pronoun resolution. On the other hand, in case of nouns, obligatory cases of noun Nh appear, in most cases, in the single form of noun phrase “Nh of Nm ” in English, or “Nm no Nh ” in Japanese. This single form can express several obligatory cases, and furthermore optional cases, for example, “rugby no coach” (obligatory case concerning what sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional cas"
C04-1174,C96-1084,0,0.117274,"y no coach” (obligatory case concerning what sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional case). Therefore, the key issue to construct nominal case frames is to analyze “Nh of Nm ” or “Nm no Nh ” phrases to distinguish obligatory case examples and others. Work which addressed indirect anaphora in English texts so far restricts relationships to a small, relatively well-defined set, mainly part-of relation like the above example (2), and utilized hand-crafted heuristic rules or hand-crafted lexical knowledge such as WordNet (Hahn et al., 1996; Vieira and Poesio, 2000; Strube and Hahn, 1999). (Poesio et al., 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolu"
C04-1174,C02-1122,1,0.873634,"able entity is parallel to that between a verb and its arguments or obligatory cases. In this paper, we call indispensable entities of nouns obligatory cases. Indirect anaphora resolution needs a comprehensive information or dictionary of obligatory cases of nouns. In case of verbs, syntactic structures such as subject/object/PP in English or case markers such as ga, wo, ni in Japanese can be utilized as a strong clue to distinguish several obligatory cases and adjuncts (and adverbs), which makes it feasible to construct case frames from large corpora automatically (Briscoe and Carroll, 1997; Kawahara and Kurohashi, 2002). (Kawahara and Kurohashi, 2004) then utilized the automatically constructed case frames to Japanese zero pronoun resolution. On the other hand, in case of nouns, obligatory cases of noun Nh appear, in most cases, in the single form of noun phrase “Nh of Nm ” in English, or “Nm no Nh ” in Japanese. This single form can express several obligatory cases, and furthermore optional cases, for example, “rugby no coach” (obligatory case concerning what sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional case). Therefore, the key issue to"
C04-1174,kawahara-etal-2002-construction,1,0.8331,"the highly restricted conditions in the example collection. For instance, maker does not have obligatory case slot for its products. This is because maker is usually used in the form of compound noun phrase, “products maker ”, and there are few occurrences of “products no maker ”. To address this problem, not only “Nm no Nh ” but also “Nm Nh ” (compound noun phrase) and “Nm ni-kansuru ‘in terms of’ Nh ” should be collected. 6.2 Experimental results of indirect anaphora resolution We conducted a preliminary experiment of our indirect anaphora resolution system using “Relevance-tagged corpus” (Kawahara et al., 2002). This corpus consists of Japanese newspaper articles, and has relevance tags, including antecedents of indirect anaphors. We prepared a small test corpus that consists of randomly selected 10 articles. The test corpus contains 217 nouns. Out of them, 106 nouns are indirect anaphors, and have 108 antecedents, which is because two nouns have double antecedents. 49 antecedents directly depend on their anaphors, and 59 do not. For 91 antecedents out of 108, a case frame of its anaphor Table 6: Experimental results of indirect anaphora resolution. precision recall F w dep. 40/46 (0.870) 40/59 (0.6"
C04-1174,J94-4001,1,0.804365,"] relief, margin, · · · (a boxlike container in a desk or a chest.) [desk, chest] desk, chest, dresser, · · · &lt;other&gt; credit, fund, saving, · · · (a person who teaches technique in some sport.) [sport] baseball, swimming, · · · &lt;belonging&gt; team, club, · · · (the total value of a company’s shares.) [company] company, corporation, · · · soccer-no Indirect Anaphora Resolution To examine the practical usefulness of the constructed nominal case frames, we built a preliminary system of indirect anaphora resolution based on the case frames. An input sentence is parsed using the Japanese parser, KNP (Kurohashi and Nagao, 1994). Then, from the beginning of the sentence, each noun x is analyzed. When x has more than one case frame, the process of antecedent estimation (stated in the next paragraph) is performed for each case frame, and the case frame with the highest similarity score (described below) and assignments of antecedents to the case frame are selected as a final result. For each case slot of the target case frame of x, its antecedent is estimated. A possible antecedent y in the target sentence and the previous two sentences is checked. This is done one by one, from the syntactically closer y. If the simila"
C04-1174,P99-1062,1,0.908492,", 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolution. As a response to these problems and following the work in (Kurohashi and Sakai, 1999), we propose a method to construct Japanese nominal case frames from large corpora, based on an accurate analysis of “Nm no Nh ” phrases using an ordinary dictionary and a thesaurus. To examine the practical usefulness of the constructed nominal case frames, we also built a system of indirect anaphora resolution based on the case frames. 2 Semantic Feature Dictionary First of all, we briefly introduce NTT Semantic Feature Dictionary employed in this paper. NTT Semantic Feature Dictionary consists of a semantic feature tree, whose 3,000 nodes are semantic features, and a nominal dictionary cont"
C04-1174,W99-0206,0,0.0263306,"f Nm ” or “Nm no Nh ” phrases to distinguish obligatory case examples and others. Work which addressed indirect anaphora in English texts so far restricts relationships to a small, relatively well-defined set, mainly part-of relation like the above example (2), and utilized hand-crafted heuristic rules or hand-crafted lexical knowledge such as WordNet (Hahn et al., 1996; Vieira and Poesio, 2000; Strube and Hahn, 1999). (Poesio et al., 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolution. As a response to these problems and following the work in (Kurohashi and Sakai, 1999), we propose a method to construct Japanese nominal case frames from large corpora, based on an accurate analysis of “Nm no Nh ” phrases using an ordinary di"
C04-1174,poesio-etal-2002-acquiring,0,0.0848411,"Missing"
C04-1174,J99-3001,0,0.0354057,"sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional case). Therefore, the key issue to construct nominal case frames is to analyze “Nh of Nm ” or “Nm no Nh ” phrases to distinguish obligatory case examples and others. Work which addressed indirect anaphora in English texts so far restricts relationships to a small, relatively well-defined set, mainly part-of relation like the above example (2), and utilized hand-crafted heuristic rules or hand-crafted lexical knowledge such as WordNet (Hahn et al., 1996; Vieira and Poesio, 2000; Strube and Hahn, 1999). (Poesio et al., 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolution. As a response to these problems and followi"
C04-1174,J00-4003,0,0.0764556,"tory case concerning what sport), “club no coach” (obligatory case concerning which institution), and “kyonen ‘last year’ no coach” (optional case). Therefore, the key issue to construct nominal case frames is to analyze “Nh of Nm ” or “Nm no Nh ” phrases to distinguish obligatory case examples and others. Work which addressed indirect anaphora in English texts so far restricts relationships to a small, relatively well-defined set, mainly part-of relation like the above example (2), and utilized hand-crafted heuristic rules or hand-crafted lexical knowledge such as WordNet (Hahn et al., 1996; Vieira and Poesio, 2000; Strube and Hahn, 1999). (Poesio et al., 2002) proposed a method of acquiring lexical knowledge from “Nh of Nm ” phrases, but again concentrated on part-of relation. In case of Japanese text analysis, (Murata et al., 1999) proposed a method of utilizing “Nm no Nh ” phrases for indirect anaphora resolution of diverse relationships. However, they basically used all “Nm no Nh ” phrases from corpora, just excluding some pre-fixed stop words. They confessed that an accurate analysis of “Nm no Nh ” phrases is necessary for the further improvement of indirect anaphora resolution. As a response to th"
C08-1054,D07-1032,1,0.550722,"ordinated. Such selectional preferences are thought to support the construction of coordinate structures and to yield similarity between conjuncts on the contrary. We present a method of coordination disambiguation without using similarities. Coordinate structures are supported by their surrounding dependency relations that provide selectional preferences. These relations implicitly work as similarities, and thus it is not necessary to use similarities explicitly. In this paper, we focus on Japanese. Coordination disambiguation is integrated in a fullylexicalized generative dependency parser (Kawahara and Kurohashi, 2007). For the selectional preferences, we use lexical knowledge, such as case frames, which is extracted from a large raw corpus. The remainder of this paper is organized as follows. Section 2 summarizes previous work related to coordination disambiguation and its integration into parsing. Section 3 brieﬂy describes the background of this study. Section 4 overviews our idea, and section 5 describes our model in detail. Section 6 is devoted to our experiments. Finally, section 7 gives the conclusions. 425 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), p"
C08-1054,W02-2016,0,0.201047,"Missing"
C08-1054,J94-4001,1,0.578675,"d the effectiveness of our approach, and endorsed our hypothesis. 1 Introduction The interpretation of coordinate structures directly affects the meaning of the text. Addressing coordination ambiguities is fundamental to natural language understanding. Previous studies on coordination disambiguation suggested that conjuncts in coordinate structures have syntactic or semantic similarities, and dealt with coordination ambiguities using (sub-)string matching, part-ofspeech matching, semantic similarities, and so forth (Agarwal and Boggess, 1992). Semantic similarities are acquired from thesauri (Kurohashi and Nagao, 1994; Resnik, 1999) or distributional similarity (Chantree et al., 2005). c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. The above methods detect the similarity between salad and pasta using a thesaurus or distributional similarity, and identify the coordinate structure that conjoins salad and pasta. They do not use the information of the word eat. On the other hand, this coordinate structure can be analyzed by using selectional preference of eat. Since eat is likely"
C08-1054,P98-2127,0,0.0137177,"is large enough to train a supervised parser. 6.2 Discussion We presented a method for coordination disambiguation without using similarities, and this method achieved better performance than the conventional approaches based on similarities. Though we do not use similarities, we implicitly consider similarities between conjuncts. This is because the heads of pre- and post-conjuncts share a case marker and a predicate, and thus they are essentially similar. Our idea is related to the notion of distributional similarity. Chantree et al. (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation. Lin extracted from a corpus dependency triples of two words and the grammatical relationship between them, and considered that similar words are likely to have similar dependency relations. The difference between Chantree et al. (2005) and ours is that their method does not use the information of verbs in the sentence under consideration, but use only the cooccurrence information extracted from a corpus. On the other hand, the disadvantage of our model is that it cannot consider the parallelism of conjuncts, which still seems to exist in especially strong coord"
C08-1054,P92-1003,0,0.115562,"es coordination disambiguation. Experimental results on web sentences indicated the effectiveness of our approach, and endorsed our hypothesis. 1 Introduction The interpretation of coordinate structures directly affects the meaning of the text. Addressing coordination ambiguities is fundamental to natural language understanding. Previous studies on coordination disambiguation suggested that conjuncts in coordinate structures have syntactic or semantic similarities, and dealt with coordination ambiguities using (sub-)string matching, part-ofspeech matching, semantic similarities, and so forth (Agarwal and Boggess, 1992). Semantic similarities are acquired from thesauri (Kurohashi and Nagao, 1994; Resnik, 1999) or distributional similarity (Chantree et al., 2005). c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. The above methods detect the similarity between salad and pasta using a thesaurus or distributional similarity, and identify the coordinate structure that conjoins salad and pasta. They do not use the information of the word eat. On the other hand, this coordinate structur"
C08-1054,H05-1105,0,0.106678,"Missing"
C08-1054,P06-1033,0,0.0200042,"ours is that their method does not use the information of verbs in the sentence under consideration, but use only the cooccurrence information extracted from a corpus. On the other hand, the disadvantage of our model is that it cannot consider the parallelism of conjuncts, which still seems to exist in especially strong coordinate structures. Handling of such parallelism is an open question of our model. The generation process adopted in this work is similar to the design of dependency structure described in Hudson (1990), which lets the conjuncts have a dependency relation to the predicate. Nilsson et al. (2006) mentioned this notion, but did not consider this idea in their experiments of tree transformations for data-driven dependency parsers. In addition, it is not necessary for our method to transform dependency trees in pre- and post-processes, because we just changed the process of generation in the generative parser. 7 Conclusion In this paper, we ﬁrst came up with a hypothesis that coordinate structures are supported by 431 ? ? (1) densya-no hassyaaizu-ya, keitaidenwa-no tyakushinon-madega ongaku-ni train-GEN departure signal cell phone-GEN ring tone-also music-ACC (departure signals of trains"
C08-1054,P05-1022,0,0.0185262,"Missing"
C08-1054,P07-1122,0,0.0473309,"Missing"
C08-1054,P06-1053,0,0.176801,"Missing"
C08-1054,P99-1081,0,0.0512103,"Missing"
C08-1054,P07-1086,0,0.302116,"Missing"
C08-1054,kawahara-kurohashi-2006-case,1,0.929336,"verb sokushin-sareta (stimulated). However, (b) is not appropriate, because we cannot say the nominal compound “jinkou-no osen” (pollution of population). In (c) and (d), the heads of conjuncts, zouka (increase) and taiki (air), are generated from osen (pollution). These cases are also inappropriate, because we cannot say the nominal compound “zouka-no osen” (pollution of increase). Accordingly, in this case, the correct scope, (a), is derived based on the selectional preferences of predicates and nouns. In this framework, we require selectional preferences. We use case frames for predicates (Kawahara and Kurohashi, 2006) and occurrences of noun-noun modiﬁcations for nouns. Both of them are extracted from a large amount of raw text. 5 Our Model of Coordination Disambiguation This section describes an integrated model of coordination disambiguation in a generative parsing framework. First, we describe resources for selectional preferences, and then illustrate our model of coordination disambiguation. 5.1 Resources for Selectional Preferences As the resources of selectional preferences to support coordinate structures, we use automatically constructed case frames and cooccurrences of noun-noun modiﬁcations. 5.1."
C08-1054,C04-1002,0,0.179018,"Missing"
C08-1054,D07-1064,0,0.69978,"Missing"
C08-1054,D07-1063,0,0.0243001,"Missing"
C08-1054,C98-2122,0,\N,Missing
C08-1097,P06-1079,0,0.46045,"Missing"
C08-1097,W03-1024,0,0.427214,"Missing"
C08-1097,N06-1023,1,0.763607,"antecedents. Seki et al. (2002) proposed a probabilistic model for zero pronoun detection and resolution that uses hand-crafted case frames. In order to alleviate the sparseness of hand-crafted case frames, Kawahara and Kurohashi (2004) introduced wide-coverage case frames to zero pronoun detection that are automatically constructed from a large corpus. They use the case frames as selectional restriction for zero pronoun resolution, but do not utilize the frequency of each example of case slots. However, since the frequency is shown to be a good clue for syntactic and case structure analysis (Kawahara and Kurohashi, 2006), we consider the frequency also can beneﬁt zero pronoun detection. Therefore we propose a probabilistic model for zero anaphora resolution that fully utilizes case frames. This model directly considers the frequency and estimates case assignments for overt case components and antecedents of zero pronoun simultaneously. In addition, our model directly links each zero pronoun to an entity, while most existing models link it to a certain mention of an entity. In our model, mentions and zero pronouns are treated similarly and all of them are linked to corresponding entities. In this point, our mo"
C08-1097,J94-4002,0,0.226897,"is problem, we utilize generalized examples. When one mention of an entity is tagged any category or recognized as an NE, we also use the category or the NE class as the content part of the entity. For examples, if an entity {Prius} is recognized as an artifact name and assigned to wo case of the case frame hanbai(1) in Table 1, the system also calculates: P (NE :ARTIFACT |hanbai(1), wo, A0(wo)=1) P (NE : ARTIFACT ) besides: P (P rius|hanbai(1), wo, A0 (wo) = 1) P (P rius) and uses the higher value. 3.3 Salience Score Previous works reported the usefulness of salience for anaphora resolution (Lappin and Leass, 1994; Mitkov et al., 2002). In order to consider salience of an entity, we introduce salience score, which is calculated by the following set of simple rules: • • • • +2 : mentioned with topical marker “wa”. +1 : mentioned without topical marker “wa”. +0.5 : assigned to a zero pronoun. ×0.7 : beginning of each sentence. For examples, we consider the salience score of the entity {Toyota} in (i) in Section 3.1. In the ﬁrst sentence, since {Toyota} is mentioned with topical marker “wa”, the salience score is 2. At the beginning of the second sentence it becomes 1.4, 774 4.2 Experimental Results Table"
C08-1097,N07-1010,0,0.0147514,"ce on Computational Linguistics (Coling 2008), pages 769–776 Manchester, August 2008 examples he, driver, friend, · · · baggage, luggage, hay, · · · car, truck, vessel, seat, · · · player, children, party, · · · experience, knowledge, · · · generalized examples with rate company, Microsoft, ﬁrm, · · · goods, product, ticket, · · · customer, company, user, · · · shop, bookstore, site · · · [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, · · · [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, · · · [CT:PERSON]:0.28, · · · [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, · · · ... ... the coreference model proposed by Luo (2007) and that proposed by Yang et al. (2008). Due to this characteristic, our model can utilize information beyond a mention and easily consider salience (the importance of an entity). 2 [CT:PERSON]:0.45, [NE:PERSON]:0.08, · · · [CT:ARTIFACT]:0.31, · · · [CT:VEHICLE]:0.32, · · · [CT:PERSON]:0.40, [NE:PERSON]:0.12, · · · [CT:ABSTRACT]:0.47, · · · ... ga (subjective) wo (objective) ni (dative) de (locative) ... hanbai (1) (sell) Table 1: Examples of Constructed Case Frames. ... ... case slot ga (subjective) tsumu (1) wo (objective) (load) ni (dative) tsumu (2) ga (subjective) (accumulate) wo (object"
C08-1097,P05-1020,0,0.0399419,"Missing"
C08-1097,I08-2080,1,0.832631,"JUMAN1 adds to common nouns. In JUMAN, about twenty categories are deﬁned and tagged to common nouns. For example, “ringo (apple),” “inu (dog)” and “byoin (hospital)” are tagged as “FOOD,” “ANIMAL” and “FACILITY,” respectively. For each category, we calculate the rate of categorized example among all case slot examples, and add it to the case slot as “[CT:FOOD]:0.07.” We also generalize NEs. We use a common standard NE deﬁnition for Japanese provided by IREX workshop (1999). IREX deﬁned eight NE classes as shown in Table 2. We ﬁrst recognize NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008), and then construct case frames from the NE-recognized corpus. 770 1 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html Table 2: Deﬁnition of NE in IREX. NE class ORGANIZATION PERSON LOCATION ARTIFACT DATE TIME MONEY PERCENT Morphological analysis, NE recognition, syntactic analysis and coreference resolution are conducted as pre-processes for zero anaphora resolution. Therefore, the model has already recognized existing entities before zero anaphora resolution. For example, let us consider the following text: Examples NHK Symphony Orchestra Kawasaki Kenjiro Rome, Sinuiju Nobel Prize July 1"
C08-1097,C02-1078,0,0.441458,"n plays an important role in discourse analysis. Zero anaphora resolution can be divided into two phases. The ﬁrst phase is zero pronoun detection and the second phase is zero pronoun resolution. Zero pronoun resolution is similar to corefc 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. * Research Fellow of the Japan Society for the Promotion of Science (JSPS) Zero pronouns are not expressed in a text and have to be detected prior to identifying their antecedents. Seki et al. (2002) proposed a probabilistic model for zero pronoun detection and resolution that uses hand-crafted case frames. In order to alleviate the sparseness of hand-crafted case frames, Kawahara and Kurohashi (2004) introduced wide-coverage case frames to zero pronoun detection that are automatically constructed from a large corpus. They use the case frames as selectional restriction for zero pronoun resolution, but do not utilize the frequency of each example of case slots. However, since the frequency is shown to be a good clue for syntactic and case structure analysis (Kawahara and Kurohashi, 2006),"
C08-1097,J01-4004,0,0.116189,"Missing"
C08-1097,P08-1096,0,0.0171136,"(Coling 2008), pages 769–776 Manchester, August 2008 examples he, driver, friend, · · · baggage, luggage, hay, · · · car, truck, vessel, seat, · · · player, children, party, · · · experience, knowledge, · · · generalized examples with rate company, Microsoft, ﬁrm, · · · goods, product, ticket, · · · customer, company, user, · · · shop, bookstore, site · · · [NE:ORGANIZATION]:0.16, [CT:ORGANIZATION]:0.13, · · · [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, · · · [CT:PERSON]:0.28, · · · [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, · · · ... ... the coreference model proposed by Luo (2007) and that proposed by Yang et al. (2008). Due to this characteristic, our model can utilize information beyond a mention and easily consider salience (the importance of an entity). 2 [CT:PERSON]:0.45, [NE:PERSON]:0.08, · · · [CT:ARTIFACT]:0.31, · · · [CT:VEHICLE]:0.32, · · · [CT:PERSON]:0.40, [NE:PERSON]:0.12, · · · [CT:ABSTRACT]:0.47, · · · ... ga (subjective) wo (objective) ni (dative) de (locative) ... hanbai (1) (sell) Table 1: Examples of Constructed Case Frames. ... ... case slot ga (subjective) tsumu (1) wo (objective) (load) ni (dative) tsumu (2) ga (subjective) (accumulate) wo (objective) Construction of Case Frames Case fr"
C08-1097,C02-1122,1,\N,Missing
C08-1132,W00-1201,0,0.0796591,"Missing"
C08-1132,W06-2920,0,0.27989,"the left side, and [ pos1 ,..., posn−1 , posn ]r means the pos-tag sequence of the modifiers attached to a head from the right side. We use the 33 pos-tags defined in Penn Chinese Treebank (Xue et al., 2002) to describe a case pattern, and make following modifications: • group common noun, proper noun and pronoun together and mark them as ‘noun’; • group predicative adjective and all the other verbs together and mark them as ‘verb’; • only regard comma, pause, colon and semi-colon as punctuations and mark them as ‘punc’, and neglect other punctuations. 2 UAS means unlabeled attachment score (Buchholz and Marsi, 2006). The sentences used for this evaluation are from Penn Chinese Treebank with gold word segmentation and pos-tag. 1050 • group cardinal number and ordinal number together and mark them as ‘num’; • keep the original definition for other postags but label them by new tags, such as labeling ‘P’ as ’prep’ and labeling ‘AD’ as ‘adv’. The task of case pattern construction is to extract cpi for each head from both the tagged corpus and the raw corpus. As we will introduce later, the Chinese dependency parser using case structures applies CKY algorithm for decoding. Thus the following substrings of cpi"
C08-1132,W06-2927,0,0.0538281,"Missing"
C08-1132,P96-1025,0,0.0610114,"the sentences are parsed by the same Chinese deterministic parser used for Chinese Gigaword analysis. The correct dependency relations created by the P(cmi |wi , cpi ) = ∏ P ( D j |wi , cpi ) j = ∏ P( w j , dis j , comma j |wi , cpi ) j 3 1051 http://chasen.org/~taku/software/TinySVM/ (4) Finally, P(wj,disj,commaj |wi,cpi) is divided as P( w j , dis j , comma j |wi , cpi ) = P( w j |wi , cpi ) × P(dis j , comma j |wi , w j , cpi ) (5) Pˆraw (dis j , comma j |wi , w j , cpi ) ; Maximum likelihood estimation is used to estimate P(wROOT|S) on training data set with the smoothing method used in (Collins, 1996). The estimation of P(cpi|wi), P(wj|wi,cpi), and P(disj, commaj |wi,wj,cpi) will be introduced in the following subsections. 3.2 • Estimate the two probabilities only by the case elements from the raw corpus, and represent and them as Pˆraw ( w j |wi , cpi ) • Estimate P(wj|wi,cpi) and P(disj, commaj |wi,wj,cpi) by equation 8 and equation 9. Pˆ ( w j |wi , cpi ) = λelement × Pˆtagged ( w j |wi , cpi ) + (1 − λelement ) × Pˆraw ( w j |wi , cpi ) Estimating P(cpi|wi) by Case Patterns Three steps are used to estimate P(cpi|wi) by maximum likelihood estimation using the constructed case patterns:"
C08-1132,D07-1097,0,0.0338055,"Missing"
C08-1132,C04-1104,0,0.019314,"Missing"
C08-1132,I08-4010,0,0.0423187,"Missing"
C08-1132,N06-1023,1,0.829634,"Missing"
C08-1132,kawahara-kurohashi-2006-case,1,0.8977,"Missing"
C08-1132,W02-2016,0,0.159082,"Missing"
C08-1132,E06-1011,0,0.328393,"ancing good parse selection on the analysis of raw corpus, and integrating pos-tagging into parsing model. Related Work To our current knowledge, there were few works about using case structures in Chinese parsing, except for the work of Wu (2003) and Han et al. (2004). Compared with them, our proposed approach presents a new type of case structures for all kinds of head-modifier pairs, which not only recognizes bi-lexical dependency but also remembers the parsing history of a head node. Parsing history has been used to improve parsing accuracy by many researchers (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006). Yamada and Matsumoto (2003) showed that keeping a small amount of parsing history was useful to improve parsing performance in a shiftreduce parser. McDonald and Pereira (2006) expanded their first-order spanning tree model to be second-order by factoring the score of the tree into the sum of adjacent edge pair scores. In our proposed approach, the case patterns remember the neighboring modifiers for a head node like McDonald and Pereira’s work. But it keeps all the parsing histories of a head, which is different from only keeping adjacent two modifiers in (McDonald and Pereira, 2006). Besid"
C08-1132,D07-1013,0,0.0594151,"Missing"
C08-1132,P07-1052,0,0.023439,"Missing"
C08-1132,D07-1111,0,0.0767765,"(McDonald and Pereira, 2006). Besides, to use the parsing histories in CKY decoding, our approach applies horizontal Markovization during case pattern construction. In general, the success of using case patterns in Chinese parsing in his paper proves again that keeping parsing history is crucial to improve parsing performance, no matter in which way and to which parsing model it is applied. There were also some works that handled lexical preference for Chinese parsing in other ways. For example, Cheng et al. (2006) and Hall et al. (2007) applied shift-reduce deterministic parsing to Chinese. Sagae and Tsujii (2007) generalized the standard deterministic framework to probabilistic parsing by using a best-first search strategy. In these works, lexical preferences were introduced as features for predicting parsing action. Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. Wang et al. (2005) proposed a completely lexicalized bottom-up generative parsing model to parse Chinese, in which a word-similarity-based smooth7 Conclusion and Future Work References T.Abekawa and M.Okumura. 2006. Japanese Dependency Parsing Using Co-occurrence Informa"
C08-1132,W05-1516,0,0.0749552,"Missing"
C08-1132,W03-1717,0,0.233813,"uracy significantly. Besides, although we only apply the proposed approach to Chinese dependency parsing currently, the same idea could be adapted to other languages easily because it doesn’t use any language specific knowledge. There are several future works under consideration, such as modifying the representation of case patterns to make it more robust, enhancing good parse selection on the analysis of raw corpus, and integrating pos-tagging into parsing model. Related Work To our current knowledge, there were few works about using case structures in Chinese parsing, except for the work of Wu (2003) and Han et al. (2004). Compared with them, our proposed approach presents a new type of case structures for all kinds of head-modifier pairs, which not only recognizes bi-lexical dependency but also remembers the parsing history of a head node. Parsing history has been used to improve parsing accuracy by many researchers (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006). Yamada and Matsumoto (2003) showed that keeping a small amount of parsing history was useful to improve parsing performance in a shiftreduce parser. McDonald and Pereira (2006) expanded their first-order spanning tree"
C08-1132,C02-1145,0,0.0791919,"Missing"
C08-1132,W03-1707,0,0.0440487,"Missing"
C08-1132,W03-3023,0,0.242349,"to make it more robust, enhancing good parse selection on the analysis of raw corpus, and integrating pos-tagging into parsing model. Related Work To our current knowledge, there were few works about using case structures in Chinese parsing, except for the work of Wu (2003) and Han et al. (2004). Compared with them, our proposed approach presents a new type of case structures for all kinds of head-modifier pairs, which not only recognizes bi-lexical dependency but also remembers the parsing history of a head node. Parsing history has been used to improve parsing accuracy by many researchers (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006). Yamada and Matsumoto (2003) showed that keeping a small amount of parsing history was useful to improve parsing performance in a shiftreduce parser. McDonald and Pereira (2006) expanded their first-order spanning tree model to be second-order by factoring the score of the tree into the sum of adjacent edge pair scores. In our proposed approach, the case patterns remember the neighboring modifiers for a head node like McDonald and Pereira’s work. But it keeps all the parsing histories of a head, which is different from only keeping adjacent two modifiers in (McDon"
C08-1132,W06-1604,0,0.0404282,"Missing"
C08-1132,W04-1116,0,0.0819482,"Missing"
C08-1132,N07-2051,1,0.888322,"Missing"
C08-1132,P07-2055,0,0.060802,"Missing"
C08-1132,J04-4004,0,\N,Missing
C08-1132,J03-4003,0,\N,Missing
C08-1132,P06-1105,0,\N,Missing
C10-2061,P08-1118,0,0.145329,"Missing"
C10-2061,C08-1031,0,0.0321247,"Missing"
C10-2061,N09-2029,0,0.0304626,"zing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the important relations for taking a bird’s-eye view on a topic. Lerman and McDonald (2009) proposed a method for generating contrastive summaries about given two entities on the basis of KL-divergence. This study is related to ours in the aspect of extracting implicit contrasts, but contrastive summaries are different from contrastive relations between statements in our study. 3 Our Method We propose a method for grasping overall information on the Web on a given query (topic). This method extracts and presents statements that are relevant to a given topic, including direct contrastive statements and contradictory/contrastive relations between these statements. As a unit for statem"
C10-2061,P98-2127,0,0.0187212,"lowing patterns. They are extracted from the statement candidates. (patent system of America is different from φ of Japan · · ·) In this sentence, “nihon” (Japan) has a meaning of “nihon-no tokkyo seido” (patent system of Japan). That is to say, “tokkyo seido” (patent system), which is the attribute of comparison, is omitted. In this study, in addition to patterns of contrastive constructs, we use checking and filtering on the basis of similarity. The use of similarity is inspired by the semantic parallelism between contrasted keywords. As this similarity, we employ distributional similarity (Lin, 1998), which is calculated using automatic dependency parses of 100 million Japanese Web pages. By searching similar keywords from the above sentence, we successfully extract a contrastive keyword pair, “amerika” (America) and “nihon” (Japan), and the above sentence as a direct contrastive statement. Similarly, a target of comparison can be omitted as in the following sentence. (4) nedan-wa gosei senzai-yori takaidesu price-TOP synthetic detergent-ABL high (price of φ is higher than synthetic detergent) 7-Eleven-wa hokano konbini-to 7-Eleven-TOP other convenience store-ABL • X-wa Y-to {chigau |koto"
C10-2061,D08-1002,0,0.193277,"olarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied (Bentivogli et al., 2009). The recent workshops of this series set up a task that recognizes contradictions. Harabagiu et al. (2006), de Marneffe et al. (2008), Voorhees (2008), and Ritter et al. (2008) focused on recognizing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the important relations for taking a bird’s-ey"
C10-2061,I08-2110,1,0.909525,"ly learned probabilities of polarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied (Bentivogli et al., 2009). The recent workshops of this series set up a task that recognizes contradictions. Harabagiu et al. (2006), de Marneffe et al. (2008), Voorhees (2008), and Ritter et al. (2008) focused on recognizing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the"
C10-2061,I08-1025,1,0.833222,"s. Then, we explain our method of extracting direct contrastive statements with contrastive keyword pairs, and identifying contradictory and contrastive relations in detail. 536 3.1 Extraction and Aggregation of Predicate-argument Structures (1) A predicate-argument structure consists of a predicate and one or more arguments that have a dependency relation to the predicate. We extract predicate-argument structures from automatic parses of Web pages on a given topic by using the method of Kawahara et al. (2008). We apply the following procedure to Web pages that are retrieved from the TSUBAKI (Shinzato et al., 2008) open search engine infrastructure, by inputting the topic as a query. 1. Extract important sentences from each Web page. Important sentences are defined as sentences neighboring the topic word(s). 2. Obtain results of morphological analysis (JUMAN1 ) and dependency parsing (KNP2 ) of the important sentences, and extract predicate-argument structures from them. 3. Filter out functional and meaningless predicate-argument structures, which are not relevant to the topic. Pointwise mutual information between the entire Web and the target Web pages for a topic is used. Note that the analyses in ste"
C10-2061,P09-1026,0,0.258539,"omatically identify such pairs. Ganapathibhotla and Liu (2008) proposed a method for detecting which entities (“target” and “basis”) in a direct contrastive statement are preferred by its author. There is also related work that focuses on noncontrastive sentences. Ohshima et al. (2006) extracted coordinated terms, which are semantically broader than our contrastive keyword pairs, using hit counts from a search engine. They made use of syntactic parallelism among coordinated terms. Their task was to input one of coordinated terms as a query, which is different from ours. Somasundaran and Wiebe (2009) presented a method for recognizing a stance in online debates. They formulated this task as debate-side classification and solved it by using automatically learned probabilities of polarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied"
C10-2061,P08-1008,0,0.150366,"ed probabilities of polarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied (Bentivogli et al., 2009). The recent workshops of this series set up a task that recognizes contradictions. Harabagiu et al. (2006), de Marneffe et al. (2008), Voorhees (2008), and Ritter et al. (2008) focused on recognizing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the important relati"
C10-2061,P09-2039,0,0.0162946,"issue. Contrastive relations are the relations between statements in which two entities or issues are contrasted. In particular, we have the following two novel contributions. • We identify contrastive relations between statements, which consist of in-document and cross-document implicit relations. These relations complement direct contrastive statements, which are explicitly mentioned in a single sentence. • We precisely extract direct contrastive statements and contrastive keyword pairs in an unsupervised manner, whereas most previous studies used supervised methods (Jindal and Liu, 2006b; Yang and Ko, 2009). Our system focuses on the Japanese language. For example, Figure 1 shows examples of extracted statements on the topic “gosei senzai” (synthetic detergent). Rounded rectangles represent statements relevant to this topic. The first statement is a direct contrastive statement, which refers to a contrastive keyword pair, “gosei senzai” (synthetic detergent) and “sekken” (soap). The pairs of statements connected with a broad arrow have contradictory relations. The pairs of statements connected with a thin arrow have contrastive relations. Users not only can see what is written on this topic at a"
C10-2061,W07-1401,0,\N,Missing
C10-2061,C98-2122,0,\N,Missing
C10-2061,P09-4001,1,\N,Missing
C10-2101,C00-1004,0,0.0101814,"on, we can rely on automatic parsing. Japanese semantic classification poses an interesting challenge: proper nouns need to be distinguished from common nouns. It is because Japanese has no orthographic distinction between common and proper nouns and no apparent morphosyntactic distinction between them. We explore lexico-syntactic clues that are extracted from automatically parsed text and investigate their effects. 1 Introduction A dictionary plays an important role in Japanese morphological analysis, or the joint task of segmentation and part-of-speech (POS) tagging (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). Like Chinese and Thai, Japanese does not delimit words by white-space. This makes the first step of natural language processing more ambiguous than simple POS tagging. Accordingly, morphemes in a pre-defined dictionary compactly represent our knowledge about both segmentation and POS. One obvious problem with the dictionary-based approach is caused by unknown morphemes, or morphemes not defined in the dictionary. Even though, historically, extensive human resources were used to build high-coverage dictionaries (Yokoi, 1995), texts other than newspaper articles, in particu"
C10-2101,N03-1002,0,0.0518665,"Missing"
C10-2101,C04-1066,0,0.102183,"ˆı, “Gypsy”) メル友 (merutomo, “keypal”) ニート (nˆıto, “NEET”) 囲炉裏 (irori, “hearth”) 圃場 (hojou, “farm field”) メーカ (mˆeka, “manufacturer”) 弊所 (heisho, “our office”) チワワ (chiwawa, “Chihuahua”) マンタ (maNta, “manta”) 甚平 (jiNbei, a kind of clothing) 着メロ (chakumero, “ringtone”) A subPOS refers to a subcategory of noun. For example, PSN-P corresponds to the POS tag “noun-person name”. category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others. in Table 1. 2.3 Related Tasks A line of research is dedicated to identify unknown morphemes with varying degrees of identification. Asahara and Matsumoto (2004) only focus on boundary identification (segmentation) of unknown morphemes. Mori and Nagao (1996), Nagata (1999) and Murawaki and Kurohashi (2008) assign POS tags at the morphology level. Uchimoto et al. (2001) assign full POS tags but unsurprisingly the accuracy is low. Nakagawa and Matsumoto (2006) also assign full POS tags. They address the fact that local information used in previous studies is inherently insufficient and present a method that uses global information, in other words, takes into consideration all occurrences of each unknown word in a document. They report an improvement in"
C10-2101,P98-1023,0,0.0541223,"eis “noise” is attributed to the genitive case marker “no” because it can denote a wide range of relations between two nouns. We might be able to avoid this problem if we focus on “floating” numeral quantifiers. A floating numeral quantifier has no direct dependency relation to the noun to be counted, as in (3) seito ga saN niN keQseki shita student NOM three CL absence do three students were absent, where the numeral quantifier modifies the verb phrase instead of the noun. Further work is needed to anchor floating numeral quantifiers since they bring a different kind of ambiguity themselves (Bond et al., 1998). Closely related to numeral quantifiers are quantificational nouns that appear as “ncf2:ooku” (“many/much”), “ncf2:subete” (“all”) and others. They distinguish common nouns from proper 882 nouns but does not make a further classification. The same is true of other numeral expressions such as “cf:hueru:ga” (“X increase in number”) and “cf:nai:ga” (“there is no X” or “X do not exist”). We found that, other than numeral expressions, some features distinguished common nouns from proper nouns because they indicated the noun denoted an attribute. Such features include “cf:naru:ni” (“ϕ become X”) an"
C10-2101,W06-1670,0,0.015495,"ama and Torisawa, 2008) or a morpheme (Sasano and Kurohashi, 2008). In either case, NER models encode the output of morphological analysis and therefore are affected by its errors. In fact, Saito et al. (2007) report that a majority of unknown named entities (those never appear in a training corpus) contain unknown morphemes as their constituents and that NER models perform poorly on them. A straightforward solution to this problem would be to acquire unknown morphemes and to assign semantic labels to them. Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006). A supersense corresponds to one of the 26 broad categories defined by WordNet (Fellbaum, 1998). Each noun synset is associated with a supersense. For example, “chair” has supersenses PERSON, ARTIFACT and ACT because it belongs to several synsets. Since supersense tagging is studied in English, it differs from our task in several respects. In English, the distinction between common and proper nouns is clear. In fact, the tagging models can use POS features even for unknown nouns. In addition, the syntactic behavior of English nouns is different from that of Japanese nouns (Gil, 1987). Definit"
C10-2101,W03-1022,0,0.0276914,"character (Asahara and Matsumoto, 2003; Kazama and Torisawa, 2008) or a morpheme (Sasano and Kurohashi, 2008). In either case, NER models encode the output of morphological analysis and therefore are affected by its errors. In fact, Saito et al. (2007) report that a majority of unknown named entities (those never appear in a training corpus) contain unknown morphemes as their constituents and that NER models perform poorly on them. A straightforward solution to this problem would be to acquire unknown morphemes and to assign semantic labels to them. Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006). A supersense corresponds to one of the 26 broad categories defined by WordNet (Fellbaum, 1998). Each noun synset is associated with a supersense. For example, “chair” has supersenses PERSON, ARTIFACT and ACT because it belongs to several synsets. Since supersense tagging is studied in English, it differs from our task in several respects. In English, the distinction between common and proper nouns is clear. In fact, the tagging models can use POS features even for unknown nouns. In addition, the syntactic behavior of English nouns is different from t"
C10-2101,W02-1001,0,0.0661671,"oq. “wife”) ラスベガス (rasubegasu, “Las Vegas”) アップル (aQpuru, “Apple/apple”) メルマガ (merumaga, abbr. of “mail magazine”) instances 84 128 131 136 187 1,622 In order to handle polysemy, we evaluated semantic classification on an instance-by-instance basis. We randomly selected 500 instances from the test data and manually assigned the correct labels to them. For comparison purposes, we also classified registered nouns. We split the training data: 829 nouns or 138,971 instances for testing and the rest for training. We trained the model with three online learning algorithms, (1) the averaged version (Collins, 2002) of Perceptron (Crammer and Singer, 2003), (2) the Passive-Aggressive algorithm (Crammer et al., 2006), and (3) the Confidence-Weighted algorithm (Crammer et al., 2009). For PassiveAggressive algorithm, we used PA-I and set parameter C to 1. For Confidence-Weighted, we used the single-constraint updates. All algorithms iterated five times through the training data. 4.2 Results Table 2 shows the results of semantic classification. All algorithms significantly improved over the baseline. As suggested by the gap in accuracy between acquired and registered nouns in the baseline method, the label d"
C10-2101,D09-1052,0,0.013188,"handle polysemy, we evaluated semantic classification on an instance-by-instance basis. We randomly selected 500 instances from the test data and manually assigned the correct labels to them. For comparison purposes, we also classified registered nouns. We split the training data: 829 nouns or 138,971 instances for testing and the rest for training. We trained the model with three online learning algorithms, (1) the averaged version (Collins, 2002) of Perceptron (Crammer and Singer, 2003), (2) the Passive-Aggressive algorithm (Crammer et al., 2006), and (3) the Confidence-Weighted algorithm (Crammer et al., 2009). For PassiveAggressive algorithm, we used PA-I and set parameter C to 1. For Confidence-Weighted, we used the single-constraint updates. All algorithms iterated five times through the training data. 4.2 Results Table 2 shows the results of semantic classification. All algorithms significantly improved over the baseline. As suggested by the gap in accuracy between acquired and registered nouns in the baseline method, the label distribution of the training data differed from that of the test data, but the decrease in accuracy was smaller than expected. The Passive-Aggressive algorithm performed"
C10-2101,P05-1004,0,0.0254129,"oto, 2003; Kazama and Torisawa, 2008) or a morpheme (Sasano and Kurohashi, 2008). In either case, NER models encode the output of morphological analysis and therefore are affected by its errors. In fact, Saito et al. (2007) report that a majority of unknown named entities (those never appear in a training corpus) contain unknown morphemes as their constituents and that NER models perform poorly on them. A straightforward solution to this problem would be to acquire unknown morphemes and to assign semantic labels to them. Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006). A supersense corresponds to one of the 26 broad categories defined by WordNet (Fellbaum, 1998). Each noun synset is associated with a supersense. For example, “chair” has supersenses PERSON, ARTIFACT and ACT because it belongs to several synsets. Since supersense tagging is studied in English, it differs from our task in several respects. In English, the distinction between common and proper nouns is clear. In fact, the tagging models can use POS features even for unknown nouns. In addition, the syntactic behavior of English nouns is different from that of Japanes"
C10-2101,P09-1073,0,0.0274314,"Missing"
C10-2101,H01-1043,1,0.846229,"Missing"
C10-2101,kawahara-kurohashi-2006-case,1,0.891403,"Missing"
C10-2101,J93-2004,0,0.0331946,"and Nagao, 1996; Murawaki and Kurohashi, 2008) directly or indirectly reply on morphology, or our knowledge on how a morpheme behaves in a sequence of morphemes. This means that semantic level distinction is difficult to make in these approaches, and in fact, is left unresolved. To be specific, nouns are only distinguished from verbs and adjectives but they have subcategories in the original tagset. These are what we try to classify acquired nouns into in this paper. 2.2 Semantic Labels The Japanese noun subcategories may require an explanation since they are different from the English ones (Marcus et al., 1993) in many respects. Singular and mass nouns are not distinguished from plural nouns because Japanese has no grammatical distinction between them. More importantly for this paper, proper nouns have subcategories such as person name, location name and organization name in addition to the distinction from common nouns. These subcategories provide important information to named entity recognition among other applications. For proper nouns, we adopt these subcategories as semantic labels in our task. In contrast to proper nouns, common nouns have only one subcategory “common.” However, we consider t"
C10-2101,C96-2202,0,0.520758,"ongly segments the phrase “さっぽ ろ駅” (saQporo eki, “Sapporo Station”), where “ さっぽろ” (saQporo) is an unknown morpheme, as follows: “さ” (sa, noun-common, “difference”), “っ” (Q, UNK), “ぽ” (po, UNK), “ろ” (ro, noun-common, “sumac”) and “駅” (eki, noun-common, “station”), where UNK refers to unknown morphemes automatically identified by the analyzer. Such an erroneous sequence has disastrous effects on applications of morphological analysis. For example, it can hardly be identified as a LOCATION in named entity recognition. One solution to the unknown morpheme problem is unknown morpheme acquisition (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008). It is the task of automatically augmenting the dictionary by acquiring unknown morphemes from text. In the above example, the goal is to acquire the morpheme “さっぽろ” (saQporo) with the POS tag “noun-location name.” However, unknown morpheme acquisition usually adopts a coarser POS tagset that only represents the morphology level distinction among noun, verb and adjective. This means that “さっぽろ” (saQporo) is acquired as just a noun and that the semantic label “location name” remains to be assigned. The reason only the morphology level distinction is made is 1 htt"
C10-2101,D08-1045,1,0.535895,"ase “さっぽ ろ駅” (saQporo eki, “Sapporo Station”), where “ さっぽろ” (saQporo) is an unknown morpheme, as follows: “さ” (sa, noun-common, “difference”), “っ” (Q, UNK), “ぽ” (po, UNK), “ろ” (ro, noun-common, “sumac”) and “駅” (eki, noun-common, “station”), where UNK refers to unknown morphemes automatically identified by the analyzer. Such an erroneous sequence has disastrous effects on applications of morphological analysis. For example, it can hardly be identified as a LOCATION in named entity recognition. One solution to the unknown morpheme problem is unknown morpheme acquisition (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008). It is the task of automatically augmenting the dictionary by acquiring unknown morphemes from text. In the above example, the goal is to acquire the morpheme “さっぽろ” (saQporo) with the POS tag “noun-location name.” However, unknown morpheme acquisition usually adopts a coarser POS tagset that only represents the morphology level distinction among noun, verb and adjective. This means that “さっぽろ” (saQporo) is acquired as just a noun and that the semantic label “location name” remains to be assigned. The reason only the morphology level distinction is made is 1 http://nlp.kuee.kyoto-u.ac.jp/ nl-"
C10-2101,P99-1036,0,0.0128113,"メーカ (mˆeka, “manufacturer”) 弊所 (heisho, “our office”) チワワ (chiwawa, “Chihuahua”) マンタ (maNta, “manta”) 甚平 (jiNbei, a kind of clothing) 着メロ (chakumero, “ringtone”) A subPOS refers to a subcategory of noun. For example, PSN-P corresponds to the POS tag “noun-person name”. category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others. in Table 1. 2.3 Related Tasks A line of research is dedicated to identify unknown morphemes with varying degrees of identification. Asahara and Matsumoto (2004) only focus on boundary identification (segmentation) of unknown morphemes. Mori and Nagao (1996), Nagata (1999) and Murawaki and Kurohashi (2008) assign POS tags at the morphology level. Uchimoto et al. (2001) assign full POS tags but unsurprisingly the accuracy is low. Nakagawa and Matsumoto (2006) also assign full POS tags. They address the fact that local information used in previous studies is inherently insufficient and present a method that uses global information, in other words, takes into consideration all occurrences of each unknown word in a document. They report an improvement in tagging proper nouns in Japanese. A related task is named entity recognition (NER). It can handle a named entity"
C10-2101,P06-1089,0,0.0133512,"fers to a subcategory of noun. For example, PSN-P corresponds to the POS tag “noun-person name”. category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others. in Table 1. 2.3 Related Tasks A line of research is dedicated to identify unknown morphemes with varying degrees of identification. Asahara and Matsumoto (2004) only focus on boundary identification (segmentation) of unknown morphemes. Mori and Nagao (1996), Nagata (1999) and Murawaki and Kurohashi (2008) assign POS tags at the morphology level. Uchimoto et al. (2001) assign full POS tags but unsurprisingly the accuracy is low. Nakagawa and Matsumoto (2006) also assign full POS tags. They address the fact that local information used in previous studies is inherently insufficient and present a method that uses global information, in other words, takes into consideration all occurrences of each unknown word in a document. They report an improvement in tagging proper nouns in Japanese. A related task is named entity recognition (NER). It can handle a named entity longer than a single morpheme and is usually formalized as a chunking problem. Since Japanese does not delimit words by white-space, the unit of chunking can be a character (Asahara and Ma"
C10-2101,I08-2080,1,0.915013,"identification (segmentation) and review the Japanese POS tagset to see why we propose a two-stage approach to assign full POS tags. The Japanese POS tagset derives from traditional grammar. It is a mixture of several linguistic levels: morphology, syntax and semantics. In other words, information encoded in a POS tag is more than how the morpheme behaves in a sequence of morphemes. In fact, POS tags given to pre-defined morphemes are useful for applications of morphological analysis, such as dependency parsing (Kudo and Matsumoto, 2002), named entity recognition (Asahara and Matsumoto, 2003; Sasano and Kurohashi, 2008) and anaphora resolution (Iida et al., 2009; Sasano and Kurohashi, 2009). In these applications, POS tags are incorporated as features for models. On the other hand, the mixed nature of the POS tagset poses a challenge to unknown morpheme acquisition. Previous approaches (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) directly or indirectly reply on morphology, or our knowledge on how a morpheme behaves in a sequence of morphemes. This means that semantic level distinction is difficult to make in these approaches, and in fact, is left unresolved. To be specific, nouns are only distinguish"
C10-2101,P08-1047,0,0.0189727,"ll POS tags. They address the fact that local information used in previous studies is inherently insufficient and present a method that uses global information, in other words, takes into consideration all occurrences of each unknown word in a document. They report an improvement in tagging proper nouns in Japanese. A related task is named entity recognition (NER). It can handle a named entity longer than a single morpheme and is usually formalized as a chunking problem. Since Japanese does not delimit words by white-space, the unit of chunking can be a character (Asahara and Matsumoto, 2003; Kazama and Torisawa, 2008) or a morpheme (Sasano and Kurohashi, 2008). In either case, NER models encode the output of morphological analysis and therefore are affected by its errors. In fact, Saito et al. (2007) report that a majority of unknown named entities (those never appear in a training corpus) contain unknown morphemes as their constituents and that NER models perform poorly on them. A straightforward solution to this problem would be to acquire unknown morphemes and to assign semantic labels to them. Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 20"
C10-2101,D09-1151,1,0.927934,"why we propose a two-stage approach to assign full POS tags. The Japanese POS tagset derives from traditional grammar. It is a mixture of several linguistic levels: morphology, syntax and semantics. In other words, information encoded in a POS tag is more than how the morpheme behaves in a sequence of morphemes. In fact, POS tags given to pre-defined morphemes are useful for applications of morphological analysis, such as dependency parsing (Kudo and Matsumoto, 2002), named entity recognition (Asahara and Matsumoto, 2003; Sasano and Kurohashi, 2008) and anaphora resolution (Iida et al., 2009; Sasano and Kurohashi, 2009). In these applications, POS tags are incorporated as features for models. On the other hand, the mixed nature of the POS tagset poses a challenge to unknown morpheme acquisition. Previous approaches (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) directly or indirectly reply on morphology, or our knowledge on how a morpheme behaves in a sequence of morphemes. This means that semantic level distinction is difficult to make in these approaches, and in fact, is left unresolved. To be specific, nouns are only distinguished from verbs and adjectives but they have subcategories in the original"
C10-2101,P06-1141,0,0.0500597,"f noun. For example, PSN-P corresponds to the POS tag “noun-person name”. category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others. in Table 1. 2.3 Related Tasks A line of research is dedicated to identify unknown morphemes with varying degrees of identification. Asahara and Matsumoto (2004) only focus on boundary identification (segmentation) of unknown morphemes. Mori and Nagao (1996), Nagata (1999) and Murawaki and Kurohashi (2008) assign POS tags at the morphology level. Uchimoto et al. (2001) assign full POS tags but unsurprisingly the accuracy is low. Nakagawa and Matsumoto (2006) also assign full POS tags. They address the fact that local information used in previous studies is inherently insufficient and present a method that uses global information, in other words, takes into consideration all occurrences of each unknown word in a document. They report an improvement in tagging proper nouns in Japanese. A related task is named entity recognition (NER). It can handle a named entity longer than a single morpheme and is usually formalized as a chunking problem. Since Japanese does not delimit words by white-space, the unit of chunking can be a character (Asahara and Ma"
C10-2101,C04-1174,1,0.810068,"n advantage. Since no manual annotation is necessary, we can utilize a huge raw corpus. On the other hand, parsing errors are inevitable. However, we can circumvent this problem by using the constraints of Japanese dependency structures: head-final and projective. The simplest example is the second last element of a sentence, which always depends on the last element. With these constraints, we can focus on syntactically unambiguous dependency pairs and extract syntactic features accurately. We follow Kawahara and Kurohashi (2001) to extract a pair of an argument noun and a predicate (cf), and Sasano et al. (2004) to extract a pair of nouns connected with the genitive case marker “no” (ncf1 and ncf2). Noun X can be part of a compound noun. We leave it for named entity recognition. Except for suf, we extract features only when X alone forms a word. Similarly, we extract suf features only when X and a suffix alone form a noun phrase. For call, ncf1, and ncf2, we generalize numerals within noun phrases. For “hoN” (book) in example 1, we extract the feature “ncf2:&lt;NUM&gt;satsu.” 3.2 Instances for Classification Now that features are extracted for each noun, the question is how to combine them together to make"
C10-2101,W02-2016,0,0.0130274,"ext and assign POS tags to them. In this section, we omit the details of boundary identification (segmentation) and review the Japanese POS tagset to see why we propose a two-stage approach to assign full POS tags. The Japanese POS tagset derives from traditional grammar. It is a mixture of several linguistic levels: morphology, syntax and semantics. In other words, information encoded in a POS tag is more than how the morpheme behaves in a sequence of morphemes. In fact, POS tags given to pre-defined morphemes are useful for applications of morphological analysis, such as dependency parsing (Kudo and Matsumoto, 2002), named entity recognition (Asahara and Matsumoto, 2003; Sasano and Kurohashi, 2008) and anaphora resolution (Iida et al., 2009; Sasano and Kurohashi, 2009). In these applications, POS tags are incorporated as features for models. On the other hand, the mixed nature of the POS tagset poses a challenge to unknown morpheme acquisition. Previous approaches (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) directly or indirectly reply on morphology, or our knowledge on how a morpheme behaves in a sequence of morphemes. This means that semantic level distinction is difficult to make in these app"
C10-2101,W04-3230,0,0.0391361,"parsing. Japanese semantic classification poses an interesting challenge: proper nouns need to be distinguished from common nouns. It is because Japanese has no orthographic distinction between common and proper nouns and no apparent morphosyntactic distinction between them. We explore lexico-syntactic clues that are extracted from automatically parsed text and investigate their effects. 1 Introduction A dictionary plays an important role in Japanese morphological analysis, or the joint task of segmentation and part-of-speech (POS) tagging (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). Like Chinese and Thai, Japanese does not delimit words by white-space. This makes the first step of natural language processing more ambiguous than simple POS tagging. Accordingly, morphemes in a pre-defined dictionary compactly represent our knowledge about both segmentation and POS. One obvious problem with the dictionary-based approach is caused by unknown morphemes, or morphemes not defined in the dictionary. Even though, historically, extensive human resources were used to build high-coverage dictionaries (Yokoi, 1995), texts other than newspaper articles, in particular web pages, conta"
C10-2101,W01-0512,0,0.0312581,"ta, “manta”) 甚平 (jiNbei, a kind of clothing) 着メロ (chakumero, “ringtone”) A subPOS refers to a subcategory of noun. For example, PSN-P corresponds to the POS tag “noun-person name”. category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others. in Table 1. 2.3 Related Tasks A line of research is dedicated to identify unknown morphemes with varying degrees of identification. Asahara and Matsumoto (2004) only focus on boundary identification (segmentation) of unknown morphemes. Mori and Nagao (1996), Nagata (1999) and Murawaki and Kurohashi (2008) assign POS tags at the morphology level. Uchimoto et al. (2001) assign full POS tags but unsurprisingly the accuracy is low. Nakagawa and Matsumoto (2006) also assign full POS tags. They address the fact that local information used in previous studies is inherently insufficient and present a method that uses global information, in other words, takes into consideration all occurrences of each unknown word in a document. They report an improvement in tagging proper nouns in Japanese. A related task is named entity recognition (NER). It can handle a named entity longer than a single morpheme and is usually formalized as a chunking problem. Since Japanese doe"
C10-2101,C98-1023,0,\N,Missing
C10-2101,1995.mtsummit-1.17,0,\N,Missing
C12-1067,D11-1003,0,0.017632,"., 2002; Yamagata et al., 2006). Nomoto generated candidates for a compression by removing bunsetsu from a source sentence and selected the best candidate using a conditional random field (Nomoto, 2008). Our work differs from these efforts in that our method has the ability to drop unimportant words from a bunsetsu. In our method, we used Lagrangian relaxation to relax unit constraints. Lagrangian relaxation is a well known technique for combinatorial optimization and it has recently been successfully applied to various natural language processing tasks (Koo et al., 2010; M.Rush et al., 2010; Chang and Collins, 2011; M.Rush and Collins, 2011). However, to the best of our knowledge, this is the first work to use the technique for sentence compression. Conclusions and Future Work We presented a novel compression method for a Japanese sentence. The proposed method was loosely based on bunsetsu-based methods. Thus, unlike word-based methods, it could easily produce grammatical compressions. Additionally, using Lagrangian relaxation, the proposed method relaxed constraints that troubled bunsetsu-based methods. In this way, unlike bunsetsu-based methods, our method could shorten each bunsetsu if it contained u"
C12-1067,P09-1093,0,0.431241,"ween Japan and Canada made a discovery) This sentence is composed of four bunsetsu: “nihon to”, “kanada no”, “kokusai kyoudou kenkyuu guru-pu ga”, and “hakken shita”. As seen in this example, a Japanese sentence can be viewed as a bunsetsu sequence as well as a word sequence. This characteristic of the Japanese language has led researchers to take two compression approaches: word-based methods and bunsetsu-based methods. Word-based methods view a source sentence as a word sequence and generate a compressed sentence by selecting a subset of words from the source sentence (Hori and Furui, 2004; Hirao et al., 2009). However, the methods do not take account of bunsetsu, and it is thus difficult to generate grammatical compressions. For example, if only content words (or only function words) in a bunsetsu are selected, the grammaticality of the corresponding part in the compressed sentence would be poor. We can avoid this problem using bunsetsu-based methods. Bunsetsu-based methods view a source sentence as a bunsetsu sequence and generate a compressed sentence by selecting a subset of bunsetsu from the source sentence (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Yamagata et al., 2006; Nomoto, 2008)"
C12-1067,A00-1043,0,0.624007,"的には 後者の方が良く機能する．しかし，文節ベースの手法は，文節をユニットとして扱うという 制約があるため，個々の文節を圧縮できない．本稿では，この欠点を克服する新しい圧縮手 法を提案する．提案手法はラグランジュ緩和を用いて上の制約を緩和し，各文節を圧縮する． 実験の結果，提案手法によって原文の情報を多く保持する文法的な圧縮文を生成できること が分かった． KEYWORDS: sentence compression, Lagrangian relaxation. KEYWORDS IN JAPANESE: 文圧縮, ラグランジュ緩和. Proceedings of COLING 2012: Technical Papers, pages 1097–1112, COLING 2012, Mumbai, December 2012. 1097 1 Introduction Sentence compression is the task of shortening a sentence while preserving its important information and grammaticality. This task is important in a wide range of applications such as automatic summarization (Jing, 2000; Lin, 2003; Zajic et al., 2007), subtitle generation (Vandeghinste and Pan, 2004), and displaying text on small screens (Corston-Oliver, 2001). In this paper, we propose a novel compression method for a Japanese sentence. Like other languages, Japanese uses sentences composed of words. However, we can also say that a Japanese sentence is composed of bunsetsu. Bunsetsu is a text unit that consists of one or more content words and possibly one or more function words. For example, consider the following sentence.1 (1) nihon to kanada no kokusai kyoudou kenkyuu guru-pu ga hakken shita Japan CNJ C"
C12-1067,D10-1125,0,0.0261617,"tsu from a source sentence (Oguro et al., 2002; Yamagata et al., 2006). Nomoto generated candidates for a compression by removing bunsetsu from a source sentence and selected the best candidate using a conditional random field (Nomoto, 2008). Our work differs from these efforts in that our method has the ability to drop unimportant words from a bunsetsu. In our method, we used Lagrangian relaxation to relax unit constraints. Lagrangian relaxation is a well known technique for combinatorial optimization and it has recently been successfully applied to various natural language processing tasks (Koo et al., 2010; M.Rush et al., 2010; Chang and Collins, 2011; M.Rush and Collins, 2011). However, to the best of our knowledge, this is the first work to use the technique for sentence compression. Conclusions and Future Work We presented a novel compression method for a Japanese sentence. The proposed method was loosely based on bunsetsu-based methods. Thus, unlike word-based methods, it could easily produce grammatical compressions. Additionally, using Lagrangian relaxation, the proposed method relaxed constraints that troubled bunsetsu-based methods. In this way, unlike bunsetsu-based methods, our method"
C12-1067,W03-1101,0,0.0352564,"能する．しかし，文節ベースの手法は，文節をユニットとして扱うという 制約があるため，個々の文節を圧縮できない．本稿では，この欠点を克服する新しい圧縮手 法を提案する．提案手法はラグランジュ緩和を用いて上の制約を緩和し，各文節を圧縮する． 実験の結果，提案手法によって原文の情報を多く保持する文法的な圧縮文を生成できること が分かった． KEYWORDS: sentence compression, Lagrangian relaxation. KEYWORDS IN JAPANESE: 文圧縮, ラグランジュ緩和. Proceedings of COLING 2012: Technical Papers, pages 1097–1112, COLING 2012, Mumbai, December 2012. 1097 1 Introduction Sentence compression is the task of shortening a sentence while preserving its important information and grammaticality. This task is important in a wide range of applications such as automatic summarization (Jing, 2000; Lin, 2003; Zajic et al., 2007), subtitle generation (Vandeghinste and Pan, 2004), and displaying text on small screens (Corston-Oliver, 2001). In this paper, we propose a novel compression method for a Japanese sentence. Like other languages, Japanese uses sentences composed of words. However, we can also say that a Japanese sentence is composed of bunsetsu. Bunsetsu is a text unit that consists of one or more content words and possibly one or more function words. For example, consider the following sentence.1 (1) nihon to kanada no kokusai kyoudou kenkyuu guru-pu ga hakken shita Japan CNJ Canada GEN i"
C12-1067,W04-1013,0,0.0109216,"Missing"
C12-1067,E06-1038,0,0.101536,"ion has been widely studied since the early 2000s. For the English language, Jing used multiple knowledge resources to decide which phrases in a source sentence to remove (Jing, 2000). Knight and Marcu modeled a generative process of a source sentence based on a noisy-channel framework and generated a compressed sentence using the model (Knight and Marcu, 2002). Turner and Charniak presented semi-supervised and unsupervised variants of the Knight and Marcu’s model (Turner and Charniak, 2005). McDonald employed a discriminative model to learn which words in a source sentence should be dropped (McDonald, 2006). Clarke and Lapata recasted previous methods as ILP and extended those with various constraints (Clarke and Lapata, 2008). Our work differs from these efforts in that we focus on Japanese sentence compression. For the Japanese language, previous compression methods can be divided into two groups: word-based methods and bunsetsu-based methods. Hori and Furui proposed a word-based method to summarize speech (Hori and Furui, 2004). They extracted a set of important words from an automatically transcribed sentence. Hirao et al. also proposed a word-based method (Hirao et al., 2009). They extended"
C12-1067,P11-1008,0,0.0146734,"006). Nomoto generated candidates for a compression by removing bunsetsu from a source sentence and selected the best candidate using a conditional random field (Nomoto, 2008). Our work differs from these efforts in that our method has the ability to drop unimportant words from a bunsetsu. In our method, we used Lagrangian relaxation to relax unit constraints. Lagrangian relaxation is a well known technique for combinatorial optimization and it has recently been successfully applied to various natural language processing tasks (Koo et al., 2010; M.Rush et al., 2010; Chang and Collins, 2011; M.Rush and Collins, 2011). However, to the best of our knowledge, this is the first work to use the technique for sentence compression. Conclusions and Future Work We presented a novel compression method for a Japanese sentence. The proposed method was loosely based on bunsetsu-based methods. Thus, unlike word-based methods, it could easily produce grammatical compressions. Additionally, using Lagrangian relaxation, the proposed method relaxed constraints that troubled bunsetsu-based methods. In this way, unlike bunsetsu-based methods, our method could shorten each bunsetsu if it contained unimportant words. Experimen"
C12-1067,D10-1001,0,0.0205967,"ntence (Oguro et al., 2002; Yamagata et al., 2006). Nomoto generated candidates for a compression by removing bunsetsu from a source sentence and selected the best candidate using a conditional random field (Nomoto, 2008). Our work differs from these efforts in that our method has the ability to drop unimportant words from a bunsetsu. In our method, we used Lagrangian relaxation to relax unit constraints. Lagrangian relaxation is a well known technique for combinatorial optimization and it has recently been successfully applied to various natural language processing tasks (Koo et al., 2010; M.Rush et al., 2010; Chang and Collins, 2011; M.Rush and Collins, 2011). However, to the best of our knowledge, this is the first work to use the technique for sentence compression. Conclusions and Future Work We presented a novel compression method for a Japanese sentence. The proposed method was loosely based on bunsetsu-based methods. Thus, unlike word-based methods, it could easily produce grammatical compressions. Additionally, using Lagrangian relaxation, the proposed method relaxed constraints that troubled bunsetsu-based methods. In this way, unlike bunsetsu-based methods, our method could shorten each b"
C12-1067,P08-1035,0,0.19704,"et al., 2009). However, the methods do not take account of bunsetsu, and it is thus difficult to generate grammatical compressions. For example, if only content words (or only function words) in a bunsetsu are selected, the grammaticality of the corresponding part in the compressed sentence would be poor. We can avoid this problem using bunsetsu-based methods. Bunsetsu-based methods view a source sentence as a bunsetsu sequence and generate a compressed sentence by selecting a subset of bunsetsu from the source sentence (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Yamagata et al., 2006; Nomoto, 2008). Bunsetsu-based methods treat each bunsetsu as a unit. Thus, the methods do not suffer from the above problem and they can generate compressions that are more grammatically correct than those generated by word-based methods. However, bunsetsu-based methods have a disadvantage in that they cannot shorten each bunsetsu in a source sentence. More precisely, when there is a compound noun in a bunsetsu and the noun contains unimportant words, bunsetsu-based methods cannot drop those words from the noun. Consider the above sentence (1) as an example. The third bunsetsu “kokusai kyoudou kenkyuu guru"
C12-1067,P05-1036,0,0.418459,"or e(“軍”) was 0.378). In this way, PROP w/ DPND could produce the same compression as HUMAN. 1109 6 Related Work Sentence compression has been widely studied since the early 2000s. For the English language, Jing used multiple knowledge resources to decide which phrases in a source sentence to remove (Jing, 2000). Knight and Marcu modeled a generative process of a source sentence based on a noisy-channel framework and generated a compressed sentence using the model (Knight and Marcu, 2002). Turner and Charniak presented semi-supervised and unsupervised variants of the Knight and Marcu’s model (Turner and Charniak, 2005). McDonald employed a discriminative model to learn which words in a source sentence should be dropped (McDonald, 2006). Clarke and Lapata recasted previous methods as ILP and extended those with various constraints (Clarke and Lapata, 2008). Our work differs from these efforts in that we focus on Japanese sentence compression. For the Japanese language, previous compression methods can be divided into two groups: word-based methods and bunsetsu-based methods. Hori and Furui proposed a word-based method to summarize speech (Hori and Furui, 2004). They extracted a set of important words from an"
C12-1067,W04-1015,0,0.221054,"い．本稿では，この欠点を克服する新しい圧縮手 法を提案する．提案手法はラグランジュ緩和を用いて上の制約を緩和し，各文節を圧縮する． 実験の結果，提案手法によって原文の情報を多く保持する文法的な圧縮文を生成できること が分かった． KEYWORDS: sentence compression, Lagrangian relaxation. KEYWORDS IN JAPANESE: 文圧縮, ラグランジュ緩和. Proceedings of COLING 2012: Technical Papers, pages 1097–1112, COLING 2012, Mumbai, December 2012. 1097 1 Introduction Sentence compression is the task of shortening a sentence while preserving its important information and grammaticality. This task is important in a wide range of applications such as automatic summarization (Jing, 2000; Lin, 2003; Zajic et al., 2007), subtitle generation (Vandeghinste and Pan, 2004), and displaying text on small screens (Corston-Oliver, 2001). In this paper, we propose a novel compression method for a Japanese sentence. Like other languages, Japanese uses sentences composed of words. However, we can also say that a Japanese sentence is composed of bunsetsu. Bunsetsu is a text unit that consists of one or more content words and possibly one or more function words. For example, consider the following sentence.1 (1) nihon to kanada no kokusai kyoudou kenkyuu guru-pu ga hakken shita Japan CNJ Canada GEN international collaborative research group NOM discover did (An internat"
C12-1117,C12-1117,1,0.0513136,"Missing"
C12-1117,P11-1070,0,0.153073,"Missing"
C12-1117,P10-1089,0,0.288916,"Missing"
C12-1117,D07-1090,0,0.0575815,"Missing"
C12-1117,N10-1081,0,0.260817,"Missing"
C12-1117,W02-1001,0,0.417986,"Missing"
C12-1117,C96-1058,0,0.19916,"Missing"
C12-1117,P99-1059,0,0.452284,"Missing"
C12-1117,I11-1087,0,0.147224,"Missing"
C12-1117,P07-1022,0,0.440494,"Missing"
C12-1117,D11-1089,0,0.279343,"Missing"
C12-1117,H01-1043,1,0.586986,"Missing"
C12-1117,kawahara-kurohashi-2006-case,1,0.86104,"Missing"
C12-1117,N03-1017,0,0.0101296,"Missing"
C12-1117,P10-1001,0,0.0582135,"Missing"
C12-1117,W02-2016,0,0.359184,"Missing"
C12-1117,N04-1016,0,0.129001,"Missing"
C12-1117,P95-1007,0,0.567534,"Missing"
C12-1117,J93-2004,0,0.0404798,"Missing"
C12-1117,P05-1012,0,0.268835,"Missing"
C12-1117,W02-1407,0,0.345597,"Missing"
C12-1117,W05-0603,0,0.17648,"Missing"
C12-1117,H05-1105,0,0.184994,"Missing"
C12-1117,C10-1100,0,0.300445,"Missing"
C12-1117,P09-2012,0,0.184366,"Missing"
C12-1117,J93-2005,0,0.0441754,"Missing"
C12-1117,N09-1059,1,0.810099,"Missing"
C12-1117,P11-2036,0,0.205655,"Missing"
C12-1117,E99-1026,0,0.642389,"Missing"
C12-1117,P07-1031,0,0.206716,"Missing"
C12-1117,E06-1011,0,\N,Missing
C12-1117,H01-1052,0,\N,Missing
C12-1120,N10-1015,0,0.0237969,"ts and derivations, respectively. sentence “音 列 の (of tone sequence)” has no corresponding part in the English sentence. “音 列 (tone sequence)” is correctly aligned to NULL, but the function word “の (of)” is incorrectly derived from “法 (method)”. On the right of Figure 6, the Japanese “HDMI は” should not depend on “異なる (different)”, but on “使用 (use)”. Because of this parsing error, the topic marker “は” is incorrectly derived from “異なる (different)”. One possible short-term solution for the parsing problem is to use the n-best parsing results in the model. An alternative solution was proposed by Burkett et al. (2010), who described a joint parsing and alignment model that exchanges useful information between the parser and aligner. 7 Translation experiments We conducted English-to-Japanese and Japanese-to-English translation experiments on the same corpus used in the alignment experiments. We translated 500 paper abstract sentences from the JST corpus. Note that these sentences were not included in the training corpus. We use Joshua4 , a Java-based opensource implementation of the hierarchical decoder, version 4.0 (Ganitkevitch et al., 2012) with default settings. It was tuned using another 500 developmen"
C12-1120,P05-1022,0,0.695573,"ees, function words giving additional information to content words are placed as children of the content words, thus it preserves the dependency relations between words over languages. In the semantic-head dependency tree (on the right of Figure 2), “medical treatment ↔ 治療”, “may ↔ かも しれ ない”, “not ↔ ない” and “weight ↔ 体重” are all children of “change ↔ 変化”, while the relations are not preserved in the syntactic-head dependency tree (on the left of Figure 2). Because of these advantages, our model uses semantic-head dependency trees. In this paper, English sentences are first parsed by nlparser (Charniak and Johnson, 2005) which outputs phrase structures that are then converted into word dependency trees by defining the head word for phrases. The conversion rules follow Collins’ head percolation table (Collins, 1999) with some modifications for acquiring semantic-head dependency trees. The following head-specifying rules are examples in which the syntactic head (underlined) and the semantic head (double underlined) is different. • VP → MD • VP → VBZ VB (ex. &quot;may change&quot; in Figure 2) JJ (ex. &quot;is large&quot;) Japanese sentences are usually parsed based on a unit called a basic phrase, which consists of one content wor"
C12-1120,D08-1033,0,0.0429933,"Missing"
C12-1120,P07-1003,0,0.025874,"dependency structures by rules defining head words for phrases (Collins, 1999). Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994) and the dependency analyzer KNP (Kawahara and Kurohashi, 2006b). For comparison, we used GIZA++ (Och and Ney, 2003), which implements the well-known word-based statistical alignment model of the IBM Models. We conducted word alignment bidirectionally with the default parameters and merged them using the grow-diag-final-and heuristics (Koehn et al., 2003). We also tested the BerkeleyAligner3 (DeNero and Klein, 2007) in the unsupervised training mode with default settings. 6.2 Experimental result and discussion The experimental results are given in Table 2. “Syntactic-head” is the alignment accuracy of the baseline system by Nakazawa and Kurohashi (2011), while “Semantic-head w/o derivation” is the result of using the baseline model on semantic-head dependency trees. The results of incorporating the monolingual derivation are given in the bottom two rows, where “all” means that we evaluated all the alignments including derivations, while “core” means that we only evaluated the core alignments. As mentione"
C12-1120,W12-3134,0,0.0297125,"parsing results in the model. An alternative solution was proposed by Burkett et al. (2010), who described a joint parsing and alignment model that exchanges useful information between the parser and aligner. 7 Translation experiments We conducted English-to-Japanese and Japanese-to-English translation experiments on the same corpus used in the alignment experiments. We translated 500 paper abstract sentences from the JST corpus. Note that these sentences were not included in the training corpus. We use Joshua4 , a Java-based opensource implementation of the hierarchical decoder, version 4.0 (Ganitkevitch et al., 2012) with default settings. It was tuned using another 500 development sentence pairs. Table 3 shows the BLEU (Papineni et al., 2002) scores for the translations. The proposed 4 http://joshua-decoder.org 1975 ! ! ! ! &quot;&quot;&quot;! Figure 6: Alignment errors of the proposed model caused by a NULL part (left) and a parsing error (right). Alignment model En → Ja Ja → En GIZA++ & grow-diag-final-and 23.84 17.75 Syntactic-head (baseline) 24.16 17.83 Semantic-head w/o derivation 24.11 18.06 Semantic-head w/ derivation (all) 24.55† 18.46†‡ 24.45 17.76 Semantic-head w/ derivation (core) Table 3: BLEU scores for En"
C12-1120,W09-1201,0,0.0937976,"Missing"
C12-1120,P09-2059,0,0.048517,"Missing"
C12-1120,W10-1736,0,0.0251259,"Missing"
C12-1120,kawahara-kurohashi-2006-case,1,0.779601,"uency with which deC is connected to A(eC , deC ) in the monolingual corpus. Taking each sentence in Figure 3 as an example sentence in the monolingual corpus, we can enumerate the derivations shown in Table 1 from the sentences. A derivation must be contiguous as a tree, and we do not consider sibling derivations. We distinguish three types of derivations: parent, pre-child (dependent from the left) and post-child (dependent from the right). This lexicalized derivation is excessively specific. For example, the highest probability derivations from “Ph.D.” acquired from the English Web corpus (Kawahara and Kurohashi, 2006a) are “a”, “student”, “thesis” in order. Consequently, using only lexicalized derivation can cause many derivation errors. We consider not only the lexicalized derivation probability, but also another probability using part-of-speech (POS) is used as the condition. Using the notation A pos (eC , deC ) for the POS of the anchor word, the monolingual derivation probability is defined as: 1 (11) p(deC |eC ) = [p(deC |A(eC , deC )) · p(deC |A pos (eC , deC ))] 2 . p(deC |A pos (eC , deC )) is also acquired from the large monolingual corpus in the same manner as p(deC |A(eC , deC )). We take the g"
C12-1120,N06-1023,1,0.945061,"uency with which deC is connected to A(eC , deC ) in the monolingual corpus. Taking each sentence in Figure 3 as an example sentence in the monolingual corpus, we can enumerate the derivations shown in Table 1 from the sentences. A derivation must be contiguous as a tree, and we do not consider sibling derivations. We distinguish three types of derivations: parent, pre-child (dependent from the left) and post-child (dependent from the right). This lexicalized derivation is excessively specific. For example, the highest probability derivations from “Ph.D.” acquired from the English Web corpus (Kawahara and Kurohashi, 2006a) are “a”, “student”, “thesis” in order. Consequently, using only lexicalized derivation can cause many derivation errors. We consider not only the lexicalized derivation probability, but also another probability using part-of-speech (POS) is used as the condition. Using the notation A pos (eC , deC ) for the POS of the anchor word, the monolingual derivation probability is defined as: 1 (11) p(deC |eC ) = [p(deC |A(eC , deC )) · p(deC |A pos (eC , deC ))] 2 . p(deC |A pos (eC , deC )) is also acquired from the large monolingual corpus in the same manner as p(deC |A(eC , deC )). We take the g"
C12-1120,W04-3250,0,0.0209261,"or the translations. The proposed 4 http://joshua-decoder.org 1975 ! ! ! ! &quot;&quot;&quot;! Figure 6: Alignment errors of the proposed model caused by a NULL part (left) and a parsing error (right). Alignment model En → Ja Ja → En GIZA++ & grow-diag-final-and 23.84 17.75 Syntactic-head (baseline) 24.16 17.83 Semantic-head w/o derivation 24.11 18.06 Semantic-head w/ derivation (all) 24.55† 18.46†‡ 24.45 17.76 Semantic-head w/ derivation (core) Table 3: BLEU scores for English-to-Japanese and Japanese-to-English translation experiments. † and ‡ marks indicate significant difference by bootstrap resampling (Koehn, 2004) from the decoder using GIZA++ & grow-diag-final-and alignment and baseline alignment respectively (p &lt; 0.05). model using all the alignments including derivations achieved the best translation quality. We believe this improvement is due to the reduction in function word alignment errors. The BLEU score decreased when only core alignments were used. This is because the exclusion of the derivations increased the ambiguity of the translation rules. Conclusion and future work In this paper, we proposed a novel approach for handling unique function words based on semantic-head dependency trees. Th"
C12-1120,P07-2045,0,0.010475,"Missing"
C12-1120,N03-1017,0,0.0194021,"k and Johnson, 2005), and then they were transformed into dependency structures by rules defining head words for phrases (Collins, 1999). Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994) and the dependency analyzer KNP (Kawahara and Kurohashi, 2006b). For comparison, we used GIZA++ (Och and Ney, 2003), which implements the well-known word-based statistical alignment model of the IBM Models. We conducted word alignment bidirectionally with the default parameters and merged them using the grow-diag-final-and heuristics (Koehn et al., 2003). We also tested the BerkeleyAligner3 (DeNero and Klein, 2007) in the unsupervised training mode with default settings. 6.2 Experimental result and discussion The experimental results are given in Table 2. “Syntactic-head” is the alignment accuracy of the baseline system by Nakazawa and Kurohashi (2011), while “Semantic-head w/o derivation” is the result of using the baseline model on semantic-head dependency trees. The results of incorporating the monolingual derivation are given in the bottom two rows, where “all” means that we evaluated all the alignments including derivations, while “core”"
C12-1120,W02-1018,0,0.0479938,"Missing"
C12-1120,I11-1089,1,0.605419,"he tree structure. Another advantage of the monolingual derivation model is that it can reduce the gaps between alignments, and preserve the dependency relations between treelets over languages. For example, the model can derive “に より (by)” from “変化 し (change)” or “ 治療 (medical treatment)” and let the dependency relation between “変化 し (change)” and “治 療 (medical treatment)” be direct parent-child like their English counterparts. This is effective for estimating the dependency relation probability described in Section 4.3. 4 Model overview The proposed model is an extension of that proposed by Nakazawa and Kurohashi (2011). This earlier model was overcoming the long-distance reordering issue by incorporating dependency trees. However, it was suffering from alignment errors for function words, which our new model solves by incorporating a monolingual derivation model. First we describe the generative story for the joint alignment model in the same manner as in previous work (Marcu and Wong, 2002; DeNero et al., 2008; Nakazawa and Kurohashi, 2011). 1. Generate ℓ concepts from which bilingual treelet pairs are generated independently. 2. For each treelet pair, derive zero or more treelets monolingually from each t"
C12-1120,J03-1002,0,0.0317637,"ffectiveness of the proposed model. 6.1 Settings For the experiments, we used the JST1 paper abstract corpus. This corpus was created by NICT2 from JST’s 2M English-Japanese paper abstract corpus using the method of Utiyama and Isahara (2007). This corpus consists of 996K parallel sentences: 24.7M words in English and 27.5M words in Japanese. Unfortunately, this corpus is not publicly available now, but they will become available in the near future. As gold-standard data, 500 sentence pairs were annotated by hand using two types of annotations: sure (S) alignments and possible (P) alignments (Och and Ney, 2003). The unit of evaluation was the word. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in the experiments are as follows: p$ = 0.1, pd = 0.9, pN = 0.1, p t = 0.8, αA = 100, αN = 100, α f e = 100, αe f = 100, p f e = 0.5, pe f = 0.5. They are borrowed from the previous work (DeNero et al., 2008; Nakazawa and Kurohashi, 2011) and changed a little. The training time was about 1 day using 200 CPU cores. It is much slower than the word-sequence-based models because"
C12-1120,P02-1040,0,0.083906,"ent model that exchanges useful information between the parser and aligner. 7 Translation experiments We conducted English-to-Japanese and Japanese-to-English translation experiments on the same corpus used in the alignment experiments. We translated 500 paper abstract sentences from the JST corpus. Note that these sentences were not included in the training corpus. We use Joshua4 , a Java-based opensource implementation of the hierarchical decoder, version 4.0 (Ganitkevitch et al., 2012) with default settings. It was tuned using another 500 development sentence pairs. Table 3 shows the BLEU (Papineni et al., 2002) scores for the translations. The proposed 4 http://joshua-decoder.org 1975 ! ! ! ! &quot;&quot;&quot;! Figure 6: Alignment errors of the proposed model caused by a NULL part (left) and a parsing error (right). Alignment model En → Ja Ja → En GIZA++ & grow-diag-final-and 23.84 17.75 Syntactic-head (baseline) 24.16 17.83 Semantic-head w/o derivation 24.11 18.06 Semantic-head w/ derivation (all) 24.55† 18.46†‡ 24.45 17.76 Semantic-head w/ derivation (core) Table 3: BLEU scores for English-to-Japanese and Japanese-to-English translation experiments. † and ‡ marks indicate significant difference by bootstrap res"
C12-1120,2007.mtsummit-papers.63,0,0.0353745,"ed treelet. If an unaligned treelet is next to an aligned one, EXPAND merges the unaligned and aligned treelets, either as a part of core treelet or derivation treelet. As the opposite direction, it excludes a marginal node from a treelet, and to make the excluded node unaligned. 6 Alignment experiments We conducted alignment experiments on the English-Japanese corpus to show the effectiveness of the proposed model. 6.1 Settings For the experiments, we used the JST1 paper abstract corpus. This corpus was created by NICT2 from JST’s 2M English-Japanese paper abstract corpus using the method of Utiyama and Isahara (2007). This corpus consists of 996K parallel sentences: 24.7M words in English and 27.5M words in Japanese. Unfortunately, this corpus is not publicly available now, but they will become available in the near future. As gold-standard data, 500 sentence pairs were annotated by hand using two types of annotations: sure (S) alignments and possible (P) alignments (Och and Ney, 2003). The unit of evaluation was the word. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in"
C12-1120,P11-1003,0,0.0234908,"Missing"
C12-1120,N09-1028,0,0.0297784,"Missing"
C12-1120,J93-2003,0,\N,Missing
C12-1120,J03-4003,0,\N,Missing
C14-1027,bethard-etal-2008-building,0,0.0271323,"the average number of clauses in a document is 3.9. The total number of clause pairs is 59,426. 3.1.2 Discourse Relation Tagset One of our supposed applications of discourse parsing is to automatically generate a bird’s eye view of a controversial topic as in Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010), which identify various relations between statements, including contradictory relations. We assume that expansion relations, such as elaboration and restatement, and temporal relations are not important for this purpose. This setting is similar to the work of Bethard et al. (2008), which annotated temporal relations independently of causal relations. We also suppose that temporal relations can be annotated separately for NLP applications that require temporal information. We determined the tagset of discourse relations 271 Upper type CONTINGENCY COMPARISON OTHER Lower type Example Cause/Reason 【ボタンを押したので】【お湯が出た。】 [since (I) pushed the button] [hot water was turned on] Purpose 【試験に受かるために】【必死に勉強した。】 [to pass the exam] [(I) studied a lot] Condition 【ボタンを押せば】【お湯が出る。】 [if (you) push the button] [hot water will be turned on] Ground 【ここにカバンがあるから】【まだ社内にいるだろう。】 [here is his/her"
C14-1027,P13-2013,0,0.0147805,"1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We develop a tagged corpus in which pairs of discourse units are ann"
C14-1027,W01-1605,0,0.230039,"the precise relations between these text fragments. This kind of analysis is called discourse parsing or discourse structure analysis, and is an important and fundamental task in natural language processing (NLP). Systems for discourse parsing are, however, available only for major languages, such as English, owing to the lack of corpora with discourse annotations. For English, several corpora with discourse annotations have been developed manually, consuming a great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotati"
C14-1027,W11-0401,0,0.0183636,"otation for English, such as the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). These corpora were developed from English newspaper articles. Several attempts have been made to manually create corpora with discourse annotations for languages other than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (newspaper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse p"
C14-1027,P12-1007,0,0.0123901,"l., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We de"
C14-1027,P13-2130,0,0.0809301,"These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of spans with a certain relation and identifying the relation between the pair. In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the procedure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a discourse unit for the span is a costly process, and thus"
C14-1027,Y12-1058,1,0.853354,"ing discourse corpora, the target documents were mainly newspaper texts, such as the Wall Street Journal for English. However, discourse parsers trained on such newspaper corpora usually have a problem of domain adaptation. That is to say, while discourse parsers trained on newspaper corpora are good at analyzing newspaper texts, they generally cannot perform well on texts of other domains. To address this problem, we set out to create an annotated corpus covering a variety of domains. Since the web contains many documents across a variety of domains, we use the Diverse Document Leads Corpus (Hangyo et al., 2012), which was extracted from the web. Each document in this corpus consists of the first three sentences of a Japanese web page, making these short documents suitable for our discourse annotation method based on crowdsourcing. We adopt the clause as a discourse unit, since spans are too fine-grained to annotate using crowdsourcing and sentences are too coarse-grained to capture discourse relations. Clauses, which are automatically identified, do not need to be manually modified since they are thought to be reliable enough. Clause identification is performed using the rules of Shibata and Kurohas"
C14-1027,W11-0404,0,0.0723435,"d cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of spans with a certain relation and identifying the relation between the pair. In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the procedure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a discourse unit for the span is a c"
C14-1027,D12-1083,0,0.0149086,"se (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We develop a tagged corp"
C14-1027,P13-1048,0,0.0168993,"rs; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We develop a tagged corpus in which pairs o"
C14-1027,W14-0705,0,0.097662,"Missing"
C14-1027,P13-1047,0,0.0145803,"RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if possible. 3 Development of Corpus with Discourse Annotations using Crowdsourcing 3.1 Corpus Specifications We develop a tagged corpus in which pairs of discourse units are annotated with discour"
C14-1027,D11-1062,0,0.101904,"reat deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of spans with a certain relation and identifying the relation between the pair. In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the procedure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a discourse un"
C14-1027,P09-2004,0,0.0313816,"e include the Potsdam Commentary Corpus (Stede, 2004) for German (newspaper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especially in a short period if p"
C14-1027,P09-1077,0,0.0140033,"er than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (newspaper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009; Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing can be attributed to the existence of corpora with discourse annotations. However, the target language is mostly English since English is the only language that has large-scale discourse corpora. To develop and improve discourse parsers for languages other than English, it is necessary to build large-scale annotated corpora, especia"
C14-1027,prasad-etal-2008-penn,0,0.712547,"to understand text, it is essential to capture the precise relations between these text fragments. This kind of analysis is called discourse parsing or discourse structure analysis, and is an important and fundamental task in natural language processing (NLP). Systems for discourse parsing are, however, available only for major languages, such as English, owing to the lack of corpora with discourse annotations. For English, several corpora with discourse annotations have been developed manually, consuming a great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource"
C14-1027,I05-1066,1,0.82951,"Missing"
C14-1027,D08-1027,0,0.22439,"Missing"
C14-1027,W04-0213,0,0.0438613,"urse annotations, which are our target. To the best of our knowledge, there have been no attempts to crowdsource discourse annotations. There are several manually-crafted corpora with discourse annotation for English, such as the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). These corpora were developed from English newspaper articles. Several attempts have been made to manually create corpora with discourse annotations for languages other than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (newspaper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents; 1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres; 267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences compared with the English corpora containing several tens of thousands sentences. 270 In recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009;"
C14-1027,N09-1064,0,0.0419951,"Missing"
C14-1027,J05-2005,0,0.430652,"nts. This kind of analysis is called discourse parsing or discourse structure analysis, and is an important and fundamental task in natural language processing (NLP). Systems for discourse parsing are, however, available only for major languages, such as English, owing to the lack of corpora with discourse annotations. For English, several corpora with discourse annotations have been developed manually, consuming a great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson, 2005). Discourse parsers trained on these corpora have also been developed and practically used. To create the same resource-rich environment for another language, a quicker method than the conventional time-consuming framework should be sought. One possible approach is to use crowdsourcing, which has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008; Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource the difficult judgments for discourse annotations, which typically consists of two steps: findi"
C14-1027,C10-1140,0,0.067365,"Missing"
C14-1027,I11-1038,0,0.0554212,"Missing"
C16-1029,W09-2307,0,0.0222709,"rvation, these tokens have the ability to determine the syntactic role of the entire compound. For example, any compound that end with a nominal suffix “度” (degree) always act as nouns in a sentence. It should be noted that because of this characteristic of suffixes, we can tag the children of suffixes in compounds based on their meaning but not their syntactic roles. We show some examples in Table 2 to illustrate our POS tagging strategy for compounds. In Table 4 we present a dependency label set developed based on the Stanford Dependencies (De Marneffe et al., 2006) and its Chinese version (Chang et al., 2009), which defines 45 dependency relations for Chinese sentences. This label set is also closely related to the Universal Dependency1 with many of their labels compatible with each other. We explain the major characteristics of our label set in the following subsection. 3.1 Chinese Specific Labels dislocated The label “dislocated” is originally defined in the universal dependencies for languages such as Japanese to describe the syntactic relation of words in a topic–comment structure, but is not defined for Chinese. However, in Chinese it is frequent to see the topic–comment structure in a senten"
C16-1029,W02-1001,0,0.112792,"lower percentage of unknown words and unknown word-POS pairs found in the corresponding test set. This is consistent with our observation that compounds with internal structures are one of the major sources of OOV words. 4.2 Morphological Analysis Experiments We compared the performance of a state-of-the-art joint word segmentation and part-of-speech tagging system (Kruengkrai et al., 2009) on the original and our re-annotated CTB5. We used the position-ofcharacter (POC) tagset and the baseline feature set described in (Shen et al., 2014). We trained all models using the averaged perceptron (Collins, 2002), which is an efficient and stable online learning algorithm. The models applied on all test sets are those that result in the best performance on the dev sets. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 2 times. We use precision, recall and the F-score to measure the performance of the systems. Precision (P) is defined as the percentage of output tokens that are consistent with the gold standard test data, and recall (R) is the percentage of tokens in the gold standard test data that are recognize"
C16-1029,N06-1023,1,0.566142,"phrase based statistical machine translation toolkit Moses (Koehn et al., 2007) with default options. We trained the 5-gram language models on the target side of the parallel corpora using the SRILM toolkit4 with interpolated Kneser-Ney discounting. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses model"
C16-1029,W03-1722,0,0.036952,"is paper, we propose a new annotation approach to Chinese word segmentation, part-ofspeech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments. 1 Introduction The definition of “word” is an open problem in Chinese linguistics. In previous studies of Chinese corpus annotation (Duan et al., 2003; Huang et al., 1997; Xia, 2000), the judgement of word-hood of a meaningful string is based on the analysis of morphology: A morpheme in Chinese is defined as the smallest combination of meaning and phonetic sound in Chinese language, which can be classified into two major types: 1). Free morphemes, which can either be words by themselves or form words with other morphemes; and 2). Bound morphemes, which can only form words by attaching to other morphemes. An issue with word definition using morpheme classification is that, it potentially undermines the consistency of the representation of wo"
C16-1029,O97-4003,0,0.219847,"e a new annotation approach to Chinese word segmentation, part-ofspeech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments. 1 Introduction The definition of “word” is an open problem in Chinese linguistics. In previous studies of Chinese corpus annotation (Duan et al., 2003; Huang et al., 1997; Xia, 2000), the judgement of word-hood of a meaningful string is based on the analysis of morphology: A morpheme in Chinese is defined as the smallest combination of meaning and phonetic sound in Chinese language, which can be classified into two major types: 1). Free morphemes, which can either be words by themselves or form words with other morphemes; and 2). Bound morphemes, which can only form words by attaching to other morphemes. An issue with word definition using morpheme classification is that, it potentially undermines the consistency of the representation of words. For example, “论"
C16-1029,P08-1102,0,0.0702105,"Missing"
C16-1029,C08-1049,0,0.0408866,"Missing"
C16-1029,W04-3250,0,0.12997,"tures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses models: In “Character” we used a simple segmentation strategy for the Chinese sentences where we treated each character as a token; in “Original” and “Re-annotated” we segmented the Chinese sentences using the corresponding models described in 2 ? < 0.05 in McNemar’s test. http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 4 http://www.speech.sri.com/projects/srilm 5 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KyotoEBMT 3 306 the last subsection. The results show, with the underlying machine translation system being the same, the segmenter trained wit"
C16-1029,P09-1058,0,0.0604459,"Missing"
C16-1029,de-marneffe-etal-2006-generating,0,0.0183273,"Missing"
C16-1029,P03-1021,0,0.012317,"irs for tuning and testing, respectively. In the first set of experiments, we segmented the Japanese sentences using JUMAN (Kurohashi et al., 1994), and the Chinese sentences using the same morphological analyzer described in the last subsection. For decoding, we used the state-of-the-art phrase based statistical machine translation toolkit Moses (Koehn et al., 2007) with default options. We trained the 5-gram language models on the target side of the parallel corpora using the SRILM toolkit4 with interpolated Kneser-Ney discounting. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default opti"
C16-1029,W15-5006,1,0.786007,"mum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses models: In “Character” we used a simple segmentation strategy for the Chinese sentences where we treated each character as a token; in “Original” and “Re-annotated” we segmented the Chinese sentences using the corresponding models described in 2 ? < 0.05 in McNemar’s test. http"
C16-1029,Y12-1033,1,0.855948,"he parallel corpora using the SRILM toolkit4 with interpolated Kneser-Ney discounting. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. In the second set of experiments, we used the same morphological analyzers to segment and tag the POS of Japanese and Chinese sentences as in the first set. We further parsed the dependency structures of the Japanese sentences using KNP (Kawahara and Kurohashi, 2006), a lexicalized probabilistic dependency parser, and for the Chinese sentences we used a second-order graph-based parser proposed in (Shen et al., 2012). For decoding, we used the tree-to-tree example-based machine translation framework KyotoEBMT5 (Richardson et al., 2015) with default options. We report results on the test set using BLEU-4 score, which was evaluated using the multi-bleu.perl script in Moses based on Juman segmentations. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). In Table 7 we compare the performance of three Moses models: In “Character” we used a simple segmentation strategy for the Chinese sentences where we treated each character as a token; in “Original” and “Re-ann"
C16-1029,P14-2042,1,0.938798,"621 466 451 258 245 33 12 6 0 0 0 Proposed annotation 137,816 75,935 75,033 35,922 30,985 17,721 21,493 18,091 13,898 12,346 12,145 4,524 0 7,518 7,134 6,646 5,970 5,521 4,033 2,980 1,661 1,316 0 888 756 627 466 451 258 245 391 17 6 13,212 438 129 Table 3. Proposed tagset for part-of-speech tagging. The underlined characters in the examples correspond to the tags on the left-most column. The CTB POS are also shown. The key in our method to define the boundaries of common words is the character-level POS pattern. Character-level POS has been introduced in previous studies (Zhang et al., 2013; Shen et al., 2014) which captures the grammatical roles of Chinese characters inside words; we further develop this idea and use it as a criterion in word definition. We treat a meaningful disyllabic strings as a word if it falls into one of the character-level POS patterns listed in Table 1. The reason we focus on disyllabic patterns instead of other polysyllabic ones is that, based on our observation, meaningful strings with 3 or more syllables (other than names and idioms) are always compounds in Chinese, and therefore can be segmented into a sequence of monosyllabic and disyllabic tokens based on their inte"
C16-1029,P11-1139,0,0.0380763,"Missing"
C16-1029,P13-1013,0,0.0165029,"1,316 1,287 888 751 621 466 451 258 245 33 12 6 0 0 0 Proposed annotation 137,816 75,935 75,033 35,922 30,985 17,721 21,493 18,091 13,898 12,346 12,145 4,524 0 7,518 7,134 6,646 5,970 5,521 4,033 2,980 1,661 1,316 0 888 756 627 466 451 258 245 391 17 6 13,212 438 129 Table 3. Proposed tagset for part-of-speech tagging. The underlined characters in the examples correspond to the tags on the left-most column. The CTB POS are also shown. The key in our method to define the boundaries of common words is the character-level POS pattern. Character-level POS has been introduced in previous studies (Zhang et al., 2013; Shen et al., 2014) which captures the grammatical roles of Chinese characters inside words; we further develop this idea and use it as a criterion in word definition. We treat a meaningful disyllabic strings as a word if it falls into one of the character-level POS patterns listed in Table 1. The reason we focus on disyllabic patterns instead of other polysyllabic ones is that, based on our observation, meaningful strings with 3 or more syllables (other than names and idioms) are always compounds in Chinese, and therefore can be segmented into a sequence of monosyllabic and disyllabic tokens"
C16-1029,D10-1082,0,0.0356933,"Missing"
C18-1049,P16-1163,0,0.374614,"ourse connectives such as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 2015; Schenk et al., 2016), convolutional neural networks (Mihaylov and Frank, 2016; Wang and Lan, 2016) and bidirectional LSTM (biLSTM) (Chen et al., 2016; Liu and Li, 2016; Dai and Huang, 2018). Although these studies on network engineering report performance improvement, Rutherford et al. (2017) demonstrated that a simple feedforward neural network was astonishingly competitive, outperforming LSTM- and Tree LSTM-based models. He claimed that training data for implicit discourse relation classification were too small to train powerful neural networks like LSTM. This motivates us to view this task from a different perspective. We argue that the neural network models need to be provided with world knowledge, which can hardly be learned from a sm"
C18-1049,N18-1013,0,0.174837,"nd “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 2015; Schenk et al., 2016), convolutional neural networks (Mihaylov and Frank, 2016; Wang and Lan, 2016) and bidirectional LSTM (biLSTM) (Chen et al., 2016; Liu and Li, 2016; Dai and Huang, 2018). Although these studies on network engineering report performance improvement, Rutherford et al. (2017) demonstrated that a simple feedforward neural network was astonishingly competitive, outperforming LSTM- and Tree LSTM-based models. He claimed that training data for implicit discourse relation classification were too small to train powerful neural networks like LSTM. This motivates us to view this task from a different perspective. We argue that the neural network models need to be provided with world knowledge, which can hardly be learned from a small amount of manually annotated data. T"
C18-1049,Q15-1024,0,0.390421,"t Journal sections of the Penn Treebank. Each discourse relation consists of two text spans (arguments) and a relation label. Arguments are annotated such that they are minimally required to infer the discourse relation. Relation labels are organized as a 3-level hierarchy in the PDTB. However, it is too difficult for current systems to perform classification in its original form, and previous studies have used more coarse-grained relation labels. Popular settings include top-level one-versus-all binary classification (Pitler et al., 2009), top-level 4-way classification (Pitler et al., 2009; Ji and Eisenstein, 2015), second-level 11-way classification (Lin et al., 2009; Rutherford et al., 2017), and modified second-level classification for the CoNLL 2015 Shared Task (Xue et al., 2015). We used second-level 11-way classification in the experiments. Following Shi and Demberg (2017), we conducted 10-fold cross validation using the whole corpus of sections 0–24 (referred to as Cross Validation). The standard approach (referred to as Most-used Split) 2 R¨onnqvist et al. (2017) also concatenated Arg1 and Arg2 in the task of Chinese implicit discourse relation classification. They did not incorporate contextual"
C18-1049,C16-1245,0,0.0191917,"t al. (2017) Dev 0.4214 0.4214 0.4252 0.4233 0.4408 0.4330 0.4350 0.4350 - Test 0.3668 0.3655 0.3799 0.3877 0.3708 0.3642 0.3603 0.3655 0.3956 0.4020 0.4465 Table 6: F1 score in Paragraph+ConceptNet. Table 7: Accuracy in the Most-used Split dataset. shown in Table 6. Comparing Table 6 with Table 1, we can see that the model ended up ignoring verylow-frequency relations such as Comparison.Concession and Expansion.Alternative, which appeared less than 300 times in training set. This problem could possibly be mitigated by leveraging an unlabeled corpus to increase the size of training instances (Jiang et al., 2016). The results for the Most-used Split dataset are shown in Table 7. In this dataset, the proposed method was outperformed by models in the literature. Although the F-measure of Args+Coref+ConceptNet was about 2 points higher than that Args, the performance varied too inconsistently to draw meaningful conclusions. For this reason, we support Shi and Demberg (2017)’s argument for the need of cross validation for implicit discourse relation classification. 4.3 Discussion As we have seen in Table 5, ConceptNet brought performance gain. However, it appears to leave much room for improvement. Consid"
C18-1049,D09-1036,0,0.512542,"U. We compared the performance of MAGE-GRU with and without the text chunk that preceded a given argument pair in the paragraph. It turned out that contextual information provided no significant improvement for implicit discourse relations. However, we also found that contextual information yielded a significant gain for explicit discourse relations. The results appear to strengthen the observation that explicit and implicit discourse relations are dissimilar (Prasad et al., 2014). 2 Related Work Before neural networks were introduced to the task of implicit discourse relation classification, Lin et al. (2009) proposed a linear classifier that was based on various lexical and syntactic features and was combined with extensive feature selection. Rutherford et al. (2017) built a simple feedforward neural network model where only one pooling layer and one hidden layer were stacked on top of word embeddings. They reported that the simple model outperformed LSTM- and Tree LSTM-based models but lost to Lin et al. (2009). LSTM has demonstrated success in a wide range of NLP tasks including implicit discourse relation classification. Chen et al. (2016) proposed a combination of a bi-LSTM and a gated releva"
C18-1049,D16-1130,0,0.472673,"uch as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 2015; Schenk et al., 2016), convolutional neural networks (Mihaylov and Frank, 2016; Wang and Lan, 2016) and bidirectional LSTM (biLSTM) (Chen et al., 2016; Liu and Li, 2016; Dai and Huang, 2018). Although these studies on network engineering report performance improvement, Rutherford et al. (2017) demonstrated that a simple feedforward neural network was astonishingly competitive, outperforming LSTM- and Tree LSTM-based models. He claimed that training data for implicit discourse relation classification were too small to train powerful neural networks like LSTM. This motivates us to view this task from a different perspective. We argue that the neural network models need to be provided with world knowledge, which can hardly be learned from a small amount of manu"
C18-1049,W10-4327,0,0.0252973,"h can hardly be learned from a small amount of manually annotated data. In this paper, we address this problem by augmenting the input text with external knowledge and context and by adopting a neural network model that can effectively handle the augmented text. Experiments show that external knowledge did improve the classification accuracy. On the other hand, contextual information provided no significant gain for implicit discourse relations while it worked for explicit ones. 1 Introduction Discourse relation recognition has a wide variety of potential applications including summarization (Louis et al., 2010), sentiment analysis (Somasundaran et al., 2009) and machine translation (Meyer et al., 2015). In one of the two most prevalent discourse treebanks, the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations are overtly marked with discourse connectives such as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relatio"
C18-1049,C16-1308,0,0.0311555,"e it found the antonym pair “hit” (Arg1) and “flopped” (Arg2) in ConceptNet. However, our model misclassified Ex2 as Expansion.Conjunction even though “yesterday” and “today” were correctly identified as antonyms. This indicates that our model might not have given due weight to ConceptNet, possibly because of some noise in the knowledge base. In our experiments, coreference resolution did not help implicit discourse relation classification. What we relied on was standard pronominal and nominal coreference resolution, but the following example suggests the need for resolving event coreference (Lu et al., 2016): 591 Args+Coref+ConceptNet Paragraph+Coref+ConceptNet Test 0.7723 (±0.008) 0.7921 (±0.006) Table 8: Accuracy of explicit discourse relation classification. Result indicates the mean accuracy across folds and the standard deviation.   Is an American Secretary of State seriously suggesting that the Khmer Rouge should help govern Cambodia? Apparently so. There are no easy choices in Cambodia, but we can’t imagine that it benefits the U.S. to become the catalyst for an all-too-familiar process that could end in another round of horror in Cambodia. (Comparison.Contrast)   In this example, Arg1"
C18-1049,K16-2014,0,0.0121567,"o types: explicit and implicit. Explicit relations are overtly marked with discourse connectives such as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 2015; Schenk et al., 2016), convolutional neural networks (Mihaylov and Frank, 2016; Wang and Lan, 2016) and bidirectional LSTM (biLSTM) (Chen et al., 2016; Liu and Li, 2016; Dai and Huang, 2018). Although these studies on network engineering report performance improvement, Rutherford et al. (2017) demonstrated that a simple feedforward neural network was astonishingly competitive, outperforming LSTM- and Tree LSTM-based models. He claimed that training data for implicit discourse relation classification were too small to train powerful neural networks like LSTM. This motivates us to view this task from a different perspective. We argue that the neural network models need to"
C18-1049,N15-1100,0,0.0306023,"mmons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/. Licence details: http: 584 Proceedings of the 27th International Conference on Computational Linguistics, pages 584–595 Santa Fe, New Mexico, USA, August 20-26, 2018. neural network model trained on small training data to recognize the antonymy. Although many recent studies make use of word2vec word embeddings (Mikolov et al., 2013), which are typically trained with a large amount of unannotated data, it is well known that synonyms and antonyms are distributionally similar and thus are hardly distinguishable (Ono et al., 2015). For this reason, we need to look for different knowledge sources as well as an efficient way to integrate them into neural network models. In this paper, we use MAGE-GRU (Dhingra et al., 2017) to encode external knowledge. It is a straightforward extension to the Gated Recurrent Unit (GRU) (Cho et al., 2014). The input to MAGE-GRU is no longer a sequence of words but a directed acyclic graph in which the word sequence is augmented with edges between arbitrarily distant words for which external knowledge suggests some explicit signals such as antonymy. Thus “loss” and “profit” in the example"
C18-1049,C08-2022,0,0.10211,"uction Discourse relation recognition has a wide variety of potential applications including summarization (Louis et al., 2010), sentiment analysis (Somasundaran et al., 2009) and machine translation (Meyer et al., 2015). In one of the two most prevalent discourse treebanks, the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations are overtly marked with discourse connectives such as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 2015; Schenk et al., 2016), convolutional neural networks (Mihaylov and Frank, 2016; Wang and Lan, 2016) and bidirectional LSTM (biLSTM) (Chen et al., 2016; Liu and Li, 2016; Dai and Huang, 2018). Although these studies on network engineering report performance improvement, Rutherford et a"
C18-1049,P09-1077,0,0.269806,"relations in English. The annotation is done as another layer on Wall Street Journal sections of the Penn Treebank. Each discourse relation consists of two text spans (arguments) and a relation label. Arguments are annotated such that they are minimally required to infer the discourse relation. Relation labels are organized as a 3-level hierarchy in the PDTB. However, it is too difficult for current systems to perform classification in its original form, and previous studies have used more coarse-grained relation labels. Popular settings include top-level one-versus-all binary classification (Pitler et al., 2009), top-level 4-way classification (Pitler et al., 2009; Ji and Eisenstein, 2015), second-level 11-way classification (Lin et al., 2009; Rutherford et al., 2017), and modified second-level classification for the CoNLL 2015 Shared Task (Xue et al., 2015). We used second-level 11-way classification in the experiments. Following Shi and Demberg (2017), we conducted 10-fold cross validation using the whole corpus of sections 0–24 (referred to as Cross Validation). The standard approach (referred to as Most-used Split) 2 R¨onnqvist et al. (2017) also concatenated Arg1 and Arg2 in the task of Chinese"
C18-1049,prasad-etal-2008-penn,0,0.628586,"work model that can effectively handle the augmented text. Experiments show that external knowledge did improve the classification accuracy. On the other hand, contextual information provided no significant gain for implicit discourse relations while it worked for explicit ones. 1 Introduction Discourse relation recognition has a wide variety of potential applications including summarization (Louis et al., 2010), sentiment analysis (Somasundaran et al., 2009) and machine translation (Meyer et al., 2015). In one of the two most prevalent discourse treebanks, the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations are overtly marked with discourse connectives such as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 201"
C18-1049,J14-4007,0,0.0127705,"is to use the pair of Arg1 and Arg2 out of context, the computer might also benefit from the wider context, given the power of MAGE-GRU. We compared the performance of MAGE-GRU with and without the text chunk that preceded a given argument pair in the paragraph. It turned out that contextual information provided no significant improvement for implicit discourse relations. However, we also found that contextual information yielded a significant gain for explicit discourse relations. The results appear to strengthen the observation that explicit and implicit discourse relations are dissimilar (Prasad et al., 2014). 2 Related Work Before neural networks were introduced to the task of implicit discourse relation classification, Lin et al. (2009) proposed a linear classifier that was based on various lexical and syntactic features and was combined with extensive feature selection. Rutherford et al. (2017) built a simple feedforward neural network model where only one pooling layer and one hidden layer were stacked on top of word embeddings. They reported that the simple model outperformed LSTM- and Tree LSTM-based models but lost to Lin et al. (2009). LSTM has demonstrated success in a wide range of NLP t"
C18-1049,C16-1180,0,0.533116,"Missing"
C18-1049,P17-1093,0,0.620484,"fired by external knowledge. They found that MAGE-GRU consistently outperformed the baseline in various tasks when coreference resolution is used as external knowledge. A huge performance gap between explicit and implicit relations leads some to transform explicit relations in unannotated corpora to generate pseudo-training data of implicit relations. Sporleder and Lascarides (2008) reported negative results, suggesting that explicit and implicit discourse relations were linguistically dissimilar. Rutherford and Xue (2015) worked on selecting discourse connectives that can be safely dropped. Qin et al. (2017) generated a different kind of pseudo-training data. They inserted to implicit relations implicit connectives PDTB annotators assigned to them. They used domain adversarial training to transfer knowledge from the recognition model supplied with implicit connectives to the model without connectives. These methods can be combined with our approach. 3 Proposed Method The overall system architecture is shown in Figure 1. It is based on the RNN architecture of Rutherford et al. (2017) although we made two major modifications to it, which will be described in Sections 3.1 and 3.3. For each of Arg1 a"
C18-1049,P17-2040,0,0.0243346,"Missing"
C18-1049,E14-1068,0,0.185046,"se parser. In their pipeline system, a CNN is used for nonexplicit relation classification.1 It ranked second on the blind datasets in the CoNLL 2016 Shared Task. Qin et al. (2016) proposed a combination of a bi-LSTM and CNNs. They constructed character-based word representations by transforming character embeddings with CNN and bi-LSTM layers. Another CNN layer was used to extract an argument representation from a sequence of words. Some recent studies exploited external knowledge sources and some of them were reported to improve the performance of implicit discourse relation classification (Rutherford and Xue, 2014). In the closed track of the CoNLL 2016 Shared Task (Xue et al., 2016), the organizers allowed participants to use a limited set of linguistic resources: Brown Clusters, VerbNet, a sentiment lexicon and an off-the-shelf word2vec model. In addition to these, Inquirer Tags, Levin classes and Modality were tested by Shi and Demberg (2017). They extracted features from these resources and added them to a neural network layer just before the output. Dhingra et al. (2017), who proposed MAGE-GRU, tested a more direct approach as a baseline, in which they appended to word embeddings a sequence of feat"
C18-1049,N15-1081,0,0.0633424,"proach as a baseline, in which they appended to word embeddings a sequence of features which are fired by external knowledge. They found that MAGE-GRU consistently outperformed the baseline in various tasks when coreference resolution is used as external knowledge. A huge performance gap between explicit and implicit relations leads some to transform explicit relations in unannotated corpora to generate pseudo-training data of implicit relations. Sporleder and Lascarides (2008) reported negative results, suggesting that explicit and implicit discourse relations were linguistically dissimilar. Rutherford and Xue (2015) worked on selecting discourse connectives that can be safely dropped. Qin et al. (2017) generated a different kind of pseudo-training data. They inserted to implicit relations implicit connectives PDTB annotators assigned to them. They used domain adversarial training to transfer knowledge from the recognition model supplied with implicit connectives to the model without connectives. These methods can be combined with our approach. 3 Proposed Method The overall system architecture is shown in Figure 1. It is based on the RNN architecture of Rutherford et al. (2017) although we made two major"
C18-1049,E17-1027,0,0.0650398,"er et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 2015; Schenk et al., 2016), convolutional neural networks (Mihaylov and Frank, 2016; Wang and Lan, 2016) and bidirectional LSTM (biLSTM) (Chen et al., 2016; Liu and Li, 2016; Dai and Huang, 2018). Although these studies on network engineering report performance improvement, Rutherford et al. (2017) demonstrated that a simple feedforward neural network was astonishingly competitive, outperforming LSTM- and Tree LSTM-based models. He claimed that training data for implicit discourse relation classification were too small to train powerful neural networks like LSTM. This motivates us to view this task from a different perspective. We argue that the neural network models need to be provided with world knowledge, which can hardly be learned from a small amount of manually annotated data. To see this, suppose that we want to classify the discourse relation of the following pair of text spans"
C18-1049,K16-2005,0,0.0361716,"Missing"
C18-1049,E17-2024,0,0.495105,"ayers. Another CNN layer was used to extract an argument representation from a sequence of words. Some recent studies exploited external knowledge sources and some of them were reported to improve the performance of implicit discourse relation classification (Rutherford and Xue, 2014). In the closed track of the CoNLL 2016 Shared Task (Xue et al., 2016), the organizers allowed participants to use a limited set of linguistic resources: Brown Clusters, VerbNet, a sentiment lexicon and an off-the-shelf word2vec model. In addition to these, Inquirer Tags, Levin classes and Modality were tested by Shi and Demberg (2017). They extracted features from these resources and added them to a neural network layer just before the output. Dhingra et al. (2017), who proposed MAGE-GRU, tested a more direct approach as a baseline, in which they appended to word embeddings a sequence of features which are fired by external knowledge. They found that MAGE-GRU consistently outperformed the baseline in various tasks when coreference resolution is used as external knowledge. A huge performance gap between explicit and implicit relations leads some to transform explicit relations in unannotated corpora to generate pseudo-train"
C18-1049,D09-1018,0,0.0373219,"nt of manually annotated data. In this paper, we address this problem by augmenting the input text with external knowledge and context and by adopting a neural network model that can effectively handle the augmented text. Experiments show that external knowledge did improve the classification accuracy. On the other hand, contextual information provided no significant gain for implicit discourse relations while it worked for explicit ones. 1 Introduction Discourse relation recognition has a wide variety of potential applications including summarization (Louis et al., 2010), sentiment analysis (Somasundaran et al., 2009) and machine translation (Meyer et al., 2015). In one of the two most prevalent discourse treebanks, the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations are overtly marked with discourse connectives such as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies"
C18-1049,speer-havasi-2012-representing,0,0.142857,"U (Dhingra et al., 2017) to encode external knowledge. It is a straightforward extension to the Gated Recurrent Unit (GRU) (Cho et al., 2014). The input to MAGE-GRU is no longer a sequence of words but a directed acyclic graph in which the word sequence is augmented with edges between arbitrarily distant words for which external knowledge suggests some explicit signals such as antonymy. Thus “loss” and “profit” in the example above are directly connected, making a downstream network layer more easily classify the discourse relation. In the experiments, we augmented the inputs with ConceptNet (Speer and Havasi, 2012) and coreference resolution. We found that MAGE-GRU significantly outperformed others when ConceptNet was used. While recurrent neural networks have considerable difficulty in capturing long-range dependencies, MAGE-GRU is expected to mitigate this problem because it creates shortcuts within word sequences. This leads us to explore another question: Do Arg1 and Arg2 provide sufficient information to determine discourse relations? Let us consider the following example:   Good service programs require recruitment, screening, training and supervision – all of high quality. They involve stipends"
C18-1049,K16-2004,0,0.0123713,"icit. Explicit relations are overtly marked with discourse connectives such as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 2015; Schenk et al., 2016), convolutional neural networks (Mihaylov and Frank, 2016; Wang and Lan, 2016) and bidirectional LSTM (biLSTM) (Chen et al., 2016; Liu and Li, 2016; Dai and Huang, 2018). Although these studies on network engineering report performance improvement, Rutherford et al. (2017) demonstrated that a simple feedforward neural network was astonishingly competitive, outperforming LSTM- and Tree LSTM-based models. He claimed that training data for implicit discourse relation classification were too small to train powerful neural networks like LSTM. This motivates us to view this task from a different perspective. We argue that the neural network models need to be provided with wor"
C18-1049,P17-1088,0,0.0196633,"tion types (e.g. AtLocation and Causes). As a result, the number of ConceptNet relation types used in the experiments was increased to 65. Although ConceptNet was a relatively high-quality and high-coverage knowledge base, it nevertheless (1) contained questionable triplets (e.g., (time, Antonym, year)) and (2) failed to cover some important relations (stock, AtLocation, market). We mitigated the first problem by checking weights ConceptNet assigned to triplets. We removed triplets whose weight was smaller than 1.0. The second problem might potentially be addressed graph embedding techniques (Xie et al., 2017), but in the experiments, we used a simpler method. For each word in the input, we prepared the top-10 nearest neighbors in terms of cosine similarity of word2vec vectors with the threshold value of 0.6. We searched ConceptNet for all combinations of the original words and neighbors. As a result, the average number of edges given to an argument pair increased from 4.0 to 17.6. As for a coreference resolution system, we used Stanford CoreNLP3 (ver.3.7.0). CoreNLP had three different coreference systems. We chose a neural model since it performed the best among the three. We tested two input for"
C18-1049,K15-2001,0,0.294472,"Missing"
C18-1049,K16-2001,0,0.433677,"tion recognition has a wide variety of potential applications including summarization (Louis et al., 2010), sentiment analysis (Somasundaran et al., 2009) and machine translation (Meyer et al., 2015). In one of the two most prevalent discourse treebanks, the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations are overtly marked with discourse connectives such as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 2015; Schenk et al., 2016), convolutional neural networks (Mihaylov and Frank, 2016; Wang and Lan, 2016) and bidirectional LSTM (biLSTM) (Chen et al., 2016; Liu and Li, 2016; Dai and Huang, 2018). Although these studies on network engineering report performance improvement, Rutherford et al. (2017) demonstra"
C18-1049,D15-1266,0,0.0567267,"sad et al., 2008), discourse relations are conventionally divided into two types: explicit and implicit. Explicit relations are overtly marked with discourse connectives such as “because” and “however.” Because of these strong cues, explicit relations are relatively easy to classify (Pitler et al., 2008; Xue et al., 2016). By contrast, implicit relations lack discourse connectives and classifying such relations remains a challenging problem. Recent studies on implicit discourse relation classification have shown success in applying various neural network models including feedforward networks (Zhang et al., 2015; Schenk et al., 2016), convolutional neural networks (Mihaylov and Frank, 2016; Wang and Lan, 2016) and bidirectional LSTM (biLSTM) (Chen et al., 2016; Liu and Li, 2016; Dai and Huang, 2018). Although these studies on network engineering report performance improvement, Rutherford et al. (2017) demonstrated that a simple feedforward neural network was astonishingly competitive, outperforming LSTM- and Tree LSTM-based models. He claimed that training data for implicit discourse relation classification were too small to train powerful neural networks like LSTM. This motivates us to view this tas"
C18-1128,P13-1133,0,0.0849537,"Missing"
C18-1128,S17-2002,0,0.0442787,"Missing"
C18-1128,C16-1276,0,0.10794,"e source language has a similar vector representation to its target-side counterpart, assuming each concept in the source language corresponds to exactly one concept in the target language. There is a rich body of work on sense embedding, which allows one surface form of a word to have sense-specific vectors (Neelakantan et al., 2014; Iacobacci et al., 2015). However, to the best of our knowledge, previous studies in this field do not target sense vectors of concepts for cross-lingual knowledge projection. Several studies proposed methods for one-to-one projection of facts (Kuo and Hsu, 2010; Feng et al., 2016). The work by Feng et al. (2016) is the most related to our study. Their model learns mappings between English and Chinese facts by manually annotated alignments. Their experimental result showed the model successfully resolved the projection ambiguity. Their experiment was, however, limited to a narrow domain due to the cost of manual annotations, indicating the difficulty of obtaining sufficient resources for learning a model. Various types of commonsense is vital to understanding languages in a wide range of tasks such as recognizing textual entailment (LoBue and Yates, 2011). Researchers h"
C18-1128,P15-1010,0,0.0242526,"released code. knowledge projection. Klein et al. (2017) and Chen et al. (2017) represented concepts in multiple languages in a unified vector space, and built knowledge base completion models based on vector representations. Their methods ensure a concept in the source language has a similar vector representation to its target-side counterpart, assuming each concept in the source language corresponds to exactly one concept in the target language. There is a rich body of work on sense embedding, which allows one surface form of a word to have sense-specific vectors (Neelakantan et al., 2014; Iacobacci et al., 2015). However, to the best of our knowledge, previous studies in this field do not target sense vectors of concepts for cross-lingual knowledge projection. Several studies proposed methods for one-to-one projection of facts (Kuo and Hsu, 2010; Feng et al., 2016). The work by Feng et al. (2016) is the most related to our study. Their model learns mappings between English and Chinese facts by manually annotated alignments. Their experimental result showed the model successfully resolved the projection ambiguity. Their experiment was, however, limited to a narrow domain due to the cost of manual anno"
C18-1128,E17-2083,0,0.0613434,"Missing"
C18-1128,P16-1137,0,0.0164472,"of a Japanese fact (koumori, CapableOf, tobu) given an English fact (bat, CapableOf, fly). We first obtain language expressions of the facts using hand-crafted templates and compute a translation probability with the translation model. 4.2 Knowledge Base Completion (KBC) KBC models evaluate the plausibility of a given fact based on existing information on the KB. For example, if we already know many animals with wings can fly and bats have wings, we can imagine that bats also can fly. We train a KBC model on the target-side KB. We use a bilinear model used in several previous studies (e.g., (Li et al., 2016)) as a component of our model, where concepts and relations are represented as vectors and matrices, respectively. This component can also be replaced with other KBC models.4 Given a fact f t = (e1 , r, e2 ), the bilinear model outputs the value of plausibility as follows. xKBC (f t ) = σ(uT1 Mr u2 ), (3) where σ is a sigmoid function, ui ∈ Rd (i = 1, 2) corresponds to vectors of concepts e1 and e2 , Mr ∈ Rd×d corresponds to a matrix of relation r, and d is a hyper parameter. We construct concept vectors by averaging pre-trained d0 -dimensional word embeddings as several previous studies did t"
C18-1128,P11-2057,0,0.0211961,"Kuo and Hsu, 2010; Feng et al., 2016). The work by Feng et al. (2016) is the most related to our study. Their model learns mappings between English and Chinese facts by manually annotated alignments. Their experimental result showed the model successfully resolved the projection ambiguity. Their experiment was, however, limited to a narrow domain due to the cost of manual annotations, indicating the difficulty of obtaining sufficient resources for learning a model. Various types of commonsense is vital to understanding languages in a wide range of tasks such as recognizing textual entailment (LoBue and Yates, 2011). Researchers have compiled resources to maintain such knowledge. Cyc (Lenat, 1995) is a seminal big project that aims to organize commonsense in logical forms. Logical forms are suitable for disambiguating the meaning of language, but we need high expertise to acquire or utilize them. In contrast, ConceptNet (Liu and Singh, 2004b; Speer et al., 2017) adopted natural language expressions such as words and phrases that may have ambiguities to represent knowledge, which made it possible to collect millions of commonsense facts in multiple languages via crowdsourcing. 3 Problem Setting Suppose we"
C18-1128,D15-1276,1,0.891939,"Missing"
C18-1128,D14-1113,0,0.0161577,"plates can be found in the released code. knowledge projection. Klein et al. (2017) and Chen et al. (2017) represented concepts in multiple languages in a unified vector space, and built knowledge base completion models based on vector representations. Their methods ensure a concept in the source language has a similar vector representation to its target-side counterpart, assuming each concept in the source language corresponds to exactly one concept in the target language. There is a rich body of work on sense embedding, which allows one surface form of a word to have sense-specific vectors (Neelakantan et al., 2014; Iacobacci et al., 2015). However, to the best of our knowledge, previous studies in this field do not target sense vectors of concepts for cross-lingual knowledge projection. Several studies proposed methods for one-to-one projection of facts (Kuo and Hsu, 2010; Feng et al., 2016). The work by Feng et al. (2016) is the most related to our study. Their model learns mappings between English and Chinese facts by manually annotated alignments. Their experimental result showed the model successfully resolved the projection ambiguity. Their experiment was, however, limited to a narrow domain due t"
C18-1128,P16-1009,0,0.0115522,"Methods We compare the performance of our proposed methods with the following baselines. • PPMI: Positive pointwise mutual information of two concepts consisting of a target-side fact f t . We count the co-occurrence of the concepts in the 200 million web sentences for Japanese, and in the Chinese Gigaword Fifth Edition for Chinese concepts. • MT: The neural MT model with an attention mechanism, which computes xMT in the proposed methods. We used an implementation by Neubig (2015) and train a model on 3.25M (en-ja) and 2.97M (en-zh) sentence pairs from dictionaries and newswire corpora. BPE (Sennrich et al., 2016) was used to reduce the vocabulary size. • KBC: The target-side bilinear KBC model which was used as the component to produce xKBC . The Japanese and Chinese models were trained on 59,274 and 318,361 facts, respectively. • MTransE: The multi-lingual translation-based KBC model (Chen et al., 2017) which learns TransE (Bordes et al., 2013) and concept-to-concept alignment jointly. Chen et al. (2017) proposed five different alignment models and reported the fourth variant performed best in their experiments. Thus we use the variant in our experiments. The proposed methods LIN and MLP use estimate"
C18-1128,speer-havasi-2012-representing,0,0.437381,"e gap across languages by cross-lingual 2 https://github.com/notani/CLKP-MTKBC 1509 Relation e1 e2 English Japanese Chinese AtLocation CapableOf MadeOf UsedFor NP NP NP NP NP VP NP VP You are likely to find e1 in e2 . e1 can e2 e1 is made of e2 . You can use e1 to e2 . e2 e1 e1 e1 Ni keyi zai e2 zhaodao e1 . e1 hui e1 . e1 keyi yong e2 zhi cheng. e2 de shihou keneng hui yong dao e1 . de e1 wo miru koto ga aru. wa e2 koto ga dekiru . wa e2 kara tsukurareru. wa e2 tame ni tsukawareru. Table 1: Examples of templates for converting facts into sentences. Constraints of part-of-speech on e1 and e2 (Speer and Havasi, 2012) are also presented. Some templates were developed by the ConceptNet organizers. The rest of the templates can be found in the released code. knowledge projection. Klein et al. (2017) and Chen et al. (2017) represented concepts in multiple languages in a unified vector space, and built knowledge base completion models based on vector representations. Their methods ensure a concept in the source language has a similar vector representation to its target-side counterpart, assuming each concept in the source language corresponds to exactly one concept in the target language. There is a rich body"
C18-1128,P03-1010,0,0.054756,"Missing"
C94-2183,J91-2003,0,\N,Missing
C94-2183,C92-1029,1,\N,Missing
C94-2183,P84-1085,0,\N,Missing
C94-2183,J86-3001,0,\N,Missing
C94-2183,P84-1055,0,\N,Missing
C94-2183,P84-1076,0,\N,Missing
chu-etal-2012-chinese,chou-etal-2008-extended,0,\N,Missing
chu-etal-2012-chinese,W08-1907,0,\N,Missing
chu-etal-2012-chinese,I05-1059,0,\N,Missing
chu-etal-2012-chinese,chou-huang-2006-hantology,0,\N,Missing
chu-etal-2014-constructing,C04-1151,0,\N,Missing
chu-etal-2014-constructing,J93-2003,0,\N,Missing
chu-etal-2014-constructing,J05-4003,0,\N,Missing
chu-etal-2014-constructing,P07-2045,0,\N,Missing
chu-etal-2014-constructing,W13-2505,1,\N,Missing
chu-etal-2014-constructing,P09-2057,0,\N,Missing
chu-etal-2014-constructing,P03-1010,0,\N,Missing
chu-etal-2014-constructing,2012.eamt-1.7,1,\N,Missing
D07-1032,P05-1022,0,0.0527499,"ltaneously evaluate not only syntactic relations but also indirect relations, such as ellipses and anaphora. This kind of mismatch also occurred for the detection of coordinate structures. Another errors were caused by an inherent characteristic of generative models. Generative models have some advantages, such as their application to language models. However, it is difﬁcult to incorporate various features that seem to be useful for addressing syntactic and coordinate ambiguity. We plan to apply discriminative reranking to the n-best parses produced by our generative model in the same way as (Charniak and Johnson, 2005). 7 Conclusion This paper has described an integrated probabilistic model for coordination disambiguation and syntactic/case structure analysis. This model takes advantage of lexical preference of a huge raw corpus and large-scale case frames and performs coordination disambiguation and syntactic/case analysis simultaneously. The experiments indicated the effectiveness of our model. Our future work involves incorporating ellipsis resolution to develop an integrated model for syntactic, case, and ellipsis analysis. Acknowledgment This research is partially supported by special coordination fund"
D07-1032,P06-1053,0,0.164169,"Missing"
D07-1032,P99-1081,0,0.433146,"ute of Information and Communications Technology, 3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan dk@nict.go.jp Abstract more than two conjuncts, commas, which have various usages, also function like coordinate conjunctions. Recognizing true coordinate conjunctions from such possible coordinate conjunctions is a task of coordination disambiguation (Kurohashi, 1995). The other is the task of identifying the range of coordinate phrases or clauses. Previous work on coordination disambiguation has focused on the task of addressing the scope ambiguity (e.g., (Agarwal and Boggess, 1992; Goldberg, 1999; Resnik, 1999; Chantree et al., 2005)). Kurohashi and Nagao proposed a similarity-based method to resolve both of the two tasks for Japanese (Kurohashi and Nagao, 1994). Their method, however, heuristically detects coordinate conjunctions by considering only similarities between possible conjuncts, and thus cannot disambiguate the following cases1 : This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis. Our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic sim"
D07-1032,kawahara-kurohashi-2006-case,1,0.856964,"divided by the square root of the number of bunsetsus covered by the path for normalization The score of each path is calculated using a dynamic programming method. We consider each path as a candidate of pre- and post-conjuncts. 5 Dependency structure T1 ,T2 Integrated Probabilistic Model for Syntactic, Coordinate and Case Structure Analysis bentou-wa tabete-te This section describes a method of integrating coordination disambiguation into a probabilistic parsing model. The integrated model is based on a fullylexicalized probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006b). 5.1 Outline of the Model T1 : D T2 : C 0 (lunchbox) tabete-te (eat) kaet-ta kaet-ta (go home) (1) T3 : D T4 : C 0 EOS (go home) (3) EOS P(kaet − ta, D |EOS) P(bentou − wa kaet − ta, D |EOS) P(bentou − wa tabe − te, D |kaet − ta) P(tabe − te, D |bentou − wa kaet − ta) (2) (4) P(kaet − ta, D |EOS) P(bentou − wa kaet − ta, D |EOS) P(bentou − wa tabe − te, C 0 |kaet − ta) P(tabe − te, C 0 |bentou − wa kaet − ta) Figure 3: Example of probability calculation. This model gives a probability to each possible dependency structure, T , and case structure, L, of the input sentence, S, and outputs the"
D07-1032,N06-1023,1,0.9302,"divided by the square root of the number of bunsetsus covered by the path for normalization The score of each path is calculated using a dynamic programming method. We consider each path as a candidate of pre- and post-conjuncts. 5 Dependency structure T1 ,T2 Integrated Probabilistic Model for Syntactic, Coordinate and Case Structure Analysis bentou-wa tabete-te This section describes a method of integrating coordination disambiguation into a probabilistic parsing model. The integrated model is based on a fullylexicalized probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006b). 5.1 Outline of the Model T1 : D T2 : C 0 (lunchbox) tabete-te (eat) kaet-ta kaet-ta (go home) (1) T3 : D T4 : C 0 EOS (go home) (3) EOS P(kaet − ta, D |EOS) P(bentou − wa kaet − ta, D |EOS) P(bentou − wa tabe − te, D |kaet − ta) P(tabe − te, D |bentou − wa kaet − ta) (2) (4) P(kaet − ta, D |EOS) P(bentou − wa kaet − ta, D |EOS) P(bentou − wa tabe − te, C 0 |kaet − ta) P(tabe − te, C 0 |bentou − wa kaet − ta) Figure 3: Example of probability calculation. This model gives a probability to each possible dependency structure, T , and case structure, L, of the input sentence, S, and outputs the"
D07-1032,W02-2016,0,0.201989,"Missing"
D07-1032,J94-4001,1,0.553519,"commas, which have various usages, also function like coordinate conjunctions. Recognizing true coordinate conjunctions from such possible coordinate conjunctions is a task of coordination disambiguation (Kurohashi, 1995). The other is the task of identifying the range of coordinate phrases or clauses. Previous work on coordination disambiguation has focused on the task of addressing the scope ambiguity (e.g., (Agarwal and Boggess, 1992; Goldberg, 1999; Resnik, 1999; Chantree et al., 2005)). Kurohashi and Nagao proposed a similarity-based method to resolve both of the two tasks for Japanese (Kurohashi and Nagao, 1994). Their method, however, heuristically detects coordinate conjunctions by considering only similarities between possible conjuncts, and thus cannot disambiguate the following cases1 : This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis. Our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic similarities and cooccurrence statistics. We integrate these probabilities into the framework of fully-lexicalized parsing based on largescale case frames. This approach si"
D07-1032,1995.iwpt-1.17,1,0.743726,"Missing"
D07-1032,C04-1002,0,0.200229,"Missing"
D07-1032,J03-4003,0,\N,Missing
D07-1032,P92-1003,0,\N,Missing
D08-1045,C00-1004,0,0.344961,"Missing"
D08-1045,C04-1066,0,0.43961,"es detected in the initial set are acquired and to terminate whenever sufﬁcient acquisition coverage is achieved. 5 Related Work Since most languages delimit words by white-space, morphological analysis in these languages is to segment words into morphemes. For example, Morpho Challenge 2007 (Kurimo et al., 2007) was evaluations of unsupervised segmentation for English, Finnish, German and Turkish. While Japanese is an agglutinative language, other non-segmented languages such as Chinese and Thai are analytic languages. Among them, Chinese has been a subject of intensive research. Peng et al. (2004) integrated new word detection into word segmentation. They detected new words by computing segment conﬁdence and re-analyzed the inputs with detected words as features. The Japanese language is unique in that it is written with several different character types. Heuristics widely used in unknown morpheme processing are based on character types. They were also used as important clues in statistical methods. Nagata (1999) integrated a probabilistic unknown word models into the word segmentation model. Uchimoto et al. (2001) incorporated them as feature functions of a Maximum Entropy-based morph"
D08-1045,kawahara-kurohashi-2006-case,1,0.902241,"Missing"
D08-1045,W04-3230,0,0.643914,"Missing"
D08-1045,C96-2202,0,0.369165,"Missing"
D08-1045,P99-1036,0,0.850927,"monotonic increase of the numbers of acquired morphemes and stored examples suggests that the vocabulary size did not converge. The number of occurrences of acquired morphemes in re-analysis was approximately the same with the number of examples kept in the storage during acquisition. This means that, in terms of frequency of 436 occurrence, about half of unknown morphemes were acquired. Most unknown morphemes belong to the “long tail” and the proposed method seems to have seized a “head” of the long tail. Although some previous studies emphasized correct identiﬁcation of low frequency terms (Nagata, 1999; Asahara and Matsumoto, 2004), it is no longer necessary because very large scale web texts are available today. If a small set of texts needs to be analyzed with high accuracy, we can incorporate similar texts retrieved from the web, to increase the number of examples of unknown morphemes. The proposed method can be modiﬁed to check if unknown morphemes detected in the initial set are acquired and to terminate whenever sufﬁcient acquisition coverage is achieved. 5 Related Work Since most languages delimit words by white-space, morphological analysis in these languages is to segment words int"
D08-1045,P06-1089,0,0.355031,"d models into the word segmentation model. Uchimoto et al. (2001) incorporated them as feature functions of a Maximum Entropy-based morphological analyzer. Asahara and Matsumoto (2004) used them as a feature of character-based chunking of unknown words using Support Vector Machines. Mori (1996) extracted words from texts and estimated their POSs using distributional analysis. The appropriateness of a word candidate was measured by the distance between probability distributions of the candidate and a model. In this method, morphological constraints were indirectly represented by distributions. Nakagawa and Matsumoto (2006) presented a method for guessing POS tags of pre-segmented unknown words that took into consideration all the occurrences of each unknown word in a document. This setting is impractical in Japanese because POS tagging is inseparable from segmentation. 6 Conclusion We propose a novel method that augments the lexicon of a Japanese morphological analyzer by acquiring unknown morphemes from texts in online mode. Unknown morphemes are acquired with high accuracy and improve the quality of morphological analysis. Unknown morphemes are one of the main sources of error in morphological analysis when w"
D08-1045,W02-1407,0,0.011072,"judge whether a compound term should be divided because there is no deﬁnite standard for morpheme boundaries in Japanese. For example, “ミンク鯨” (minku-kujira, minke whale) can be extracted as a single morpheme or decomposed into “ミンク” and “鯨.” While segmentation is an open question in Japanese morphological analysis, “correct” segmentation is not necessarily important for applications using morphological analysis. Even if a noun is split into two or more morphemes in morphological analysis, they are chunked to form a phrasal unit called bunsetsu in dependency parsing, and to extract a keyword (Nakagawa and Mori, 2002). To avoid the decompositionality problem, we adopted manual evaluation. We analyzed the target texts with both the initial lexicon and the augmented lexicon. Then we checked differences between the two analyses and extracted sentences that were affected by the augmentation. Among these sentences, we evaluated randomly selected 50 sentences per query. We checked the accuracy of segmentation and POS tagging of each “diff” block, which is illustrated in Figure 3. The segmentation of a block was judged correct unless morpheme boundaries were clearly wrong. In the evaluation of POS tagging, we did"
D08-1045,C04-1081,0,0.0605092,"nown morphemes detected in the initial set are acquired and to terminate whenever sufﬁcient acquisition coverage is achieved. 5 Related Work Since most languages delimit words by white-space, morphological analysis in these languages is to segment words into morphemes. For example, Morpho Challenge 2007 (Kurimo et al., 2007) was evaluations of unsupervised segmentation for English, Finnish, German and Turkish. While Japanese is an agglutinative language, other non-segmented languages such as Chinese and Thai are analytic languages. Among them, Chinese has been a subject of intensive research. Peng et al. (2004) integrated new word detection into word segmentation. They detected new words by computing segment conﬁdence and re-analyzed the inputs with detected words as features. The Japanese language is unique in that it is written with several different character types. Heuristics widely used in unknown morpheme processing are based on character types. They were also used as important clues in statistical methods. Nagata (1999) integrated a probabilistic unknown word models into the word segmentation model. Uchimoto et al. (2001) incorporated them as feature functions of a Maximum Entropy-based morph"
D08-1045,I08-1025,1,0.826055,"Missing"
D08-1045,W01-0512,0,0.656354,"ic languages. Among them, Chinese has been a subject of intensive research. Peng et al. (2004) integrated new word detection into word segmentation. They detected new words by computing segment conﬁdence and re-analyzed the inputs with detected words as features. The Japanese language is unique in that it is written with several different character types. Heuristics widely used in unknown morpheme processing are based on character types. They were also used as important clues in statistical methods. Nagata (1999) integrated a probabilistic unknown word models into the word segmentation model. Uchimoto et al. (2001) incorporated them as feature functions of a Maximum Entropy-based morphological analyzer. Asahara and Matsumoto (2004) used them as a feature of character-based chunking of unknown words using Support Vector Machines. Mori (1996) extracted words from texts and estimated their POSs using distributional analysis. The appropriateness of a word candidate was measured by the distance between probability distributions of the candidate and a model. In this method, morphological constraints were indirectly represented by distributions. Nakagawa and Matsumoto (2006) presented a method for guessing POS"
D09-1151,W03-2607,0,0.0484652,"loit “Nm no Nh ” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufﬁcient. Recently, a number of researchers argued for using the Web as a source of lexical knowledge, and the Web has been shown to be a useful resource for anaphora resolution (Bunescu, 2003; Markert et al., 2003; Poesio et al., 2004). Hence, in this study, we acquire the lexical knowledge for associative anaphora resolution from “Nm no Nh ” phrases in the Web by using the method described in (Sasano et al., 2004). We proposed a method for acquiring such lexical knowledge, called nominal case frames (NCFs), using an ordinary language dictionary and “Nm no Nh ” phrases, and constructed NCFs from newspaper corpora. In this study, we aim to acquire a sufﬁcient amount of lexical knowledge by constructing NCFs from the Web. As for associative anaphora resolution itself, we propose an"
D09-1151,T75-2034,0,0.726189,"on in Japanese. Associative anaphora is a type of bridging anaphora, in which the anaphor and its antecedent are not coreferent. Our model regards associative anaphora as a kind of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the a"
D09-1151,C08-1033,0,0.0204589,"resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1 The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Associative anaphora resolution is"
D09-1151,W04-0706,0,0.0333806,"ame manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1 The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Assoc"
D09-1151,C02-1122,1,0.799214,"gatory cases. In this paper, we call indispensable entities of nouns obligatory cases. Note that, obligatory does not mean grammatically obligatory but obligatory to interpret the meaning of the noun. Associative anaphora resolution needs comprehensive information of obligatory cases of nouns. Nominal case frames (NCFs) describe such information, and we construct them from the Web. 2.1 Automatic Construction of NCFs First, we brieﬂy introduce our method for constructing NCFs from raw corpora proposed in (Sasano et al., 2004). Whereas verbal case frame construction uses arguments of each verb (Kawahara and Kurohashi, 2002), nominal case frame construction basically uses adnominal constituents of each noun. However, while the meaning of a verbal argument can be distinguished by the postposition, such as “ga” (nominative), “wo” (accusative), and “ni” (dative), the meaning of an adnominal constituent can not be distinguished easily, because most adnominal constituents appear with the same postposition “no” (of). Thus, we ﬁrst conduct a semantic analysis of adnominal constituents, and then construct NCFs using the results as follows: 1. Collect syntactically unambiguous noun phrases “Nm no Nh ” from the automatic r"
D09-1151,kawahara-kurohashi-2006-case,1,0.905283,"Missing"
D09-1151,P99-1062,1,0.826617,"Missing"
D09-1151,J94-4002,0,0.237357,"he roof” in (4) means ”the roof of the house,” and it is easy to recognize by looking the ﬁrst appearance of ”the roof.” (4) I saw the roof of the house. The roof was painted dark green. While verbal case frames describe both obligatory and optional cases, nominal case frames describe only obligatory cases. Therefore, we consider all case slots of nominal case frames as the target of associative anaphora resolution. Let us consider following example: and uses the geometric mean of them. 3.3 Salience Score Filtering Previous work has reported the usefulness of salience for anaphora resolution (Lappin and Leass, 1994; Mitkov et al., 2002). In order to consider the salience of a discourse entity, we introduce the concept of salience score, which is calculated by the following set of simple rules, and only consider the entities that have the salience score no less than 1 as candidate antecedents of an associative anaphor. • +2 : mentioned with topical marker “wa,” or at the end of a sentence. • +1 : mentioned without topical marker “wa.” • +1 : assigned to a zero pronoun. (5) Toyota-wa 1997-nen hybrid car Prius-wo • ×α : beginning of each sentence. year hatsubai. 2000-nen-kara-wa kaigai-demo launched year o"
D09-1151,W03-2606,0,0.48494,"” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufﬁcient. Recently, a number of researchers argued for using the Web as a source of lexical knowledge, and the Web has been shown to be a useful resource for anaphora resolution (Bunescu, 2003; Markert et al., 2003; Poesio et al., 2004). Hence, in this study, we acquire the lexical knowledge for associative anaphora resolution from “Nm no Nh ” phrases in the Web by using the method described in (Sasano et al., 2004). We proposed a method for acquiring such lexical knowledge, called nominal case frames (NCFs), using an ordinary language dictionary and “Nm no Nh ” phrases, and constructed NCFs from newspaper corpora. In this study, we aim to acquire a sufﬁcient amount of lexical knowledge by constructing NCFs from the Web. As for associative anaphora resolution itself, we propose an integrated probabilist"
D09-1151,W02-1112,0,0.0205245,"ronymic relation between “a house” and “the roof” in (1), such knowledge as “a roof” is a part of a building or vehicle is required. To recognize the attributive relation between “Prius” and “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was a house. The roof was white. (2) Toyota launched the hybrid car Prius in 1997. The price was 21.5 million yen. To acquire such lexical knowledge, various studies have been carried out. Early studies used hand-crafted lexical knowledge such as WordNet (Strube and Hahn, 1999; Vieira and Poesio, 2000; Meyer and Dale, 2002), but obtained poor or mediocre results. Hence, Poesio et al. (2002) proposed to exploit “Nh of Nm ” phrases in large corpora to resolve associative anaphora in English; Murata et al. (1999) proposed to exploit “Nm no Nh ” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the a"
D09-1151,W99-0205,0,0.226022,"of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1 The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we"
D09-1151,poesio-etal-2002-acquiring,0,0.199098,"dge as “a roof” is a part of a building or vehicle is required. To recognize the attributive relation between “Prius” and “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was a house. The roof was white. (2) Toyota launched the hybrid car Prius in 1997. The price was 21.5 million yen. To acquire such lexical knowledge, various studies have been carried out. Early studies used hand-crafted lexical knowledge such as WordNet (Strube and Hahn, 1999; Vieira and Poesio, 2000; Meyer and Dale, 2002), but obtained poor or mediocre results. Hence, Poesio et al. (2002) proposed to exploit “Nh of Nm ” phrases in large corpora to resolve associative anaphora in English; Murata et al. (1999) proposed to exploit “Nm no Nh ” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufﬁcient. Recently, a number of researche"
D09-1151,P04-1019,0,0.524727,"resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1 The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative"
D09-1151,I08-2080,1,0.852002,"ples phological analyzer JUMAN3 adds to common nouns. In JUMAN, about twenty categories are deﬁned and tagged to common nouns. For example, “kuruma (car),” “niwatori (chicken),” and “tatemono (building)” are tagged as “VEHICLE,” “ANIMAL” and “FACILITY,” respectively. For each category, we calculated the rate of categorized examples among all case slot examples, and added it to the case slot as “[CT:VEHICLE]:0.13.” We also generalized NEs. We used a common standard NE deﬁnition for Japanese provided by IREX workshop (1999). We ﬁrst recognized NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008), and then constructed NCFs from the NE-recognized corpus. As well as categories, for each NE class, we calculated the NE rate among all case slot examples, and added it to the case slot as “[NE:PERSON]:0.22.” The generalized examples are also included in Table 1. 3 Probabilistic Model In this study, we apply a lexicalized probabilistic model for zero anaphora resolution proposed in (Sasano et al., 2008) to associative anaphora resolution. 3.1 A Lexicalized Probabilistic Model for Zero Anaphora Resolution In English, overt pronouns such as “she” and deﬁnite noun phrases such as “the company” a"
D09-1151,C04-1174,1,0.935232,"anguage Processing, pages 1455–1464, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufﬁcient. Recently, a number of researchers argued for using the Web as a source of lexical knowledge, and the Web has been shown to be a useful resource for anaphora resolution (Bunescu, 2003; Markert et al., 2003; Poesio et al., 2004). Hence, in this study, we acquire the lexical knowledge for associative anaphora resolution from “Nm no Nh ” phrases in the Web by using the method described in (Sasano et al., 2004). We proposed a method for acquiring such lexical knowledge, called nominal case frames (NCFs), using an ordinary language dictionary and “Nm no Nh ” phrases, and constructed NCFs from newspaper corpora. In this study, we aim to acquire a sufﬁcient amount of lexical knowledge by constructing NCFs from the Web. As for associative anaphora resolution itself, we propose an integrated probabilistic model for zero anaphora and associative anaphora resolution, in which associative anaphora is regarded as a kind of zero anaphora and resolved by using the same model as zero anaphora. Our model assumes"
D09-1151,C08-1097,1,0.932782,"3.” We also generalized NEs. We used a common standard NE deﬁnition for Japanese provided by IREX workshop (1999). We ﬁrst recognized NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008), and then constructed NCFs from the NE-recognized corpus. As well as categories, for each NE class, we calculated the NE rate among all case slot examples, and added it to the case slot as “[NE:PERSON]:0.22.” The generalized examples are also included in Table 1. 3 Probabilistic Model In this study, we apply a lexicalized probabilistic model for zero anaphora resolution proposed in (Sasano et al., 2008) to associative anaphora resolution. 3.1 A Lexicalized Probabilistic Model for Zero Anaphora Resolution In English, overt pronouns such as “she” and deﬁnite noun phrases such as “the company” are anaphors that refer to preceding entities (antecedents). On the other hand, in Japanese, anaphors are often omitted, which are called zero pronouns, and zero anaphora resolution is one of the most important techniques for semantic analysis in Japanese. Here, we introduce our model for zero anaphora resolution (Sasano et al., 2008). This model ﬁrst resolves coreference and identiﬁes discourse entities;"
D09-1151,N09-1059,1,0.836542,"uto ⇐ no:Ken (indispensable) (ADN) (8) Kˆoen-ni ikuto benchi-ga atta. park went bench was (I went to the park. There was a bench in φ.) TAG: benchi ⇐ no:kˆoen (possible) (ADN) We used 62 documents for testing and used the other 124 documents for calculating several probabilities. In the 62 test documents, 110 associative anaphoric relations were tagged. Each parameter for the proposed model was estimated using maximum likelihood from raw corpora, the tagged corpus, and case frames. As verbal case frames, we used the case frames constructed from the Web corpus comprising 1.6 billion sentences (Sasano et al., 2009). In order to concentrate on associative anaphora resolution, we used the correct morphemes, named entities, syntactic structures, and coreference re1461 Table 4: Experimental results of associative anaphora resolution with two baseline models and our model with/without generalized examples. Model Recall Precision F-measure Random* 0.148 0.035 0.056 (16.3/110) (16.3/467.5) 0.55 0.50 Recall 0.45 .427 0.40 F-measure 0.35 Salience0.400 (44/110) based Precision 0.30 0.135 (44/325) 0.202 0.257 (35/136) 0.268 (38/142) 0.333 (51/153) 0.363 (57/157) 0.285 Proposed 0.25 CT 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0"
D09-1151,J99-3001,0,0.0450061,"hat have an identity relation with their antecedents but the anaphor and its antecedent have different head 1 The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Associative anaphora resolution is decomposed into two steps: acquiring lexical knowledge for associative anaphora resolution, and resolving associative anaphora using the acquired knowledge. Grammatical salience plays a lesser role for resolving anaphors with full lexical heads, than for pronominal anaphora (Strube and Hahn, 1999; Modjeska, 2002). Furthermore, since associative anaphors and their antecedents usually have different head nouns, string matching technique cannot be applied. Therefore, a large and diverse amount of lexical knowledge is essential to understand associative anaphora. For example, to recognize the meronymic relation between “a house” and “the roof” in (1), such knowledge as “a roof” is a part of a building or vehicle is required. To recognize the attributive relation between “Prius” and “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was"
D09-1151,J00-4003,0,0.360919,"mple, to recognize the meronymic relation between “a house” and “the roof” in (1), such knowledge as “a roof” is a part of a building or vehicle is required. To recognize the attributive relation between “Prius” and “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was a house. The roof was white. (2) Toyota launched the hybrid car Prius in 1997. The price was 21.5 million yen. To acquire such lexical knowledge, various studies have been carried out. Early studies used hand-crafted lexical knowledge such as WordNet (Strube and Hahn, 1999; Vieira and Poesio, 2000; Meyer and Dale, 2002), but obtained poor or mediocre results. Hence, Poesio et al. (2002) proposed to exploit “Nh of Nm ” phrases in large corpora to resolve associative anaphora in English; Murata et al. (1999) proposed to exploit “Nm no Nh ” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, bu"
D09-1151,W06-1311,0,0.0285888,"domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1 The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Associative anaphora resolution is decomposed into two steps: acquiring lexical knowledge for associative anaphora resolution, and resolving associative anaphora using the acquired knowledge. Grammatical salience plays a lesser role for resolving anaphors with full lexical heads, than for pronominal anaphora (Strube and Hahn, 1999; Modjeska, 2002). Furthermore, since associative anaphors and their antecedents usually have different head nouns, string matching technique cannot be applied. Therefore, a large and diverse amount"
D09-1151,P03-2013,0,0.0231579,"language dictionary and “Nm no Nh ” phrases, and constructed NCFs from newspaper corpora. In this study, we aim to acquire a sufﬁcient amount of lexical knowledge by constructing NCFs from the Web. As for associative anaphora resolution itself, we propose an integrated probabilistic model for zero anaphora and associative anaphora resolution, in which associative anaphora is regarded as a kind of zero anaphora and resolved by using the same model as zero anaphora. Our model assumes zero pronouns that represent indispensable entities of target noun phrases, which are called zero adnominal in (Yamura-Takei, 2003), and conducts zero pronoun resolution. Let us consider the associative anaphoric relation between “Prius” and “kakaku” (price). Although “kakaku” itself is considered as the anaphor from a conventional point of view (3a), our model assumes a zero pronoun φ and considers it as the anaphor (3b). is a roof of some building, and “coach” is a coach of some sports. The relation between a noun and its indispensable entities is parallel to that between a verb and its arguments or obligatory cases. In this paper, we call indispensable entities of nouns obligatory cases. Note that, obligatory does not"
D09-1151,C96-1084,0,\N,Missing
D11-1047,P05-1032,0,0.834312,"en, translation rules are extracted from these aligned sentences. Finally, the translation rules are used in a decoding step to translate sentences. We use the term translation rule in a very broad sense here, as it may refer to substring pairs as in (Koehn et al., 2003), synchronous grammar rules as in (Chiang, 2007) or treelet pairs as in (Quirk et al., 2005; Nakazawa and Kurohashi, 2008). As the size of bilingual corpus grow larger, the number of translation rules to be stored can easily become unmanageable. As a solution to this problem in the context of phrase-based Machine Translation, (Callison-Burch et al., 2005) proposed to prealign the example corpora, but delay the rule extraction to the decoding stage. They showed that using Suffix Arrays, it was possible to efficiently retrieve all sentences containing substrings of the sentence to be translated, and thus extract the needed translation rules on-the-fly. (Lopez, 2007) proposed an Sadao Kurohashi Graduate School of Informatics Kyoto University Kyoto, Japan kuro@i.kyoto-u.ac.jp extension of this method for retrieving discontinuous substrings, making it suitable for systems such as (Chiang, 2007). In this paper, we propose a method to apply the same"
D11-1047,J07-2003,0,0.120855,"ed (EBMT) and Statistical Machine Translation (SMT) paradigms make use of the translation examples provided by a parallel bilingual corpus to produce new translations. Most of these translation systems process the example data in a similar way: The parallel sentences are first word-aligned. Then, translation rules are extracted from these aligned sentences. Finally, the translation rules are used in a decoding step to translate sentences. We use the term translation rule in a very broad sense here, as it may refer to substring pairs as in (Koehn et al., 2003), synchronous grammar rules as in (Chiang, 2007) or treelet pairs as in (Quirk et al., 2005; Nakazawa and Kurohashi, 2008). As the size of bilingual corpus grow larger, the number of translation rules to be stored can easily become unmanageable. As a solution to this problem in the context of phrase-based Machine Translation, (Callison-Burch et al., 2005) proposed to prealign the example corpora, but delay the rule extraction to the decoding stage. They showed that using Suffix Arrays, it was possible to efficiently retrieve all sentences containing substrings of the sentence to be translated, and thus extract the needed translation rules o"
D11-1047,N03-1017,0,0.00298173,"ible matchings. 1 Introduction The popular Example-Based (EBMT) and Statistical Machine Translation (SMT) paradigms make use of the translation examples provided by a parallel bilingual corpus to produce new translations. Most of these translation systems process the example data in a similar way: The parallel sentences are first word-aligned. Then, translation rules are extracted from these aligned sentences. Finally, the translation rules are used in a decoding step to translate sentences. We use the term translation rule in a very broad sense here, as it may refer to substring pairs as in (Koehn et al., 2003), synchronous grammar rules as in (Chiang, 2007) or treelet pairs as in (Quirk et al., 2005; Nakazawa and Kurohashi, 2008). As the size of bilingual corpus grow larger, the number of translation rules to be stored can easily become unmanageable. As a solution to this problem in the context of phrase-based Machine Translation, (Callison-Burch et al., 2005) proposed to prealign the example corpora, but delay the rule extraction to the decoding stage. They showed that using Suffix Arrays, it was possible to efficiently retrieve all sentences containing substrings of the sentence to be translated,"
D11-1047,D07-1104,0,0.0703752,"pairs as in (Quirk et al., 2005; Nakazawa and Kurohashi, 2008). As the size of bilingual corpus grow larger, the number of translation rules to be stored can easily become unmanageable. As a solution to this problem in the context of phrase-based Machine Translation, (Callison-Burch et al., 2005) proposed to prealign the example corpora, but delay the rule extraction to the decoding stage. They showed that using Suffix Arrays, it was possible to efficiently retrieve all sentences containing substrings of the sentence to be translated, and thus extract the needed translation rules on-the-fly. (Lopez, 2007) proposed an Sadao Kurohashi Graduate School of Informatics Kyoto University Kyoto, Japan kuro@i.kyoto-u.ac.jp extension of this method for retrieving discontinuous substrings, making it suitable for systems such as (Chiang, 2007). In this paper, we propose a method to apply the same idea to Syntax-Based SMT and EBMT (Quirk et al., 2005; Mi et al., 2008; Nakazawa and Kurohashi, 2008). Since Syntax-Based systems usually work with the parse trees of the source-side sentences, we will need to be able to retrieve efficiently examples trees from fragments (treelets) of the parse tree of the sentenc"
D11-1047,P08-1023,0,0.0782839,"e rule extraction to the decoding stage. They showed that using Suffix Arrays, it was possible to efficiently retrieve all sentences containing substrings of the sentence to be translated, and thus extract the needed translation rules on-the-fly. (Lopez, 2007) proposed an Sadao Kurohashi Graduate School of Informatics Kyoto University Kyoto, Japan kuro@i.kyoto-u.ac.jp extension of this method for retrieving discontinuous substrings, making it suitable for systems such as (Chiang, 2007). In this paper, we propose a method to apply the same idea to Syntax-Based SMT and EBMT (Quirk et al., 2005; Mi et al., 2008; Nakazawa and Kurohashi, 2008). Since Syntax-Based systems usually work with the parse trees of the source-side sentences, we will need to be able to retrieve efficiently examples trees from fragments (treelets) of the parse tree of the sentence we want to translate. We will also propose extensions of this method allowing more flexible matchings. 2 Overview of the method 2.1 Treelet retrieval We first formalize the setting of this chapter by providing some definitions. Definition 2.1 (Treelets). A treelet is a connected subgraph of a tree. A treelet T1 is a subtreelet of another treelet T2 if"
D11-1047,P05-1034,0,0.21599,"slation (SMT) paradigms make use of the translation examples provided by a parallel bilingual corpus to produce new translations. Most of these translation systems process the example data in a similar way: The parallel sentences are first word-aligned. Then, translation rules are extracted from these aligned sentences. Finally, the translation rules are used in a decoding step to translate sentences. We use the term translation rule in a very broad sense here, as it may refer to substring pairs as in (Koehn et al., 2003), synchronous grammar rules as in (Chiang, 2007) or treelet pairs as in (Quirk et al., 2005; Nakazawa and Kurohashi, 2008). As the size of bilingual corpus grow larger, the number of translation rules to be stored can easily become unmanageable. As a solution to this problem in the context of phrase-based Machine Translation, (Callison-Burch et al., 2005) proposed to prealign the example corpora, but delay the rule extraction to the decoding stage. They showed that using Suffix Arrays, it was possible to efficiently retrieve all sentences containing substrings of the sentence to be translated, and thus extract the needed translation rules on-the-fly. (Lopez, 2007) proposed an Sadao"
D11-1047,P03-1010,0,0.0272825,"Missing"
D11-1047,2007.mtsummit-papers.63,0,0.0905342,"Missing"
D11-1047,J01-1001,0,0.0364403,"to lst; Tnew ← [parent(root(T)), (0, . . . , #T, . . . , 0)]; Append Tnew to lst; possible that some tree exactly identical to the query tree (or some tree having a very large treelet in common with the query tree) do exist in the database. This case is obviously a best case for translation, but unfortunately could be a worst-case for our algorithm, as it means that all of the (possibly trillions of) treelets of the query tree will be computed. To solve this issue, we try to consider a concept analogous to that of maximal substring, or substring class, found in Suffix Trees and Suffix Arrays (Yamamoto and Church, 2001). The idea is that in most cases where a query tree is “full” (that is all of its treelets are not empty), most of the larger treelets will share the same occurrences (in the database trees that are very similar to the query tree). We formalize this as follow: Definition 4.2 (domination and maximal treelets). Let T1 be a subtreelet of T2 . If for every matching M1 of occ(T1 ), there exist a matching M2 of occ(T2 ) such that M2 |T 1 = M1 , we say that T1 is dominated by T2 . A treelet is maximal if it is not dominated by any other treelet. 4.2 Pruning non-maximal treelets If T1 is dominated by"
D11-1047,2005.eamt-1.39,0,0.0712749,"o-root. Once the Path-to-Root Array is built, for a linear treelet T, we can find its occurrences by a binary search of the first and last path-to-root starting with the labels of T. See figure 3 for an example. Memory cost is quite manageable, since we only need 10 bytes per nodes in total. 5 bytes per pointer in the array (tree id: 4 bytes, start position: 1 byte), and 5 bytes per nodes to store the database in memory (label id:4 bytes, parent position: 1 byte). All the optimization tricks proposed in (Lopez, 2007) for Suffix Arrays can be used here, especially the optimization proposed in (Zhang and Vogel, 2005). 3.5 Inverted Index and Precomputation Instead of a Path-to-Root array, one can simply use an inverted index. The inverted index associates with every label the set of its occurrences, each occurrences being represented by a tuple containing the index of the tree, the position of the label in the tree, and the position of the parent of the label in 512 the tree. Knowing the position of the parent will allow to compute treelets of size 2 by intersection (D-coverage). This is less effective than the PathTo-Root Array approach, but open the possibilities for the flexible search discussed in sect"
D11-1047,D09-1108,0,0.0544875,"Missing"
D11-1056,C00-1004,0,0.0265175,"the probability of every possible state of the jointly sampled random variables, we only compare the current state with a proposed state. This greatly eases the sampling procedure while retaining the efficiency of typebased sampling. Experiments show that the proposed method quickly corrects the initial segmentation given by a morphological analyzer. 2 Related Work Japanese Morphological Analysis and Lexical Acquisition Word segmentation for Japanese is usually solved as the joint task of segmentation and part-of-speech tagging, which is called morphological analysis (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). The standard approach in Japanese morphological analysis is lattice-based path selection instead of characterbased IOB tagging. Given a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using pre-defined parameters. This approach enables fast decoding and achieves accuracy high enough for practical use. This success, however, depends on a highcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara an"
D11-1056,C04-1066,0,0.067009,"Missing"
D11-1056,P09-2085,0,0.0198782,"l , we can again explain the model with the Chinese restaurant process. Unlike the unigram model, however, the bigram model depends on the latent table assignments z−i . h P2 (wi |h−i ) = n(w−ii−1 ,wi ) + α1 P1 (wi |h−i ) h n(w−ii−1 ,∗) + α1 (2) h P1 (wi |h−i ) = tw−i i + α0 P0 (wi ) h t∗ −i + α0 (3) h where h−i = (w−i , z−i ), tw−i is the number of tai h−i bles labeled with wi and t∗ is the total number of tables. Thanks to exchangeability, we do not need to track the exact seating assignments. Still, we need to maintain a histogram for each w that consists of frequencies of table customers (Blunsom et al., 2009). 608 P (k; λ) = e−λ P (c1 , · · · , ck , k|Θ) = k+1 ∏ i=1 P (c1 , · · · , ck , k|Θ) P (k|Θ) λk k! P (ci |ci−1 ) Θ is the zerogram model, and c0 and ck+1 are a word boundary marker. P (k|Θ) can be estimated by randomly generating words from the model. We use different λ for different scripts. The Japanese writing system uses several scripts, and each word can be classified by script such as hiragana, katakana, kanji, the mixture of hiragana and kanji, etc. The optimal value for λ depends on scripts. For example, katakana, which predominantly denotes loan words, is longer on average than hiraga"
D11-1056,C10-1060,0,0.0141074,"e conjecture that words in their dictionary are not noun phrases. External resources used by Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers. In our settings, we are given a list of entries from external resources. Almost all of them are noun phrases and each entry consists of"
D11-1056,P08-1046,0,0.0272229,"rs in length. We conjecture that words in their dictionary are not noun phrases. External resources used by Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers. In our settings, we are given a list of entries from external resources. Almost all of them are noun phrases"
D11-1056,P08-1047,0,0.0318127,"ment noun phrases. The new statistical method provides a straightforward solution to this problem. Meanwhile, our language models have their own problem. The assumption that language is a sequence of invariant words fails to capture rich morphology, as our segmentation criteria specify that each verb or adjective consists of an invariant stem and an ending that changes its form according to its grammatical roles. For this reason, we limit our scope to noun phrases in this paper. Use of Noun Phrases Named entity recognition (NER) is a field where encyclopedic knowledge plays an important role. Kazama and Torisawa (2008) encode information extracted from a gazetteer (e.g. Wikipedia) as features of a CRFbased Japanese NE tagger. They formalize the NER task as the character-based labeling of IOB tags. Noun phrases extracted from a gazetteer are also straightforwardly represented as IOB tags. However, this does not fully solve the knowledge bottleneck problem. They also used the output of a morphological analyzer, which does not utilize encyclopedic knowledge. NER performance may be affected by segmentation errors in morphological analysis involving unknown words. Chinese word segmentation is often formalized as"
D11-1056,W04-3230,0,0.301474,"sible state of the jointly sampled random variables, we only compare the current state with a proposed state. This greatly eases the sampling procedure while retaining the efficiency of typebased sampling. Experiments show that the proposed method quickly corrects the initial segmentation given by a morphological analyzer. 2 Related Work Japanese Morphological Analysis and Lexical Acquisition Word segmentation for Japanese is usually solved as the joint task of segmentation and part-of-speech tagging, which is called morphological analysis (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). The standard approach in Japanese morphological analysis is lattice-based path selection instead of characterbased IOB tagging. Given a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using pre-defined parameters. This approach enables fast decoding and achieves accuracy high enough for practical use. This success, however, depends on a highcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004),"
D11-1056,J09-4006,0,0.0164298,"y. 5.4 Additional Constraints Partial annotations (Tsuboi et al., 2008; Neubig and Mori, 2010) can be used for inference. If we know in advance that a certain position is a boundary or nonboundary, we simply keep it unaltered. As partiallyannotated text, we can use markup. Suppose that the original text is written with wiki markup as follows: *JR[[宇野線]][[常山駅]] [gloss] JR Ube Line Tsuneyama Station It is clear that the position between “線” (line) and “常” (tsune) is a boundary. Similarly, we can impose our trivial rules of segmentation on the model. For example, we can keep punctuation markers (Li and Sun, 2009) separate from others. 6 Experiments 6.1 Settings Data Set We evaluated our approach on Japanese Wikipedia. For each entry of Wikipedia, we regarded the title as a noun phrase and used both the title and main text for segmentation. We separately applied our segmentation procedure to each entry. We constructed the data set as follows. We extracted each entry from an XML dump of Japanese Wikipedia.4 We normalized the title by dropping trailing parentheses that disambiguate entries with similar names (e.g. “赤城 (空母)” for Akagi (aircraft carrier)). We extracted the main text from wikitext and used"
D11-1056,I05-3025,0,0.0168433,"lize the NER task as the character-based labeling of IOB tags. Noun phrases extracted from a gazetteer are also straightforwardly represented as IOB tags. However, this does not fully solve the knowledge bottleneck problem. They also used the output of a morphological analyzer, which does not utilize encyclopedic knowledge. NER performance may be affected by segmentation errors in morphological analysis involving unknown words. Chinese word segmentation is often formalized as a character tagging problem (Xue, 2003). In this setting, it is easy to incorporate external resources into the model. Low et al. (2005) introduce an external dictionary as features of a discriminative model. However, they only use words up to 4 characters in length. We conjecture that words in their dictionary are not noun phrases. External resources used by Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsuper"
D11-1056,P09-1012,0,0.417177,"gmentation. We segment each entry noun phrase into words. To do this, we examine the main text of the entry, on the assumption that if the noun phrase in question consists of more than one word, its constituents appear in the main text either freely or as part of other noun phrases. For “常山城” (tsuneyama-jou), its constituent “常山” (tsune) appears by itself and as constituents of other nouns phrases such as “常山山 頂” (peak of Tsuneyama) and “常山駅” (Tsuneyama Station) while “山城” (yamashiro) does not. To segment each noun phrase, we use nonparametric Bayesian language models (Goldwater et al., 2009; Mochihashi et al., 2009). Our approach 605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 605–615, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics is based on two key factors: the bigram model and type-based block sampling. The bigram model alleviates a problem of the unigram model, that is, a tendency to misidentify a sequence of words in common collocations as a single word. Type-based sampling (Liang et al., 2010) has the ability to directly escape a local optimum, making inference very efficient. However, type-based samplin"
D11-1056,D08-1045,1,0.826396,"overage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004), it by no means provides a definitive solution because it suffers from locality of contextual information available for identification (Nakagawa and Matsumoto, 2006). Therefore we like to perform separate lexical acquisition processes in which wider context can be 606 examined. Our approach in this paper has a complementary relationship with unknown word acquisition from text, which we previously proposed (Murawaki and Kurohashi, 2008). Since, unlike Chinese and Thai, Japanese is rich in morphology, morphological regularity can be used to determine if an unknown word candidate in text is indeed the word to be acquired. In general, this method works pretty well, but one exception is noun phrases. Noun phrases can hardly be distinguished from single nouns because in Japanese, no morphological marker is attached to join nouns to form a noun phrase. We previously resort to a heuristic measure to segment noun phrases. The new statistical method provides a straightforward solution to this problem. Meanwhile, our language models h"
D11-1056,W96-0205,0,0.0420374,"phological analyzer JUMAN is capable of handling phrases, each of which consists of more than one word. All we need to do is POS tagging. where G is a distribution over a countably infinite set of words, and DP(α0 , P0 ) is a Dirichlet process (Ferguson, 1973) with the concentration parameter α0 and the base distribution P0 , for which we use a zerogram model described in Section 4.3. Marginalizing out G, we can interpret the model as a Chinese restaurant process. Suppose that we have observed i − 1 words w−i = w1 , · · · , wi−1 , the probability of wi is given by 4.3 Zerogram Model Following Nagata (1996) and Mochihashi et al. (2009), we model the zerogram distribution P0 with the word length k and the character sequence w = c1 , · · · , ck . Specifically, we define P0 as the combination of a Poisson distribution with mean λ and a bigram distribution over characters. P0 (w) = P (k; λ) w P1 (wi = w|w−i ) = w nw −i + α0 P0 , i − 1 + α0 (1) where nw −i is the number of word label w observed in w−i . The unigram model is known for its tendency to misidentify a sequence of words in common collocations as a single word (Goldwater et al., 2009). In preliminary experiments, we found that the unigram m"
D11-1056,P06-1089,0,0.0182333,"a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using pre-defined parameters. This approach enables fast decoding and achieves accuracy high enough for practical use. This success, however, depends on a highcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004), it by no means provides a definitive solution because it suffers from locality of contextual information available for identification (Nakagawa and Matsumoto, 2006). Therefore we like to perform separate lexical acquisition processes in which wider context can be 606 examined. Our approach in this paper has a complementary relationship with unknown word acquisition from text, which we previously proposed (Murawaki and Kurohashi, 2008). Since, unlike Chinese and Thai, Japanese is rich in morphology, morphological regularity can be used to determine if an unknown word candidate in text is indeed the word to be acquired. In general, this method works pretty well, but one exception is noun phrases. Noun phrases can hardly be distinguished from single nouns b"
D11-1056,neubig-mori-2010-word,0,0.0192007,"l counts accordingly. 4. Calculate the proposed conditional probability while adding words one-by-one. 5. Decide whether to accept the proposal according to (4). If the proposal is accepted, we finalize the arrangement; otherwise we revert to the current state. We implement skip approximation (Liang et al., 2010) and sample each type once per iteration. This is motivated by the observation that although the joint sampling of a large number of positions is computationally expensive, the proposal is accepted very infrequently. 5.4 Additional Constraints Partial annotations (Tsuboi et al., 2008; Neubig and Mori, 2010) can be used for inference. If we know in advance that a certain position is a boundary or nonboundary, we simply keep it unaltered. As partiallyannotated text, we can use markup. Suppose that the original text is written with wiki markup as follows: *JR[[宇野線]][[常山駅]] [gloss] JR Ube Line Tsuneyama Station It is clear that the position between “線” (line) and “常” (tsune) is a boundary. Similarly, we can impose our trivial rules of segmentation on the model. For example, we can keep punctuation markers (Li and Sun, 2009) separate from others. 6 Experiments 6.1 Settings Data Set We evaluated our a"
D11-1056,C10-1092,0,0.0240923,"ic Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers. In our settings, we are given a list of entries from external resources. Almost all of them are noun phrases and each entry consists of one or more words. A na¨ıve implementation would be to use noun phrases as they are. In fact, ipadic1 regards as single words a large number of long proper nouns lik"
D11-1056,C04-1081,0,0.0343283,"output of a morphological analyzer, which does not utilize encyclopedic knowledge. NER performance may be affected by segmentation errors in morphological analysis involving unknown words. Chinese word segmentation is often formalized as a character tagging problem (Xue, 2003). In this setting, it is easy to incorporate external resources into the model. Low et al. (2005) introduce an external dictionary as features of a discriminative model. However, they only use words up to 4 characters in length. We conjecture that words in their dictionary are not noun phrases. External resources used by Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text"
D11-1056,C08-1113,0,0.018171,"and updating the model counts accordingly. 4. Calculate the proposed conditional probability while adding words one-by-one. 5. Decide whether to accept the proposal according to (4). If the proposal is accepted, we finalize the arrangement; otherwise we revert to the current state. We implement skip approximation (Liang et al., 2010) and sample each type once per iteration. This is motivated by the observation that although the joint sampling of a large number of positions is computationally expensive, the proposal is accepted very infrequently. 5.4 Additional Constraints Partial annotations (Tsuboi et al., 2008; Neubig and Mori, 2010) can be used for inference. If we know in advance that a certain position is a boundary or nonboundary, we simply keep it unaltered. As partiallyannotated text, we can use markup. Suppose that the original text is written with wiki markup as follows: *JR[[宇野線]][[常山駅]] [gloss] JR Ube Line Tsuneyama Station It is clear that the position between “線” (line) and “常” (tsune) is a boundary. Similarly, we can impose our trivial rules of segmentation on the model. For example, we can keep punctuation markers (Li and Sun, 2009) separate from others. 6 Experiments 6.1 Settings Dat"
D11-1056,W01-0512,0,0.0420106,"1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). The standard approach in Japanese morphological analysis is lattice-based path selection instead of characterbased IOB tagging. Given a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using pre-defined parameters. This approach enables fast decoding and achieves accuracy high enough for practical use. This success, however, depends on a highcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004), it by no means provides a definitive solution because it suffers from locality of contextual information available for identification (Nakagawa and Matsumoto, 2006). Therefore we like to perform separate lexical acquisition processes in which wider context can be 606 examined. Our approach in this paper has a complementary relationship with unknown word acquisition from text, which we previously proposed (Murawaki and Kurohashi, 2008). Since, unlike Chinese and Thai, Japanese is rich in morphology, morphological regularity can be used to determine if an unknown"
D11-1056,C08-1128,0,0.0314486,"ers. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers. In our settings, we are given a list of entries from external resources. Almost all of them are noun phrases and each entry consists of one or more words. A na¨ıve implementation would be to use noun phrases as they are. In fact, ipadic1 regards as single words a large number of"
D11-1056,O03-4002,0,0.040052,"from a gazetteer (e.g. Wikipedia) as features of a CRFbased Japanese NE tagger. They formalize the NER task as the character-based labeling of IOB tags. Noun phrases extracted from a gazetteer are also straightforwardly represented as IOB tags. However, this does not fully solve the knowledge bottleneck problem. They also used the output of a morphological analyzer, which does not utilize encyclopedic knowledge. NER performance may be affected by segmentation errors in morphological analysis involving unknown words. Chinese word segmentation is often formalized as a character tagging problem (Xue, 2003). In this setting, it is easy to incorporate external resources into the model. Low et al. (2005) introduce an external dictionary as features of a discriminative model. However, they only use words up to 4 characters in length. We conjecture that words in their dictionary are not noun phrases. External resources used by Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2"
D11-1056,N10-1082,0,\N,Missing
D11-1056,D10-1083,0,\N,Missing
D11-1056,1995.mtsummit-1.17,0,\N,Missing
D13-1095,kawahara-etal-2002-construction,1,\N,Missing
D13-1095,poesio-etal-2010-creating,0,\N,Missing
D13-1095,C08-1097,1,\N,Missing
D13-1095,N06-1023,1,\N,Missing
D13-1095,P09-2022,0,\N,Missing
D13-1095,P06-1079,0,\N,Missing
D13-1095,D10-1086,0,\N,Missing
D13-1095,E12-1072,0,\N,Missing
D13-1095,I11-1085,1,\N,Missing
D13-1095,Y12-1058,1,\N,Missing
D13-1121,Y00-1002,0,0.0604865,"ons. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into active sentences. Murata et al. (2006) developed a machinelearning-based method for Japanese ca"
D13-1121,J10-4006,0,0.019436,"pus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasin"
D13-1121,W07-1522,0,0.160739,"Missing"
D13-1121,C02-1122,1,0.790053,"al ablative genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. and tagged their cases in the active voice. Then, they trained SVM classiﬁers using the tagged corpus. Their features for training SVM were made by using several lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method ﬁrst parses the input sentences, and then constructs case frames by collecting reliable modiﬁer-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances"
D13-1121,N06-1023,1,0.94546,"ned from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations."
D13-1121,kawahara-etal-2004-toward,1,0.755809,"al entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositional case particles function as case markers. (1) 女が woman-ga 男に man-ni 突き落とされた． was pushed down (A woman was pushed down by a man.) (2) 男が man-ga 女を woman-wo 突き落とした． pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero ana"
D13-1121,korhonen-etal-2006-large,0,0.175816,"mes by collecting reliable modiﬁer-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances that can ﬁll a case slot, which is fully lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). We list some Japanese postpositional case particles with their typical grammatical functions in Table 1 and show examples of case frames in Table 2.4 Ideally, one case frame is constructed for each meaning and voice of the target predicate. However, since Kawahara and Kurohashi’s method is unsupervised, several case frames are actually constructed 4 Niyotte in Table 2 is a Japanese functional phrase that indicates agent in this case. We treat niyotte as a case particle in this paper for the sake of simplicity. 1215 Case Frame: “突き落とされる-5 (be pushed down-5)” { 京子 (Kyoko):3, 監督 (manager):1, ·"
D13-1121,J04-1003,0,0.029428,"panese. Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did"
D13-1121,P08-1050,0,0.01896,"on patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alter"
D13-1121,P98-2127,0,0.150639,"ces of the ga-case of the case frame “突き落とされ る-5 (be pushed down-5)” and the wo-case of the case frame “突き落とす-4 (push down-4),” which are considered to be aligned and represent patient, are similar. Thus, we exploit semantic similarity simSEM between the instances of the corresponding cases. We ﬁrst deﬁne an asymmetric similarity measure between C1 and C2 , each of which is a set of case slot instances, as follows: sima (C1 , C2 ) = 1  max (sim(i1 , i2 )), i2 ∈C2 |C1 | i1 ∈C1 where sim(i1 , i2 ) is the similarity between instances. In this study, we apply a distributional similarity measure (Lin, 1998), which was computed from the Web corpus used to construct the case frames. We next deﬁne a symmetric similarity measure between C1 and C2 as an average of sima (C1 , C2 ) and sima (C2 , C1 ). 1 sims (C1 , C2 ) = (sima (C1 , C2 )+sima (C2 , C1 )). 2 Then we deﬁne semantic similarity of a case alignment A between case frames CF1 and CF2 . simSEM (A) = 3. Preference of alternation patterns: fP P . N 1  sims (C1,i , C2,a(i) ), N i=1 1217  ,GHQWLID FRUUHVSRQGLQJ FDVHIUDPH where N denotes the number of case slots of CF1 , C1,i denotes a set of instances of the i-th case slot of CF1 , and C"
D13-1121,A00-2034,0,0.0687522,"ve voices in Japanese. Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-craft"
D13-1121,P06-2076,0,0.133723,"passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into active sentences. Murata et al. (2006) developed a machinelearning-based method for Japanese case alternation. They extracted 3,576 case particles in passive sentences from the Kyoto University Text Corpus Case particle ga wo ni de kara no Case Frame: “突き落とされる-4 (be pushed down-4)” { 女性 (woman):5, 僕 (I):2, 女 (woman):2, · · · }-ga { 海 (sea):229, 川 (bottom):115, 池 (pond):51, · · · }-ni { 継母(stepmother):2, ペガサス(Pegasus):2, · · · }-niyotte ··· Grammatical function nominative accusative dative locative, instrumental ablative genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. an"
D13-1121,P98-2151,0,0.0145837,"f the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositional case particles function as case markers. (1) 女が woman-ga 男に man-ni 突き落とされた． was pushed down (A woman was pushed down by a man.) (2) 男が man-ga 女を woman-wo 突き落とした． pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero anaphors (Nagao and Hasida, 1998). In Japanese, zero anaphora frequently occurs, and the omitted unnormalizedcase of a zero anaphor is often the same as the surface case of its antecedent (Sasano and Kurohashi, 2011). Therefore, surface case analysis suits zero anaphora resolution. On the other hand, when 2 Ga, wo, and ni are typical Japanese postpositional case particles. In most cases, they indicate nominative, accusative, and dative, respectively. 1213 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for"
D13-1121,J05-1004,0,0.0714087,"several lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method ﬁrst parses the input sentences, and then constructs case frames by collecting reliable modiﬁer-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances that can ﬁll a case slot, which is fully lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). We list some Japanese postpositional case particles with their typical grammatical functions in Table 1 and show examples of case frames in Table 2."
D13-1121,I11-1085,1,0.838446,"nstead, postpositional case particles function as case markers. (1) 女が woman-ga 男に man-ni 突き落とされた． was pushed down (A woman was pushed down by a man.) (2) 男が man-ga 女を woman-wo 突き落とした． pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero anaphors (Nagao and Hasida, 1998). In Japanese, zero anaphora frequently occurs, and the omitted unnormalizedcase of a zero anaphor is often the same as the surface case of its antecedent (Sasano and Kurohashi, 2011). Therefore, surface case analysis suits zero anaphora resolution. On the other hand, when 2 Ga, wo, and ni are typical Japanese postpositional case particles. In most cases, they indicate nominative, accusative, and dative, respectively. 1213 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics we focus on the resulting predicate argument structures, the normalized-case structure is more useful. Speciﬁcally, since a normalized-case structure repres"
D13-1121,P08-1057,0,0.0164512,"raints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dea"
D13-1121,D09-1067,0,0.100645,"xical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the pass"
D13-1121,D08-1055,0,0.165115,"ur method aligns a case frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositio"
D13-1121,I11-1126,0,0.146631,"ase frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositional case particles functi"
D13-1121,C98-2122,0,\N,Missing
D13-1121,C98-2146,0,\N,Missing
D14-1063,J04-2004,0,0.158637,"d in the previously mentioned (Heaﬁeld et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to ﬁnd the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More speciﬁcally, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are however based on diﬀerent paradigm, typically trying to directly learn a transducer rather than extracting SCFG-like rules. Closer to our context, (de Gispert et al., 2010) propose to use Finite-State Transducers in the context of Hierarchical Phrase Based Translation. Their method is to iteratively construct and minimize the full “top-level lattice” representing"
D14-1063,W05-1506,0,0.152204,"en remove v1 and v2 . The backward vertex merging is deﬁned similarly to the forward merging, but with going right to left and inverting the role of the incoming and outgoing edges. 581 misations in the future. 4.4 5.1 Overview The outline of the decoding algorithm is described by algorithm 3. For simplicity, the description only compute the optimal model score over the translations in the candidate set. It is however trivial to adapt the description to keep track of which sentence correspond to this optimal score and output it instead of the score. Likewise, using the technique described in (Huang and Chiang, 2005), one can easily output k-best lists of translations. For simplicity again, we consider that a n-gram language model score is the only stateful nonlocal feature used for computing the model score, although in a tree-to-tree setting, other features (local in a tree representation but not in a string representation) could be used. The model score of a translation t has therefore the shape: Handling of Edge Features In the context of parameter tuning, we usually want the decoder to output not only the translations, but also a list of features characterizing the way the translation was constructed"
D14-1063,J10-3008,0,0.0464334,"Missing"
D14-1063,knight-al-onaizan-1998-translation,0,0.0880962,"f stack based decoding for Phrase-Based Machine Translation. (Chiang, 2007) introduced the cube-pruning approach, which has been further improved in the previously mentioned (Heaﬁeld et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to ﬁnd the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More speciﬁcally, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are however based on diﬀerent paradigm, typically trying to directly learn a transducer rather than extracting SCFG-like rules. Closer to our context, (de Gispert et al., 2010) propose to use Finite-State Transducers in the c"
D14-1063,N03-1017,0,0.00633596,"e/search/ Representation: Peak memory used Average search time #vertices (avg/max) #edges (avg/max) Original 39 GB 6.13s 65K (1300K) 92K (1512K) Optimized 16GB 3.31s 32K (446K) 83K (541K) Expanded 85GB 9.95s 263K (5421K) 263K (5421K) Table 1: Impact of the lattice representation on performances. System Lattice No-variations Moses (for scale) JA–EN 29.43 28.91 28.86 7 Related work Searching for the most optimal translation in an implicitly deﬁned set has been the focus of a lot of research in Machine Translation and it would be diﬃcult to cover all of it. Among the most inﬂuential approaches, (Koehn et al., 2003) was using a form of stack based decoding for Phrase-Based Machine Translation. (Chiang, 2007) introduced the cube-pruning approach, which has been further improved in the previously mentioned (Heaﬁeld et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to ﬁnd the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More speciﬁcally, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Mac"
D14-1063,P07-2045,0,0.00493772,"posed in section 3. That is, we consider rules for which null-aligned words are bypassable by epsilonedges, for which Non-terminal are allowed to take several alternative positions around the word that is thought to be their governor, and for which we consider alternative morphologies of a few words (“is/are”, “a/an”). We compare this approach with heuristically selecting only one possibility for each variation present in the lattice rule extracted from a single example. Results shown on ﬁgure 2 show that we do obtain a signiﬁcant improvement in translation quality. Note that the Moses score (Koehn et al., 2007), taken from the oﬃcial results of NTCIR-10 is only here “for scale”, as our MT system uses a quite diﬀerent pipeline. 5 in particular, we factored out the representation optimisation time, which is reasonable if we are in the setting of a parameter tuning step in which the same sentences are translated repeatedly 585 System Peak memory used Average search time Average model score Nb wins K-decoder 52G 3.47s -107.55 401 Lattice-rule decoder 16G 3.31s -107.39 579 Table 3: Evaluation of the performances of our lattice-rule decoder compared with a state-ofthe-art decoder using an expanded ﬂat rep"
D14-1063,N04-1014,0,0.124171,"lution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More speciﬁcally, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are however based on diﬀerent paradigm, typically trying to directly learn a transducer rather than extracting SCFG-like rules. Closer to our context, (de Gispert et al., 2010) propose to use Finite-State Transducers in the context of Hierarchical Phrase Based Translation. Their method is to iteratively construct and minimize the full “top-level lattice” representing the whole set of translations bottom-up. It is an approach more focused on the Finite State Machine aspect than our, Table 2: Impact on"
D14-1063,2011.iwslt-evaluation.24,0,0.113928,"an edge is labeled by a nonterminal X, we ﬁrst traverse the corresponding lattice RHS(X) following the same process. Such a partial translation can be reduced compactly to a scored language model state (l, r, s), where l represent the ﬁrst n words1 of the partial translation, r its last n words and s its partial score. It is clear that if two partial translations have the same l and r parts but diﬀerent score, we can discard the one with the lowest score, as it cannot be a part of the optimal translation. Further, using the state reduction techniques described in (Li and Khudanpur, 2008) and (Heaﬁeld et al., 2011), we can often reduce the size of l and r to less than n, allowing further opportunities for discarding sub-optimal Decoding algorithm In order to make an optimal use of these lattice-rule representations, we developed a decoding algorithm for translation candidate sets represented as a set of lattice-rules. For the most part, this algorithm re-use many of the techniques previously developed for decoding translation search spaces, but adapt them to our setting. 1 582 n being the order of the language mode partial translations. For better behavior during the cube-pruning step of the algorithm ("
D14-1063,D12-1107,0,0.135391,"for discarding sub-optimal Decoding algorithm In order to make an optimal use of these lattice-rule representations, we developed a decoding algorithm for translation candidate sets represented as a set of lattice-rules. For the most part, this algorithm re-use many of the techniques previously developed for decoding translation search spaces, but adapt them to our setting. 1 582 n being the order of the language mode partial translations. For better behavior during the cube-pruning step of the algorithm (see later), the partial score s of a partial translation includes rest-costs estimates (Heaﬁeld et al., 2012). We deﬁne the concatenation operation on scored language model states to be: (l1 , r1 , s1 ) ⊕ (l2 , r2 , s2 ) = (l3 , r3 , s3 ), where s3 = s1 + s2 + λlm(r1 , l2 ), with lm(r1 , l2 ) being the language model probability of l2 given r1 with rest-costs adjustments. r3 and l3 are the resulting minimized states. Similarly, if an edge e is labeled by a word, we deﬁne the concatenation of a scored state with an edge to be (l1 , r1 , s1 ) ⊕ e = (l2 , r2 , s2 ) where s2 = s1 + score(e) + λlm(word(e)|r1 ). Conveniently for us, the KenLM2 opensource library (Heaﬁeld, 2011) provides functionalities for"
D14-1063,W08-0402,0,0.0988031,"by a nonterminal until v. If an edge is labeled by a nonterminal X, we ﬁrst traverse the corresponding lattice RHS(X) following the same process. Such a partial translation can be reduced compactly to a scored language model state (l, r, s), where l represent the ﬁrst n words1 of the partial translation, r its last n words and s its partial score. It is clear that if two partial translations have the same l and r parts but diﬀerent score, we can discard the one with the lowest score, as it cannot be a part of the optimal translation. Further, using the state reduction techniques described in (Li and Khudanpur, 2008) and (Heaﬁeld et al., 2011), we can often reduce the size of l and r to less than n, allowing further opportunities for discarding sub-optimal Decoding algorithm In order to make an optimal use of these lattice-rule representations, we developed a decoding algorithm for translation candidate sets represented as a set of lattice-rules. For the most part, this algorithm re-use many of the techniques previously developed for decoding translation search spaces, but adapt them to our setting. 1 582 n being the order of the language mode partial translations. For better behavior during the cube-prun"
D14-1063,N13-1116,0,0.0222895,"Missing"
D14-1063,J97-2003,0,0.052651,"plicitly deﬁned set has been the focus of a lot of research in Machine Translation and it would be diﬃcult to cover all of it. Among the most inﬂuential approaches, (Koehn et al., 2003) was using a form of stack based decoding for Phrase-Based Machine Translation. (Chiang, 2007) introduced the cube-pruning approach, which has been further improved in the previously mentioned (Heaﬁeld et al., 2013). (Rush and Collins, 2011) recently proposed an algorithm promising to ﬁnd the optimal solution, but that is rather slow in practice. Weighted Finite State Machines have seen a variety of use in NLP (Mohri, 1997). More speciﬁcally, some other previous work on Machine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Translation, (Knight and Al-Onaizan, 1998) was already proposing to use Weighted Transducers to decode the “IBM” models of translation (Brown et al., 1993). (Casacuberta and Vidal, 2004) and (Kumar et al., 2006) also propose to directly model the translation process with Finite State Transducers. (Graehl and Knight, 2004) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are h"
D14-1063,P14-2022,0,0.010908,"e possibilities for vertex merging. On the other hand, for large grammars, the “top-level lattice” will be huge, creating the need to prune vertices during the construction. Furthermore, the composition of the “top-level lattice” with a language model will imply redundant computations (as lower-level lattices will potentially be expanded several times in the top-level lattice). As we do not construct the global lattice explicitly, we do not need to prune vertices (we only prune language model states). And each edge of each lattice rule is crossed only once during our decoding. Very recently, (Heaﬁeld et al., 2014) also considered using the redundancy of translation hypotheses to optimize phrase-based stack decoding. To do so, they group the partial hypotheses in a trie structure. We are not aware of other work proposing “lattice rules” as a native format for expressing translational equivalences. Work like (de Gispert et al., 2010) rely on SCFG rules created along the (Chiang, 2007) approach, while work like (Casacuberta and Vidal, 2004) adopt a pure Finite State Transducer paradigm (thus without explicit SCFG-like rules). 8 sentation of a search space to make this search more eﬃcient. We demonstrate t"
D14-1063,P14-5014,1,0.622254,"situation, it seems like it would be better to delay the full speciﬁcation of the rule until decoding time, when the decoder can have access to the surrounding context of the rule and make a more informed choice. In particular, we can expect features such as language model or governor-dependent features (in the case of tree-to-tree Machine translation) to help remove the ambiguities. We detail some cases for which we encode variations as lattice-rule. Motivation Setting This work was developed mainly in the context of a syntactic-dependency-based tree-to-tree translation system described in (Richardson et al., 2014). Although it is a tree-to-tree system, we simplify the decoding step by “ﬂattening” the target-side tree translation rules into string expansion rules (keeping track of the dependency structure in state features). Thus our setting is actually quite similar to that of many tree-to-string and string-to-string systems. Aiming at simplicity and generality, we will set aside the question of target-side syntactic information and only describe our algorithms in a “tree-to-string” setting. We will also consider a n-gram language model score as our only stateful non-local feature. However, this tree-t"
D14-1063,W11-2123,0,0.0200826,"rest-costs estimates (Heaﬁeld et al., 2012). We deﬁne the concatenation operation on scored language model states to be: (l1 , r1 , s1 ) ⊕ (l2 , r2 , s2 ) = (l3 , r3 , s3 ), where s3 = s1 + s2 + λlm(r1 , l2 ), with lm(r1 , l2 ) being the language model probability of l2 given r1 with rest-costs adjustments. r3 and l3 are the resulting minimized states. Similarly, if an edge e is labeled by a word, we deﬁne the concatenation of a scored state with an edge to be (l1 , r1 , s1 ) ⊕ e = (l2 , r2 , s2 ) where s2 = s1 + score(e) + λlm(word(e)|r1 ). Conveniently for us, the KenLM2 opensource library (Heaﬁeld, 2011) provides functionalities for easily computing such concatenation operations. Lines 5 and 6 are the “cube-pruning-like” part of the algorithm. The function pruneK returns the K best combinations of states in best[v] and decode(RHS(X)), where best means “whose sum of partial score is highest”. It can be implemented eﬃciently through the algorithms proposed in (Huang and Chiang, 2005) or (Chiang, 2007). The L ←max st operation on lines 6 and 10 has the following meaning: L is a list of scored language model state and st is a scored language model state. L ←max st means that, if L already contain"
D14-1063,2010.iwslt-keynotes.2,0,\N,Missing
D14-1063,P11-1008,0,\N,Missing
D15-1276,Y12-1058,1,0.850756,"Missing"
D15-1276,D14-1011,0,0.0376258,"alysis. To solve this problem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 (1) Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Jap"
D15-1276,kawahara-kurohashi-2006-case,1,0.834187,"Missing"
D15-1276,kawahara-etal-2002-construction,1,0.269195,"Missing"
D15-1276,kruengkrai-etal-2006-conditional,0,0.0255249,"nguage models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 (1) Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Japanese morphological analyzers, JUMAN a. 外国 /"
D15-1276,W04-3230,0,0.189553,"nary of JUMAN and an additional dictionary comprising 0.8 million words, both of which have lemma, POS and inflection information. The additional dictionary mainly consists of itemizations in articles and article titles in Japanese Wikipedia. We define the scoring function as follows: scoreB (y) = Φ(y) · w,  (1) where y is a tagged word sequence, Φ(y) is a feature vector for y, and w  is a weight vector. Each element in w  gives a weight to its corresponding feature in Φ(y). We use the unigram and the bigram features composed from word base form, POS and inflection described in Kudo et al. (2004). We also use additional lexical features such as character type, and trigram features used in Zhang and Clark (2008). To learn the weight vector, we adopt exact soft confidence-weighted learning (Wang et al., 2012). To consider out-of-vocabulary (OOV) words that are not found in the dictionary, we automatically generate words at the lookup step by segmenting the input string by character types2 . For training, we regard words that are not found in the dictionary but found in the training corpus as OOV words to learn their weights. 3.3 RNNLM Integrated Model Based on retrained RNNLM, we calcul"
D15-1276,I13-1181,0,0.106274,"s to normalize informally spelled words in microblogs. Therefore, their objective is different from ours. Some studies have used character-based language models for Chinese word segmentation and POS tagging (Zheng et al., 2013; Liu et al., 2014). Although their approaches have no drawbacks of learning incorrect segmentations, they only capture more local information than word-based language models. Word embeddings have been also used for morphological analysis. Neural network based models have been proposed for Chinese word segmentation and POS tagging (Pei et al., 2014) or word segmentation (Mansur et al., 2013). These methods acquire word embeddings from a corpus, and then use them as the input of the neural networks. Our proposed model learns word embeddings via RNNLM, and these embeddings are used for scoring word transitions in morphological analysis. Our usage of word embeddings is different from the previous studies. 3 Proposed Method We propose a new morphological analysis model that considers semantic plausibility of word sequences by using RNNLM. We integrate RNNLM into morphological analysis (Figure 2). We train the RNNLM using both an automatically analyzed corpus and a manually labeled co"
D15-1276,D13-1061,0,0.0242531,"of morphological analysis. They also divided Ngram frequencies into three binned features: highfrequency, middle-frequency and low-frequency. Such coarse features cannot express slight differences in the likelihood of language models. Kaji and Kitsuregawa (2014) used a bigram language model feature for Japanese word segmentation and POS tagging. Their objective of using a language model is to normalize informally spelled words in microblogs. Therefore, their objective is different from ours. Some studies have used character-based language models for Chinese word segmentation and POS tagging (Zheng et al., 2013; Liu et al., 2014). Although their approaches have no drawbacks of learning incorrect segmentations, they only capture more local information than word-based language models. Word embeddings have been also used for morphological analysis. Neural network based models have been proposed for Chinese word segmentation and POS tagging (Pei et al., 2014) or word segmentation (Mansur et al., 2013). These methods acquire word embeddings from a corpus, and then use them as the input of the neural networks. Our proposed model learns word embeddings via RNNLM, and these embeddings are used for scoring w"
D15-1276,P14-1028,0,0.0729368,"ir objective of using a language model is to normalize informally spelled words in microblogs. Therefore, their objective is different from ours. Some studies have used character-based language models for Chinese word segmentation and POS tagging (Zheng et al., 2013; Liu et al., 2014). Although their approaches have no drawbacks of learning incorrect segmentations, they only capture more local information than word-based language models. Word embeddings have been also used for morphological analysis. Neural network based models have been proposed for Chinese word segmentation and POS tagging (Pei et al., 2014) or word segmentation (Mansur et al., 2013). These methods acquire word embeddings from a corpus, and then use them as the input of the neural networks. Our proposed model learns word embeddings via RNNLM, and these embeddings are used for scoring word transitions in morphological analysis. Our usage of word embeddings is different from the previous studies. 3 Proposed Method We propose a new morphological analysis model that considers semantic plausibility of word sequences by using RNNLM. We integrate RNNLM into morphological analysis (Figure 2). We train the RNNLM using both an automaticall"
D15-1276,P14-2042,1,0.826308,"m, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis. 1 (1) Introduction In contrast to space-delimited languages like English, word segmentation is the first and most crucial step for natural language processing (NLP) in unsegmented languages like Japanese, Chinese, and Thai (Kudo et al., 2004; Kaji and Kitsuregawa, 2014; Shen et al., 2014; Kruengkrai et al., 2006). Word segmentation is usually performed jointly with related analysis: POS tagging for Chinese, and POS tagging and lemmatization (analysis of inflected words) for Japanese. Morphological analysis including word segmentation has been widely and actively studied, and for example, Japanese word segmentation accuracy is in the high 90s. However, we often observe that strange outputs of downstream NLP applications such as machine translation and question answering come from incorrect word segmentations. For example, the state-of-the-art and popular Japanese morphological"
D15-1276,P99-1023,0,0.0242946,"nt model makes decoding harder than nonrecurrent neural network language models. However, we use RNNLM because the model outperforms other NNLMs (Mikolov, 2012) and the result suggests that the model is more likely to capture semantic plausibility. Since a sentence rarely contains ambiguous and semantically appropriate word sequences, we think that beam search with enough beam size is able to keep the ambiguous candidates of word sequences. In the case of nonrecurrent NNLMs and the base model, which uses trigram features, we can conduct exact decoding using the second-order Viterbi algorithm (Thede and Harper, 1999). 4 4.1 Experiments words (Cp ) as 5, which is the default value in the implementation of Mikolov et al. (2011). We tuned the parameters of our proposed model and the baseline model (α and Lp ) and the parameters of language models using grid search on the development data. We set α = 0.3, Lp =1.5 for the proposed model (“ Base + RNNLMretrain ”).3 We measured the performance of the baseline models and the proposed model by F-value of word segmentation and F-value of joint evaluation of word segmentation and POS tagging. We calculated F-value for the two corpora (news and web) and the merged co"
D15-1276,I11-1035,0,0.0155168,"incorrect analyses which support such a semantically strange sequence. This would prefer analysis toward semantically appropriate word sequences. When a morphological analyzer utilizes such a generalized and reasonable language model, it can penalize strange segmentations like “外国 (foreign)/人参 (carrot)/ 政権 (regime),” leading to better accuracy. We furthermore retrain RNNLM using an annotated corpus of manually segmented 45k sentences, which further improves morphological analysis. 2 Related Work There have been several studies that have integrated language models into morphological analysis. Wang et al. (2011) improved Chinese word segmentation and POS tagging by using N-gram features learned from an automatically segmented corpus. However, since the auto-segmented corpus inevitably contains segmentation errors, frequent N-grams are not always correct and thus this problem might affect the performance of morphological analysis. They also divided Ngram frequencies into three binned features: highfrequency, middle-frequency and low-frequency. Such coarse features cannot express slight differences in the likelihood of language models. Kaji and Kitsuregawa (2014) used a bigram language model feature fo"
D15-1276,P08-1101,0,0.0244808,"S and inflection information. The additional dictionary mainly consists of itemizations in articles and article titles in Japanese Wikipedia. We define the scoring function as follows: scoreB (y) = Φ(y) · w,  (1) where y is a tagged word sequence, Φ(y) is a feature vector for y, and w  is a weight vector. Each element in w  gives a weight to its corresponding feature in Φ(y). We use the unigram and the bigram features composed from word base form, POS and inflection described in Kudo et al. (2004). We also use additional lexical features such as character type, and trigram features used in Zhang and Clark (2008). To learn the weight vector, we adopt exact soft confidence-weighted learning (Wang et al., 2012). To consider out-of-vocabulary (OOV) words that are not found in the dictionary, we automatically generate words at the lookup step by segmenting the input string by character types2 . For training, we regard words that are not found in the dictionary but found in the training corpus as OOV words to learn their weights. 3.3 RNNLM Integrated Model Based on retrained RNNLM, we calculate an RNNLM score (scoreR (y)) to be integrated into the base model. The RNNLM score is defined as the log probabili"
D15-1276,zhang-etal-2004-interpreting,0,0.0211283,"Missing"
D16-1049,W13-2201,0,0.479219,"i, Sakyo-ku, Kyoto, Japan 2 Japan Science and Technology Agency, Kawaguchi-shi, Saitama, Japan otani.naoki.65v@st.kyoto-u.ac.jp nakazawa@pa.jst.jp Abstract Thus, many previous studies focused on pairwise comparisons instead of absolute evaluations. The same task is given to multiple workers, and their responses are aggregated to obtain a reliable answer. We must, therefore, develop methods that robustly estimate the MT performance based on many pairwise comparisons. Some aggregation methods have been proposed for MT competitions hosted by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013; Hopkins and May, 2013; Sakaguchi et al., 2014), where a ranking of the submitted systems is produced by aggregating many manual judgments of pairwise comparisons of system outputs. However, existing methods do not consider the following important issues. Recent work on machine translation has used crowdsourcing to reduce costs of manual evaluations. However, crowdsourced judgments are often biased and inaccurate. In this paper, we present a statistical model that aggregates many manual pairwise comparisons to robustly measure a machine translation system’s performance. Our method applies gra"
D16-1049,J15-2005,0,0.0181214,"resented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estimates segment specific and judge specific parameters. The Bradley–Terry (BT) model was the result of a seminal study on aggregating pairwise comparisons (Bradley and Terry, 1952; Chen et al., 2013; Dras, 2015). Recently, Chen et al. (2013) explicitly incorporated the quality of judges into the BT model, and applied it to quality control in crowdsourcing. The previously mentioned methods focused on pairwise comparisons of all combination of the MT systems, and thus, the number of comparisons increases rapidly as the number of systems increases. 3 Problem Setting We first describe the problem setting, as shown in Figure 1. Assume that there are a group of systems I indexed by i, a set of segments J indexed by j, and a set of judges K indexed by k. Before a manual evaluation, we fix an arbitrary basel"
D16-1049,goto-etal-2014-crowdsourcing,0,0.0380694,"Missing"
D16-1049,D15-1052,0,0.0127855,"used to produce the WMT13 official rankings (Bojar et al., 2013), considering statistical significance of the results (Koehn, 2012). Hopkins and May (2013) noted that we should consider the relative matchup difficulty, and proposed a statistical aggregation model. Their model assumes that the quality of each system can be represented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estimates segment specific and judge specific parameters. The Bradley–Terry (BT) model was the result of a seminal study on aggregating pairwise comparisons (Bradley and Terry, 1952; Chen et al., 2013; Dras, 2015). Recently, Chen et al. (2013) explicitly incorporated the quality of judges into the BT model, and applied it to quality control in crowdsourcing. The previously mentioned methods focused on pairwise comparisons of all combination of the MT systems, and thus, the number of comparisons"
D16-1049,P13-1139,0,0.382213,"Japan 2 Japan Science and Technology Agency, Kawaguchi-shi, Saitama, Japan otani.naoki.65v@st.kyoto-u.ac.jp nakazawa@pa.jst.jp Abstract Thus, many previous studies focused on pairwise comparisons instead of absolute evaluations. The same task is given to multiple workers, and their responses are aggregated to obtain a reliable answer. We must, therefore, develop methods that robustly estimate the MT performance based on many pairwise comparisons. Some aggregation methods have been proposed for MT competitions hosted by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013; Hopkins and May, 2013; Sakaguchi et al., 2014), where a ranking of the submitted systems is produced by aggregating many manual judgments of pairwise comparisons of system outputs. However, existing methods do not consider the following important issues. Recent work on machine translation has used crowdsourcing to reduce costs of manual evaluations. However, crowdsourced judgments are often biased and inaccurate. In this paper, we present a statistical model that aggregates many manual pairwise comparisons to robustly measure a machine translation system’s performance. Our method applies graded response model from"
D16-1049,2012.iwslt-papers.5,0,0.085833,"nts to produce reliable rankings. 1 We also show that our method accurately replicated the WMT13 official system scores using a few comparisons. However, this is not the main focus of this paper. 512 Figure 1: Illustration of manual pairwise comparison. Each system yields translations. Judges compare them with a baseline translation and report their preferences. Our goal is to aggregate the judgments to determine the performance of each system. Frequency based approaches were used to produce the WMT13 official rankings (Bojar et al., 2013), considering statistical significance of the results (Koehn, 2012). Hopkins and May (2013) noted that we should consider the relative matchup difficulty, and proposed a statistical aggregation model. Their model assumes that the quality of each system can be represented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estima"
D16-1049,P15-2097,0,0.0240859,"official rankings (Bojar et al., 2013), considering statistical significance of the results (Koehn, 2012). Hopkins and May (2013) noted that we should consider the relative matchup difficulty, and proposed a statistical aggregation model. Their model assumes that the quality of each system can be represented by a Gaussian distribution. Sakaguchi et al. (2014) applied TrueSkill (Herbrich et al., 2006) to reduce the number of comparisons to reach the final estimate based on an active learning strategy. The same model was recently used for grammatical error correction (Grundkiewicz et al., 2015; Napoles et al., 2015). These methods acquire the final system-level scores, whereas our model also estimates segment specific and judge specific parameters. The Bradley–Terry (BT) model was the result of a seminal study on aggregating pairwise comparisons (Bradley and Terry, 1952; Chen et al., 2013; Dras, 2015). Recently, Chen et al. (2013) explicitly incorporated the quality of judges into the BT model, and applied it to quality control in crowdsourcing. The previously mentioned methods focused on pairwise comparisons of all combination of the MT systems, and thus, the number of comparisons increases rapidly as t"
D16-1049,W14-3301,0,0.0892015,"Missing"
D16-1049,W15-3001,0,\N,Missing
D16-1247,P05-1022,0,0.0860784,"Missing"
D16-1247,P05-1033,0,0.032199,"are not necessarily adjuncts, but any words or phrases. For example, suppose the Japanese input sentence in Figure 1 has “ 突然 (suddenly)”, but the training corpus provides only a translation rule without the word. In this case we cannot directly use the rule for the translation because it does not know where to insert the translation of the floating word in the output. As another example, there is no context information available for the children of the OOV word in the input sentence, so we need some special process to translate them. Previous work deals with this problem by using glue rules (Chiang, 2005) or limiting the dependency structures to be well-formed (Shen et al., 2008). Richardson et al. (2016) introduces the concept of flexible non-terminals. It provides multiple possible insertion positions for the floating subtree rather than fixed insertion positions. A possible insertion position must satisfy the following conditions: • it must be a child of the aligned word of the parent of the floating subtree 2271 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2271–2277, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Lin"
D16-1247,D14-1063,1,0.845883,"ed”. Also, insertion positions do not violate the projectivity of the target tree. Flexible non-terminals are analogous to the auxiliary tree of the tree adjoining grammars (TAG) (Joshi, 1985), which is successfully adopted in machine translation (DeNeefe and Knight, 2009). The difference is that TAG is defined on the constituency trees rather than the dependency trees. Flexible non-terminals are powerful to handle floating subtrees and it achieve better translation quality. However the computational cost of decoding becomes high even though they are compactly represented in the lattice form (Cromieres and Kurohashi, 2014). In our experiments, using flexible nonterminals causes the decoding to be 3 to 6 times slower than when they are not used. Flexible nonterminals increase the number of translation rules because the insertion positions are selected during the decoding. However, we think it is possible to restrict possible insertion positions or even select only one insertion position by looking at the tree structures on both sides. In this paper, we propose a method to select the appropriate insertion position before decoding. This can not only reduce the decoding time but also improve the translation quality"
D16-1247,D09-1076,0,0.029638,"mber 1-5, 2016. 2016 Association for Computational Linguistics • it must not violate the projectivity of the dependency tree For example, possible insertion positions for the floating word “突然” are shown in gray arrows in Figure 1. Since “突然” is a child of “電話する”, and the translation of “電話する” is “called”, insertion positions must be a child of “called”. Also, insertion positions do not violate the projectivity of the target tree. Flexible non-terminals are analogous to the auxiliary tree of the tree adjoining grammars (TAG) (Joshi, 1985), which is successfully adopted in machine translation (DeNeefe and Knight, 2009). The difference is that TAG is defined on the constituency trees rather than the dependency trees. Flexible non-terminals are powerful to handle floating subtrees and it achieve better translation quality. However the computational cost of decoding becomes high even though they are compactly represented in the lattice form (Cromieres and Kurohashi, 2014). In our experiments, using flexible nonterminals causes the decoding to be 3 to 6 times slower than when they are not used. Flexible nonterminals increase the number of translation rules because the insertion positions are selected during the"
D16-1247,W04-3250,0,0.110387,"SMT 18.45 64.51 - 27.48 68.37 - 27.96 78.90 - 34.65 77.25 Hiero 18.72 65.11 - 30.19 73.47 - 27.71 80.91 - 35.43 81.04 No Flexible 20.28 65.08 1.00 28.77 75.21 1.00 24.85 66.60 1.00 30.51 73.08 1.00 Baseline 21.61 69.82 6.28 30.57 76.13 3.30 28.79 78.11 5.16 34.32 77.82 5.28 Proposed 22.07† 70.49† 2.25 30.50 76.69† 1.27 29.83† 79.73† 2.21 34.71† 79.25† 1.89 Table 4: The results of the translation experiments. † means the Proposed method achieved significantly better score than the Baseline (p &lt; 0.01). 2002) and RIBES (Isozaki et al., 2010) with the significance testing by bootstrap resampling (Koehn, 2004). RIBES is more sensitive to word order than BLEU, so we expect an improvement in RIBES. We also investigated relative decoding time compared to the No Flexible setting. Note that we used the word “decoding” for only exploring the search space, and it does not include constructing the search space (as the table lookup in Phrase-based SMT). Our whole translation process is: 1. translation rule extraction 2. insertion-position selection 3. decoding At the time of the second step, we have all the translation rules applicable to the input sentence. The computation time for each step is 3 ≫ 1 ≫ 2 s"
D16-1247,J94-4001,1,0.321808,"Missing"
D16-1247,P15-2047,0,0.0263548,"Missing"
D16-1247,D15-1279,0,0.0372074,"Missing"
D16-1247,P02-1040,0,0.0995467,"Missing"
D16-1247,P14-5014,1,0.907225,"Missing"
D16-1247,N16-1002,1,0.844414,"Missing"
D16-1247,D11-1046,0,0.0232715,"→ Ja Ja → Zh Zh → Ja 15.7M 5.7M 160K 58K 160K 58K 3.39 3.15 3.72 3.41 89 71 61 79 0.089 0.058 0.105 0.056 97.08 97.72 96.51 97.99 55.00 89.03 68.04 83.16 Table 2: The statistics of the data and results of the insertion position selection experiments. sentences. English sentences are first parsed by nlparser (Charniak and Johnson, 2005) and then converted into word dependency trees using Collins’ head percolation table (Collins, 1999). We used Chinese word segmenter KKN (Shen et al., 2014) and dependency parser SKP (Shen et al., 2012) for Chinese sentences. The supervised word alignment Nile (Riesa et al., 2011) was used. We used a state-of-the-art dependency tree-to-tree decoder (Richardson et al., 2014) with the default settings. The neural network is constructed and trained using the Chainer (Tokui et al., 2015). 3.1 Insertion Position Selection The training, development and test data for the neural network is automatically generated by the procedure explained in Section 2.2. The size of the generated data from the ASPEC and the average number of insertion positions for each floating subtree are shown in Table 2. We trained the model for 100 epochs and used the best model on the development data f"
D16-1247,P08-1066,0,0.0357112,"ppose the Japanese input sentence in Figure 1 has “ 突然 (suddenly)”, but the training corpus provides only a translation rule without the word. In this case we cannot directly use the rule for the translation because it does not know where to insert the translation of the floating word in the output. As another example, there is no context information available for the children of the OOV word in the input sentence, so we need some special process to translate them. Previous work deals with this problem by using glue rules (Chiang, 2005) or limiting the dependency structures to be well-formed (Shen et al., 2008). Richardson et al. (2016) introduces the concept of flexible non-terminals. It provides multiple possible insertion positions for the floating subtree rather than fixed insertion positions. A possible insertion position must satisfy the following conditions: • it must be a child of the aligned word of the parent of the floating subtree 2271 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2271–2277, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics • it must not violate the projectivity of the dependency tree For e"
D16-1247,Y12-1033,1,0.907194,"Missing"
D16-1247,P14-2042,1,0.862962,"Missing"
D16-1247,J03-4003,0,\N,Missing
D16-1247,D10-1092,0,\N,Missing
D18-2010,D15-1276,1,0.72855,"Daisuke Kawahara Arseny Tolmachev Graduate School Graduate School Graduate School of Informatics of Informatics of Informatics Kyoto University Kyoto University Kyoto University Yoshida-honmachi, Sakyo-ku Yoshida-honmachi, Sakyo-ku Yoshida-honmachi, Sakyo-ku Kyoto, 606-8501, Japan Kyoto, 606-8501, Japan Kyoto, 606-8501, Japan dk@i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp arseny@kotonoha.ws Abstract open domain data (like web texts) the accuracy decreases, and it is difficult to improve that accuracy without creating costly annotations by trained experts. The Juman++ Japanese morphological analyzer (Morita et al. 2015, referred to also as V1), which uses a combination of a linear model and a neural network-based language model (RNNLM) to compute a semantic plausibility of a segmentation. Juman++ has achieved state-of-the-art analysis accuracy on Jumandic (the JUMAN dictionary and segmentation standard (Kurohashi and Kawahara, 2012)) based corpora, and drastically reduced the number of intolerable analysis errors. Unfortunately, its execution speed was extremely slow and this has limited the practical usage of Juman++. We have developed a morphological analysis toolkit consisting of three components: a morp"
D18-2010,P11-2093,0,0.0214977,"ctable to compute scores for all paths through the lattice. Instead, we use beam search. Additionally, because the RNNLM is computationally-heavy, we compute the RNN score only for the paths which remain in the beam of the special end-of-string token. The overall design is to cut off improbable analysis results with a simple model and then re-rank by RNNLM. edge, a similar set of tools has never been developed before. Juman++ V2 and the development tools are language and segmentation standard independent and are released under the permissive Apache 2 open source license. 2 Related Work KyTea (Neubig et al., 2011) is a similar tool that can perform morphological analysis for languages with the continuous script. It can also be trained using partial annotation data and output point-wise confidence scores for the analysis result which were used for creating partially annotated data in an active learning scenario. Still, by using a pointwise approach and estimating auxiliary tags (like POS) after computing segmentation, KyTea trades off accuracy for simplicity. Juman++ is faster, has better accuracy, does tag estimation jointly with segmentation, uses an online learning approach and can use longer context"
D18-2010,D08-1112,0,0.183796,"Missing"
D19-1581,baccianella-etal-2010-sentiwordnet,0,0.0467437,"AL, the polarity of a latter event is automatically identified as either positive or negative, according to the seed lexicon (the positive word is colored red and the negative word blue). We propagate the latter event’s polarity to the former event. The same polarity as the latter event is used for the discourse relation C AUSE, and the reversed polarity for C ONCESSION. In CA and CO, the latter event’s polarity is not known. Depending on the discourse relation, we encourage the two events’ polarities to be the same (CA) or reversed (CO). Details are given in Section 3.2. Wilson et al., 2005; Baccianella et al., 2010) and the roles of negation and intensification (Reitan et al., 2015; Wilson et al., 2005; Zhu et al., 2014) are among the most important topics. In contrast, we are more interested in recognizing the sentiment polarity of an event that pertains to commonsense knowledge (e.g., getting money and catching cold). Label propagation from seed instances is a common approach to inducing sentiment polarities. While Takamura et al. (2005) and Turney (2002) worked on word- and phrase-level polarities, Ding and Riloff (2018) dealt with event-level polarities. Takamura et al. (2005) and Turney (2002) linke"
D19-1581,D14-1179,0,0.0106333,"Missing"
D19-1581,N19-1423,0,0.0175883,"nce score of vi , and NACP is the number of the events of the ACP Corpus. To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if p(x) &gt; 0 and negative if p(x) ≤ 0. 4.2 Model Configurations As for Encoder, we compared two types of neural networks: BiGRU and BERT. GRU (Cho et al., 2014) is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states. BERT (Devlin et al., 2019) is a pre-trained multi-layer bidirectional Transformer (Vaswani et al., 2017) encoder. Its output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of Encoder, see Sections A.2. We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: LAL , LAL + LCA + LCO , LACP , and LACP + LAL + LCA + LCO . 4.3 Results and Discussion Table 3 shows accuracy. As the Random baseline suggests, positive and negati"
D19-1581,D12-1034,0,0.0604866,"Missing"
D19-1581,prasad-etal-2008-penn,0,0.22874,"s is challenging because, as the examples above suggest, the polarity of an event is not necessarily predictable from its constituent words. Combined with the unbounded combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from small annotated data. In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure 1, our key idea is that we can exploit discourse relations (Prasad et al., 2008) to efficiently propagate polarity from seed predicates that directly report one’s emotions (e.g., “to be glad” is positive). Suppose that events x1 are x2 are in the discourse relation of C AUSE (i.e., x1 causes x2 ). If the seed lexicon suggests x2 is positive, x1 is also likely to be positive because it triggers the positive emotion. The fact that x2 is known to be negative indicates the negative polarity of x1 . Similarly, if x1 and x2 are in the discourse relation of C ONCESSION (i.e., x2 in spite of x1 ), the reverse of x2 ’s polarity can be propagated to x1 . Even if x2 ’s polarity is n"
D19-1581,W15-2914,0,0.0736966,"Missing"
D19-1581,P06-2059,0,0.246356,"but B”). We shift our scope to event pairs that are more complex than phrase pairs, and consequently exploit discourse connectives as eventlevel counterparts of phrase-level conjunctions. Ding and Riloff (2018) constructed a network of events using word embedding-derived similarities. Compared with this method, our discourse relation-based linking of events is much simpler and more intuitive. Some previous studies made use of document structure to understand the sentiment. Shimizu et al. (2018) proposed a sentiment-specific pretraining strategy using unlabeled dialog data (tweet-reply pairs). Kaji and Kitsuregawa (2006) proposed a method of building a polarity-tagged corpus (ACP Corpus). They automatically gathered sentences that had positive or negative opinions utilizing HTML layout structures in addition to linguistic patterns. Our method depends only on raw texts and thus has wider applicability. 3 Proposed Method 3.1 Polarity Function Our goal is to learn the polarity function p(x), which predicts the sentiment polarity score of an event x. We approximate p(x) by a neural network with the following form: p(x) = tanh(Linear(Encoder(x))). (1) Encoder outputs a vector representation of the event x. Linear"
D19-1581,P16-1162,0,0.0274769,"Missing"
D19-1581,kawahara-kurohashi-2006-case,1,0.50785,"ents have the same polarities. CO (C ONCESSION Pairs) The seed lexicon matches neither the former nor the latter event, and their discourse relation type is C ONCESSION. We assume the two events have the reversed polarities. NCA 1 X = λCA (p(yi1 ) − p(yi2 ))2 NCA +µ NCO 1 X (p(zi1 ) + p(zi2 ))2 NCO 1 NCO i=1 N CO X X (1 − p(u)2 ). i=1 u∈{zi1 ,zi2 } (4) The difference is that the first term makes the scores of the two events distant from each other. 4 Experiments 4.1 Dataset 4.1.1 AL, CA, and CO As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by Kawahara and Kurohashi (2006). To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP1 and in-house postprocessing scripts (Saito et al., 2018). KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives (Prasad et al., 2008) such as “の で” (because) and “の に” (in spite of ) were present. We treated Cause/Reason (原 因・理由) and Condition (条件) in the original tagset (Kawahara et"
D19-1581,P18-1140,0,0.025224,"corpus. Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data. It also improves supervised learning results when labeled data are small. 1 Introduction Affective events (Ding and Riloff, 2018) are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one’s wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems (Shi and Yu, 2018), question-answering systems (Oh et al., 2012), and humor recognition (Liu et al., 2018). In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from −1 (negative) to 1 (positive). Learning affective events is challenging because, as the examples above suggest, the polarity of an event is not necessarily predictable from its constituent words. Combined with the unbounded combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from"
D19-1581,C14-1027,1,0.849662,"hashi (2006). To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP1 and in-house postprocessing scripts (Saito et al., 2018). KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives (Prasad et al., 2008) such as “の で” (because) and “の に” (in spite of ) were present. We treated Cause/Reason (原 因・理由) and Condition (条件) in the original tagset (Kawahara et al., 2014) as C AUSE and Concession (逆 接)2 as C ONCESSION, respectively. Here is an example of event pair extraction. 1 http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?KNP 2 To be precise, this discourse type is semantically broader than Concession and extends to the area of Contrast. 5760 Type of pairs AL (Automatically Labeled Pairs) CA (C AUSE Pairs) CO (C ONCESSION Pairs) # of pairs 1,000,000 5,000,000 5,000,000 Dataset Train Dev Table 1: Statistics of the AL, CA, and CO datasets. Test (1) 重大な失敗を犯したので、仕事をクビ になった。 Because [I] made a serious mistake, [I] got fired. From this sentence, we extracted the ev"
D19-1581,P18-2121,0,0.0231642,"Turney (2002) linked instances using co-occurrence information and/or phrase-level coordinations (e.g., “A and B” and “A but B”). We shift our scope to event pairs that are more complex than phrase pairs, and consequently exploit discourse connectives as eventlevel counterparts of phrase-level conjunctions. Ding and Riloff (2018) constructed a network of events using word embedding-derived similarities. Compared with this method, our discourse relation-based linking of events is much simpler and more intuitive. Some previous studies made use of document structure to understand the sentiment. Shimizu et al. (2018) proposed a sentiment-specific pretraining strategy using unlabeled dialog data (tweet-reply pairs). Kaji and Kitsuregawa (2006) proposed a method of building a polarity-tagged corpus (ACP Corpus). They automatically gathered sentences that had positive or negative opinions utilizing HTML layout structures in addition to linguistic patterns. Our method depends only on raw texts and thus has wider applicability. 3 Proposed Method 3.1 Polarity Function Our goal is to learn the polarity function p(x), which predicts the sentiment polarity score of an event x. We approximate p(x) by a neural netwo"
D19-1581,P05-1017,0,0.401563,"a given event. We trained the models using a Japanese web corpus. Given the minimum amount of supervision, they performed well. In addition, the combination of annotated and unannotated data yielded a gain over a purely supervised baseline when labeled data were small. 2 Related Work Learning affective events is closely related to sentiment analysis. Whereas sentiment analysis usually focuses on the polarity of what are described (e.g., movies), we work on how people are typically affected by events. In sentiment analysis, much attention has been paid to compositionality. Word-level polarity (Takamura et al., 2005; 5758 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5758–5765, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Type AL AL CA CO Former event Latter event 試合に勝つ ([I] win the game) 嬉しい ([I] am glad) +1 ピクニックに⾏く ([I] go to a picnic) +1 +1 Propagate the same polarity 天気が⼼配だ ([I] am worried about the weather) −1 Propagate the reverse polarity 暖房がない (There is no heating) 寒い ([I] am cold) Encourage them to have the same polarity 視⼒が良い ([I] h"
D19-1581,P02-1053,0,0.0476432,"ion, we encourage the two events’ polarities to be the same (CA) or reversed (CO). Details are given in Section 3.2. Wilson et al., 2005; Baccianella et al., 2010) and the roles of negation and intensification (Reitan et al., 2015; Wilson et al., 2005; Zhu et al., 2014) are among the most important topics. In contrast, we are more interested in recognizing the sentiment polarity of an event that pertains to commonsense knowledge (e.g., getting money and catching cold). Label propagation from seed instances is a common approach to inducing sentiment polarities. While Takamura et al. (2005) and Turney (2002) worked on word- and phrase-level polarities, Ding and Riloff (2018) dealt with event-level polarities. Takamura et al. (2005) and Turney (2002) linked instances using co-occurrence information and/or phrase-level coordinations (e.g., “A and B” and “A but B”). We shift our scope to event pairs that are more complex than phrase pairs, and consequently exploit discourse connectives as eventlevel counterparts of phrase-level conjunctions. Ding and Riloff (2018) constructed a network of events using word embedding-derived similarities. Compared with this method, our discourse relation-based linkin"
D19-1581,H05-1044,0,0.305502,": AL, CA, and CO. In AL, the polarity of a latter event is automatically identified as either positive or negative, according to the seed lexicon (the positive word is colored red and the negative word blue). We propagate the latter event’s polarity to the former event. The same polarity as the latter event is used for the discourse relation C AUSE, and the reversed polarity for C ONCESSION. In CA and CO, the latter event’s polarity is not known. Depending on the discourse relation, we encourage the two events’ polarities to be the same (CA) or reversed (CO). Details are given in Section 3.2. Wilson et al., 2005; Baccianella et al., 2010) and the roles of negation and intensification (Reitan et al., 2015; Wilson et al., 2005; Zhu et al., 2014) are among the most important topics. In contrast, we are more interested in recognizing the sentiment polarity of an event that pertains to commonsense knowledge (e.g., getting money and catching cold). Label propagation from seed instances is a common approach to inducing sentiment polarities. While Takamura et al. (2005) and Turney (2002) worked on word- and phrase-level polarities, Ding and Riloff (2018) dealt with event-level polarities. Takamura et al. (20"
D19-1581,P14-1029,0,0.0165217,"d lexicon (the positive word is colored red and the negative word blue). We propagate the latter event’s polarity to the former event. The same polarity as the latter event is used for the discourse relation C AUSE, and the reversed polarity for C ONCESSION. In CA and CO, the latter event’s polarity is not known. Depending on the discourse relation, we encourage the two events’ polarities to be the same (CA) or reversed (CO). Details are given in Section 3.2. Wilson et al., 2005; Baccianella et al., 2010) and the roles of negation and intensification (Reitan et al., 2015; Wilson et al., 2005; Zhu et al., 2014) are among the most important topics. In contrast, we are more interested in recognizing the sentiment polarity of an event that pertains to commonsense knowledge (e.g., getting money and catching cold). Label propagation from seed instances is a common approach to inducing sentiment polarities. While Takamura et al. (2005) and Turney (2002) worked on word- and phrase-level polarities, Ding and Riloff (2018) dealt with event-level polarities. Takamura et al. (2005) and Turney (2002) linked instances using co-occurrence information and/or phrase-level coordinations (e.g., “A and B” and “A but B"
D19-5201,E06-1031,0,0.0437063,"ion web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 http://lotus.kuee.kyoto-u.ac.jp/WAT/ km-en-d"
D19-5201,W16-4601,1,0.763733,"Missing"
D19-5201,W19-6613,1,0.928331,"Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation results for each particJaRuNC Corpus For the Russian↔Japanese task we asked participants to use the JaRuNC corpus5 (Imankulova et al., 2019) which belongs to the news commentary domain. This dataset was manually aligned and cleaned and is trilingual. It can be used to evaluate Russian↔English 6 http://www.phontron.com/kftt/ https://datarepository.wolframcloud.com/ resources/Japanese-English-Subtitle-Corpus 8 https://wit3.fbk.eu/ 9 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 10 https://cms.unov.org/UNCorpus/ 11 https://translate.yandex.ru/corpus?lang=en 12 http://lotus.kuee.kyoto-u.ac.jp/WAT/ News-Commentary/news-commentary-v14.en-ru. filtered.tar.gz 7 4 http://ufal.mff.cuni.cz/~ramasamy/parallel/ html/ 5 https://github.com/aizhanti/JaR"
D19-5201,W17-5701,1,0.779618,"Missing"
D19-5201,D10-1092,0,0.105912,"t of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for each task were calculated using the corresponding reference translations. The automatic evaluation system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provid"
D19-5201,W14-7001,1,0.794275,"t 400 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. WAT receives submissions at any time; i.e., there is no submission deadline of translation results w.r.t automatic evaluation of translation quality. Introduction The Workshop on Asian Translation (WAT) is an open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2018 (Nakazawa et al., 2014, 2015, 2016, 2017, 2018), WAT2019 brings together machine 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/ 3 https://ufal.mff.cuni.cz/hindi-visual-genome/ wat-2019-multimodal-task 1 One paper was withdrawn post acceptance and hence only 6 papers will be in the proceedings. 1 Proceedings of the 6th Workshop on Asian Translation, pages 1–35 Hong Kong, China, November 4, 2019. ©2019 Association for Computational Linguistics Lang JE JC Train 3,008,500 672,315 Dev 1,790 2,090 DevTest 1,784 2,148 Lang zh-ja ko-ja en-ja Test 1,812 2,107 Lang zh-ja ko-ja en-ja Table 1: Statistics for ASPEC Dataset"
D19-5201,W15-5001,1,0.855752,"Missing"
D19-5201,P17-4012,0,0.147541,"MT RBMT Other Other ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ja-hi ✓ ✓ ✓ ✓ EnTam ta-en en-ta ✓ ✓ IITB en-hi hi-ja ✓ ✓ ✓ ✓ TDDC ja-en ✓ hi-en ✓ ✓ NewsCommentary ru-ja ja-ru JIJI ja-en en-ja ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ALT en-my km-en ✓ ✓ Multimodal en-hi ✓ my-en ✓ ✓ en-km ✓ model) for Chinese segmentation. • The Moses toolkit for English and Indonesian tokenization. • Mecab-ko16 for Korean segmentation. • Indic NLP Library17 for Indic language segmentation. • The tools included in the ALT corpus for Myanmar and Khmer segmentation. • subword-nmt18 for all languages. 3.3.1 NMT with Attention We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems of NMT with attention (System ID: NMT). We used the following OpenNMT configuration. • • • • • • • • When we built BPE-codes, we merged source and target sentences and we used 100,000 for s option. We used 10 for vocabulary-threshold when subword-nmt applied BPE. 3.2.2 The default values were used for the other system parameters. For EnTam, News Commentary • The Moses toolkit for English and Russian only for the News Commentary data. 3.3.2 Transformer (Tensor2Tensor) For the News Commentary and English↔Tamil tasks, we used tensor2tensor’s20 im"
D19-5201,W18-1819,0,0.026575,"multilingual model. As for the English↔Tamil task, we train separate baseline models for each translation direction with 32,000 separate sub-word vocabularies. • Mecab19 for Japanese segmentation. • The EnTam corpus is not tokenized by any external toolkits. • Both corpora are further processed by tensor2tensor’s internal pre/postprocessing which includes sub-word segmentation. 3.2.3 For Multi-Modal Task • Hindi Visual Genome comes untokenized and we did not use or recommend any specific external tokenizer. 3.3.3 Transformer (OpenNMT-py) For the Multimodal task, we used the Transformer model (Vaswani et al., 2018) as implemented in OpenNMT-py (Klein et al., 2017) and used the “base” model with default parameters for the multi-modal task baseline. We have generated the vocabulary of 32k subword types jointly for both the source and target languages. The vocabulary is shared between the encoder and decoder. • The standard OpenNMT-py sub-word segmentation was used for pre/postprocessing for the baseline system and each participant used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_freque"
D19-5201,P11-2093,0,0.0158555,"tion system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each submission: • Human Evaluation: whether or not they submit the results for human evaluation; Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model23 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.24 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.25 For Korean segmentation, we used mecab-ko.26 For Myanmar and Khmer segmentations, we used myseg.py27 and kmseg.py28 . For English and Russian tokenizations, we used tokenizer.perl29 in the Moses toolkit. For Hindi and Tamil tokenizations, we used Indic NLP Library.30 The detailed procedu"
D19-5201,P02-1040,0,0.106619,"used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 Baseline NMT Methods We used the following NMT with attention for most of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for e"
D19-5201,W16-2342,0,0.0144454,"on results that participants permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyot"
D19-5201,J82-2005,0,0.731235,"Missing"
D19-5201,D19-5224,0,0.0224104,"Missing"
D19-5201,W15-3049,0,0.0158524,"s permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-da"
D19-5201,W12-5611,1,0.827974,"/ 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k translation quality as well but this is beyond the scope of this years sub-task. Refer to Table 10 for the statistics of the in-domain parallel corpora. In addition we encouraged the participants to use out-of-domain parallel corpora from various sources such as KFTT,6 JESC,7 TED,8 ASPEC,9 UN,10 Yandex11 and Russian↔English news-commentary corpus.12 EnTam Corpus For Tamil↔English translation task we asked the participants to use the publicly available EnTam mixed domain corpus4 (Ramasamy et al., 2012). This corpus contains training, development and test sentences mostly from the news-domain. The other domains are Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation r"
D19-5201,2006.amta-papers.25,0,0.0857626,"hed are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 htt"
D19-5201,2007.mtsummit-papers.63,0,0.678495,",000 2,000 Dev 2,000 2,000 2,000 Table 2: Statistics for JPC • Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Korean↔Japanese language pairs. In the future, we will add more Asian languages such as Vietnamese, Thai and so on. 2 Train 1,000,000 1,000,000 1,000,000 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million Japanese-English scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from Utiyama and Isahara (2007). Each sentence pair is accompanied by a similarity score calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the 2.2 JPC JPO Patent Corpus (JPC) for the patent tasks was constructed by the Japan Patent Office (JPO) in collaboration with NICT. The corpus consists of Chinese-Japanese, KoreanJapanese and English-Japanese patent descriptions whose International Patent Classi2 Disclosure Period 2016-01-01 to 2017-12-31 2018-01-01 to 2018-06-30 Train 1,089,346 (614,817) 314,649 (218,495) Dev Texts Items 1,153 2,"
D19-5814,Y18-1026,1,0.838874,"Missing"
D19-5814,kawahara-etal-2002-construction,1,0.260604,"Missing"
D19-5814,P18-1044,1,0.824968,"rained an MC model using an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the same QA form. 3 Figure 2: An example of RC-QA dataset. focus on the ga case (nominative), the wo case (accusative), and the ni case (dative), which are targeted in the Japanese PAS analysis literature (Shibata et al., 2016; Shibata and Kurohashi, 2018; Kurita et al., 2018; Ouchi et al., 2017). As a source corpus, we use blog articles included in the Driving Experience Corpus (Iwai et al., 2019). We first detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP1 to the corpus. KNP tends to overgenerate such predicates, but most erroneous ones are filtered out by the following crowdsourcing step. We extract the sentence that contains the predicate and preceding three sentences as a document. Then, we automatically generate a question using the following template for nominative. • ［述語］の主語は何か？ (W"
D19-5814,N18-2089,0,0.0255049,"te-ofthe-art neural model. • We construct PAS-QA and RC-QA datasets in the driving domain using crowdsourcing. • We improve Japanese PAS analysis by combining the PAS-QA and RC-QA datasets. The current affiliation is Yahoo Japan Corporation. 98 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 98–104 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2 Related Work &apos;؞RFXPHQW  எは右⾞線に移動した。 , PRYHGWRWKHULJKWODQH  ऋ ংॵॡছ⾒شた؛ 120 VDZ WKHUHDUYLHZPLUURU 2.1 QA Dataset Construction FitzGerald et al. (2018) and Michael et al. (2018) constructed QA-SRL Bank 2.0 and QAMRs using crowdsourcing, respectively. They asked crowdworkers to generate question-answer pairs that represent a PAS. These datasets are similar to our PAS-QA dataset, but different in that we focus on omitted arguments and automatically generate questions (see Section 3.1). Many RC-QA datasets have been constructed in recent years. For example, Rajpurkar et al. (2016) constructed SQuAD 1.1, which contains 100K crowdsourced questions and answer spans in a Wikipedia article. Rajpurkar et al. (2018) updated SQuAD 1.1 to 2.0 by adding unanswerable questions. So"
D19-5814,N19-1423,0,0.131266,"g unanswerable questions. Some RC-QA datasets have been built in a specific domain (Welbl et al., 2017; Suster and Daelemans, 2018; Pampari et al., 2018). ؞4XHVWLRQ n⾒た|の主語は何か︖ :KDWLVWKHVXEMHFWRInVDZ|&quot; ؞$QVZHU எ , Figure 1: An example of PAS-QA dataset. &apos; ؞RFXPHQW எの⾞の前をংॖॡपऽञऋढञ ऋढथःञ؛ $SROLFHRIILFHUVWUDGGOLQJKLVELNH ZDVUXQQLQJLQIURQWRIPFDU ؞4XHVWLRQ は何に乗っていた︖ :KDWZDVWKHSROLFHRIILFHUULGLQJ&quot; ؞$QVZHU ংॖॡ KLVELNH 2.2 Machine Comprehension Models Many MC models based on neural networks have been proposed to solve RC-QA datasets. For example, Devlin et al. (2019) proposed an MC model using a language representation model, BERT, which achieved a high-ranked accuracy on the SQuAD 1.1 leaderboard as of September 30, 2019. As a previous study of transfer learning of MC models to other tasks, Pan et al. (2018) pre-trained an MC model using an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the"
D19-5814,D18-1260,0,0.0259608,"ool of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan {ntakahashi, shibata, dk, kuro}@nlp.ist.i.kyoto-u.ac.jp Norio Takahashi Abstract its answer are constructed, and an MC model is trained using these datasets (e.g., Rajpurkar et al. (2016) and Trischler et al. (2017)). MC has made remarkable progress in the last couple of years, and MC models have even exceeded human accuracy in some datasets (Devlin et al., 2019). However, MC accuracy is not necessarily high for documents that contain anaphoric phenomena and those that need external knowledge or inference (Mihaylov et al., 2018; Yang et al., 2018). In this paper, we propose a Japanese PAS analysis method based on the MC framework for a specific domain. In particular, we focus on a challenging task of finding an antecedent of a zero pronoun within PAS analysis. We construct a widecoverage QA dataset for PAS analysis (PAS-QA) in the domain and feed it to an MC model to perform PAS analysis. We also construct a QA dataset for reading comprehension (RC-QA) in the same domain and jointly use the two datasets in the MC model to improve PAS analysis. We consider the domain of blogs on driving because of the following two r"
D19-5814,P18-1191,0,0.0239211,"show its superiority to a state-ofthe-art neural model. • We construct PAS-QA and RC-QA datasets in the driving domain using crowdsourcing. • We improve Japanese PAS analysis by combining the PAS-QA and RC-QA datasets. The current affiliation is Yahoo Japan Corporation. 98 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 98–104 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2 Related Work &apos;؞RFXPHQW  எは右⾞線に移動した。 , PRYHGWRWKHULJKWODQH  ऋ ংॵॡছ⾒شた؛ 120 VDZ WKHUHDUYLHZPLUURU 2.1 QA Dataset Construction FitzGerald et al. (2018) and Michael et al. (2018) constructed QA-SRL Bank 2.0 and QAMRs using crowdsourcing, respectively. They asked crowdworkers to generate question-answer pairs that represent a PAS. These datasets are similar to our PAS-QA dataset, but different in that we focus on omitted arguments and automatically generate questions (see Section 3.1). Many RC-QA datasets have been constructed in recent years. For example, Rajpurkar et al. (2016) constructed SQuAD 1.1, which contains 100K crowdsourced questions and answer spans in a Wikipedia article. Rajpurkar et al. (2018) updated SQuAD 1.1 to 2.0 by adding"
D19-5814,P17-1146,0,0.0248456,"ing an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the same QA form. 3 Figure 2: An example of RC-QA dataset. focus on the ga case (nominative), the wo case (accusative), and the ni case (dative), which are targeted in the Japanese PAS analysis literature (Shibata et al., 2016; Shibata and Kurohashi, 2018; Kurita et al., 2018; Ouchi et al., 2017). As a source corpus, we use blog articles included in the Driving Experience Corpus (Iwai et al., 2019). We first detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP1 to the corpus. KNP tends to overgenerate such predicates, but most erroneous ones are filtered out by the following crowdsourcing step. We extract the sentence that contains the predicate and preceding three sentences as a document. Then, we automatically generate a question using the following template for nominative. • ［述語］の主語は何か？ (What is the subject of"
D19-5814,P18-1192,0,0.0624874,"Missing"
D19-5814,D18-1191,0,0.0335015,"Missing"
D19-5814,P18-2124,0,0.0315232,"LUURU 2.1 QA Dataset Construction FitzGerald et al. (2018) and Michael et al. (2018) constructed QA-SRL Bank 2.0 and QAMRs using crowdsourcing, respectively. They asked crowdworkers to generate question-answer pairs that represent a PAS. These datasets are similar to our PAS-QA dataset, but different in that we focus on omitted arguments and automatically generate questions (see Section 3.1). Many RC-QA datasets have been constructed in recent years. For example, Rajpurkar et al. (2016) constructed SQuAD 1.1, which contains 100K crowdsourced questions and answer spans in a Wikipedia article. Rajpurkar et al. (2018) updated SQuAD 1.1 to 2.0 by adding unanswerable questions. Some RC-QA datasets have been built in a specific domain (Welbl et al., 2017; Suster and Daelemans, 2018; Pampari et al., 2018). ؞4XHVWLRQ n⾒た|の主語は何か︖ :KDWLVWKHVXEMHFWRInVDZ|&quot; ؞$QVZHU எ , Figure 1: An example of PAS-QA dataset. &apos; ؞RFXPHQW எの⾞の前をংॖॡपऽञऋढञ ऋढथःञ؛ $SROLFHRIILFHUVWUDGGOLQJKLVELNH ZDVUXQQLQJLQIURQWRIPFDU ؞4XHVWLRQ は何に乗っていた︖ :KDWZDVWKHSROLFHRIILFHUULGLQJ&quot; ؞$QVZHU ংॖॡ KLVELNH 2.2 Machine Comprehension Models Many MC models based on neural networks have been proposed to s"
D19-5814,P16-1162,0,0.0116349,"owledge, and suggest that it is better to construct both PAS-QA and RCQA datasets to develop a PAS analyzer for a new 5.1 Experimental Settings We adopt BERT (Devlin et al., 2019) as an MC model. We split the triplets in the PAS-QA dataset as shown in Table 4. All sentences in these datasets are preprocessed using the Japanese morphological analyzer, JUMAN++3 . We trained a Japanese pre-trained BERT model using Japanese Wikipedia, which consists of approximately 18 million sentences. The input sentences were segmented into words by JUMAN++, and words were broken into subwords by applying BPE (Sennrich et al., 2016). The parameters of BERT are the same as English BERTBASE . The number of epochs for the pre-training was 30. The state-of-the-art baseline PAS analyzer, NN-PAS, was trained using the existing PAS dataset, KWDLC4 (Kyoto University Web Document Leads Corpus), as described in Shibata and Kurohashi (2018). We also trained an NN-PAS model using the PAS-QA dataset in addition to KWDLC (hereafter, NN-PAS′ ). For this training, the PAS-QA dataset was converted to the same format as KWDLC, where questions are deleted, and only answers are used. The PAS-QA test data is used to compare the baseline meth"
D19-5814,P16-1117,1,0.848269,"f MC models to other tasks, Pan et al. (2018) pre-trained an MC model using an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the same QA form. 3 Figure 2: An example of RC-QA dataset. focus on the ga case (nominative), the wo case (accusative), and the ni case (dative), which are targeted in the Japanese PAS analysis literature (Shibata et al., 2016; Shibata and Kurohashi, 2018; Kurita et al., 2018; Ouchi et al., 2017). As a source corpus, we use blog articles included in the Driving Experience Corpus (Iwai et al., 2019). We first detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP1 to the corpus. KNP tends to overgenerate such predicates, but most erroneous ones are filtered out by the following crowdsourcing step. We extract the sentence that contains the predicate and preceding three sentences as a document. Then, we automatically generate a question using the f"
D19-5814,P18-1054,1,0.905025,"asks, Pan et al. (2018) pre-trained an MC model using an RC-QA dataset and transfered the pre-trained knowledge to sequence-tosequence models. They used SQuAD 1.1 as the RC-QA dataset and experimented on translation and summarization. While they used different models for pre-training and fine-tuning, we use the same MC model by constructing PAS-QA and RC-QA datasets in the same QA form. 3 Figure 2: An example of RC-QA dataset. focus on the ga case (nominative), the wo case (accusative), and the ni case (dative), which are targeted in the Japanese PAS analysis literature (Shibata et al., 2016; Shibata and Kurohashi, 2018; Kurita et al., 2018; Ouchi et al., 2017). As a source corpus, we use blog articles included in the Driving Experience Corpus (Iwai et al., 2019). We first detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP1 to the corpus. KNP tends to overgenerate such predicates, but most erroneous ones are filtered out by the following crowdsourcing step. We extract the sentence that contains the predicate and preceding three sentences as a document. Then, we automatically generate a question using the following template for nominat"
D19-5814,D18-1548,0,0.0565549,"Missing"
D19-5814,N18-1140,0,0.0508556,"Missing"
D19-5814,W17-2623,0,0.0517339,"Missing"
D19-5814,W17-4413,0,0.0307523,"ng, respectively. They asked crowdworkers to generate question-answer pairs that represent a PAS. These datasets are similar to our PAS-QA dataset, but different in that we focus on omitted arguments and automatically generate questions (see Section 3.1). Many RC-QA datasets have been constructed in recent years. For example, Rajpurkar et al. (2016) constructed SQuAD 1.1, which contains 100K crowdsourced questions and answer spans in a Wikipedia article. Rajpurkar et al. (2018) updated SQuAD 1.1 to 2.0 by adding unanswerable questions. Some RC-QA datasets have been built in a specific domain (Welbl et al., 2017; Suster and Daelemans, 2018; Pampari et al., 2018). ؞4XHVWLRQ n⾒た|の主語は何か︖ :KDWLVWKHVXEMHFWRInVDZ|&quot; ؞$QVZHU எ , Figure 1: An example of PAS-QA dataset. &apos; ؞RFXPHQW எの⾞の前をংॖॡपऽञऋढञ ऋढथःञ؛ $SROLFHRIILFHUVWUDGGOLQJKLVELNH ZDVUXQQLQJLQIURQWRIPFDU ؞4XHVWLRQ は何に乗っていた︖ :KDWZDVWKHSROLFHRIILFHUULGLQJ&quot; ؞$QVZHU ংॖॡ KLVELNH 2.2 Machine Comprehension Models Many MC models based on neural networks have been proposed to solve RC-QA datasets. For example, Devlin et al. (2019) proposed an MC model using a language representation model, BERT, which achieved"
D19-5814,D18-1259,0,0.0330184,"to University Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan {ntakahashi, shibata, dk, kuro}@nlp.ist.i.kyoto-u.ac.jp Norio Takahashi Abstract its answer are constructed, and an MC model is trained using these datasets (e.g., Rajpurkar et al. (2016) and Trischler et al. (2017)). MC has made remarkable progress in the last couple of years, and MC models have even exceeded human accuracy in some datasets (Devlin et al., 2019). However, MC accuracy is not necessarily high for documents that contain anaphoric phenomena and those that need external knowledge or inference (Mihaylov et al., 2018; Yang et al., 2018). In this paper, we propose a Japanese PAS analysis method based on the MC framework for a specific domain. In particular, we focus on a challenging task of finding an antecedent of a zero pronoun within PAS analysis. We construct a widecoverage QA dataset for PAS analysis (PAS-QA) in the domain and feed it to an MC model to perform PAS analysis. We also construct a QA dataset for reading comprehension (RC-QA) in the same domain and jointly use the two datasets in the MC model to improve PAS analysis. We consider the domain of blogs on driving because of the following two reasons. Firstly, we"
D19-6014,N16-1014,0,0.0289546,"distribution. Additionally, we extend the CVAE with a reconstruction mechanism (Tu et al., 2017) to alleviate the model’s tendency to concentrate on a small number of highly typical events. Diversity-Promoting Objective Functions In dialogue response generation, seq2seq is known to suffer from the generic response problem: The model often ends up blindly generating uninformative responses such as “I don’t know”. A popular approach to this problem is to rerank the candidate outputs, which are usually produced by beam search, according to the mutual information with the conversational context (Li et al., 2016). 4.1 Objective Function We introduce a probabilistic latent variable z and assume that y depends on both x and z. The conditional log likelihood of y given x is written as: Z log p(y|x) = log pθ (y, z|x)dz (1) z Z = log pθ (y|z, x)pθ (z|x)dz. (2) We notice that the reconstruction mechanism (Tu et al., 2017) serves the same purpose in a more straightforward manner, albeit stemming from a different motivation. The reconstruction mechanism forces the model to reconstruct the input from the hidden states of the decoder. Although it was originally proposed for machine translation to prevent over-t"
D19-6014,K16-1002,0,0.218923,"ormation, VAEs adopt probabilistic generation: a VAE encodes y into the probability distribution of z, instead of a point in a low-dimensional vector space. It then reconstructs the input y from z drawn from 114 the posterior distribution. z is assumed to have a prior distribution, for which a multivariate Gaussian distribution is often used. As straightforward extensions of VAEs, conditional VAEs (CVAEs) let probabilistic distributions be conditioned on a common observed variable x (Kingma et al., 2014; Sohn et al., 2015). In our case, x is a current event while y is a next event to predict. Bowman et al. (2016) applied VAEs to text generation. They constructed VAEs using RNNs as its components and found that VAEs with an RNNbased decoder failed to encode meaningful information to z. To alleviate this problem, they proposed simple but effective heuristics: KL cost annealing and word dropout. We also employ these techniques. Figure 1: The neural network architecture of our event prediction model. ⊕ denotes vector concatenation. If a VAE-based text generation model is conditioned on text, it can be seen as a CVAE-based seq2seq model (Zhao et al., 2017; Serban et al., 2017; Zhang et al., 2016). Since a"
D19-6014,P11-2057,0,0.0118185,"the model from concentrating on highly typical events. To facilitate fair and systematic evaluation of the diversityaware models, we also extend existing evaluation datasets by tying each current event to multiple next events. Experiments show that the CVAE-based models drastically outperform deterministic models in terms of precision and that the reconstruction mechanism improves the recall of CVAE-based models without sacrificing precision.1 1 Introduction Typical event sequences are an important class of commonsense knowledge that enables deep text understanding (Schank and Abelson, 1975; LoBue and Yates, 2011). Following previous work (Nguyen et al., 2017), we work on the task of generating a next event conditioned on a current event, which we call event prediction. For example, we want a computer to recognize that the event “board bus” is typically followed by another event “pay bus fare” and to generate the latter word sequence given the former. 1 The source code and the new test sets are publicly available at https://github.com/hkiyomaru/ diversity-aware-event-prediction. 113 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 113–122 c Hongkong, Chin"
D19-6014,P08-1090,0,0.572552,"fare” and to generate the latter word sequence given the former. 1 The source code and the new test sets are publicly available at https://github.com/hkiyomaru/ diversity-aware-event-prediction. 113 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 113–122 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics pre-defined set of candidates for a missing event. A popular strategy is to rank candidates by similarity with the remaining part of the event sequence. Similarity measures include pointwise mutual information (Chambers and Jurafsky, 2008), conditional bigram probability (Jans et al., 2012), and cosine similarities based on latent semantic indexing and word embeddings (Granroth-Wilding and Clark, 2016). For its reliance on pre-defined candidates, however, the classification approach is constrained by its limited flexibility. In the generation task, a model is to directly generate a missing event, usually in the form of a word sequence (Pichotta and Mooney, 2016; Hu et al., 2017; Nguyen et al., 2017), although one previous study adopted a predicate-argument structure-based event representation (Weber et al., 2018). Nguyen et al."
D19-6014,N16-1012,0,0.0220961,"ies memorized event sequences extracted from a corpus and inevitably suffered from low generalization capability and a scalability problem. A promising approach to modeling wide-coverage knowledge is to generalize events by representing them in a continuous space (Granroth-Wilding and Clark, 2016; Nguyen et al., 2017; Hu et al., 2017). Nguyen et al. (2017) generate a next event using the sequenceto-sequence (seq2seq) framework, which was first proposed for machine translation (Bahdanau et al., 2014) and subsequently applied to various NLP tasks including text summarization (Rush et al., 2015; Chopra et al., 2016) and dialog generation (Sordoni et al., 2015; Serban et al., 2016). One limitation of the simple seq2seq models, which are deterministic in nature, is their inability to take into account an important characteristic of events: What can happen after a current event is usually diverse. For the example of “board bus” mentioned above, “get off bus” as well as “pay bus fare” is a valid next event. The inherent diversity makes it difficult to train deterministic models, and during testing, they can hardly generate multiple next events that are both valid and diverse. To address this problem, we firs"
D19-6014,I17-2007,0,0.120154,"ational Autoencoder with Reconstruction Hirokazu Kiyomaru Kazumasa Omura Yugo Murawaki Daisuke Kawahara Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan {kiyomaru, omura, murawaki, dk, kuro}@nlp.ist.i.kyoto-u.ac.jp Abstract Early studies memorized event sequences extracted from a corpus and inevitably suffered from low generalization capability and a scalability problem. A promising approach to modeling wide-coverage knowledge is to generalize events by representing them in a continuous space (Granroth-Wilding and Clark, 2016; Nguyen et al., 2017; Hu et al., 2017). Nguyen et al. (2017) generate a next event using the sequenceto-sequence (seq2seq) framework, which was first proposed for machine translation (Bahdanau et al., 2014) and subsequently applied to various NLP tasks including text summarization (Rush et al., 2015; Chopra et al., 2016) and dialog generation (Sordoni et al., 2015; Serban et al., 2016). One limitation of the simple seq2seq models, which are deterministic in nature, is their inability to take into account an important characteristic of events: What can happen after a current event is usually diverse. For the examp"
D19-6014,P02-1040,0,0.104688,"). Second, the event pairs were sometimes hard to interpret because they were extracted from adjacent descriptions out of context. The results suggest that further studies in this area should use Wikihow with caution. l1 : Strange expression. l2 : No relation. l3 : A and B are related, but one does not happen after the other. l4 : A happens after B. l5 : B happens after A. 3 https://code.google.com/archive/p/ word2vec/ 117 6 Experiments 6.1 where BLEU is the sentence-level variant of a well-known metric that measures the geometric mean of modified n-gram precision with the penalty of brevity (Papineni et al., 2002). The final score was averaged over the entire test set. We refer to the precision and recall as P@N and R@N, respectively. F@N is the harmonic mean of P@N and R@N. We report the scores with N = 5 and 10, in accordance with the average number of next events in our new test sets. For comparison, we also followed the experimental procedure of Nguyen et al. (2017), where event prediction models deterministically output a single next event using greedy decoding. For CVAEs, we did this by setting z at the mean of the predicted Gaussian prior. The outputs were evaluated by BLEU. We refer to the crit"
D19-6014,E14-1024,0,0.158375,"ntly outperformed the simple seq2seq models in terms of precision (i.e., validity) without hurting recall (i.e., diversity) while forcing the simple seq2seq models to generate diverse outputs yielded low precision. The reconstruction mechanism consistently improved recall of the CVAE-based models while keeping or even increasing precision. We also confirmed that the original test sets failed to detect the clear differences between the models. 2 2.1 Related Work Event Prediction There is a growing body of work on learning typical event sequences (Chambers and Jurafsky, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016; Hu et al., 2017; Nguyen et al., 2017). While early studies explicitly store event sequences in a symbolic manner, a recent approach to this task is to train neural network models that implicitly represent event sequence knowledge as continuous model parameters. In both cases, models are usually evaluated by how well they restore a missing portion of an event sequence. We collectively refer to this task as event prediction. Event prediction can be categorized into two tasks: classification and generation. In the classification task,"
D19-6014,P16-1027,0,0.0766384,"event. A popular strategy is to rank candidates by similarity with the remaining part of the event sequence. Similarity measures include pointwise mutual information (Chambers and Jurafsky, 2008), conditional bigram probability (Jans et al., 2012), and cosine similarities based on latent semantic indexing and word embeddings (Granroth-Wilding and Clark, 2016). For its reliance on pre-defined candidates, however, the classification approach is constrained by its limited flexibility. In the generation task, a model is to directly generate a missing event, usually in the form of a word sequence (Pichotta and Mooney, 2016; Hu et al., 2017; Nguyen et al., 2017), although one previous study adopted a predicate-argument structure-based event representation (Weber et al., 2018). Nguyen et al. (2017) worked on the task of generating a next event given a single event, which we follow in this paper. They adopted the seq2seq framework (Sutskever et al., 2014) and investigated how recurrent neural network (RNN) variants, the number of RNN layers, and the presence or absence of an attention mechanism (Bahdanau et al., 2014) affected the performance. Hu et al. (2017) gave a sequence of events to the model to generate the"
D19-6014,E12-1034,0,0.280491,"Missing"
D19-6014,D15-1044,0,0.0159523,"Abstract Early studies memorized event sequences extracted from a corpus and inevitably suffered from low generalization capability and a scalability problem. A promising approach to modeling wide-coverage knowledge is to generalize events by representing them in a continuous space (Granroth-Wilding and Clark, 2016; Nguyen et al., 2017; Hu et al., 2017). Nguyen et al. (2017) generate a next event using the sequenceto-sequence (seq2seq) framework, which was first proposed for machine translation (Bahdanau et al., 2014) and subsequently applied to various NLP tasks including text summarization (Rush et al., 2015; Chopra et al., 2016) and dialog generation (Sordoni et al., 2015; Serban et al., 2016). One limitation of the simple seq2seq models, which are deterministic in nature, is their inability to take into account an important characteristic of events: What can happen after a current event is usually diverse. For the example of “board bus” mentioned above, “get off bus” as well as “pay bus fare” is a valid next event. The inherent diversity makes it difficult to train deterministic models, and during testing, they can hardly generate multiple next events that are both valid and diverse. To address"
D19-6014,D16-1050,0,0.0243173,"predict. Bowman et al. (2016) applied VAEs to text generation. They constructed VAEs using RNNs as its components and found that VAEs with an RNNbased decoder failed to encode meaningful information to z. To alleviate this problem, they proposed simple but effective heuristics: KL cost annealing and word dropout. We also employ these techniques. Figure 1: The neural network architecture of our event prediction model. ⊕ denotes vector concatenation. If a VAE-based text generation model is conditioned on text, it can be seen as a CVAE-based seq2seq model (Zhao et al., 2017; Serban et al., 2017; Zhang et al., 2016). Since a CVAE learns probabilistic generation, it is suitable for tasks where the output is not uniquely determined according to the input. One of the representative applications of CVAE-based text generation is dialogue response generation, or the task of generating possible replies to a human utterance (Zhao et al., 2017; Serban et al., 2017). Applying CVAEs to next event prediction is a natural choice because the task is also characterized by output diversity. 2.3 3 Problem Setting Given a current event x, we are to generate a variety of events, each of which, y, often happens after x. x a"
D19-6014,P17-1061,0,0.116486,"urrent event while y is a next event to predict. Bowman et al. (2016) applied VAEs to text generation. They constructed VAEs using RNNs as its components and found that VAEs with an RNNbased decoder failed to encode meaningful information to z. To alleviate this problem, they proposed simple but effective heuristics: KL cost annealing and word dropout. We also employ these techniques. Figure 1: The neural network architecture of our event prediction model. ⊕ denotes vector concatenation. If a VAE-based text generation model is conditioned on text, it can be seen as a CVAE-based seq2seq model (Zhao et al., 2017; Serban et al., 2017; Zhang et al., 2016). Since a CVAE learns probabilistic generation, it is suitable for tasks where the output is not uniquely determined according to the input. One of the representative applications of CVAE-based text generation is dialogue response generation, or the task of generating possible replies to a human utterance (Zhao et al., 2017; Serban et al., 2017). Applying CVAEs to next event prediction is a natural choice because the task is also characterized by output diversity. 2.3 3 Problem Setting Given a current event x, we are to generate a variety of events, ea"
D19-6014,N15-1020,0,0.0316724,"Missing"
D19-6014,D18-1413,0,0.0199242,"tion (Chambers and Jurafsky, 2008), conditional bigram probability (Jans et al., 2012), and cosine similarities based on latent semantic indexing and word embeddings (Granroth-Wilding and Clark, 2016). For its reliance on pre-defined candidates, however, the classification approach is constrained by its limited flexibility. In the generation task, a model is to directly generate a missing event, usually in the form of a word sequence (Pichotta and Mooney, 2016; Hu et al., 2017; Nguyen et al., 2017), although one previous study adopted a predicate-argument structure-based event representation (Weber et al., 2018). Nguyen et al. (2017) worked on the task of generating a next event given a single event, which we follow in this paper. They adopted the seq2seq framework (Sutskever et al., 2014) and investigated how recurrent neural network (RNN) variants, the number of RNN layers, and the presence or absence of an attention mechanism (Bahdanau et al., 2014) affected the performance. Hu et al. (2017) gave a sequence of events to the model to generate the next one. Accordingly, they worked on hierarchically encoding the given event sequence using word-level and event-level RNNs. All of these models are dete"
E09-1020,P03-1010,0,0.0353717,"P. 3.3 Experimental Results We evaluated the monolink algorithm with two languages pairs: French-English and JapaneseEnglish. 169 For the English-French Pair, we used 200,000 sentence pairs extracted from the Hansard corpus (Germann, 2001). Evaluation was done with the scripts and gold standard provided during the workshop HLT-NAACL 20031 (Mihalcea and Pedersen, 2003). Null links are not considered for the evaluation. For the English-Japanese evaluation, we used 100,000 sentence pairs extracted from a corpus of English/Japanese news. We used 1000 sentence pairs extracted from pre-aligned data(Utiyama and Isahara, 2003) as a gold standard. We segmented all the Japanese data with the automatic segmenter Juman (Kurohashi and Nagao, 1994). There is a caveat to this evaluation, though. The reason is that the segmentation and alignment scheme used in our gold standard is not very fine-grained: mostly, big chunks of the Japanese sentence covering several words are aligned to big chunks of the English sentence. For the evaluation, we had to consider that when two chunks are aligned, there is a link between every pair of words belonging to each chunk. A consequence is that our gold standard will contain a lot more l"
E09-1020,J93-2003,0,0.0261638,"etwork. We want to model the joint probability of the Weather(W) being sunny or rainy, the Sprinkle(S) being on or off, and the Lawn(L) being wet or dry. Figure 1 show the dependencies of Introduction and Related Work Automatic word alignment of parallel corpora is an important step for data-oriented Machine translation (whether Statistical or Example-Based) as well as for automatic lexicon acquisition. Many algorithms have been proposed in the last twenty years to tackle this problem. One of the most successfull alignment procedure so far seems to be the so-called “IBM model 4” described in (Brown et al., 1993). It involves a very complex distortion model (here and in subsequent usages “distortion” will be a generic term for the reordering of the words occurring in the translation process) with many parameters that make it very complex to train. By contrast, the first alignment model we are going to propose is fairly simple. But this simplicity will allow us to try and experiment different ideas for making a better use of the sentence structures in the alignment process. This model (and even more so its subsequents variations), although simple, do not have a computationally efficient procedure for a"
E09-1020,P84-1004,0,0.0673359,"Missing"
E09-1020,P01-1067,0,0.258075,"Missing"
E09-1020,J03-4003,0,0.0120588,"can see this as another case of “conservation of structure”). Practically, using Psets of adjacent positions create a distortion model where permutation of words are not penalized, but gaps are penalized. 4.2 AER 0.197 0.166 0.135 0.26 0.281 0.205 0.162 0.121 P 0.881 0.882 0.887 0.819 0.667 0.754 0.806 0.849 R 0.731 0.813 0.851 0.665 0.805 0.863 0.890 0.927 Table 1: Results for English/French Experimental Results The evaluation setting is the same as in the previous section. We created syntactic trees for every sentences. For English,we used the Dan Bikel implementation of the Collins parser (Collins, 2003). For French, the SYGMART parser (Chauch´e, 1984) and for Japanese, the KNP parser (Kurohashi and Nagao, 1994). The line SDM:Parsing (SDM standing for “Structure-based Distortion Monolink”) shows the results obtained by using P-sets from the trees produced by these parsers. The line SDM:Adjacency shows results obtained by using adjacent positions P-sets ,as described at the end of the previous section (therefore, SDM:Adjacency do not use any parser). Several interesting observations can be made from the results. First, our structure-based distortion model did improve the results of the monolin"
E09-1020,P06-3003,1,0.849329,"ncepts. We propose a simple generative model to describe the generation of a sentence pair (or rather, its underlying bag of concepts): only one-to-one alignments can be seen as a limitation compared to others models that can often produce at least one-to-many alignments, but on the good side, this allow the monolink model to be nicely symmetric. Additionally, as already argued in (Melamed, 2000), there are ways to determine the boundaries of some multi-words phrases (Melamed, 2002), allowing to treat several words as a single token. Alternatively, a procedure similar to the one described in (Cromieres, 2006), where substrings instead of single words are aligned (thus considering every segmentation possible) could be used. With the monolink model, we want to do two things: first, we want to find out good values for the distributions Psize and Pconcept . Then we want to be able to find the most likely alignment a given the sentence pair (e, f ). We will consider Psize to be a uniform distribution over the integers up to a sufficiently big value (since it is not possible to have a uniform distribution over an infinite discrete set). We will not need to determine the exact value of Psize . The assump"
E09-1020,P03-1011,0,0.0250021,"side each monolink subgraph can still be computed with the efficient procedure described in section 3.2. We do not have the space to describe in details the messages sent between P-set V-Nodes and Word V-Nodes, but they are easily computed from P the principles of the BP Palgorithm. Let NE = |ps| and N = F ps∈se ps∈sf |ps|. Then the complexity of one BP iteration will be O(NG · ND + |e |· |f |). An interesting aspect of this model is that it is flexible towards enforcing the respect of the structures by the alignment, since not every P-set need to have an equivalent in the opposite sentence. (Gildea, 2003) has shown that too strict an enforcement can easily degrade alignment quality and that good balance was difficult to find. Another interesting aspect is the fact that we have a somehow “parameterless” distortion model. There is only one real-valued parameter to control the distortion: α. And even this parameter is actually pre-set before any training on real data. The distortion is therefore totally controlled by the two sets of P-sets on each side of the sentence. Finally, although we introduced the P-sets as being generated from a syntactic tree, they do not need to. In particular, we found"
E09-1020,J94-4001,1,0.377153,"nglish. 169 For the English-French Pair, we used 200,000 sentence pairs extracted from the Hansard corpus (Germann, 2001). Evaluation was done with the scripts and gold standard provided during the workshop HLT-NAACL 20031 (Mihalcea and Pedersen, 2003). Null links are not considered for the evaluation. For the English-Japanese evaluation, we used 100,000 sentence pairs extracted from a corpus of English/Japanese news. We used 1000 sentence pairs extracted from pre-aligned data(Utiyama and Isahara, 2003) as a gold standard. We segmented all the Japanese data with the automatic segmenter Juman (Kurohashi and Nagao, 1994). There is a caveat to this evaluation, though. The reason is that the segmentation and alignment scheme used in our gold standard is not very fine-grained: mostly, big chunks of the Japanese sentence covering several words are aligned to big chunks of the English sentence. For the evaluation, we had to consider that when two chunks are aligned, there is a link between every pair of words belonging to each chunk. A consequence is that our gold standard will contain a lot more links than it should, some of them not relevants. This means that the recall will be largely underestimated and the pre"
E09-1020,J00-2004,0,0.262181,"ribution is a forest. If it is not the case, the BP algorithm is not even guaranteed to converge. It has been shown, however, that the BP algorithm do converge in many practical cases, and that the results it produces are often surprisingly good approximations (see, for example, (Murphy et al., 1999) or (Weiss and Freeman, 2001) ). 3 The monolink model We are now going to present a simple alignment model that will serve both to illustrate the efficiency of the BP algorithm and as basis for further improvement. As previously mentioned, this model is mostly identical to one already proposed in (Melamed, 2000). The training and decoding procedures we propose are however different. (Yedidia et al., 2003) gives a very good presentation of the sum-product BP algorithm, as well as some theoretical justifications for its success. We will just give an outline of the algorithm. The BP algorithm is a message-passing algorithm. Messages are sent during several iterations until convergence. At each iteration, each V-Node sends to its neighboring F-Nodes a message representing an estimation of its own marginal values. The 3.1 Description Following the usual convention, we will designate the two sides of a sen"
E09-1020,W03-0301,0,0.0810179,"his will not be the e(t) bi e(t) (j)/((1 − bi f (t) (j)) · mji ) 2- Compute the beliefs bei (j):bi (j)e(t+1) = f (t+1) · mji ϕtp.e i (j) then normalize the bi (j)e(t+1) so that P3- Ande(t+1) = 1. j bi (j) A similar algorithm can be found for the maxproduct BP. 3.3 Experimental Results We evaluated the monolink algorithm with two languages pairs: French-English and JapaneseEnglish. 169 For the English-French Pair, we used 200,000 sentence pairs extracted from the Hansard corpus (Germann, 2001). Evaluation was done with the scripts and gold standard provided during the workshop HLT-NAACL 20031 (Mihalcea and Pedersen, 2003). Null links are not considered for the evaluation. For the English-Japanese evaluation, we used 100,000 sentence pairs extracted from a corpus of English/Japanese news. We used 1000 sentence pairs extracted from pre-aligned data(Utiyama and Isahara, 2003) as a gold standard. We segmented all the Japanese data with the automatic segmenter Juman (Kurohashi and Nagao, 1994). There is a caveat to this evaluation, though. The reason is that the segmentation and alignment scheme used in our gold standard is not very fine-grained: mostly, big chunks of the Japanese sentence covering several words ar"
E09-1020,W99-0604,0,0.0752204,"guess. In contrast, our BP/EM training do not need to compute correlation scores and start the training with uniform parameters. We only evaluated the CLA on the French/English pair. The first iteration of CLA did improve alignment quality, but subsequent ones decreased it. The reported score for CLA is therefore the one obtained during the best iteration. The BP/EM training demonstrate a clear superiority over the CLA here, since it produce almost 7 points of AER improvement over CLA. In order to have a comparison with a wellknown and state-of-the-art system, we also used the GIZA++ program (Och and Ney, 1999) to align the same data. We tried alignments in both direction and provide the results for the direction that gave the best results. The settings used were the ones used by the training scripts of the Moses system2 , which we assumed to be fairly optimal. We tried alignment with the default Moses settings (5 iterations of model 1, 5 of Hmm, 3 of model 3, 3 of model 4) and also tried with increased number of iterations for each model (up to 10 per model). We are aware that the score we obtained for model 4 in English-French is slightly worse than what is usually reported for a similar size of t"
E17-1054,I13-1124,1,0.935349,"sed on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were several studies using additional knowledge to improve syntactic and semantic tasks. McClosky et al. (2006) used an addi569 gold standard data and the automatic parses, correct dependencies were collected as positive examples and incorrect dependencies were collected as negative examples. Then selecting high-quality dependencies was regarded as a binary classification problem. To conduct such binary classification, they employed a set of basic features from Yu et al. (2008). In addition to these basic features, Jin et al. (2013) considered context features that are thought to affect parsing performance. Since the input for high-quality dependency selection method is a dependency tree, tree features are used to identify dependency quality. Also, some dependency parsers output the score of each dependency (i.e., edge confidence value) during the parsing process. They used the real value of the score as an additional feature. We first apply this approach to select high-quality dependencies from automatic parses. tactic and semantic knowledge, i.e., case frames. Word embeddings can also be incorporated into our method bu"
E17-1054,jin-etal-2014-framework,1,0.856177,"n for Japanese, there are also ambiguous case makers that can represent multiple cases. The most common one, for example, is “wa” case in Japanese. This case marker always functions as a topic marker. The argument in “wa” case is normally emphasized as the topic of the sentence. It can be equal to either “ga” case or “wo” case, and sometimes “ni” case. To avoid such ambiguous cases, one can simply discard all the instances in “wa” case to make case frames more precise. For languages that lack such case markers (e.g., English and Chinese), case frames are composed of automatic syntactic roles (Jin et al., 2014). Such syntactic roles include “subject”, “direct object”, “indirect object” and “prepositional phrases”. Such surface cases have limitations on case representation especially for Chinese. Take the following sentences as examples. (1) 苹果 (apples) / 我 (I) / 吃了 (eaten) / 很多 (a lot). (2) 我 (I) / 苹果 (apples) / 吃了 (eaten) / 很多 (a lot). (3) 我 (I) / 吃了 (eaten) / 很多(a lot) / 苹果 (apples). (4) 我 (I) / 吃了 (eaten) / 很多(a lot). 2 https://code.google.com/archive/p/ word2vec/source/default/source 571 feature description Freq the co-occurrence frequency of a predicate-argument pair without considering the syn"
E17-1054,W09-1206,0,0.0917277,"Missing"
E17-1054,W15-3119,1,0.282594,"of the human-annotated data is used to apply SRL using the base model. From these results, we acquire training data for semantic role selection by collecting each semantic role. We then judge the correctness of each semantic role according to the gold standard annotations. All correct semantic roles are used as positive examples and the incorrect ones are used as negative examples for semantic role selection. Judging whether an automatic semantic role is reliable can be regarded as a binary classification problem. We use SVMs to solve this problem. We use the feature set for SRL described in Jin et al. (2015) as basic features. It contains predicate features that are extracted from the target predicate; argument features which are extracted from each candidate argument. We also use surface case frames, which have a positive effect on SRL, as additional knowledge. 6.3 Using high-quality deep case frames for SRL Syntactic information such as dependencies is essential for SRL. In Section 4, we used surface 573 feature description SRFreq the co-occurrence frequency of a predicate-argument pair without considering the semantic role of the argument SRPmi the PMI value for each predicate-argument pair wi"
E17-1054,W10-0907,0,0.0729771,"Missing"
E17-1054,N06-1023,1,0.860357,"apers, pages 568–577, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics tional unlabeled corpus to reduce data sparsity. In syntactic level of NLP, rich knowledge, such as predicate-argument structures and case frames, is strong backups for various kinds of tasks. Case frames, which clarify relations between a predicate and its arguments, can support tasks ranging from fundamental analysis, such as dependency parsing and word similarity calculation, to multilingual applications, such as machine translation. Japanese case frames have been successfully compiled (Kawahara and Kurohashi, 2006), where each case slot is represented as its case marker in Japanese such as ‘ga’, ‘wo’, and ‘ni’. For the case frames of other languages such as English and Chinese, because there are no such case markers that can help clarify syntactic structures, instead of using case markers like in Japanese, syntactic surface cases (i.e., subject, object, prepositional phrase, etc.) are used for argument representation (Jin et al., 2014). Case frames can be automatically acquired using a different method such as Chinese Restaurant Process (CRP) (Kawahara et al., 2014) for different languages. In our work,"
E17-1054,E14-1007,1,0.844973,"bject, object, etc.) For each predicate, all the PASs are clustered into different case frames to reflect different semantic usages. We show an example of case frames for the verb ‘谢’ in Table 1, which has multiple meanings. ‘谢(1)’ is the case frame used to represent the sense of ‘withering of flower’. Similarly, the sense of ‘谢’ which means ‘to thank’ is represented by case frame ‘谢(2)’. ‘谢(3)’ is the case frame for the sense of ‘curtain call’. In other words, case frames are knowledge that solves word sense disambiguation (WSD) by clustering the PASs. We applied the CRP method described by (Kawahara et al., 2014) for clustering the high-quality PASs to compile high-quality case frames. 4 Applying high-quality surface case frames to SRL 4.1 High-quality dependency selection Dependency parsing has been widely employed for knowledge acquisition related to predicateargument structures. The dependency parsing performance determines the quality of acquired knowledge, regardless of target languages. Reducing dependency parsing errors and selecting highquality dependencies are of primary importance. Jin et al. (2013) used a single set of dependency labeled corpus (a treebank), a part of which was used to trai"
E17-1054,D09-1003,0,0.0700095,"Missing"
E17-1054,P08-1068,0,0.098446,"Missing"
E17-1054,Y01-1001,0,0.222646,"Missing"
E17-1054,C10-1081,0,0.0714146,"Missing"
E17-1054,E09-1026,0,0.0722539,"Missing"
E17-1054,P06-1043,0,0.0308194,"of framework has the ability to use SRL information in syntactic parsing for improvement, but needs a much larger search space for decoding. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were several studies using additional knowledge to improve syntactic and semantic tasks. McClosky et al. (2006) used an addi569 gold standard data and the automatic parses, correct dependencies were collected as positive examples and incorrect dependencies were collected as negative examples. Then selecting high-quality dependencies was regarded as a binary classification problem. To conduct such binary classification, they employed a set of basic features from Yu et al. (2008). In addition to these basic features, Jin et al. (2013) considered context features that are thought to affect parsing performance. Since the input for high-quality dependency selection method is a dependency tree, tree features"
E17-1054,W09-1203,0,0.042817,"Missing"
E17-1054,W08-1810,0,0.0664419,"Missing"
E17-1054,W09-1217,0,0.0302214,"presentation can be learned from unlabeled data (Bengio et al., 2003). Deschacht and Moens (2009) used word similarity learned from unlabeled data as additional features for SRL. Word embeddings have also been used in several NLP tasks including SRL (Collobert et al., 2011). Instead of using word-level lexical knowledge, our work uses synRelated work The CoNLL-2009 shared task (Hajiˇc et al., 2009) features a substantial number of studies on SRL that used Propbank as one of the resources. The participating systems can be categorized into two types: joint learning of syntactic parsing and SRL (Tang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but needs a much larger search space for decoding. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2"
E17-1054,D14-1041,0,0.0174453,"ang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but needs a much larger search space for decoding. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were several studies using additional knowledge to improve syntactic and semantic tasks. McClosky et al. (2006) used an addi569 gold standard data and the automatic parses, correct dependencies were collected as positive examples and incorrect dependencies were collected as negative examples. Then selecting high-quality dependencies was regarded as a binary classification problem. To conduct such binary classification, they employed a set of basic features from Yu et al. (2008). In addition to these basic features, Jin et al. (2013) considered context features that are thought to affec"
E17-1054,P09-2019,0,0.0822879,"Missing"
E17-1054,W09-1208,0,0.0338018,"evel lexical knowledge, our work uses synRelated work The CoNLL-2009 shared task (Hajiˇc et al., 2009) features a substantial number of studies on SRL that used Propbank as one of the resources. The participating systems can be categorized into two types: joint learning of syntactic parsing and SRL (Tang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but needs a much larger search space for decoding. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were several studies using additional knowledge to improve syntactic and semantic tasks. McClosky et al. (2006) used an addi569 gold standard data and the automatic parses, correct dependencies were collected as positive examples and incorrect dependencies were collected as negative exa"
H01-1043,A97-1052,0,0.0695649,"Missing"
H01-1043,J94-4001,1,0.832916,"tsumu ‘accumulate experience’. Since these couples multiply to millions of combinations, it is diﬃcult to make a wide-coverage case frame dictionary from a small corpus like an analyzed corpus. We, however, use a raw corpus, so that this problem can be addressed. The clustering process is to merge examples which does not have diﬀerent usages but belong to diﬀerent case frames because of diﬀerent closest case components. 2. VARIOUS METHODS FOR CASE FRAME CONSTRUCTION We employ the following procedure of case frame construction from raw corpus (Figure 1): 1. A large raw corpus is parsed by KNP [5], and reliable modiﬁer-head relations are extracted from the parse results. We call these modiﬁer-head relations examples. 2. The extracted examples are distinguished by the verb and its closest case component. We call these data example patterns. 3. The example patterns are clustered based on a thesaurus. We call the output of this process example case frames, which is the ﬁnal result of the system. We call words which compose case components case examples, and a group of case examples case example group. In Figure 1, nimotsu ‘baggage’, busshi 1 In English, several unsupervised methods have b"
H01-1043,W97-0123,0,0.0794265,". However, information quantity of this is equivalent to that of the co-occurrences (II of Figure 1), so verb sense ambiguity becomes a problem as well. We distinguish examples by the verb and its closest case component. Our method can address the two problems above: verb sense ambiguity and sparse data. On the other hand, semantic markers can be used as case components instead of case examples. These we call semantic case frames (IV of Figure 1). Constructing semantic case frames by hand leads to the problem mentioned in Section 1. Utsuro et al. constructed semantic case frames from a corpus [8]. There are three main diﬀerences to our approach: they use an annotated corpus, depend deeply on a thesaurus, and did not resolve verb sense ambiguity. 3. COLLECTING EXAMPLES This section explains how to collect examples shown in Figure 1. In order to improve the quality of collected examples, reliable modiﬁer-head relations are extracted from the parsed corpus. 3.1 Conditions of case components When examples are collected, case markers, case examples, and case components must satisfy the following conditions. Conditions of case markers Case components which have the following case markers (C"
H01-1043,P93-1032,0,\N,Missing
I05-1017,A00-2018,0,0.0162257,". The prepositional phrase in (1a) “with a fork” modiﬁes the verb “ate”, because “with a fork” describes how the salad is eaten. The prepositional phrase in (1b) “with croutons” modiﬁes the noun “the salad”, because “with croutons” describes the salad. To disambiguate such PP-attachment ambiguity, some kind of world knowledge is required. However, it is currently diﬃcult to give such world knowledge to computers, and this situation makes PP-attachment disambiguation diﬃcult. Recent state-of-the-art parsers perform with the practical accuracy, but seem to suﬀer from the PP-attachment ambiguity [2, 3]. For NLP tasks including PP-attachment disambiguation, corpus-based approaches have been the dominant paradigm in recent years. They can be divided into two classes: supervised and unsupervised. Supervised methods automatically learn rules from tagged data, and achieve good performance for many NLP tasks, especially when lexical information, such as words, is given. Such methods, however, cannot avoid the sparse data problem. This is because tagged data are not suﬃcient enough to discriminate a large variety of lexical information. To deal with this problem, many smoothing techniques have bee"
I05-1017,P98-2177,0,0.624239,"Missing"
I05-1017,P00-1014,0,0.724913,"Missing"
I05-1017,H94-1048,0,0.544766,"Missing"
I05-1017,C94-2195,0,0.0573136,"Missing"
I05-1017,W95-0103,0,0.163443,"Missing"
I05-1017,W97-0109,0,0.133315,"Missing"
I05-1017,W97-1016,0,0.0565704,"Missing"
I05-1017,W99-0606,0,0.215158,"Missing"
I05-1017,J93-1005,0,0.397374,"Missing"
I05-1017,C02-1004,0,0.203169,"Missing"
I05-1017,J95-4004,0,0.278935,"Missing"
I05-1017,N01-1025,0,0.0702764,"Missing"
I05-1017,J93-2004,0,\N,Missing
I05-1017,J03-4003,0,\N,Missing
I05-1017,C98-2172,0,\N,Missing
I05-1060,C04-1176,0,0.0660833,". There are several related work which can contribute the modification and extension of our methods. When using a Japanese-English dictionary, if we understand the translation is transliteration, we can utilize the information more effectively, handling inflections. In this sense, work by Knight and Graehl can be incorporated into our method [2]. 692 T. Nakazawa, D. Kawahara, and S. Kurohashi In order to handle spelling variation problems, there have been many methods proposed [3], and we can utilize recently proposed robust treatment of Japanese Katakana spelling variation by Masuyama et al. [5]. Our second method using Japanese-English dictionary and the English corpus can be considered as a translation acquisition method. It is interesting to compare these results with other web-based methods, such as Utsuro et al. [8, 1]. There have been many studies that extract compound nouns. Nakagawa et al. focused on the tendency that most of technical terms are compound nouns, and proposed a method of extracting technical terms by using frequency and variety of its neiboring words [10, 7]. In view of information retrieval, Yamada et al. aimed at imporving information retrieval using matching"
I05-1060,C04-1149,0,0.0204093,"e effectively, handling inflections. In this sense, work by Knight and Graehl can be incorporated into our method [2]. 692 T. Nakazawa, D. Kawahara, and S. Kurohashi In order to handle spelling variation problems, there have been many methods proposed [3], and we can utilize recently proposed robust treatment of Japanese Katakana spelling variation by Masuyama et al. [5]. Our second method using Japanese-English dictionary and the English corpus can be considered as a translation acquisition method. It is interesting to compare these results with other web-based methods, such as Utsuro et al. [8, 1]. There have been many studies that extract compound nouns. Nakagawa et al. focused on the tendency that most of technical terms are compound nouns, and proposed a method of extracting technical terms by using frequency and variety of its neiboring words [10, 7]. In view of information retrieval, Yamada et al. aimed at imporving information retrieval using matching of compounds [9]. It is similar to our study in handling compounds. 8 Conclusion This paper proposed an automatic segmentation method of Japanese Katakana compounds, which makes it possible to construct precise and concise Katakana"
I05-1060,J98-4003,0,\N,Missing
I05-1066,W99-0204,0,0.369105,"tification of word chain and similarity between two sentences. Our experiments demonstrates generated slides are far easier to read in comparison with original texts. 1 Introduction A presentation with slides is so effective to pass information to people in many situations, such as an academic conference or business. Although some softwares, such as PowerPoint and Keynote, help us with making presentation slides, it is still cumbersome to make them from scratch. Some researchers have developed a method of (semi-)automatically making presentation slides from a technical paper or a news article [1, 2]. However, input texts of their systems were supposed to be documents whose structure is annotated: in [1], TEX source and in [2], semantically annotated documents by GDA tag. In this paper, we propose a method of automatically generating summary slides from a raw text. An example of a text is shown in Figure 1 and an example slide that is generated from the text is shown in Figure 2 (the translated slide is shown in Figure 3). In a slide, topic/non-topic parts that are extracted from the original text are itemized and each item is indented based on the discourse structure of the text. In part"
I05-1066,J94-4001,1,0.67148,"mple of a text is shown in Figure 1 and an example slide that is generated from the text is shown in Figure 2 (the translated slide is shown in Figure 3). In a slide, topic/non-topic parts that are extracted from the original text are itemized and each item is indented based on the discourse structure of the text. In particular, a big contrast/list structure in the text is an important clue for producing an easy-to-read slide. The outline of our procedure is as follows: 1. Inputsentences are processedbyJapanese morphologicalanalyzer,JUMAN[3], and are parsed by Japanese syntactic analyzer, KNP [4]. 2. Each sentence is segmented into clauses and the discourse structure of the text is analyzed. 3. Topic/non-topic parts are extracted from the text. 4. Summary slides are generated by displaying the topic/non-topic parts based on the discourse structure. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 754–766, 2005. c Springer-Verlag Berlin Heidelberg 2005  Automatic Slide Generation Based on Discourse Structure Analysis 755 Fig. 1. An example of a text Fig. 2. An example of a slide Our method not only helps us with making presentation slides but also creates a full-automatic presentati"
I05-1066,P02-1028,1,0.846446,"t only helps us with making presentation slides but also creates a full-automatic presentation. That is to say, input texts are spoken via textto-speech engine while presenting automatically generated summary slides. We call this system “text-to-presentation”, as illustrated in Figure 4. For the input of text-to-speech engine, written texts are not appropriate, because unnatural speech might be produced due to difficult words or long compound nouns, which are unsuitable for speech synthesis. Therefore, written texts are automatically converted into spoken texts based on paraphrasing technique [5, 6] and then are inputted into speech synthesis. The rest of this paper is organized as follows. Section 2 introduces how to analyze discourse structure. Section 3 explains how to extract topic/non-topic parts and Section 4 introduces how to generate a slide. Next, in Section 5, we describe our implemented system and report the evaluation. Finally, in Section 6, we discuss the related work, and in Section 7, we conclude this paper. 756 T. Shibata and S. Kurohashi Railway Recovery (1) – Interruption of the three train services, JR Kobe-line, Hankyu Express Kobeline and Hanshin Electric Railway • 4"
I05-1066,N04-1031,1,0.788941,"t only helps us with making presentation slides but also creates a full-automatic presentation. That is to say, input texts are spoken via textto-speech engine while presenting automatically generated summary slides. We call this system “text-to-presentation”, as illustrated in Figure 4. For the input of text-to-speech engine, written texts are not appropriate, because unnatural speech might be produced due to difficult words or long compound nouns, which are unsuitable for speech synthesis. Therefore, written texts are automatically converted into spoken texts based on paraphrasing technique [5, 6] and then are inputted into speech synthesis. The rest of this paper is organized as follows. Section 2 introduces how to analyze discourse structure. Section 3 explains how to extract topic/non-topic parts and Section 4 introduces how to generate a slide. Next, in Section 5, we describe our implemented system and report the evaluation. Finally, in Section 6, we discuss the related work, and in Section 7, we conclude this paper. 756 T. Shibata and S. Kurohashi Railway Recovery (1) – Interruption of the three train services, JR Kobe-line, Hankyu Express Kobeline and Hanshin Electric Railway • 4"
I05-1066,C94-2183,1,0.754633,"C o n tr a st C2-2 C1-1 C2-1 C3-1 S t a r t T o p ic -c h a in in g S1 S2 S3 T o p ic c h -da ion im n gin a n t S t a r t : 10 C4-1 S4 LCoi s nt t : r a10s t : 0 C5-1 List C5-2 T. . o p i c -c h a i n i n g : 0 L i s t : 0 E. . l a b o r a t i o n : 15 CoT o pn it c r -ca s th a : i 0n i n g : 10 .E . l a b o r a t i o n : 25 .. S5 Fig. 5. The model of discourse structure P re d i c ate L i st C C A Cl au se Contrast B S e nte nc e A C Fig. 6. Segmenting a sentence into discourse units In the subsequent subsections, we describe how to analyze the discourse structure. This method is based on [7]. We start with the starting node and build a discourse tree in an incremental fashion. An input sentence is first parsed and segmented into clauses, and relations between clauses in a sentence are analyzed. Then, the sentence is connected to the most related preceding sentence and the coherence relation between the sentences is determined. 2.2 Segmenting a Sentence into Discourse Units First, an input sentence is parsed by KNP and segmented into discourse units, which are the basic units of not only discourse structure but also extraction of topic/non-topic parts described in Section 3. Japan"
I05-1066,C94-1056,0,0.125226,"Missing"
I05-1066,C02-1084,1,0.887646,"Missing"
I05-1066,J00-3005,0,0.149372,"Missing"
I05-1066,W01-1605,0,0.0732597,"Missing"
I05-1066,A00-1043,0,0.073137,"Missing"
I05-1085,W02-1022,0,0.025457,"structural paraphrases. So far, we implemented a system that paraphrases compound nouns into nominal phrases. It is our future work to build a system that generates other kinds of structural paraphrases. 7 Related Work Lexical choice has been widely discussed in both paraphrasing and natural language generation (NLG). However, to the best of our knowledge, no researches address topic adaptation. Previous approaches are topic-independent or speciﬁc to only certain topic. Lexical choice has been one of the central issues in NLG. However, the main focus is mapping from concepts to words, (e.g., [1]). In NLG, a work by Edmonds and Hirst is related to our research [5]. They proposed a computational model that represents the connotation of words. Some paraphrasing researches focus on lexical choice. Murata and Isahara addressed paraphrasing written language to spoken language. They used only probability in spoken language corpus [12]. Kaji et al. also discussed paraphrasing written language to spoken language, and they used the probabilities in written and spoken language corpora [11]. On the other hand, Inkpen et al. examined paraphrasing positive and negative text [9]. They used the comp"
I05-1085,N03-2003,0,0.0178103,"Web Pages Topic classification (Section 4) Arts category Written Spoken Recreation category Written Spoken Science category Written Spoken ......... Fig. 1. Written and spoken language corpora construction 3 Style Classiﬁcation In order to construct written and spoken language corpora classiﬁed into topic categories, ﬁrst of all, Web pages are classiﬁed into written and spoken language pages (Figure 1). Note that what is called spoken language here is not real utterance but chat like texts. Although it is not real spoken language, it works as a good substitute, as some researchers pointed out [2,11]. We follow a method proposed by Kaji et al (2004). Their method classiﬁes Web pages into three types: (1) written language page, (2) spoken language page, and (3) ambiguous page. Then, Web pages classiﬁed into type (1) or (2) are used. Ambiguous pages are discarded because classiﬁcation precision decreases if such pages are used. This Section summarizes their method. See [11] for detail. Note that for this method the target language is Japanese, and its procedure is dependent on Japanese characteristics. 3.1 Basic Idea Web pages are classiﬁed based on interpersonal expressions, which imply an"
I05-1085,J96-2004,0,0.0239183,"Missing"
I05-1085,J93-1003,0,0.0242405,"Missing"
I05-1085,P99-1022,0,0.033979,"Missing"
I05-1085,P02-1028,1,0.841937,"strated here are translation of the original Japanese words. Lexical Choice via Topic Adaptation for Paraphrasing Written Language 991 These examples can be explained in the following way. Consider topical words in Business. When the probability is estimated using the whole corpora, it is inﬂuenced by the topic but Business, where topical words in Business are often inappropriate for spoken language. Therefore, we think that Baseline is biased to classify topical words as inappropriate. Besides the lexical choice method addressed in this paper, we proposed lexical paraphrase generation method [10]. Our future direction is to apply these methods to written language texts and evaluate the output of text-to-speech. So far, the methods were tested on a small set of reports. Although the main focus of this paper is lexical paraphrases, we think that it is also important to deal with structural paraphrases. So far, we implemented a system that paraphrases compound nouns into nominal phrases. It is our future work to build a system that generates other kinds of structural paraphrases. 7 Related Work Lexical choice has been widely discussed in both paraphrasing and natural language generation"
I05-1085,N04-1031,1,0.912542,"Web Pages Topic classification (Section 4) Arts category Written Spoken Recreation category Written Spoken Science category Written Spoken ......... Fig. 1. Written and spoken language corpora construction 3 Style Classiﬁcation In order to construct written and spoken language corpora classiﬁed into topic categories, ﬁrst of all, Web pages are classiﬁed into written and spoken language pages (Figure 1). Note that what is called spoken language here is not real utterance but chat like texts. Although it is not real spoken language, it works as a good substitute, as some researchers pointed out [2,11]. We follow a method proposed by Kaji et al (2004). Their method classiﬁes Web pages into three types: (1) written language page, (2) spoken language page, and (3) ambiguous page. Then, Web pages classiﬁed into type (1) or (2) are used. Ambiguous pages are discarded because classiﬁcation precision decreases if such pages are used. This Section summarizes their method. See [11] for detail. Note that for this method the target language is Japanese, and its procedure is dependent on Japanese characteristics. 3.1 Basic Idea Web pages are classiﬁed based on interpersonal expressions, which imply an"
I05-1085,murata-isahara-2002-automatic,0,0.0134943,"est of our knowledge, no researches address topic adaptation. Previous approaches are topic-independent or speciﬁc to only certain topic. Lexical choice has been one of the central issues in NLG. However, the main focus is mapping from concepts to words, (e.g., [1]). In NLG, a work by Edmonds and Hirst is related to our research [5]. They proposed a computational model that represents the connotation of words. Some paraphrasing researches focus on lexical choice. Murata and Isahara addressed paraphrasing written language to spoken language. They used only probability in spoken language corpus [12]. Kaji et al. also discussed paraphrasing written language to spoken language, and they used the probabilities in written and spoken language corpora [11]. On the other hand, Inkpen et al. examined paraphrasing positive and negative text [9]. They used the computational model proposed by Edmonds and Hirst [5]. The proposed method is based on the probability, which can be considered as a simple language model. In language model works, many researchers have discussed topic adaptation in order to precisely estimate the probability of topical words [6,7,13]. Our work can be regarded as one applica"
I05-1085,J02-2001,0,\N,Missing
I08-1025,kawahara-kurohashi-2006-case,1,0.542577,"Missing"
I08-1025,J94-4001,1,0.406601,"example of a web standard format data is shown in Figure 2. Extracted sentences are enclosed by &lt;RawString> tags, and the analyzed results of the sentences are enclosed by &lt;Annotation> tags. Sentences in a web page and their analyzed results can be obtained by looking at these tags in the standard format data corresponding to the page. 2.2 Construction of Web Standard Format Data Collection We have crawled 218 million web pages over three months, May - July in 2007, by using the ShimCrawler,2 and then converted these pages into web standard format data with results of a Japanese parser, KNP (Kurohashi and Nagao, 1994), through our conversion tools. Note that this web page collec2 http://www.logos.t.u-tokyo.ac.jp/crawler/ &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?> &lt;StandardFormat Url=&quot;http://www.kantei.go.jp/jp/koizumiprofile/1_sin nen.html&quot; OriginalEncoding=&quot;Shift_JIS&quot; Time=&quot;2006-08 -14 19:48:51&quot;>&lt;Text Type=&quot;default&quot;> &lt;S Id=&quot;1&quot; Length=&quot;70&quot; Offset=&quot;525&quot;> &lt;RawString>小泉総理の好きな格言のひとつに「無信不立 (信無く ば立たず)」があります。&lt;/RawString> &lt;Annotation Scheme=&quot;KNP&quot;> &lt;![CDATA[* 1D &lt;文 頭>&lt;サ 変>&lt;人 名>&lt;助 詞>&lt;連 体 修 飾>&lt;体 言>&lt;係:ノ格>&lt;区切:0-4>&lt;RID:1056> 小泉 こいずみ 小泉 名詞 6 人名 5 * 0 * 0 NIL &lt;文頭>&lt;漢字>&lt; かな漢字>&lt;名詞相当語>&lt;自立>&lt;タグ単位始>&lt;文節始>&lt;固有キー> ... ま す ま す ま す 接 尾 辞"
I08-2080,C02-1025,0,0.014193,"among conventional systems, introduced the bunsetsu1 feature in order to consider wider context, but considers only adjacent bunsetsus. *Research Fellow of the Japan Society for the Promotion of Science (JSPS) 1 Bunsetsu is a commonly used linguistic unit in Japanese, consisting of one or more adjacent content words and zero or more following functional words. Sadao Kurohashi Graduate School of Infomatics, Kyoto University kuro@i.kyoto-u.ac.jp On the other hand, as for English or Chinese, various NER systems have explored global information and reported their effectiveness. In (Malouf, 2002; Chieu and Ng, 2002), information about features assigned to other instances of the same token is utilized. (Ji and Grishman, 2005) uses the information obtained from coreference analysis for NER. (Mohit and Hwa, 2005) uses syntactic features in building a semi-supervised NE tagger. In this paper, we present a Japanese NER system that uses global information obtained from several structural analyses. To be more speciﬁc, our system is based on SVM, recognizes NEs after syntactic, case and coreference analyses and uses information obtained from these analyses and the NER results for the previous context, integrally"
I08-2080,C02-1122,1,0.872771,"eme. Second, JUMAN adds categories to some morphemes, which can be utilized for NER. In JUMAN, about thirty categories are deﬁned and tagged to about one ﬁfth of morphemes. For example, “ringo (apple),” “inu (dog)” and “byoin (hospital)” are tagged as “FOOD,” “ANIMAL” and “FACILITY,” respectively. 4.3 Syntactic, Case and Coreference Analyses syntactic analysis Syntactic analysis is performed by using the Japanese parser KNP (Kurohashi and Nagao, 1994). KNP employs some heuristic rules to determine the head of a modiﬁer. case analysis Case analysis is performed by using the system proposed in (Kawahara and Kurohashi, 2002). This system uses Japanese case frames that are automatically constructed from a large corpus. To utilize case analysis for NER, we constructed case frames that include NE labels in advance. We explain details in Section 4.4.2. The case analysis is applied to each predicate in an input sentence. For details see (Kawahara and Kurohashi, 2002). coreference analysis Coreference analysis is performed by using the coreference analyzer proposed by (Sasano et al., 2007). As will be mentioned in 4.4 Feature Extraction 4.4.1 Basic Features As basic features for chunking, our NER system uses the morphe"
I08-2080,J94-4001,1,0.541158,"he outputs of JUMAN and ChaSen. Although both analyses are reasonable, JUMAN divided “Gaimusho” and “shin-Bei” into two morphemes, while ChaSen left them as a single morpheme. Second, JUMAN adds categories to some morphemes, which can be utilized for NER. In JUMAN, about thirty categories are deﬁned and tagged to about one ﬁfth of morphemes. For example, “ringo (apple),” “inu (dog)” and “byoin (hospital)” are tagged as “FOOD,” “ANIMAL” and “FACILITY,” respectively. 4.3 Syntactic, Case and Coreference Analyses syntactic analysis Syntactic analysis is performed by using the Japanese parser KNP (Kurohashi and Nagao, 1994). KNP employs some heuristic rules to determine the head of a modiﬁer. case analysis Case analysis is performed by using the system proposed in (Kawahara and Kurohashi, 2002). This system uses Japanese case frames that are automatically constructed from a large corpus. To utilize case analysis for NER, we constructed case frames that include NE labels in advance. We explain details in Section 4.4.2. The case analysis is applied to each predicate in an input sentence. For details see (Kawahara and Kurohashi, 2002). coreference analysis Coreference analysis is performed by using the coreference"
I08-2080,N03-1002,0,0.128241,"ot use structural information. We also conducted experiments on IREX NE data and an NE-annotated web corpus and conﬁrmed that structural information improves the performance of NER. 1 Introduction Named entity recognition (NER) is the task of identifying and classifying phrases into certain classes of named entities (NEs), such as names of persons, organizations and locations. Japanese texts, which we focus on, are written without using blank spaces. Therefore, Japanese NER has tight relation with morphological analysis, and thus it is often performed immediately after morphological analysis (Masayuki and Matsumoto, 2003; Yamada, 2007). However, such approaches rely only on local context. The Japanese NER system proposed in (Nakano and Hirai, 2004), which achieved the highest F-measure among conventional systems, introduced the bunsetsu1 feature in order to consider wider context, but considers only adjacent bunsetsus. *Research Fellow of the Japan Society for the Promotion of Science (JSPS) 1 Bunsetsu is a commonly used linguistic unit in Japanese, consisting of one or more adjacent content words and zero or more following functional words. Sadao Kurohashi Graduate School of Infomatics, Kyoto University kuro"
I08-2080,P05-3015,0,0.0221901,"cience (JSPS) 1 Bunsetsu is a commonly used linguistic unit in Japanese, consisting of one or more adjacent content words and zero or more following functional words. Sadao Kurohashi Graduate School of Infomatics, Kyoto University kuro@i.kyoto-u.ac.jp On the other hand, as for English or Chinese, various NER systems have explored global information and reported their effectiveness. In (Malouf, 2002; Chieu and Ng, 2002), information about features assigned to other instances of the same token is utilized. (Ji and Grishman, 2005) uses the information obtained from coreference analysis for NER. (Mohit and Hwa, 2005) uses syntactic features in building a semi-supervised NE tagger. In this paper, we present a Japanese NER system that uses global information obtained from several structural analyses. To be more speciﬁc, our system is based on SVM, recognizes NEs after syntactic, case and coreference analyses and uses information obtained from these analyses and the NER results for the previous context, integrally. At this point, it is true that NER results are useful for syntactic, case and coreference analyses, and thus these analyses and NER should be performed in a complementary way. However, since we fo"
I08-2080,W02-1036,0,0.148842,"iro. winner Sinuiju from Amnokkang get across (φ gets across the Amnokkang River from Sinuiju.) starter Kawasaki-ha genzai 4 shou 3 pai. now won lost (The winning pitcher is the starter Kenjiro Kawasaki. Kawasaki has won 4 and lost 3.) 4.1 Outline of Our NER System Our NER system performs the chunking process based on morpheme units because character-based methods do not outperform morpheme-based methods (Masayuki and Matsumoto, 2003) and are not suitable for considering wider context. A wide variety of trainable models have been applied to Japanese NER task, including maximum entropy models (Utsuro et al., 2002), support vector machines (Nakano and Hirai, 2004; Yamada, 2007) and conditional random ﬁelds (Fukuoka, 2006). Our system applies SVMs because, for Japanese NER, SVM-based systems achieved higher F-measure than the other systems. (Isozaki and Kazawa, 2003) proposed an SVM-based NER system with Viterbi search, which outperforms an SVM-based NER system with sequential determination, and our system basically follows this system. Our NER system consists of the following four steps: 1. 2. 3. 4. (3) Dai 10 setsu-wa Kawasaki Frontale-to taisen. the round against Kawasaki-ha genzai 4 shou 3 pai. now w"
I08-2080,W02-2019,0,\N,Missing
I08-2080,P05-1051,0,\N,Missing
I08-2110,P97-1004,0,0.0316159,"imilarity is high are used for the flexible matching. By utilizing only synonymous expressions extracted from a dictionary whose distributional similarity is high, we can exclude synonymous expressions extracted from a dictionary that are rarely used, and the pair of words whose distributional similarity is high that is not actually a synonymous expression (is not listed in a dictionary). Another point of our method is to introduce SYNGRAPH data structure. So far, the effectiveness of handling expressive divergence has been shown for IR using a thesaurus-based query expansion (Voorhees, 1994; Jacquemin et al., 1997). However, their methods are based on a bag-of-words approach and thus does not pay attention to sentence-level synonymy with syntactic structure. MT requires such precise handling of synonymy, and advanced IR and QA also need it. To handle sentence-level synonymy precisely, we have to consider the combination of expressive divergence, which may cause combinatorial explosion. To overcome this problem, an ID is assigned to each synonymous group, and then SYNGRAPH data structure is introduced to pack expressive divergence. and Nagao, 1988; Tsurumaru et al., 1986), they extracted only hypernym-hy"
I08-2110,H01-1043,1,0.823939,"dinner). In some cases, however, a word other than the last word can be a hypernym or synonym. These cases can be detected by sentence-final patterns as follows (the underlined expressions represent the patterns): 2 2.2 Calculating the Distributional Similarity using a Web Corpus The similarity between a pair of synonymous expressions is calculated based on distributional similarity (J.R.Firth, 1957; Harris, 1968) using the Web corpus collected by (Kawahara and Kurohashi, 2006). The similarity between two predicates is defined to be one between the patterns of case examples of each predicate (Kawahara and Kurohashi, 2001). The similarity between two nouns are defined Synonymy Database This section describes a method for constructing a synonymy database. First, synonym/hypernym relations are automatically extracted from an ordinary dictionary, and the distributional similarity of a pair of synonymous expressions is calculated using a Web corpus. Then, the extracted synonymous expressions whose similarity is high are used for the flexible matching. 2.1 Synonym/hypernym Extraction from an Ordinary Dictionary Although there were some attempts to extract synonymous expressions from a dictionary (Nakamura Hypernyms"
I08-2110,kawahara-kurohashi-2006-case,1,0.822697,"atte (balance) tyushin (center) tonaru (become) ten (spot). For example, the last word shokuji (meal) can be extracted as the hypernym of yushoku (dinner). In some cases, however, a word other than the last word can be a hypernym or synonym. These cases can be detected by sentence-final patterns as follows (the underlined expressions represent the patterns): 2 2.2 Calculating the Distributional Similarity using a Web Corpus The similarity between a pair of synonymous expressions is calculated based on distributional similarity (J.R.Firth, 1957; Harris, 1968) using the Web corpus collected by (Kawahara and Kurohashi, 2006). The similarity between two predicates is defined to be one between the patterns of case examples of each predicate (Kawahara and Kurohashi, 2001). The similarity between two nouns are defined Synonymy Database This section describes a method for constructing a synonymy database. First, synonym/hypernym relations are automatically extracted from an ordinary dictionary, and the distributional similarity of a pair of synonymous expressions is calculated using a Web corpus. Then, the extracted synonymous expressions whose similarity is high are used for the flexible matching. 2.1 Synonym/hyperny"
I08-2110,J94-4001,1,0.639043,"xception of polysemic words, synonymous groups are constructed first. They are given IDs, hereafter called SYNID4 . Then, hypernymhyponym relations are established between synonymous groups. We call this resulting data as synonymy database. Figure 1 shows examples of synonymous groups in the synonymy database. In this paper, SYNID is denoted by using English gloss word, surrounded by “ h i ”. 3 SYNGRAPH 3.1 SYNGRAPH Data Structure SYNGRAPH data structure is an acyclic directed graph, and the basis of SYNGRAPH is the dependency structure of an original sentence (in this paper, a robust parser (Kurohashi and Nagao, 1994) is always employed). In the dependency structure, each node consists of one content word and zero or more function words, which is called a basic node hereafter. If the content word of a basic node belongs to a synonymous group, a new node with the SYNID is attached to it, and it is called a SYN node hereafter. For example, in Figure 2, the shaded nodes are basic nodes and the other nodes are SYN nodes5 . Then, if the expression conjoining two or more 4 Spelling variations such as use of Hiragana, Katakana or Kanji are handled by the morphological analyzer JUMAN (Kurohashi et al., 1994). 5 Th"
I08-2110,2005.iwslt-1.27,1,0.868044,"Missing"
I08-2110,C88-2098,0,0.350351,"ing, which brings great difficulty to many NLP tasks, such as machine translation (MT), information retrieval (IR), and question answering (QA). For example, suppose an input sentence (1) is given to a Japanese-English example-based machine translation system. (1) hotel ni ichiban chikai eki wa doko-desuka hotel best near station where is Even if a very similar translation example (TE) “(2-a) ↔ (2-b)” exists in the TEs, a simple exact matching method cannot utilize this example for the translation. An ordinary dictionary is a knowledge source to provide synonym and hypernym-hyponym relations (Nakamura and Nagao, 1988; Tsurumaru et al., 1986). A problem in using synonymous expressions extracted from a dictionary is that some of them are not appropriate since they are rarely used. For example, a synonym pair “suidou”1 = “kaikyou(strait)” is extracted. Recently, some work has been done on corpusbased paraphrase extraction (Lin and Pantel, 2001; Barzilay and Lee, 2003). The basic idea of their methods is that two words with similar meanings are used in similar contexts. Although their methods can obtain broad-coverage paraphrases, the obtained paraphrases are not accurate enough to be utilized 787 1 This word"
I08-2110,C86-1105,0,0.629138,"fficulty to many NLP tasks, such as machine translation (MT), information retrieval (IR), and question answering (QA). For example, suppose an input sentence (1) is given to a Japanese-English example-based machine translation system. (1) hotel ni ichiban chikai eki wa doko-desuka hotel best near station where is Even if a very similar translation example (TE) “(2-a) ↔ (2-b)” exists in the TEs, a simple exact matching method cannot utilize this example for the translation. An ordinary dictionary is a knowledge source to provide synonym and hypernym-hyponym relations (Nakamura and Nagao, 1988; Tsurumaru et al., 1986). A problem in using synonymous expressions extracted from a dictionary is that some of them are not appropriate since they are rarely used. For example, a synonym pair “suidou”1 = “kaikyou(strait)” is extracted. Recently, some work has been done on corpusbased paraphrase extraction (Lin and Pantel, 2001; Barzilay and Lee, 2003). The basic idea of their methods is that two words with similar meanings are used in similar contexts. Although their methods can obtain broad-coverage paraphrases, the obtained paraphrases are not accurate enough to be utilized 787 1 This word usually means “water sup"
I08-2110,N03-1003,0,\N,Missing
I11-1051,W02-2016,0,0.602073,"Missing"
I11-1051,J94-4001,1,0.570926,"mpirically proven to be effective for coordination disambiguation. However, a unified approach that combines both clues has not been explored comprehensively. In this paper, we propose a unified framework for coordi• selectional preferences. Syntactic, lexical and semantic parallelism of conjuncts is frequently observed in coordinate structures. For example, Dubey et al. (2005) empirically confirmed syntactic parallelism in coordinate structures. This clue was modeled by string matching, part-of-speech matching, number agreement, semantic similarities, and so forth (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994; Resnik, 1999; Chantree et al., 2005; 456 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of syntactic parallelism in coordinate structures into their MaxEnt reranking parser. K¨ubler et al. (2009) used a reranking parser with automatically detected scope possibilities to improve German parsing. As for a generative parser, Dubey et al. (2006"
I11-1051,C08-1012,0,0.0266675,"Missing"
I11-1051,P98-2127,0,0.0196256,"ant role in sentence meanings. For instance, examples are distinguished not by verbs (e.g., yaku (bake/broil/have difficulty)), but by pairs (e.g., pan-wo yaku (bake bread), nikuwo yaku (broil meat), and te-wo yaku (have difficulty)). Predicate-argument examples are aggregated in this way, and yield basic case frames. Thereafter, the basic case frames are clustered to merge similar case frames. For example, since pan-wo yaku (bake bread) and niku-wo yaku (broil meat) are similar, they are clustered. The similarity is measured by using a distributional thesaurus based on the study described in Lin (1998). By using this gradual procedure, we constructed case frames from a web corpus. The case frames were obtained from approximately 1.6 billion sentences extracted from the web. They consisted of 43,000 predicates, and the average number of case frames for a verb was 22.2. In Table 1, some examples of the resulting case frames of the verb yaku are listed. 4.3 Our Model We employ the probabilistic generative model of dependency and case structure analysis (Kawahara and Kurohashi, 2008) as a base model. This base model resolves coordination ambiguities only on the basis of selectional preferences"
I11-1051,P05-1022,0,0.0605805,"rved in coordinate structures. For example, Dubey et al. (2005) empirically confirmed syntactic parallelism in coordinate structures. This clue was modeled by string matching, part-of-speech matching, number agreement, semantic similarities, and so forth (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994; Resnik, 1999; Chantree et al., 2005; 456 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of syntactic parallelism in coordinate structures into their MaxEnt reranking parser. K¨ubler et al. (2009) used a reranking parser with automatically detected scope possibilities to improve German parsing. As for a generative parser, Dubey et al. (2006) proposed an unlexicalized PCFG parser that modified PCFG probabilities to condition the existence of a coordinate structure. Hogan (2007) proposed a generative lexicalized parser that considered the symmetry of part-of-speech tags and phrase categories of conjuncts, which is more shallow information than our synchro"
I11-1051,H05-1105,0,0.0219366,"ectively. These types of parallelism contribute to identifying the coordinate structure that conjoins Caesar salad and Italian pasta. The other clue is selectional preferences, such as eat in the above example. Since eat is likely to have salad and pasta as its objects, it is plausible that salad and pasta are coordinated. Such selectional preferences of predicates are thought to support the construction of coordinate structures, and were used in Japanese dependency parsing by Kawahara and Kurohashi (2008). Selectional preferences of nouns (noun-noun modifications) were used by Resnik (1999), Nakov and Hearst (2005) and Kawahara and Kurohashi (2008). For example, let us see the following examples: 1 Introduction Coordinate structures are a potential source of syntactic ambiguity in natural language. Although many methods have been proposed to resolve the ambiguities of coordinate structures, coordination disambiguation still remains a difficult problem for state-of-the-art parsers. Previous studies on coordination disambiguation used two kinds of clues: (2) a. mail and securities fraud b. corn and peanut butter • parallelism of conjuncts, and In (2a), the coordination of mail and securities is guided by"
I11-1051,H05-1104,0,0.0319942,"ided by the estimation that mail fraud is a salient compound nominal phrase. In (2b), on the contrary, the coordinate structure that conjoins corn and peanut butter is led because corn butter is not a familiar concept. Each clue has been empirically proven to be effective for coordination disambiguation. However, a unified approach that combines both clues has not been explored comprehensively. In this paper, we propose a unified framework for coordi• selectional preferences. Syntactic, lexical and semantic parallelism of conjuncts is frequently observed in coordinate structures. For example, Dubey et al. (2005) empirically confirmed syntactic parallelism in coordinate structures. This clue was modeled by string matching, part-of-speech matching, number agreement, semantic similarities, and so forth (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994; Resnik, 1999; Chantree et al., 2005; 456 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of synta"
I11-1051,C04-1002,0,0.0414497,"Missing"
I11-1051,D07-1064,0,0.0158621,"e categories of conjuncts, which is more shallow information than our synchronization model. She also used cooccurrence statistics of conjunct heads, which are similar to our modeling of lexical parallelism, but her model did not use selectional preferences. Kurohashi and Nagao (1994) proposed a rulebased method of Japanese dependency parsing that included coordination disambiguation. Their method first detects coordinate structures in a sentence using dynamic programming, and then determines the dependency structure of the sentence under the constraints of the detected coordinate structures. Shimbo and Hara (2007) and Hara et al. (2009) considered many features for coordination disambiguation and automatically optimized their weights, which were heuristically determined in Kurohashi and Nagao (1994), by using a discriminative learning model. nation disambiguation by incorporating both the clues into a generative parser. To capture syntactic parallelism of conjuncts, we formulate the generative process of pre-modifiers of conjuncts in a synchronized manner. In the above example, the generation process of Caesar from salad is synchronized with that of Italian from pasta. An interpretation of an unbalance"
I11-1051,P06-1053,0,0.0219553,"hi and Nagao, 1994; Resnik, 1999; Chantree et al., 2005; 456 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of syntactic parallelism in coordinate structures into their MaxEnt reranking parser. K¨ubler et al. (2009) used a reranking parser with automatically detected scope possibilities to improve German parsing. As for a generative parser, Dubey et al. (2006) proposed an unlexicalized PCFG parser that modified PCFG probabilities to condition the existence of a coordinate structure. Hogan (2007) proposed a generative lexicalized parser that considered the symmetry of part-of-speech tags and phrase categories of conjuncts, which is more shallow information than our synchronization model. She also used cooccurrence statistics of conjunct heads, which are similar to our modeling of lexical parallelism, but her model did not use selectional preferences. Kurohashi and Nagao (1994) proposed a rulebased method of Japanese dependency parsing that included"
I11-1051,W07-2201,0,0.208556,"Missing"
I11-1051,C96-1058,0,0.184382,"sents the situation of the same case slot of the pre-conjunct. In practice, to avoid the data sparseness problem, we interpolate this probability, which is conditioned on case frames, with the probability conditioned on predicates in the same manner as in Collins (1999). This probability is estimated on the basis of the cooccurrence data of coordinated nouns described in section 4.2.3. 4.4 Practical Issue The proposed model considers all the possible dependency structures including coordination ambiguities. To reduce this high computational cost, we introduced the CKY framework to the search (Eisner, 1996). 4.3.2 Generative Probability of Argument Nouns In the base model, the generative probability of argument nouns in a clause is defined as the product of the generative probability of an argument noun P njk : ∏ ∏ (5) njk ∈N sj P njk , sj :A(sj )=Y 5 Experiments 5.1 Experimental Settings where N sj is a set of nouns including a noun filled in the case slot sj and its coordinated nouns. The generative probability of an argument noun is given as follows: P njk = P (njk |CF l , sj ). # of sents. 1,000 1,000 759 (6) In our model, the direct argument noun filled in the case slot sj is generated with"
I11-1051,P09-1109,0,0.0155,"which is more shallow information than our synchronization model. She also used cooccurrence statistics of conjunct heads, which are similar to our modeling of lexical parallelism, but her model did not use selectional preferences. Kurohashi and Nagao (1994) proposed a rulebased method of Japanese dependency parsing that included coordination disambiguation. Their method first detects coordinate structures in a sentence using dynamic programming, and then determines the dependency structure of the sentence under the constraints of the detected coordinate structures. Shimbo and Hara (2007) and Hara et al. (2009) considered many features for coordination disambiguation and automatically optimized their weights, which were heuristically determined in Kurohashi and Nagao (1994), by using a discriminative learning model. nation disambiguation by incorporating both the clues into a generative parser. To capture syntactic parallelism of conjuncts, we formulate the generative process of pre-modifiers of conjuncts in a synchronized manner. In the above example, the generation process of Caesar from salad is synchronized with that of Italian from pasta. An interpretation of an unbalanced coordinate structure"
I11-1051,P07-1086,0,0.024373,", pages 456–464, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP factors parallelism. Several other studies have considered parallelism in parsing models. Charniak and Johnson (2005) incorporated some features of syntactic parallelism in coordinate structures into their MaxEnt reranking parser. K¨ubler et al. (2009) used a reranking parser with automatically detected scope possibilities to improve German parsing. As for a generative parser, Dubey et al. (2006) proposed an unlexicalized PCFG parser that modified PCFG probabilities to condition the existence of a coordinate structure. Hogan (2007) proposed a generative lexicalized parser that considered the symmetry of part-of-speech tags and phrase categories of conjuncts, which is more shallow information than our synchronization model. She also used cooccurrence statistics of conjunct heads, which are similar to our modeling of lexical parallelism, but her model did not use selectional preferences. Kurohashi and Nagao (1994) proposed a rulebased method of Japanese dependency parsing that included coordination disambiguation. Their method first detects coordinate structures in a sentence using dynamic programming, and then determines"
I11-1051,kawahara-kurohashi-2006-case,1,0.810006,"ga (NOM), wo (ACC), ni (DAT) and de (LOC). Example words are expressed only in English due to space limitation. The number following each word denotes its frequency. In (b), P (A(GEN=N)|tibet, Ac (GEN)=Y) means that nothing is generated from tibet, whereas the head of the pre-conjunct has a genitive case. This probability has a small value because of non-synchronization (unbalanced coordinate structure). pendency parser, KNP,4 which is also used as a base model in the following sections. 4.2.1 Automatically Constructed Case Frames 4.2 Resources We employ automatically constructed case frames (Kawahara and Kurohashi, 2006). This section outlines the method for constructing the case frames. A large corpus is automatically parsed, and case As the resources of selectional preferences to support coordinate structures, we use automatically constructed case frames and cooccurrences of noun-noun modifications. As a parser for extracting these resources, we use the Japanese de4 459 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP lect cooccurrences of coordinated nouns from automatic parses of a large corpus. We extracted 54.1 million unique noun pairs from the web corpus of 1.6 billion sentences. frames are constructed"
I11-1051,C08-1054,1,0.614389,"similarity. In addition, syntactic parallelism can be observed; each conjunct has a modifier Caesar and Italian, respectively. These types of parallelism contribute to identifying the coordinate structure that conjoins Caesar salad and Italian pasta. The other clue is selectional preferences, such as eat in the above example. Since eat is likely to have salad and pasta as its objects, it is plausible that salad and pasta are coordinated. Such selectional preferences of predicates are thought to support the construction of coordinate structures, and were used in Japanese dependency parsing by Kawahara and Kurohashi (2008). Selectional preferences of nouns (noun-noun modifications) were used by Resnik (1999), Nakov and Hearst (2005) and Kawahara and Kurohashi (2008). For example, let us see the following examples: 1 Introduction Coordinate structures are a potential source of syntactic ambiguity in natural language. Although many methods have been proposed to resolve the ambiguities of coordinate structures, coordination disambiguation still remains a difficult problem for state-of-the-art parsers. Previous studies on coordination disambiguation used two kinds of clues: (2) a. mail and securities fraud b. corn"
I11-1051,E09-1047,0,0.0245714,"Missing"
I11-1051,J03-4003,0,\N,Missing
I11-1051,C98-2122,0,\N,Missing
I11-1051,P92-1003,0,\N,Missing
I11-1085,P00-1022,0,0.0686796,"two tasks. Sasano et al. (2008) proposed a lexicalized probabilistic model for zero anaphora resolution, which adopted an entity-mention model and simultaneously resolved predicateargument structures and zero anaphora. Taira et al. (2008) proposed a model for analyzing predicate-argument structures by using decision lists, which integrated the tasks of semantic role labeling and zero-pronoun identiﬁcation. Imamura et al. (2009) proposed a discriminative model for analyzing predicate-argument structures that simultaneously conducted zero anaphora resolution. For languages other than Japanese, Ferrandez and Peral (2000) proposed a hand-engineered rulebased method for both determining anaphoricity and identifying antecedents in Spanish zero pronoun resolution. Zhao and Ng (2007) proposed feature-based methods to Chinese zero anaphora resolution. Kong and Zhou (2010) proposed a tree kernel-based uniﬁed framework for Chinese zero anaphora resolution, which dealt with three subtasks: zero anaphora detection, anaphoricity determination, and antecedent identiﬁcation. 3.2 Generalization of examples The data sparseness problem is alleviated to some extent but not eliminated by using case frames that are automaticall"
I11-1085,P02-1014,0,0.012246,"pproach to Japanese Zero Anaphora Resolution with Large-scale Lexicalized Case Frames Ryohei Sasano Precision and Intelligence Laboratory Tokyo Institute of Technology sasano@pi.titech.ac.jp Sadao Kurohashi Graduate School of Informatics Kyoto University kuro@i.kyoto-u.ac.jp Abstract this paper aims to resolve zero anaphora in Web text, since this involves a wider range of writing styles and is thus considered to be a more practical setting. Reference resolution systems generally require a variety of information sources ranging from syntactic and discourse preferences to semantic preferences (Ng and Cardie, 2002; Haghighi and Klein, 2010). Since syntactic and discourse preferences are not word-speciﬁc, they can be learned from a relatively small annotated corpus. Semantic preferences, on the other hand, represent highly lexicalized knowledge, and hence it is difﬁcult to learn these from a small annotated corpus. In some cases, knowledge of the relations between a predicate and its particular argument is insufﬁcient, particularly for zero anaphora resolution. For example, although the dative argument of the predicate ‘yaku (bake/burn)’ is generally ﬁlled by a disk, such as a CD or DVD, it is often ﬁll"
I11-1085,N10-1061,0,0.0130866,"ero Anaphora Resolution with Large-scale Lexicalized Case Frames Ryohei Sasano Precision and Intelligence Laboratory Tokyo Institute of Technology sasano@pi.titech.ac.jp Sadao Kurohashi Graduate School of Informatics Kyoto University kuro@i.kyoto-u.ac.jp Abstract this paper aims to resolve zero anaphora in Web text, since this involves a wider range of writing styles and is thus considered to be a more practical setting. Reference resolution systems generally require a variety of information sources ranging from syntactic and discourse preferences to semantic preferences (Ng and Cardie, 2002; Haghighi and Klein, 2010). Since syntactic and discourse preferences are not word-speciﬁc, they can be learned from a relatively small annotated corpus. Semantic preferences, on the other hand, represent highly lexicalized knowledge, and hence it is difﬁcult to learn these from a small annotated corpus. In some cases, knowledge of the relations between a predicate and its particular argument is insufﬁcient, particularly for zero anaphora resolution. For example, although the dative argument of the predicate ‘yaku (bake/burn)’ is generally ﬁlled by a disk, such as a CD or DVD, it is often ﬁlled by a person, such as ‘mu"
I11-1085,J05-1004,0,0.0103313,"2011 AFNLP 3 Case Frames Therefore, in this paper, we extend Sasano et al. (2008)’s model by incorporating it into a loglinear framework, and introduce overlapping features such as lexical features with different granularities. In addition, we also investigate the relative importance of each feature for resolving zero anaphora in Web text. 2 3.1 Lexicalized case frames Our model exploits lexicalized case frames that are automatically constructed from 1.6 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. Case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), and for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003). However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in the case frames. Each case frame describes surface cases that each predicate has and examples that can ﬁll a case slot, which is fully-lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). Note that case frames offer not only knowledge of the relations between a predicate and its particular case slot, but also knowledge of the relations among a predicate"
I11-1085,P06-1079,0,0.583107,"me is constructed for each meaning of ‘yaku (bake/burn),’ such as ‘bake foods,’ ‘have difﬁculty,’ and ‘burn on a disk.’ Related Work Several approaches to Japanese zero anaphora resolution have been proposed. Seki et al. (2002) proposed a probabilistic model for zero pronoun detection and resolution that used hand-crafted case frames. Kawahara and Kurohashi (2004) introduced wide-coverage case frames that were automatically constructed from a large corpus to alleviate the sparseness of hand-crafted case frames. They used the case frames as selectional restrictions for zero pronoun resolution. Iida et al. (2006) explored a machine learning method using rich syntactic pattern features that represented the syntactic relations between a zero-pronoun and its candidate antecedent. Since predicate-argument structure analysis and zero anaphora resolution are closely related, several approaches have simultaneously solved these two tasks. Sasano et al. (2008) proposed a lexicalized probabilistic model for zero anaphora resolution, which adopted an entity-mention model and simultaneously resolved predicateargument structures and zero anaphora. Taira et al. (2008) proposed a model for analyzing predicate-argume"
I11-1085,C08-1097,1,0.333216,"ence it is difﬁcult to learn these from a small annotated corpus. In some cases, knowledge of the relations between a predicate and its particular argument is insufﬁcient, particularly for zero anaphora resolution. For example, although the dative argument of the predicate ‘yaku (bake/burn)’ is generally ﬁlled by a disk, such as a CD or DVD, it is often ﬁlled by a person, such as ‘musuko (son),’ if the accusative argument is ﬁlled by ‘te (hands)’ as in the example (i).2 Thus, knowledge of relations among a predicate and its multiple arguments is required to take such preferences into account. Sasano et al. (2008) exploited large-scale case frames that were automatically constructed from 1.6 billion Web sentences as such a lexical resource, and proposed a probabilistic model for Japanese zero anaphora resolution. Their model demonstrated moderate performance, but it could not easily introduce new features, especially overlapping ones, nor take into consideration the importance of each feature, due to the assumption of independence in estimating probability. However, we think a variety of clues can be useful for zero anaphora resolution, where it is important to exploit overlapping features and consider"
I11-1085,W07-1522,0,0.583086,"Missing"
I11-1085,N09-1059,1,0.800155,"ment of a predicate in grand-parent node ... ... Inter sentential (21 categories) B1: Mentioned in the adjacent sentence B1-ga-ov: Overt nominative argument of a predicate in the adjacent sentence B1-ga-om: Omitted nominative argument of a predicate in the adjacent sentence B1-wo-ov: Overt accusative argument of a predicate in the adjacent sentence ... B3: ... 6 Experiments Mentioned in more than two sentences before 6.1 Setting ... ... B2: Mentioned in two sentences before B2-ga-ov: Overt nominative argument of a predicate in the two sentence before We used the same data set as described in (Sasano et al., 2009). This data set consisted of 186 Web documents (979 sentences, 19,677 morphemes), in which all predicate-argument relations were manually annotated. There were 2,137 predicates in this corpus, and 683 zero anaphoric relations were annotated. We call this data set a Web Corpus after this. We performed 6-fold cross-validation. We used correct morphemes, named entities, dependency structures, and coreference relations that were manually annotated to concentrate on zero anaphora resolution. We tested 10 initial values as parameter Λ. Since the parameters converged to almost the same value for each"
I11-1085,P09-2022,0,0.275444,"zero-pronoun and its candidate antecedent. Since predicate-argument structure analysis and zero anaphora resolution are closely related, several approaches have simultaneously solved these two tasks. Sasano et al. (2008) proposed a lexicalized probabilistic model for zero anaphora resolution, which adopted an entity-mention model and simultaneously resolved predicateargument structures and zero anaphora. Taira et al. (2008) proposed a model for analyzing predicate-argument structures by using decision lists, which integrated the tasks of semantic role labeling and zero-pronoun identiﬁcation. Imamura et al. (2009) proposed a discriminative model for analyzing predicate-argument structures that simultaneously conducted zero anaphora resolution. For languages other than Japanese, Ferrandez and Peral (2000) proposed a hand-engineered rulebased method for both determining anaphoricity and identifying antecedents in Spanish zero pronoun resolution. Zhao and Ng (2007) proposed feature-based methods to Chinese zero anaphora resolution. Kong and Zhou (2010) proposed a tree kernel-based uniﬁed framework for Chinese zero anaphora resolution, which dealt with three subtasks: zero anaphora detection, anaphoricity"
I11-1085,C02-1078,0,0.591997,"es that can ﬁll a case slot, which is fully-lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). Note that case frames offer not only knowledge of the relations between a predicate and its particular case slot, but also knowledge of the relations among a predicate and its multiple case slots. Table 1 shows examples of constructed case frames.3 A different case frame is constructed for each meaning of ‘yaku (bake/burn),’ such as ‘bake foods,’ ‘have difﬁculty,’ and ‘burn on a disk.’ Related Work Several approaches to Japanese zero anaphora resolution have been proposed. Seki et al. (2002) proposed a probabilistic model for zero pronoun detection and resolution that used hand-crafted case frames. Kawahara and Kurohashi (2004) introduced wide-coverage case frames that were automatically constructed from a large corpus to alleviate the sparseness of hand-crafted case frames. They used the case frames as selectional restrictions for zero pronoun resolution. Iida et al. (2006) explored a machine learning method using rich syntactic pattern features that represented the syntactic relations between a zero-pronoun and its candidate antecedent. Since predicate-argument structure analys"
I11-1085,C02-1122,1,0.811993,"oint Conference on Natural Language Processing, pages 758–766, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 3 Case Frames Therefore, in this paper, we extend Sasano et al. (2008)’s model by incorporating it into a loglinear framework, and introduce overlapping features such as lexical features with different granularities. In addition, we also investigate the relative importance of each feature for resolving zero anaphora in Web text. 2 3.1 Lexicalized case frames Our model exploits lexicalized case frames that are automatically constructed from 1.6 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. Case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), and for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003). However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in the case frames. Each case frame describes surface cases that each predicate has and examples that can ﬁll a case slot, which is fully-lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). Note that case frames offer not only knowledge of the relations bet"
I11-1085,D08-1055,0,0.119836,"tional restrictions for zero pronoun resolution. Iida et al. (2006) explored a machine learning method using rich syntactic pattern features that represented the syntactic relations between a zero-pronoun and its candidate antecedent. Since predicate-argument structure analysis and zero anaphora resolution are closely related, several approaches have simultaneously solved these two tasks. Sasano et al. (2008) proposed a lexicalized probabilistic model for zero anaphora resolution, which adopted an entity-mention model and simultaneously resolved predicateargument structures and zero anaphora. Taira et al. (2008) proposed a model for analyzing predicate-argument structures by using decision lists, which integrated the tasks of semantic role labeling and zero-pronoun identiﬁcation. Imamura et al. (2009) proposed a discriminative model for analyzing predicate-argument structures that simultaneously conducted zero anaphora resolution. For languages other than Japanese, Ferrandez and Peral (2000) proposed a hand-engineered rulebased method for both determining anaphoricity and identifying antecedents in Spanish zero pronoun resolution. Zhao and Ng (2007) proposed feature-based methods to Chinese zero anap"
I11-1085,D07-1057,0,0.670011,"ed predicateargument structures and zero anaphora. Taira et al. (2008) proposed a model for analyzing predicate-argument structures by using decision lists, which integrated the tasks of semantic role labeling and zero-pronoun identiﬁcation. Imamura et al. (2009) proposed a discriminative model for analyzing predicate-argument structures that simultaneously conducted zero anaphora resolution. For languages other than Japanese, Ferrandez and Peral (2000) proposed a hand-engineered rulebased method for both determining anaphoricity and identifying antecedents in Spanish zero pronoun resolution. Zhao and Ng (2007) proposed feature-based methods to Chinese zero anaphora resolution. Kong and Zhou (2010) proposed a tree kernel-based uniﬁed framework for Chinese zero anaphora resolution, which dealt with three subtasks: zero anaphora detection, anaphoricity determination, and antecedent identiﬁcation. 3.2 Generalization of examples The data sparseness problem is alleviated to some extent but not eliminated by using case frames that are automatically constructed from a large corpus. For instance, there are thousands of named entities (NEs) that cannot intrinsically be covered. Sasano et al. (2008) generaliz"
I11-1085,N06-1023,1,0.859924,"r that maximizes posterior probability. Occupancy of case slot We believe that there is a relation between the occupancy of a case slot and its generativity of zero pronouns. For example, since the ni (dative) case of ‘yaku (3)’ often appears, we assume this slot is just omitted as a zero pronoun even if there is no overt argument. However, since the ni (dative) case of ‘yaku (1)’ rarely appears, we assume there is no case slot if there is no overt argument. Therefore, we use the log occupancy of the case slot, whose value is the same as the log of the generative probability of a case slot in Kawahara and Kurohashi (2006)’s model and estimated from case structure analysis of a large raw corpus: 5 Features 5.1 Lexical features We exploit six types of lexical features: word PMI (Pointwise Mutual Information), cluster PMI, category PMI, NE PMI, occupancy of a case slot, and overt argument assignment score. Their values are real values that are calculated by using case frames. Since our model is based on the entitymention model that assigns zero pronouns not to a certain mention but to an entity, several values can be calculated for a certain lexical feature by taking coreferential mentions into consideration. In"
I11-1085,P08-1047,0,0.0110442,"lization of examples The data sparseness problem is alleviated to some extent but not eliminated by using case frames that are automatically constructed from a large corpus. For instance, there are thousands of named entities (NEs) that cannot intrinsically be covered. Sasano et al. (2008) generalized examples of case slots based on 22 common noun categories deﬁned in the Japanese morphological analyzer JUMAN,4 and 8 NE classes deﬁned by the IREX Committee (1999) to deal with this problem. In addition, we generalized case slot examples based on automatically acquired multi-word noun clusters. Kazama and Torisawa (2008) proposed the parallelization of EM-based clustering with the aim of enabling large-scale clustering and using the resulting clusters in NE recognition. We used the resulting 2,000 clusters acquired from 1 million unique multi-word nouns. Table 2 lists examples of the resulting 2,000 clusters.3 As well as common noun categories and NE classes, we calculated the average of the probabilities that each case slot example belonged to 3 4 759 We use English examples for the sake of readability. http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html Case slot ga nominative wo accusative ni dative de"
I11-1085,D10-1086,0,0.529249,"for analyzing predicate-argument structures by using decision lists, which integrated the tasks of semantic role labeling and zero-pronoun identiﬁcation. Imamura et al. (2009) proposed a discriminative model for analyzing predicate-argument structures that simultaneously conducted zero anaphora resolution. For languages other than Japanese, Ferrandez and Peral (2000) proposed a hand-engineered rulebased method for both determining anaphoricity and identifying antecedents in Spanish zero pronoun resolution. Zhao and Ng (2007) proposed feature-based methods to Chinese zero anaphora resolution. Kong and Zhou (2010) proposed a tree kernel-based uniﬁed framework for Chinese zero anaphora resolution, which dealt with three subtasks: zero anaphora detection, anaphoricity determination, and antecedent identiﬁcation. 3.2 Generalization of examples The data sparseness problem is alleviated to some extent but not eliminated by using case frames that are automatically constructed from a large corpus. For instance, there are thousands of named entities (NEs) that cannot intrinsically be covered. Sasano et al. (2008) generalized examples of case slots based on 22 common noun categories deﬁned in the Japanese morph"
I11-1085,korhonen-etal-2006-large,0,0.00952014,"matically constructed from 1.6 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. Case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), and for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003). However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in the case frames. Each case frame describes surface cases that each predicate has and examples that can ﬁll a case slot, which is fully-lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). Note that case frames offer not only knowledge of the relations between a predicate and its particular case slot, but also knowledge of the relations among a predicate and its multiple case slots. Table 1 shows examples of constructed case frames.3 A different case frame is constructed for each meaning of ‘yaku (bake/burn),’ such as ‘bake foods,’ ‘have difﬁculty,’ and ‘burn on a disk.’ Related Work Several approaches to Japanese zero anaphora resolution have been proposed. Seki et al. (2002) proposed a probabilistic model for zero pronoun detection and resolution that used hand-crafted case"
I11-1085,J94-4002,0,0.113455,"of zero anaphora in Web text, we only introduce simple features as non-lexical features. Case/location We use case/location features to reﬂect syntactic, functional, and locational preferences. We considered 85 case/location categories, examples of which are summarized in Table 4. If an antecedent candidate appears in a certain case/location category, the corresponding feature value is 1; otherwise 0. These features are made for each case, respectively, i.e. there are a total of 255 case/location features. Salience Previous work has reported the usefulness of salience in anaphora resolution (Lappin and Leass, 1994; Mitkov et al., 2002; Sasano et al., 2008). We introduce salience features to take 763 Recall Precision F-measure Sasano et al. (2008)’s 0.341 0.306 0.322 model (233/683) (233/762) 0.334 0.412 0.369 Proposed model with merged case frames (228/683) (228/553) Proposed model 0.379 0.403 0.391 (259/683) (259/642) Removed feature type None (Use all features) (Lexical features) − Word PMI − Cluster PMI − Category PMI Table 5: Experimental results of zero anaphora resolution on Web Corpus. Case Type Recall NOM Intra0.504 sentential (120/238) Inter0.460 sentential (104/226) ACC Intra0.250 sentential"
I11-1089,N10-1015,0,0.21905,"Missing"
I11-1089,P05-1022,0,0.0543808,"nments (Och and Ney, 2003). The unit of evaluation was the word for all the languages. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in the experiments are summarized in Table 4.1. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). For English sentences, Charniak’s nlparser was used to convert them into phrase structures (Charniak and Johnson, 2005), and then they were transformed into dependency structures by rules defining head words for phrases. Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-based statistical alignment model 3 Zh-Ja Zh Ja 680K 18.8M 22.3M 27.7 32.9 Table 1: Statistics of the training corpus. 4.1 Settings 2 En-Ja En Ja 996K 24.7M 27.5M 24.8 27.6 αA 100 10 αN 100 100 pt 0.8 0.8 αf e , αef 10"
I11-1089,I08-1012,0,0.20628,"e summarized in Table 4.1. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). For English sentences, Charniak’s nlparser was used to convert them into phrase structures (Charniak and Johnson, 2005), and then they were transformed into dependency structures by rules defining head words for phrases. Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-based statistical alignment model 3 Zh-Ja Zh Ja 680K 18.8M 22.3M 27.7 32.9 Table 1: Statistics of the training corpus. 4.1 Settings 2 En-Ja En Ja 996K 24.7M 27.5M 24.8 27.6 αA 100 10 αN 100 100 pt 0.8 0.8 αf e , αef 100 100 pf e , pef 0.5 0.5 Table 2: Hyper parameters used in experiments. of the IBM Models. We conducted word alignment bidirectionally with the default parameters and merged them using the grow-diag-final-and heuristic (Koehn et al., 2003). Furthermore, we used the BerkeleyAligner4"
I11-1089,P03-1012,0,0.029688,"me corpus used in Section 4. 794 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 794–802, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP phrase, and incorrectly aligned “the ↔ は “. The word sequential model is prone to many such errors even for short simple sentences of a distant language pair. Even if the word order differs greatly between languages, phrase dependencies tend to hold between languages. This is also true in Figure 1. Therefore, incorporating dependency analysis into the alignment model is useful for distant language pairs. Cherry and Lin (2003) proposed a model that uses a source side dependency tree structure and constructs a discriminative model. However, the drawback is that the alignment unit is the word, and thus, it can only find one-to-one alignments. The capability of generating many-to-many correspondences is also important because one or more words often correspond to more than one word on the other side. Nakazawa and Kurohashi (2009) also proposed a model focusing on dependency relations. They modeled phrase dependency relations in dependency trees on both sides. The model is also capable of estimating many-to-many corres"
I11-1089,W02-1018,0,0.0641453,"++. Black boxes depict the system output, while dark (Sure) and light (Possible) gray cells denote gold-standard alignments. as a sequence of words works adequately. This does not hold for distant language pairs such as English-Japanese or Chinese-Japanese, in which word orders differ greatly. We incorporate dependency relations of words into the alignment model and define the reorderings on the word dependency trees. Figure 1 shows an example of the dependency trees for Japanese and English. 2.1 Generative Story Description Dependency Tree-based Alignment Model Similar to the previous works (Marcu and Wong, 2002; DeNero et al., 2008), we first describe the generative story for the joint alignment model. Our model is an extension of the one proposed by Denero et al. (2008). Two main drawbacks of the previous model are the lack of structural information and a naive distortion model. For similar language pairs such as French-English (Marcu and Wong, 2002) or Spanish-English (DeNero et al., 2008), even a simple model that handles sentences 1. Generate ` concepts from which subtree pairs are generated independently. 2. Combine the subtrees in each language so as to create parallel sentences. 795 Here, sub"
I11-1089,P10-1146,0,0.0419626,"Missing"
I11-1089,D08-1077,0,0.0302391,"Missing"
I11-1089,P07-1003,0,0.228894,"For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-based statistical alignment model 3 Zh-Ja Zh Ja 680K 18.8M 22.3M 27.7 32.9 Table 1: Statistics of the training corpus. 4.1 Settings 2 En-Ja En Ja 996K 24.7M 27.5M 24.8 27.6 αA 100 10 αN 100 100 pt 0.8 0.8 αf e , αef 100 100 pf e , pef 0.5 0.5 Table 2: Hyper parameters used in experiments. of the IBM Models. We conducted word alignment bidirectionally with the default parameters and merged them using the grow-diag-final-and heuristic (Koehn et al., 2003). Furthermore, we used the BerkeleyAligner4 (DeNero and Klein, 2007) with default settings for unsupervised training. Experimental results for English-Japanese are shown in Table 4.1, and those for ChineseJapanese are shown in Table 4.1. The alignment accuracy of the initialization described in Section 3.1 is indicated as “Initialization”, while the accuracy after conducting Gibbs sampling is indicated as “Proposed”. 4.2 Discussion For English-Japanese, our proposed model achieved reasonably high alignment accuracy compared with that of GIZA++ and the BerkeleyAligner. Using tree structures combined with the bi-directional alignment results leads to better accu"
I11-1089,W09-2302,1,0.889421,"nguages, phrase dependencies tend to hold between languages. This is also true in Figure 1. Therefore, incorporating dependency analysis into the alignment model is useful for distant language pairs. Cherry and Lin (2003) proposed a model that uses a source side dependency tree structure and constructs a discriminative model. However, the drawback is that the alignment unit is the word, and thus, it can only find one-to-one alignments. The capability of generating many-to-many correspondences is also important because one or more words often correspond to more than one word on the other side. Nakazawa and Kurohashi (2009) also proposed a model focusing on dependency relations. They modeled phrase dependency relations in dependency trees on both sides. The model is also capable of estimating many-to-many correspondences automatically without any heuristics through maximum likelihood estimation. One serious drawback of their model is that it tends to acquire incorrect larger subtrees. For models that can handle multiple levels (or sizes) of structures, larger structures always defeat smaller ones in maximum likelihood estimation, and the best solution is to align one sentence as a structure with the other for al"
I11-1089,D08-1033,0,0.19178,"Missing"
I11-1089,J03-1002,0,0.0157033,"aper abstract corpus provided by JST and NICT. This corpus was developed during a project in Japan called the “Development and Research of Chinese-Japanese Natural Language Processing Technology”. The statistics of these corpora are shown in Table 4.1. Unfortunately, these two corpora are not freely available now, but they will become available to everyone in near future. As gold-standard data, we used 479 sentence pairs of English-Japanese and 510 sentence pairs of Chinese-Japanese. These were annotated by hand using two types of annotations: sure (S) alignments and possible (P ) alignments (Och and Ney, 2003). The unit of evaluation was the word for all the languages. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in the experiments are summarized in Table 4.1. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). For English sentences, Charniak’s nlparser was used to convert them into phrase structures (Charniak and Johnson, 2005"
I11-1089,N06-1023,1,0.874151,"f Chinese-Japanese. These were annotated by hand using two types of annotations: sure (S) alignments and possible (P ) alignments (Och and Ney, 2003). The unit of evaluation was the word for all the languages. We used precision, recall, and alignment error rate (AER) as evaluation criteria. All the experiments were run on the original forms of words. The hyper parameters for our model used in the experiments are summarized in Table 4.1. Japanese sentences were converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). For English sentences, Charniak’s nlparser was used to convert them into phrase structures (Charniak and Johnson, 2005), and then they were transformed into dependency structures by rules defining head words for phrases. Chinese sentences were converted into dependency trees using the word segmentation and POS-tagging tool by Canasai et al. (2009) and the dependency analyzer CNP (Chen et al., 2008). For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-based statistical alignment model 3 Zh-Ja Zh Ja 680K 18.8M 22.3M 27.7 32.9 Table 1: Statistics of"
I11-1089,C10-1123,0,0.0870215,"Missing"
I11-1089,N03-1017,0,0.0765982,"Missing"
I11-1089,2007.mtsummit-papers.63,0,0.0964688,"es and update the current counts by counting subtree pairs and dependency relations in the samples. EXPAND-1 does not have any restrictions on its operation. However, for EXPAND-2, if the root 798 4. Go back to step 2. 4 # of sentences # of words ave. sent. length Alignment Experiments We conducted alignment experiments on EnglishJapanese and Chinese-Japanese corpora to show the effectiveness of the proposed model. En-Ja Zh-Ja For English-Japanese, the JST2 paper abstract corpus was used. This corpus was created by NICT3 from JST’s 2M English-Japanese paper abstract corpus using the method of Utiyama and Isahara (2007). For Chinese-Japanese, we used the paper abstract corpus provided by JST and NICT. This corpus was developed during a project in Japan called the “Development and Research of Chinese-Japanese Natural Language Processing Technology”. The statistics of these corpora are shown in Table 4.1. Unfortunately, these two corpora are not freely available now, but they will become available to everyone in near future. As gold-standard data, we used 479 sentence pairs of English-Japanese and 510 sentence pairs of Chinese-Japanese. These were annotated by hand using two types of annotations: sure (S) alig"
I11-1089,P07-2045,0,0.0131114,"sed”) and 2 steps going down (via “for”), so the dependency relation is (U p, Down) = (1, 2). Consequently, the tuple is represented as a triplet of non-negative integers Rf = (N, U p, Down). The dependency relation probabilities for the foreign language side are drawn from an unknown probability distribution θf e and for the English side from θef , with both obeying the DP: θf e (Rf ) Mf e (Rf ) θef (Re ) Mef (Re ) sults of the standard word alignment tool GIZA++. Many machine translation studies use heuristics to combine the two alignment results, one of which is called grow-diag-final-and (Koehn et al., 2007). Our heuristic is similar to this, but the difference is that we combine the two results based on dependency trees, and not on word sequences. The initialization is carried out by the following steps: 1. Take the intersection of the two results. 2. Add alignment points connected to at least one accepted point in terms of the dependency tree (corresponds to grow-diag). 3. Add alignment points between two unaligned words (corresponds to final-and). Initial boundaries of subtrees and their alignments, and also the counts of subtree pairs and dependency relations are thus acquired. 3.2 Sampling O"
I11-1089,P09-1058,0,0.102602,"Missing"
I11-1089,J93-2003,0,\N,Missing
I11-1089,P05-1033,0,\N,Missing
I11-1115,C08-1001,0,0.352799,"rafsky, 2009). This method extracts two events that share a participant, called a protagonist. Since these methods rely on the coreference analysis result, they are hard to be applied to languages where omitted arguments or zero anaphora are often utilized. Kasch and Oates proposed a method for extracting script-like structures from collections of Web documents (Kasch and Oates, 2010). Their method is topic-driven, and the experiment was performed on only one situation eating at a restaurant. There is some work for acquiring two related events taking argument sharing approach (Torisawa, 2006; Abe et al., 2008). Torisawa proposed a method for acquiring inference rules with temporal constrains by using verb-verb co-occurrences in Japanese coordinated sentences and verb-noun co-occurrences (Torisawa, 2006). Abe et al. acquire semantic relations between events by coupling the pattern-based relation-oriented approach and the anchor-based argument-oriented approach (Abe et al., 2008). Their method first acquires candidate predicate pairs by exploiting a patternbased method, and then seeks anchors indicative of the shared argument. If anchors are found, the predicate pair is verified. These methods can ac"
I11-1115,N04-1038,0,0.046661,"y occur in the following form (for simplicity, the sentences are explained in English): (1) a. A man picked up a purse and brought (φ) to the police. 1 Introduction Natural language understanding requires a wide variety of knowledge. One is the relation between predicate and argument. This relation has been automatically acquired in the form of case frames from a large corpus, and is utilized for parsing (Kawahara and Kurohashi, 2006). Another is the relation between events. The relation between events includes temporal relation, causality, and so on, and is useful for coreference resolution (Bean and Riloff, 2004) and anaphora resolution (Gerber and Chai, 2010). This paper extracts two strongly-related events. Since the meaning of a predicate itself is often ambiguous, an event is treated as predicate-argument structure, namely the predicate with their relevant arguments. An example of two strongly-related events is shown below1 : 1 nom, acc, dat denotes nominative, accusative, dative, respectively. b. (φ) picked up a purse and brought (φ) to the police. In the sentence (1-a), the argument A1 and A2 are omitted in PA2 . Moreover, as an agent is specifically omitted, in the sentence (1-b), the argument"
I11-1115,P10-1100,0,0.0324732,"Manually Constructed Resource Singh and Williams constructed a common sense knowledge base concerned with ordinary human activity (Singh and Williams, 2003). The knowledge base consists of 80,000 propositions with 415,000 temporal and atemporal links between propositions. Espinosa and Lieberman proposed an EventNet, a toolkit for inferring temporal relations between commonsense events from the Openmind Commonsense Knowledge Base (Espinosa and Lieberman, 2005). Recently, Regneri et al. collect natural language descriptions from volunteers over the Internet, and compute a temporal script graph (Regneri et al., 2010). They collected 493 event sequence descriptions for the 22 scenarios such as “eating in a fast-food restaurant” using the Amazon Mechanical Turk. 2.2 Automatic Acquisition of Event Relation from Corpus There are several types in the event relation acquisition. One is the inference rule acquisition. Lin and Pantel extended the distributional hypothesis on words, and calculated two paths in a dependency tree (Lin and Pantel, 2001). If two paths tend to link the same sets of words, these are regarded as being similar. For example, they calculated the similarity between “X is the author of Y” and"
I11-1115,N06-1008,0,0.106728,"Chambers and Jurafsky, 2009). This method extracts two events that share a participant, called a protagonist. Since these methods rely on the coreference analysis result, they are hard to be applied to languages where omitted arguments or zero anaphora are often utilized. Kasch and Oates proposed a method for extracting script-like structures from collections of Web documents (Kasch and Oates, 2010). Their method is topic-driven, and the experiment was performed on only one situation eating at a restaurant. There is some work for acquiring two related events taking argument sharing approach (Torisawa, 2006; Abe et al., 2008). Torisawa proposed a method for acquiring inference rules with temporal constrains by using verb-verb co-occurrences in Japanese coordinated sentences and verb-noun co-occurrences (Torisawa, 2006). Abe et al. acquire semantic relations between events by coupling the pattern-based relation-oriented approach and the anchor-based argument-oriented approach (Abe et al., 2008). Their method first acquires candidate predicate pairs by exploiting a patternbased method, and then seeks anchors indicative of the shared argument. If anchors are found, the predicate pair is verified. T"
I11-1115,P08-1090,0,0.662972,"per extracts two strongly-related events. Since the meaning of a predicate itself is often ambiguous, an event is treated as predicate-argument structure, namely the predicate with their relevant arguments. An example of two strongly-related events is shown below1 : 1 nom, acc, dat denotes nominative, accusative, dative, respectively. b. (φ) picked up a purse and brought (φ) to the police. In the sentence (1-a), the argument A1 and A2 are omitted in PA2 . Moreover, as an agent is specifically omitted, in the sentence (1-b), the argument A1 in PA1 is also omitted. The coreference-based method (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) is hard to be applied to such a language since an argument rarely appears in both PA1 and PA2 . Our proposed method extracts strongly-related events in a two-phrase construct. First, since the arguments, such as A2 and A3 , which specify the meaning of the predicate occur in at least one predicate-argument structure, the co-occurrence measure between “pick up purse” and “bring to police” can be calculated from their occurrences. Thus, we can regard “pick up purse” and “bring to police”, whose mutual information is high, as strongly-related events. 1028 Proceeding"
I11-1115,P09-1068,0,0.40465,"ated events. Since the meaning of a predicate itself is often ambiguous, an event is treated as predicate-argument structure, namely the predicate with their relevant arguments. An example of two strongly-related events is shown below1 : 1 nom, acc, dat denotes nominative, accusative, dative, respectively. b. (φ) picked up a purse and brought (φ) to the police. In the sentence (1-a), the argument A1 and A2 are omitted in PA2 . Moreover, as an agent is specifically omitted, in the sentence (1-b), the argument A1 in PA1 is also omitted. The coreference-based method (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) is hard to be applied to such a language since an argument rarely appears in both PA1 and PA2 . Our proposed method extracts strongly-related events in a two-phrase construct. First, since the arguments, such as A2 and A3 , which specify the meaning of the predicate occur in at least one predicate-argument structure, the co-occurrence measure between “pick up purse” and “bring to police” can be calculated from their occurrences. Thus, we can regard “pick up purse” and “bring to police”, whose mutual information is high, as strongly-related events. 1028 Proceedings of the 5th International Joi"
I11-1115,P10-1160,0,0.02521,"he sentences are explained in English): (1) a. A man picked up a purse and brought (φ) to the police. 1 Introduction Natural language understanding requires a wide variety of knowledge. One is the relation between predicate and argument. This relation has been automatically acquired in the form of case frames from a large corpus, and is utilized for parsing (Kawahara and Kurohashi, 2006). Another is the relation between events. The relation between events includes temporal relation, causality, and so on, and is useful for coreference resolution (Bean and Riloff, 2004) and anaphora resolution (Gerber and Chai, 2010). This paper extracts two strongly-related events. Since the meaning of a predicate itself is often ambiguous, an event is treated as predicate-argument structure, namely the predicate with their relevant arguments. An example of two strongly-related events is shown below1 : 1 nom, acc, dat denotes nominative, accusative, dative, respectively. b. (φ) picked up a purse and brought (φ) to the police. In the sentence (1-a), the argument A1 and A2 are omitted in PA2 . Moreover, as an agent is specifically omitted, in the sentence (1-b), the argument A1 in PA1 is also omitted. The coreference-based"
I11-1115,W10-0905,0,0.0710966,"nd “X wrote Y”. Another type is the script-like knowledge acquisition. Chambers and Jurafsky learn narrative schemas, which mean coherent sequences or sets of events, from unlabeled corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). This method extracts two events that share a participant, called a protagonist. Since these methods rely on the coreference analysis result, they are hard to be applied to languages where omitted arguments or zero anaphora are often utilized. Kasch and Oates proposed a method for extracting script-like structures from collections of Web documents (Kasch and Oates, 2010). Their method is topic-driven, and the experiment was performed on only one situation eating at a restaurant. There is some work for acquiring two related events taking argument sharing approach (Torisawa, 2006; Abe et al., 2008). Torisawa proposed a method for acquiring inference rules with temporal constrains by using verb-verb co-occurrences in Japanese coordinated sentences and verb-noun co-occurrences (Torisawa, 2006). Abe et al. acquire semantic relations between events by coupling the pattern-based relation-oriented approach and the anchor-based argument-oriented approach (Abe et al.,"
I11-1115,N06-1023,1,0.869013,"e relation of arguments, and thus cannot extract an argument such as A3 . In languages where an argument is often omitted, such as Japanese, sentences illustrating the above two events usually occur in the following form (for simplicity, the sentences are explained in English): (1) a. A man picked up a purse and brought (φ) to the police. 1 Introduction Natural language understanding requires a wide variety of knowledge. One is the relation between predicate and argument. This relation has been automatically acquired in the form of case frames from a large corpus, and is utilized for parsing (Kawahara and Kurohashi, 2006). Another is the relation between events. The relation between events includes temporal relation, causality, and so on, and is useful for coreference resolution (Bean and Riloff, 2004) and anaphora resolution (Gerber and Chai, 2010). This paper extracts two strongly-related events. Since the meaning of a predicate itself is often ambiguous, an event is treated as predicate-argument structure, namely the predicate with their relevant arguments. An example of two strongly-related events is shown below1 : 1 nom, acc, dat denotes nominative, accusative, dative, respectively. b. (φ) picked up a pur"
I11-1115,P08-1047,0,0.0609489,"e 1 shows examples of clause relation and predicate-argument structure extraction. We consider PA pairs that occur with a clause relation sequence as standard. In the case of clause relation purpose, PA pairs occur in the following form: PA2 tame-ni PA1 , and so PA1 and PA2 are transposed. In the case of the clause relation contradiction, the negation flag in the predicate of PA2 is reversed. Argument Generalization An argument is generalized to its word class so as to alleviate the problem of data sparseness. As a word class, a large-scale clustering result of verb-noun dependency relations (Kazama and Torisawa, 2008) is used. The number of word class is 2,000, and this word class covers one million noun phrases. Table 2 shows examples of a word class and its words. In pairs of the extracted PAs, the noun n is replaced with the word class hci for which the probability P (c|n) is maximal. For example, “PA1 : ka(mosquito) ni sa-sareru (bitten), PA2 : hareru (swollen)” is changed to “PA1 : h77i ni sa-sareru, PA2 : hareru” since “ka” belongs to the word class h77i. In the same way, “PA1 : hachi(bee) ni sasareru, PA2 : hareru” is changed to “PA1 : h77i ni sa-sareru, PA2 : hareru”, and thus, these two PAs can be"
I11-1116,D09-1163,0,0.0291565,"r PLSI is that it has a natural way of inferring the probabilities of unseen texts, which are not included in the training data. When we compute the probabilities of an unseen text t, the variational parameters γt and φt are estimated using Eqs.(3) and (4). Then, for example, the probabilities of words given t can be obtained using Eq.(5). 3.4 LDA in IR Certain works using LDA for IR are closely related to our work. Wei and Croft (2006) incorporate LDA into a query likelihood model, while Zhou and Wade’s work can be viewed as a study that incorporates LDA into a KL-divergence retrieval model (Zhou and Wade, 2009). Such works successfully utilize the latent information in texts through LDA, and report that the latent information is effective for ad-hoc retrieval. Although there are many differences between our work and those mentioned above, one of the biggest differences is that whereas the other works explored the effectiveness of the latent information for ad-hoc retrieval, we explore it for RF beyond ad-hoc retrieval. 4 Proposed Method 4.1 Overview An overview of the proposed method is illustrated in Figure 1. First, when a query is submitted by a user, we obtain the initial search results (Step 1)"
I13-1005,P05-1012,0,0.0340903,"a query sentence. One stream is based on linguistically-motivated approaches that exploit natural language analysis to identify dependencies between words. For example, Jones proposed an information retrieval method that exploits linguistically-motivated analysis, especially dependency relations (Jones, 1999). However, Jones noted that dependency relations did not contribute to significantly improving performance due to the low accuracy and robustness of syntactic parsers. Subsequently, both the accuracy and robustness of dependency parsers were dramatically improved (Nivre and Scholz, 2004; McDonald et al., 2005), with such parsers being applied more recently to information retrieval (Lee et al., 2006; Song et al., 2008; ShinIn the area of question answering, predicateargument structures have been used to precisely match a query with a passage in a document (e.g., (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Bilotti et al., 2010)). However, candidate documents to extract an answer are retrieved using conventional search engines without NOM (nominative), ACC (accusative), DAT (dative), ALL (allative), GEN (genitive), CMI (comitative), LOC (locative), ABL (ablative), CMP (comparative), DEL (de"
I13-1005,C04-1100,0,0.0219297,"endency relations (Jones, 1999). However, Jones noted that dependency relations did not contribute to significantly improving performance due to the low accuracy and robustness of syntactic parsers. Subsequently, both the accuracy and robustness of dependency parsers were dramatically improved (Nivre and Scholz, 2004; McDonald et al., 2005), with such parsers being applied more recently to information retrieval (Lee et al., 2006; Song et al., 2008; ShinIn the area of question answering, predicateargument structures have been used to precisely match a query with a passage in a document (e.g., (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Bilotti et al., 2010)). However, candidate documents to extract an answer are retrieved using conventional search engines without NOM (nominative), ACC (accusative), DAT (dative), ALL (allative), GEN (genitive), CMI (comitative), LOC (locative), ABL (ablative), CMP (comparative), DEL (delimitative) and TOP (topic marker). 38 structure analysis normalizes the following linguistic expressions: predicate-argument structures. 3 Information retrieval exploiting predicate-argument structures • relative clause • passive voice (the predicate is normalized to active voice) 3.1"
I13-1005,C04-1010,0,0.0118925,"dencies between words in a query sentence. One stream is based on linguistically-motivated approaches that exploit natural language analysis to identify dependencies between words. For example, Jones proposed an information retrieval method that exploits linguistically-motivated analysis, especially dependency relations (Jones, 1999). However, Jones noted that dependency relations did not contribute to significantly improving performance due to the low accuracy and robustness of syntactic parsers. Subsequently, both the accuracy and robustness of dependency parsers were dramatically improved (Nivre and Scholz, 2004; McDonald et al., 2005), with such parsers being applied more recently to information retrieval (Lee et al., 2006; Song et al., 2008; ShinIn the area of question answering, predicateargument structures have been used to precisely match a query with a passage in a document (e.g., (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Bilotti et al., 2010)). However, candidate documents to extract an answer are retrieved using conventional search engines without NOM (nominative), ACC (accusative), DAT (dative), ALL (allative), GEN (genitive), CMI (comitative), LOC (locative), ABL (ablative), CM"
I13-1005,J05-1004,0,0.0628789,"Missing"
I13-1005,de-marneffe-etal-2006-generating,0,0.0580818,"Missing"
I13-1005,D07-1002,0,0.026795,". However, Jones noted that dependency relations did not contribute to significantly improving performance due to the low accuracy and robustness of syntactic parsers. Subsequently, both the accuracy and robustness of dependency parsers were dramatically improved (Nivre and Scholz, 2004; McDonald et al., 2005), with such parsers being applied more recently to information retrieval (Lee et al., 2006; Song et al., 2008; ShinIn the area of question answering, predicateargument structures have been used to precisely match a query with a passage in a document (e.g., (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Bilotti et al., 2010)). However, candidate documents to extract an answer are retrieved using conventional search engines without NOM (nominative), ACC (accusative), DAT (dative), ALL (allative), GEN (genitive), CMI (comitative), LOC (locative), ABL (ablative), CMP (comparative), DEL (delimitative) and TOP (topic marker). 38 structure analysis normalizes the following linguistic expressions: predicate-argument structures. 3 Information retrieval exploiting predicate-argument structures • relative clause • passive voice (the predicate is normalized to active voice) 3.1 Overview Our key idea i"
I13-1005,I08-1025,1,0.929082,"that in a document, a mismatch of dependency type can cause another problem. This is because previous models did not distinguish dependency types. For example, the dependency “YouTube←acquire” in query sentence (2) can be found in the following irrelevant document. Introduction Most conventional approaches to information retrieval (IR) deal with words as independent terms. In query sentences1 and documents, however, dependencies exist between words.2 To capture these dependencies, some extended IR models have been proposed in the last decade (Jones, 1999; Lee et al., 2006; Song et al., 2008; Shinzato et al., 2008). These models, however, did not achieve consistent significant improvements over models based on independent words. One of the reasons for this is the linguistic variations of syntax, that is, languages are syntactically expressed in various ways. For instance, the same or similar meaning can be expressed using the passive voice or the active voice in a sentence. Previous approaches based on dependencies cannot identify such variations. This is because they use the output of a dependency parser, which generates syntactic (grammatical) dependencies built (3) Google acquired PushLife for $25M ."
I13-1005,N06-1023,1,0.839455,"Missing"
I13-1005,J03-4003,0,\N,Missing
I13-1005,P06-1128,0,\N,Missing
I13-1019,C00-1004,0,0.0299959,"using only simple rules, and the lexicon augmentation approach would be better for them. However, this is not true for onomatopoeias. Although Japanese is rich in onomatopoeias and some of them do not Introduction Morphological analysis is the ﬁrst step in many natural language applications. Since words are not segmented by explicit delimiters in Japanese, Japanese morphological analysis consists of two subtasks: word segmentation and part-of-speech (POS) tagging. Japanese morphological analysis has successfully adopted lexicon-based approaches for newspaper articles (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004), in which an input sentence is transformed into a lattice of candidate words using a pre-deﬁned lexicon, and an optimal path in the lattice is then selected. Figure 1 shows an example of a word lattice for morphological analysis and an optimal path. Since the transformation from a sentence into a word lattice basically depends on the pre-deﬁned lexicon, the existence of unknown words, i.e., words that are not included in the predeﬁned lexicon, is a major problem in Japanese morphological analysis. There are two major approaches to this problem: one is to augment the lexico"
I13-1019,P12-1109,0,0.0250511,"and thus, all the variations cannot be covered by a dictionary. In addition, as mentioned above, since we [adjective] Figure 2: Example of a word lattice with new nodes “ぉぃ,” “ぉぃしかった,” and “でーーす.” The broken lines indicate the added nodes and paths, and the bold lines indicate the optimal path. have to take into account the adjacent word to accurately recognize rendaku, the lexical knowledge alone is not sufﬁcient for rendaku recognition. For languages other than Japanese, there is much work on text normalization that aims to handle informal expressions in social media (Beaufort et al., 2010; Liu et al., 2012; Han et al., 2012). However, their target languages are segmented languages such as English and French, and thus they can focus only on normalization. On the other hand, since Japanese is an unsegmented language, we have to also consider the word segmentation task. 3 Proposed Method 3.1 Overview We use the rule-based Japanese morphological analyzer JUMAN version 5.1 as our baseline system. Basically we only improve the method for building a word lattice and do not change the process for ﬁnding an optimal path from the lattice. That is, our proposed system only adds new nodes to the word latti"
I13-1019,C04-1066,0,0.0273809,"ana and Chinese characters. 䈚䈎䈦䈢 Lattice: BOS 䈌 [Unknown word] (at) [Verb] [particle] 䈚 䈎䈦䈢 䈆 [Unknown word] 䈪 (do) (bought) (go out) [verb] [verb] [verb] 䊷䊷 [Unknown word] 䈍䈇 䈪䊷䊷䈜 [interjection] [auxiliary verb] (hey) 2.4 Related work 䈪 (scolded) 䈜 (do) EOS [verb] (was) 䈌䈆䈚䈎䈦䈢 (delicious) Much work has been done on Japanese unknown word processing. Several approaches aimed to acquire unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and others aimed to introduce better unknown word model to morphological analyzer (Nagata, 1999; Uchimoto et al., 2001; Asahara and Matsumoto, 2004; Nakagawa and Uchimoto, 2007). However, there are few works that focus on certain types of unknown words. Kazama et al. (1999)’s work is one of them. Kazama et al. improved the morphological analyzer JUMAN to deal with the informal expressions in online chat conversations. They focused on substitution and insertion, which are also the target of this paper. However, while our approach aims to develop heuristics to ﬂexibly search the lexicon, they expanded the lexicon, and thus their approach cannot deal with an inﬁnite number of derivations, such as “冷たーーい,” and “冷ーたー いー” for the original word"
I13-1019,P10-1079,0,0.0312367,"owercases into a word, and thus, all the variations cannot be covered by a dictionary. In addition, as mentioned above, since we [adjective] Figure 2: Example of a word lattice with new nodes “ぉぃ,” “ぉぃしかった,” and “でーーす.” The broken lines indicate the added nodes and paths, and the bold lines indicate the optimal path. have to take into account the adjacent word to accurately recognize rendaku, the lexical knowledge alone is not sufﬁcient for rendaku recognition. For languages other than Japanese, there is much work on text normalization that aims to handle informal expressions in social media (Beaufort et al., 2010; Liu et al., 2012; Han et al., 2012). However, their target languages are segmented languages such as English and French, and thus they can focus only on normalization. On the other hand, since Japanese is an unsegmented language, we have to also consider the word segmentation task. 3 Proposed Method 3.1 Overview We use the rule-based Japanese morphological analyzer JUMAN version 5.1 as our baseline system. Basically we only improve the method for building a word lattice and do not change the process for ﬁnding an optimal path from the lattice. That is, our proposed system only adds new nodes"
I13-1019,C96-2202,0,0.308474,"nsformed into a lattice of candidate words using a pre-deﬁned lexicon, and an optimal path in the lattice is then selected. Figure 1 shows an example of a word lattice for morphological analysis and an optimal path. Since the transformation from a sentence into a word lattice basically depends on the pre-deﬁned lexicon, the existence of unknown words, i.e., words that are not included in the predeﬁned lexicon, is a major problem in Japanese morphological analysis. There are two major approaches to this problem: one is to augment the lexicon by acquiring unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and the other is to introduce better unknown word processing to the morphological ana162 International Joint Conference on Natural Language Processing, pages 162–170, Nagoya, Japan, 14-18 October 2013. Unknown words derived from known words Type Unknown word Rendaku* (sequential voicing) (たまご) ざけ ((tamago-)zake, sake-nog) Substitution with long sound symbols* ほんとー (troo) Substitution with lowercases* ぁなた (y0u) Substitution with normal symbols うれ∪い (h@ppy) Insertion of long sound symbols* 冷たーーーい (coooool) Insertion of lowercases* 冷たぁぁぁい (coooool) Insertion of vow"
I13-1019,D08-1045,1,0.775899,"e of candidate words using a pre-deﬁned lexicon, and an optimal path in the lattice is then selected. Figure 1 shows an example of a word lattice for morphological analysis and an optimal path. Since the transformation from a sentence into a word lattice basically depends on the pre-deﬁned lexicon, the existence of unknown words, i.e., words that are not included in the predeﬁned lexicon, is a major problem in Japanese morphological analysis. There are two major approaches to this problem: one is to augment the lexicon by acquiring unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and the other is to introduce better unknown word processing to the morphological ana162 International Joint Conference on Natural Language Processing, pages 162–170, Nagoya, Japan, 14-18 October 2013. Unknown words derived from known words Type Unknown word Rendaku* (sequential voicing) (たまご) ざけ ((tamago-)zake, sake-nog) Substitution with long sound symbols* ほんとー (troo) Substitution with lowercases* ぁなた (y0u) Substitution with normal symbols うれ∪い (h@ppy) Insertion of long sound symbols* 冷たーーーい (coooool) Insertion of lowercases* 冷たぁぁぁい (coooool) Insertion of vowel characters 冷たあああい (coooool)"
I13-1019,den-etal-2008-proper,0,0.0322159,"Missing"
I13-1019,P99-1036,0,0.307605,"new words that often consist of katakana and Chinese characters. 䈚䈎䈦䈢 Lattice: BOS 䈌 [Unknown word] (at) [Verb] [particle] 䈚 䈎䈦䈢 䈆 [Unknown word] 䈪 (do) (bought) (go out) [verb] [verb] [verb] 䊷䊷 [Unknown word] 䈍䈇 䈪䊷䊷䈜 [interjection] [auxiliary verb] (hey) 2.4 Related work 䈪 (scolded) 䈜 (do) EOS [verb] (was) 䈌䈆䈚䈎䈦䈢 (delicious) Much work has been done on Japanese unknown word processing. Several approaches aimed to acquire unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and others aimed to introduce better unknown word model to morphological analyzer (Nagata, 1999; Uchimoto et al., 2001; Asahara and Matsumoto, 2004; Nakagawa and Uchimoto, 2007). However, there are few works that focus on certain types of unknown words. Kazama et al. (1999)’s work is one of them. Kazama et al. improved the morphological analyzer JUMAN to deal with the informal expressions in online chat conversations. They focused on substitution and insertion, which are also the target of this paper. However, while our approach aims to develop heuristics to ﬂexibly search the lexicon, they expanded the lexicon, and thus their approach cannot deal with an inﬁnite number of derivations,"
I13-1019,D12-1039,0,0.0610311,"variations cannot be covered by a dictionary. In addition, as mentioned above, since we [adjective] Figure 2: Example of a word lattice with new nodes “ぉぃ,” “ぉぃしかった,” and “でーーす.” The broken lines indicate the added nodes and paths, and the bold lines indicate the optimal path. have to take into account the adjacent word to accurately recognize rendaku, the lexical knowledge alone is not sufﬁcient for rendaku recognition. For languages other than Japanese, there is much work on text normalization that aims to handle informal expressions in social media (Beaufort et al., 2010; Liu et al., 2012; Han et al., 2012). However, their target languages are segmented languages such as English and French, and thus they can focus only on normalization. On the other hand, since Japanese is an unsegmented language, we have to also consider the word segmentation task. 3 Proposed Method 3.1 Overview We use the rule-based Japanese morphological analyzer JUMAN version 5.1 as our baseline system. Basically we only improve the method for building a word lattice and do not change the process for ﬁnding an optimal path from the lattice. That is, our proposed system only adds new nodes to the word lattice built by the bas"
I13-1019,P07-2055,0,0.0238124,"䈎䈦䈢 Lattice: BOS 䈌 [Unknown word] (at) [Verb] [particle] 䈚 䈎䈦䈢 䈆 [Unknown word] 䈪 (do) (bought) (go out) [verb] [verb] [verb] 䊷䊷 [Unknown word] 䈍䈇 䈪䊷䊷䈜 [interjection] [auxiliary verb] (hey) 2.4 Related work 䈪 (scolded) 䈜 (do) EOS [verb] (was) 䈌䈆䈚䈎䈦䈢 (delicious) Much work has been done on Japanese unknown word processing. Several approaches aimed to acquire unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and others aimed to introduce better unknown word model to morphological analyzer (Nagata, 1999; Uchimoto et al., 2001; Asahara and Matsumoto, 2004; Nakagawa and Uchimoto, 2007). However, there are few works that focus on certain types of unknown words. Kazama et al. (1999)’s work is one of them. Kazama et al. improved the morphological analyzer JUMAN to deal with the informal expressions in online chat conversations. They focused on substitution and insertion, which are also the target of this paper. However, while our approach aims to develop heuristics to ﬂexibly search the lexicon, they expanded the lexicon, and thus their approach cannot deal with an inﬁnite number of derivations, such as “冷たーーい,” and “冷ーたー いー” for the original word “冷たい.” In addition, Ikeda et"
I13-1019,I08-1025,1,0.83086,"ed as others. We counted the number of different outputs for 100,000 sentences. We then calculated the estimated numbers of positive/negative changes for the sentences by using the equations: pared with trie search. However, the number of potential entries of onomatopoeias without repetition is not so large, and thus our system adds all possible entries of onomatopoeias without repetition into the trie-based lexicon in advance. 4 Experiments 4.1 Setting We used 100,000 Japanese sentences to evaluate our approach. These sentences were obtained from an open search engine infrastructure TSUBAKI (Shinzato et al., 2008), which included at least one hiragana character and consisted of more than twenty characters We ﬁrst estimated the recall. Since it is too costly to create a set of data with all unknown words annotated, we made a set of data with only our target unknown words annotated. We could apply a set of regular expressions to reduce the unknown word candidates by limiting the type of unknown words. We manually annotated 100 expressions for each type, and estimated the recall. A high recall, however, does not always imply that the proposed system performs well. It might be possible that our proposed me"
I13-1019,C00-1057,0,0.0449741,"Missing"
I13-1019,W01-0512,0,0.182595,"often consist of katakana and Chinese characters. 䈚䈎䈦䈢 Lattice: BOS 䈌 [Unknown word] (at) [Verb] [particle] 䈚 䈎䈦䈢 䈆 [Unknown word] 䈪 (do) (bought) (go out) [verb] [verb] [verb] 䊷䊷 [Unknown word] 䈍䈇 䈪䊷䊷䈜 [interjection] [auxiliary verb] (hey) 2.4 Related work 䈪 (scolded) 䈜 (do) EOS [verb] (was) 䈌䈆䈚䈎䈦䈢 (delicious) Much work has been done on Japanese unknown word processing. Several approaches aimed to acquire unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and others aimed to introduce better unknown word model to morphological analyzer (Nagata, 1999; Uchimoto et al., 2001; Asahara and Matsumoto, 2004; Nakagawa and Uchimoto, 2007). However, there are few works that focus on certain types of unknown words. Kazama et al. (1999)’s work is one of them. Kazama et al. improved the morphological analyzer JUMAN to deal with the informal expressions in online chat conversations. They focused on substitution and insertion, which are also the target of this paper. However, while our approach aims to develop heuristics to ﬂexibly search the lexicon, they expanded the lexicon, and thus their approach cannot deal with an inﬁnite number of derivations, such as “冷たーーい,” and “冷"
I13-1019,W04-3230,0,0.845235,"the lexicon augmentation approach would be better for them. However, this is not true for onomatopoeias. Although Japanese is rich in onomatopoeias and some of them do not Introduction Morphological analysis is the ﬁrst step in many natural language applications. Since words are not segmented by explicit delimiters in Japanese, Japanese morphological analysis consists of two subtasks: word segmentation and part-of-speech (POS) tagging. Japanese morphological analysis has successfully adopted lexicon-based approaches for newspaper articles (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004), in which an input sentence is transformed into a lattice of candidate words using a pre-deﬁned lexicon, and an optimal path in the lattice is then selected. Figure 1 shows an example of a word lattice for morphological analysis and an optimal path. Since the transformation from a sentence into a word lattice basically depends on the pre-deﬁned lexicon, the existence of unknown words, i.e., words that are not included in the predeﬁned lexicon, is a major problem in Japanese morphological analysis. There are two major approaches to this problem: one is to augment the lexicon by acquiring unkno"
I13-1020,I05-3018,0,0.0286895,"est score on at least one corpus (Tseng et al., 177 F 95.26 95.40 95.73 95.40 95.46 95.65 Table 6. F-measure on CTB7 test set compared with previous work. “+”: semisupervised systems. System Tseng 05 Asahara 05 Chen 05 Best closed Zhang 07 Zhao 07 Baseline MaxSub-S MaxSub-L+ AS 94.7 95.2 94.5 95.2 95.1 95.5 95.07 95.17 95.34 CityU 94.3 94.1 94.0 94.3 95.1 95.6 94.53 94.61 94.79 MSR 96.4 95.8 96.0 96.4 97.2 97.5 96.25 96.42 96.64 PKU 95.0 94.1 95.0 95.0 95.1 95.4 95.13 95.31 95.55 Table 7. F-measure on SIGHAN Bakeoff-2005 test set compared with previous work. “+”: semisupervised systems. 2005; Asahara et al., 2005; Chen et al., 2005). “Best closed” summarizes the best official results on all four corpora. “Zhao 07” and “Zhang 06” represent the supervised segmentation systems in (Zhao and Kit, 2007; Zhang et al., 2006). “Baseline”, “Maxsub-Test” and “MaxSub-U” refer to the same systems as in Table 5. For the unlabeled data, we have used the test sets of corresponding corpora for “MaxSub-Test”, and the Chinese Gigaword for “MaxSub-U”. Other parameters were left unchanged. The results do not indicate that our approach performs better than other systems. However, this is largely because of our baseline not"
I13-1020,W02-1001,0,0.013634,"sult in the best performance on the CTB7 dev set. We have used two different types of unlabeled data. One is the test set itself, which means the system is purely supervised. Another is a largescale dataset, which is the Chinese Gigaword Second Edition (LDC2007T03). This dataset is a collection of news articles from 1991 to 2004 published by Central News Agency (Taiwan), Xinhua News Agency and Lianhe Zaobao Newspaper. It includes a total amount of over 1.2 billion characters in both simplified Chinese and traditional Chinese. We have trained all models using the averaged perceptron algorithm (Collins, 2002), which we selected because of its efficiency and stability. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data with a frequency higher than a threshold, . This threshold was tuned using the development data. In order to use the maximized substring features, we have used training data as unlabeled data for supervised models, and used both the training data and Chinese Gigaword for semi-supervised models. We have applied the same parameters for all models, which are tuned on the CTB7 dev set: , , , and . We have used precision,"
I13-1020,I05-3017,0,0.0426989,"s “normal”, and all other cases are “infrequent”. 176 To evaluate our approach, we have conducted word segmentation experiments on two datasets. The first is Chinese Treebank 7 (CTB7), which is a widely used version of the Penn Chinese Treebank dataset for the evaluations of word segmentation techniques. We have adopted the same setting of data division as (Wang et al., 2011): the training set, dev set and test set. For CTB7, these sets have 31,131, 10,136 and 10,180 sentences respectively. The second dataset is the second international Chinese word segmentation bakeoff (SIGHAN Bakeoff-2005) (Emerson, 2005), which has four independent subsets: the Academia Sinica Corpus (AS), the Microsoft Research Corpus (MSR), the Hong Kong City University Corpus (CityU) and the Peking University Corpus (PKU). Since POS tags are not available in this dataset, we have omitted all templates that include them. The models and parameters applied on all test sets are those that result in the best performance on the CTB7 dev set. We have used two different types of unlabeled data. One is the test set itself, which means the system is purely supervised. Another is a largescale dataset, which is the Chinese Gigaword Se"
I13-1020,I05-3019,0,0.0295139,"one corpus (Tseng et al., 177 F 95.26 95.40 95.73 95.40 95.46 95.65 Table 6. F-measure on CTB7 test set compared with previous work. “+”: semisupervised systems. System Tseng 05 Asahara 05 Chen 05 Best closed Zhang 07 Zhao 07 Baseline MaxSub-S MaxSub-L+ AS 94.7 95.2 94.5 95.2 95.1 95.5 95.07 95.17 95.34 CityU 94.3 94.1 94.0 94.3 95.1 95.6 94.53 94.61 94.79 MSR 96.4 95.8 96.0 96.4 97.2 97.5 96.25 96.42 96.64 PKU 95.0 94.1 95.0 95.0 95.1 95.4 95.13 95.31 95.55 Table 7. F-measure on SIGHAN Bakeoff-2005 test set compared with previous work. “+”: semisupervised systems. 2005; Asahara et al., 2005; Chen et al., 2005). “Best closed” summarizes the best official results on all four corpora. “Zhao 07” and “Zhang 06” represent the supervised segmentation systems in (Zhao and Kit, 2007; Zhang et al., 2006). “Baseline”, “Maxsub-Test” and “MaxSub-U” refer to the same systems as in Table 5. For the unlabeled data, we have used the test sets of corresponding corpora for “MaxSub-Test”, and the Chinese Gigaword for “MaxSub-U”. Other parameters were left unchanged. The results do not indicate that our approach performs better than other systems. However, this is largely because of our baseline not being optimized for"
I13-1020,J04-1004,0,0.199407,"machine learning techniques have boosted the performance of CWS systems. On the other hand, a major difficulty in CWS is the problem of identifying out-of-vocabulary (OOV) words, as the Chinese language is continually and rapidly evolving, particularly with the rapid growth of the internet. A recent line of research to overcome this difficulty is through exploiting characteristics of frequent substrings in unlabeled data. Statistical criteria for measuring the likelihood of a substring being a word have been proposed in previous studies of unsupervised segmentation, such as accessor variety (Feng et al., 2004) and branching entropy (Jin and Tanaka-Ishii, 2006). This kind of criteria has been applied to enhance the performance of supervised segmentation systems (Zhao and Kit, 2007; Zhao and Kit, 2008; Sun and Xu, 2011) by identifying unknown word boundaries. In this paper, instead of investigating statistical characteristics of batched substrings, we propose a novel method that extracts substrings as reliable word boundary estimations. The technique uses large-scale unlabeled data, and processes it on the fly. To illustrate the idea, we first consider the following example taken from a scientific te"
I13-1020,P06-2056,0,0.120261,"the performance of CWS systems. On the other hand, a major difficulty in CWS is the problem of identifying out-of-vocabulary (OOV) words, as the Chinese language is continually and rapidly evolving, particularly with the rapid growth of the internet. A recent line of research to overcome this difficulty is through exploiting characteristics of frequent substrings in unlabeled data. Statistical criteria for measuring the likelihood of a substring being a word have been proposed in previous studies of unsupervised segmentation, such as accessor variety (Feng et al., 2004) and branching entropy (Jin and Tanaka-Ishii, 2006). This kind of criteria has been applied to enhance the performance of supervised segmentation systems (Zhao and Kit, 2007; Zhao and Kit, 2008; Sun and Xu, 2011) by identifying unknown word boundaries. In this paper, instead of investigating statistical characteristics of batched substrings, we propose a novel method that extracts substrings as reliable word boundary estimations. The technique uses large-scale unlabeled data, and processes it on the fly. To illustrate the idea, we first consider the following example taken from a scientific text: “使一致认定界限数的期望值近似于一致正确界限 数的期望值，求得一致认定界限的期望值/认定界 限"
I13-1020,P09-1058,0,0.332013,"The remainder of this paper is organized as follows. Section 2 describes our baseline segmentation system, defines maximized substrings, and proposes an efficient algorithm for retrieving these substrings from unlabeled data. Section 3 172 7 or more BB2B3M...ME introduces the maximized substring features. Section 4 presents the experimental results. Section 5 discusses related work. The final section summarizes our conclusions. 2. Approach 2.1 Baseline Segmentation System We have used a word-character hybrid model as our baseline Chinese word segmentation system (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). As shown in Figure 1, this hybrid model constructs a lattice that consists of wordlevel and character-level nodes from a given input sentence. Word-level nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Characterlevel nodes have special tags called position-ofcharacter (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tagset is illustrated in Table 2. Previous studies have shown that jointly processing word segmentation and"
I13-1020,J09-4006,0,0.0128228,"r impacts. Although the idea behind co-occurrence sub-sequence is similar with maximized substrings, there are several restrictions: it requires post-processing to remove overlapping instances; sub-sequences are retrievable only from different sentences; and the retrieval is performed only on training and testing data. In (Sun and Xu, 2011), the authors proposed a semi-supervised segmentation system enhanced with multiple statistical criteria. Large-scale unlabeled data were used in their experiments. Li and Sun presented a model to learn features of word delimiters from punctuation marks in (Li and Sun, 2009). Wang et al. proposed a semisupervised word segmentation method that took advantages from auto-analyzed data (Wang et al., 2011). Nakagawa showed the advantage of the hybrid model combining both character-level information and word-level information in Chinese and Japanese word segmentation (Nakagawa, 2004). In (Nakagawa and Uchimoto, 2007) and (Kruengkrai et al., 2009a; 2009b) the researchers presented word-character hybrid models for joint word segmentation and POS tagging, and achieved the state-of-the-art accuracy on Chinese and Japanese datasets. 6. Conclusion We propose a simple yet eff"
I13-1020,C04-1067,0,0.206711,"izes our conclusions. 2. Approach 2.1 Baseline Segmentation System We have used a word-character hybrid model as our baseline Chinese word segmentation system (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). As shown in Figure 1, this hybrid model constructs a lattice that consists of wordlevel and character-level nodes from a given input sentence. Word-level nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Characterlevel nodes have special tags called position-ofcharacter (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tagset is illustrated in Table 2. Previous studies have shown that jointly processing word segmentation and part-of-speech tagging is preferable to separate processing, which can propagate errors (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). If the training data was annotated by part-of-speech tags, we have combined them with both wordlevel and character-level nodes. XYZ ABC ABC ABCC JQK … XYZ ABC “ABCCKID…” ABC ABCC ABCK “ABCCFAL…” JQK ABCK ABCMN “ABCCTEA…” … ABCCFA “ABCCFAL…” “ABCCTEA…” ABCMN"
I13-1020,P07-2055,0,0.0621851,"Missing"
I13-1020,I11-1035,0,0.101994,"itional source of information, “MaxSub-Test” outperforms the baseline method by 0.14 points in F-score. This indicates that our method of using maximized substrings can enhance the segmentation performance even with a purely supervised approach. The improvement increases to 0.47 points in F-score for “MaxSub-U”, which demonstrates the effectiveness of using largescale unlabeled data. We have compared our approach with previous work in Table 6. Two methods from (Kruengkrai et al., 2009a; 2009b) are referred to as “Kruengkrai 09a” and “Kruengkrai 09b”, and are taken directly from the report of (Wang et al., 2011). “Wang 11” refers to the semi-supervised system in (Wang et al., 2011). We have observed that our system “MaxSub-U” achieves the best segmentation among these systems. Also, although the performance of our baseline is lower than the systems “Kruengkrai 09a” and “Kruengkrai 09b” because of differences in implementation, the system “MaxSub-Test” (which has used no external resource) has achieved a comparable result. The results for the SIGHAN Bakeoff-2005 dataset are shown in Table 7. The first three rows (“Tseng 05”, “Asahara 05” and “Chen 05”) show the results of systems that have reached the"
I13-1020,D11-1090,0,0.093584,"ually and rapidly evolving, particularly with the rapid growth of the internet. A recent line of research to overcome this difficulty is through exploiting characteristics of frequent substrings in unlabeled data. Statistical criteria for measuring the likelihood of a substring being a word have been proposed in previous studies of unsupervised segmentation, such as accessor variety (Feng et al., 2004) and branching entropy (Jin and Tanaka-Ishii, 2006). This kind of criteria has been applied to enhance the performance of supervised segmentation systems (Zhao and Kit, 2007; Zhao and Kit, 2008; Sun and Xu, 2011) by identifying unknown word boundaries. In this paper, instead of investigating statistical characteristics of batched substrings, we propose a novel method that extracts substrings as reliable word boundary estimations. The technique uses large-scale unlabeled data, and processes it on the fly. To illustrate the idea, we first consider the following example taken from a scientific text: “使一致认定界限数的期望值近似于一致正确界限 数的期望值，求得一致认定界限的期望值/认定界 限数的值。” Without any knowledge of the Chinese language one may still notice that some substrings like “一致” and “的期望值”, occur multiple times in the sentence and are"
I13-1020,I05-3027,0,0.089569,"Missing"
I13-1020,Y06-1012,0,0.024426,"n System We have used a word-character hybrid model as our baseline Chinese word segmentation system (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). As shown in Figure 1, this hybrid model constructs a lattice that consists of wordlevel and character-level nodes from a given input sentence. Word-level nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Characterlevel nodes have special tags called position-ofcharacter (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tagset is illustrated in Table 2. Previous studies have shown that jointly processing word segmentation and part-of-speech tagging is preferable to separate processing, which can propagate errors (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). If the training data was annotated by part-of-speech tags, we have combined them with both wordlevel and character-level nodes. XYZ ABC ABC ABCC JQK … XYZ ABC “ABCCKID…” ABC ABCC ABCK “ABCCFAL…” JQK ABCK ABCMN “ABCCTEA…” … ABCCFA “ABCCFAL…” “ABCCTEA…” ABCMN “ABCCFAT…” “ABCCDEA…” “ABCCDEA…” Hash1 Hash2 Occur(ABCC) Has"
I13-1020,P06-2123,0,0.0449809,"5 Chen 05 Best closed Zhang 07 Zhao 07 Baseline MaxSub-S MaxSub-L+ AS 94.7 95.2 94.5 95.2 95.1 95.5 95.07 95.17 95.34 CityU 94.3 94.1 94.0 94.3 95.1 95.6 94.53 94.61 94.79 MSR 96.4 95.8 96.0 96.4 97.2 97.5 96.25 96.42 96.64 PKU 95.0 94.1 95.0 95.0 95.1 95.4 95.13 95.31 95.55 Table 7. F-measure on SIGHAN Bakeoff-2005 test set compared with previous work. “+”: semisupervised systems. 2005; Asahara et al., 2005; Chen et al., 2005). “Best closed” summarizes the best official results on all four corpora. “Zhao 07” and “Zhang 06” represent the supervised segmentation systems in (Zhao and Kit, 2007; Zhang et al., 2006). “Baseline”, “Maxsub-Test” and “MaxSub-U” refer to the same systems as in Table 5. For the unlabeled data, we have used the test sets of corresponding corpora for “MaxSub-Test”, and the Chinese Gigaword for “MaxSub-U”. Other parameters were left unchanged. The results do not indicate that our approach performs better than other systems. However, this is largely because of our baseline not being optimized for these corpora. Nevertheless, when compared with the baseline, our approach has yielded consistent improvements across the four corpora, and on the PKU corpus we have performed better than"
I13-1030,P07-2045,0,0.0133813,"Missing"
I13-1030,P07-1016,0,0.0276113,"ration, ranging from simple edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) proposed the concept of ‘semantic transliteration’, which is the consideration of inherent semantic information in transliterations. Their example is the inﬂuence of the source language and gender of foreign names on their transliterations into Chinese. Hagiwara and Sekine (2012) expanded upon this idea by considering a ‘latent class’ 1 Introduction A large, high-quality bilingual lexicon is of great utility to any dictionary-based system that processes bilingual data. The abili"
I13-1030,D09-1092,0,0.700347,"o recover the letter sequence ‘gli’ than if it were originally French. While these methods consider limited semantic features, they do not make use of the rich contextual information available from comparable corpora. We show such contextual information, in the form of bilingual topic distributions, to be highly eﬀective in generating transliterations. Bilingual lexicon mining from non-parallel data has been tackled in recent research such as Tamura et al. (2012) and Haghighi et al. (2008), and we build upon the techniques of multilingual topic extraction from Wikipedia pioneered by Ni et al. (2009). Previous research in lexicon mining has tended to focus on semantic features, such as context similarity vectors and topic models, but these have yet to be applied to the task of transliteration mining. We use the word-topic distribution similarities explored in Vulić et al. (2011) as baseline word similarity measures. In some cases it is possible to use monolingual corpora for transliteration mining, as English is often written alongside transliterations (Kaji et al., 2011), however we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー"
I13-1030,I08-6006,0,0.1384,"eindependent transliteration framework applicable to bilingual lexicon extraction. Our approach is to employ a bilingual topic model to enhance the output of a state-of-the-art graphemebased transliteration baseline. We demonstrate that this method is able to extract a high-quality bilingual lexicon from a comparable corpus, and we extend the topic model to propose a solution to the out-of-domain problem. 2 Previous Work Previous work has considered various methods for transliteration, ranging from simple edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) prop"
I13-1030,W10-2408,0,0.129572,"le to bilingual lexicon extraction. Our approach is to employ a bilingual topic model to enhance the output of a state-of-the-art graphemebased transliteration baseline. We demonstrate that this method is able to extract a high-quality bilingual lexicon from a comparable corpus, and we extend the topic model to propose a solution to the out-of-domain problem. 2 Previous Work Previous work has considered various methods for transliteration, ranging from simple edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) proposed the concept of ‘semantic transliteration’, whic"
I13-1030,P11-2010,0,0.0543863,"Missing"
I13-1030,P03-1021,0,0.00875904,"(Kaji et al., 2011), however we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー co m p u t e r Figure 1: Example of Japanese–English transliteration phrase alignment. tion of an easily reproducable baseline system. We use the default conﬁguration of Moses (Koehn et al., 2007) to train our baseline system, with the distortion limit set to 1 (as transliteration requires monotonic alignment). Character alignment is performed by GIZA++ (Och and Ney, 2003) with the ‘grow-diag-ﬁnal’ heuristic for training. We apply standard tuning with MERT (Och, 2003) on the BLEU (Papineni et al., 2001) score. The language model is built with SRILM (Stolcke, 2002) using Kneser-Ney smoothing (Kneser and Ney, 1995). The system described above has been implemented as speciﬁed in previous work such as Matthews (2007) (Chinese and Arabic), Hong et al. (2009) (Korean), and Antony et al. (2010) (Kannada). We demonstrate that this standard, highly-regarded baseline can be greatly improved with our proposed method. 4 Semantic Model Having set up the baseline system, we turn to the task of combining a semantic model with our transliteration engine. We employ the met"
I13-1030,W12-4404,0,0.0779492,"edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) proposed the concept of ‘semantic transliteration’, which is the consideration of inherent semantic information in transliterations. Their example is the inﬂuence of the source language and gender of foreign names on their transliterations into Chinese. Hagiwara and Sekine (2012) expanded upon this idea by considering a ‘latent class’ 1 Introduction A large, high-quality bilingual lexicon is of great utility to any dictionary-based system that processes bilingual data. The ability to automatically generate such a l"
I13-1030,J03-1002,0,0.00413423,"e monolingual corpora for transliteration mining, as English is often written alongside transliterations (Kaji et al., 2011), however we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー co m p u t e r Figure 1: Example of Japanese–English transliteration phrase alignment. tion of an easily reproducable baseline system. We use the default conﬁguration of Moses (Koehn et al., 2007) to train our baseline system, with the distortion limit set to 1 (as transliteration requires monotonic alignment). Character alignment is performed by GIZA++ (Och and Ney, 2003) with the ‘grow-diag-ﬁnal’ heuristic for training. We apply standard tuning with MERT (Och, 2003) on the BLEU (Papineni et al., 2001) score. The language model is built with SRILM (Stolcke, 2002) using Kneser-Ney smoothing (Kneser and Ney, 1995). The system described above has been implemented as speciﬁed in previous work such as Matthews (2007) (Chinese and Arabic), Hong et al. (2009) (Korean), and Antony et al. (2010) (Kannada). We demonstrate that this standard, highly-regarded baseline can be greatly improved with our proposed method. 4 Semantic Model Having set up the baseline system, we"
I13-1030,W09-3524,0,0.31861,"ethod is able to extract a high-quality bilingual lexicon from a comparable corpus, and we extend the topic model to propose a solution to the out-of-domain problem. 2 Previous Work Previous work has considered various methods for transliteration, ranging from simple edit distance and noisy-channel models (Brill et al., 2001) to conditional random ﬁelds (Ganesh et al., 2008) and ﬁnite state automata (Noeman and Madkour, 2010). We construct a baseline by modelling transliteration as a PhraseBased Statistical Machine Translation (PBSMT) task, a popular and well-studied approach (Matthews, 2007; Hong et al., 2009; Antony et al., 2010). The vast majority of previous work on transliteration has considered only lexical features, for example spelling similarity and transliteration symbol mapping, however we build on the inspiration of Li et al. (2007) and later Hagiwara and Sekine (2012), who introduced semantic features to a transliteration model. Li et al. (2007) proposed the concept of ‘semantic transliteration’, which is the consideration of inherent semantic information in transliterations. Their example is the inﬂuence of the source language and gender of foreign names on their transliterations into"
I13-1030,2001.mtsummit-papers.68,0,0.0205042,"ever we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー co m p u t e r Figure 1: Example of Japanese–English transliteration phrase alignment. tion of an easily reproducable baseline system. We use the default conﬁguration of Moses (Koehn et al., 2007) to train our baseline system, with the distortion limit set to 1 (as transliteration requires monotonic alignment). Character alignment is performed by GIZA++ (Och and Ney, 2003) with the ‘grow-diag-ﬁnal’ heuristic for training. We apply standard tuning with MERT (Och, 2003) on the BLEU (Papineni et al., 2001) score. The language model is built with SRILM (Stolcke, 2002) using Kneser-Ney smoothing (Kneser and Ney, 1995). The system described above has been implemented as speciﬁed in previous work such as Matthews (2007) (Chinese and Arabic), Hong et al. (2009) (Korean), and Antony et al. (2010) (Kannada). We demonstrate that this standard, highly-regarded baseline can be greatly improved with our proposed method. 4 Semantic Model Having set up the baseline system, we turn to the task of combining a semantic model with our transliteration engine. We employ the method of bilingual LDA (Mimno et al.,"
I13-1030,W10-2405,0,0.0252588,"007) to train our baseline system, with the distortion limit set to 1 (as transliteration requires monotonic alignment). Character alignment is performed by GIZA++ (Och and Ney, 2003) with the ‘grow-diag-ﬁnal’ heuristic for training. We apply standard tuning with MERT (Och, 2003) on the BLEU (Papineni et al., 2001) score. The language model is built with SRILM (Stolcke, 2002) using Kneser-Ney smoothing (Kneser and Ney, 1995). The system described above has been implemented as speciﬁed in previous work such as Matthews (2007) (Chinese and Arabic), Hong et al. (2009) (Korean), and Antony et al. (2010) (Kannada). We demonstrate that this standard, highly-regarded baseline can be greatly improved with our proposed method. 4 Semantic Model Having set up the baseline system, we turn to the task of combining a semantic model with our transliteration engine. We employ the method of bilingual LDA (Mimno et al., 2009), an extension of monolingual Latent Dirichlet Allocation (LDA) (Blei et al., 2003) as the semantic model. Monolingual LDA takes as its input a set of monolingual documents and generates a wordtopic distribution ϕ classifying words appearing in these documents into semantically simila"
I13-1030,P95-1050,0,0.391041,"features used for SVM training are baseline, Cos, Cue and KL scores. The similarity measures Cos, Cue and KL are deﬁned below. Figure 2: Graphical model for Bilingual LDA with K topics, D document pairs and hyperparameters α and β. Topics for each document are sampled from the common distribution θ, and the two languages have word-topic distributions ϕ and ψ. 4.1 Motivation for Bilingual LDA We choose to employ a bilingual topic model to measure semantic similarity (i.e. topic similarity) of word pairs rather than the more intuitive method of comparing monolingual context similarity vectors (Rapp, 1995) for reasons of robustness and scalability. Measuring context similarity on a word level requires a bilingual lexicon to match crosslanguage word pairs and such bilingual data is often expensive or unavailable. There are also problems with directly comparing collocations and word concurrence of distant language pairs as they do not always correspond predictably. Therefore our proposed method provides a more robust approach using coarser semantic features. The use of topic models as a semantic similarity measure is a scalable method because document-aligned bilingual training data is growing ev"
I13-1030,D11-1089,0,0.0461589,"c distributions, to be highly eﬀective in generating transliterations. Bilingual lexicon mining from non-parallel data has been tackled in recent research such as Tamura et al. (2012) and Haghighi et al. (2008), and we build upon the techniques of multilingual topic extraction from Wikipedia pioneered by Ni et al. (2009). Previous research in lexicon mining has tended to focus on semantic features, such as context similarity vectors and topic models, but these have yet to be applied to the task of transliteration mining. We use the word-topic distribution similarities explored in Vulić et al. (2011) as baseline word similarity measures. In some cases it is possible to use monolingual corpora for transliteration mining, as English is often written alongside transliterations (Kaji et al., 2011), however we consider the more general setting where such information is unavailable. 3 ko n p yu u ta a コ ン ピ ュ ー タ ー co m p u t e r Figure 1: Example of Japanese–English transliteration phrase alignment. tion of an easily reproducable baseline system. We use the default conﬁguration of Moses (Koehn et al., 2007) to train our baseline system, with the distortion limit set to 1 (as transliteration re"
I13-1030,D12-1003,0,\N,Missing
I13-1030,I05-1060,1,\N,Missing
I13-1030,P02-1040,0,\N,Missing
I13-1030,P08-1088,0,\N,Missing
I13-1030,P11-2084,0,\N,Missing
I13-1124,D07-1111,0,0.0500263,"Missing"
I13-1124,D09-1060,0,0.106189,"uncle nodes are also considered as other features. Similarly, we use leftmost and rightmost uncle nodes. 4 Experiments 4.1 Experimental Settings We first experiment on English, Chinese and Japanese. For English, we employ MSTparser1 as a base dependency parser and use sections 02 to 21 from Wall Street Journal (WSJ) corpus in Penn Treebank (PTB) to train a dependency parsing model. Then, we use section 22 from WSJ to apply the dependency parsing model to acquire the training data for dependency classification. MXPOST2 tagger is used for English automatic POS tagging. For Chinese, we use CNP (Chen et al., 2009) parser to train a dependency parser using section 1 to 270, 400 to 931 and 1001 to 1151 from Penn Chinese Treebank (CTB). Sections 301 to 325 are used to apply dependency parsing to acquire training data for dependency classification. We use MMA (Kruengkrai et al., 2009) to apply both segmentation and POS tagging. Different from the previous two languages which take words as the basic unit, experiments on Japanese are based on the unit of the phrase segments bunsetsu. We first use JUMAN3 for Japanese morphological analysis. Then KNP4 is utilized for Japanese dependency parsing. Section 950112"
I13-1124,W06-1604,0,0.0820275,"Missing"
I13-1124,W11-0314,0,0.0322624,"Missing"
I13-1124,P11-1109,1,0.837391,"Missing"
I13-1124,N06-1023,1,0.740308,"Missing"
I13-1124,kawahara-kurohashi-2010-acquiring,1,0.871315,"Missing"
I13-1124,I08-2097,1,0.901315,"Missing"
I13-1124,P09-1058,0,0.0510456,"use sections 02 to 21 from Wall Street Journal (WSJ) corpus in Penn Treebank (PTB) to train a dependency parsing model. Then, we use section 22 from WSJ to apply the dependency parsing model to acquire the training data for dependency classification. MXPOST2 tagger is used for English automatic POS tagging. For Chinese, we use CNP (Chen et al., 2009) parser to train a dependency parser using section 1 to 270, 400 to 931 and 1001 to 1151 from Penn Chinese Treebank (CTB). Sections 301 to 325 are used to apply dependency parsing to acquire training data for dependency classification. We use MMA (Kruengkrai et al., 2009) to apply both segmentation and POS tagging. Different from the previous two languages which take words as the basic unit, experiments on Japanese are based on the unit of the phrase segments bunsetsu. We first use JUMAN3 for Japanese morphological analysis. Then KNP4 is utilized for Japanese dependency parsing. Section 950112, 950113 and 9509ED from Kyoto Corpus are used to apply dependency parsing and acquire training data for dependency selection. We employ SVM-Light5 with polynomial kernel (degree 3) to solve the binary classification. In order to compare with previous work by Yu et al. (2"
I13-1124,D07-1013,0,0.0180192,"ing. The experimental results on English, Chinese and 947 International Joint Conference on Natural Language Processing, pages 947–951, Nagoya, Japan, 14-18 October 2013. by labeling each dependency according to the gold standard data. All the correct dependencies are defined as reliable and vice versa. 3.2 Features for Dependency Classification Most basic features consider that word pairs are much less likely to have a dependency relation when there are punctuation between them. On the other hand, based on the fact that dependencies with longer distance always show worse parsing performance (McDonald and Nivre, 2007), distance is another important factor that reflects the difficulty of judging whether two words have a dependency relation. Yu et al. (2008) used the features mentioned above and PoS features except the word features and did not use the context features, which are described later. In addition to these basic features, we consider context features that are thought to affect the parsing performance. Table 2 lists these context features. In some more complex cases, it is also necessary to observe larger span of context. In order to learn such linguistic characteristics automatically, besides POS"
I13-1124,P07-1052,0,0.0616152,"Missing"
I13-1124,W09-1120,0,0.0431396,"Missing"
I13-1124,D11-1076,0,\N,Missing
I13-1163,2012.eamt-1.7,1,0.845483,"s. Fragments with less than 3 words may be produced in this process, and we discard them like previous studies. 4 Experiments In our experiments, we compared our proposed fragment extraction method with (Munteanu and Marcu, 2006). We manually evaluated the accuracy of the extracted fragments. Moreover, we used the extracted fragments as additional MT training data, and evaluated the effectiveness of the fragments for MT. We conducted experiments on Chinese–Japanese data. In all our experiments, we preprocessed the data by segmenting Chinese and Japanese sentences using a segmenter proposed by Chu et al. (2012) and JUMAN (Kurohashi et al., 1994) respectively. 4.1 Data 4.1.1 Parallel Corpus The parallel corpus we used is a scientific paper abstract corpus provided by JST4 and NICT5 . This corpus was created by the Japanese project “Development and Research of Chinese–Japanese Natural Language Processing Technology”, containing 680k sentences (18.2M Chinese and 21.8M Japanese tokens respectively). This corpus contains various domains such as chemistry, physics, biology and agriculture etc. 4.1.2 Quasi–Comparable Corpora The quasi–comparable corpora we used are scientific paper abstracts collected from"
I13-1163,C04-1151,0,0.226919,"several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accurately. Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments"
I13-1163,W11-1209,0,0.0172353,"wo generative alignment models for extracting parallel fragments from comparable sentences. However, the extracted fragments slightly decrease MT performance when appending them to in–domain training data. We think the reason is that because the comparable sentences are quite noisy, the alignment models cannot accurately extract parallel fragments. To solve this problem we only use alignment models for parallel fragment candidate detection, and use an accurate lexicon filter to guarantee the accuracy of the extracted parallel fragments. Besides the above studies, there are some other efforts. Hewavitharana and Vogel (2011) propose a method that calculates both the inside and outside probabilities for fragments in a comparable sentence pair, and show that the context of the sentence helps fragment extraction. However, the proposed method only can be efficient in a controlled manner that supposes the source fragment was known, and search for the target fragment. Another study uses a syntax–based alignment model to extract parallel fragments from noisy parallel data (Riesa and Marcu, 2012). Since their method is designed for noisy parallel data, we believe that the method cannot accurately extract parallel fragmen"
I13-1163,P07-2045,0,0.0250392,"rom quasi–comparable corpora. To solve this problem, we propose an accurate parallel fragment extraction system that uses an alignment model to locate the parallel fragment candidates, and uses an accurate lexicon filter to identify the truly parallel ones. Experimental results indicate that our system can accurately extract parallel fragments, and our proposed method significantly outperforms a state–of–the–art approach. Furthermore, we investigate the factors that may affect the performance of our system in detail. 1 Introduction In statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009"
I13-1163,J05-4003,0,0.53616,"(Brown et al., 1993; Koehn et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot"
I13-1163,N10-1063,0,0.0218879,"since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accuratel"
I13-1163,P09-2057,0,0.013739,"et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel"
I13-1163,P03-1010,0,0.0486948,"machine translation (SMT) (Brown et al., 1993; Koehn et al., 2007), since translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English–French, English–Arabic, English–Chinese and several European language pairs, parallel data remains a scarce resource. As non–parallel corpora are far more available, extracting parallel data from non–parallel corpora is an attractive research field. Most previous studies focus on extracting parallel sentences from comparable corpora (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; Abdul-Rauf and Schwenk, 2011). Quasi–comparable corpora that contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in– topic) or not (out–topic) (Fung and Cheung, 2004), are available in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the"
I13-1163,C12-1166,0,0.0237647,"Missing"
I13-1163,P11-2084,0,0.0442669,"Missing"
I13-1163,P06-1011,0,0.56179,"le in far larger quantities than comparable corpora. In quasi–comparable corpora, there are few or no parallel sentences. However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accurately. Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments unreliable (Munteanu and Marcu, 2006). Some studies develop alignment models for comparable sentences to extract parallel fragments (Quirk et al., 2007). Because the comparable sentences are quite noisy, the extracted fragments are not accurate. In this paper, we propose an accurate parallel fragment extraction system. We locate parallel fragment candidates using an alignment model, and use an accurate lexicon filter to identify the truly parallel ones. Experimental results on Chinese–Japanese corpora show that our proposed method significantly outperforms a state–of–the– art approach, which indicate the effectiveness of our para"
I13-1163,2007.mtsummit-papers.50,0,0.444915,"However, there could be parallel fragments in comparable sentences that are also helpful for SMT. Previous studies for parallel fragment extraction from comparable sentences have the problem that they cannot extract parallel fragments accurately. Some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus. They locate the source and target fragments independently, making the extracted fragments unreliable (Munteanu and Marcu, 2006). Some studies develop alignment models for comparable sentences to extract parallel fragments (Quirk et al., 2007). Because the comparable sentences are quite noisy, the extracted fragments are not accurate. In this paper, we propose an accurate parallel fragment extraction system. We locate parallel fragment candidates using an alignment model, and use an accurate lexicon filter to identify the truly parallel ones. Experimental results on Chinese–Japanese corpora show that our proposed method significantly outperforms a state–of–the– art approach, which indicate the effectiveness of our parallel fragment extraction system. Moreover, we investigate the factors that may affect the performance of our system"
I13-1163,N12-1061,0,0.0190235,"o guarantee the accuracy of the extracted parallel fragments. Besides the above studies, there are some other efforts. Hewavitharana and Vogel (2011) propose a method that calculates both the inside and outside probabilities for fragments in a comparable sentence pair, and show that the context of the sentence helps fragment extraction. However, the proposed method only can be efficient in a controlled manner that supposes the source fragment was known, and search for the target fragment. Another study uses a syntax–based alignment model to extract parallel fragments from noisy parallel data (Riesa and Marcu, 2012). Since their method is designed for noisy parallel data, we believe that the method cannot accurately extract parallel fragments from comparable sentences. 3 Proposed Method 3.1 System Overview Figure 1 shows an overview of our parallel fragment extraction system. We first apply comparable sentence extraction using a combination method of (Abdul-Rauf and Schwenk, 2011) (1)(2) and (Munteanu and Marcu, 2005) (3), which were originally used for extracting parallel sentences from comparable corpora. We translate the source sentences to target language with a SMT system trained on a parallel corpu"
I13-1163,J93-2003,0,\N,Missing
izumi-etal-2014-constructing,W09-3401,0,\N,Missing
izumi-etal-2014-constructing,D08-1103,0,\N,Missing
izumi-etal-2014-constructing,D12-1111,0,\N,Missing
J94-4001,J83-2002,0,0.041279,"cate) are called renyoh chuushi-ho (see example sentence (iv) of Table 1). A renyoh chuushi-ho appears in an embedded sentence to modify nouns and is also used to connect two or more sentences. This form is used frequently in Japanese and is a major cause of structural ambiguity. Many major sentential components are omitted in the posterior part of renyoh chuushi expressions, thus complicating the analysis. For the successful analysis of long sentences, these conjunctive phrases and clauses, including renyoh chuushi-ho, must be recognized correctly. Nevertheless, most work in this area (e.g., Dahl and McCord 1983; Fong and Berwick 1985; Hirschman 1986; Kaplan and Maxwell 1988; Sag et al. 1985; Sedogbo 1985; Steedman 1990; Woods 1973) has concerned the problem of creating candidate conjunctive structures or explaining correct conjunctive structures, and not the method for selecting correct structures among many candidates. A method proposed by some researchers (Agarwal and Boggess 1992; Nagao et al. 1983) for selecting the correct structure is, in outline, that the two most similar components to the left side and to the right side of a conjunction are detected as two conjoinedheads in a conjunctive str"
J94-4001,P85-1014,0,0.061048,"h chuushi-ho (see example sentence (iv) of Table 1). A renyoh chuushi-ho appears in an embedded sentence to modify nouns and is also used to connect two or more sentences. This form is used frequently in Japanese and is a major cause of structural ambiguity. Many major sentential components are omitted in the posterior part of renyoh chuushi expressions, thus complicating the analysis. For the successful analysis of long sentences, these conjunctive phrases and clauses, including renyoh chuushi-ho, must be recognized correctly. Nevertheless, most work in this area (e.g., Dahl and McCord 1983; Fong and Berwick 1985; Hirschman 1986; Kaplan and Maxwell 1988; Sag et al. 1985; Sedogbo 1985; Steedman 1990; Woods 1973) has concerned the problem of creating candidate conjunctive structures or explaining correct conjunctive structures, and not the method for selecting correct structures among many candidates. A method proposed by some researchers (Agarwal and Boggess 1992; Nagao et al. 1983) for selecting the correct structure is, in outline, that the two most similar components to the left side and to the right side of a conjunction are detected as two conjoinedheads in a conjunctive structure. For example, in"
J94-4001,C88-1061,0,0.025976,"of Table 1). A renyoh chuushi-ho appears in an embedded sentence to modify nouns and is also used to connect two or more sentences. This form is used frequently in Japanese and is a major cause of structural ambiguity. Many major sentential components are omitted in the posterior part of renyoh chuushi expressions, thus complicating the analysis. For the successful analysis of long sentences, these conjunctive phrases and clauses, including renyoh chuushi-ho, must be recognized correctly. Nevertheless, most work in this area (e.g., Dahl and McCord 1983; Fong and Berwick 1985; Hirschman 1986; Kaplan and Maxwell 1988; Sag et al. 1985; Sedogbo 1985; Steedman 1990; Woods 1973) has concerned the problem of creating candidate conjunctive structures or explaining correct conjunctive structures, and not the method for selecting correct structures among many candidates. A method proposed by some researchers (Agarwal and Boggess 1992; Nagao et al. 1983) for selecting the correct structure is, in outline, that the two most similar components to the left side and to the right side of a conjunction are detected as two conjoinedheads in a conjunctive structure. For example, in &quot;John enjoyed the book and liked the pla"
J94-4001,P92-1003,0,\N,Missing
jin-etal-2014-framework,boas-2002-bilingual,0,\N,Missing
jin-etal-2014-framework,N06-1023,1,\N,Missing
jin-etal-2014-framework,P09-1058,0,\N,Missing
jin-etal-2014-framework,P13-1085,0,\N,Missing
jin-etal-2014-framework,P90-1034,0,\N,Missing
jin-etal-2014-framework,P11-1109,1,\N,Missing
jin-etal-2014-framework,D09-1060,0,\N,Missing
jin-etal-2014-framework,P98-2127,0,\N,Missing
jin-etal-2014-framework,C98-2122,0,\N,Missing
jin-etal-2014-framework,D13-1014,0,\N,Missing
jin-etal-2014-framework,I13-1124,1,\N,Missing
jin-etal-2014-framework,korhonen-etal-2006-large,0,\N,Missing
jin-etal-2014-framework,D11-1076,0,\N,Missing
jin-etal-2014-framework,kawahara-kurohashi-2010-acquiring,1,\N,Missing
kawahara-etal-2002-construction,W99-0307,0,\N,Missing
kawahara-etal-2002-construction,H94-1020,0,\N,Missing
kawahara-etal-2002-construction,poesio-2000-annotating,0,\N,Missing
kawahara-etal-2004-toward,kawahara-etal-2002-construction,1,\N,Missing
kawahara-etal-2004-toward,poesio-etal-2002-acquiring,0,\N,Missing
kawahara-etal-2004-toward,C02-1122,1,\N,Missing
kawahara-etal-2004-toward,P99-1062,1,\N,Missing
kawahara-etal-2004-toward,P03-1023,0,\N,Missing
kawahara-kurohashi-2006-case,C02-1122,1,\N,Missing
kawahara-kurohashi-2006-case,N04-1016,0,\N,Missing
kawahara-kurohashi-2006-case,J03-3005,0,\N,Missing
kawahara-kurohashi-2006-case,J03-3001,0,\N,Missing
kawahara-kurohashi-2010-acquiring,I05-1017,1,\N,Missing
kawahara-kurohashi-2010-acquiring,W98-1505,0,\N,Missing
kawahara-kurohashi-2010-acquiring,J93-2004,0,\N,Missing
kawahara-kurohashi-2010-acquiring,H05-1059,0,\N,Missing
kawahara-kurohashi-2010-acquiring,C94-1042,0,\N,Missing
kawahara-kurohashi-2010-acquiring,D08-1093,0,\N,Missing
kawahara-kurohashi-2010-acquiring,kawahara-uchimoto-2008-method,1,\N,Missing
kawahara-kurohashi-2010-acquiring,W02-0907,0,\N,Missing
kawahara-kurohashi-2010-acquiring,A97-1052,0,\N,Missing
kawahara-kurohashi-2010-acquiring,J93-2002,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P07-1052,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P03-1007,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P98-1013,0,\N,Missing
kawahara-kurohashi-2010-acquiring,C98-1013,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P93-1032,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P98-1071,0,\N,Missing
kawahara-kurohashi-2010-acquiring,C98-1068,0,\N,Missing
kawahara-kurohashi-2010-acquiring,J05-1004,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P87-1027,0,\N,Missing
kawahara-kurohashi-2010-acquiring,P99-1051,0,\N,Missing
kawahara-kurohashi-2010-acquiring,korhonen-etal-2006-large,0,\N,Missing
kawahara-kurohashi-2010-acquiring,kawahara-kurohashi-2006-case,1,\N,Missing
kawahara-kurohashi-2010-acquiring,N01-1025,0,\N,Missing
L16-1101,P98-1013,0,0.434745,"n et al., 2009; Razmara et al., 2013). That is augmenting the translation model for the OOV words by using the translation knowledge of their paraphrases in the translation model. Previous studies use paraphrases generated by bilingual pivoting (Callison-Burch et al., 2006), distributional similarity (Marton et al., 2009), and graph propagation (Razmara et al., 2013), which suffer from high computational complexity. In this study, we propose using word embeddings (Mikolov et al., 2013) to address this problem. We also propose using semantic lexicons including WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) for paraphrasing. In addition, we apply a method to combine these two types of paraphrases (Faruqui et al., 2015), which achieves further improvements in SMT. 2. Paraphrasing Out-of-Vocabulary Words In this paper, we study on phrase based SMT (Koehn et al., 2003). Figure 1 shows an overview of our proposed method. We first construct a phrase table based on unsupervised word alignments, containing phrase pairs together with their feature scores. From the development and test -.)///)0.)///),1-./0.2)$031-./0.2)4)) -5)///)05)///),1-5"
L16-1101,N06-1003,0,0.312393,"In statistical machine translation (SMT) (Koehn et al., 2003), because translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English-French, English-Arabic, English-Chinese and several European language pairs, parallel data remains a scarce resource. Moreover, even for these language pairs, the available domains are limited. The scarceness of parallel corpora makes the coverage of the translation model low, which leads to high out-ofvocabulary (OOV) word rates when conducting translation (Callison-Burch et al., 2006). Even we have parallel corpora in sufficient size in one domain, this OOV problem occurs when the domain shifts. Irvine et al. (2013a) showed that SMT performance decreases significantly when using a system trained on one domain to translate texts in different domains mainly because of OOVs. As one of the ways to address the OOV problem, paraphrasing has been proposed (Callison-Burch et al., 2006; Marton et al., 2009; Razmara et al., 2013). That is augmenting the translation model for the OOV words by using the translation knowledge of their paraphrases in the translation model. Previous stud"
L16-1101,D13-1167,0,0.0222894,"setting. We processed the training corpus using sub-sentence splitting following (Chu et al., 2012b). The development and test sets have only one reference, which contain 1,050 and 998 sentences respectively. For more details of this task, please refer to (Federico et al., 2012). 2.3. Combination 3.2. Settings One problem of word embeddings is that they are learnt without supervision, which limits of the quality. To achieve better quality embeddings, applying the existing semantic lexicons to change the objective of embedding training (Yu and Dredze, 2014), and relation-specific augmentation (Chang et al., 2013) have been studied. Here, we apply the word embedding retrofitting method (Faruqui et al., 2015), because of its independence from the embedding learning method and efficiency. This method minimizes the following objective so that the retrofitted word embedding qi will be close to both the original embedding qˆi and its neighbor qj in the semantic lexicons: For decoding, we used the state-of-the-art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options. We trained a 5-gram language model on the Chinese side of the parallel corpus using the SRILM toolkit4 with interpolated Kn"
L16-1101,2012.eamt-1.7,1,0.855467,"h we set both to 1 in our experiments. Taking the first derivative of Ψ with respect to one qi vector, we arrive at the following online update by equating it to zero: ∑ j:(i,j)∈E βij qj + αi qˆi qi = ∑ (2) j:(i,j)∈E βij + αi Following (Faruqui et al., 2015), we run 10 iterations for this update. After retrofitting, we use the new embeddings for paraphrasing in the same manner as before. 3. Experiments We conducted English-to-Chinese translation experiments in a low resource setting. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012a), and tokenizing English sentences. 3.1. Task We conducted our experiments on the OLYMPICS task of IWSLT 2012 (Federico et al., 2012). The OLYMPICS task is carried out using parts of the HIT Olympic Trilingual Corpus (HIT) (Yang et al., 2006) and the Basic Travel Expression Corpus (BTEC) as an additional training corpus. The HIT corpus is a multilingual corpus that covers 5 domains (traveling, dining, sports, traffic and business) that are closely related to the Beijing 2008 Olympic Games. The HIT corpus contains around 52k sentences 2.8 million words in total. The BTEC corpus is a multiling"
L16-1101,2012.iwslt-evaluation.12,1,0.823586,"h we set both to 1 in our experiments. Taking the first derivative of Ψ with respect to one qi vector, we arrive at the following online update by equating it to zero: ∑ j:(i,j)∈E βij qj + αi qˆi qi = ∑ (2) j:(i,j)∈E βij + αi Following (Faruqui et al., 2015), we run 10 iterations for this update. After retrofitting, we use the new embeddings for paraphrasing in the same manner as before. 3. Experiments We conducted English-to-Chinese translation experiments in a low resource setting. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012a), and tokenizing English sentences. 3.1. Task We conducted our experiments on the OLYMPICS task of IWSLT 2012 (Federico et al., 2012). The OLYMPICS task is carried out using parts of the HIT Olympic Trilingual Corpus (HIT) (Yang et al., 2006) and the Basic Travel Expression Corpus (BTEC) as an additional training corpus. The HIT corpus is a multilingual corpus that covers 5 domains (traveling, dining, sports, traffic and business) that are closely related to the Beijing 2008 Olympic Games. The HIT corpus contains around 52k sentences 2.8 million words in total. The BTEC corpus is a multiling"
L16-1101,P11-2071,0,0.0597613,"Missing"
L16-1101,N15-1184,0,0.116725,"ase pair. PPDB is packaged in 6 sizes from S to XXXL to leverage precision and coverage, and it is also divided into to lexical and phrasal paraphrases. In our experiments, we used the lexical paraphrases in the XL size. where n is the vocabulary size; E denotes a relation in the semantic lexicons; αi and βij control the relative strengths of associations, which we set both to 1 in our experiments. Taking the first derivative of Ψ with respect to one qi vector, we arrive at the following online update by equating it to zero: ∑ j:(i,j)∈E βij qj + αi qˆi qi = ∑ (2) j:(i,j)∈E βij + αi Following (Faruqui et al., 2015), we run 10 iterations for this update. After retrofitting, we use the new embeddings for paraphrasing in the same manner as before. 3. Experiments We conducted English-to-Chinese translation experiments in a low resource setting. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012a), and tokenizing English sentences. 3.1. Task We conducted our experiments on the OLYMPICS task of IWSLT 2012 (Federico et al., 2012). The OLYMPICS task is carried out using parts of the HIT Olympic Trilingual Corpus (HIT) (Yang et al., 200"
L16-1101,2012.iwslt-evaluation.1,0,0.0309765,"Missing"
L16-1101,N13-1092,0,0.104069,"Missing"
L16-1101,W13-2233,0,0.0525157,"Missing"
L16-1101,Q13-1035,0,0.0750334,"quantity of parallel data are crucial. However, except for a few language pairs, such as English-French, English-Arabic, English-Chinese and several European language pairs, parallel data remains a scarce resource. Moreover, even for these language pairs, the available domains are limited. The scarceness of parallel corpora makes the coverage of the translation model low, which leads to high out-ofvocabulary (OOV) word rates when conducting translation (Callison-Burch et al., 2006). Even we have parallel corpora in sufficient size in one domain, this OOV problem occurs when the domain shifts. Irvine et al. (2013a) showed that SMT performance decreases significantly when using a system trained on one domain to translate texts in different domains mainly because of OOVs. As one of the ways to address the OOV problem, paraphrasing has been proposed (Callison-Burch et al., 2006; Marton et al., 2009; Razmara et al., 2013). That is augmenting the translation model for the OOV words by using the translation knowledge of their paraphrases in the translation model. Previous studies use paraphrases generated by bilingual pivoting (Callison-Burch et al., 2006), distributional similarity (Marton et al., 2009), a"
L16-1101,D13-1109,0,0.0374,"Missing"
L16-1101,N03-1017,0,0.0240435,"em in statistical machine translation (SMT) with low resources. OOV paraphrasing that augments the translation model for the OOV words by using the translation knowledge of their paraphrases has been proposed to address the OOV problem. In this paper, we propose using word embeddings and semantic lexicons for OOV paraphrasing. Experiments conducted on a low resource setting of the OLYMPICS task of IWSLT 2012 verify the effectiveness of our proposed method. Keywords: Paraphrasing, Out-of-Vocabulary Word, Word Embedding, Semantic Lexicon 1. Introduction In statistical machine translation (SMT) (Koehn et al., 2003), because translation knowledge is acquired from parallel data, the quality and quantity of parallel data are crucial. However, except for a few language pairs, such as English-French, English-Arabic, English-Chinese and several European language pairs, parallel data remains a scarce resource. Moreover, even for these language pairs, the available domains are limited. The scarceness of parallel corpora makes the coverage of the translation model low, which leads to high out-ofvocabulary (OOV) word rates when conducting translation (Callison-Burch et al., 2006). Even we have parallel corpora in"
L16-1101,P07-2045,0,0.00855169,"applying the existing semantic lexicons to change the objective of embedding training (Yu and Dredze, 2014), and relation-specific augmentation (Chang et al., 2013) have been studied. Here, we apply the word embedding retrofitting method (Faruqui et al., 2015), because of its independence from the embedding learning method and efficiency. This method minimizes the following objective so that the retrofitted word embedding qi will be close to both the original embedding qˆi and its neighbor qj in the semantic lexicons: For decoding, we used the state-of-the-art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options. We trained a 5-gram language model on the Chinese side of the parallel corpus using the SRILM toolkit4 with interpolated Kneser-Ney discounting, and used it for all the experiments. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. The skip-gram model (Mikolov et al., 2013) was trained on the English Gigaword version 5.0,5 with the word2vec tool. We removed the punctuations in the corpus, obtaining about 3.95B tokens with a vocabulary size of 854k. The context window size was set to 5, and the vector size was"
L16-1101,W04-3250,0,0.0538852,"ord2vec” denotes the system paraphrased with the word embeddings obtained by word2vec; “WordNet synonyms”, “WordNet all”, “FrameNet” and “PPDB” denote the systems paraphrased with different semantic lexicons. “Word2vec retrofitted *” denote the systems paraphrased with the word2vec word embeddings retrofitted by different semantic lexicons. They were evaluated on BLEU-4 scores and OOV rates. The OOV rate was the percentage of the OOV words out of the total number of source words in the development/test sets. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). We can see that the OOV rate of the Baseline system is high, because of the small size of the parallel corpus for training. Both Word2vec and semantic lexicons decrease the OOV rate, and thus improve the MT performance. Although Word2vec is unsupervised learnt, it shows better results than the semantic lexicons that are either manual created or collected with supervised data. The reason for this is the lower coverage of the semantic lexicons compared to Word2vec, leading to lower OOV decreases. The combination of Word2vec and the semantic lexicons by retrofitting outperforms either method, b"
L16-1101,D09-1040,0,0.38028,"Missing"
L16-1101,P03-1021,0,0.00539277,"ndence from the embedding learning method and efficiency. This method minimizes the following objective so that the retrofitted word embedding qi will be close to both the original embedding qˆi and its neighbor qj in the semantic lexicons: For decoding, we used the state-of-the-art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options. We trained a 5-gram language model on the Chinese side of the parallel corpus using the SRILM toolkit4 with interpolated Kneser-Ney discounting, and used it for all the experiments. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. The skip-gram model (Mikolov et al., 2013) was trained on the English Gigaword version 5.0,5 with the word2vec tool. We removed the punctuations in the corpus, obtaining about 3.95B tokens with a vocabulary size of 854k. The context window size was set to 5, and the vector size was set to 200. Ψ(Q) = n ∑ ∑ {αi ||qi − qˆi ||2 + βij ||qi − qj ||2 } (1) i=1 3 (i,j)∈E 4 5 http://www.cis.upenn.edu/˜ccb/ppdb/ 645 http://www.speech.sri.com/projects/srilm LDC2011T07 Method Baseline Word2vec WordNet synonyms Word2vec retrofitted by WordNet synonyms WordNet all"
L16-1101,P13-1109,0,0.249507,"parallel corpora makes the coverage of the translation model low, which leads to high out-ofvocabulary (OOV) word rates when conducting translation (Callison-Burch et al., 2006). Even we have parallel corpora in sufficient size in one domain, this OOV problem occurs when the domain shifts. Irvine et al. (2013a) showed that SMT performance decreases significantly when using a system trained on one domain to translate texts in different domains mainly because of OOVs. As one of the ways to address the OOV problem, paraphrasing has been proposed (Callison-Burch et al., 2006; Marton et al., 2009; Razmara et al., 2013). That is augmenting the translation model for the OOV words by using the translation knowledge of their paraphrases in the translation model. Previous studies use paraphrases generated by bilingual pivoting (Callison-Burch et al., 2006), distributional similarity (Marton et al., 2009), and graph propagation (Razmara et al., 2013), which suffer from high computational complexity. In this study, we propose using word embeddings (Mikolov et al., 2013) to address this problem. We also propose using semantic lexicons including WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and the Paraphra"
L16-1101,P14-2089,0,0.0257282,"entences, we consider the OLYMPICS task as a low resource setting. We processed the training corpus using sub-sentence splitting following (Chu et al., 2012b). The development and test sets have only one reference, which contain 1,050 and 998 sentences respectively. For more details of this task, please refer to (Federico et al., 2012). 2.3. Combination 3.2. Settings One problem of word embeddings is that they are learnt without supervision, which limits of the quality. To achieve better quality embeddings, applying the existing semantic lexicons to change the objective of embedding training (Yu and Dredze, 2014), and relation-specific augmentation (Chang et al., 2013) have been studied. Here, we apply the word embedding retrofitting method (Faruqui et al., 2015), because of its independence from the embedding learning method and efficiency. This method minimizes the following objective so that the retrofitted word embedding qi will be close to both the original embedding qˆi and its neighbor qj in the semantic lexicons: For decoding, we used the state-of-the-art phrase based SMT toolkit Moses (Koehn et al., 2007) with default options. We trained a 5-gram language model on the Chinese side of the para"
L16-1101,C98-1013,0,\N,Missing
L16-1348,W06-2810,0,0.0605838,"Missing"
L16-1348,P91-1022,0,0.765952,", and then align the source and target sentences based on sentence length and/or bilingual lexicons (Ma, 2006). However, the monolingually determined sentence boundaries are not optimized for sentence alignment, because translation equivalents might cross the monolingual sentence boundaries. In this paper, we propose a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy. Sentence alignment methods are generally based on two kinds of algorithms: length-based algorithms align sentences according solely to their lengths (Brown et al., 1991), while lexicon-based algorithms use lexical information to calculate similarity between source and target sentences (Ma, 2006). Length-based algorithms are typically faster but not suited for processing noisy corpus (corpus containing article pairs with omitted or wrong translations). Lexicon-based algorithms, like the one used in Champollion (Ma, 2006)1 are more robust. However their performance highly depends on the coverage and quality of the lexicon, and large-scale lexicon with high quality is not easy to obtain especially for low resource language pairs. In this paper, we propose to use"
L16-1348,Y15-1033,1,0.941209,"sentences (Ma, 2006). Length-based algorithms are typically faster but not suited for processing noisy corpus (corpus containing article pairs with omitted or wrong translations). Lexicon-based algorithms, like the one used in Champollion (Ma, 2006)1 are more robust. However their performance highly depends on the coverage and quality of the lexicon, and large-scale lexicon with high quality is not easy to obtain especially for low resource language pairs. In this paper, we propose to use lexicons generated by a pivotbased MT system, which could be constructed even for low resource languages (Dabre et al., 2015). Experiments conducted on Chinese-Japanese scientific articles verify the effectiveness of our proposed method. 2. Simultaneous Sentence Boundary Detection and Alignment Our proposed alignment method consists in the following steps 1 1. Split source and target articles into sentence candidates. 2. Normalize words in sentence candidates to maximize matching rate between words in source, target, and lexicons. 3. Compute alignment path with the highest similarity between source and target. An alignment path is composed of pairs of article segments.2 4. Adjust sentence boundaries by merging sente"
L16-1348,W04-1101,0,0.0367831,"Missing"
L16-1348,D07-1103,0,0.100786,"Missing"
L16-1348,N03-1017,0,0.00709525,"he input article pairs using pivot-based MT, achieving better coverage of the input words with fewer entries than pre-existing dictionaries. Pivot-based MT makes it possible to build dictionaries for language pairs that have scarce parallel data. The alignment method is implemented in a tool that will be freely available in the near future. Keywords: Sentence Alignment, Sentence Segmentation, Pivot-based Machine Translation 1. Introduction Sentence alignment is a task that consists in aligning the parallel sentences in a translated article pair, which are crucial for machine translation (MT) (Koehn et al., 2003). Previous studies first split the source and target articles into sentences respectively using punctuation information, and then align the source and target sentences based on sentence length and/or bilingual lexicons (Ma, 2006). However, the monolingually determined sentence boundaries are not optimized for sentence alignment, because translation equivalents might cross the monolingual sentence boundaries. In this paper, we propose a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy. Sentence alignment methods are"
L16-1348,C10-2081,0,0.0412132,"Missing"
L16-1348,ma-2006-champollion,0,0.123676,"l data. The alignment method is implemented in a tool that will be freely available in the near future. Keywords: Sentence Alignment, Sentence Segmentation, Pivot-based Machine Translation 1. Introduction Sentence alignment is a task that consists in aligning the parallel sentences in a translated article pair, which are crucial for machine translation (MT) (Koehn et al., 2003). Previous studies first split the source and target articles into sentences respectively using punctuation information, and then align the source and target sentences based on sentence length and/or bilingual lexicons (Ma, 2006). However, the monolingually determined sentence boundaries are not optimized for sentence alignment, because translation equivalents might cross the monolingual sentence boundaries. In this paper, we propose a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy. Sentence alignment methods are generally based on two kinds of algorithms: length-based algorithms align sentences according solely to their lengths (Brown et al., 1991), while lexicon-based algorithms use lexical information to calculate similarity between so"
L16-1348,moore-2002-fast,0,0.168305,"Missing"
L16-1348,2010.amta-papers.14,0,0.0401683,"Missing"
L16-1348,P94-1012,0,0.0759164,".2871 0.1824 0.7646 0.7653 0.7883 0.7932 0.7955 0.7968 0.4820 0.2372 0.5619 0.5393 0.4352 0.3049 Table 5: Sentence alignment results of the proposed method “Hard+Soft” with similarity threshold tuned on the development set for maximum F measure and precision ≥ 0.9. Dictionary None EDR MT-Noun MT-NVAA EDR+MT-Noun EDR+MT-NVAA Coverage 0.27 0.39 0.42 0.45 0.46 0.48 Table 6: Lexicons coverage on the test set. segmentation and alignment issues, taking advantage of intermediate alignment data to adjust sentence boundaries. Lexicon-based algorithms require a dictionary that may be generated offline (Wu, 1994; Ma, 2006), or online, for example by comparing the number of occurrences and distribution of words on both sides of the bilingual corpus (Kay and R¨oscheisen, 1993). Our proposed method follows an intermediate approach, where lexicons are generated automatically using a pivot-based MT system. Since lexicons are generated from words in the corpus, they achieve a high coverage ratio with a low number of entries. MT systems have previously been used for sentence alignment, by calculating similarity scores between target and MT translation of the source. (Adafre and De Rijke, 2006) uses word-lev"
L16-1348,P11-2111,0,0.0604551,"Missing"
L16-1348,J93-1006,0,\N,Missing
L16-1350,W14-7001,1,0.377751,"her unit belonging to the same paper in the translated data are extracted. Therefore, there is no sentence pairs sharing the same paper across the training, development, development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acq"
L16-1350,W15-5001,1,0.860841,"development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acquired in the workshop. WAT is unique in the following points: As described in Section 2., ASPEC-JC includes only 8 scientific fields. The distribution of the fields is s"
L16-1350,2007.mtsummit-papers.63,1,0.812108,"Missing"
L16-1468,J93-2003,0,0.0626585,"Missing"
L16-1468,D14-1179,0,0.0370302,"Missing"
L16-1468,2012.eamt-1.7,1,0.859034,"parallel sentences extracted by the baseline system. The models trained on the seed parallel corpus are used for producing the NN features for training and testing the classifier. We tried the use of both the two types of models to score the parallel sentence candidates for extraction. 3. Experiments We evaluated classification accuracy, and conducted extraction and translation experiments on Chinese-Japanese data to verify the effectiveness of our proposed NN features. In all our experiments, we preprocessed the data by segmenting Chinese and Japanese sentences using a segmenter proposed by Chu et al. (2012) and JUMAN (Kurohashi et al., 1994) respectively. 3.1. Data The seed parallel corpus we used is the Chinese-Japanese section of the Asian Scientific Paper Excerpt Corpus (ASPEC),5 containing 680k sentences pairs (18.2M Chinese and 21.8M Japanese tokens, respectively). Also, we downloaded Chinese6 (20120921) and Japanese7 (20120916) Wikipedia database dumps. We used an opensource Python script8 to extract and clean the text from the dumps. Since the Chinese dump is mixed of Traditional and Simplified Chinese, we converted all Traditional Chinese to Simplified Chinese using a conversion table pu"
L16-1468,chu-etal-2014-constructing,1,0.784656,"Missing"
L16-1468,Y15-1033,1,0.847103,"ning “+NN-ASPEC” and “+NN-WIKI” are also greatly different (680k versus 126k). As NMT is sensitive to the quality and quantity of the training data (Bahdanau et al., 2014), “+NN-ASPEC” outperformed “+NN-WIKI”. 4. Related Work Several studies have exploited the NN features for SMT. (Sutskever et al., 2014) used the NN features to rerank the N-best list of a SMT system, which achieved a BLEU score that is close to the previous state of the art. (Cho et al., 2014) scored the phrase pairs of a SMT system with a neural translation model, and used the scores as additional NN features for decoding. (Dabre et al., 2015) used the NN features for a pivot-based SMT system for dictionary construction. In contrast, we score the sentence pairs of with a neural translation model, and use the scores as NN features for parallel sentence extraction from comparable corpora. 5. Conclusion In this paper, we incorporated the NN features for parallel sentence extraction from comparable corpora for the first time. Experimental results verified the effectiveness of NN features for this task. As future work, we plan to address the domain problem of “+NN-ASPEC” by a NN based sentence selection method. Namely, we train NN model"
L16-1468,P07-2045,0,0.00628691,"Missing"
L16-1468,J05-4003,0,0.739012,"o be viewed as bilingual language models, they can be used to generate scores for candidate translations as neural network (NN) features for reranking the n-best lists produced by a statistical machine translation (SMT) system, whose quality rivals the state of the art (Sutskever et al., 2014). Comparable corpora are a set of monolingual corpora that describe roughly the same topic in different languages. Although they are not exact translation equivalents of each other, there are a large amount of parallel sentences contained in the comparable texts. The task of parallel sentence extraction (Munteanu and Marcu, 2005) is to identify truly parallel sentences from the erroneous ones from comparable corpora. Intuitively, because the NN features give a measure of the bilingual similarity of a sentence pair, they could be helpful for this task. However, this assumption has not been verified previously. In this paper, we incorporate the NN features into a robust parallel sentence extraction system (Chu et al., 2014), which consists of a parallel sentence candidate filter and a binary classifier for parallel sentence identification. The NN features are naturally used as additional features for the classifier. Exp"
L18-1050,O14-4001,0,0.02906,"Missing"
L18-1050,P14-2082,0,0.0160505,"e relationship between event information and time information in text. For example, temporal ordering of events that estimates the temporal relations of event-event and event-time was studied in TempEval 1, 2, 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), and the timeline generation task that links event and time in multiple documents was studied in SemEval 15 (Minard et al., 2015). In order to train models and evaluate results in these tasks, corpora in which event information is correlated with time information in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the expressions with weak temporality are interpreted. To understand temporal information in text exhaustively, we propose an annotation scheme that anchors various expressions to the time axis, reflecting personal interpretation of text and common sense. Using this scheme, we annotate Kyoto University Text Corpus (Kawahara et al., 2002), which is a Japanese newspaper corpus annotated with"
L18-1050,D16-1005,0,0.0404101,"Missing"
L18-1050,kawahara-etal-2002-construction,1,0.603761,"in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the expressions with weak temporality are interpreted. To understand temporal information in text exhaustively, we propose an annotation scheme that anchors various expressions to the time axis, reflecting personal interpretation of text and common sense. Using this scheme, we annotate Kyoto University Text Corpus (Kawahara et al., 2002), which is a Japanese newspaper corpus annotated with predicateargument structures and coreference relations. The points of our annotation scheme are two-fold. One of the points is to annotate various expressions that can have temporality. We annotate not only expressions with strong temporality but also expressions with weak temporality. Many previous studies annotate “events” that express situations that happen or occur, which are defined in the guideline of TimeML (Sauri et al., 2006). Therefore, expressions as in the following example are not annotated. (1) Businesses are emerging on the I"
L18-1050,P12-1010,0,0.0463033,"Missing"
L18-1050,S15-2132,0,0.0345274,"Missing"
L18-1050,P16-1207,0,0.285186,"event information and time information in text. For example, temporal ordering of events that estimates the temporal relations of event-event and event-time was studied in TempEval 1, 2, 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), and the timeline generation task that links event and time in multiple documents was studied in SemEval 15 (Minard et al., 2015). In order to train models and evaluate results in these tasks, corpora in which event information is correlated with time information in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the expressions with weak temporality are interpreted. To understand temporal information in text exhaustively, we propose an annotation scheme that anchors various expressions to the time axis, reflecting personal interpretation of text and common sense. Using this scheme, we annotate Kyoto University Text Corpus (Kawahara et al., 2002), which is a Japanese newspaper corpus annotated with predicateargument struc"
L18-1050,S13-2001,0,0.104484,"large amount of texts, we need an information analysis technology to integrate, summarize and compare related texts. In order to analyze texts written at different times or texts referring to different times, it is necessary to interpret the temporal information implied in the texts. There have been many studies and tasks to understand the relationship between event information and time information in text. For example, temporal ordering of events that estimates the temporal relations of event-event and event-time was studied in TempEval 1, 2, 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), and the timeline generation task that links event and time in multiple documents was studied in SemEval 15 (Minard et al., 2015). In order to train models and evaluate results in these tasks, corpora in which event information is correlated with time information in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the expressions with weak temporality are interpreted"
L18-1050,S07-1014,0,0.31054,"ract knowledge about a certain topic from this large amount of texts, we need an information analysis technology to integrate, summarize and compare related texts. In order to analyze texts written at different times or texts referring to different times, it is necessary to interpret the temporal information implied in the texts. There have been many studies and tasks to understand the relationship between event information and time information in text. For example, temporal ordering of events that estimates the temporal relations of event-event and event-time was studied in TempEval 1, 2, 3 (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), and the timeline generation task that links event and time in multiple documents was studied in SemEval 15 (Minard et al., 2015). In order to train models and evaluate results in these tasks, corpora in which event information is correlated with time information in text have been developed (Pustejovsky et al., 2003; Cassidy et al., 2014; Reimers et al., 2016). In these studies, expressions which have clear temporality were annotated, but in order to know how people understand texts from the perspective of time, it is essential to know how the exp"
L18-1637,J94-4001,1,0.151172,"us with discourse annotations produced by Kawahara et al. (2014). The target documents are web pages extracted from the Kyoto University Web Leads Corpus (Hangyo et al., 2012). Each document consists of the first three sentences of a Japanese web page. The web pages cover a variety of domains and the first three sentences are long enough to annotate with discourse relations through crowdsourcing. They adopted a clause as the discourse unit. The clause is a span delimited by relatively strong boundaries in a sentence. They are automatically identified with hand-written rules by the KNP parser (Kurohashi and Nagao, 1994). Kawahara et al. (2014) annotated all possible combinations of clauses with discourse annotations. Table 1 shows the discourse relation tagset. This tagset consists of two layers, where the upper layer contains three classes and the lower layer contains seven classes. 4044 Upper type Lower type Cause/Reason Purpose CONTINGENCY Condition Example 【ボタンを押したので】【お湯が出た】 [Since (I) pushed the button] [hot water came out] 【試験に受かるために】【必死に勉強した】 [To pass the exam] [(I) desperately studied] 【ボタンを押せば】【お湯が出る。】 [If (you) push the button] [hot water will be turned on] Ground 【ここにカバンがあるから】【まだ社内にいるだろう。】 [Here i"
L18-1637,prasad-etal-2008-penn,0,0.493981,"Missing"
L18-1637,J14-4007,0,0.0487645,"Missing"
L18-1637,D08-1027,0,0.223774,"Missing"
L18-1637,P12-1008,0,0.0498295,"Missing"
murawaki-kurohashi-2010-online,W01-0512,0,\N,Missing
murawaki-kurohashi-2010-online,W04-3230,0,\N,Missing
murawaki-kurohashi-2010-online,C04-1067,0,\N,Missing
murawaki-kurohashi-2010-online,N09-1024,0,\N,Missing
murawaki-kurohashi-2010-online,C04-1066,0,\N,Missing
murawaki-kurohashi-2010-online,D08-1045,1,\N,Missing
murawaki-kurohashi-2010-online,C96-2202,0,\N,Missing
murawaki-kurohashi-2010-online,C08-1128,0,\N,Missing
murawaki-kurohashi-2010-online,P99-1036,0,\N,Missing
murawaki-kurohashi-2010-online,P09-1012,0,\N,Missing
murawaki-kurohashi-2010-online,J04-1004,0,\N,Missing
murawaki-kurohashi-2010-online,I08-1002,0,\N,Missing
murawaki-kurohashi-2010-online,1995.mtsummit-1.17,0,\N,Missing
murawaki-kurohashi-2010-online,I08-1025,1,\N,Missing
N04-1031,H01-1043,1,\N,Missing
N04-1031,W00-0906,0,\N,Missing
N04-1031,J02-2001,0,\N,Missing
N04-1031,N03-2003,0,\N,Missing
N04-1031,W03-1602,0,\N,Missing
N04-1031,P01-1008,0,\N,Missing
N04-1031,P02-1028,1,\N,Missing
N04-1031,N03-1024,0,\N,Missing
N04-1031,N03-1003,0,\N,Missing
N04-1031,takezawa-etal-2002-toward,0,\N,Missing
N04-1031,maekawa-etal-2000-spontaneous,0,\N,Missing
N06-1023,P98-1013,0,0.00845536,"Missing"
N06-1023,A00-2031,0,0.0129625,"Missing"
N06-1023,A00-2018,0,0.0421672,"Missing"
N06-1023,W98-1511,0,0.060744,"Missing"
N06-1023,C02-1122,1,0.948491,"and part-of-speech categories (Bikel, 2004). This paper aims at exploiting much more lexical information, and proposes a fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. Lexical information is extracted not from a small tagged corpus, but from a huge raw corpus as case frames. This model performs case structure analysis by a generative probabilistic model based on the case frames, and selects the syntactic structure that has the highest case structure probability. 2 Automatically Constructed Case Frames We employ automatically constructed case frames (Kawahara and Kurohashi, 2002) for our model of 177 Table 1: Case frame examples (examples are expressed only in English for space limitation.). CS ga youritsu (1) wo (support) ni ga youritsu (2) wo (support) ni .. .. . . itadaku (1) ga wo (have) ga itadaku (2) wo (be given) kara .. .. . . examples <agent>, group, party, · · · <agent>, candidate, applicant <agent>, district, election, · · · <agent> <agent>, member, minister, · · · <agent>, candidate, successor .. . <agent> soup <agent> advice, instruction, address <agent>, president, circle, · · · .. . case structure analysis. This section outlines the method for construct"
N06-1023,kawahara-kurohashi-2006-case,1,0.714718,"es are distinguished not by verbs (e.g., “tsumu” (load/accumulate)), but by couples (e.g., “nimotsu-wo tsumu” (load baggage) and “keiken-wo tsumu” (accumulate experience)). Modifier-head examples are aggregated in this way, and yield basic case frames. Thereafter, the basic case frames are clustered to merge similar case frames. For example, since “nimotsu-wo tsumu” (load baggage) and “busshi-wo tsumu” (load supply) are similar, they are clustered. The similarity is measured using a thesaurus (Ikehara et al., 1997). Using this gradual procedure, we constructed case frames from the web corpus (Kawahara and Kurohashi, 2006). The case frames were obtained from approximately 470M sentences extracted from the web. They consisted of 90,000 verbs, and the average number of case frames for a verb was 34.3. In Figure 1, some examples of the resulting case frames are shown. In this table, ‘CS’ means a case slot. <agent> in the table is a generalized example, which is given to the case slot where half of the examples belong to <agent> in a thesaurus (Ikehara et al., 1997). <agent> is also given to “ga” case slot that has no examples, because “ga” case components are usually agentive and often omitted. 3 Integrated Probab"
N06-1023,kawahara-etal-2002-construction,1,0.879956,"Missing"
N06-1023,P03-1054,0,0.00381633,"Missing"
N06-1023,W02-2016,0,0.479957,"Missing"
N06-1023,J94-4001,1,0.326326,"Missing"
N06-1023,W98-1510,0,0.0598714,"Missing"
N06-1023,2000.iwpt-1.43,0,0.0658874,"Missing"
N06-1023,J04-4004,0,\N,Missing
N06-1023,J03-4003,0,\N,Missing
N06-1023,C98-1013,0,\N,Missing
N07-2051,P06-2009,0,0.0289029,"Missing"
N07-2051,I05-3003,0,0.0265937,"Missing"
N07-2051,P06-2041,0,0.024755,"Missing"
N07-2051,C04-1010,0,0.0289809,". Abstract This paper presents a three-step dependency parser to parse Chinese deterministically. By dividing a sentence into several parts and parsing them separately, it aims to reduce the error propagation coming from the greedy characteristic of deterministic parsing. Experimental results showed that compared with the deterministic parser which parsed a sentence in sequence, the proposed parser achieved extremely significant improvement on dependency accuracy. 1 Introduction Recently, as an attractive alternative to probabilistic parsing, deterministic parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004) has drawn great attention with its high efficiency, simplicity and good accuracy comparable to the state-of-the-art generative probabilistic models. The basic idea of deterministic parsing is using a greedy parsing algorithm that approximates a globally optimal solution by making a sequence of locally optimal choices (Hall et al., 2006). This greedy idea guarantees the simplicity and efficiency, but at the same time it also suffers from the error propagation from the previous parsing choices to the left decisions. For example, given a Chinese sentence, which means Paternity test is a test tha"
N07-2051,H01-1014,0,0.0158207,"ne the root finding problem as a classification problem. A classifier, where we still select SVM, is trained to classify each word to be root or not. Then the word with the highest classification score is chosen as root. All the binary features for root finding are listed in Table 3. Here the baseNP chunker introduced in section 3.2 is used to get the BaseNPn feature. 5 Experimental Results 5.1 Data Set and Experimental Setting We use Penn Chinese Treebank 5.1 as data set. To transfer the phrase structure into dependency structure, head rules are defined based on Xia’s head percolation table (Xia and Palmer, 2001). 16,984 sentences and 1,292 sentences are used for training and testing. The same training data is also used to train the sentence segmenter, the baseNP chunker, the sub-sentence root finder, and the sentence root finder. During both training and testing, the gold-standard word segmentation and pos-tag are applied. TinySVM is selected as a SVM toolkit. We use a polynomial kernel and set the degree as 2 in all the experiments. 5.2 Three-step Parsing vs. One-step Parsing First, we evaluated the dependency accuracy and root accuracy of both three-step parsing and one-step parsing. Three-step par"
N07-2051,C02-1145,0,0.196029,"Missing"
N07-2051,W03-3023,0,0.757626,"een different parsing stages. Abstract This paper presents a three-step dependency parser to parse Chinese deterministically. By dividing a sentence into several parts and parsing them separately, it aims to reduce the error propagation coming from the greedy characteristic of deterministic parsing. Experimental results showed that compared with the deterministic parser which parsed a sentence in sequence, the proposed parser achieved extremely significant improvement on dependency accuracy. 1 Introduction Recently, as an attractive alternative to probabilistic parsing, deterministic parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004) has drawn great attention with its high efficiency, simplicity and good accuracy comparable to the state-of-the-art generative probabilistic models. The basic idea of deterministic parsing is using a greedy parsing algorithm that approximates a globally optimal solution by making a sequence of locally optimal choices (Hall et al., 2006). This greedy idea guarantees the simplicity and efficiency, but at the same time it also suffers from the error propagation from the previous parsing choices to the left decisions. For example, given a Chinese sentence, which means Pat"
N09-1059,P06-2004,0,0.0609428,"Missing"
N09-1059,H01-1052,0,0.418193,"lly utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve"
N09-1059,P01-1005,0,0.487383,"lly utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve"
N09-1059,D07-1090,0,0.0402824,"e structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words. 1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural language processing (NLP) applications, such as prepositional phrase (PP) attachment, other-anaphora resolution, spelling correction, confusable word set disambiguation and machine translation (Volk, 2001; Modjeska et al., 2003; Lapata and Keller, 2005; Atterer and Sch¨utze, 2006; Brants et al., 2007). Most of the previous work utilized only the surface information of the corpora, such as n-grams, co-occurrence counts, and simple surface syntax. This may be because these studies did not require structured knowledge, and for such studies, the size of currently available corpora is considered to have been almost enough. For instance, while Brants et al. (2007) reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens, the 521 increase became small at the corpus size of larger than 30 billion tokens. Howev"
N09-1059,C08-1036,0,0.0132129,"machine translation systems. They reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens. Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semisupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from vari522 ety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 bill"
N09-1059,halacsy-etal-2004-creating,0,0.012625,"(Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from vari522 ety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news"
N09-1059,N06-1023,1,0.817137,"emantic case frames, which describe the cases each predicate has and the types of nouns that can ﬁll a case slot. Note that case frames offer not only the knowledge of the relationships between a predicate and its particular case slot, but also the knowledge of the relationships among a predicate and its multiple case slots. To obtain such knowledge, very large corpora seem to be necessary; however it is still unknown how much corpora would be required to obtain good coverage. For examples, Kawahara and Kurohashi proposed a method for constructing wide-coverage case frames from large corpora (Kawahara and Kurohashi, 2006b), and a model for syntactic and case structure analysis of Japanese that based upon case frames (Kawahara and Kurohashi, 2006a). However, they did not demonstrate whether the coverage of case frames was wide enough for these tasks and how dependent the performance of the model was on the corpus size for case frame construction. This paper aims to address these questions. We collect a very large Japanese corpus consisting of about 100 billion words, or 1.6 billion unique sentences from the Web. Subsets of the corpus are randomly selected to obtain corpora of different sizes ranging from 1.6 m"
N09-1059,kawahara-kurohashi-2006-case,1,0.83525,"Missing"
N09-1059,D07-1032,1,0.900202,"Missing"
N09-1059,kawahara-etal-2004-toward,1,0.836044,"statistics for the constructed case frames. The number of predicates, the average number of examples and unique examples for a case slot, and whole ﬁle size were conﬁrmed to be heavily dependent upon the corpus size. However, the average number of case frames for a predicate and case slots for a case frame did not. 525 5.2.1 Setting In order to investigate the coverage of the resultant case frames, we used a syntactic relation, case structure, and anaphoric relation annotated corpus consisting of 186 web documents (979 sentences). This corpus was manually annotated using the same criteria as Kawahara et al. (2004). There were 2,390 annotated relationships between predicates and their direct (not omitted) case components and 837 zero anaphoric relations in the corpus. We used two evaluation metrics depending upon whether the target case component was omitted or not. For the overt case component of a predicate, we judged the target component was covered by case frames if the target component itself was included in the examples for one of the corresponding case slots of the case frame. For the omitted case component, we checked not only the target component itself but also all mentions that refer to the s"
N09-1059,J03-3001,0,0.0365786,"zes ranging from 1.6 million to 1.6 billion sentences. We construct case frames from each corpus and apply them to syntactic and case structure analysis, and zero anaphora resolution, in order to investigate the Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 521–529, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics relationships between the corpus size and the performance of these analyses. 2 Related Work Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data s"
N09-1059,E06-1030,0,0.0160577,"unking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have collected corpora from the Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from vari522 ety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi ("
N09-1059,N06-1020,0,0.0173976,"Web by themselves. For English, Banko and Brill (2001b) collected a corpus with 1 billion words from vari522 ety of English texts. Liu and Curran (2006) created a Web corpus for English that contained 10 billion words and showed that for content-sensitive spelling correction the Web corpus results were better than using a search engine. Halacsy et al. (2004) created a corpus with 1 billion words for Hungarian from the Web by downloading 18 million pages. Others utilize publicly available corpus such as the North American News Corpus (NANC) and the Gigaword Corpus (Graff, 2003). For instance, McClosky et al. (2006) proposed a simple method of self-training a two phase parser-reranker system using NANC. As for Japanese, Kawahara and Kurohashi (2006b) collected 23 million pages and created a corpus with approximately 20 billion words. Google released Japanese n-gram constructed from 20 billion Japanese sentences (Kudo and Kazawa, 2007). Several news wires are publicly available consisting of tens of million sentences. Kotonoha project is now constructing a balanced corpus of the presentday written Japanese consisting of 50 million words (Maekawa, 2006). 3 Construction of Case Frames Case frames describe t"
N09-1059,W03-1023,0,0.0493633,"six different sizes. Then, we applied these case frames to syntactic and case structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words. 1 Introduction Very large corpora obtained from the Web have been successfully utilized for many natural language processing (NLP) applications, such as prepositional phrase (PP) attachment, other-anaphora resolution, spelling correction, confusable word set disambiguation and machine translation (Volk, 2001; Modjeska et al., 2003; Lapata and Keller, 2005; Atterer and Sch¨utze, 2006; Brants et al., 2007). Most of the previous work utilized only the surface information of the corpora, such as n-grams, co-occurrence counts, and simple surface syntax. This may be because these studies did not require structured knowledge, and for such studies, the size of currently available corpora is considered to have been almost enough. For instance, while Brants et al. (2007) reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens, the 521 incre"
N09-1059,P08-1052,0,0.0187231,"mputational Linguistics relationships between the corpus size and the performance of these analyses. 2 Related Work Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). Volk (2001) proposed a method for resolving PP attachment ambiguities based upon Web data. Modjeska et al. (2003) used the Web for resolving nominal anaphora. Lapata and Keller (2005) investigated the performance of web-based models for a wide range of NLP tasks, such as MT candidate selection, article generation, and countability detection. Nakov and Hearst (2008) solved relational similarity problems using the Web as a corpus. With respect to the effect of corpus size on NLP tasks, Banko and Brill (2001a) showed that for content sensitive spelling correction, increasing the training data size improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 mil"
N09-1059,I08-2080,1,0.879813,"Missing"
N09-1059,C08-1097,1,0.538794,"construct case frames from the NErecognized corpus. Similar to the categories, for each NE class, we calculate the NE ratio among all the case slot examples, and add it to the case slot (e.g. [NE:PERSON]:0.12). The generalized examples are also included in Table 1. 4 Discourse Analysis with Case Frames In order to investigate the effect of corpus size on complex NLP tasks, we apply the constructed cases frames to an integrated probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006a) and a probabilistic model for Japanese zero anaphora resolution (Sasano et al., 2008). In this section, we brieﬂy describe these models. 4.1 Model for Syntactic and Case Structure Analysis Kawahara and Kurohashi (2006a) proposed an integrated probabilistic model for Japanese syntactic and case structure analysis based upon case frames. Case structure analysis recognizes predicate argument structures. Their model gives a probability to each possible syntactic structure T and case structure L of the input sentence S, and outputs the syntactic and case structure that have the highest probability. That is to say, the system selects the syntactic structure Tbest and the case struct"
N09-1059,P08-1076,0,0.0205197,"improved the accuracy. Atterer and Sch¨utze (2006) investigated the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decision; they found that the combined system only improved the performance of the parser for small training sets. Brants et al. (2007) varied the amount of language model training data from 13 million to 2 trillion tokens and applied these models to machine translation systems. They reported that translation quality continued to improve with increasing corpus size for training language models at even size of 2 trillion tokens. Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semisupervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. There are several methods to extract useful information from very large corpora. Search engines, such as Google and Altavista, are often used to obtain Web counts (e.g. (Nakov and Hearst, 2005; Gledson and Keane, 2008)). However, search engines are not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations. Therefore, several researchers have"
N10-1120,P08-1034,0,0.00994089,"Missing"
N10-1120,D08-1083,0,0.793046,"ion, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently. Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008). However, these methods are based on ﬂat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rigid rules seems useful. Wilson et al. (2005) proposed a method for sentiment classiﬁcation which utilizes head-modiﬁer relation and machine learning. However, the method is based on bag-of-features and polarity reversal occurred by content words is not handled. One issue of the approach to use senten"
N10-1120,I08-1039,0,0.252237,"t classiﬁcation, sentiment polarities can be reversed. For example, let us consider the sentence “The medicine kills cancer cells.” While the phrase cancer cells has negative polarity, the word kills reverses the polarity, and the whole sentence has positive polarity. Thus, in sentiment classiﬁcation, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently. Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008). However, these methods are based on ﬂat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rig"
N10-1120,P06-2059,0,0.00796876,"and sj are equal, we set the initial parameter λi of the feature to a random number in [0.9, 1.1], otherwise we set to a random number in [−0.1, 0.1]7 . By setting such initial values, the initial model parameters have a property that two phrases with head-modiﬁer relation tend to have the same polarity, which is intuitively reasonable. 3 Experiments We conducted experiments of sentiment classiﬁcation on four Japanese corpora and four English corpora. 3.1 Data We used four corpora for experiments of Japanese sentiment classiﬁcation: the Automatically Constructed Polarity-tagged corpus (ACP) (Kaji and Kitsuregawa, 2006), the Kyoto University and NTT Blog corpus (KNB) 8 , the NTCIR Japanese opinion corpus (NTC-J) (Seki et al., 2007; Seki et al., 2008), the 50 Topics Evaluative Information corpus (50 Topics) (Nakagawa et al., 2008). The ACP corpus is an automatically constructed corpus from HTML documents on the Web using lexico-syntactic patterns and layout structures. The size of the corpus is large (it consists of 650,951 instances), and we used 1/100 of the whole corpus. The KNB corpus consists of Japanese blogs, and is manually annotated. The NTC-J corpus consists of Japanese newspaper articles. There are"
N10-1120,W02-1011,0,0.0398013,"ideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classiﬁcation for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features. 1 Introduction Sentiment classiﬁcation is a useful technique for analyzing subjective information in a large number of texts, and many studies have been conducted (Pang and Lee, 2008). A typical approach for sentiment classiﬁcation is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002), which is widely used in topic-based text classiﬁcation. In the approach, a subjective sentence is represented as a set of words in the sentence, ignoring word order and head-modiﬁer relation between words. However, sentiment classiﬁcation is different from traditional topic-based text classiﬁcation. Topic-based text classiﬁcation is generally a linearly separable problem ((Chakrabarti, 2002), p.168). For example, when a document contains some domain-speciﬁc words, the document will probably belong to the domain. However, in sentiment classiﬁcation, sentiment polarities can be reversed. For e"
N10-1120,W96-0213,0,0.0529099,"form, coarse-grained part-of-speech (POS) tag, 789 si si &qi si &qi &ri si &ui,1 , · · · , si &ui,mi si &ci,1 , · · · , si &ci,mi si &fi,1 , · · · , si &fi,mi si &ui,1 &ui,2 , · · · , si &ui,mi −1 &ui,mi si &bi,1 &bi,2 , · · · , si &bi,mi −1 &bi,mi Edge Features si &sj si &sj &rj si &sj &rj &qj si &sj &bi,1 , · · · , si &sj &bi,mi si &sj &bj,1 , · · · , si &sj &bj,mj Table 1: Features Used in This Study ﬁne-grained POS tag of the k-th word in the i-th phrase. We used the morphological analysis system JUMAN and the dependency parser KNP2 for processing Japanese data, and the POS tagger MXPOST (Ratnaparkhi, 1996) and the dependency parser MaltParser3 for English data. KNP outputs phrase-based dependency trees, but MaltParser outputs word-based dependency trees, and we converted the word-based ones to phrase-based ones using simple heuristic rules explained in Appendix A. The prior polarity of a phrase qi ∈ {+1, 0, −1} is the innate sentiment polarity of a word contained in the phrase, which can be obtained from sentiment polarity dictionaries. We used sentiment polarity dictionaries made by Kobayashi et al. (2007) and Higashiyama et al. (2008)4 for Japanese experiments (The resulting dictionary contai"
N10-1120,C08-1106,0,0.00864051,"ical sentiment classiﬁcation method incorporating head-modiﬁer relation. Ikeda et al. (2008) proposed a machine learning approach to handle sentiment polarity reversal. For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features. The method can handle only words with prior polarities, and does not use syntactic dependency structures. Conditional random ﬁelds with hidden variables have been studied so far for other tasks. LatentDynamic Conditional Random Fields (LDCRF) (Morency et al., 2007; Sun et al., 2008) are probabilistic models with hidden variables for sequential labeling, and belief propagation is used for inference. Out method is similar to the models, but there are several differences. In our method, only one variable which represents the polarity of the whole sentence is observable, and dependency relation among random variables is not a linear chain but a tree structure which is identical to the syntactic dependency. 5 Conclusion In this paper, we presented a dependency tree-based method for sentiment classiﬁcation using conditional random ﬁelds with hidden variables. In this method, t"
N10-1120,H05-1044,0,0.326676,"s are based on ﬂat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rigid rules seems useful. Wilson et al. (2005) proposed a method for sentiment classiﬁcation which utilizes head-modiﬁer relation and machine learning. However, the method is based on bag-of-features and polarity reversal occurred by content words is not handled. One issue of the approach to use sentence composition and machine learning is that only the whole sentence is labeled with its polarity in general corpora for sentiment classiﬁcation, and each component of the sentence is not labeled, though such information is necessary for supervised ma786 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of"
N15-1125,W07-0702,0,0.0286083,"9) for a large number (over 25) of languages (other than Indian) including Japanese and Hindi (Japanese to Hindi translation being our focus) of approximately 30000 lines. We chose this setting because we feel that this multilingual approach is especially important for lowresource language pairs. Typically system combination methods like linear interpolation are used to combine the direct and pivot phrase tables by modifying the probabilities of phrase pairs leading to the modification of the underlying distribution which affects the resultant translation quality. The Multiple Decoding Paths (Birch and Osborne, 2007) (MDP) feature has been used 4 The construction of a multilingual corpus has already the benefit that each new language added to it will allow direct translation with a SMT system for N new language pairs. 1193 • Most works focus on obtaining pivot based phrase tables on relatively larger corpora than the ones used for the direct phrase table. We use the same corpora sizes for the pivot as well as direct tables. • We verify that Multiple Decoding Paths (MDP) feature of Moses is much more effective than plain linear interpolation, especially when more pivot languages are used together. • We sho"
N15-1125,P10-1137,1,0.846116,"ments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval. We now talk about the languages, corpora and experiments conducted. 3 Description of Languages, Corpora and Experiments We first describe the pivot languages and the corpora we use. We follow this with a description of the triangulation method which we use to construct phrase tables using the pivot languages, the methods used to combine the constructed tables and then the experiments that use them. 1194 3"
N15-1125,I13-1167,0,0.0314773,"Missing"
N15-1125,W09-0431,0,0.0211025,"as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval."
N15-1125,P07-2045,0,0.00823489,"thods by: Firstly, combining the pivot based phrase tables into a single table using equation 5 (using the ratio of BLEU scores as interpolation weights) followed by using this table to support the direct phrase table by MDP. Note that the right way would be to use the BLEU scores on the tuning set but our objective was to show that even in the best case scenario (also called Oracle7 scenario) this method is still inferior compared to only using the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) us"
N15-1125,W07-0733,0,0.0240646,"translation system. Reordering tables are supplementary and can usually be replaced by a simple distortion model. Major problems arise when source-pivot and pivot-target corpora belong to different domains leading to rather poor quality translations. Even if the individual corpora are large, one will run into domain adaptation problems. In such a scenario the availability of a small size multilingual corpus of a few thousand lines belonging to a single domain can be beneficial. The setting of this paper is: to combine two source-target phrase tables of different domains for domain adaptation (Koehn and Schroeder, 2007) but not so extensively in a pivot language scenario, especially when multiple pivots are involved (7 in our case). Our work is different from other previous works in the following ways: • We work on a realistic low resource setting for translation between Japanese and Hindi in which we use small sized multilingual corpora containing translations of a sentence in multiple languages. 1. We suppose the existence of a multilingual corpus with sentences aligned across N 4 different languages. • We focus on the impact of using a relatively large number of pivot languages (7 to be precise) to improv"
N15-1125,W04-3250,0,0.0268447,"the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) using 10 as the n-best list size for intermediate and target language translations. The experiments we performed are given below. Each experiment involves either the creation of a phrase tables or combination of phrase tables. We tune, test and evaluate these tables or combinations. 1. A src (source) to tgt (target) direct phrase table. 2. For piv in Pivot Languages Set; the set of pivot languages to be used (Tables 1 and 2): 7 By Oracle sc"
N15-1125,kunchukuttan-etal-2014-shata,1,0.889032,"Missing"
N15-1125,D09-1141,0,0.0177672,"ce this results in multiplicative error propagation Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to get a source-target phrase table. They then combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by"
N15-1125,2001.mtsummit-papers.46,0,0.15184,"rks to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval. We now talk about the languages, corpora and experiments conducted. 3 Description of Languages, Corpora and Experiments We fi"
N15-1125,P03-1021,0,0.0135287,"ur objective was to show that even in the best case scenario (also called Oracle7 scenario) this method is still inferior compared to only using the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) using 10 as the n-best list size for intermediate and target language translations. The experiments we performed are given below. Each experiment involves either the creation of a phrase tables or combination of phrase tables. We tune, test and evaluate these tables or combinations. 1. A src (sour"
N15-1125,P02-1040,0,0.0937469,"so called Oracle7 scenario) this method is still inferior compared to only using the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) using 10 as the n-best list size for intermediate and target language translations. The experiments we performed are given below. Each experiment involves either the creation of a phrase tables or combination of phrase tables. We tune, test and evaluate these tables or combinations. 1. A src (source) to tgt (target) direct phrase table. 2. For piv in Pivot Languages Set;"
N15-1125,N09-2056,0,0.0204827,"hen combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related fi"
N15-1125,P14-2125,0,0.0121014,"s had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval. We now talk about the languages, corpora and ex"
N15-1125,E12-1055,0,0.131834,"ture work. 2 Related Work Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to get a source-target phrase table. They then combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider Eng"
N15-1125,N07-1061,0,0.736099,"he parallel corpus, it is impossible to achieve the same level of translation quality as that in the case of resource rich languages. To remedy this scenario, an intermediate resource rich language can be exploited. Although, finding a direct parallel corpus between source and target languages might be difficult, there are higher odds of finding a pair of parallel corpora: one between the source language and an intermediate resource rich language (henceforth called pivot1 ) and one between that pivot and the target language. Using the methods developed for Pivot Based SMT (Wu and Wang, 2007) (Utiyama and Isahara, 2007) one can use the source-pivot and pivot-target parallel corpora to develop a source-target translation system (henceforth called as pivot based system 2 ) . Moreover, if there exists a small source-target parallel corpus then the resulting system (henceforth called as direct system3 ) can be supported by the pivot based source-target system to significantly improve the translation quality. Note that in this paper we use the terms ”translation system” and ”phrase table” interchangeably since the phrase table is the We present our work on leveraging multilingual parallel corpora of small sizes f"
N15-1125,P07-1108,0,0.621386,"ted to the size of the parallel corpus, it is impossible to achieve the same level of translation quality as that in the case of resource rich languages. To remedy this scenario, an intermediate resource rich language can be exploited. Although, finding a direct parallel corpus between source and target languages might be difficult, there are higher odds of finding a pair of parallel corpora: one between the source language and an intermediate resource rich language (henceforth called pivot1 ) and one between that pivot and the target language. Using the methods developed for Pivot Based SMT (Wu and Wang, 2007) (Utiyama and Isahara, 2007) one can use the source-pivot and pivot-target parallel corpora to develop a source-target translation system (henceforth called as pivot based system 2 ) . Moreover, if there exists a small source-target parallel corpus then the resulting system (henceforth called as direct system3 ) can be supported by the pivot based source-target system to significantly improve the translation quality. Note that in this paper we use the terms ”translation system” and ”phrase table” interchangeably since the phrase table is the We present our work on leveraging multilingual paral"
N15-1125,P09-1018,0,0.362193,"airs being acquired that impact translation quality. Section 2 contains the related work. Section 3 begins with a basic description about the languages involved, followed by the corpora details and the experimental methodology. Section 4 consists of results, observations and discussions. The paper ends with conclusions and future work. 2 Related Work Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to get a source-target phrase table. They then combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots ha"
N15-1125,2009.mtsummit-papers.7,0,\N,Missing
N16-1002,P05-1022,0,0.257273,"the proposed system with no insertion position features (‘Flexible’). Signiﬁcance was calculated with bootstrapping for p < 0.05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightmost slots in the target-side parent. For referenc"
N16-1002,N12-1047,0,0.0282404,"lation quality (BLEU and RIBES). Results marked with † are signiﬁcantly higher than the baseline system and those marked with ‡ are signiﬁcantly higher than the proposed system with no insertion position features (‘Flexible’). Signiﬁcance was calculated with bootstrapping for p < 0.05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules b"
N16-1002,P05-1033,0,0.605916,"ine’ into the rule ‘[X] を 読んだ → read [X]’. We cannot however decide clearly where to insert the rule ‘昨日 → yesterday’ as there is no matching non-terminal in the rule containing its parent in the input sentence (‘読んだ’). We use the term ﬂoating to describe words such as ‘yesterday’ in this example, i.e. for an input subtree matched to the source side of a rule, children of the input root that are not contained in the source side of the rule as terminals and cannot be inserted using ﬁxed-position non-terminals in the rule. Previous work deals with this problem by either using simple glue rules (Chiang, 2005) or limiting rules in a way to avoid isolated ﬂoating children (Shen et al., 2008). For example, it is possible to disallow the ﬁrst rule in Figure 1 when translating a sentence such as that in Figure 2 with uncovered children (in this case the word ‘yesterday’). This method greatly reduces the expressiveness and ﬂexibility of translation rules. In our generalized model, we allow any number of terminals and non-terminals and permit arbitrarily many ﬂoating children in each rule. To our knowledge this is the ﬁrst study to take this more comprehensive approach. Note that in the case of constitue"
N16-1002,P10-1146,0,0.0632456,"Missing"
N16-1002,W06-1628,0,0.0734738,"Missing"
N16-1002,D14-1063,1,0.772667,"e than four ﬂexible non-terminals, each with more than four possible insertion positions. Such rules will already generate hundreds of simple rules. In the most extreme cases, we may encounter rules having more than ten ﬂexible non-terminals, leading to the generation of many millions of simple rules. This explosion of rules can lead to impractical decoding time and memory usage. It is therefore important to make use of the compact encoding of many simple rules provided by the concept of ﬂexible non-terminals in the decoding process itself. We use the decoding approach of right-hand lattices (Cromières and Kurohashi, 2014), an eﬃcient way of encoding many simple rules. The idea is to encode the translation rules into a lattice form, then use this lattice to decode eﬃciently without the need to expand the ﬂexible non-terminals explicitly. Figure 5 shows how the concept of ﬂexible non-terminals can be eﬃciently encoded into lattice form. The top half shows a target-side tree translation rule with ﬂexible non-terminals X1, X2, X3 and X4 allowed to be inserted at any position that is a child of the word ‘a’, with the constraint that X1 comes before X2 and that X2 comes before X3. X5 is another ﬂexible nonterminal t"
N16-1002,N04-1014,0,0.0958519,"Missing"
N16-1002,W11-2123,0,0.0177269,"atures to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightmost slots in the target-side parent. For reference, we also show results using Moses (Koehn et al., 2007) with default settings and distortion limit set to 20 (‘Moses’). The proposed system (‘Flexible’) adds ﬂexible non-terminals with multiple insertion positions, how"
N16-1002,D10-1092,0,0.11044,"nals with multiple insertion positions, however we do not yet add the insertion choice features. This means that the insertion positions are in practice chosen by the language model. Note that we do not get a 16 substantial hit in performance by adding the ﬂexible non-terminals because of their compact lattice representation. The systems ‘+Pref’, ‘+Pref+Ins’ and ‘+Pref+Ins+Rel’ show the results of adding insertion choice position features (left/right preference, insertion position choice, relative position choice). We give translation scores measured in BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010), which is designed to reﬂect quality of translation word order more eﬀectively than BLEU. The translation evaluation is shown in Table 2. 5 Discussion and Error Analysis The experimental results showed a signiﬁcantly positive improvement in terms of both BLEU and RIBES over the baseline tree-to-tree system. The baseline system uses ﬁxed non-terminals and is competitive with the most popular stringto-string system (Moses). The extensions of the proposed model (adding a variety of features) also all showed signiﬁcant improvement over the baseline, and approximately half of the extended settings"
N16-1002,N06-1023,1,0.700597,"y higher than the baseline system and those marked with ‡ are signiﬁcantly higher than the proposed system with no insertion position features (‘Flexible’). Signiﬁcance was calculated with bootstrapping for p < 0.05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), jo"
N16-1002,N03-1017,0,0.0791733,"Missing"
N16-1002,P07-2045,0,0.0109225,"with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightmost slots in the target-side parent. For reference, we also show results using Moses (Koehn et al., 2007) with default settings and distortion limit set to 20 (‘Moses’). The proposed system (‘Flexible’) adds ﬂexible non-terminals with multiple insertion positions, however we do not yet add the insertion choice features. This means that the insertion positions are in practice chosen by the language model. Note that we do not get a 16 substantial hit in performance by adding the ﬂexible non-terminals because of their compact lattice representation. The systems ‘+Pref’, ‘+Pref+Ins’ and ‘+Pref+Ins+Rel’ show the results of adding insertion choice position features (left/right preference, insertion pos"
N16-1002,P06-1077,0,0.0513765,"input sentence into manageable parts, translating these segments, then arranging them in an appropriate order. The ﬁrst two steps have roughly the same diﬃculty for close and distant language pairs, however the reordering step is considerably more challenging for language pairs with dissimilar syntax. We need to In order to improve upon linear string-based approaches, syntax-based approaches have also been proposed. Tree-to-string translation has been the most popular syntax-based paradigm in recent years, which is reﬂected by a number of reordering approaches considering source-only syntax (Liu et al., 2006; Neubig, 2013). One particularly interesting approach is to project source dependency parses to the target side and then learn a probability model for reordering children using features such as source and target head words (Quirk et al., 2005). While tree-to-tree translation (Graehl and 11 Proceedings of NAACL-HLT 2016, pages 11–19, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Figure 1: Examples of tree-to-tree translation rules extracted from an aligned and parsed bitext. Colored boxes represent aligned phrases and [X] is a non-terminal. Figure 2:"
N16-1002,P13-4016,0,0.0159157,"to manageable parts, translating these segments, then arranging them in an appropriate order. The ﬁrst two steps have roughly the same diﬃculty for close and distant language pairs, however the reordering step is considerably more challenging for language pairs with dissimilar syntax. We need to In order to improve upon linear string-based approaches, syntax-based approaches have also been proposed. Tree-to-string translation has been the most popular syntax-based paradigm in recent years, which is reﬂected by a number of reordering approaches considering source-only syntax (Liu et al., 2006; Neubig, 2013). One particularly interesting approach is to project source dependency parses to the target side and then learn a probability model for reordering children using features such as source and target head words (Quirk et al., 2005). While tree-to-tree translation (Graehl and 11 Proceedings of NAACL-HLT 2016, pages 11–19, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Figure 1: Examples of tree-to-tree translation rules extracted from an aligned and parsed bitext. Colored boxes represent aligned phrases and [X] is a non-terminal. Figure 2: Combination of"
N16-1002,P02-1040,0,0.11352,"‘Flexible’) adds ﬂexible non-terminals with multiple insertion positions, however we do not yet add the insertion choice features. This means that the insertion positions are in practice chosen by the language model. Note that we do not get a 16 substantial hit in performance by adding the ﬂexible non-terminals because of their compact lattice representation. The systems ‘+Pref’, ‘+Pref+Ins’ and ‘+Pref+Ins+Rel’ show the results of adding insertion choice position features (left/right preference, insertion position choice, relative position choice). We give translation scores measured in BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010), which is designed to reﬂect quality of translation word order more eﬀectively than BLEU. The translation evaluation is shown in Table 2. 5 Discussion and Error Analysis The experimental results showed a signiﬁcantly positive improvement in terms of both BLEU and RIBES over the baseline tree-to-tree system. The baseline system uses ﬁxed non-terminals and is competitive with the most popular stringto-string system (Moses). The extensions of the proposed model (adding a variety of features) also all showed signiﬁcant improvement over the baseline, and approximat"
N16-1002,P05-1034,0,0.0590265,"ly more challenging for language pairs with dissimilar syntax. We need to In order to improve upon linear string-based approaches, syntax-based approaches have also been proposed. Tree-to-string translation has been the most popular syntax-based paradigm in recent years, which is reﬂected by a number of reordering approaches considering source-only syntax (Liu et al., 2006; Neubig, 2013). One particularly interesting approach is to project source dependency parses to the target side and then learn a probability model for reordering children using features such as source and target head words (Quirk et al., 2005). While tree-to-tree translation (Graehl and 11 Proceedings of NAACL-HLT 2016, pages 11–19, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Figure 1: Examples of tree-to-tree translation rules extracted from an aligned and parsed bitext. Colored boxes represent aligned phrases and [X] is a non-terminal. Figure 2: Combination of translation rules, demonstrating non-terminal substitution and multiple possible insertion positions for a non-matching input phrase (‘昨日’). Knight, 2004; Cowan and Collins, 2006; Chiang, 2010) has been somewhat less popular tha"
N16-1002,P14-5014,1,0.849806,"is relatively simple when the target structure of rules is restricted to ‘well-formed’ dependencies (Shen et al., 2008), however in this paper we consider more general rules with ﬂexible non-terminal insertion positions. 2 Dependency Tree-To-Tree Translation Dependency tree-to-tree translation begins with the extraction of translation rules from a bilingual corpus that has been parsed and word aligned. Figure 1 shows an example of three rules that can be extracted from aligned and parsed sentence pairs. In this paper we consider rules similar to previous work on tree-to-tree de12 pendency MT (Richardson et al., 2014). The simplest type of rule, containing only terminal symbols, can be extracted trivially from aligned subtrees (see rules 2 and 3 in Figure 1). Non-terminals can be added to rules (see rule 1 in Figure 1) by omitting aligned subtrees and replacing on each side with non-terminal symbols. We can naturally express phrase reordering as the source/target-side non-terminals are aligned. Decoding is performed by combining these rules to form a complete translation, as shown in Figure 2. We are able to translate part of the sentence with non-ambiguous reordering (‘read a magazine’), as we can insert"
N16-1002,D11-1046,0,0.356462,".05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightmost slots in the target-side parent. For reference, we also show results using Moses (Koehn et al., 2007) with default settings and distortion limit set to 20 (‘Moses"
N16-1002,P08-1066,0,0.599498,"nt aligned phrases and [X] is a non-terminal. Figure 2: Combination of translation rules, demonstrating non-terminal substitution and multiple possible insertion positions for a non-matching input phrase (‘昨日’). Knight, 2004; Cowan and Collins, 2006; Chiang, 2010) has been somewhat less popular than treeto-string translation, we believe there are many beneﬁts of considering target-side syntax. In particular, reordering can be deﬁned naturally with non-terminals in the target-side grammar. This is relatively simple when the target structure of rules is restricted to ‘well-formed’ dependencies (Shen et al., 2008), however in this paper we consider more general rules with ﬂexible non-terminal insertion positions. 2 Dependency Tree-To-Tree Translation Dependency tree-to-tree translation begins with the extraction of translation rules from a bilingual corpus that has been parsed and word aligned. Figure 1 shows an example of three rules that can be extracted from aligned and parsed sentence pairs. In this paper we consider rules similar to previous work on tree-to-tree de12 pendency MT (Richardson et al., 2014). The simplest type of rule, containing only terminal symbols, can be extracted trivially from"
N16-1002,Y12-1033,1,0.706535,"ked with ‡ are signiﬁcantly higher than the proposed system with no insertion position features (‘Flexible’). Signiﬁcance was calculated with bootstrapping for p < 0.05. Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based decoding (Cromières and Kurohashi, 2014) to support multiple non-terminal insertion positions and default tuning using, k-best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kurohashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5gram language model with modiﬁed Kneser-Ney smoothing built with KenLM (Heaﬁeld, 2011). 4.2 Evaluation As our baseline (‘Baseline’), we used the default tree-to-tree settings and features of KyotoEBMT, allowing only ﬁxed-position nonterminals. We dealt with ﬂoating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining ﬂoating children to the rightm"
N16-1002,N04-4026,0,0.343208,"rder for dependency parses, in which words can have arbitrarily many children. Previous approaches have tackled this problem by restricting grammar rules, reducing the expressive power of the translation model. The ﬁrst approaches to reordering were based on linear distortion (Koehn et al., 2003), which models the probability of swapping pairs of phrases over some given distance. The linear distance is the only parameter, ignoring any contextual information, however this model has been shown to work well for string-to-string translation. Linear reordering was improved with lexical distortion (Tillmann, 2004), which characterizes reordering in terms of type (monotone, swap, or discontinuous) as opposed to distance. This approach however is prone to sparsity problems, in particular for distant language pairs. In this paper we propose a general model for dependency tree-to-tree reordering based on ﬂexible non-terminals that can compactly encode multiple insertion positions. We explore how insertion positions can be selected even in cases where rules do not entirely cover the children of input sentence words. The proposed method greatly improves the ﬂexibility of translation rules at the cost of only"
N18-2041,D14-1181,0,0.00413864,"ng the explicit and implicit sentiment in the financial text, de Kauter et al. (2015) used a fine-grained sentiment annotation scheme. Kumar et al. (2017) used a classical supervised approach based on Support Vector Regression for sentiment analysis in financial domain. Oliveira et al. (2013) relied on multiple regression models. Akhtar et al. (2017) used an ensemble of four different systems for predicting the sentiment. It used a combination of Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), Convolutional Neural Network (CNN) (Kim, 2014) and Support Vector Regression (SVR) (Smola and Sch¨olkopf, 2004). Yang et al. (2016) used a hierarchical attention network to build the document representation incrementally for document classification. Our model focuses on interpretability and usage of knowledge bases. Knowledge bases have been recognized important for natural language understanding tasks (Minsky, 1986). Our main contribution is a two-layered attention network which utilizes background knowledge bases to build good word level representation at the primary level. The secondary attention mechanism works on top of the primary l"
N18-2041,baccianella-etal-2010-sentiwordnet,0,0.0675258,"Neural Network. It handles the long-term dependencies where the current output is dependent on many prior inputs. BiLSTM, in essence, is a combination of two different LSTM - one working in forward and the other working in the backward direction. The contextual information about both past and future helps in determining the current output. αt ∝ hcTt Ws us H= X t αt hbt (4) (5) where Ws is a parameter matrix and us is the context vector to be learned. H is finally fed to a one layer feed forward neural network. 254 2.2 Relevant Terms and Embeddings lexicon (Wilson et al., 2005), SentiWordNet (Baccianella et al., 2010) and Vader sentiment (Gilbert, 2014). From the above lexicons we extracted the agreement score (Rao and Srivastava, 2012) and the count of the number of occurrences of all positive and negative words in the text. - Word embedding: We use the 300-dimensional pre-trained Word2Vec and GloVe embedding. The sentence embedding was obtained by concatenating the embedding for all words in the sentence. External knowledge can provide explicit information for the model which the training data lacks. This helps the model to make better predictions. We relied on Knowledge Graph Embeddings based on WordNet"
N18-2041,W14-4012,0,0.0165288,"Missing"
N18-2041,S17-2152,0,0.0447809,"Missing"
N18-2041,S17-2138,0,0.0160322,"et al., 2014) model. An example of the DT expansion of the word ’touchpad’ is mouse, trackball, joystick and trackpad. 2.3 Experiments https://wordnet.princeton.edu 255 3.3 Models Single systems Mansar et al. (Team Fortia-FBK) Akhtar et al. - LSTM Akhtar et al. - GRU Akhtar et al. - CNN L3 (proposed) Ensembled systems Lan et al. (Team ECNU) Akhtar et al. E1 (proposed) Results We compare our system with the state-of-the-art systems of SemEval 2017 Task 5 and the system proposed by Akhtar et al. (2017). Table 1 shows evaluation of our various models. Team ECNU (Lan et al., 2017) and Fortia-FBK (Mansar et al., 2017) were the top systems for sub-tracks 1 and 2 respectively. Team ECNU and Fortia-FBK reported a cosine similarity of 0.777 and 0.745 for sub-tracks 1 and 2 respectively. Team ECNU employed a number of systems - Support Vector Regression, XGBoost Regressor, AdaBoost Regressor and Bagging Regressor ensembled together. Team Fortia-FBK used a Convolutional Neural Network for this task. The system proposed by Akhtar et al. utilizes an ensemble of LSTM, GRU, CNN and a SVR and reported a cosine similarity of 0.797 and 0.786 for the two sub-tracks. Our proposed system has a cosine similarity of 0.794 a"
N18-2041,D14-1162,0,0.0809956,"d test instances respectively. The task was to predict a regression score in between -1 and 1 indicating the sentiment with -1 being negative and +1 being positive. 2.2.2 Distributional Thesaurus Distributional Thesaurus (DT) (Biemann and Riedl, 2013) is an automatically computed word list which ranks words according to their semantic similarity. We use a pre-trained DT to expand a current word. For each current word, top-4 target words are found which are the relevant terms. The relevant embeddings are obtained by using a 300-dimensional pre-trained Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) model. An example of the DT expansion of the word ’touchpad’ is mouse, trackball, joystick and trackpad. 2.3 Experiments https://wordnet.princeton.edu 255 3.3 Models Single systems Mansar et al. (Team Fortia-FBK) Akhtar et al. - LSTM Akhtar et al. - GRU Akhtar et al. - CNN L3 (proposed) Ensembled systems Lan et al. (Team ECNU) Akhtar et al. E1 (proposed) Results We compare our system with the state-of-the-art systems of SemEval 2017 Task 5 and the system proposed by Akhtar et al. (2017). Table 1 shows evaluation of our various models. Team ECNU (Lan et al., 2017) and Fortia-FBK (Mansar et al."
N18-2041,P13-2005,0,0.0343581,"he-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively. 1 Introduction With the rise of microblogging websites, people have access and option to reach to the large crowd using as few words as possible. Microblog and news headlines are one of the common ways to dispense information online. The dynamic nature of these texts can be effectively used in the financial domain to track and predict the stock prices (Goonatilake and Herath, 2007). These can be used by an individual or an organization to make an informed prediction related to any company or stock (Si et al., 2013). This gives rise to an interesting problem of sentiment analysis in financial domain. A study indicates that sentiment analysis of public mood derived from Twitter feeds can be used to eventually forecast movements of individual stock prices (Smailovi´c et al., 2014). An efficient system for sentiment analysis is a core component of a company involved in financial stock market price prediction. 253 Proceedings of NAACL-HLT 2018, pages 253–258 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 → − ← − The two hidden states ht and ht for forward and bac"
N18-2041,H05-1044,0,0.0997265,"LSTM) is a special kind of Recurrent Neural Network. It handles the long-term dependencies where the current output is dependent on many prior inputs. BiLSTM, in essence, is a combination of two different LSTM - one working in forward and the other working in the backward direction. The contextual information about both past and future helps in determining the current output. αt ∝ hcTt Ws us H= X t αt hbt (4) (5) where Ws is a parameter matrix and us is the context vector to be learned. H is finally fed to a one layer feed forward neural network. 254 2.2 Relevant Terms and Embeddings lexicon (Wilson et al., 2005), SentiWordNet (Baccianella et al., 2010) and Vader sentiment (Gilbert, 2014). From the above lexicons we extracted the agreement score (Rao and Srivastava, 2012) and the count of the number of occurrences of all positive and negative words in the text. - Word embedding: We use the 300-dimensional pre-trained Word2Vec and GloVe embedding. The sentence embedding was obtained by concatenating the embedding for all words in the sentence. External knowledge can provide explicit information for the model which the training data lacks. This helps the model to make better predictions. We relied on Kn"
N18-2041,N16-1174,0,0.085616,". (2015) used a fine-grained sentiment annotation scheme. Kumar et al. (2017) used a classical supervised approach based on Support Vector Regression for sentiment analysis in financial domain. Oliveira et al. (2013) relied on multiple regression models. Akhtar et al. (2017) used an ensemble of four different systems for predicting the sentiment. It used a combination of Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014), Convolutional Neural Network (CNN) (Kim, 2014) and Support Vector Regression (SVR) (Smola and Sch¨olkopf, 2004). Yang et al. (2016) used a hierarchical attention network to build the document representation incrementally for document classification. Our model focuses on interpretability and usage of knowledge bases. Knowledge bases have been recognized important for natural language understanding tasks (Minsky, 1986). Our main contribution is a two-layered attention network which utilizes background knowledge bases to build good word level representation at the primary level. The secondary attention mechanism works on top of the primary layer to build meaningful sentence representations. This provides a good intuitive wor"
N18-2041,S17-2089,0,\N,Missing
N19-1085,W06-0901,0,0.0221327,"ts, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility L"
N19-1085,araki-etal-2014-detecting,0,0.0839054,"n event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and"
N19-1085,D17-1226,0,0.583099,"Missing"
N19-1085,D15-1247,0,0.312944,"(2013) provide empirical support for the usefulness of event arguments for event coreference resolution. Hence, it should not be surprising that, with just a few exceptions (e.g., Sangeetha and 786 ma m1 m2 m3 event mention The result of the election last October surprised everyone. He was elected as president in 2005. The presidential election took place on October 20th. The opposition party won the election. DATE-compatibility with ma no yes yes Table 1: Examples of NER-based sample filtering. The phrases tagged as DATE are underlined, and the trigger words are boldfaced. 3.1 Arock (2012); Araki and Mitamura (2015); Lu and Ng (2017)), argument features have been extensively exploited in event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity b"
N19-1085,R13-1021,0,0.547603,"een extensively exploited in event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al."
N19-1085,J14-2004,0,0.266736,"een proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train the model as an argument compatibility classifi"
N19-1085,I13-1100,1,0.839226,"e aforementioned challenge, we propose a framework for transferring argument (in)compatibility knowledge to the event coreference resolution system, specifically by adopting the interactive inference network (Gong et al., 2018) as our model structure. The idea is as follows. First, we train a network to determine whether the corresponding arguments of an event mention pair are compatible on automatically labeled training instances collected from a large unlabeled news corpus. Second, to transfer the knowledge of argument (in)compatibility to an 2 Related Work Ablation experiments conducted by Chen and Ng (2013) provide empirical support for the usefulness of event arguments for event coreference resolution. Hence, it should not be surprising that, with just a few exceptions (e.g., Sangeetha and 786 ma m1 m2 m3 event mention The result of the election last October surprised everyone. He was elected as president in 2005. The presidential election took place on October 20th. The opposition party won the election. DATE-compatibility with ma no yes yes Table 1: Examples of NER-based sample filtering. The phrases tagged as DATE are underlined, and the trigger words are boldfaced. 3.1 Arock (2012); Araki a"
N19-1085,chen-ng-2014-sinocoreferencer,1,0.911459,"inary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pret"
N19-1085,N15-1116,1,0.936491,"Missing"
N19-1085,P82-1020,0,0.819375,"Missing"
N19-1085,K16-1024,0,0.269017,"the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this"
N19-1085,P17-1171,0,0.0133487,"ihood above threshold θM are added to the new compatible set. On the other hand, mention pairs with a coreference likelihood below θm are added to the initial incompatible set to form the new incompatible set. With the new compatible and incompatible sets, we can start another iteration of transfer learning to train a coreference resolver with improved quality. In this work, we set θM to 0.8 and θm to 0.2. Exact match A binary feature indicating whether a given token appears in the context of both event mentions. This feature is proved useful for several NLP tasks operating on pairs of texts (Chen et al., 2017; Gong et al., 2018; Pan et al., 2018). Trigger position We encode the position of the trigger word by adding a binary feature to indicate whether a given token is a trigger word. 789 Figure 3: Model structure. 4 Encoding layer We pass the sequence of embedding vectors into a biLSTM layer (Hochreiter and Schmidhuber, 1997), resulting in a sequence of hidden vectors of size |h|: hia = biLST M (emb(wai ), hi−1 a ) hib = biLST M (emb(wbi ), hi−1 b ) 4.1 4.1.1 Experimental Setup Corpora We use English Gigaword (Parker et al., 2009) as the unlabeled corpus for argument compatibility learning. This"
N19-1085,D12-1045,0,0.211618,"e coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train the model as an argument compatibility classifier with event mentions extracted from a large unlabeled news corpus. Task definition Given a pair of event mentions (ma , mb ) with related triggers, predict whether their arguments are compatible or not. Here"
N19-1085,W09-3208,0,0.410956,"n party won the election. DATE-compatibility with ma no yes yes Table 1: Examples of NER-based sample filtering. The phrases tagged as DATE are underlined, and the trigger words are boldfaced. 3.1 Arock (2012); Araki and Mitamura (2015); Lu and Ng (2017)), argument features have been extensively exploited in event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng"
N19-1085,D16-1038,0,0.544427,"Missing"
N19-1085,liu-etal-2014-supervised,0,0.642134,"systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in or"
N19-1085,L16-1631,1,0.818701,"ether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train"
N19-1085,D14-1162,0,0.079855,"m preprocessing on the input data. Inference layer In the pretraining stage, we feed fev to a fully-connected inference layer to make a binary prediction of argument compatibility. As for the fine-tuning stage, we concatenate an auxiliary feature vector faux to fev before feeding it into the inference layer. faux consists of two features, a one-hot vector that encodes the sentence distance between the two event mentions and the difference of the word embedding vectors of the two triggers. Network structure Each word embedding is initialized with the 300-dimensional pretrained GloVe embedding (Pennington et al., 2014). The character embedding layer is a combination of an 8-dimensional embedding layer and three 1D convolution layers with a kernel size of 5 with 100 filters. The size of the biLSTM layer is 200. The maximum length of a word is 16 characters; shorter words are padded with zero and longer 790 biLSTM (standard) biLSTM (transfer) Interact (standard) Interact (transfer) Interact (transfer, 2nd iter) Interact (transfer, 3rd iter) Jiang et al. (2017) MUC 29.49 33.84 31.12 34.28 35.66 36.05 30.63 B3 43.15 42.91 42.84 42.93 43.20 43.07 43.84 CEAFe 39.91 38.39 39.01 39.95 40.02 39.69 39.86 BLANC 24.15"
N19-1085,P17-1009,1,0.738198,"upport for the usefulness of event arguments for event coreference resolution. Hence, it should not be surprising that, with just a few exceptions (e.g., Sangeetha and 786 ma m1 m2 m3 event mention The result of the election last October surprised everyone. He was elected as president in 2005. The presidential election took place on October 20th. The opposition party won the election. DATE-compatibility with ma no yes yes Table 1: Examples of NER-based sample filtering. The phrases tagged as DATE are underlined, and the trigger words are boldfaced. 3.1 Arock (2012); Araki and Mitamura (2015); Lu and Ng (2017)), argument features have been extensively exploited in event coreference systems to capture the argument compatibility between two event mentions. Basic features such as the number of overlapping arguments and the number of unique arguments, and a binary feature encoding whether arguments are conflicting have been proposed (Chen et al., 2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument he"
N19-1085,C16-1308,1,0.831745,"the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train the model as an argument compatibility classifier with event mentions extracted from a large unlabeled news corpus. Task definition Given a pair of event mentions (ma , mb ) with related triggers, predict whether their arguments are compatible or not. Here, an event mention"
N19-1085,J01-4004,0,0.456765,"sentence pairs (Gong et al., 2018). Thus, we believe this network is suitable for capturing the argument compatibility between two event mentions. The model consists of the following components: Model inputs The input to the model is a pair of event mentions (ma , mb ), with ma being the antecedent mention of mb : Mention-Pair Event Coreference Model With the argument compatibility classifier trained in the previous stage, we use the labeled event coreference corpus to fine-tune the model into an event coreference resolver. We design the event coreference resolver to be a mention-pair model (Soon et al., 2001), which takes a pair of event mentions as the input and outputs the likelihood of them being coreferent. ma = {wa1 , wa2 , ..., waN } mb = {wb1 , wb2 , ..., wbN } (1) Each event mention is represented by a sequence of N tokens consisting of one trigger word and its context. Here, we take the context to be the words within an n-word window around the trigger. In this work, n is set to 10. With the pairwise event coreference predictions, we further conduct best-first clustering (Ng and Cardie, 2002) on the pairwise results to build the event coreference clusters of each document. Best-first clus"
N19-1085,H05-1004,0,0.604554,"d 2. As for the interactive inference network, an improvement of 1.75 points in AVG-F is achieved, as can be seen in rows 3 and 4. These results provide suggestive evidence that our proposed transfer learning framework, which utilizes a large unlabeled corpus to perform argument compatibility learning, is effective. Evaluation Metrics We follow the standard evaluation setup adopted in the official evaluation of the KBP event nugget detection and coreference task. This evaluation setup is based on four distinct scoring measures — MUC (Vilain et al., 1995) , B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011) — and the unweighted average of their F-scores (AVG-F). We use AVG-F as the main evaluation measure when comparing system performances. 4.2 Effect of iterative relabeling We achieve another boost in performance by using the trained event coreference resolver to relabel the training samples for argument compatibility learning. The best result is achieved after two iterations (row 5) with an improvement of 2.26 points in AVG-F compared to the standard interactive inference network (row 3). However, we are not able to obtain further gains with more iterations"
N19-1085,P10-1040,0,0.00563563,"teractive inference network can better capture the complex interactions between two event mentions, accounting for the difference in performance. words are cropped. For the interaction layer, we use convolution layers with a kernel size of 3 in combination with max-pooling layers. The size of the inference layer is 128. Sigmoid activation is used for the inference layer, and all other layers use ReLU as the activation function. Event mention detection model For word embeddings, we use the concatenation of a 300dimensional pretrained GloVe embedding and the 50-dimensional embedding proposed by Turian et al. (2010). The character embedding layer is a combination of an 8-dimensional embedding layer and three 1D convolution layers with kernel sizes of 3, 4, 5 with 50 filters. 4.1.3 Effect of transfer learning Regardless of the network structure, we observe a considerable improvement in performance by pretraining the model as an argument compatibility classifier. The biLSTM baseline model achieves an improvement of 1.25 points in AVG-F by doing transfer learning, as can be seen in rows 1 and 2. As for the interactive inference network, an improvement of 1.75 points in AVG-F is achieved, as can be seen in r"
N19-1085,P14-5010,0,0.00326788,"ence clusters of these event mentions. (2) where emb(w) is the embedding vector of token w. Interaction layer The interaction layer captures the relations between two event mentions based on the hidden vectors ha and hb . The interaction tensor I, a 3-D tensor of shape (N , N , |h|), is calculated by taking the pairwise multiplication of the corresponding hidden vectors: Iij = hia ◦ hjb Evaluation (3) Finally, we apply a multi-layer convolutional neural network to extract the event pair representation vector fev . 4.1.2 Implementation Details Preprocessing We use the Stanford CoreNLP toolkit (Manning et al., 2014) to perform preprocessing on the input data. Inference layer In the pretraining stage, we feed fev to a fully-connected inference layer to make a binary prediction of argument compatibility. As for the fine-tuning stage, we concatenate an auxiliary feature vector faux to fev before feeding it into the inference layer. faux consists of two features, a one-hot vector that encodes the sentence distance between the two event mentions and the difference of the word embedding vectors of the two triggers. Network structure Each word embedding is initialized with the 300-dimensional pretrained GloVe e"
N19-1085,M95-1005,0,0.878772,"VG-F by doing transfer learning, as can be seen in rows 1 and 2. As for the interactive inference network, an improvement of 1.75 points in AVG-F is achieved, as can be seen in rows 3 and 4. These results provide suggestive evidence that our proposed transfer learning framework, which utilizes a large unlabeled corpus to perform argument compatibility learning, is effective. Evaluation Metrics We follow the standard evaluation setup adopted in the official evaluation of the KBP event nugget detection and coreference task. This evaluation setup is based on four distinct scoring measures — MUC (Vilain et al., 1995) , B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and BLANC (Recasens and Hovy, 2011) — and the unweighted average of their F-scores (AVG-F). We use AVG-F as the main evaluation measure when comparing system performances. 4.2 Effect of iterative relabeling We achieve another boost in performance by using the trained event coreference resolver to relabel the training samples for argument compatibility learning. The best result is achieved after two iterations (row 5) with an improvement of 2.26 points in AVG-F compared to the standard interactive inference network (row 3). However, we are not"
N19-1085,Q15-1037,0,0.304132,"2009; Chen and Ji, 2009; Chen and Ng, 2016). More sophisticated features based on different kinds of similarity measures have also been considered, such as the surface similarity based on Dice coefficient and the WuPalmer WordNet similarity between argument heads (McConky et al., 2012; Cybulska and Vossen, 2013; Araki et al., 2014; Liu et al., 2014; Krause et al., 2016). However, these features are computed using either the outputs of event argument extractors and entity coreference resolvers (Ahn, 2006; Chen and Ng, 2014, 2015; Lu and Ng, 2016) or semantic parsers (Bejan and Harabagiu, 2014; Yang et al., 2015; Peng et al., 2016) and therefore suffer from serious error propagation issues (see Lu and Ng (2018)). Several previous works proposed joint models to address this problem (Lee et al., 2012; Lu et al., 2016), while others utilized iterative methods to propagate argument information (Liu et al., 2014; Choubey and Huang, 2017) in order to alleviate this issue. However, all of these methods still rely on argument extractors to identify arguments and their roles. 3 Argument Compatibility Learning In the pretraining stage, we train the model as an argument compatibility classifier with event menti"
N19-1085,P02-1014,1,0.591394,"an event coreference resolver. We design the event coreference resolver to be a mention-pair model (Soon et al., 2001), which takes a pair of event mentions as the input and outputs the likelihood of them being coreferent. ma = {wa1 , wa2 , ..., waN } mb = {wb1 , wb2 , ..., wbN } (1) Each event mention is represented by a sequence of N tokens consisting of one trigger word and its context. Here, we take the context to be the words within an n-word window around the trigger. In this work, n is set to 10. With the pairwise event coreference predictions, we further conduct best-first clustering (Ng and Cardie, 2002) on the pairwise results to build the event coreference clusters of each document. Best-first clustering is an agglomerative clustering algorithm that links each event mention to the antecedent event mention with the highest coreference likelihood given the likelihood is above an empirically determined threshold. 3.3 Model Structure Embedding layer We represent each input token by the concatenation of the following components: Word embedding The word representation of the given token. We use pretrained word vectors to initialize the word embedding layer. Character embedding To identify (in)com"
N19-1085,P18-1091,0,0.0199144,"the new compatible set. On the other hand, mention pairs with a coreference likelihood below θm are added to the initial incompatible set to form the new incompatible set. With the new compatible and incompatible sets, we can start another iteration of transfer learning to train a coreference resolver with improved quality. In this work, we set θM to 0.8 and θm to 0.2. Exact match A binary feature indicating whether a given token appears in the context of both event mentions. This feature is proved useful for several NLP tasks operating on pairs of texts (Chen et al., 2017; Gong et al., 2018; Pan et al., 2018). Trigger position We encode the position of the trigger word by adding a binary feature to indicate whether a given token is a trigger word. 789 Figure 3: Model structure. 4 Encoding layer We pass the sequence of embedding vectors into a biLSTM layer (Hochreiter and Schmidhuber, 1997), resulting in a sequence of hidden vectors of size |h|: hia = biLST M (emb(wai ), hi−1 a ) hib = biLST M (emb(wbi ), hi−1 b ) 4.1 4.1.1 Experimental Setup Corpora We use English Gigaword (Parker et al., 2009) as the unlabeled corpus for argument compatibility learning. This corpus consists of the news articles f"
N19-1281,D07-1090,0,0.0345957,"Neural models with word or n2744 Proceedings of NAACL-HLT 2019, pages 2744–2755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics gram embeddings are even larger, easily reaching gigabytes. This makes it difficult to deploy MA in space-constrained environments such as mobile applications and browsers. It has been shown that simple or straightforward models can match or outperform complex models when using a large number of training data. For example, a straightforward backoff technique rivals a complicated smoothing technique for language models (Brants et al., 2007). Pretraining a bidirectional language model on a large dataset helps to solve a variety of NLP tasks (Devlin et al., 2018). Our approach is inspired by this line of work. Contributions We propose a very straightforward fully-neural morphological analyzer which uses only character unigrams as its input1 . Such an analyzer, when trained only on human-annotated gold data has low accuracy. However, when trained on a large amount of automatically tagged silver data, the analyzer rivals and even outperforms, albeit slightly, the bootstrapping analyzer. We conclude that there is no need for rich inp"
N19-1281,P16-1039,0,0.017769,"and performing heavyweight NN evaluation only after lightweight scoring by the linear model. Direct lattice-based approaches are not very popular for Chinese, but some are lattice-based in spirit. A line of work by Zhang and Clark (2008, 2010) builds the lattice dynamically from partial words, searching paths with a perceptron-based scorer and customized beam search. The dictionary is built dynamically from the training data as frequent word-tag pairs which help the system to prune unlikely POS tags for word candidates. One more variation on lattice-based approaches for Chinese is the work by Cai and Zhao (2016). In this work, a segmentation dictionary is used to construct a subnetwork, which combines character representations into word representations used for computing sentence-wise segmentation scores. This can be seen as explicitly learning dictionary information by a model. Resulting segmentation is still created from the start to the end by growing words one by one while performing beam search. The follow up (Cai et al., 2017) simplifies that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS ta"
N19-1281,I13-1018,0,0.0162083,"tions. This also can be seen as dynamically constructing a lattice while performing the search in it at the same time. Lattice-based approaches are popular for the Japanese language. Most of the time, the lattice is based on words which are present in a segmentation dictionary and a rule-based component for handling out-of-dictionary words. Usually, there are no machine-learning components in lattice creation, but the scoring can be machine-learning based. We believe that the availability of high quality consistent morphological analysis dictionaries is the reason for that. Still, the work of Kaji and Kitsuregawa (2013) is a counterexample of a lattice-based approach for Japanese which uses a machine-learning component for creating the lattice. Traditional lattice-based approaches for Japanese use mostly POS tags or other hidden information accessible from the dictionary to score paths through the lattice. JUMAN (Kurohashi, 1994) is one of the first analyzers, which uses a hidden Markov model with manually-tuned weights for scoring. Lattice path scores are computed using connection weights for each pair of part of speech tags. Probably the most known and used morphological analyzer for Japanese is MeCab (Kud"
N19-1281,P17-2096,0,0.0165623,"frequent word-tag pairs which help the system to prune unlikely POS tags for word candidates. One more variation on lattice-based approaches for Chinese is the work by Cai and Zhao (2016). In this work, a segmentation dictionary is used to construct a subnetwork, which combines character representations into word representations used for computing sentence-wise segmentation scores. This can be seen as explicitly learning dictionary information by a model. Resulting segmentation is still created from the start to the end by growing words one by one while performing beam search. The follow up (Cai et al., 2017) simplifies that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS tagging. Transition-based approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et"
N19-1281,D18-2012,0,0.0344,"e 1: Proposed model. We encode character unigram embeddings into shared representations for each character. The shared representation is projected into a tag-specific representations from which we independently infer segmentation and per-character tags. Introduction Languages with a continuous script, like Japanese and Chinese, do not have natural word boundaries in most cases. Natural language processing for such languages requires to perform some variation of word segmentation. Although some NLP applications, like neural machine translation, started to use unsupervised segmentation methods (Kudo and Richardson, 2018), resulting segmentation often has decisions which are not natural to humans. Supervised segmentation based on a human-defined standard is essential for applications which are designed for interaction on a word-level granularity, for example, full-text search. Segmentation is commonly done jointly with part of speech (POS) tagging and usually referred to as Morphological Analysis. Modern Japanese Morphological Analyzers (MA) are very accurate, having a >99 segmentation tokenwise F1 score on news domain and a >98.5 F1 on web domain (Tolmachev et al., 2018). They often use segmentation dictionar"
N19-1281,D16-1070,0,0.0255772,"train word and character n-gram embeddings. Yang et al. (2017) pretrain a part of the model on different data sources, including automatically segmented text, but the model itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation standards). In addition to corpus statistics from a raw corpus, Zhao and Kit (2008) exploit heterogeneous annotations. Li et al. (2015) use corpora with different annotation standards. They combine tags into “bundles” (e.g. [NN, n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard (2010) show that tri-training helps English POS-tagging with SVM and MaxEntbased approaches. Zhou et al. (2017) use selftraining and tri-training for Chinese word segmentation. They, h"
N19-1281,W04-3230,0,0.0191528,"13) is a counterexample of a lattice-based approach for Japanese which uses a machine-learning component for creating the lattice. Traditional lattice-based approaches for Japanese use mostly POS tags or other hidden information accessible from the dictionary to score paths through the lattice. JUMAN (Kurohashi, 1994) is one of the first analyzers, which uses a hidden Markov model with manually-tuned weights for scoring. Lattice path scores are computed using connection weights for each pair of part of speech tags. Probably the most known and used morphological analyzer for Japanese is MeCab (Kudo et al., 2004), where CRFs were used for learning the scoring. MeCab is very fast: it can analyze almost 50k sentences per second. It also achieves acceptable accuracy, and so the tool is very popular. The speed is realized by precomputing feature weights, but it takes a lot of space when the total number of features gets large. For example, the UniDic model for modern Japanese v2.3.02 takes 5.5GB because it uses many feature templates. There were studies which tried to integrate NN into lattice-based approaches as well. Juman++ (Morita et al., 2015) uses dictionary-based lattice construction with the combi"
N19-1281,N19-1423,0,0.0470242,"Missing"
N19-1281,Y12-1058,1,0.800125,"on and 2 for the first POS tag layer. 4 Experiments We conduct experiments on Japanese morphological analysis. For training we use two data sources. The first is usual human-annotated gold training data. The second is silver data from the results of automatic analysis. We use Juman++ V2 – the current state-of-the-art analyzer for the JUMAN segmentation standard as the bootstrap analyzer. We use two gold corpora. The first is the Kyoto University Text Corpus (Kurohashi and Nagao (2003), referred to as KU), containing newspaper data. The second is the Kyoto University Web Document Leads Corpus (Hangyo et al. (2012), referred to as Leads) which consists of web documents. Corpus statistics are shown in Table 1. We denote models which use gold training data by G. We take raw data to generate our silver annotated data from a crawled web corpus of 9.8B unique sentences. We sample 3B sentences randomly from it and analyze them using the Juman++ baseline model. From it we sample 500M sentences, which become our training silver data, prioritizing sentences which contain at least one not very frequent word. We prepare both top-scored (denoted as T) and non-ambigous in beam (denoted as B) variants of the silver d"
N19-1281,P12-1110,0,0.0271262,"d approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features beforehand and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods"
N19-1281,P17-1111,1,0.788175,"put data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features beforehand and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods pretrain word and cha"
N19-1281,P15-1172,0,0.0273814,"and and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods pretrain word and character n-gram embeddings. Yang et al. (2017) pretrain a part of the model on different data sources, including automatically segmented text, but the model itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation standards). In addition to corpus statistics from a raw corpus, Zhao and Kit (2008) exploit heterogeneous annotations. Li et al. (2015) use corpora with different annotation standards. They combine tags into “bundles” (e.g. [NN, n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard"
N19-1281,D11-1090,0,0.0311703,"ready existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features beforehand and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods pretrain word and character n-gram embeddings. Yang et al. (2017) pretrain a part of the model on different data sources, including automatically segmented text, but the model itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation sta"
N19-1281,D18-1529,0,0.0249978,"Missing"
N19-1281,P08-1076,0,0.0539664,"odel itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation standards). In addition to corpus statistics from a raw corpus, Zhao and Kit (2008) exploit heterogeneous annotations. Li et al. (2015) use corpora with different annotation standards. They combine tags into “bundles” (e.g. [NN, n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard (2010) show that tri-training helps English POS-tagging with SVM and MaxEntbased approaches. Zhou et al. (2017) use selftraining and tri-training for Chinese word segmentation. They, however, also pretrain other features like word-context character embeddings, chraracter unigrams and bigrams. 3 Proposed Approach In order for MA to be practical, it should be"
N19-1281,P15-1167,0,0.0198254,"he follow up (Cai et al., 2017) simplifies that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS tagging. Transition-based approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as fe"
N19-1281,D15-1276,1,0.858442,"nown and used morphological analyzer for Japanese is MeCab (Kudo et al., 2004), where CRFs were used for learning the scoring. MeCab is very fast: it can analyze almost 50k sentences per second. It also achieves acceptable accuracy, and so the tool is very popular. The speed is realized by precomputing feature weights, but it takes a lot of space when the total number of features gets large. For example, the UniDic model for modern Japanese v2.3.02 takes 5.5GB because it uses many feature templates. There were studies which tried to integrate NN into lattice-based approaches as well. Juman++ (Morita et al., 2015) uses dictionary-based lattice construction with the combination of two models for path scoring: the feature-based linear model using soft-confidence weighted learning (Wang et al., 2016) and a recurrent neural network (Mikolov, 2012). It significantly reduced the number of both segmentation and POS tagging errors. However, it was very slow, being able to analyze only about 15 sentences per second, hence the original version was impractical. The following improvement (Tolmachev et al., 2018) greatly increased analysis speed by doing aggressive beam trimming and performing heavyweight NN evalua"
N19-1281,P11-2093,0,0.0265267,"hould be possible to segment a sentence using the dictionary entries only in a single correct way. Such dictionaries are often maintained together with annotated corpora. On the other hand, Chinese-focused systems do not put much focus on dictionaries. Still, almost all aproaches use rich feature templates or additional resources such as pretrained character n-gram or word embeddings, which increase the model size. Pointwise approaches make a segmentation decision independently for each position. They can be seen as a sequence tagging task. Such approaches are more popular for Chinese. KyTea (Neubig et al., 2011) is an example of this approach in Japanese. It makes a binary decision for each character: whether to insert a boundary before it or not. It can be seen as sequence tagging with {B, I} tagset. POS tagging is done after inferring segmentation. The decisions are made by feature-based approaches, using characters, character n-grams, character type information, and dictionary information as features. KyTea can use word features obtained from a dictionary. It checks whether the character sequence before and after the current character forms a word from the dictionary. It also checks whether the cu"
N19-1281,C04-1081,0,0.105546,"ictionary. It checks whether the character sequence before and after the current character forms a word from the dictionary. It also checks whether the current word is inside a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition a"
N19-1281,I17-1018,0,0.0180553,"sequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition as well. The character representation was pretrained on a language modeling task. Shao et al. (2017) used a bidirectional recurrent network with GRU cells followed by a CRF layer for joint segmentation and POS tagging. They used pretrained character n-gram embeddings together with sub-character level information extracted by CNNs as features. Using a dictionary with NN is also popular (Zhang et al., 2018b; Liu et al., 2018). Search-based approaches induce a structure over a sentence and perform a search over it. A most frequently used structure is a lattice which contains all possible segmentation tokens. The search then finds the highest scoring path through the lattice. Another branch of s"
N19-1281,P10-2038,0,0.024593,"(2015) use corpora with different annotation standards. They combine tags into “bundles” (e.g. [NN, n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard (2010) show that tri-training helps English POS-tagging with SVM and MaxEntbased approaches. Zhou et al. (2017) use selftraining and tri-training for Chinese word segmentation. They, however, also pretrain other features like word-context character embeddings, chraracter unigrams and bigrams. 3 Proposed Approach In order for MA to be practical, it should be not only accurate, but also fast and have relatively compact models. The speed of search-based approaches is dependent on how computationally heavy a weighting function is. Heavyweight models, like neural networks, require a large number of compu"
N19-1281,D18-2010,1,0.918019,"unsupervised segmentation methods (Kudo and Richardson, 2018), resulting segmentation often has decisions which are not natural to humans. Supervised segmentation based on a human-defined standard is essential for applications which are designed for interaction on a word-level granularity, for example, full-text search. Segmentation is commonly done jointly with part of speech (POS) tagging and usually referred to as Morphological Analysis. Modern Japanese Morphological Analyzers (MA) are very accurate, having a >99 segmentation tokenwise F1 score on news domain and a >98.5 F1 on web domain (Tolmachev et al., 2018). They often use segmentation dictionaries which define possible words. Also, their models are generally large and unwieldy, spanning hundreds of megabytes in case of traditional symbolic featurebased approaches. Neural models with word or n2744 Proceedings of NAACL-HLT 2019, pages 2744–2755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics gram embeddings are even larger, easily reaching gigabytes. This makes it difficult to deploy MA in space-constrained environments such as mobile applications and browsers. It has been shown that simple or strai"
N19-1281,I05-3027,0,0.0633992,"checks whether the current word is inside a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition as well. The character representation was pretrained on a language modeling task. Shao et al. (2017) used a bidirectional recurrent ne"
N19-1281,I11-1035,0,0.015456,"Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features beforehand and train an analyzer afterwards. In addition to that, Zhang et al. (2013) use a variation of smoothing for handling automatic annotation errors. A lot of neural-based methods pretrain word and character n-gram embeddings. Yang et al. (2017) pretrain a part of the model on different data sources, including automatically segmented text, but the model itself is trained only on the gold data. Another approach is to use heterogeneous data (annotated in incompatible annotation standards). In addition to"
N19-1281,O03-4002,0,0.176261,"n use word features obtained from a dictionary. It checks whether the character sequence before and after the current character forms a word from the dictionary. It also checks whether the current word is inside a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags"
N19-1281,P17-1078,0,0.015059,"that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS tagging. Transition-based approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomute the features bef"
N19-1281,D13-1031,0,0.143383,"de a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition as well. The character representation was pretrained on a language modeling task. Shao et al. (2017) used a bidirectional recurrent network with GRU cells followed by a CRF l"
N19-1281,D12-1046,0,0.0612836,"Missing"
N19-1281,D17-1079,0,0.0183386,"n]) and infer them at the same time while paying attention to ambiguity. Chen et al. (2016) train a classifier that can annotate several standards jointly. Finally, it is possible to use raw or automaticallyannotated data directly. A study (Suzuki and Isozaki, 2008) is an example of a feature-based algorithm which uses raw data. Tri-training (Zhou and Li, 2005) is a generic way to use raw data. They propose to train on automatically analyzed examples where two of three diverse analyzers agree. Søgaard (2010) show that tri-training helps English POS-tagging with SVM and MaxEntbased approaches. Zhou et al. (2017) use selftraining and tri-training for Chinese word segmentation. They, however, also pretrain other features like word-context character embeddings, chraracter unigrams and bigrams. 3 Proposed Approach In order for MA to be practical, it should be not only accurate, but also fast and have relatively compact models. The speed of search-based approaches is dependent on how computationally heavy a weighting function is. Heavyweight models, like neural networks, require a large number of computations, and we think that it will be very difficult to create a practical search-based fully NN morpholo"
N19-1281,P16-1040,0,0.0155005,"., 2017) simplifies that model and shows that greedy search can be enough for estimating segmentation when using neural networks. Still, this line of work does not consider POS tagging. Transition-based approaches treat input data (most frequently – characters) as input queue and store a current, possibly incomplete, token in a buffer. Models usually infer whether they should create a new token from a character in the input 2746 2 https://unidic.ninjal.ac.jp/ queue or append an input character to the already existing token. Neural models are often used in this paradigm (Ma and Hinrichs, 2015; Zhang et al., 2016; Yang et al., 2017; Ma et al., 2018; Zhang et al., 2018a). Almost all of them use both word and charcter n-gram embeddings. This paradigm was extended to do parsing jointly with MA (Hatori et al., 2012; Kurita et al., 2017). Semi-supervised approaches to segmentation and POS tagging fall into several categories. The first one uses raw or automatically-annotated data to precompute feature representations and then uses these feature representations for supervised learning. For example, Sun and Xu (2011) and Wang et al. (2011) use data from automatically segmented texts as features. They precomu"
N19-1281,P08-1101,0,0.0439278,"., 2016) and a recurrent neural network (Mikolov, 2012). It significantly reduced the number of both segmentation and POS tagging errors. However, it was very slow, being able to analyze only about 15 sentences per second, hence the original version was impractical. The following improvement (Tolmachev et al., 2018) greatly increased analysis speed by doing aggressive beam trimming and performing heavyweight NN evaluation only after lightweight scoring by the linear model. Direct lattice-based approaches are not very popular for Chinese, but some are lattice-based in spirit. A line of work by Zhang and Clark (2008, 2010) builds the lattice dynamically from partial words, searching paths with a perceptron-based scorer and customized beam search. The dictionary is built dynamically from the training data as frequent word-tag pairs which help the system to prune unlikely POS tags for word candidates. One more variation on lattice-based approaches for Chinese is the work by Cai and Zhao (2016). In this work, a segmentation dictionary is used to construct a subnetwork, which combines character representations into word representations used for computing sentence-wise segmentation scores. This can be seen as"
N19-1281,D10-1082,0,0.0710804,"Missing"
N19-1281,Y06-1012,0,0.0229629,"urrent word is inside a word. Neural networks were shown to be useful for Japanese in this paradigm as well (Kitagawa and Komachi, 2017). They use character embeddings, character type embeddings, character n-gram embeddings, and tricks to incorporate dictionary information into the model. Many studies on Chinese adopt the pointwise approach. Often, the segmentation task is reformulated as sequence tagging (Xue, 2003) with {B, I, E, S} tagset. Peng et al. (2004) showed that CRFs help further in this task. This tactic was followed by many subsequent feature-based approaches (Tseng et al., 2005; Zhao et al., 2006; Zhang et al., 2013), using character n-gram, character type and word features. Neural networks were applied to this paradigm as well. Zheng et al. (2013) used a feed-forward network on character and categorical features that were shown to be useful for computing a segmentation score from a fixed window. Qi et al. (2014) used a similar architecture. They predicted not only segmentation but POS tags and performed 2745 named entity recognition as well. The character representation was pretrained on a language modeling task. Shao et al. (2017) used a bidirectional recurrent network with GRU cell"
P02-1028,S01-1004,0,\N,Missing
P02-1028,J94-4001,1,\N,Missing
P02-1028,S01-1008,0,\N,Missing
P02-1028,H01-1043,1,\N,Missing
P02-1028,P01-1008,0,\N,Missing
P03-2027,J94-4001,1,\N,Missing
P03-2027,W03-1108,0,\N,Missing
P03-2027,C02-1166,0,\N,Missing
P03-2027,W02-0902,0,\N,Missing
P03-2027,C02-1084,1,\N,Missing
P06-2097,C02-1122,1,0.766177,"り込み を入れます。(We cut in this cherry tomato, because we’ll fry it in oil.) Note that relations between clauses are recognized by clause-end patterns. Verb sense disambiguation by assigning to a case frame In general, a verb has multiple meanings/usages. For example, “入れる” has multiple usages, “塩を 入れ る (add salt)” and “包丁を 入れる (carve with a knife)” , which appear in different topics. We do not extract a surface form of verb but a case frame, which is assigned by case analysis. Case frames are automatically constructed from Web cooking texts (12 million sentences) by clustering similar verb usages (Kawahara and Kurohashi, 2002). An example of the automatically constructed case frame is shown in Table 3. For example, “塩を入れる (add salt)” is assigned to ireru:1 (add) and “包丁を入れる (carve with a knife)” is assigned to case frame ireru:2 (carve). 759 3.1.2 Cue phrases As Grosz and Sidner (Grosz and Sidner, 1986) pointed out, cue phrases such as now and well serve to indicate a topic change. We use approximately 20 domain-independent cue phrases, such as “では (then)”, “次は (next)” and “そうしまし たら (then)”. 3.1.3 Noun Chaining In text segmentation algorithms such as TextTiling (Hearst.M, 1997), lexical chains are widely utilized f"
P06-2097,J94-4001,1,0.635821,"t off a step of this eggplant.) お鍋にお水を入れます． (Pour water into a pan.) [food state] ex. ニンジンの水分がなくなりました． (There is no water in the carrot.) [note] ex. 芯は切らないで下さい． (Don’t cut this core off.) [substitution] ex. 青ねぎでも結構です． (You may use a leek.) [food/tool presentation] ex. 今日はこのハンド ミキサーを使います． Today, we use this handy mixer.) [small talk] ex. こんにちは． (Hello.) tures. An example of closed captions is shown in Figure 2. We first process them with the Japanese morphological analyzer, JUMAN (Kurohashi et al., 1994), and make syntactic/case analysis and anaphora resolution with the Japanese analyzer, KNP (Kurohashi and Nagao, 1994). Then, we perform the following process to extract linguistic features. 3.1.1 Extracting Utterances Referring to Actions Considering a clause as a basic unit, utterances referring to an action are extracted in the form of case frame, which is assigned by case analysis. This procedure is performed for generalization and word sense disambiguation. For example, “塩を入れる (add salt)” and “砂糖を鍋に入 れる (add sugar into a pan)” are assigned to case frame ireru:1 (add) and “包丁を入れる (carve with a knife)” is assigned to case frame ireru:2 (carve). We describe this procedure in detail below. Utterance-type rec"
P06-2097,N04-1015,0,0.0349953,"a motion is continuous, and extracted features such as color histograms and motion vectors. Then, they classified the shots based on HMMs into several classes (for baseball sports video, for example, pitch view, running overview or audience view). In these studies, to achieve high accuracy, they relied on handmade domain-specific knowledge or trained HMMs with manually labeled data. Therefore, they cannot be easily extended to new domains In the field of Natural Language Processing, Barzilay and Lee have recently proposed a probabilistic content model for representing topics and topic shifts (Barzilay and Lee, 2004). This content model is based on HMMs wherein a state corresponds to a topic and generates sentences relevant to that topic according to a state-specific language model, which are learned from raw texts via analysis of word distribution patterns. In this paper, we describe an unsupervised topic identification method integrating linguistic and visual information using HMMs. Among several types of videos, in which instruction videos (howto videos) about sports, cooking, D.I.Y., and others are the most valuable, we focus on cooking TV programs. In an example shown in Figure 1, preparation, sautei"
P06-2097,J00-3005,0,0.0183663,"1997). This method is based on lexical co-occurrence. Galley et al. presented a domain-independent topic segmentation algorithm for multi-party speech (Gal756 ley et al., 2003). This segmentation algorithm uses automatically induced decision rules to combine linguistic features (lexical cohesion and cue phrases) and speech features (silences, overlaps and speaker change). These studies aim just at segmenting a given text, not at identifying topics of segmented texts. Marcu performed rhetorical parsing in the framework of Rhetorical Structure Theory (RST) based on a discourse-annotated corpus (Marcu, 2000). Although this model is suitable for analyzing local modification in a text, it is difficult for this model to capture the structure of topic transition in the whole text. In contrast, Barzilay and Lee modeled a content structure of texts within specific domains, such as earthquake and finance (Barzilay and Lee, 2004). They used HMMs wherein each state corresponds to a distinct topic (e.g., in earthquake domain, earthquake magnitude or previous earthquake occurrences) and generates sentences relevant to that topic according to a state-specific language model. Their method first create cluster"
P06-2097,P03-1071,0,0.0296491,"ructed by hand. Therefore, referring to (Hamada et al., 2000), we focus our attention on color distribution at the bottom of the image, which is comparatively easy to exploit. As shown in Figure 1, we utilize the mass point of RGB in the bottom of the image at each clause. 3.3 Audio Features A cooking video contains various types of audio information, such as instructor’s speech, cutting sounds and frizzling sound. If cutting sound or frizzling sound could be distinguished from other sounds, they could be an aid to topic identification, but it is difficult to recognize them. As Galley et al. (Galley et al., 2003) pointed out, a longer silence often appears when topic changes, and we can utilize it as a clue to topic change. In this study, silence is automatically extracted by finding duration below a certain amplitude level which lasts more than one second. 4 Topic Identification based on HMMs We employ HMMs for topic identification, where a hidden state corresponds to a topic and various features described in Section 3 are observed. In our model, considering the case frame as a basic unit, the case frame and background image are observed from the state, and discourse features indicating to topic shif"
P06-2097,J86-3001,0,0.218376,", “塩を 入れ る (add salt)” and “包丁を 入れる (carve with a knife)” , which appear in different topics. We do not extract a surface form of verb but a case frame, which is assigned by case analysis. Case frames are automatically constructed from Web cooking texts (12 million sentences) by clustering similar verb usages (Kawahara and Kurohashi, 2002). An example of the automatically constructed case frame is shown in Table 3. For example, “塩を入れる (add salt)” is assigned to ireru:1 (add) and “包丁を入れる (carve with a knife)” is assigned to case frame ireru:2 (carve). 759 3.1.2 Cue phrases As Grosz and Sidner (Grosz and Sidner, 1986) pointed out, cue phrases such as now and well serve to indicate a topic change. We use approximately 20 domain-independent cue phrases, such as “では (then)”, “次は (next)” and “そうしまし たら (then)”. 3.1.3 Noun Chaining In text segmentation algorithms such as TextTiling (Hearst.M, 1997), lexical chains are widely utilized for detecting a topic shift. We utilize such a feature as a clue to topic persistence. When two continuous actions are performed to the same ingredient, their topics are often identical. For example, because “おろす (grate)” and “ 上げる (raise)” are performed to the same ingredient “かぶら"
P06-2097,C04-1174,1,0.84514,"t “かぶら (turnip)” , the topics (in this instance, preparation) in the two utterances are identical. (5) a. かぶらをおろし金でおろしていきます。 (We’ll grate a turnip.) b. お ろし た か ぶ ら をざ るに 上げ ま す。 (Raise this turnip on this basket.) However, in the case of spoken language, because there exist many omissions, it is often the case that noun chaining cannot be detected with surface word matching. Therefore, we detect noun chaining by using the anaphora resolution result2 of verbs (ex.(6)) and nouns (ex.(7)). The verb, noun anaphora resolution is conducted by the method proposed by (Kawahara and Kurohashi, 2004), (Sasano et al., 2004), respectively. (6) a. キャベツを切ります。 (Cut a cabbage.) b. 一度 [キャベツを] 洗います。 (Wash it once.) (7) a. にんじんを大体４ｃｍくらい切ります。 (Slice a carrot into 4-cm pieces.) b. [にんじ んの] 皮をぐ るっと むきます。 (Peel its skin.) 3.1.4 Verb Chaining When a verb of a clause is identical with that of the previous clause, they are likely to have the same topic. We utilize the fact that the adjoining two clauses contain an identical verbs or not as an observed feature. (8) a. とうがらしを入れて下さい。(Add some red peppers.) 2 [ ] indicates an element complemented with anaphora resolution. - case frame bj (cfk ): the probability that case frame cfk"
P06-2097,J97-1003,0,0.190978,"Missing"
P07-2035,P91-1019,0,0.0582789,"f the domains, such as tourism and shampoo, are often used in the web sites of companies (BUSINESS) that provide services or goods related to RECREATION or LIVING. As a result, the method tends to wrongly associate those words with BUSINESS. ING,       6 Related Work HowNet (Dong and Dong, 2006) and WordNet provide domain information for Chinese and English, but there has been no domain resource for Japanese that are publicly available.8 Domain dictionary construction methods that have been developed so far are all based on highly structured lexical resources like LDOCE or WordNet (Guthrie et al., 1991; Agirre et al., 2001) and hence not applicable to languages for which such highly structured lexical resources are not available. Accordingly, contributions of this study are twofold: (i) We constructed the first Japanese domain dictionary that is fully available. (ii) We developed the domain dictionary construction method that requires neither document collections nor highly structured lexical resources. 8 Some human-oriented dictionaries provide domain information. However, domains they cover are all technical ones rather than common domains such as those assumed here. 140 7 Conclusion Towa"
P07-2035,1997.tmi-1.2,0,0.00861697,"developed a technique for estimating the domain of unknown words. CULTURE 1 Introduction We constructed a lexical resource that represents the domain relation among Japanese fundamental words (JFWs), and we call it the domain dictionary. 1 It associates JFWs with domains in which they are typhome run is ically used. For example, 2 associated with the domain SPORTS . That is, we aim to make explicit the horizontal relation between words, the domain relation, while thesauri indicate the vertical relation called IS-A. 3  1 In fact, there have been a few domain resources in Japanese like Yoshimoto et al. (1997). But they are not publicly available. 2 Domains are CAPITALIZED in this paper. 3 The lack of the horizontal relationship is also known as the “tennis problem” (Fellbaum, 1998, p.10). LIVING SCIENCE RECREATION DIET BUSINESS SPORTS TRANSPORTATION MEDIA HEALTH EDUCATION GOVERNMENT It has been created based on web directories such as Open Directory Project with some adjustments. In addition, NODOMAIN was prepared for those words that do not belong to any particular domain. As for the latter issue, you might use keyword extraction techniques; identifying words that represent a domain from the docu"
P08-2018,P07-2035,1,0.412593,"Missing"
P08-2018,P04-1033,0,0.030175,"07), it is in an early phase of development. In contrast, our method requires no training data. All you need is a manageable amount of fundamental words with domains. Also note that our method is NOT tailored to the 12 domains. If you want your own domains to categorize, it is only necessary to construct your own dictionary, which is also domain-independent and not time-consuming. In fact, there have been other proposals without the burden of preparing training data. Liu et al. (2004) prepare representative words for each class, by which they collect initial training data to build classifier. Ko and Seo (2004) automatically collect training data using a large amount of unlabeled data and a small amount of seed information. However, the novelty of this study is the on-the-fly estimation of unknown words’ domains. This feature is very useful for categorizing Blog articles that are updated on a daily basis and filled with newly coined words. Domain information has been used for many NLP tasks. Magnini et al. (2002) show the effectiveness of domain information for WSD. Piao et al. (2003) use domain tags to extract MWEs. Previous domain resources include WordNet 72 (Fellbaum, 1998) and HowNet (Dong and"
P08-2018,E03-1056,0,0.0273939,"Top N 1. 2. 3. F only 0.89 0.96 0.98 F+SU 0.91 0.97 0.98 F+AU 0.94 0.98 0.99 Components Module This is basically the same as the others except that it extracts fundamental words from the unknown word itself. For example, the domain of finance market is estimated from the domains of finance and market. 5.1 SPOR 42 15 27 22 Result of Blog Categorization Table 3 shows the accuracy of categorization. The F only column indicates that a rather simple method like the one in §3 works well, if fundamental words are given good clues for categorization: the domain 71 in our case. This is consistent with Kornai et al. (2003), who claim that only positive evidence matter in categorization. Also, F+SU slightly outperformed F only, and F+AU outperformed the others. This shows that the domain estimation of unknown words moderately improves Blog categorization. Errors are mostly due to the system’s incorrect focus on topics of secondary importance. For example, in an article on a sightseeing trip, which should be RECREATION, the author frequently mentions the means of transportation. As a result, the article was wrongly categorized as TRAFFIC. 5.3 Result of Domain Estimation The accuracy of the domain estimation of un"
P08-2018,W03-1807,0,0.0318113,"Missing"
P09-2013,C04-1002,1,0.597478,"in linear-time with a single scan of a sentence. In this paper, we show a pseudo code of the algorithm and evaluate its performance empirically on the Kyoto University Corpus. Experimental results show that the proposed algorithm with the voted perceptron yields reasonably good accuracy. pen-wo pen-acc 3 4 age-ta. give-past. 4 - Figure 1: Sample sentence (bunsetsu-based) among bunsetsus. A bunsetsu is a base phrasal unit and consists of one or more content words followed by zero or more function words. In addition, most of algorithms of Japanese dependency parsing, e.g., (Sekine et al., 2000; Sassano, 2004), assume the three constraints below. (1) Each bunsetsu has only one head except the rightmost one. (2) Dependency links between bunsetsus go from left to right. (3) Dependency links do not cross one another. In other words, dependencies are projective. A sample sentence in Japanese is shown in Figure 1. We can see all the constraints are satisfied. 1 Introduction Single scan algorithms of parsing are important for interactive applications of NLP. For instance, such algorithms would be more suitable for robots accepting speech inputs or chatbots handling natural language inputs which should re"
P09-2013,W02-2016,0,0.737657,"mes into base phrases and decide dependency relations of the phrases in a strict left-toright manner. We show a pseudo code of the algorithm and evaluate its performance empirically with the voted perceptron on the Kyoto University Corpus (Kurohashi and Nagao, 1998). 3 Previous Work As far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan. Most of the modern dependency parsers for Japanese require bunsetsu chunking (base phrase chunking) before dependency parsing (Sekine et al., 2000; Kudo and Matsumoto, 2002; Sassano, 2004). Although wordbased parsers are proposed in (Mori et al., 2000; Mori, 2002), they do not build bunsetsus and are not compatible with other Japanese dependency parsers. Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. But they are basically word-based. 2 Japanese Sentence Structure In Japanese NLP, it is often assumed that the structure of a sentence is given by dependency relations 49 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49–52, c Suntec, Singapore, 4 August 2009. 2009 ACL a"
P09-2013,C00-1081,0,0.0357853,"toright manner. We show a pseudo code of the algorithm and evaluate its performance empirically with the voted perceptron on the Kyoto University Corpus (Kurohashi and Nagao, 1998). 3 Previous Work As far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan. Most of the modern dependency parsers for Japanese require bunsetsu chunking (base phrase chunking) before dependency parsing (Sekine et al., 2000; Kudo and Matsumoto, 2002; Sassano, 2004). Although wordbased parsers are proposed in (Mori et al., 2000; Mori, 2002), they do not build bunsetsus and are not compatible with other Japanese dependency parsers. Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. But they are basically word-based. 2 Japanese Sentence Structure In Japanese NLP, it is often assumed that the structure of a sentence is given by dependency relations 49 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49–52, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP Meg Meg ID 0 Head 1 Type B ga kare ni subj him to 1 2 3 7 3 7 D B D an"
P09-2013,C02-1157,0,0.0152005,"show a pseudo code of the algorithm and evaluate its performance empirically with the voted perceptron on the Kyoto University Corpus (Kurohashi and Nagao, 1998). 3 Previous Work As far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan. Most of the modern dependency parsers for Japanese require bunsetsu chunking (base phrase chunking) before dependency parsing (Sekine et al., 2000; Kudo and Matsumoto, 2002; Sassano, 2004). Although wordbased parsers are proposed in (Mori et al., 2000; Mori, 2002), they do not build bunsetsus and are not compatible with other Japanese dependency parsers. Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. But they are basically word-based. 2 Japanese Sentence Structure In Japanese NLP, it is often assumed that the structure of a sentence is given by dependency relations 49 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49–52, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP Meg Meg ID 0 Head 1 Type B ga kare ni subj him to 1 2 3 7 3 7 D B D ano that 4 6 D"
P09-2013,C00-2109,0,0.179817,"rsing simultaneously in linear-time with a single scan of a sentence. In this paper, we show a pseudo code of the algorithm and evaluate its performance empirically on the Kyoto University Corpus. Experimental results show that the proposed algorithm with the voted perceptron yields reasonably good accuracy. pen-wo pen-acc 3 4 age-ta. give-past. 4 - Figure 1: Sample sentence (bunsetsu-based) among bunsetsus. A bunsetsu is a base phrasal unit and consists of one or more content words followed by zero or more function words. In addition, most of algorithms of Japanese dependency parsing, e.g., (Sekine et al., 2000; Sassano, 2004), assume the three constraints below. (1) Each bunsetsu has only one head except the rightmost one. (2) Dependency links between bunsetsus go from left to right. (3) Dependency links do not cross one another. In other words, dependencies are projective. A sample sentence in Japanese is shown in Figure 1. We can see all the constraints are satisfied. 1 Introduction Single scan algorithms of parsing are important for interactive applications of NLP. For instance, such algorithms would be more suitable for robots accepting speech inputs or chatbots handling natural language inputs"
P09-2013,E99-1026,0,0.0235308,"Thus we cannot directly compare results with ours. To enable us to compare them we gave bunsetsu chunked sentences by our parser to the parser of (Sassano, 2004) instead of giving directly the correct chunked sentences 1. major POS, minor POS, conjugation type, conjugation form, surface form (lexicalized form) 2. Content word or function word 3. Punctuation (periods and commas) 4. Open parentheses and close parentheses 5. Location (at the beginning or end of the sentence) Gap features between two morphemes are also used since they have proven to be very useful and contribute to the accuracy (Uchimoto et al., 1999; Kudo and Matsumoto, 2002). They are represented as a binary feature and include distance (1, 2, 3, 4 – 10, or 11 ≤), particles, parentheses, and punctuation. In our proposed algorithm basically two morphemes are examined to estimate their dependency relation. Context information about the current morphemes to be estimated would be very useful and we can incorporate such information into our model. We assume that we have the j-th morpheme and the i-th one in Figure 3. We also use the j − n, ..., j − 1, j + 1, ..., j + n morphemes and the i − n, ..., i − 1, i + 1, ..., i + n ones, where n 51 W"
P09-2013,W06-2920,0,\N,Missing
P09-4001,I08-1025,1,0.821789,"of information credibility. (1) Credibility of information contents, (2) Credibility of the information sender, and (3) Credibility estimated from the document style and superficial characteristics. In order to help people judge the credibility of information from these viewpoints, we have been developing an information analysis system called WISDOM. Figure 1 shows the analysis result of WISDOM on the analysis topic “Is bio-ethanol good for the environment?” Figure 2 shows the system architecture of WISDOM. Given an analysis topic (query), WISDOM sends the query to the search engine TSUBAKI (Shinzato et al., 2008), and TSUBAKI returns a list of the top N relevant Web pages (N is usually set to 1000). Then, those pages are automatically analyzed, and major and contradictory expressions and evaluative expressions are extracted. Furthermore, the information senders of the Web pages, which were analyzed beforehand, are collected and the distribution is calculated. The WISDOM analysis results can be viewed from several viewpoints by changing the tabs using a Web browser. The leftmost tab, “Summary,” shows the summary of the analysis, with major phrases and major/contradictory statements first. http://clusty"
P10-1037,W00-1303,0,0.143207,"sing is a bunsetsu, the concept of which was initially introduced by Hashimoto (1934). We assume that in Japanese we have a sequence of bunsetsus before parsing a sentence. A bunsetsu contains one or more content words and zero or more function words. A sample sentence in Japanese is shown in Figure 2. This sentence consists of five bunsetsus: 1 Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2 Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary classifiers can work with Sassano’s for Japanese. classification function is G(x) = sign{f (x)}. In pool-based active learning with large margin classifiers, selection of examples can be done as follows: 1. Compute f (xi ) over all unlabeled examples xi in th"
P10-1037,W02-2016,0,0.707836,"to convert a treebank to suitable labeled instances by using the algorithm in Figure 4. Note A basic syntactic unit used in Japanese parsing is a bunsetsu, the concept of which was initially introduced by Hashimoto (1934). We assume that in Japanese we have a sequence of bunsetsus before parsing a sentence. A bunsetsu contains one or more content words and zero or more function words. A sample sentence in Japanese is shown in Figure 2. This sentence consists of five bunsetsus: 1 Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2 Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary classifiers can work with Sassano’s for Japanese. classification function is G(x) = sign{f (x)}. In pool-based active learn"
P10-1037,C08-1059,0,0.0957453,"Missing"
P10-1037,P05-1012,0,0.118043,"ed instances by using the algorithm in Figure 4. Note A basic syntactic unit used in Japanese parsing is a bunsetsu, the concept of which was initially introduced by Hashimoto (1934). We assume that in Japanese we have a sequence of bunsetsus before parsing a sentence. A bunsetsu contains one or more content words and zero or more function words. A sample sentence in Japanese is shown in Figure 2. This sentence consists of five bunsetsus: 1 Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2 Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary classifiers can work with Sassano’s for Japanese. classification function is G(x) = sign{f (x)}. In pool-based active learning with large margin classifiers, s"
P10-1037,W03-3017,0,0.361766,"e in Japanese is shown in Figure 2. This sentence consists of five bunsetsus: 1 Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2 Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary classifiers can work with Sassano’s for Japanese. classification function is G(x) = sign{f (x)}. In pool-based active learning with large margin classifiers, selection of examples can be done as follows: 1. Compute f (xi ) over all unlabeled examples xi in the pool. 2. Sort xi with |f (xi ) |in ascending order. 3. Select top m examples. This type of selection methods with SVMs is discussed in (Tong and Koller, 2000; Schohn and Cohn, 2000). They obtain excellent results on text classification. These selection"
P10-1037,P06-2082,0,0.0144062,"dependency parsing. It is observed that active learning of parsing with the averaged perceptron, which is one of the large margin classifiers, works also well for Japanese dependency analysis. 7 Following (Freund and Schapire, 1999), we use the term “support vectors” for AP as well as SVM. “Support vectors” of AP means vectors which are selected in the training phase and contribute to the prediction. 8 Thus it is very important to construct models for estimating the actual annotation cost as Haertel et al. (2008) do. 9 Hwa (2004) discusses similar aspects of researches on active learning. 10 Ohtake (2006) examines heuristic methods of selecting sentences. 363 In addition, as far as we know, we are the first to propose the active learning methods of using partial dependency relations in a given sentence for parsing and we have evaluated the effectiveness of our methods. Furthermore, we have tried to obtain more labeled examples from precious labeled ones that annotators give by utilizing syntactic constraints of the Japanese language. It is noteworthy that linguistic constraints have been shown useful for reducing annotations in active learning for NLP. Experimental results show that our propos"
P10-1037,W07-1516,0,0.0628488,"e learning. 1 Introduction Reducing annotation cost is very important because supervised learning approaches, which have been successful in natural language processing, require typically a large number of labeled examples. Preparing many labeled examples is time consuming and labor intensive. One of most promising approaches to this issue is active learning. Recently much attention has been paid to it in the field of natural language processing. Various tasks have been targeted in the research on active learning. They include word sense disambiguation, e.g., (Zhu and Hovy, 2007), POS tagging (Ringger et al., 2007), named entity recognition (Laws and Sch¨utze, 2008), word segmentation, e.g., (Sassano, 2002), and parsing, e.g., (Tang et al., 2002; Hwa, 2004). It is the main purpose of this study to propose methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. Typically in active learning for parsing a 2 Active Learning 2.1 Pool-based Active Learning Our base framework of active learning is based on the algorithm of (Lewis and Gale, 1994), which is called pool-based active learning. Following their s"
P10-1037,W04-3202,0,0.0237858,". If we sample smaller units rather than sentences, we have partially annotated sentences and have to use a parsing algorithm that can be trained from incompletely annotated sentences. Therefore, it is difficult to use some of probabilistic models for parsing. 4 Figure 3: Algorithm of Japanese dependency parsing that the algorithm in Figure 4 does not generate every pair of bunsetsus.3 4 Active Learning for Parsing Most of the methods of active learning for parsing in previous work use selection of sentences that seem to contribute to the improvement of accuracy (Tang et al., 2002; Hwa, 2004; Baldridge and Osborne, 2004). Although Hwa suggests that sample selection for parsing would be improved by selecting finer grained constituents rather than sentences (Hwa, 2004), such methods have not been investigated so far. Typical methods of selecting sentences are 5 Active Learning for Japanese Dependency Parsing In this section we describe sample selection methods which we investigated. 5.1 Sentence-wise Sample Selection Passive Selection (Passive) This method is to select sequentially sentences that appear in the training corpus. Since it gets harder for the readers to reproduce the same experimental setting, we 3"
P10-1037,P02-1064,1,0.850573,"roaches, which have been successful in natural language processing, require typically a large number of labeled examples. Preparing many labeled examples is time consuming and labor intensive. One of most promising approaches to this issue is active learning. Recently much attention has been paid to it in the field of natural language processing. Various tasks have been targeted in the research on active learning. They include word sense disambiguation, e.g., (Zhu and Hovy, 2007), POS tagging (Ringger et al., 2007), named entity recognition (Laws and Sch¨utze, 2008), word segmentation, e.g., (Sassano, 2002), and parsing, e.g., (Tang et al., 2002; Hwa, 2004). It is the main purpose of this study to propose methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. Typically in active learning for parsing a 2 Active Learning 2.1 Pool-based Active Learning Our base framework of active learning is based on the algorithm of (Lewis and Gale, 1994), which is called pool-based active learning. Following their sequential sampling algorithm, we show in Figure 1 the basic flow of pool-based active learning"
P10-1037,P08-2017,0,0.0190572,"e building an annotated cor7 Conclusion We have investigated that active learning methods for Japanese dependency parsing. It is observed that active learning of parsing with the averaged perceptron, which is one of the large margin classifiers, works also well for Japanese dependency analysis. 7 Following (Freund and Schapire, 1999), we use the term “support vectors” for AP as well as SVM. “Support vectors” of AP means vectors which are selected in the training phase and contribute to the prediction. 8 Thus it is very important to construct models for estimating the actual annotation cost as Haertel et al. (2008) do. 9 Hwa (2004) discusses similar aspects of researches on active learning. 10 Ohtake (2006) examines heuristic methods of selecting sentences. 363 In addition, as far as we know, we are the first to propose the active learning methods of using partial dependency relations in a given sentence for parsing and we have evaluated the effectiveness of our methods. Furthermore, we have tried to obtain more labeled examples from precious labeled ones that annotators give by utilizing syntactic constraints of the Japanese language. It is noteworthy that linguistic constraints have been shown useful"
P10-1037,C04-1002,1,0.919691,"ight. • Dependencies do not cross one another. Figure 1: Flow of the pool-based active learning Lisa-ga Lisa-subj ID 0 Head 4 kare-ni to him 1 4 ano pen-wo that pen-acc 2 3 3 4 We can see that these constraints are satisfied in the sample sentence in Figure 2. In this paper we also assume that the above constraints hold true when we discuss algorithms of Japanese parsing and active learning for it. age-ta. give-past. 4 - Figure 2: Sample sentence. An English translation is “Lisa gave that pen to him.” 3.3 Algorithm of Japanese Dependency Parsing 3.1 Syntactic Units We use Sassano’s algorithm (Sassano, 2004) for Japanese dependency parsing. The reason for this is that it is very accurate and efficient1 . Furthermore, it is easy to implement. His algorithm is one of the simplest form of shift-reduce parsers and runs in linear-time.2 Since Japanese is a head final language and its dependencies are projective as described in Section 3.2, that simplification can be made. The basic flow of Sassano’s algorithm is shown in Figure 3, which is slightly simplified from the original by Sassano (2004). When we use this algorithm with a machine learning-based classifier, function Dep() in Figure 3 uses the cl"
P10-1037,J04-3001,0,0.815136,"processing, require typically a large number of labeled examples. Preparing many labeled examples is time consuming and labor intensive. One of most promising approaches to this issue is active learning. Recently much attention has been paid to it in the field of natural language processing. Various tasks have been targeted in the research on active learning. They include word sense disambiguation, e.g., (Zhu and Hovy, 2007), POS tagging (Ringger et al., 2007), named entity recognition (Laws and Sch¨utze, 2008), word segmentation, e.g., (Sassano, 2002), and parsing, e.g., (Tang et al., 2002; Hwa, 2004). It is the main purpose of this study to propose methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. Typically in active learning for parsing a 2 Active Learning 2.1 Pool-based Active Learning Our base framework of active learning is based on the algorithm of (Lewis and Gale, 1994), which is called pool-based active learning. Following their sequential sampling algorithm, we show in Figure 1 the basic flow of pool-based active learning. Various methods for selecting informative example"
P10-1037,C08-1046,0,0.172139,"relation. In order to prepare training examples for the trainable classifier used with his algorithm, we first have to convert a treebank to suitable labeled instances by using the algorithm in Figure 4. Note A basic syntactic unit used in Japanese parsing is a bunsetsu, the concept of which was initially introduced by Hashimoto (1934). We assume that in Japanese we have a sequence of bunsetsus before parsing a sentence. A bunsetsu contains one or more content words and zero or more function words. A sample sentence in Japanese is shown in Figure 2. This sentence consists of five bunsetsus: 1 Iwatate et al. (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al., 2005). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). After considering these results, we have concluded so far that Sassano’s is a reasonable choice for our purpose. 2 Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). Classifiers with Nivre’s are required to handle multiclass prediction, while binary cla"
P10-1037,P02-1016,0,0.636014,"Missing"
P10-1037,E99-1026,0,0.0497161,"a parser. The dependency accuracy is the percentage of correct dependencies. This measure is commonly used for the Kyoto University Corpus. In our experiments we used the Kyoto University Corpus Version 2 (Kurohashi and Nagao, 1998). Initial seed sentences and a pool of unlabeled sentences for training are taken from the articles on January 1st through 8th (7,958 sentences) and the test data is a set of sentences in the articles on January 9th (1,246 sentences). The articles on January 10th were used for development. The split of these articles for training/test/development is the same as in (Uchimoto et al., 1999). 6.7 Results and Discussion Learning Curves First we compare methods for sentence wise selection. Figure 5 shows that M IN is the best among them, while AVG is not good and similar to PASSIVE. It is observed that active learning with large margin classifiers also works well for Sassano’s algorithm of Japanese dependency parsing. Next we compare chunk-wise selection with sentence-wise one. The comparison is shown in Figure 6. Note that we must carefully consider how to count labeled examples. In sentence wise selection we obviously count the number of sentences. However, it is impossible to co"
P10-1037,W03-3023,0,0.142385,"Missing"
P10-1037,D07-1082,0,0.0448324,"led examples as compared to passive learning. 1 Introduction Reducing annotation cost is very important because supervised learning approaches, which have been successful in natural language processing, require typically a large number of labeled examples. Preparing many labeled examples is time consuming and labor intensive. One of most promising approaches to this issue is active learning. Recently much attention has been paid to it in the field of natural language processing. Various tasks have been targeted in the research on active learning. They include word sense disambiguation, e.g., (Zhu and Hovy, 2007), POS tagging (Ringger et al., 2007), named entity recognition (Laws and Sch¨utze, 2008), word segmentation, e.g., (Sassano, 2002), and parsing, e.g., (Tang et al., 2002; Hwa, 2004). It is the main purpose of this study to propose methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. Typically in active learning for parsing a 2 Active Learning 2.1 Pool-based Active Learning Our base framework of active learning is based on the algorithm of (Lewis and Gale, 1994), which is called pool-base"
P11-1109,P05-1074,0,0.327239,"Missing"
P11-1109,N03-1003,0,0.822953,"he same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same 1087 concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define the same concept “osteoporosis”, include two pai"
P11-1109,P01-1008,0,0.454546,"ntroduction Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same 1087 concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define"
P11-1109,D07-1017,0,0.0267206,"cription of related work on the same research issue. Section 2 describes related works. Section 3 presents our proposed method. Section 4 reports on evaluation results. Section 5 concludes the paper. 2 Related Work The existing work for paraphrase extraction is categorized into two groups. The first involves a distributional similarity approach pioneered by Lin and Pantel (2001). Basically, this approach assumes that two expressions that have a large distributional similarity are paraphrases. There are also variants of this approach that address entailment acquisition (Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009). These methods can be applied to a normal monolingual corpus, and it has been shown that a large number of paraphrases or entailment rules could be extracted. How1088 ever, the precision of these methods has been relatively low. This is due to the fact that the evidence, i.e., distributional similarity, is just indirect evidence of paraphrase/entailment. Accordingly, these methods occasionally mistake antonymous pairs for paraphrases/entailment pairs, since an expression and its antonymous counterpart are also likely to have a large distribut"
P11-1109,N06-1003,0,0.101957,"Missing"
P11-1109,C04-1051,0,0.665037,". Our objective is to extract phrasal paraphrases from pairs of sentences that define the same concept. We propose a supervised method that exploits various kinds of lexical similarity features and contextual features. Sentences defining certain concepts are acquired automatically on a large scale from the Web by applying a quite simple supervised method. Previous methods most relevant to our work used parallel corpora such as multiple translations of the same source text (Barzilay and McKeown, 2001) or automatically acquired parallel news texts (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). The former requires a large amount of manual labor to translate the same texts Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics in several ways. The latter would suffer from the fact that it is not easy to automatically retrieve large bodies of parallel news text with high accuracy. On the contrary, recognizing definition sentences for the same concept is quite an easy task at least for Japanese, as we will show, and we were able to find a huge amount"
P11-1109,P05-1014,0,0.0131216,"same cuisine, or the description of related work on the same research issue. Section 2 describes related works. Section 3 presents our proposed method. Section 4 reports on evaluation results. Section 5 concludes the paper. 2 Related Work The existing work for paraphrase extraction is categorized into two groups. The first involves a distributional similarity approach pioneered by Lin and Pantel (2001). Basically, this approach assumes that two expressions that have a large distributional similarity are paraphrases. There are also variants of this approach that address entailment acquisition (Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009). These methods can be applied to a normal monolingual corpus, and it has been shown that a large number of paraphrases or entailment rules could be extracted. How1088 ever, the precision of these methods has been relatively low. This is due to the fact that the evidence, i.e., distributional similarity, is just indirect evidence of paraphrase/entailment. Accordingly, these methods occasionally mistake antonymous pairs for paraphrases/entailment pairs, since an expression and its antonymous counterpart are also likely to h"
P11-1109,D09-1122,1,0.885272,"Missing"
P11-1109,N06-1058,0,0.0634,"Missing"
P11-1109,D07-1073,1,0.816716,"We describe them below. 3.1 Definition sentence acquisition We acquire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia. 92.2, and 91.4, respectively. Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences. After adding definition (2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 sentences from Wikipedia articles, which are typi(Osteoporosis is a disease that makes bones fragile.) cally the first sentence of the body of each article (Kazama and Torisawa, 2007), we obtained a total Fujii and Ishikawa (2002) developed an unsuper- of 2,141,878 definition sentence candidates, which vised method to find definition sentences from the covered 867,321 concepts ranging from weapons to Web using 18 sentential templates and a language rules of baseball. Then, we coupled two definition model constructed from an encyclopedia. On the sentences whose defined concepts were the same other hand, we developed a supervised method to and obtained 29,661,812 definition sentence pairs. achieve a higher precision. Obviously, our method is tailored to Japanese. For We use"
P11-1109,P08-1047,1,0.825091,"e defined similarly. 1090 Figure 1: Illustration of features f8-12. didate phrase of s2 but do appear in the other part of s2 , i.e. they are extra morphemes for s1 ’s candidate phrase. On the other hand, f9 is zero since there is no such extra morpheme in s2 ’s candidate phrase. Also, features f10-12 have positive values since the two candidate phrases share two parent dependency tree fragments, (that increases) and (of fracture). We have also tried the following features, which we do not detail due to space limitation: the similarity of candidate phrases based on semantically similar nouns (Kazama and Torisawa, 2008), entailing/entailed verbs (Hashimoto et al., 2009), and the identity of the pronunciation and base form of the head morpheme; N -grams (N =1,2,3) of child and parent contexts represented by either the inflected form, base form, pronunciation, or POS of morOriginal definition sentence pair (s1 , s2 ) s1 : Osteoporosis is a disease that reduces bone mass and makes bones fragile. s2 : Osteoporosis is a disease that decreases the quantity of bone and increases the risk of bone fracture. Paraphrased definition sentence pair (s01 , s02 ) s01 : Osteoporosis is a disease that decreases the quantity o"
P11-1109,D08-1084,0,0.0310218,"Missing"
P11-1109,J10-3003,0,0.0405564,"unt of parallel corpora, as noted before. We avoid this by using definition sentences, which can be easily acquired on a large scale from the Web, as parallel corpora. Murata et al. (2004) used definition sentences in two manually compiled dictionaries, which are considerably fewer in the number of definition sentences than those on the Web. Thus, the coverage of their method should be quite limited. Furthermore, the precision of their method is much poorer than ours as we report in Section 4. For a more extensive survey on paraphrasing methods, see Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). 3 Proposed method Our method, targeting the Japanese language, consists of two steps: definition sentence acquisition and paraphrase extraction. We describe them below. 3.1 Definition sentence acquisition We acquire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia. 92.2, and 91.4, respectively. Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences. After adding definition (2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 sentences f"
P11-1109,P10-1134,0,0.0253121,"ences from the covered 867,321 concepts ranging from weapons to Web using 18 sentential templates and a language rules of baseball. Then, we coupled two definition model constructed from an encyclopedia. On the sentences whose defined concepts were the same other hand, we developed a supervised method to and obtained 29,661,812 definition sentence pairs. achieve a higher precision. Obviously, our method is tailored to Japanese. For We use one sentential template and an SVM clas- a language-independent method of definition acquisifier. Specifically, we first collect definition sen- sition, see Navigli and Velardi (2010) as an example. tence candidates by a template “ˆNP とは.*”, where ˆ is the beginning of sentence and NP is the noun 3.2 Paraphrase extraction phrase expressing the concept to be defined followed Paraphrase extraction proceeds as follows. First, by a particle sequence, “と” (comitative) and “は” each sentence in a pair is parsed by the depen(topic) (and optionally followed by comma), as ex- dency parser KNP2 and dependency tree fragemplified in (2). As a result, we collected 3,027,101 ments that constitute linguistically well-formed consentences. Although the particle sequence tends stituents are"
P11-1109,W04-3219,0,0.024729,"Missing"
P11-1109,P02-1006,0,0.0207055,"uire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia. 92.2, and 91.4, respectively. Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences. After adding definition (2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 sentences from Wikipedia articles, which are typi(Osteoporosis is a disease that makes bones fragile.) cally the first sentence of the body of each article (Kazama and Torisawa, 2007), we obtained a total Fujii and Ishikawa (2002) developed an unsuper- of 2,141,878 definition sentence candidates, which vised method to find definition sentences from the covered 867,321 concepts ranging from weapons to Web using 18 sentential templates and a language rules of baseball. Then, we coupled two definition model constructed from an encyclopedia. On the sentences whose defined concepts were the same other hand, we developed a supervised method to and obtained 29,661,812 definition sentence pairs. achieve a higher precision. Obviously, our method is tailored to Japanese. For We use one sentential template and an SVM clas- a lang"
P11-1109,I05-5011,0,0.0171765,"Missing"
P11-1109,P07-1058,0,0.0128043,"the Web are a treasure trove of paraphrase knowledge (Section 4.2). II. Our method of paraphrase acquisition from definition sentences is more accurate than wellknown competing methods (Section 4.1). We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al., 2007) (SMT method), and that of Murata et al. (2004) (Mrt method). The first two methods are well known for accurately extracting semantically equivalent phrase pairs from parallel corpora.8 Then, we verify claim This scheme is similar to the one proposed by Szpektor et al. (2007). We adopt this scheme since paraphrase judgment might be unstable between annotators unless they are given a particular context based on which they make a judgment. As de6 The remaining 36 pairs were discarded as they contained scribed below, we use definition sentences as congarbled characters of Japanese. texts. We admit that annotators might be biased by 7 http://www.statmt.org/moses/ 8 this in some unexpected way, but we believe that As anonymous reviewers pointed out, they are unsuperthis is a more stable method than that without con- vised methods and thus unable to be adapted to defini"
P11-1109,C08-1107,0,\N,Missing
P11-1109,P07-2045,0,\N,Missing
P13-1016,J96-1002,0,0.0215273,"i, and sj , which is the word specified at j, or both the context of i and the context of j simultaneously. Distance is considered using the distance class d. Distortion is represented by distance and orientation. The pair model considers distortion using six joint classes of d and o. 3.2 Pair Model The pair model utilizes the word at the CP, the word at an NPC, and the context of the CP and the NPC simultaneously to estimate the NP. This can be done by our distortion model definition and the learning strategy described in the previous section. In this work, we use the maximum entropy method (Berger et al., 1996) as a discriminative machine learning method. The reason for this is that a model based on the maximum entropy method can calculate probabilities. However, if we use scores as an approximation of the distortion probabilities, various discriminative machine learning methods can be applied to build the distortion model. Let s be a source word and sn1 = s1 s2 ...sn be a source sentence. We add a beginning of sentence (BOS) marker to the head of the source sentence and an end of sentence (EOS) marker to the end, so the source sentence S is expressed as sn+1 0 (s0 = BOS, sn+1 = EOS). Our distortion"
P13-1016,J07-2003,0,0.673825,"for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, 1 A language model also supports the estimation. In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. 2 155 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 155–165, c"
P13-1016,C10-2033,0,0.528996,"Missing"
P13-1016,D08-1089,0,0.0586676,"he NPC. 3 Proposed Method In this section, we first define our distortion model and explain our learning strategy. Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their mod"
P13-1016,P06-1077,0,0.0741182,"nces appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores considering the distances for decoders that do not generate hypotheses from left to right. If it could be applied to these methods, our distortion model might contribute to tree-based SMT methods. Investigating the effects will be for future work. 6 Conclusion This paper described our distortion models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model, but it can consider"
P13-1016,N04-1035,0,0.0561875,"n the effect of distances appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores considering the distances for decoders that do not generate hypotheses from left to right. If it could be applied to these methods, our distortion model might contribute to tree-based SMT methods. Investigating the effects will be for future work. 6 Conclusion This paper described our distortion models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model,"
P13-1016,P09-2061,0,0.018513,"n if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their model does not simultaneously consider both the word specified at the CP and the word specified at the NPCs. Green et al. (2010) proposed distortion models that used context. Their model (the outbound model) estimates how far the NP should be from the CP using the word at the CP and its context.5 Their model does not simultaneously con5 3.1 Distortion Model and Learning Strategy First, we define our distortion model. Let i be a CP, j be an NPC, S be a source sentence, and X be the random"
P13-1016,P03-1021,0,0.0383462,"ors, we removed articles {a, an, the} in English and particles {ga, wo, wa} in Japanese before performing word alignments because these function words do not correspond to any words in the other languages. After word alignment, we restored the removed words and shifted the word alignment positions to the original word positions. We used 5gram language models that were trained using the English side of each set of bilingual training data. We used an in-house standard phrase-based SMT system compatible with the Moses decoder (Koehn et al., 2007). The SMT weighting parameters were tuned by MERT (Och, 2003) using the development data. To stabilize the MERT results, we tuned three times by MERT using the first half of the development data and we selected the SMT weighting parameter set that performed the best on the second half of the development data based on the BLEU scores from the three SMT weighting parameter sets. We compared systems that used a common SMT feature set from standard SMT features and different distortion model features. The common SMT feature set consists of: four translation model features, phrase penalty, word penalty, and a language model feature. The compared different di"
P13-1016,N10-1129,0,0.272219,"at the CP but also the word at a NP candidate (NPC) should be considered simultaneously. In (c) and (d) in Figure 2, the word (kare) at the CP is the same and karita (borrowed) and katta (bought) are at the NPCs. Karita is the word at the NP and katta is not the word at the NP for (c), while katta is the word at the NP and karita is not the word at the NP for (d). From these examples, considering what the word is at the NP 3 NP is not always one position, because there may be multiple correct hypotheses. 4 This definition is slightly different from that of existing methods such as Moses and (Green et al., 2010). In existing methods, CP is the rightmost position of the last translated source phrase and NP is the leftmost position of the source phrase to be translated next. Note that existing methods do not consider word-level correspondences. 156 sider both the word specified at the CP and the word specified at an NPC. For example, the outbound model considers the word specified at the CP, but does not consider the word specified at an NPC. Their models also do not consider relative word order. In contrast, our distortion model solves the aforementioned problems. Our distortion models utilize the wor"
P13-1016,P02-1040,0,0.0864299,"010). The relative source sentence position is discretized into five bins, one for each quintile of the sentence. For the inbound model13 , i of the feature templates was changed to j. Features occurring four or more times in the training sentences were used. The maximum entropy method with Gaussian prior smoothing was used to estimate the model parameters. The MSD bidirectional lexical distortion model was built using all of the data used to build the translation model. 4.4 Results and Discussion We evaluated translation quality based on the caseinsensitive automatic evaluation score BLEU-4 (Papineni et al., 2002). We used distortion limits of 10, 20, 30, and unlimited (∞), which limited the number of words for word reordering to a maximum number. Table 3 presents our main results. The proposed SEQUENCE outperformed the baselines for both Japanese to English and Chinese to English translation. This demonstrates the effectiveness of the proposed SEQUENCE. The scores of the proposed SEQUENCE were higher than those 4.2 Training for the Proposed Models Our distortion model was trained as follows: We used 0.2 million sentence pairs and their word alignments from the data used to build the translation model"
P13-1016,N04-4026,0,0.287066,"Missing"
P13-1016,2005.iwslt-1.8,0,0.323302,"between the CP and the NPC. 3 Proposed Method In this section, we first define our distortion model and explain our learning strategy. Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP"
P13-1016,J97-3002,0,0.0396293,"istance class feature used in the model was the same (e.g., distortions from 5 to 20 were the same distance class feature), PAIR produced average distortion probabilities that were almost the same. In contrast, the average distortion probabilities for SEQUENCE decreased when the lengths of the distortions increased, even if the distance class feature was the same, and this behavior was the same as that of CORPUS. This confirms that the proposed SEQUENCE could learn the effect of distances appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering dista"
P13-1016,P07-2045,0,0.0157849,"ummary, in order to estimate the NP, the following should be considered simultaneously: the word at the NP, the word at the CP, the relative word order among the NPCs, the words surrounding NP and CP (context), and the words between the CP and the NPC. 3 Proposed Method In this section, we first define our distortion model and explain our learning strategy. Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP"
P13-1016,C04-1073,0,0.12617,"9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, 1 A language model also supports the estimation. In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. 2 155 Proceedings of the 51st Annual Meeting of the Association fo"
P13-1016,P12-1095,0,0.0139432,"models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model, but it can consider rich context. Experiments indicate that our models achieved better performance and the sequence model could learn the effect of distances appropriately. Since our models do not require a parser, they can be applied to many languages. Future work includes application to other language pairs, incorporation into ITG constraint methods and other reordering methods, and application to tree-based SMT methods. 5 Related Works We discuss related works other than discussed in Section 2. Xiong et al. (2012) proposed a model predicting the orientation of an argument with respect to its verb using a parser. Syntactic structures and predicate-argument structures are useful for reordering. However, orientations do not handle distances. Thus, our distortion model does not compete against the methods predicting orientations using a parser and would assist them if used References 16 Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association"
P13-1016,W04-3250,0,0.119941,"Missing"
P13-1016,W08-0401,1,0.819648,"e (e.g., distortions from 5 to 20 were the same distance class feature), PAIR produced average distortion probabilities that were almost the same. In contrast, the average distortion probabilities for SEQUENCE decreased when the lengths of the distortions increased, even if the distance class feature was the same, and this behavior was the same as that of CORPUS. This confirms that the proposed SEQUENCE could learn the effect of distances appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to u"
P13-1016,C04-1030,1,0.952204,"experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, 1 A language model also supports the estimation. In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. 2 155 Proceedings of the 51st"
P13-1016,P06-1067,0,\N,Missing
P13-1016,P01-1067,0,\N,Missing
P14-2042,W02-1001,0,0.14194,"undaries. The lower part of the lattice (character-level nodes) represents unknown words, where each node carries a position-of-character tag, in addition to other types of information that can also be found on a wordlevel node. A sequence of character-level nodes are considered as an unknown word if and only if the sequence of POC tags forms one of the cases listed in Table 3. This table also illustrates the permitted transitions between adjacent characterlevel nodes. We use the standard dynamic programming technique to search for the best path in the lattice. We use the averaged perceptron (Collins, 2002), an efficient online learning algorithm, to train the model. 3.2 Features We show the feature templates of our model in Table 4. The features consist of two categories: baseline features, which are modified from the templates proposed in (Kruengkrai et al., 2009); and proposed features, which encode characterlevel POS information. Baseline features: For word-level nodes that represent known words, we use the symbols , and to denote the word form, POS tag and length of the word, respectively. The functions and return the first and last character of . If has only one character, we omit the temp"
P14-2042,P08-1102,0,0.353209,"s, the function returns its character-level POS. The subscript indices 0 and -1 as well as 256 (a) Word Segmentation Results System P R Baseline 97.48 98.44 CharPOS 97.55 98.51 other symbols stand for the same meaning as they are in the baseline features. 4 4.1 Evaluation Settings (b) Joint Segmentation and POS Tagging Results System P R F Baseline 93.01 93.95 93.48 CharPOS 93.42 94.18 93.80 To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 time"
P14-2042,C08-1049,0,0.0588611,"s, the function returns its character-level POS. The subscript indices 0 and -1 as well as 256 (a) Word Segmentation Results System P R Baseline 97.48 98.44 CharPOS 97.55 98.51 other symbols stand for the same meaning as they are in the baseline features. 4 4.1 Evaluation Settings (b) Joint Segmentation and POS Tagging Results System P R F Baseline 93.01 93.95 93.48 CharPOS 93.42 94.18 93.80 To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 time"
P14-2042,P09-1058,0,0.850882,"haracter-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech v"
P14-2042,Y06-1012,0,0.108255,"hod that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Charact"
P14-2042,P13-1013,0,0.0488773,"e focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun noun + verb verb + adjective adjective + verb verb + verb Examples of Verb 投资 (invest : throw + wealth) 心疼 (feel sorry : heart + hurt) 认清 (realize : recognize + clear) 痛恨 (hate : pain + hate) 审查 (inspect : examine + review) Table 1. Character-level POS sequence as a more specif"
P14-2042,D10-1082,0,0.22597,"ces 0 and -1 as well as 256 (a) Word Segmentation Results System P R Baseline 97.48 98.44 CharPOS 97.55 98.51 other symbols stand for the same meaning as they are in the baseline features. 4 4.1 Evaluation Settings (b) Joint Segmentation and POS Tagging Results System P R F Baseline 93.01 93.95 93.48 CharPOS 93.42 94.18 93.80 To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 times. We applied a similar strategy in building the lexicon for character"
P14-2042,P11-1141,0,0.0183567,"roduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun noun + verb verb + adjective adjective + verb verb + verb Examples of Verb 投资 (invest : throw + wealth) 心疼 (feel sorry : heart + hurt) 认清 (realize : recognize + clear) 痛恨 (hate : pain + hate) 审查 (inspect : examine + review) Table 1. Chara"
P14-2042,D12-1132,0,0.0193696,"In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun noun + verb verb + adjective adjective + verb verb + verb Examples of Verb 投资 (invest : throw + wealth) 心疼 (feel sorry : heart + hurt) 认清 (realize : recognize + clear) 痛恨 (hate : pain + hate) 审查 (inspect : examine + review) Table 1. Character-level POS sequ"
P14-2042,W04-3236,0,0.260357,"evel POS tagging. We propose a method that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) point"
P14-2042,C04-1067,0,0.0947917,"We propose a method that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some"
P14-2042,P07-2055,0,0.0411452,"Missing"
P14-2042,C10-2139,0,0.0156538,"ith word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun noun + verb"
P14-2042,P11-1139,0,0.375647,"256 (a) Word Segmentation Results System P R Baseline 97.48 98.44 CharPOS 97.55 98.51 other symbols stand for the same meaning as they are in the baseline features. 4 4.1 Evaluation Settings (b) Joint Segmentation and POS Tagging Results System P R F Baseline 93.01 93.95 93.48 CharPOS 93.42 94.18 93.80 To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 times. We applied a similar strategy in building the lexicon for character-level POS,"
P14-2042,O03-4002,0,0.0598797,"g jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Part-of-Speech verb + noun"
P14-5014,C12-1120,1,0.90165,"Missing"
P14-5014,Y12-1033,1,0.51826,"Missing"
P14-5014,P05-1032,0,0.0621194,"Missing"
P14-5014,J07-2003,0,0.0747281,"a number of features and create a linear model scoring each possible combination of hypotheses (see Section 5). We then attempt to ﬁnd the combination that maximizes this model score. The combination of rules is constrained by the structure of the input dependency tree. If we only consider local features1 , then a simple bottom-up dynamic programming approach can eﬃciently ﬁnd the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁ"
P14-5014,P05-1022,0,0.0606666,"Missing"
P14-5014,D12-1107,0,0.120955,"only consider local features1 , then a simple bottom-up dynamic programming approach can eﬃciently ﬁnd the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 4). This lattice representation also allows the decoder to make choices between various morphological variations of a • Ex"
P14-5014,D11-1047,1,0.910385,"eval and translation hypothesis construction An important characteristic of our system is that we do not extract and store translation rules in advance: the alignment of translation examples is performed oﬄine. However, for a given input sentence i, the steps for ﬁnding examples partially matching i and extracting their translation hypotheses is an online process. This approach could be considered to be more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and sco"
P14-5014,W08-0402,0,0.0545771,"f rules is constrained by the structure of the input dependency tree. If we only consider local features1 , then a simple bottom-up dynamic programming approach can eﬃciently ﬁnd the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 3, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 4). This lattice representation also allows the d"
P14-5014,D07-1104,0,0.0190578,"ded translation. 3 Example retrieval and translation hypothesis construction An important characteristic of our system is that we do not extract and store translation rules in advance: the alignment of translation examples is performed oﬄine. However, for a given input sentence i, the steps for ﬁnding examples partially matching i and extracting their translation hypotheses is an online process. This approach could be considered to be more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full"
P14-5014,D11-1125,0,0.0337923,"m examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 4). This lattice representation also allows the decoder to make choices between various morphological variations of a • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the Pairwise Ranking Optimization (PRO) algorithm (Hopkins and May, 2011) and parameter optimization with MegaM4 . We use the implementation of PRO that is provided with the Moses SMT system and the default settings of MegaM. 6 Experiments In order to evaluate our system, we conducted translation experiments on four language pairs: Japanese-English (JA–EN), EnglishJapanese (EN–JA), Japanese-Chinese (JA– ZH) and Chinese-Japanese (ZH–JA). For Japanese-English, we evaluated on the NTCIR-10 PatentMT task data (patents) (Goto et al., 2013) and compared our system with the oﬃcial baseline scores. For JapaneseChinese, we used parallel scientiﬁc paper excerpts from the ASP"
P14-5014,P05-1034,0,0.286199,"Missing"
P14-5014,N06-1023,1,0.797643,"Missing"
P14-5014,W11-2123,0,\N,Missing
P14-5014,2011.iwslt-evaluation.24,0,\N,Missing
P16-1117,D15-1041,0,0.0501244,"Missing"
P16-1117,D14-1082,0,0.0543916,"o anaphora. For identifying inter-sentential zero anaphora, an antecedent has to be searched in a broad search space, and the salience of discourse entities has to be captured. Therefore, the task of identifying inter-sentential zero anaphora is more difficult than that of intra-sentential zero anaphora. Thus, Ouchi et al. (2015) and Iida et al. (2015) focused on only intra-sentential zero anaphora. Following this trend, this paper focuses on intra-sentential zero anaphora. Recently, NN-based approaches have achieved improvement for several NLP tasks. For example, in transition-based parsing, Chen and Manning (2014) proposed an NN-based approach, where the words, POS tags, and dependency labels are first represented by embeddings individually. Then, an NN-based classifier is built to make parsing decisions, where an input layer is a concatenation of embeddings of words, POS tags, and dependency labels. This model has been extended by several studies (Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015). In semantic role labeling, Zhou and Xu (2015) propose an end-to-end approach using recurrent NN, where an original text is the input, and semantic role labeling is performed without any interm"
P16-1117,W02-1001,0,0.459465,"Missing"
P16-1117,P15-1033,0,0.0463766,"Missing"
P16-1117,Y12-1058,1,0.722115,"|1 denotes the Hamming distance between the gold PA graph yˆk and a candidate PA graph yk . Stochastic gradient descent is used for parameter inference. Derivatives with respect to parameters are taken using backpropagation. Adam (Kingma and Ba, 2014) is adopted as the optimizer. For initialization of the embeddings of a predicate/argument, the embeddings of the predicate/argument trained by the method described in Section 4.1 are utilized. The weight matrices are randomly initialized. 5 Experiment 5.1 Experimental Setting The KWDLC (Kyoto University Web Document Leads Corpus) evaluation set (Hangyo et al., 2012) was used for our experiments, because it contains 1240 a wide variety of Web documents, such as news articles and blogs. This evaluation set consists of the first three sentences of 5,000 Web documents. Morphology, named entities, dependencies, PASs, and coreferences were manually annotated. This evaluation set was divided into 3,694 documents (11,558 sents.) for training, 512 documents (1,585 sents.) for development, and 700 documents (2,195 sents.) for testing. Table 1 shows the statistics of the number of arguments in the test set. While “dep argument” means that the argument and a predica"
P16-1117,D13-1095,1,0.828124,"sis, dependency analysis, and named entities were used. The sentences having a predicate that takes multiple arguments in the same case role were excluded from training and test examples, since the base model cannot handle this phenomena (it assumes that each predicate has only one argument with one case role). For example, the following sentence, added as well as a NULL node in a PA graph of the base model. When the argument predication score is calculated for “author” or “reader,” because its lemma does not appear in a document, for each noun in the following noun list of “author”/“reader” (Hangyo et al., 2013), the argument prediction score is calculated, and the maximum score is used as a feature. • author: “私” (I), “我々” (we), “僕” (I), “弊 社” (our company), · · · • reader: “あなた” (you), “客” (customer), “ 君” (you), “皆様”(you all), · · · In the argument prediction model training described in Section 4.1, a Japanese Web corpus consisting of 10M sentences was used. We preformed syntactic parsing with a publicly available Japanese parser, KNP4 . The number of negative samples was 5, and the number of epochs was 10. In the model training described in Section 4.3, the dimensions of both embeddings for predi"
P16-1117,D14-1163,0,0.0301029,"given the predicate “逮捕” (arrest) and its ACC “犯人” (suspect). 4 Proposed Model 4.1 Argument Prediction Model No external knowledge is utilized in the base model. One of the most important types of knowledge in PAS analysis is selectional preferences. Sasano and Kurohashi (2011) and Hangyo et al. (2013) extract knowledge of the selectional preferences in the form of case frames from a raw corpus, and the selectional preference score is used as a feature. In this work, argument prediction model is trained using a neural network from a raw corpus, in a similar way to Titov and Khoddam (2015) and Hashimoto et al. (2014). PASs are first extracted from an automaticallyparsed raw corpus, and in each PAS, the argument ai is generated with the following probability p(ai |P AS−ai ): p(ai |P AS−ai ) = T exp(v T ai Wai (Wpred v pred + Z ∑ j=i Waj v aj )) (4) where P AS−ai represents a PAS excluding the target argument ai , v pred , v ai and v aj represent embeddings of the predicate, argument ai and argument aj , and Wpred , Wai , and Waj represent transformation matrices for a predicate and an argument ai and aj . Z is the partition function. Figure 4 illustrates the argument prediction model. The PAS “警察” (police"
P16-1117,I11-1023,0,0.420368,"Missing"
P16-1117,D15-1260,0,0.226421,"capture interactions between predicates and their arguments, and thus performs the best among the three types. This method is adopted as our base model (see Section 3 for details). Most methods for PAS analysis handle both intra-sentential and inter-sentential zero anaphora. For identifying inter-sentential zero anaphora, an antecedent has to be searched in a broad search space, and the salience of discourse entities has to be captured. Therefore, the task of identifying inter-sentential zero anaphora is more difficult than that of intra-sentential zero anaphora. Thus, Ouchi et al. (2015) and Iida et al. (2015) focused on only intra-sentential zero anaphora. Following this trend, this paper focuses on intra-sentential zero anaphora. Recently, NN-based approaches have achieved improvement for several NLP tasks. For example, in transition-based parsing, Chen and Manning (2014) proposed an NN-based approach, where the words, POS tags, and dependency labels are first represented by embeddings individually. Then, an NN-based classifier is built to make parsing decisions, where an input layer is a concatenation of embeddings of words, POS tags, and dependency labels. This model has been extended by severa"
P16-1117,P09-2022,0,0.359611,"Missing"
P16-1117,P15-1093,0,0.408129,"admissible PA graphs for the input sentence x. Score(x, y) is defined as follows3 : ∑ ∑ scorel (x, e)+ scoreg (x, ei , ej ). e∈E(y) ei ,ej ∈Epair (y) scorel (x, e) = θ l · ϕl (x, e) scoreg (x, ei , ej ) = θ g · ϕg (x, ei , ej ) 1 2 3 4 5 (2) 6 (3) 7 8 where E(y) is the edge set on the candidate graph y, Epair (y) is a set of edge pairs in the edge set E(y), scorel (x, e) and scoreg (x, ei , ej ) represent 2 For example, in the sentence “今日は 暑い” (today-TOP hot), the predicate “暑い” does not take “今日”, which represents time, as an argument. Therefore, these nodes do not always have a relation. 3 Ouchi et al. (2015) introduce two models: Per-Case Joint Model and All-Cases Joint Model. Since All-Cases Joint Model performed better than Per-Case Joint Model, AllCases Joint Model is adopted as our base model. Input: sentence x, parameter θ Output: a locally optimal PA graph y˜ Sample a PA graph y (0) from G(x) t←0 repeat ∪ Y ← N eighborG(y (t) ) y (t) y (t+1) ← argmax Score(x, y; θ) y∈Y t←t+1 until y (t) = y (t+1) return y˜ ← y (t) Figure 3: Hill climbing algorithm for obtaining optimal PA graph. Given N training examples D = {(x, yˆ)}N k , the model parameter θ are estimated. θ is the set of θ l and θ g , a"
P16-1117,I11-1085,1,0.702167,"Missing"
P16-1117,D08-1055,0,0.41578,"Missing"
P16-1117,N15-1001,0,0.0269968,"th the NOM case is predicted given the predicate “逮捕” (arrest) and its ACC “犯人” (suspect). 4 Proposed Model 4.1 Argument Prediction Model No external knowledge is utilized in the base model. One of the most important types of knowledge in PAS analysis is selectional preferences. Sasano and Kurohashi (2011) and Hangyo et al. (2013) extract knowledge of the selectional preferences in the form of case frames from a raw corpus, and the selectional preference score is used as a feature. In this work, argument prediction model is trained using a neural network from a raw corpus, in a similar way to Titov and Khoddam (2015) and Hashimoto et al. (2014). PASs are first extracted from an automaticallyparsed raw corpus, and in each PAS, the argument ai is generated with the following probability p(ai |P AS−ai ): p(ai |P AS−ai ) = T exp(v T ai Wai (Wpred v pred + Z ∑ j=i Waj v aj )) (4) where P AS−ai represents a PAS excluding the target argument ai , v pred , v ai and v aj represent embeddings of the predicate, argument ai and argument aj , and Wpred , Wai , and Waj represent transformation matrices for a predicate and an argument ai and aj . Z is the partition function. Figure 4 illustrates the argument prediction"
P16-1117,P15-1032,0,0.0491291,"Missing"
P16-1117,D14-1109,0,0.0166218,"e and a global score for the edge pair ei and ej , ϕl (x, e) and ϕg (x, ei , ej ) represent local features and global features. While ϕl (x, e) is defined for each edge e, ϕg (x, ei , ej ) is defined for each edge pair ei , ej (i = j) . θ l and θ g represent model parameters for local and global features. By using global scores, the interaction between multiple case assignments of multiple predicates can be considered. 3.2 Inference and Training Since global features make the inference of finding the maximum scoring PA graph more difficult, the randomized hill-climbing algorithm proposed in (Zhang et al., 2014) is adopted. Figure 3 describes the pseudo code for hillclimbing algorithm. First, an initial PA graph y (0) is sampled from the set of admissible PA graph G(x). Then, the union Y is constructed from the set of neighboring graphs N eighborG(y (t) ), which is a set of admissible graphs obtained by changing one edge in y (t) , and the current graph y (t) . The current graph y (t) is updated to a higher scoring graph y (t+1) . This process continues until no more improvement is possible, and finally an optimal graph y˜ can be obtained. (1) where G(x) is a set of admissible PA graphs for the input"
P16-1117,P15-1109,0,0.138911,"Missing"
P16-3002,N03-1017,0,0.0321069,"Missing"
P16-3002,li-etal-2010-enriching,0,0.0603641,"Missing"
P16-3002,W09-3818,0,0.0243735,"word if the nodes in dependency trees have different spans. For example, in Figure 1 there are two nodes for the word “boy” because they have different spans (i.e., (2, 4) and (2, 7)). The construction of a dependency forest from dependency trees is done by sharing the common nodes and edges (Line 1). The common nodes are those with the same span and part-of-speech (POS) . Note that the dependency forest obtained from this method does not necessarily encode exactly the dependency trees from which they are created. Usually there are more trees that can be extracted from the dependency forests (Boullier et al., 2009). In our experiment, when we use the term “a n-best dependency forest”, we indicate a dependency forest that is created from n-best dependency trees. 2.2 Finding Alignments over Forest Following the hierarchical alignment model (Riesa et al., 2011), our model searches for the best alignment by constructing partial alignments (hypotheses) over target dependency forests in a bottom-up manner as shown in Figure 1. The algorithm for constructing alignments is shown in Algorithm 2. Note that source dependency forests are included in the input to the algorithm. This is optional but can be included f"
P16-3002,I11-1089,1,0.748713,"e used 300, 100, 100 sentences from ASPEC-JE2 for training, development and test data, respectively.4 Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128. We used different number of parse trees to create a target forest, e.g., 1, 10, 20, 50, 100 and 200.5 The baseline in this experiment is a model with 1-best parse trees on the target side. For reference, we also experimented on Nile6 , the Bayesian subtree alignment model (Nakazawa model) (Nakazawa and Kurohashi, 2011) and IBM Model4.7 We used Nile without automatically extracted rule features and constellation features to make a fair comparison with our model. We observed the improvement of alignments by using forests. We checked whether good parse trees were chosen when higher F-scores were achieved. It turned out that better parse trees led to higher F-scores, as shown in Figure 2a, but it was not always the case. Figure 2a shows an improved example by using 100-best trees on the target side. In the figure, we can observe that “の” and “of” are correctly aligned. We observe that the English 1-best parse t"
P16-3002,J04-4002,0,0.342213,"Missing"
P16-3002,J93-2003,0,0.150318,"Missing"
P16-3002,J07-2003,0,0.103676,"t, we compute its score using local features (Line 7) and pushed to a priority queue Bv (Line 8). These partial alignments are represented by black squares in a blue container in Figure 1. Then, we compute partial alignments for the target words covered by the node, by combining tails’ partial alignments and one column alignments for its word using nonlocal features (Line 10 - 14), which is represented by the orange arrows in Figure 1. k-best combined partial alignments are put in Yv (Line 14). They are represented by black squares in a yellow container in Figure 1. Here, we use cube pruning (Chiang, 2007) to get the approximate k-best combinations. Note that in the search over constituency parse trees, one column alignment matrices are generated only on the leaf node (Riesa et al., 2011), whereas we generate them also on nonleaf nodes in the search over dependency forests. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Input : Source and target sentence s, t Dependency forest Fs over s Dependency forest Ft over t Set of feature functions h Weight vector w Beam size k Output: A k-best list of alignments over s and t for v ∈TopologicalSort(Ft ) do links = ∅ Bv = ∅ i = word-index-of(v) links = {(0, i)} ∪Sin"
P16-3002,P11-1042,0,0.0350707,"Missing"
P16-3002,D11-1046,0,0.0967341,"one by sharing the common nodes and edges (Line 1). The common nodes are those with the same span and part-of-speech (POS) . Note that the dependency forest obtained from this method does not necessarily encode exactly the dependency trees from which they are created. Usually there are more trees that can be extracted from the dependency forests (Boullier et al., 2009). In our experiment, when we use the term “a n-best dependency forest”, we indicate a dependency forest that is created from n-best dependency trees. 2.2 Finding Alignments over Forest Following the hierarchical alignment model (Riesa et al., 2011), our model searches for the best alignment by constructing partial alignments (hypotheses) over target dependency forests in a bottom-up manner as shown in Figure 1. The algorithm for constructing alignments is shown in Algorithm 2. Note that source dependency forests are included in the input to the algorithm. This is optional but can be included for richer features. Each node in the forest has partial alignments sorted by alignment scores. Because it is computationally expensive to keep all possible partial alignments for each node, we keep a beam size of k. A partial alignment for a node i"
P16-3002,P08-1067,0,0.0388108,"alignments sorted by alignment scores. Because it is computationally expensive to keep all possible partial alignments for each node, we keep a beam size of k. A partial alignment for a node is an alignment matrix for target words that are cov9 ered by the node. In Figure 1, each partial alignment is represented as a black square. Scores of the partial alignments are a linear combination of features. There are two types of features: local and non-local features. A feature f is defined to be local if and only if it can be factored among the local productions in a tree, and non-local otherwise (Huang, 2008). We visit the nodes in the topological order, to guarantee that we visit a node after visiting all its tail nodes (Line 1). For each node, we first generates partial alignments, which are one column alignment matrices for its word. Because of time complexity, we only generates null, single link and double link alignment (Line 5). A single and double link alignment refer to a column matrix having exactly one and two alignments, respectively, as shown in Figure 1. For each partial alignment, we compute its score using local features (Line 7) and pushed to a priority queue Bv (Line 8). These par"
P16-3002,D10-1052,0,0.0546363,"Missing"
P16-3002,P08-1066,0,0.0553831,"Missing"
P16-3002,P14-1138,0,0.0195662,"ult: Black boxes represent golden alignments. Triangles represent 1-best model alignments. Circles represent the alignments of proposed model. Black and red arcs represent 1-best parses and chosen parses respectively. humans. However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbitrary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently (Tamura et al., 2014; Songyot and Chiang, 2014; Yang et al., 2013). They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses nbest alignments. Venugopal et al. (2008) used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use nbest alignments already obtained from some alignment models, our model finds k-best list of alignments for given sentences. Mi et al. (2008) and Tu et al. (2010) used packed constituency forests and dependency forests resp"
P16-3002,C10-1123,0,0.238834,"ing syntactic information as features, and it searches for k-best partial alignments on the target constituent parse trees. It achieved significantly better results than the IBM Model4 in Arabic-English and ChineseEnglish word alignment tasks, even though the model was trained on only 2,280 and 1,102 parallel sentences as gold standard alignments. However, their models rely only on 1-best source and target side parse trees, which are not necessarily good for word alignment tasks. In SMT, forest-based decoding has been proposed for both constituency and dependency parse trees (Mi et al., 2008; Tu et al., 2010). A forest is a compact representation of n-best parse trees. It provides more alternative parse trees to choose from during decoding, leading to significant improvements in translation quality. In this paper, we borrow this idea to build an alignment model using dependency forests rather than 1-best parses, which makes it possible to provide the model with more alternative parse trees that may be suitable for word alignment tasks. The motivation of using dependency forests instead of constituency forests in our model is that dependency forests are more appropriate for alignments between langu"
P16-3002,2008.amta-papers.18,0,0.0237822,". However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbitrary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently (Tamura et al., 2014; Songyot and Chiang, 2014; Yang et al., 2013). They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses nbest alignments. Venugopal et al. (2008) used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use nbest alignments already obtained from some alignment models, our model finds k-best list of alignments for given sentences. Mi et al. (2008) and Tu et al. (2010) used packed constituency forests and dependency forests respectively for decoding. The best path that is suitable for translation is chosen from the forest during decoding, leading to significant improvement in translation quality. Note that they do not use forests for obtaining word ali"
P16-3002,P13-1017,0,0.0179165,"riangles represent 1-best model alignments. Circles represent the alignments of proposed model. Black and red arcs represent 1-best parses and chosen parses respectively. humans. However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbitrary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently (Tamura et al., 2014; Songyot and Chiang, 2014; Yang et al., 2013). They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses nbest alignments. Venugopal et al. (2008) used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use nbest alignments already obtained from some alignment models, our model finds k-best list of alignments for given sentences. Mi et al. (2008) and Tu et al. (2010) used packed constituency forests and dependency forests respectively for decoding. The best path that is s"
P16-3002,N07-1051,0,\N,Missing
P16-3002,D09-1106,0,\N,Missing
P16-3002,D14-1197,0,\N,Missing
P17-1111,D15-1159,0,0.0641307,"Missing"
P17-1111,P16-1231,0,0.0930603,"y role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features. 1 Introduction Dependency parsers have been enhanced by the use of neural networks and embedding vectors (Chen and Manning, 2014; Weiss et al., 2015; Zhou et al., 2015; Alberti et al., 2015; Andor et al., 2016; Dyer et al., 2015). When these dependency parsers process sentences in English and other languages that use symbols for word separations, they can be very accurate. However, for languages that do not contain word separation symbols, dependency parsers are used in pipeline processes with word segmentation and POS tagging models, and encounter serious problems because of error propagations. In particular, Chinese word segmentation is notoriously difficult because sentences are written without word dividers and Chinese words are not clearly defined. Hence, the pipeline of word segmentation, POS"
P17-1111,D15-1041,0,0.0182796,"Missing"
P17-1111,D14-1082,0,0.525801,"character strings are available. Otherwise, the embeddings of the character strings are used. mension and are chosen in the neural computation graph. We avoid using the “UNK” vector as far as possible, because this degenerates the information about unknown tokens. However, models use the “UNK” vector if the parser encounters characters that are not in the pre-trained embeddings, though this is quite uncommon. 2.3 Feed-forward Neural Network 2.3.1 Neural Network We present a feed-forward neural network model in Figure 2. The neural network for greedy training is based on the neural networks of Chen and Manning (2014) and Weiss et al. (2015). We add the dynamic generation of the embeddings of character strings for unknown tokens, as described in Section 2.2. This neural network has two hidden layers with 8,000 dimensions. This is larger than Chen and Manning (2014) (200 dimensions) or Weiss et al. (2015) (1,024 or 2,048 dimensions). We use the ReLU for the activation function of the hidden layers (Nair and Hinton, 2010) and the softmax function for the output layer of the greedy 1206 Type Value Type Features Size of h1 ,h2 Initial learning rate Initial learning rate of beam decoding Embedding vocabulary si"
P17-1111,P16-2006,0,0.0133814,"17. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1111 these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing. We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens. Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering. Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their models, the bi-LSTM is used to represent the tokens including their context. Indeed, such neural networks can observe whole sentence through the bi-LSTM. This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014). As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models. We also develop joint models with ngram character string bi-LSTM. In the experiments, we obtain state-of-the-art Chinese word segmentation and POS tagging scores, and the pipeline of the dependency model achieves the better de"
P17-1111,P15-1033,0,0.0154611,"Missing"
P17-1111,I11-1136,0,0.04379,"he difference in the training set size. We present the final results with four hidden layers in Table 10. 3.2.7 Bi-LSTM Model We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine t"
P17-1111,P12-1110,0,0.112837,"be directly applied to joint models because they use given word segmentations. To solve 1204 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1204–1214 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1111 these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing. We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens. Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering. Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their models, the bi-LSTM is used to represent the tokens including their context. Indeed, such neural networks can observe whole sentence through the bi-LSTM. This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014). As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-"
P17-1111,P08-1102,0,0.221872,"Missing"
P17-1111,Q16-1023,0,0.0637301,", Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1111 these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing. We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens. Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering. Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their models, the bi-LSTM is used to represent the tokens including their context. Indeed, such neural networks can observe whole sentence through the bi-LSTM. This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014). As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models. We also develop joint models with ngram character string bi-LSTM. In the experiments, we obtain state-of-the-art Chinese word segmentation and POS tagging scores, and the pipeline of the dependency mode"
P17-1111,W04-0308,0,0.00996663,"des. The words in the stack are formed by the following transition operations. • SH(t) (shift): Shift the first character of the buffer to the top of the stack as a new word. • AP (append): Append the first character of the buffer to the end of the top word of the stack. • RR (reduce-right): Reduce the right word of the top two words of the stack, and make the right child node of the left word. • RL (reduce-left): Reduce the left word of the top two words of the stack, and make the left child node of the right word. The RR and RL operations are the same as those of the arc-standard algorithm (Nivre, 2004a). SH makes a new word whereas AP makes the current word longer by adding one character. The POS tags are attached with the SH(t) transition. In this paper, we explore both greedy models and beam decoding models. This parsing algorithm works in both types. We also develop a joint model of word segmentation and POS tagging, along with a dependency parsing model. The joint model of word segmentation and POS tagging does not have RR and RL transitions. 2.2 Based on Hatori et al. (2012), we use a modified arc-standard algorithm for character transiBuﬀer (character-based) 有 Embeddings of Character"
P17-1111,I11-1035,0,0.103015,"er string embeddings can capture similarities among them. Following the bi-LSTM layer, the feature function extracts the corresponding outputs of the bi-LSTM layer. We summarize the features in Table 3. Finally, MLP and the softmax function outputs the transition probability. We use an MLP with three hidden layers as for the model in Section 2.3. We train this neural network with the loss function for the greedy training. Experimental Settings We use the Penn Chinese Treebank 5.1 (CTB5) and 7 (CTB-7) datasets to evaluate our models, following the splitting of Jiang et al. (2008) for CTB-5 and Wang et al. (2011) for CTB-7. The statistics of datasets are presented in Table 4. We use the Chinese Gigaword Corpus for embedding pre-training. Our model is developed for unlabeled dependencies. The development set is used for parameter tuning. Following Hatori et al. (2012) and Zhang et al. (2014), we use the standard word-level evaluation with F1-measure. The POS tags and dependencies cannot be correct unless the corresponding words are correctly segmented. We trained three models: SegTag, SegTagDep and Dep. SegTag is the joint word segmentation and POS tagging model. SegTagDep is the full joint segmentatio"
P17-1111,P13-2110,0,0.0806643,"ed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473. Table 11: Bi-LSTM feature extraction model. “4feat.” and “8feat.” denote the use of four an"
P17-1111,P15-1032,0,0.167578,"ble. Otherwise, the embeddings of the character strings are used. mension and are chosen in the neural computation graph. We avoid using the “UNK” vector as far as possible, because this degenerates the information about unknown tokens. However, models use the “UNK” vector if the parser encounters characters that are not in the pre-trained embeddings, though this is quite uncommon. 2.3 Feed-forward Neural Network 2.3.1 Neural Network We present a feed-forward neural network model in Figure 2. The neural network for greedy training is based on the neural networks of Chen and Manning (2014) and Weiss et al. (2015). We add the dynamic generation of the embeddings of character strings for unknown tokens, as described in Section 2.2. This neural network has two hidden layers with 8,000 dimensions. This is larger than Chen and Manning (2014) (200 dimensions) or Weiss et al. (2015) (1,024 or 2,048 dimensions). We use the ReLU for the activation function of the hidden layers (Nair and Hinton, 2010) and the softmax function for the output layer of the greedy 1206 Type Value Type Features Size of h1 ,h2 Initial learning rate Initial learning rate of beam decoding Embedding vocabulary size Embedding vector size"
P17-1111,P13-1013,0,0.0249876,"Table 10. 3.2.7 Bi-LSTM Model We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. http://arxiv.org/abs"
P17-1111,P14-1125,0,0.623085,"oint models because they use given word segmentations. To solve 1204 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1204–1214 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1111 these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing. We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens. Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering. Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their models, the bi-LSTM is used to represent the tokens including their context. Indeed, such neural networks can observe whole sentence through the bi-LSTM. This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014). As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models. We als"
P17-1111,N15-1005,0,0.402275,"cause of the large mini-batch size. The neural network is implemented with Theano. 1209 Model Seg POS Model Hatori+12 SegTag Hatori+12 SegTag(d) Hatori+12 SegTagDep Hatori+12 SegTagDep(d) M. Zhang+14 EAG Y. Zhang+15 97.66 98.18 97.73 98.26 97.76 98.04 93.61 94.08 94.46 94.64 94.36 94.47 SegTag(g) SegTag 98.41 98.60 94.84 94.76 Table 5: Joint segmentation and POS tagging scores. Both scores are in F-measure. In Hatori et al. (2012), (d) denotes the use of dictionaries. (g) denotes greedy trained models. All scores for previous models are taken from Hatori et al. (2012), Zhang et al. (2014) and Zhang et al. (2015). 3.2 3.2.1 First, we evaluate the joint segmentation and POS tagging model (SegTag). Table 5 compares the performance of segmentation and POS tagging using the CTB-5 dataset. We train two modles: a greedy-trained model and a model trained with beams of size 4. We compare our model to three previous approaches: Hatori et al. (2012), Zhang et al. (2014) and Zhang et al. (2015). Our SegTag joint model is superior to these previous models, including Hatori et al. (2012)’s model with rich dictionary information, in terms of both segmentation and POS tagging accuracy. 3.2.2 Joint Segmentation, POS"
P17-1111,D08-1059,0,0.017231,"t-test. tice that the MLP with four hidden layers performs better than the MLP with three hidden layers, but we could not find definite differences in the experiments in CTB-5. We speculate that this is caused by the difference in the training set size. We present the final results with four hidden layers in Table 10. 3.2.7 Bi-LSTM Model We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Mo"
P17-1111,D10-1082,0,0.0712524,"ind definite differences in the experiments in CTB-5. We speculate that this is caused by the difference in the training set size. We present the final results with four hidden layers in Table 10. 3.2.7 Bi-LSTM Model We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering. 4 Related Work Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat"
P17-1111,D13-1061,0,0.0688612,"cy parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires 1211 Model Seg POS Dep Hatori+12 M. Zhang+14 EAG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473. Table 11: Bi-LSTM feature extraction model. “4feat.” and “8feat.” denote the use of four and eight features. base parsers. In neural joint models, Zheng et al. (2013) propose a neural network-based Chinese word segmentation model based on tag inferences. They extend their models for joint segmentation and POS tagging. Zhu et al. (2015) propose the re-ranking system of parsing results with recursive convolutional neural network. 5 Conclusion We propose the joint parsing models by the feedforward and bi-LSTM neural networks. Both of them use the character string embeddings. The character string embeddings help to capture the similarities of incomplete tokens. We also explore the neural network with few features using n-gram bi-LSTMs. Our SegTagDep joint mode"
P17-1111,P15-1117,0,0.0303434,"Missing"
P17-1111,P15-1112,0,0.019685,"AG SegTagDep (g) 97.75 97.76 98.24 94.33 94.36 94.49 81.56 81.70 80.15 Bi-LSTM 4feat.(g) Bi-LSTM 8feat.(g) 97.72 97.70 93.12 93.37 79.03 79.38 Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473. Table 11: Bi-LSTM feature extraction model. “4feat.” and “8feat.” denote the use of four and eight features. base parsers. In neural joint models, Zheng et al. (2013) propose a neural network-based Chinese word segmentation model based on tag inferences. They extend their models for joint segmentation and POS tagging. Zhu et al. (2015) propose the re-ranking system of parsing results with recursive convolutional neural network. 5 Conclusion We propose the joint parsing models by the feedforward and bi-LSTM neural networks. Both of them use the character string embeddings. The character string embeddings help to capture the similarities of incomplete tokens. We also explore the neural network with few features using n-gram bi-LSTMs. Our SegTagDep joint model achieves better scores of Chinese word segmentation and POS tagging than previous joint models, and our SegTag and Dep pipeline model achieves state-of-the-art score of"
P17-2061,W04-3250,0,0.160581,"43 20.01 37.66 35.79 33.74 34.61 37.57 37.23 37.77 Table 2: Domain adaptation results (BLEU-4 scores) for WIKI-CJ using ASPEC-CJ. 5 Results Tables 1 and 2 show the translation results on the Chinese-to-English and Chinese-to-Japanese tasks, respectively. The entries with SMT and NMT are the PBSMT and NMT systems, respectively; others are the different methods described in Section 3. In both tables, the numbers in bold indicate the best system and all systems that were not significantly different from the best system. The significance tests were performed using the bootstrap resampling method (Koehn, 2004) at p < 0.05. We can see that without domain adaptation, the SMT systems perform significantly better than the NMT system on the resource poor domains, i.e., IWSLT-CE and WIKI-CJ; while on the resource rich domains, i.e., NTCIR-CE and ASPECCJ, NMT outperforms SMT. Directly using the SMT/NMT models trained on the out-of-domain data to translate the in-domain data shows bad performance. With our proposed “Mixed fine tuning” domain adaptation method, NMT significantly outperforms SMT on the in-domain tasks. Comparing different domain adaptation methods, “Mixed fine tuning” shows the best perfor6"
P17-2061,D14-1179,0,0.0132915,"Missing"
P17-2061,P07-2045,0,0.00716769,"1 2.55 16.12 16.56 15.02 16.41 18.77 18.10 17.65 test 2013 14.67 7.29 4.74 2.85 17.12 17.54 15.96 16.80 18.63 17.97 17.94 average 14.31 7.87 4.33 2.60 16.41 16.34 14.97 15.82 18.01 17.43 17.11 Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE. System WIKI-CJ SMT WIKI-CJ NMT ASPEC-CJ SMT ASPEC-CJ NMT Fine tuning Multi domain Multi domain w/o tags Multi domain + Fine tuning Mixed fine tuning Mixed fine tuning w/o tags Mixed fine tuning + Fine tuning For performance comparison, we also conducted experiments on phrase based SMT (PBSMT). We used the Moses PBSMT system (Koehn et al., 2007) for all of our MT experiments. For the respective tasks, we trained 5-gram language models on the target side of the training data using the KenLM toolkit6 with interpolated KneserNey discounting, respectively. In all of our experiments, we used the GIZA++ toolkit7 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. For both MT systems, we preprocessed the data as follows. For Chinese, we used KyotoMorph8 for segmentation, which was trained on the CTB version 5 (CTB5) and SCTB (Chu et al., 2016b). For English, we tokeniz"
P17-2061,L16-1468,1,0.872655,"Missing"
P17-2061,W16-5407,1,0.90265,"ag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpus of the latter domain was automatically extracted (Chu et al., 2016a). We observed that “mixed fine tuning” works significantly better than methods that use fine tuning ∗ This work was done when the first author was a researcher of Japan Science and Technology Agency. 385 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 385–391 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2061 Source-Target (out-of-domain) NMT e control the politeness of NMT translations. The overview of this method is shown in the dotted section in Fig"
P17-2061,2015.iwslt-evaluation.11,0,0.147063,"em without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpu"
P17-2061,W16-4616,1,0.50204,"Missing"
P17-2061,D16-1046,0,0.0226074,"hat combines the best of existing approaches and show that it is effective. We can further fine tune the multi domain model on the in-domain data, which is named as “multi domain + fine tuning.” • To the best of our knowledge this is the first work on an empirical comparison of various domain adaptation methods. 2 3.3 Related Work The proposed mixed fine tuning method is a combination of the above methods (shown in Figure 2). The training procedure is as follows: Fine tuning has also been explored for various NLP tasks using neural networks such as sentiment analysis and paraphrase detection (Mou et al., 2016). Tag based NMT has also been shown to be effective for control the politeness of translations (Sennrich et al., 2016a) and multilingual NMT (Johnson et al., 2016). Besides fine tuning and multi domain NMT using tags, another direction of domain adaptation for NMT is using in-domain monolingual data. Either training an in-domain recurrent neural network (RNN) language model for the NMT decoder (G¨ulc¸ehre et al., 2015) or generating synthetic data by back translating target in-domain monolingual data (Sennrich et al., 2016b) have been studied. 3 1. Train an NMT model on out-of-domain data till"
P17-2061,W15-5001,1,0.85268,"Missing"
P17-2061,Q17-1024,0,0.0315585,"Missing"
P17-2061,P03-1021,0,0.0134846,"i domain w/o tags Multi domain + Fine tuning Mixed fine tuning Mixed fine tuning w/o tags Mixed fine tuning + Fine tuning For performance comparison, we also conducted experiments on phrase based SMT (PBSMT). We used the Moses PBSMT system (Koehn et al., 2007) for all of our MT experiments. For the respective tasks, we trained 5-gram language models on the target side of the training data using the KenLM toolkit6 with interpolated KneserNey discounting, respectively. In all of our experiments, we used the GIZA++ toolkit7 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. For both MT systems, we preprocessed the data as follows. For Chinese, we used KyotoMorph8 for segmentation, which was trained on the CTB version 5 (CTB5) and SCTB (Chu et al., 2016b). For English, we tokenized and lowercased the sentences using the tokenizer.perl script in Moses. Japanese was segmented using JUMAN9 (Kurohashi et al., 1994). For NMT, we further split the words into subwords using byte pair encoding (BPE) (Sennrich et al., 2016c), which has been shown to be effective for the rare word problem in NMT. Another motivation of using sub-word"
P17-2061,N16-1005,0,0.0989293,"Missing"
P17-2061,P16-1009,0,0.516582,"al with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpus of the latter domain"
P17-2061,P16-1162,0,0.519258,"al with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpus of the latter domain"
P17-2061,D16-1163,0,0.0622938,"shortcomings. 1 Introduction One of the most attractive features of neural machine translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) is that it is possible to train an end to end system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted r"
P17-2061,W14-7001,1,\N,Missing
P17-2061,2015.iwslt-evaluation.1,0,\N,Missing
P18-1044,D17-1230,0,0.040184,"bata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2017a). 476 3.2 while fixing the validator V . This generator training loss using the validator can be explained as follows. The generator tries to increase the validator scores to 1, while the validator is fixed. If the validator is well-trained, it returns scores close to 1 for correct PAS labels that the generator outputs, and 0 for wrong labels. Therefore, in Equation (6), the generator tries to predict correct labels in order to increase the scores of fixed validator. Note that the validator has a sigmoid function for the output of scores. Therefore"
P18-1044,W17-4712,0,0.0227115,"phora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2017a). 476 3.2 while fixing the validator V . This generator training loss using the validator can be explained as follows. The generator tries to increase the validator scores to 1, while the validator is fixed. If the validator is well-trained, it returns scores close to 1 for correct PAS labels that the generator outputs, and 0 for wrong labels. Therefore, in Equation (6), the generator tries to predict correct labels in order to increase the scores of fixed validator. Note that the validator has a sigmoid function"
P18-1044,P17-1001,0,0.313019,"rresponding case argument or that the case argument is not written in the sentence. involved), Our contributions are summarized as follows: (1) a novel adversarial training model for PAS analysis; (2) learning from a raw corpus as a source of external knowledge; and (3) as a result, we achieve state-of-the-art performance on Japanese PAS analysis. ing pre-trained external knowledge in the form of word embeddings has also been ubiquitous. However, such external knowledge is overwritten in the task-specific training. The other approach to using raw corpora for PAS analysis is data augmentation. Liu et al. (2017b) generate pseudo training data from a raw corpus and use them for their zero pronoun resolution model. They generate the pseudo training data by dropping certain words or pronouns in a raw corpus and assuming them as correct antecedents. After generating the pseudo training data, they rely on ordinary supervised training based on neural networks. In this paper, we propose a neural semisupervised model for Japanese PAS analysis. We adopt neural adversarial training to directly exploit the advantage of using a raw corpus. Our model consists of two neural network models: a generator model of Ja"
P18-1044,P17-1010,0,0.385019,"rresponding case argument or that the case argument is not written in the sentence. involved), Our contributions are summarized as follows: (1) a novel adversarial training model for PAS analysis; (2) learning from a raw corpus as a source of external knowledge; and (3) as a result, we achieve state-of-the-art performance on Japanese PAS analysis. ing pre-trained external knowledge in the form of word embeddings has also been ubiquitous. However, such external knowledge is overwritten in the task-specific training. The other approach to using raw corpora for PAS analysis is data augmentation. Liu et al. (2017b) generate pseudo training data from a raw corpus and use them for their zero pronoun resolution model. They generate the pseudo training data by dropping certain words or pronouns in a raw corpus and assuming them as correct antecedents. After generating the pseudo training data, they rely on ordinary supervised training based on neural networks. In this paper, we propose a neural semisupervised model for Japanese PAS analysis. We adopt neural adversarial training to directly exploit the advantage of using a raw corpus. Our model consists of two neural network models: a generator model of Ja"
P18-1044,I17-2022,0,0.791284,"ents are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2017a). 476 3.2 while fixing th"
P18-1044,Y12-1058,1,0.875776,"ormances are evaluated with microaveraged F-measure (Shibata et al., 2016). Table 4: KWDLC training data statistics for each case. Case Zero Ouchi+ 2015 Shibata+ 2016 76.5 89.3 42.1 53.4 Gen Gen+Adv 91.5 92.0‡ 56.2 58.4‡ Table 5: The results of case analysis (Case) and zero anaphora resolution (Zero). We use Fmeasure as an evaluation measure. ‡ denotes that the improvement is statistically significant at p &lt; 0.05, compared with Gen using paired t-test. 4 4.1 Experiments Experimental Settings Following Shibata et al. (2016), we use the KWDLC (Kyoto University Web Document Leads Corpus) corpus (Hangyo et al., 2012) for our experiments.1 This corpus contains various Web documents, such as news articles, personal blogs, and commerce sites. In KWDLC, lead three sentences of each document are annotated with PAS structures including zero pronouns. For a raw corpus, we use a Japanese web corpus created by Hangyo et al. (2012), which has no duplicated sentences with KWDLC. This raw corpus is automatically parsed by the Japanese dependency parser KNP. We focus on intra-sentential anaphora resolution, and so we apply a preprocess to KWDLC. We regard the anaphors whose antecedents are in the preceding sentences a"
P18-1044,D13-1095,1,0.948228,"xi), while this does not have direct dependencies to the second predicate. A zero anaphora resolution model has to find “タクシー”(taxi) from the sentence, and assign it to the NOM case of the second predicate. LD = − Ex∼pdata (x) [log D(x)]  +Ez∼pz (z) [log(1 − D(G(z)))] , (1) and they train the discriminator by minimizing this loss while fixing the generator G. Similarly, the generator G is trained through minimizing In the zero anaphora resolution task, some correct arguments are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i"
P18-1044,W07-1522,0,0.029009,"ochs. model of Ouchi et al. (2015). They achieved state-of-the-art results on case analysis and zero anaphora resolution using the KWDLC corpus. They use an external resource to extract selectional preferences. Since our model uses an external resource, we compare our model with the models of Shibata et al. (2016) and Ouchi et al. (2015). Ouchi et al. (2017) proposed a semantic role labeling-based PAS analysis model using GridRNNs. Matsubayashi and Inui (2017) proposed a case label selection model with feature-based neural networks. They conducted their experiments on NAIST Text Corpus (NTC) (Iida et al., 2007, 2016). NTC consists of newspaper articles, and does not include the annotations of author/reader expressions that are common in Japanese natural sentences. However, the Gen+Adv model judged the DAT argument as “author”. Although we cannot specify φ as “author” only from this sentence, “author” is a possible argument depending on the context. 4.4.2 Validator Analysis We also evaluate the performance of the validator during the adversarial training with raw corpora. Figure 3 shows the validator performance and the generator performance of Zero on the development set. The validator score is eva"
P18-1044,P15-1093,0,0.291601,"rvised and unsupervised training of the generator, while we do not use minibatch for validator training. Therefore, we use k = 16 and l = 4. Other parameters are summarized in Table 2. Note that the validator is a simple neural network compared with the generator. The validator has limited inputs of predicates and arguments and no inputs of other words in sentences. This allows the generator to overwhelm the validator during the adversarial training. 479 KWDLC NOM ACC DAT # of dep # of zero 7,224 6,453 1,555 515 448 1,248 reader “あなた” (you), “君” (you), “客” (customer), “皆様” (you all) Following Ouchi et al. (2015) and Shibata et al. (2016), we conduct two kinds of analysis: (1) case analysis and (2) zero anaphora resolution. Case analysis is the task to determine the correct case labels when predicates and their arguments have direct dependencies but their case markers are hidden by surface markers, such as topic markers. Zero anaphora resolution is a task to find certain case arguments that do not have direct dependencies to their predicates in the sentence. Following Shibata et al. (2016), we exclude predicates that the same arguments are filled in multiple cases of a predicate. This is relatively un"
P18-1044,D16-1132,0,0.151108,"ora resolution task, some correct arguments are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classific"
P18-1044,P17-1146,0,0.565249,", some correct arguments are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2"
P18-1044,P16-1113,0,0.0310735,"e concatenated and fed into the bi-LSTM layers. The bi-LSTM layers read these embeddings in forward and backward order and outputs the distributed representations of a predicate and a candidate argument: hpredj and hargi . Note that we also use the exophora entities, i.e., an author and a reader, as argument candidates. Therefore, we use specific embeddings for them. These embeddings are not generated by the biLSTM layers but are directly used in the argument selection model. We also use path embeddings to capture a dependency relation between a predicate and its candidate argument as used in Roth and Lapata (2016). Although Roth and Lapata (2016) use a one-way LSTM layer to represent the dependency path from a predicate to its potential argument, we use a bi-LSTM layer for this purpose. We feed the embeddings of words and POS tags to the bi-LSTM layer. In this way, the resulting path embedding represents both predicate-toargument and argument-to-predicate paths. We concatenate the bidirectional path embeddings to generate hpathij , which represents the dependency relation between the predicate j and its candidate argument i. For the argument selection model, we apply the argument selection model (Zhang"
P18-1044,I11-1085,1,0.926589,"Missing"
P18-1044,P16-1117,1,0.670851,"zing In the zero anaphora resolution task, some correct arguments are not specified in the article. This is called as exophora. We consider “author” and “reader” arguments as exophora (Hangyo et al., 2013). They are frequently dropped from Japanese natural sentences. Sentence (4) is an example of dropped nominative arguments. In this sentence, the nominative argument is “あなた” (you), but “あ なた” (you) does not appear in the sentence. This is also included in zero anaphora resolution. Except these special arguments of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017),"
P18-1044,W17-2629,0,0.170923,"ts of exophora, we focus on intra-sentential anaphora resolution in the same way as (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017). We also attach NULL labels to cases that predicates do not have. LG = i 1 Xh  log 1 − D(G(z(i) )) , (2) |Dz | i while fixing the discriminator D. By doing this, the discriminator tries to descriminate the generated images from real images, while the generator tries to generate images that can deceive the adversarial discriminator. This training scheme is applied for many generative tasks including sentence generation (Subramanian et al., 2017), machine translation (Britz et al., 2017), dialog generation (Li et al., 2017), and text classification (Liu et al., 2017a). 476 3.2 while fixing the validator V . This generator training loss using the validator can be explained as follows. The generator tries to increase the validator scores to 1, while the validator is fixed. If the validator is well-trained, it returns scores close to 1 for correct PAS labels that the generator outputs, and 0 for wrong labels. Therefore, in Equation (6), the generator tries to predict correct labels in order to increase the scores of fixed validator. Note"
P18-1044,E17-1063,0,0.0804674,"generating the pseudo training data, they rely on ordinary supervised training based on neural networks. In this paper, we propose a neural semisupervised model for Japanese PAS analysis. We adopt neural adversarial training to directly exploit the advantage of using a raw corpus. Our model consists of two neural network models: a generator model of Japanese PAS analysis and a so-called “validator” model of the generator prediction. The generator neural network is a model that predicts probabilities of candidate arguments of each predicate using RNN-based features and a head-selection model (Zhang et al., 2017). The validator neural network gets inputs from the generator and scores them. This validator can score the generator prediction even when PAS gold labels are not available. We apply supervised learning to the generator and unsupervised learning to the entire network using a raw corpus. 2 Task Description Japanese PAS analysis determines essential case roles of words for each predicate: who did what to whom. In many languages, such as English, case roles are mainly determined by word order. However, in Japanese, word order is highly flexible. In Japanese, major case roles are the nominative ca"
P18-1054,W03-2604,0,0.577921,"proach for the entity representation. The reason why we do not use this is that if we take a clustering approach in our setting, zero pronouns need to be first identified before clustering, and thus, it is hard to perform CR and PA jointly. Lee et al. (2017) take an end-to-end approach, aiming at not relying on hand-engineering mention detector (consider all spans as potential mentions). In used Japanese evaluation corpora, since the basic unit for the annotations and our analyses (CR and PA) is fixed, we do not need consider all spans. In Japanese, CR has not been actively studied other than Iida et al. (2003); Sasano et al. (2007) Related Work Predicate Argument Structure Analysis. Early studies have handled both intra- and intersentential anaphora (Taira et al., 2008; Sasano and Kurohashi, 2011), and Hangyo et al. (2013) present a method for handling exophora. Recent studies, however, focus on only intra-sentential anaphora (Ouchi et al., 2015; Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017), because the analysis of intersentential anaphora is extremely difficult. Neural network-based approaches (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017"
P18-1054,P16-1074,0,0.0479842,"pired by (Wiseman et al., 2016), which described an English CR system, where entities are represented by embeddings, and they are updated by CR results dynamically. We perform Japanese CR and PA by extending this idea. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically. 2 reported the salience score did not improve the performance. In contrast, we perform CR automatically, and capture the entity salience by using RNNs. For Chinese, where zero anaphors are often used, neural network-based approaches (Chen and Ng, 2016; Yin et al., 2017) outperformed conventional machine learning approaches (Zhao and Ng, 2007). Coreference Resolution. CR has been actively studied in English and Chinese. Neural networkbased approaches (Wiseman et al., 2016; Clark and Manning, 2016b,a; Lee et al., 2017) outperformed conventional machine learning approaches (Clark and Manning, 2015). Wiseman et al. (2016) and Clark and Manning (2016b) learn an entity representation and integrate this into a mentionbased model. Our work is inspired by Wiseman et al. (2016), which learn the entity representation by using Recurrent Neural Network"
P18-1054,D16-1132,0,0.234641,"ions). In used Japanese evaluation corpora, since the basic unit for the annotations and our analyses (CR and PA) is fixed, we do not need consider all spans. In Japanese, CR has not been actively studied other than Iida et al. (2003); Sasano et al. (2007) Related Work Predicate Argument Structure Analysis. Early studies have handled both intra- and intersentential anaphora (Taira et al., 2008; Sasano and Kurohashi, 2011), and Hangyo et al. (2013) present a method for handling exophora. Recent studies, however, focus on only intra-sentential anaphora (Ouchi et al., 2015; Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017), because the analysis of intersentential anaphora is extremely difficult. Neural network-based approaches (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) have improved its performance. Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) consider an entity, and its salience score is calculated based on simple rules. However, they used gold coreference links to form the entities, and 580 since the use of zero pronouns is more common and problematic. Semantic Role Lab"
P18-1054,D17-1195,0,0.0371246,"Missing"
P18-1054,P15-1136,0,0.0318079,"solution drastically. 2 reported the salience score did not improve the performance. In contrast, we perform CR automatically, and capture the entity salience by using RNNs. For Chinese, where zero anaphors are often used, neural network-based approaches (Chen and Ng, 2016; Yin et al., 2017) outperformed conventional machine learning approaches (Zhao and Ng, 2007). Coreference Resolution. CR has been actively studied in English and Chinese. Neural networkbased approaches (Wiseman et al., 2016; Clark and Manning, 2016b,a; Lee et al., 2017) outperformed conventional machine learning approaches (Clark and Manning, 2015). Wiseman et al. (2016) and Clark and Manning (2016b) learn an entity representation and integrate this into a mentionbased model. Our work is inspired by Wiseman et al. (2016), which learn the entity representation by using Recurrent Neural Networks (RNNs). Clark and Manning (2016b) adopt a clustering approach for the entity representation. The reason why we do not use this is that if we take a clustering approach in our setting, zero pronouns need to be first identified before clustering, and thus, it is hard to perform CR and PA jointly. Lee et al. (2017) take an end-to-end approach, aiming"
P18-1054,D16-1245,0,0.0286012,"demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically. 2 reported the salience score did not improve the performance. In contrast, we perform CR automatically, and capture the entity salience by using RNNs. For Chinese, where zero anaphors are often used, neural network-based approaches (Chen and Ng, 2016; Yin et al., 2017) outperformed conventional machine learning approaches (Zhao and Ng, 2007). Coreference Resolution. CR has been actively studied in English and Chinese. Neural networkbased approaches (Wiseman et al., 2016; Clark and Manning, 2016b,a; Lee et al., 2017) outperformed conventional machine learning approaches (Clark and Manning, 2015). Wiseman et al. (2016) and Clark and Manning (2016b) learn an entity representation and integrate this into a mentionbased model. Our work is inspired by Wiseman et al. (2016), which learn the entity representation by using Recurrent Neural Networks (RNNs). Clark and Manning (2016b) adopt a clustering approach for the entity representation. The reason why we do not use this is that if we take a clustering approach in our setting, zero pronouns need to be first identified before clustering, an"
P18-1054,P16-1061,0,0.0377746,"demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically. 2 reported the salience score did not improve the performance. In contrast, we perform CR automatically, and capture the entity salience by using RNNs. For Chinese, where zero anaphors are often used, neural network-based approaches (Chen and Ng, 2016; Yin et al., 2017) outperformed conventional machine learning approaches (Zhao and Ng, 2007). Coreference Resolution. CR has been actively studied in English and Chinese. Neural networkbased approaches (Wiseman et al., 2016; Clark and Manning, 2016b,a; Lee et al., 2017) outperformed conventional machine learning approaches (Clark and Manning, 2015). Wiseman et al. (2016) and Clark and Manning (2016b) learn an entity representation and integrate this into a mentionbased model. Our work is inspired by Wiseman et al. (2016), which learn the entity representation by using Recurrent Neural Networks (RNNs). Clark and Manning (2016b) adopt a clustering approach for the entity representation. The reason why we do not use this is that if we take a clustering approach in our setting, zero pronouns need to be first identified before clustering, an"
P18-1054,N16-1099,0,0.0164145,"n simple rules. However, they used gold coreference links to form the entities, and 580 since the use of zero pronouns is more common and problematic. Semantic Role Labeling. Japanese PA is similar to Semantic Role Labeling (SRL) in English. Neural network-based approaches have improved the performance (Zhou and Xu, 2015; He et al., 2017). In these approaches, an appropriate argument for a predicate is searched among mentions in a text. The notion entity is not considered. Other Entity-Centric Study. There are several studies that consider the notion entity in other areas: text comprehension (Kobayashi et al., 2016; Henaff et al., 2016) and language modeling (Ji et al., 2017). necessary. The same phenomenon happens when a relative clause is used. When an argument is modified by a relative clause, we do not know its case role to the predicate in the relative clause. In the example, although “パン” has a dependency relation with “買った” (bought), the analysis of identifying ACC is necessary. Zero anaphora resolution (ZAR): Some arguments are not included in the phrases with which a predicate has a dependency relation. While pronouns are mostly used in English, they are rarely used in Japanese. This phenomenon"
P18-1054,D17-1018,0,0.0212645,"thod can improve the performance of the inter-sentential zero anaphora resolution drastically. 2 reported the salience score did not improve the performance. In contrast, we perform CR automatically, and capture the entity salience by using RNNs. For Chinese, where zero anaphors are often used, neural network-based approaches (Chen and Ng, 2016; Yin et al., 2017) outperformed conventional machine learning approaches (Zhao and Ng, 2007). Coreference Resolution. CR has been actively studied in English and Chinese. Neural networkbased approaches (Wiseman et al., 2016; Clark and Manning, 2016b,a; Lee et al., 2017) outperformed conventional machine learning approaches (Clark and Manning, 2015). Wiseman et al. (2016) and Clark and Manning (2016b) learn an entity representation and integrate this into a mentionbased model. Our work is inspired by Wiseman et al. (2016), which learn the entity representation by using Recurrent Neural Networks (RNNs). Clark and Manning (2016b) adopt a clustering approach for the entity representation. The reason why we do not use this is that if we take a clustering approach in our setting, zero pronouns need to be first identified before clustering, and thus, it is hard to"
P18-1054,Y12-1058,1,0.869579,"gnment is used with probability ϵt (at the t-th iteration) and the system output otherwise. Exponential decay is used: ϵt = k t (we set k = 0.75 for our experiments). Entity Embedding Update ei [ ] ∑ hi (k) ), e k i−1 7 Experiments 7.1 Experimental Setting (12) The two kinds of evaluation sets were used for our experiments. One is the KWDLC (Kyoto UniIn both CR and PA, the embeddings of entities other than the referred entity k are not updated (l) (l) (ei ← ei−1 (l = k)). 5 When arg is NAPA , the entity embedding is set to a zero vector. 584 versity Web Document Leads Corpus) evaluation set (Hangyo et al., 2012), and the other is Kyoto Corpus. KWDLC consists of the first three sentences of 5,000 Web documents (15,000 sentences) and Kyoto Corpus consists of 550 News documents (5,000 sentences). Word segmentations, POSs, dependencies, PASs, and coreferences were manually annotated (the closest referents and antecedents were annotated for zero anaphora and coreferences, respectively). Since we want to focus on the accuracy of CR and PA, gold segmentations, POSs, and dependencies were used. KWDLC (Web) was divided into 3,694 documents (11,558 sents.) for training, 512 documents (1,585 sents.) for develop"
P18-1054,I17-2022,0,0.126541,"Missing"
P18-1054,D13-1095,1,0.900532,"erform CR and PA jointly. Lee et al. (2017) take an end-to-end approach, aiming at not relying on hand-engineering mention detector (consider all spans as potential mentions). In used Japanese evaluation corpora, since the basic unit for the annotations and our analyses (CR and PA) is fixed, we do not need consider all spans. In Japanese, CR has not been actively studied other than Iida et al. (2003); Sasano et al. (2007) Related Work Predicate Argument Structure Analysis. Early studies have handled both intra- and intersentential anaphora (Taira et al., 2008; Sasano and Kurohashi, 2011), and Hangyo et al. (2013) present a method for handling exophora. Recent studies, however, focus on only intra-sentential anaphora (Ouchi et al., 2015; Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017), because the analysis of intersentential anaphora is extremely difficult. Neural network-based approaches (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) have improved its performance. Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) consider an entity, and its salience score is calculated based on"
P18-1054,P17-1044,0,0.0237235,"(Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) have improved its performance. Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) consider an entity, and its salience score is calculated based on simple rules. However, they used gold coreference links to form the entities, and 580 since the use of zero pronouns is more common and problematic. Semantic Role Labeling. Japanese PA is similar to Semantic Role Labeling (SRL) in English. Neural network-based approaches have improved the performance (Zhou and Xu, 2015; He et al., 2017). In these approaches, an appropriate argument for a predicate is searched among mentions in a text. The notion entity is not considered. Other Entity-Centric Study. There are several studies that consider the notion entity in other areas: text comprehension (Kobayashi et al., 2016; Henaff et al., 2016) and language modeling (Ji et al., 2017). necessary. The same phenomenon happens when a relative clause is used. When an argument is modified by a relative clause, we do not know its case role to the predicate in the relative clause. In the example, although “パン” has a dependency relation with “"
P18-1054,P15-1093,0,0.0178397,"ctor (consider all spans as potential mentions). In used Japanese evaluation corpora, since the basic unit for the annotations and our analyses (CR and PA) is fixed, we do not need consider all spans. In Japanese, CR has not been actively studied other than Iida et al. (2003); Sasano et al. (2007) Related Work Predicate Argument Structure Analysis. Early studies have handled both intra- and intersentential anaphora (Taira et al., 2008; Sasano and Kurohashi, 2011), and Hangyo et al. (2013) present a method for handling exophora. Recent studies, however, focus on only intra-sentential anaphora (Ouchi et al., 2015; Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017), because the analysis of intersentential anaphora is extremely difficult. Neural network-based approaches (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) have improved its performance. Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) consider an entity, and its salience score is calculated based on simple rules. However, they used gold coreference links to form the entities, and 580 since the use of zero pronouns is more"
P18-1054,D17-1135,0,0.0431536,"t al., 2016), which described an English CR system, where entities are represented by embeddings, and they are updated by CR results dynamically. We perform Japanese CR and PA by extending this idea. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically. 2 reported the salience score did not improve the performance. In contrast, we perform CR automatically, and capture the entity salience by using RNNs. For Chinese, where zero anaphors are often used, neural network-based approaches (Chen and Ng, 2016; Yin et al., 2017) outperformed conventional machine learning approaches (Zhao and Ng, 2007). Coreference Resolution. CR has been actively studied in English and Chinese. Neural networkbased approaches (Wiseman et al., 2016; Clark and Manning, 2016b,a; Lee et al., 2017) outperformed conventional machine learning approaches (Clark and Manning, 2015). Wiseman et al. (2016) and Clark and Manning (2016b) learn an entity representation and integrate this into a mentionbased model. Our work is inspired by Wiseman et al. (2016), which learn the entity representation by using Recurrent Neural Networks (RNNs). Clark and"
P18-1054,P17-1146,0,0.762862,"nese evaluation corpora, since the basic unit for the annotations and our analyses (CR and PA) is fixed, we do not need consider all spans. In Japanese, CR has not been actively studied other than Iida et al. (2003); Sasano et al. (2007) Related Work Predicate Argument Structure Analysis. Early studies have handled both intra- and intersentential anaphora (Taira et al., 2008; Sasano and Kurohashi, 2011), and Hangyo et al. (2013) present a method for handling exophora. Recent studies, however, focus on only intra-sentential anaphora (Ouchi et al., 2015; Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017), because the analysis of intersentential anaphora is extremely difficult. Neural network-based approaches (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) have improved its performance. Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) consider an entity, and its salience score is calculated based on simple rules. However, they used gold coreference links to form the entities, and 580 since the use of zero pronouns is more common and problematic. Semantic Role Labeling. Japanese PA i"
P18-1054,D07-1057,0,0.032112,"presented by embeddings, and they are updated by CR results dynamically. We perform Japanese CR and PA by extending this idea. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically. 2 reported the salience score did not improve the performance. In contrast, we perform CR automatically, and capture the entity salience by using RNNs. For Chinese, where zero anaphors are often used, neural network-based approaches (Chen and Ng, 2016; Yin et al., 2017) outperformed conventional machine learning approaches (Zhao and Ng, 2007). Coreference Resolution. CR has been actively studied in English and Chinese. Neural networkbased approaches (Wiseman et al., 2016; Clark and Manning, 2016b,a; Lee et al., 2017) outperformed conventional machine learning approaches (Clark and Manning, 2015). Wiseman et al. (2016) and Clark and Manning (2016b) learn an entity representation and integrate this into a mentionbased model. Our work is inspired by Wiseman et al. (2016), which learn the entity representation by using Recurrent Neural Networks (RNNs). Clark and Manning (2016b) adopt a clustering approach for the entity representation"
P18-1054,P16-1113,0,0.0150691,"carg, mi , c)) carg∈ ARG(mi ) • whether a pair of mi and ant has an entry in a synonym dictionary. When a candidate antecedent is NACR , the input vector is just the embedding of a target mention mi , and the same neural network with different weight matrices calculates a score. The following margin objective is trained: LCR = sm PA (arg, mi , c) W2PA PA , W PA are weight matrices, and v PA where W1,c 2 input is an input vector, a concatenation of the following vectors: • embeddings of mi and arg 2 • path embedding: the dependency path between a predicate and an argument is an important clue. Roth and Lapata (2016) learn a representation of a lexicalized dependency path for SRL. An LSTM reads words3 from an argument to a predicate along with a dependency path, and the final hidden state is adopted as the embedding of the dependency path.4 For case analysis, the direct dependency relation between a predicate and its argument can be represented as the path embedding. (7) ant∈T (mi ) where T (mi ) denotes the set of true antecedents of mi . 5.3 Predicate Argument Structure Analysis When a target phrase is a predicate phrase, PA is performed. For each case of a predicate, PA searches an appropriate argument"
P18-1054,P15-1109,0,0.0247672,"rk-based approaches (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) have improved its performance. Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) consider an entity, and its salience score is calculated based on simple rules. However, they used gold coreference links to form the entities, and 580 since the use of zero pronouns is more common and problematic. Semantic Role Labeling. Japanese PA is similar to Semantic Role Labeling (SRL) in English. Neural network-based approaches have improved the performance (Zhou and Xu, 2015; He et al., 2017). In these approaches, an appropriate argument for a predicate is searched among mentions in a text. The notion entity is not considered. Other Entity-Centric Study. There are several studies that consider the notion entity in other areas: text comprehension (Kobayashi et al., 2016; Henaff et al., 2016) and language modeling (Ji et al., 2017). necessary. The same phenomenon happens when a relative clause is used. When an argument is modified by a relative clause, we do not know its case role to the predicate in the relative clause. In the example, although “パン” has a dependen"
P18-1054,I11-1085,1,0.740371,"tering, and thus, it is hard to perform CR and PA jointly. Lee et al. (2017) take an end-to-end approach, aiming at not relying on hand-engineering mention detector (consider all spans as potential mentions). In used Japanese evaluation corpora, since the basic unit for the annotations and our analyses (CR and PA) is fixed, we do not need consider all spans. In Japanese, CR has not been actively studied other than Iida et al. (2003); Sasano et al. (2007) Related Work Predicate Argument Structure Analysis. Early studies have handled both intra- and intersentential anaphora (Taira et al., 2008; Sasano and Kurohashi, 2011), and Hangyo et al. (2013) present a method for handling exophora. Recent studies, however, focus on only intra-sentential anaphora (Ouchi et al., 2015; Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017), because the analysis of intersentential anaphora is extremely difficult. Neural network-based approaches (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) have improved its performance. Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) consider an entity, and its salience sco"
P18-1054,P16-1117,1,0.95597,"pans as potential mentions). In used Japanese evaluation corpora, since the basic unit for the annotations and our analyses (CR and PA) is fixed, we do not need consider all spans. In Japanese, CR has not been actively studied other than Iida et al. (2003); Sasano et al. (2007) Related Work Predicate Argument Structure Analysis. Early studies have handled both intra- and intersentential anaphora (Taira et al., 2008; Sasano and Kurohashi, 2011), and Hangyo et al. (2013) present a method for handling exophora. Recent studies, however, focus on only intra-sentential anaphora (Ouchi et al., 2015; Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017), because the analysis of intersentential anaphora is extremely difficult. Neural network-based approaches (Shibata et al., 2016; Iida et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) have improved its performance. Although most of studies did not consider the notion entity, Sasano and Kurohashi (2011) consider an entity, and its salience score is calculated based on simple rules. However, they used gold coreference links to form the entities, and 580 since the use of zero pronouns is more common and problematic"
P18-1054,D08-1055,0,0.0992483,"Missing"
P99-1062,P87-1019,0,0.0336399,"Missing"
P99-1062,J94-4001,1,0.841616,"ucing resources employed, we explain the algorithm of the two analyses. 4.1 R e s o u r c e s 4.1.1 RSK RSK (Reikai Shougaku Kokugojiten), a Japanese dictionary for children, is used to find semantic roles of nouns in DBA. The reason why we use a dictionary for children is that, generally speaking, definition sentences of such a dictionary are described by basic words, 483 which helps the system finding links between N1 and a semantic role of a head word. All definition sentences in RSK were analyzed by JUMAN, a Japanese morphological analyzer, and KNP, a Japanese syntactic and case analyzer (Kurohashi and Nagao, 1994; Kurohashi and Nagao, 1998). Then, a genus word for a head word, like a person for coach were detected in the definition sentences by simple rules: in a Japanese definition sentence, the last word is a genus word in almost all cases; if there is a noun coordination at the end, all of those nouns are regarded as genus words. 4.1.2 NTT Semantic Feature Dictionary NTT Communication Science Laboratories (NTT CS Lab) constructed a semantic feature tree, whose 3,000 nodes are semantic features, and a nominal dictionary containing about 300,000 nouns, each of which is given one or more appropriate s"
P99-1062,W98-0605,1,0.796458,"-part gray no seihuku 'uniform' modification senmonka 'expert' no chousa 'study' agent rugby no coach subject yakyu 'baseball' no senshu 'player' category kaze 'cold' no virus result ryokou 'travel' no jyunbi 'preparation' purpose toranpu 'card' no tejina 'trick' instrument The conventional approach to this problem was to classify semantic relations, such as possession, whole-part, modification, and others. Then, classification rules were crafted by hand, or detected from relation-tagged examples by a machine learning technique (Shimazu et al., 1987; Sumita et al., 1990; Tomiura et al., 1995; Kurohashi et al., 1998). The problem in such an approach is to set up the semantic relations. For example, the above examples and their classification came from the IPA nominal dictionary (InformationTechnology Promotion Agency, Japan, 1996). Is it possible to find clear boundaries among subject, category, result, purpose, instrument, and others? No matter how fine-grained relations we set up, we always encounter phrases which are on the boundary or belong to two or more relations. This paper proposes a completely different approach to the task, which exploits semantic role information of nouns in an ordinary dictio"
P99-1062,P98-2180,0,0.0325323,"there have been several related research conducts: the mental space theory is discussing the functional behavior of nouns (Fauconnier, 1985); the generative lexicon theory accounts for the problem of creative word senses based on the qualia structure of a word (Pustejovsky, 1995); Dahl et al. (1987) and Macleod et al. (1997) discussed the treatment of nominalizations. Compared with these studies, the point of this paper is that an ordinary dictionary can be a useful resource of semantic roles of nouns. Our approach using an ordinary dictionary is similar to the approach used to creat MindNet (Richardson et al., 1998). However, the semanitc analysis of noun phrases is a much more specialized and suitable application of utilizing dictionary entries. Conclusion The paper proposed a method of analyzing Japanese N1 no N2 phrases based on a dictionary, interpreting obscure phrases very clearly. The method can be applied to the analysis of compound nouns, like baseball player. Roughly speaking, the semantic diversity in compound nouns is a subset of that in N1 no N2 phrases. Furthermore, the method must be applicable to 7 the analysis of English noun phrases. The translated explanation in the paper naturally ind"
P99-1062,P87-1018,0,0.154712,"'I' no kuruma 'car' possession tsukue 'desk' no ashi 'leg' whole-part gray no seihuku 'uniform' modification senmonka 'expert' no chousa 'study' agent rugby no coach subject yakyu 'baseball' no senshu 'player' category kaze 'cold' no virus result ryokou 'travel' no jyunbi 'preparation' purpose toranpu 'card' no tejina 'trick' instrument The conventional approach to this problem was to classify semantic relations, such as possession, whole-part, modification, and others. Then, classification rules were crafted by hand, or detected from relation-tagged examples by a machine learning technique (Shimazu et al., 1987; Sumita et al., 1990; Tomiura et al., 1995; Kurohashi et al., 1998). The problem in such an approach is to set up the semantic relations. For example, the above examples and their classification came from the IPA nominal dictionary (InformationTechnology Promotion Agency, Japan, 1996). Is it possible to find clear boundaries among subject, category, result, purpose, instrument, and others? No matter how fine-grained relations we set up, we always encounter phrases which are on the boundary or belong to two or more relations. This paper proposes a completely different approach to the task, whi"
P99-1062,C98-2175,0,\N,Missing
richardson-etal-2014-bilingual,W10-2408,0,\N,Missing
richardson-etal-2014-bilingual,P07-2045,0,\N,Missing
richardson-etal-2014-bilingual,J03-1002,0,\N,Missing
richardson-etal-2014-bilingual,I13-1030,1,\N,Missing
richardson-etal-2014-bilingual,J98-4003,0,\N,Missing
richardson-etal-2014-bilingual,I08-6006,0,\N,Missing
S01-1009,S01-1008,0,0.026602,"f KWIC (numbers indicate phrase frequency). The lexicographers pick up a typical expression of the target word from the KWIC. If its sense is context-independently clear, the expression is adopted as it is. If its sense is not clear, some pre/post expressions are supplemented by referring original sentences in the newspaper corpus. Then, we asked a translation company to translate the Japanese expressions. As a result, a TM containing 320 head words and 6920 records was constructed (one head word has 21.6 records on average). The average number of words of a Japanese expression is 4.5. 3 task(Shirai, 2001). 40 target words of the tran: lation task consists of 20 nouns and 20 verbs: difficult nouns and verbs, 10 intermediate nom and verbs, and 5 easy nouns and verbs. For each target word, 30 instances were ch&lt; sen from Mainichi Newspaper corpus (in tot 1,200 instances) and they are also overlappe with the dictionary task. Since the dictionar task uses 100 instances for each target won the translation task used 1st, 4th, 7th, ... 90t instances of the dictionary task. As a gold standard data, zero or more ar propriate TM records were assigned to each ir stance by the same translation company. Ar p"
S01-1009,S01-1013,0,\N,Missing
S16-1178,W13-2322,0,0.0782985,"Missing"
S16-1178,W09-1206,0,0.0946195,"Missing"
S16-1178,P13-2131,0,0.133802,"3 0.60 0.63 labels, we apply this transition without asking for the perceptron prediction. Finally, if the network is not confident about the prediction – that is, if the probability for the highest scoring candidate is lower than 0.9 – we disregard the NN prediction and choose the prediction given by the perceptron algorithm. Experiments All the experiments were performed on the LDC2015E86 dataset, provided by the organizers. In our experiments we followed the standard train/dev/test split (16, 833, 1, 368 and 1, 371 sentences, respectively). Parser performance was evaluated with the Smatch (Cai and Knight, 2013) scoring script v2.0.2 4 (Table 2). As expected, using SRL features resulted in better performance compared to the baseline model (roughly a 2 F1 points gain). Conditioning on a wider context was also beneficial – widening the context to include more configuration elements is often a good feature expansion technique (Toutanova et al., 2003; Chen et al., 2014). In contrast to our expectations, the NN classifier did not improve the parser performance. This might be due to the higher complexity of the AMR parsing task or the peculiarities of the underlying parsing algorithm (as mentioned in Secti"
S16-1178,D14-1082,0,0.0538673,"ks. At the time of writing this paper two AMR parsers are publicly available: graph-based JAMR (Flanigan et al., 2014) and transition-based CAMR (Wang et al., 2015a; Wang et al., 2015b). The latter has served as our baseline model, which we tried to improve by incorporating additional features defined for a wider conditioning context and a neural network (NN) classifier. Preprocessing We used the Stanford CoreNLP v3.6.0 toolkit (Manning et al., 2014) to get named entity (NE) and dependency information, the latter in the form of Stanford dependencies was obtained from the NN dependency parser (Chen and Manning, 2014). We used a publicly available semantic role labelling (SRL) system with a predicate disambiguation module (Bj¨orkelund et al., 2009). The system is a part of MATE tools1 , which also include a lemmatizer, a part of speech (POS) tagger, and a dependency parser. We use them to obtain lemmas and POS tags. All the tools were used with the pretrained models. Using MALT dependencies instead of or in tandem with Stanford dependencies did not change the 1 https://storage.googleapis.com/ google-code-archive-downloads/v2/code. google.com/mate-tools 1154 Proceedings of SemEval-2016, pages 1154–1159, c S"
S16-1178,C14-1078,0,0.0301038,"e performed on the LDC2015E86 dataset, provided by the organizers. In our experiments we followed the standard train/dev/test split (16, 833, 1, 368 and 1, 371 sentences, respectively). Parser performance was evaluated with the Smatch (Cai and Knight, 2013) scoring script v2.0.2 4 (Table 2). As expected, using SRL features resulted in better performance compared to the baseline model (roughly a 2 F1 points gain). Conditioning on a wider context was also beneficial – widening the context to include more configuration elements is often a good feature expansion technique (Toutanova et al., 2003; Chen et al., 2014). In contrast to our expectations, the NN classifier did not improve the parser performance. This might be due to the higher complexity of the AMR parsing task or the peculiarities of the underlying parsing algorithm (as mentioned in Section 3, we discarded some actionspecific features due to the difficulty of their integration into the NN model). Further investigation on this matter is required to draw ground conclusions. 8 Conclusion We have performed a range of experiments which resulted in improving the performance of the baseline AMR parsing system. The results show that a richer feature"
S16-1178,W02-1001,0,0.466357,".ac.jp {kuro,dk}@i.kyoto-u.ac.jp Abstract Inspired by the results of Chen and Manning (2014) and Weiss et al. (2015), who obtained state-of-the-art results in transition-based dependency parsing using Feedforward Neural Networks (FFNN), and taking into account the transition nature of the CAMR model, we performed a series of experiments in the same direction. Neural networks have been successfully applied to many NLP fields and we were curious to examine their potential in the task of AMR parsing. Specifically, we investigated the possibility of constraining the averaged perceptron algorithm (Collins, 2002) predictions by those of an FFNN at the initial step of the inference process. This paper describes our contribution to the SemEval 2016 Workshop. We participated in the Shared Task 8 on Meaning Representation parsing using a transition-based approach, which builds upon the system of Wang et al. (2015a) and Wang et al. (2015b), with additions that utilize a Feedforward Neural Network classifier and an enriched feature set. We observed that exploiting Neural Networks in Abstract Meaning Representation parsing is challenging and we could not benefit from it, while the feature enhancements yielde"
S16-1178,dorr-etal-1998-thematic,0,0.232536,"Missing"
S16-1178,P14-1134,0,0.462078,"performance over the baseline model. 1 2 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism which represents sentence meaning in a form of a rooted directed acyclic graph. AMR graph nodes represent concepts, labelled directed edges between the nodes show the relationships between concepts. The AMR formalism was created in order to explore the semantics behind natural language units for further analysis and application in various tasks. At the time of writing this paper two AMR parsers are publicly available: graph-based JAMR (Flanigan et al., 2014) and transition-based CAMR (Wang et al., 2015a; Wang et al., 2015b). The latter has served as our baseline model, which we tried to improve by incorporating additional features defined for a wider conditioning context and a neural network (NN) classifier. Preprocessing We used the Stanford CoreNLP v3.6.0 toolkit (Manning et al., 2014) to get named entity (NE) and dependency information, the latter in the form of Stanford dependencies was obtained from the NN dependency parser (Chen and Manning, 2014). We used a publicly available semantic role labelling (SRL) system with a predicate disambigua"
S16-1178,P14-5010,0,0.00862248,"ips between concepts. The AMR formalism was created in order to explore the semantics behind natural language units for further analysis and application in various tasks. At the time of writing this paper two AMR parsers are publicly available: graph-based JAMR (Flanigan et al., 2014) and transition-based CAMR (Wang et al., 2015a; Wang et al., 2015b). The latter has served as our baseline model, which we tried to improve by incorporating additional features defined for a wider conditioning context and a neural network (NN) classifier. Preprocessing We used the Stanford CoreNLP v3.6.0 toolkit (Manning et al., 2014) to get named entity (NE) and dependency information, the latter in the form of Stanford dependencies was obtained from the NN dependency parser (Chen and Manning, 2014). We used a publicly available semantic role labelling (SRL) system with a predicate disambiguation module (Bj¨orkelund et al., 2009). The system is a part of MATE tools1 , which also include a lemmatizer, a part of speech (POS) tagger, and a dependency parser. We use them to obtain lemmas and POS tags. All the tools were used with the pretrained models. Using MALT dependencies instead of or in tandem with Stanford dependencies"
S16-1178,W04-2705,0,0.062934,"hI (Figure 1 (3) and (4)) . We then conto hE , h 1 0 1 1156 I catenate layers hE 1 and h1 (Figure 1 (5)) and pass the resultant vector to the last hidden layer h3 , applying the tanh function again (Figure 1 (6)). Finally, a softmax layer is added on top of h3 in order to calculate probabilities of the output classes (Figure 1 (7)). 5 Feature Sets We have designed two separate feature sets for the NN and perceptron classifiers. The feature set for the latter is roughly the same as in (Wang et al., 2015b) (Table 1). Following the authors of CAMR, we also make use of the NomBank 1.0 dictionary (Meyers et al., 2004) 2 . Unfortunately, we could not obtain the copy of the same SRL system which was used by the authors. Therefore, we also measure accuracy improvement from incorporating the semantic features defined in the original paper but extracted after processing the data with a different SRL system (marked with a ?). We also measure the improvement from incorporating the features extracted from a wider configuration context – they were not included into the baseline model and are marked with a •. In the case of the NN classifier we follow a standard feature extraction procedure and discard transition-sp"
S16-1178,N03-1033,0,0.111853,"All the experiments were performed on the LDC2015E86 dataset, provided by the organizers. In our experiments we followed the standard train/dev/test split (16, 833, 1, 368 and 1, 371 sentences, respectively). Parser performance was evaluated with the Smatch (Cai and Knight, 2013) scoring script v2.0.2 4 (Table 2). As expected, using SRL features resulted in better performance compared to the baseline model (roughly a 2 F1 points gain). Conditioning on a wider context was also beneficial – widening the context to include more configuration elements is often a good feature expansion technique (Toutanova et al., 2003; Chen et al., 2014). In contrast to our expectations, the NN classifier did not improve the parser performance. This might be due to the higher complexity of the AMR parsing task or the peculiarities of the underlying parsing algorithm (as mentioned in Section 3, we discarded some actionspecific features due to the difficulty of their integration into the NN model). Further investigation on this matter is required to draw ground conclusions. 8 Conclusion We have performed a range of experiments which resulted in improving the performance of the baseline AMR parsing system. The results show th"
S16-1178,N15-1040,0,0.524577,"l, we performed a series of experiments in the same direction. Neural networks have been successfully applied to many NLP fields and we were curious to examine their potential in the task of AMR parsing. Specifically, we investigated the possibility of constraining the averaged perceptron algorithm (Collins, 2002) predictions by those of an FFNN at the initial step of the inference process. This paper describes our contribution to the SemEval 2016 Workshop. We participated in the Shared Task 8 on Meaning Representation parsing using a transition-based approach, which builds upon the system of Wang et al. (2015a) and Wang et al. (2015b), with additions that utilize a Feedforward Neural Network classifier and an enriched feature set. We observed that exploiting Neural Networks in Abstract Meaning Representation parsing is challenging and we could not benefit from it, while the feature enhancements yielded an improved performance over the baseline model. 1 2 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism which represents sentence meaning in a form of a rooted directed acyclic graph. AMR graph nodes represent concepts, labelled dir"
S16-1178,P15-2141,0,0.474588,"l, we performed a series of experiments in the same direction. Neural networks have been successfully applied to many NLP fields and we were curious to examine their potential in the task of AMR parsing. Specifically, we investigated the possibility of constraining the averaged perceptron algorithm (Collins, 2002) predictions by those of an FFNN at the initial step of the inference process. This paper describes our contribution to the SemEval 2016 Workshop. We participated in the Shared Task 8 on Meaning Representation parsing using a transition-based approach, which builds upon the system of Wang et al. (2015a) and Wang et al. (2015b), with additions that utilize a Feedforward Neural Network classifier and an enriched feature set. We observed that exploiting Neural Networks in Abstract Meaning Representation parsing is challenging and we could not benefit from it, while the feature enhancements yielded an improved performance over the baseline model. 1 2 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism which represents sentence meaning in a form of a rooted directed acyclic graph. AMR graph nodes represent concepts, labelled dir"
S16-1178,P15-1032,0,0.0502542,"Missing"
S16-1178,W09-1201,1,\N,Missing
shibata-etal-2014-large,W10-0905,0,\N,Missing
shibata-etal-2014-large,chambers-jurafsky-2010-database,0,\N,Missing
shibata-etal-2014-large,N06-1023,1,\N,Missing
shibata-etal-2014-large,P10-1160,0,\N,Missing
shibata-etal-2014-large,N04-1038,0,\N,Missing
shibata-etal-2014-large,P08-1047,0,\N,Missing
shibata-etal-2014-large,P08-1090,0,\N,Missing
shibata-etal-2014-large,P09-1068,0,\N,Missing
shibata-etal-2014-large,P10-1100,0,\N,Missing
shibata-etal-2014-large,I11-1115,1,\N,Missing
shibata-etal-2014-large,P12-2031,0,\N,Missing
shinzato-etal-2008-large,J94-4001,1,\N,Missing
shinzato-etal-2008-large,kawahara-kurohashi-2006-case,1,\N,Missing
shinzato-etal-2008-large,I08-1025,1,\N,Missing
W00-1016,J94-4001,1,0.832848,"The helpsystem reported in the paper answers what, how and symptom questions. In addition, it can interpret addition and answer utterances contextually. The request utterances are out of the system scope currently. 4 Outline of the helpsystem The system is comprised of the following components (Figure 1): U s e r I n t e r f a c e : Users access to the helpsystern via a WWW browser by using CGI based HTML forms. The helpsystem is 143 actually running on a workstation in our lab. I n p u t A n a l y z e r : The user utterance is transformed into a dependency structure by a robust parser, KNP (Kurohashi and Nagao, 1994; Kurohashi and Nagao, 1998), and utterance-pattern rules are applied to extract the utterance type and the utterance content. Japanese is head-final and the final expression shows an utterance type. Therefore, the longest matching of utterancepattern rules form the end of the utter° ance can detect the utterance type in most cases. For example, if the final expression is ""niha doushitara ii desu ka (How can I -- -)"", how type is assigned; if ""no baai ha (In case...)"", addition type. K n o w l e d g e base: The knowledge base is written in a natural language, in a dictionary-like structure. D"
W00-1310,P99-1022,0,0.0575575,"Missing"
W00-1310,P99-1043,0,0.0681003,"Missing"
W00-1310,X96-1031,0,0.0276616,"docum e nt is considered, the more reliable and realistic t h e co-occurrence information will be. Then, the row size of a worddocument co-occurrence matrix m ay become very large. Since enormous amounts of online text are available these days, row size can become more t h a n a million documents. Then, it is not practical to use a word-docmnent cooccurrence m a t r i x as it is. It is necessary to reduce row size and to simulate the tendency in the original m a t r i x by a reduced matrix. 2.2 W2 4 As such a m a t r i x reduction, we utilized a learning m e t h o d developed by HNC Software (Ilgen and Rushall, 1996). 1 representation of word co-occurrence information; and show that the context can be represented as a sum of word co-occurrence vectors in a d o c m n e n t and it is incorporated in a nonlocal language model. Word Wl Figure 2: ~Vord-word co-occurrence matrix. Figure 1: V~rord-document co-occurrence matrix. 2 Wl w2 w3 w4 w5 w6 F(w,:, wj) (2) This is because x/F(wi) corresponds to the magnitude of the row-vector of wl, and F(wl, wi) corresponds to the dot product of the row-vector of wl and t hat of wj in the word-docmnent cooccurrence matrix. 2. Given a reduced row size, a matrix is initiali"
W00-1310,P94-1013,0,0.0606007,"Missing"
W00-1707,C92-1029,1,0.8014,"Missing"
W00-1707,C94-2183,1,0.7942,"Missing"
W00-1707,W97-0713,0,0.0636486,"Missing"
W00-1707,J91-2003,0,0.0927815,"Missing"
W00-1707,J86-3001,0,\N,Missing
W03-0312,A00-2018,0,\N,Missing
W03-0312,S01-1009,1,\N,Missing
W03-0312,2002.tmi-papers.9,0,\N,Missing
W03-0312,J94-4001,1,\N,Missing
W03-0312,W01-1402,0,\N,Missing
W03-0312,C94-1015,0,\N,Missing
W03-0312,C90-3044,0,\N,Missing
W03-0312,W01-1406,0,\N,Missing
W03-0312,2001.mtsummit-ebmt.4,0,\N,Missing
W03-0312,C90-3101,0,\N,Missing
W03-0312,2001.mtsummit-papers.5,1,\N,Missing
W06-0123,I05-3017,0,0.0673595,"sks, which are: (c) Wn (n=-1,0,1) Feature Wn (n=-1,0,1) mean the lexicon words in different positions (the word containing C0 and one word to its left and right) and they are also binary features. We select all the possible words in the lexicon that satisfy the requirements, not like only selecting the longest one in (J.K.Low et al.,2005). To create the lexicon, we use following steps. First, a lexicon from NICT (National Institute of Information and Communications Technology, Japan) is used as the basic lexicon, which is extracted from Peking University Corpus of the second SIGHAN Bakeoff (T.Emerson, 2005), Penn Chinese Treebank 4.0 (N.Xue et al.,2002), a Chinese-to-English Wordlist 1 and part of NICT corpus (K.Uchimoto et al.,2004; Y.J.Zhang et al.,2005). Then, all the words containing digits and letters are removed 1 http://projects.ldc.upenn.edu/Chinese/ from this lexicon. At last, all the punctuations in Penn Chinese Treebank 5.1 (N.Xue et al.,2002) and all the words in the training data of UPUC and MSRA corpuses are added into the lexicon. Besides of above features, some extra features are defined only for NER task. First, we add some character-based features to improve the accuracy of per"
W06-0123,I05-3025,0,0.0494544,"Missing"
W06-0123,C04-1067,0,0.0727626,"Missing"
W06-0123,W96-0213,0,0.260018,"Missing"
W06-0123,W04-2208,0,0.040584,"Missing"
W06-0123,C02-1145,0,0.0585979,"Missing"
W06-0123,I05-2015,0,0.0323549,"Missing"
W06-0123,W03-1026,0,\N,Missing
W06-0123,2005.mtsummit-papers.10,0,\N,Missing
W09-2302,P03-1012,0,0.0940334,"nformation from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohashi (2008) also proposed a model focusing on the dependency relations. Their model has the constraint that content words can only correspond to content words on the other side, and the same applies for function words. This sometimes leads to an incorrect alignment. We have removed this constraint to make more flexible alignments possible. Moreover, in their model,"
W09-2302,W06-1628,0,0.0396246,"Missing"
W09-2302,P03-1011,0,0.0171911,"methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohashi (2008) also pro"
W09-2302,N06-1023,1,0.761571,"thm. During the Step 2 iterations, word correspondences are grown into phrase correspondences. 2 Proposed Model We suppose that Japanese is the source language and English is the target language in the description of our model. Note that the model is not specialized for this language pair, and it can be applied to any language pair. Because our model uses dependency tree structures, both source and target sentences are parsed beforehand. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). MSTparser (McDonald et al., 2005) is used to convert English sentences. Figure 1 shows an example of dependency structures. The root of a tree is placed at the extreme left and words are placed from top to bottom. 2.1 Overview This section outlines our proposed model in comparison to the IBM models, which are the conventional statistical alignment models. In the IBM models (Brown et al., 1993), the best alignment ˆ a between a given source sentence f and its target sentence e is acquired by the following equation: ˆ = argmax p(f , a|e) a a = argmax p(f |e, a) · p(a|e) a (1) 11 ཷ (accept) ග A"
W09-2302,N03-1017,0,0.0740256,"Missing"
W09-2302,N06-1014,0,0.0253971,"positions in a sentence. Finally, the proposed model can find the best ˆ by not using f -to-e alone, but simultaalignment a neously with e-to-f . That is, Equation 1 is modified as follows: ˆ = argmax p(f |e, a) · p(a|e) · a a p(e|f , a) · p(a|f ) (4) Since our model regards a phrase as a basic unit, the above formula is calculated in a straightforward way. In contrast, the IBM models can consider a many-to-one alignment by combining one-to-one alignments, but they cannot consider a one-to-many or many-to-many alignment. The models are estimated by EM-like algorithm which is very similar to (Liang et al., 2006). The important difference is that we are using tree structures. We maximize the data likelihood: ∑ max (log pef (f , e; θef ) + log pf e (f , e; θf e )) θef ,θf e f ,e (5) In the E-step, we compute the posterior distribution of the alignments with the current parameter θ: q(a; f , e) := pef (a|f , e; θef ) · pf e (a|f , e; θf e ) (6) In the M-step, we update the parameter θ: θ0 := argmax θ ∑ q(a; f , e) log pef (a, f , e; θef ) a,f ,e + ∑ q(a; f , e) log pf e (a, f , e; θf e ) a,f ,e = argmax θ ∑ a,f ,e + ∑ a,f ,e q(a; f , e) log p(e) · pef (a, f |e; θef ) q(a; f , e) log p(f ) · pf e (a, e|f"
W09-2302,H05-1066,0,0.0162692,"rrespondences are grown into phrase correspondences. 2 Proposed Model We suppose that Japanese is the source language and English is the target language in the description of our model. Note that the model is not specialized for this language pair, and it can be applied to any language pair. Because our model uses dependency tree structures, both source and target sentences are parsed beforehand. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). MSTparser (McDonald et al., 2005) is used to convert English sentences. Figure 1 shows an example of dependency structures. The root of a tree is placed at the extreme left and words are placed from top to bottom. 2.1 Overview This section outlines our proposed model in comparison to the IBM models, which are the conventional statistical alignment models. In the IBM models (Brown et al., 1993), the best alignment ˆ a between a given source sentence f and its target sentence e is acquired by the following equation: ˆ = argmax p(f , a|e) a a = argmax p(f |e, a) · p(a|e) a (1) 11 ཷ (accept) ග A (light) ⣲Ꮚ photogate (device) 䛻 is"
W09-2302,W01-1406,0,0.0181239,"he other is that the method needs to have the capability of generating phrase correspondences, that is, one-to-many or many-to-many word correspondences. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discr"
W09-2302,2008.amta-papers.15,1,0.523133,"da and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohashi (2008) also proposed a model focusing on the dependency relations. Their model has the constraint that content words can only correspond to content words on the other side, and the same applies for function words. This sometimes leads to an incorrect alignment. We have removed this constraint to make more flexible alignments possible. Moreover, in their model, some function words are brought together, and thus they cannot handle the situation where each function word corresponds to a different part. The smallest unit of our model is a single word, which should solve this problem. Proceedings of SSST"
W09-2302,J03-1002,0,0.0107511,"ග A (ni) 䛿 (ha) ཷ ཷ (accept) (light) photodetector ⏝䛔䛯 . 䚹 the 䜢 photodetector . 䚹 Figure 2: An example of hill-climbing. 4 Experimental Results We conducted alignment experiments. A JST1 Japanese-English paper abstract corpus consisting of 1M parallel sentences was used for the model training. This corpus was constructed from a 2M Japanese-English paper abstract corpus by NICT2 using the method of Uchiyama and Isahara (2007). As gold-standard data, we used 475 sentence pairs which were annotated by hand. The annotations were only sure (S) alignments (there were no possible (P ) alignments) (Och and Ney, 2003). The unit of evaluation was word-base for both Japanese and English. We used precision, recall, and F-measure as evaluation criteria. We conducted two experiments to reveal 1) the contribution of our proposed model compared to the existing models, and 2) the effectiveness of using dependency tree structure and phrases, which are larger alignment units than words. Trainings were run on the original forms of words for both the proposed model and the models used for comparison. 4.1 Comparison with Word Sequential Model For comparison, we used GIZA++ (Och and Ney, 2003) which implements the promi"
W09-2302,P05-1034,0,0.0606573,"Missing"
W09-2302,2007.mtsummit-papers.63,0,0.258947,"Missing"
W09-2302,C00-2131,1,0.765909,"difference in word order. The other is that the method needs to have the capability of generating phrase correspondences, that is, one-to-many or many-to-many word correspondences. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tre"
W09-2302,P01-1067,0,0.0712088,"nces. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohas"
W09-2302,J93-2003,0,\N,Missing
W09-2302,2001.mtsummit-ebmt.4,0,\N,Missing
W09-2908,E99-1023,0,0.0385225,"ral labels. In an SE-algorithm (Sekine et al., 1998), S is assigned to NE composed of one morpheme, B, I, E is assigned to the beginning, middle, end of NE, respectively, and O is assigned to the morpheme that is not an NE2 . The labels S, B, I, and E are prepared for each NE classes, and thus the total number of labels is 33 (= 8 * 4 + 1). The model for the label estimation is learned based on machine learning. The following features are generally utilized: characters, type of 2 Related Work 2 Besides, there are IOB1, IOB2 algorithm using only I,O,B and IOE1, IOE2 algorithm using only I,O,E (Kim and Veenstra, 1999). In Japanese Named Entity Recognition, the definition of Named Entity in IREX Workshop (IREX 56 final output analysis direction Habu PERSON 0.111 Habu-Yoshiharu PERSON 0.438 Yoshiharu MONEY 0.075 Habu-YoshiharuMeijin ORGANIZATION 0.083 Habu PERSON 0.111 Habu-Yoshiharu PERSON 0.438 Yoshiharu Yoshiharu-Meijin MONEY 0.075 OTHERe 0.092 Habu-Yoshiharu + Meijin PSN+OTHERe 0.438+0.245 Yoshiharu + Meijin MNY+OTHERe 0.075+0.245 Meijin Meijin OTHERe 0.245 OTHERe 0.245 (a):initial state (b):final output Figure 2: An overview of our proposed method. (the bunsetsu “Habu-Yoshiharu-Meijin”) Then, the best l"
W09-2908,N03-1002,0,0.0756675,"Missing"
W09-2908,I08-2080,1,0.891158,"ed to be S-PERSON (PERSON consisting of one morpheme) by utilizing the surrounding information such as the suffix “san” (Mr.) and the verb “kikoku shita” (return home). On the other hand, in sentence (2), when the label of “shinyou” (credit) is recognized to be B-ORGANIZATION (the beginning of ORGANIZATION), only information from “hatsudou” (invoke) to “kyusai” (relief) can be utilized, and thus the information of the morpheme “ginkou” (bank) that is apart from “shinyou” by three morphemes cannot be utilized. To cope with this problem, Nakano et al. (Nakano and Hirai, 2004) and Sasano et al. (Sasano and Kurohashi, 2008) utilized information of the head of bunsetsu1 . In their methods, when the label of “shinyou” is recognized, the information of the morpheme “ginkou” can be utilized. However, these methods do not work when the morpheme that we want to refer to is not a head of bunsetsu as in sentence (3). In this example, when “gaikoku” (foreign) is recognized to be BARTIFACT (the beginning of ARTIFACT), we want to refer to “hou” (law), not “ihan” (violation), which is the head of the bunsetsu. This paper proposes Japanese bottom-up named This paper proposes Japanese bottom-up named entity recognition using"
W09-2908,W98-1120,0,0.0383443,"1999) is usually used. In this definition, NEs are classified into eight classes: PERSON, LOCATION, ORGANIZATION, ARTIFACT, DATE, TIME, MONEY, and PERCENT. Table 1 shows example instances of each class. NER methods are divided into two approaches: rule-based approach and machine learning approach. According to previous work, machine learning approach achieved better performance than rule-based approach. In general, a machine learning method is formalized as a sequential labeling problem. This problem is first assigning each token (character or morpheme) to several labels. In an SE-algorithm (Sekine et al., 1998), S is assigned to NE composed of one morpheme, B, I, E is assigned to the beginning, middle, end of NE, respectively, and O is assigned to the morpheme that is not an NE2 . The labels S, B, I, and E are prepared for each NE classes, and thus the total number of labels is 33 (= 8 * 4 + 1). The model for the label estimation is learned based on machine learning. The following features are generally utilized: characters, type of 2 Related Work 2 Besides, there are IOB1, IOB2 algorithm using only I,O,B and IOE1, IOE2 algorithm using only I,O,E (Kim and Veenstra, 1999). In Japanese Named Entity Re"
W09-2908,D07-1032,1,0.826126,"Missing"
W09-2908,P06-1141,0,\N,Missing
W09-2908,P08-1047,0,\N,Missing
W09-3817,P06-1105,0,0.022166,"r instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Kawahara and Kurohashi, 2006b). While Abekawa and Okumura (2006) used only cooccurrence statistics of verbal arguments, Kawahara and Kurohashi (2006b) assessed predicateargument structures by checking case frames, which are semantic frames that are automatically compiled for each predicate sense from a large raw corpus. These methods outperformed the accuracy of supervised dependency parsers. In such linguistically-motivated approaches, well-formedness within a clause was considered, but coherence between clauses was not considered. Even if intra-clause relations (i.e., a predicate-argument"
W09-3817,W06-2920,0,0.0186736,"at are consistent within each clause and between clauses. We conﬁrm that this method contributes to the improvement of dependency parsing of Japanese. 1 Introduction The approaches of dependency parsing basically assess the likelihood of a dependency relation between two words or phrases and subsequently collect all the assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, 108 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108–116, c Paris, October 2009. 2009 Association for Computational Linguistics ( a 1 ( ) p o i n t o  w a a 2 ( ) o i n t"
W09-3817,E06-1011,0,0.0324589,"Missing"
W09-3817,D07-1101,0,0.0538155,"Missing"
W09-3817,D07-1100,0,0.0263215,"Missing"
W09-3817,C04-1010,0,0.0118624,"quently collect all the assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, 108 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108–116, c Paris, October 2009. 2009 Association for Computational Linguistics ( a 1 ( ) p o i n t o  w a a 2 ( ) o i n t  T 3 ) , p p a O o P i p o n t i n o t   w a T , O p o i p P o n t i n o  t  w a T , O P h h i t o t s u  n h i i o n e  D A T o m a o t r g o m a e n i t z t o t n e s u   D n i A i o T e m a e p t a c o m e t t o t n e s u   D n i A T e m a k o t r g o m a e n i t z e 3 3 3 3 3 3 3 3"
W09-3817,W04-3205,0,0.44609,"tion, ... mail, post, postage, ... .. . woman, ... baggage, supply, goods, ... person, Japan, parental house, ... mail, post, courier, ... .. . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we use the following abbreviations: NOM (nominative), ACC (accusative), ABL (ablative), CMI ("
W09-3817,P06-1015,0,0.0784278,"2 .. . systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, 1999) projects, such knowledge has been obtained manually, but it is difﬁcult to manually develop broad-coverage knowledge that is sufﬁcient for practical use in NLP applications. On the other hand, the automatic acquisition of such inference knowledge from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordina"
W09-3817,N06-1007,0,0.103357,"I, ... mail, message, information, ... friend, address, direction, ... mail, post, postage, ... .. . woman, ... baggage, supply, goods, ... person, Japan, parental house, ... mail, post, courier, ... .. . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we use the foll"
W09-3817,P02-1006,0,0.0289784,"edge between Case Frames 12 12 .. . systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, 1999) projects, such knowledge has been obtained manually, but it is difﬁcult to manually develop broad-coverage knowledge that is sufﬁcient for practical use in NLP applications. On the other hand, the automatic acquisition of such inference knowledge from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted en"
W09-3817,kawahara-kurohashi-2006-case,1,0.391505,"8) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Kawahara and Kurohashi, 2006b). While Abekawa and Okumura (2006) used only cooccurrence statistics of verbal arguments, Kawahara and Kurohashi (2006b) assessed predicateargument structures by checking case frames, which are semantic frames that are automatically compiled for each predicate sense from a large raw corpus. These methods outperformed the accuracy of supervised dependency parsers. In such linguistically-motivated approaches, well-formedness within a clause was considered, but coherence between clauses was not considered. Even if intra-clause relations (i.e., a predicate-argument structure within a clause) are"
W09-3817,C04-1002,0,0.023783,"assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, 108 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108–116, c Paris, October 2009. 2009 Association for Computational Linguistics ( a 1 ( ) p o i n t o  w a a 2 ( ) o i n t  T 3 ) , p p a O o P i p o n t i n o t   w a T , O p o i p P o n t i n o  t  w a T , O P h h i t o t s u  n h i i o n e  D A T o m a o t r g o m a e n i t z t o t n e s u   D n i A i o T e m a e p t a c o m e t t o t n e s u   D n i A T e m a k o t r g o m a e n i t z e 3 3 3 3 3 3 3 3 3 e h h t a k u"
W09-3817,N06-1023,1,0.323178,"8) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Kawahara and Kurohashi, 2006b). While Abekawa and Okumura (2006) used only cooccurrence statistics of verbal arguments, Kawahara and Kurohashi (2006b) assessed predicateargument structures by checking case frames, which are semantic frames that are automatically compiled for each predicate sense from a large raw corpus. These methods outperformed the accuracy of supervised dependency parsers. In such linguistically-motivated approaches, well-formedness within a clause was considered, but coherence between clauses was not considered. Even if intra-clause relations (i.e., a predicate-argument structure within a clause) are"
W09-3817,P08-1068,0,0.0155249,"ld and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difﬁcult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufﬁciently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Ka"
W09-3817,C08-1107,0,0.0124509,"ed much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordinated verb pairs and noun-verb co-occurrences. Pekar (2006) also collected entailment knowledge with discourse structure constraints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of ﬁve semantic relations by using a search engine. Inui et al. (2005) classiﬁed"
W09-3817,W02-2016,0,0.040659,"words or phrases and subsequently collect all the assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, 108 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108–116, c Paris, October 2009. 2009 Association for Computational Linguistics ( a 1 ( ) p o i n t o  w a a 2 ( ) o i n t  T 3 ) , p p a O o P i p o n t i n o t   w a T , O p o i p P o n t i n o  t  w a T , O P h h i t o t s u  n h i i o n e  D A T o m a o t r g o m a e n i t z t o t n e s u   D n i A i o T e m a e p t a c o m e t t o t n e s u   D n i A T e m a k o t r g o m a e n"
W09-3817,N06-1008,0,0.0536864,"... .. . person, I, ... mail, message, information, ... friend, address, direction, ... mail, post, postage, ... .. . woman, ... baggage, supply, goods, ... person, Japan, parental house, ... mail, post, courier, ... .. . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we"
W09-3817,J94-4001,1,0.60798,"Missing"
W09-3817,P06-1107,0,0.0442754,"message, information, ... friend, address, direction, ... mail, post, postage, ... .. . woman, ... baggage, supply, goods, ... person, Japan, parental house, ... mail, post, courier, ... .. . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we use the following abbreviations: NOM"
W09-3817,P98-2127,0,0.0136007,"ll remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 1 In this paper, we use the following abbreviations: NOM (nominative), ACC (accusative), ABL (ablative), CMI (comitative) and TOP (topic marker). 110 wo tsumu” (load baggage) and “busshi-wo tsumu” (load supply) are similar, they are clustered together. The similarity is measured by using a distributional thesaurus based on the study described in Lin (1998). In this section, we ﬁrst describe our unit of transition knowledge, case frames, brieﬂy. We then detail the acquisition method of the transition knowledge, and report experimental results. Finally, we refer to related work to the acquisition of such knowledge. 3.2 Acquisition of Transition Knowledge from Large Corpus 3.1 The Unit of Transition Knowledge: Case Frames To acquire the transition knowledge, we collect the clause pairs in a large raw corpus that have a dependency relation and represent them as pairs of case frames. For example, from the following sentence, a case frame pair, (mato"
W09-3817,J93-2004,0,0.0317551,", 619-0289, Japan dk@nict.go.jp Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp Abstract i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difﬁcult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufﬁciently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbit"
W09-3817,J04-4004,0,\N,Missing
W09-3817,J03-4003,0,\N,Missing
W09-3817,C98-2122,0,\N,Missing
W09-3817,I08-1065,0,\N,Missing
W09-3817,D07-1096,0,\N,Missing
W10-3902,I08-2080,1,0.893285,"Missing"
W10-3902,I08-2110,1,0.834465,"&lt;/TOPIC&gt; &lt;EXPOSITION&gt;: [ આ (explain)|ॻ (write) | هड़ (describe) |( ࡌهmention) |( هwrite down) |ड़Δ (express)][Δ (do)]? [(Δ (be) |Δ (be) |ΕΔ (reru)|ΒΕΔ (rareru)]?; &lt;DOC&gt;: [Σϒ (Web)| (Web)]? [จॻ (document)|ϖʔδ (page)| (homepage) |ใ (information)|จষ (sentences)|ςΩετ (text)]; &lt;PREDICATE&gt;: [Δ (know)| (look for)| ௐΔ (ﬁnd)|ݟΔ (watch)|ݟΔ (ﬁnd out)| ಡΉ (read)][ (tai)|Δ (iru)]; Figure 4: Example of a search topic. ument collecting and document scoring. In both steps, the proposed method considered synonyms automatically extracted from ordinary dictionaries and Web pages (Shibata et al., 2008). For calculating the scores, we selected the value of 0.2 as the parameter β. This value was estimated using the dry-run data set of NTCIR-3. For each topic, we retrieved 1,000 documents and then assessed search performance according to MRR, P@10, R-prec, MAP, DCGN (Jarvelin and Kekalainen, 2002), and QMeasure (QM) (Sakai, 2004). We calculated these scores for each topic then averaged them. Note that unjudged documents were treated as irrelevant when computing the scores. As the graded relevance for DCGN and QM, we mapped highly relevant, relevant and partially relevant to 3, 2 and 1, respect"
W10-3902,shinzato-etal-2008-large,1,0.880934,"proposed method retrieves them by dependency relations in phrases. Therefore, the proposed method does not need to adjust window size, and naturally performs document retrieval based on noun phrases by using dependency relations. Linguistically motivated IR research pointed out that dependency relations did not contribute to signiﬁcantly improving performance due to low accuracy and robustness of syntactic parsers (Jones, 1999). Current state-of-the-art parsers, however, can perform high accuracy for real-world sentences. Therefore, dependency relations are remarked in IR (Miyao et al., 2006; Shinzato et al., 2008b). For instance, Miyao et al. (Miyao et al., 2006) proposed an IR system for a biomedical domain that performs deep linguistic analysis on a query and each document. Their system represented relations between words by a predicate-argument structure, and used ontological databases for handling synonyms. Their experiments using a small number of short queries showed that their proposed system signiﬁcantly improved search performance versus a system not performing deep linguistic analysis. Shinzato et al. (Shinzato et al., 2008b) proposed a Web search system that handles not only words but also"
W10-3902,P06-1128,0,0.0173836,"rds in phrases, the proposed method retrieves them by dependency relations in phrases. Therefore, the proposed method does not need to adjust window size, and naturally performs document retrieval based on noun phrases by using dependency relations. Linguistically motivated IR research pointed out that dependency relations did not contribute to signiﬁcantly improving performance due to low accuracy and robustness of syntactic parsers (Jones, 1999). Current state-of-the-art parsers, however, can perform high accuracy for real-world sentences. Therefore, dependency relations are remarked in IR (Miyao et al., 2006; Shinzato et al., 2008b). For instance, Miyao et al. (Miyao et al., 2006) proposed an IR system for a biomedical domain that performs deep linguistic analysis on a query and each document. Their system represented relations between words by a predicate-argument structure, and used ontological databases for handling synonyms. Their experiments using a small number of short queries showed that their proposed system signiﬁcantly improved search performance versus a system not performing deep linguistic analysis. Shinzato et al. (Shinzato et al., 2008b) proposed a Web search system that handles n"
W10-3902,I08-1025,1,0.913061,"proposed method retrieves them by dependency relations in phrases. Therefore, the proposed method does not need to adjust window size, and naturally performs document retrieval based on noun phrases by using dependency relations. Linguistically motivated IR research pointed out that dependency relations did not contribute to signiﬁcantly improving performance due to low accuracy and robustness of syntactic parsers (Jones, 1999). Current state-of-the-art parsers, however, can perform high accuracy for real-world sentences. Therefore, dependency relations are remarked in IR (Miyao et al., 2006; Shinzato et al., 2008b). For instance, Miyao et al. (Miyao et al., 2006) proposed an IR system for a biomedical domain that performs deep linguistic analysis on a query and each document. Their system represented relations between words by a predicate-argument structure, and used ontological databases for handling synonyms. Their experiments using a small number of short queries showed that their proposed system signiﬁcantly improved search performance versus a system not performing deep linguistic analysis. Shinzato et al. (Shinzato et al., 2008b) proposed a Web search system that handles not only words but also"
W10-3903,C04-1057,0,0.0444115,"Missing"
W10-3903,W00-0405,0,0.0438307,"document may belong to several topics. 1 2 http://clusty.com/ http://www.nist.gov/tac/ 12 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12–20, Beijing, August 2010 In the summarization process, two types of redundancies need to be addressed. First, each topic summary should not contain any redundancy. We refer to this problem as redundancy within a summary. This problem is well known in the field of multi-document summarization (Mani, 2001) and several methods have been proposed to solve it, such as Maximum Marginal Relevance (MMR) (Goldstein et al., 2000) (Mori et al., 2004), using Integer Linear Programming (ILP) (Filatova and Hatzivassiloglou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document sum"
W10-3903,N09-1041,0,0.0272123,"2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document summarization. Some methods have been proposed to generate topic summaries from documents (Radev and Fan, 2000) (Haghighi and Vanderwende, 2009), but to the best of our knowledge, the redundancy between summaries has not yet been addressed in any study. In this paper, we focus on the document clustering process and the reduction of redundancy between summaries in the summarization process. Furthermore, we propose a method using PLSI (Hofmann, 1999) to summarize search results. In the proposed method, we employ PLSI to estimate the membership degree of each document to each topic, and then classify the search results into topics using this information. In the same way, we employ PLSI to estimate the membership degree of each keyword to"
W10-3903,W04-1013,0,0.00576527,"ubjects’ ability for recognizing topics; that is, our method was able to estimate a reliable membership degree p(z|d). Thus, it seems that our method using p(z|d) is able to classify search results into topics to some extent. 3.5 Degree of Reduction in Redundancy between Summaries Third, we investigated how well the proposed method reduced the redundancy between summaries. To be more precise, we used three measures as w score(z, w) to generate summaries and investigated which measure generated the least redundant summaries. Generally, methods for reducing redundancy are evaluated using ROUGE (Lin, 2004), BE (Hovy et al., 2005), or Pyramid (Nenkova and Passonneau, 2004). However, the use of these methods require that ideal summaries are created by humans, and this was not possible for the same reason as mentioned previously. Thus, we did not perform a direct evaluation using the methods such as ROUGE, but instead evaluated how well our method performed in reducing redundancy between summaries using the membership degree p(z|w) as w score(z, w). The evaluation process was as follows. We used three measures as w score(z, w), and generated three sets of summaries. # subjects 5 16 15 6 6 # subjec"
W10-3903,W01-0100,0,0.208767,"summarized individually. Note that a method for soft clustering should be employed in this process, as one document may belong to several topics. 1 2 http://clusty.com/ http://www.nist.gov/tac/ 12 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12–20, Beijing, August 2010 In the summarization process, two types of redundancies need to be addressed. First, each topic summary should not contain any redundancy. We refer to this problem as redundancy within a summary. This problem is well known in the field of multi-document summarization (Mani, 2001) and several methods have been proposed to solve it, such as Maximum Marginal Relevance (MMR) (Goldstein et al., 2000) (Mori et al., 2004), using Integer Linear Programming (ILP) (Filatova and Hatzivassiloglou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measure"
W10-3903,C04-1063,0,0.0749806,"Missing"
W10-3903,N04-1019,0,0.0781497,"Missing"
W10-3903,W00-1110,0,0.0167853,"lou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document summarization. Some methods have been proposed to generate topic summaries from documents (Radev and Fan, 2000) (Haghighi and Vanderwende, 2009), but to the best of our knowledge, the redundancy between summaries has not yet been addressed in any study. In this paper, we focus on the document clustering process and the reduction of redundancy between summaries in the summarization process. Furthermore, we propose a method using PLSI (Hofmann, 1999) to summarize search results. In the proposed method, we employ PLSI to estimate the membership degree of each document to each topic, and then classify the search results into topics using this information. In the same way, we employ PLSI to estimate the mem"
W10-3903,shinzato-etal-2008-large,1,0.888049,"Missing"
W10-3903,I08-1025,1,0.824317,"Missing"
W10-3903,E09-1089,0,0.0185139,"dancy between summaries. The evaluation results show that our method performs well in classifying search results and successfully reduces the redundancy between summaries. Sz D Dz Sz’ W Dz’ Figure 1: Overview of the proposed method. 2 Proposed Method 2.1 Overview Figure 1 gives an overview of the proposed method, which comprises the following four steps. Step 1. Acquisition of Search Results Using a search engine, obtain the search results for a given query. Step 2. Keyword Extraction Extract the keywords related to the query from the search results using the method proposed by Shibata et al. (2009). Step 3. Document Clustering Estimate the membership degree of each document to each topic using PLSI, and classify the search results into topics. Step 4. Summarization For each topic, generate a summary by extracting the important sentences specific to each topic from each document cluster. In the following subsections, we describe each step in detail. 2.2 Step 1. Acquisition of Search Results First, we obtain the search results for a given query using a search engine. To be more precise, we obtain the top N 0 documents of the search engine results. Next, we remove those documents that shou"
W12-5210,J93-2003,0,0.152829,"d by using a diﬀerent distortion model for the keywords and the words within each keyword. In particular, we show that our modiﬁed version of the HMM alignment algorithm of (Vogel et al., 1996) perform better than both the original HMM algorithm and the commonly used word-aligner GIZA++ for the keyword alignment task. 2 Related works If one has a large amount of technical documents in the relevant domain that are written in the two languages of interest, one can of course extract bilingual term pairs by running one of the many word-alignment algorithm that have already been proposed (such as (Brown et al., 1993) or (Vogel et al., 1996)). However, such bilingual collection of documents are not easily obtained. Besides, the word-alignment algorithms will not by themselves indicate which set of words represent a technical term. We will focus here on approaches that do not require such large bilingual collection of technical documents. (Lin et al., 2008) reported experiments of extracting term pairs from part of document which contains parentheses. It is based on the fact that the English translation of a technical term is sometimes written inside a parenthesis next to it. Their method involves building"
W12-5210,N06-1014,0,0.0275331,"e sequence can be controlled easily by applying the following rules to the state transition model: 1) if current state is einew then only transition to eicont is allowed; 2) if current state is eicont , then transition to any eicont or EOK is allowed; 3) if current state is EOK , then only transition to einew is allowed. For state emission model, the following rule are applied: visible state f j.EOK of foreign keyword f j can only be emitted by hidden state ei.EOK . 3.5 Variants The natural way of ﬁnding the best alignment according to a trained HMM model is to use Viterbi decoding. However, (Liang et al., 2006) reports that one can get better results by computing the expected probability of each link and keeping those above a certain threshold. We can apply this idea here, although in our case we will want to use the expected probability of two keyword being aligned (easily computed from the expected probabilities of the word links). 89 Our HMM model is, like the original one, a 1-to-n alignment model. As is often done in such case, we can align each keyword list twice, with each language taken alternatively as the source language. We then combine the resulting alignments by intersecting their set o"
W12-5210,P08-1113,0,0.0251726,"a large amount of technical documents in the relevant domain that are written in the two languages of interest, one can of course extract bilingual term pairs by running one of the many word-alignment algorithm that have already been proposed (such as (Brown et al., 1993) or (Vogel et al., 1996)). However, such bilingual collection of documents are not easily obtained. Besides, the word-alignment algorithms will not by themselves indicate which set of words represent a technical term. We will focus here on approaches that do not require such large bilingual collection of technical documents. (Lin et al., 2008) reported experiments of extracting term pairs from part of document which contains parentheses. It is based on the fact that the English translation of a technical term is sometimes written inside a parenthesis next to it. Their method involves building a kind of parallel corpus by extracting the English terms in parenthesis and the words appearing before the parentheses, ﬁltering the non-translation words, and then aligning the English terms with an unsupervised word alignment method. (Nagata et al., 2001) extracted term pairs not only from parentheses, but also from glossaries and parallel"
W12-5210,W01-1413,0,0.0357744,"n approaches that do not require such large bilingual collection of technical documents. (Lin et al., 2008) reported experiments of extracting term pairs from part of document which contains parentheses. It is based on the fact that the English translation of a technical term is sometimes written inside a parenthesis next to it. Their method involves building a kind of parallel corpus by extracting the English terms in parenthesis and the words appearing before the parentheses, ﬁltering the non-translation words, and then aligning the English terms with an unsupervised word alignment method. (Nagata et al., 2001) extracted term pairs not only from parentheses, but also from glossaries and parallel paragraph. In their works, term pairs are ﬁltered out if they are unlikely translation of each other. The likeliness is calculated based on their locations. Foreign and English terms which are appearing close to each other are considered to have higher translation likeliness. Their work is focused on extracting technical term pairs from Japanese documents. (Ren et al., 2010) extracted bilingual technical term pairs from keyword section of abstracts of Chinese research papers. In their method, Chinese and Eng"
W12-5210,J03-1002,0,0.00652354,"here are a total of 807 Japanese keywords in the test set. 2 http://ci.nii.ac.jp 90 4.2 Baseline and Setup Three diﬀerent methods are used for baseline. As the ﬁrst baseline, keywords are aligned monotonically in a way similar to (Ren et al., 2010) except we did not do the partial translation alignment as we want to consider unsupervised methods with no seed dictionary available. This baseline is called mono-all when we apply it to all keyword list, and mono-same when we apply it only to keyword list with the same length. For the second baseline, alignment is done using GIZA++ alignment tool (Och and Ney, 2003) with its default setting. Alignment was done both way then merged by intersection3 . With this method we conduct two experiments: using word (wGIZA-i) and keyword (kGIZA-i) as the alignment unit. When using word as the alignment unit, a keyword links is created for each word link. The wGIZA-i approach will tend to produce more links and possibly many-to-many keyword alignments, resulting in higher recall but lower precision than the kGIZA-i approach. For the third baseline, standard HMM (sHMM) is used for the alignment, with keyword as the alignment unit. For the implementation of our constra"
W12-5210,C96-2141,0,0.678978,"en done to extract automatically such bilingual dictionaries from technical documents. This paper will try to further this research, in particular by following a previously proposed idea of aligning the keywords list of technical and scientiﬁc documents. We propose an approach to adapt existing word-alignment algorithm to the task of keyword-list alignment. This is done by enforcing constraints on the keyword boundaries and by using a diﬀerent distortion model for the keywords and the words within each keyword. In particular, we show that our modiﬁed version of the HMM alignment algorithm of (Vogel et al., 1996) perform better than both the original HMM algorithm and the commonly used word-aligner GIZA++ for the keyword alignment task. 2 Related works If one has a large amount of technical documents in the relevant domain that are written in the two languages of interest, one can of course extract bilingual term pairs by running one of the many word-alignment algorithm that have already been proposed (such as (Brown et al., 1993) or (Vogel et al., 1996)). However, such bilingual collection of documents are not easily obtained. Besides, the word-alignment algorithms will not by themselves indicate whi"
W13-2505,C04-1151,0,0.794908,", 606-8501, Japan {chu,nakazawa}@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp Abstract Non–parallel corpora include various levels of comparability: noisy parallel, comparable and quasi–comparable. Noisy parallel corpora contain non–aligned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is signi"
W13-2505,I05-1059,0,0.0349896,"n Hanzi and Kanji. Table 1 gives some examples of common Chinese characters in Traditional Chinese, Simplified Chinese and Japanese with their Unicode. Since Chinese characters contain significant semantic information, and common Chinese characters share the same meaning, they can be valuable linguistic clues for many Chinese–Japanese NLP tasks. Many studies have exploited common Chinese characters. Tan et al. (1995) used the occurrence of identical common Chinese characters in Chinese and Japanese (e.g. “snow” in Table 1) in automatic sentence alignment task for document– level aligned text. Goh et al. (2005) detected common Chinese characters where Kanji are identical to Traditional Chinese, but different from Simplified Chinese (e.g. “love” in Table 1). Using a Chinese encoding converter1 that can convert Traditional Chinese into Simplified Chinese, they built a Japanese–Simplified Chinese dictionary partly using direct conversion of Japanese into Chinese for Japanese Kanji words. Chu et al. (2011) made use of the Unihan database2 to detect common Chinese characters which are visual variants of each other (e.g. “begin” in Table 1), and proved the effectiveness of common Chinese characters in Chi"
W13-2505,P07-2045,0,0.00752983,"rable corpora. We adopt a system proposed by Munteanu and Marcu (2005), which is for parallel sentence extraction from comparable corpora. We extend the system in several aspects to make it even suitable for quasi–comparable corpora. The core component of the system is a classifier which can identify parallel sentences from non–parallel sentences. Previous method of classifier training by the Cartesian product is not practical, because it differs from the real process of parallel sentence extraction. We propose a novel Introduction In statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2007), the quality and quantity of the parallel sentences are crucial, because translation knowledge is acquired from a sentence–level aligned parallel corpus. However, except for a few language pairs, such as English–French, English–Arabic and English– Chinese, parallel corpora remain a scarce resource. The cost of manual construction for parallel corpora is high. As non–parallel corpora are far more available, constructing parallel corpora from non–parallel corpora is an attractive research field. 34 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 34–42, c Sofia, B"
W13-2505,N03-2016,0,0.0538577,"Missing"
W13-2505,W06-2810,0,0.0821037,"Missing"
W13-2505,2011.mtsummit-papers.53,1,0.83952,"Missing"
W13-2505,J05-4003,0,0.130834,"lel corpora contain non–aligned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conduct"
W13-2505,2012.eamt-1.7,1,0.810272,"training and testing method is that features from the IR results can be used. Here, we use the ranks of the retrieved documents returned by the IR framework as feature. 4 4.2.1 Settings • Probabilistic dictionary: We took the top 5 translations with translation probability larger than 0.1 created from the parallel corpus. Experiments We conducted classification and translation experiments to evaluate the effectiveness of our proposed parallel sentence extraction system. • IR tool: Indri7 with the top 10 results. • Segmenter: For Chinese, we used a segmenter optimized for Chinese–Japanese SMT (Chu et al., 2012a). For Japanese, we used JUMAN (Kurohashi et al., 1994). 4.1 Data 4.1.1 Parallel Corpus The parallel corpus we used is a scientific paper abstract corpus provided by JST3 and NICT4 . This corpus was created by the Japanese project “Development and Research of Chinese– Japanese Natural Language Processing Technology”, containing various domains such as chemistry, physics, biology and agriculture etc. This corpus is aligned in both sentence–level and document–level, containing 680k sentences and 100k articles. • Alignment: GIZA++8 . • SMT: We used the state–of–the–art phrase– based SMT toolkit"
W13-2505,P06-1011,0,0.0243421,"ank (Proposed)”, where parallel subsentential fragments are in bold. We investigated the alignment results of the extracted sentences. We found that most of the parallel subsentential fragments were correctly aligned with the help of the parallel sentences in the baseline system. Therefore, translation performance was improved by appending the extracted sentences. However, it also led to many wrong alignments among the non– parallel fragments which are harmful to translation. In the future, we plan to further extract these parallel subsentential fragments, which can be more effective for SMT (Munteanu and Marcu, 2006). 6 Conclusion and Future Work In this paper, we proposed a novel method of classifier training and testing that simulates the real parallel sentence extraction process. Furthermore, we used linguistic knowledge of Chinese character features. Experimental results of parallel sentence extraction from quasi–comparable corpora indicated that our proposed system performs significantly better than the previous study. 5 Related Work As parallel sentences trend to appear in similar document pairs, many studies first conduct document matching, then identify the parallel sen40 Our approach can be impro"
W13-2505,chu-etal-2012-chinese,1,0.833746,"training and testing method is that features from the IR results can be used. Here, we use the ranks of the retrieved documents returned by the IR framework as feature. 4 4.2.1 Settings • Probabilistic dictionary: We took the top 5 translations with translation probability larger than 0.1 created from the parallel corpus. Experiments We conducted classification and translation experiments to evaluate the effectiveness of our proposed parallel sentence extraction system. • IR tool: Indri7 with the top 10 results. • Segmenter: For Chinese, we used a segmenter optimized for Chinese–Japanese SMT (Chu et al., 2012a). For Japanese, we used JUMAN (Kurohashi et al., 1994). 4.1 Data 4.1.1 Parallel Corpus The parallel corpus we used is a scientific paper abstract corpus provided by JST3 and NICT4 . This corpus was created by the Japanese project “Development and Research of Chinese– Japanese Natural Language Processing Technology”, containing various domains such as chemistry, physics, biology and agriculture etc. This corpus is aligned in both sentence–level and document–level, containing 680k sentences and 100k articles. • Alignment: GIZA++8 . • SMT: We used the state–of–the–art phrase– based SMT toolkit"
W13-2505,N10-1063,0,0.489733,"anslated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. We extract Chinese–Japanese parallel sentences from quasi–comparable corpora, which are"
W13-2505,2012.eamt-1.37,0,0.0343286,"Missing"
W13-2505,P09-2057,0,0.767195,"igned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting"
W13-2505,P03-1010,0,0.55095,"asi–comparable. Noisy parallel corpora contain non–aligned sentences that are nevertheless mostly bilingual translations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many"
W13-2505,2007.mtsummit-papers.63,0,0.0421083,"lations of the same document, comparable corpora contain non–sentence– aligned, non–translated bilingual documents that are topic–aligned, while quasi–comparable corpora contain far more disparate very–non–parallel bilingual documents that could either be on the same topic (in–topic) or not (out–topic) (Fung and Cheung, 2004). Most studies focus on extracting parallel sentences from noisy parallel corpora or comparable corpora, such as bilingual news articles (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Munteanu and Marcu, 2005; Tillmann, 2009; Abdul-Rauf and Schwenk, 2011), patent data (Utiyama and Isahara, 2007; Lu et al., 2010) and Wikipedia (Adafre and de Rijke, 2006; Smith et al., 2010). Few studies have been conducted on quasi–comparable corpora. Quasi–comparable corpora are available in far larger quantities than noisy parallel or comparable corpora, while the parallel sentence extraction task is significantly more difficult. Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. We extra"
W13-2505,I05-1023,0,0.0313432,"with two different approaches: binary classification (Munteanu and Marcu, 2005; Tillmann, 2009; Smith et al., 2010; S¸tefˇanescu et al., 2012) and translation similarity measures (Utiyama and Isahara, 2003; Fung and Cheung, 2004; Abdul-Rauf and Schwenk, 2011). We adopt the binary classification approach with a novel classifier training and testing method and Chinese character features. Few studies have been conducted for extracting parallel sentences from quasi–comparable corpora. We are aware of only two previous efforts. Fung and Cheung (2004) proposed a multi-level bootstrapping approach. Wu and Fung (2005) exploited generic bracketing Inversion Transduction Grammars (ITG) for this task. Our approach differs from the previous studies that we extend the approach for comparable corpora in several aspects to make it work well for quasi–comparable corpora. Table 4: BLEU scores for Chinese–to–Japanese translation experiments (“†” and “‡” denotes the result is better than “Munteanu+ 2005 (Cartesian)” significantly at p &lt; 0.05 and p &lt; 0.01 respectively, “*” denotes the result is better than “Baseline” significantly at p &lt; 0.01). 4.3.3 Discussion The translation results indicate that compared to the pre"
W13-2505,J93-2003,0,\N,Missing
W13-5714,candito-etal-2010-statistical,0,0.0627122,"Missing"
W13-5714,W11-3801,0,0.234471,"tep enables arguments for the verb to be correctly dependent on the main verb and the main verb to be dependent on the auxiliary verb in the ambiguous annotation scheme in the SJTree.1 Figure 1 shows the original SJTree phrase structure and its corresponding converted representation in dependency grammars. 3 Parsing Model Our parsing model gives a probability to each possible dependency tree T for a sentence S = e1 , e2 , ..., en , where ei is a Korean word. The model finally selects the dependency tree T ∗ that maximizes P (T |S) as follows: T ∗ = arg max P (T |S). (1) T 1 (Oh and Cha, 2010; Choi and Palmer, 2011) also introduced an conversion algorithm of dependency grammars for the SJTree. (Choi and Palmer, 2011) proposed head percolation rules for the SJTREE. However, we found some errors such as S related rules, where it gives lower priority to S than VP. It would fail to assign a head node correctly for S → VP S. Moreover, they did not consider auxiliary verb constructions annotated as VP in the SJTREE. According to their head rules, arguments for the main verb are dependent on the auxiliary verb instead of the main verb because of the annotation of the corpus (in general, VP → VP VP where the for"
W13-5714,P99-1062,1,0.536574,"inbu has 3 occurrences. 3 common = 0 if either i1 or i2 is not included in CoreNet. P ratioci = P p |ex | p |ey | j=1 i=1 (8) where i is the number of the occurrences of the instance examples of the same case marker and j is the number of the occurrences of the instance examples of the all case marker. 5.2 Constructing the database of N1 ui N2 structures We also integrate lexical information on Korean noun phrases of the form N1 ui N2 , which roughly corresponds to N2 of N1 in English. Even though Korean genitive marker ui does not have a broad usage as much as no in Japanese as described in (Kurohashi and Sakai, 1999), it sometime does not modify the immediate constituent such as Kyungjiui meylonhyang binwuleul (‘Melon-flavored soap of Kyungji’) where Kyungjiui modifies binwuleul instead of meylonhyang. The N1 ui N2 structure is very useful to recognize the meaning of natural language text can improve head-modifier relationships between genitive nouns. 6 Experiment and Results 6.1 Parsing results We use the Sejong Treebank corpus (SJTree) in our experiment.4 We use standard dataset split for training, development and testing. We report here final evaluation results on the baseline unlexicalized parsing and"
W13-5714,W12-3411,1,0.423407,"Missing"
W13-5714,W02-2207,0,0.597908,"Missing"
W13-5714,W10-1406,0,0.243484,"Missing"
W13-5714,N06-1023,1,0.788493,"ng. P (T |S) is defined as the product of probabilities as follows: P (T |S) = Y P (Epa , dist|eh ), (2) Epa ∈T where Epa represents a clause dominated by a predicate or a genitive nominal phrase, eh represents the head Korean word of Epa , and dist is the distance between Epa and eh . Instead of specifying the actual distance, it is classified into three bins: 1, 2 – 5, and 6 –. If the dependent Korean word appears right next to the head, the distance is 1. If it appears between 2 and 5, the distance is 2. If it appears past 6, the distance is 6. P (T |S) is calculated in the similar way as (Kawahara and Kurohashi, 2006a). We describe the outline of this model below. Each probability in equation (2) is decomposed into two ways according to the type of Epa . If Epa is a clause dominated by a predicate, it is decomposed into a predicate-argument structure (content part) P Am and a function part fm . eh is also decomposed into a content part ch and a function part fh . P (Epa , dist|eh ) = P (P Am , fm , dist|ch , fh ) = P (P Am |fm , dist, ch , fh ) × P (fm , dist|ch , fh ) ≈ P (P Am |fm , ch ) × P (fm , dist|fh ) (3) The first term in the last equation represents a fullylexicalized generative probability of t"
W13-5714,kawahara-kurohashi-2006-case,1,0.778624,"ng. P (T |S) is defined as the product of probabilities as follows: P (T |S) = Y P (Epa , dist|eh ), (2) Epa ∈T where Epa represents a clause dominated by a predicate or a genitive nominal phrase, eh represents the head Korean word of Epa , and dist is the distance between Epa and eh . Instead of specifying the actual distance, it is classified into three bins: 1, 2 – 5, and 6 –. If the dependent Korean word appears right next to the head, the distance is 1. If it appears between 2 and 5, the distance is 2. If it appears past 6, the distance is 6. P (T |S) is calculated in the similar way as (Kawahara and Kurohashi, 2006a). We describe the outline of this model below. Each probability in equation (2) is decomposed into two ways according to the type of Epa . If Epa is a clause dominated by a predicate, it is decomposed into a predicate-argument structure (content part) P Am and a function part fm . eh is also decomposed into a content part ch and a function part fh . P (Epa , dist|eh ) = P (P Am , fm , dist|ch , fh ) = P (P Am |fm , dist, ch , fh ) × P (fm , dist|ch , fh ) ≈ P (P Am |fm , ch ) × P (fm , dist|fh ) (3) The first term in the last equation represents a fullylexicalized generative probability of t"
W14-7001,W14-7005,0,0.0335781,"Missing"
W14-7001,W14-7011,1,0.826327,"Missing"
W14-7001,P11-2093,0,0.053248,"h language. The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-based SMT We used Berkeley parser to obtain target language syntax. We used the following Moses’ configuration for the string-to-tree syntax-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring 5 6 http://nlp.stanford.edu/software/segmenter.shtml 3 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html 4 Figure 1: The submission web page for participants For Japanese segmentation we use three different tools, which are Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 7 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0 8 . For Chinese segmentation we use two different tools, which are KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 9 (Tseng, 2005). For English segmentation we use tokenizer.perl 10 in the Moses toolkit. The detailed procedures for the automatic evaluation are shown at WAT2014 evaluation web page 11 . after WAT2014. Everybody can use the system by registering on the registration web page 12 . 5 Human Evaluat"
W14-7001,W14-7007,0,0.033389,"Missing"
W14-7001,W14-7002,0,0.0428991,"ieved better quality WEBLIO-EJ1 1 in the automatic evaluation, howthan RBMT system. ever it is much worse in the human evaluation. According to the descriptions of the two submissions, • The translation quality of the widely used the difference of the two is whether it uses the forsystems was Phrase-based SMT < Hierarchiest input or not. It is natural that using the forcal PBSMT < Syntax-based SMT (S2T and est input improves the translation quality, thus we T2S). conducted the human evaluation of WEBLIO-EJ1 • Forest-to-String Syntax-based SMT system 2 compared to WEBLIO-EJ1 1, which means we (Neubig, 2014) achieved the best quality for used WEBLIO-EJ1 1 as the baseline for the huall the translation directions. man evaluation. The HUMAN score was 2.50 ± 4.17 which Statistical Significance Testing between means there is no significant difference between Submissions the two, and this result is far from the results of Tables 5, 6, 7 and 8 show the results of statistical the official results. Actually, taking the confidence significance testings of JE, EJ, JC and CJ transintervals into consideration, this conclusion can be lations respectively where all the pairs of submisderived under some probabil"
W14-7001,P13-2121,0,0.018775,"Missing"
W14-7001,W14-7006,0,0.0412465,"Missing"
W14-7001,2009.iwslt-papers.4,0,0.110246,"ine system at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, five commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and translating using the systems were published on the WAT 2014 web page3 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 2. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Since our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system description of these systems are anonymized in this paper. We describe the d"
W14-7001,P02-1040,0,0.11454,"Words. • distortion-limit = 20 • msd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuring The default values were used for the other system parameters. The default values were used for the other system parameters. 3.4 Hierarchical Phrase-based SMT 4 Automatic Evaluation We used the following Moses’ configuration for the hierarchical phrase-based SMT system. 4.1 Procedure of Calculating Automatic Evaluation Score • max-chart-span = 1000 • Phrase score option: GoodTuring We calculated automatic evaluation scores of the translation results applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated with multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated with RIBES.py version 1.02.4 6 . All scores of each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results have been tokenized with word segmentation tools on each language. The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-based SMT We used Berkeley parser to obtain target language syntax. We used the following Moses’ c"
W14-7001,P06-1055,0,0.0857644,"we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, five commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and translating using the systems were published on the WAT 2014 web page3 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 2. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Since our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system description of these systems are anonymized in this paper. We describe the detail of the baseline SMT systems. 2.2 ASPEC-JC ASPEC-JC is a parallel corpus consisting of J"
W14-7001,W14-7008,0,0.0763585,"Missing"
W14-7001,D10-1092,0,0.117188,"sd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuring The default values were used for the other system parameters. The default values were used for the other system parameters. 3.4 Hierarchical Phrase-based SMT 4 Automatic Evaluation We used the following Moses’ configuration for the hierarchical phrase-based SMT system. 4.1 Procedure of Calculating Automatic Evaluation Score • max-chart-span = 1000 • Phrase score option: GoodTuring We calculated automatic evaluation scores of the translation results applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated with multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated with RIBES.py version 1.02.4 6 . All scores of each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results have been tokenized with word segmentation tools on each language. The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-based SMT We used Berkeley parser to obtain target language syntax. We used the following Moses’ configuration for the string-to-tr"
W14-7001,W14-7012,1,0.854354,"Missing"
W14-7001,P07-2045,0,0.0113934,"ilarity score. baseline system at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, five commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and translating using the systems were published on the WAT 2014 web page3 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 2. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Since our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system description of these systems are anonymized in this pap"
W14-7001,W14-7003,0,0.0378915,"Missing"
W14-7001,W04-3250,0,0.570886,"Missing"
W14-7001,2007.mtsummit-papers.63,0,0.0465476,"submit translation results at any time. 2.1 ASPEC-JE The training data of ASPEC-JE was constructed by the NICT from approximately 2 million Japanese-English scientific paper abstracts owned by the JST. Because the paper abstracts are kind • Domain and language pairs WAT is the world’s first workshop that uses 1 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 1 Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 1‒19, Tokyo, Japan, 4th October 2014. 2014 Copyright is held by the author(s). of comparable corpora, the sentence correspondences are automatically found using the method of (Utiyama and Isahara, 2007). Each sentence pair is accompanied with the similarity score and the field symbol. The similarity scores are calculated by the method of (Utiyama and Isahara, 2007). The field symbols are single letters AZ and show the scientific field of each document2 . The correspondance between the symbols and field names, along with the frequency and occurance ratios for the training data, are given in the README of ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from Japanese-English paper abstracts owned by JST that are not contained in the training data"
W14-7001,W14-7010,0,0.0441404,"Missing"
W14-7001,W14-7004,0,0.0675856,"Missing"
W14-7001,W15-5003,0,\N,Missing
W14-7001,W15-5009,0,\N,Missing
W14-7001,W15-5007,0,\N,Missing
W14-7001,W14-7009,0,\N,Missing
W14-7001,W15-5008,0,\N,Missing
W14-7001,W15-5012,0,\N,Missing
W14-7001,W15-5010,0,\N,Missing
W14-7001,W15-5013,0,\N,Missing
W14-7001,W15-5005,0,\N,Missing
W14-7012,P05-1022,0,0.0795328,"Missing"
W14-7012,D11-1047,1,0.840049,"trict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in (Cromières and Kurohashi, 2011) to perform this step as efficiently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from 1. We project the part of s that is matched into the target side t using the alignment of s and t. This is trivial if each word of s and t is aligned, but this is not typically the case. Therefore our translation hypotheses will often have some target words/nodes marked as optionals: this means that we will decide if they should be added to the final translation only at the moment of combination. 2. We insert the"
W14-7012,N06-1023,1,0.85285,"Missing"
W14-7012,Y12-1033,1,0.90464,"Missing"
W14-7012,D07-1104,0,0.0445887,"Missing"
W14-7012,P05-1032,0,0.0996159,"Missing"
W14-7012,P05-1034,0,0.142807,"Missing"
W14-7012,N12-1047,0,0.0343404,"earch space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. 6 Reranking We reranked the n-best output of our system using an additional two language models: a standard 7-gram language model with Modified Kneser-Ney smoothing and a Recurrent Neural Network Language Model (RNNLM) (Mikolov et. al, 2010). The RNNLM model was trained with hidden layer size 200, and 5000 sentences from the training fold were used as validation data. Reranking was conducted by first calculating the various language model scores for each 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation"
W14-7012,J07-2003,0,0.0674616,"er of features and create a linear model scoring each possible combination of hypotheses (see Section 5). We then attempt to find the combination that maximizes this model score. The combination of rules is constrained by the structure of the input dependency tree. If we only consider local features1 , then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. 6 Reranking We reranked the n-best output of our sys"
W14-7012,2011.iwslt-evaluation.24,0,0.0249287,"Missing"
W14-7012,D12-1107,0,0.0121569,"sider local features1 , then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. 6 Reranking We reranked the n-best output of our system using an additional two language models: a standard 7-gram language model with Modified Kneser-Ney smoothing and a Recurrent Neural Network Language Model (RNNLM) (Mikolov et. al, 2010). The RNNLM model was trained with hidden layer size 200, and 5000 sentences from the training fold"
W14-7012,W08-0402,0,0.017326,"is constrained by the structure of the input dependency tree. If we only consider local features1 , then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity2 . However, non-local features (such as language models) will force us to prune the search space. This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations(Heafield et al., 2012). • Example penalty and example size • Translation probability • Language model score • Optional words added/removed The optimal weights for each feature are estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. 6 Reranking We reranked the n-best output of our system using an additional two language models: a standard 7-gram language model with Modified Kneser-Ney smoothing and a Recurrent Neural Network Language Model (RNNLM) (Mikolov et. al, 2010). The RNNLM model was tra"
W14-7012,C12-1120,1,\N,Missing
W14-7012,D14-1063,1,\N,Missing
W14-7012,W11-2123,0,\N,Missing
W15-0813,P09-4001,1,0.792485,"ntradictory Event Pairs As mentioned above, the recognition of contradictory event pairs is related to RTE. In some RTE tasks, contradiction is one of the relations between text and hypothesis. For example, Harabagiu et al. (2006) proposed a method to recognize contradictions between texts using negation expressions, antonyms, and discourse analysis. Recognition of contradictory event pairs plays an important role in the systems that detect contradictions between information extracted from web texts. For example, there are several systems that detect contradictory information, such as WISDOM (Akamine et al., 2009), Statement Map (Murakami et al., 2009), and Dispute Finder (Ennals et al., 2010). 2.2 Acquisition of Contradictory Event Pairs Hashimoto et al. (2012) and Kloetzer et al. (2013) proposed methods for acquiring contradictory event pairs. Hashimoto et al. (2012) collected Japanese contradictory and consistent event pairs using templates of semantic polarities that indicate excitatory, inhibitory, and neutral properties. A template consists of a particle and a predicate, such as “を (particle) 破壊する (destroy)” and “を (particle) 進行させ る (develop).” They collected one million contradictory event pairs"
W15-0813,D12-1057,0,0.0743454,"Missing"
W15-0813,izumi-etal-2014-constructing,1,0.777391,"rs simultaneously, contradictions of such pairs have a strong relation with negation, such as ⟨having a meal, not having a meal⟩ and ⟨eating to excess, eating moderately⟩. There are also contradictory event pairs based on sibling relations, such as ⟨being in Tokyo, being in Kyoto⟩, where “Tokyo” and “Kyoto” have a sibling relation. We therefore classify negation and sibling relations into binary (e.g., “single” and “married”), discrete (e.g., “Tokyo” and “Kyoto”), and continuous (e.g., “expensive” and “cheap”). Furthermore, negation has the following two classes that can cause contradictions (Izumi et al., 2014): sequential event relations, such as “getting on” and “getting off,” and counterpart perspective relations, such as “selling” and “buying.” We added these classes to our taxonomy. The subclasses of simultaneous contradictions are detailed below. 1-a. binary When an event pair includes mutually exclusive antonyms (e.g., “single” and “married”) or a predicate and its negation (e.g., “going” and “not going”), these events are contradictory. We call such contradictory event pairs binary. 1-b. discrete When an event pair consists of predicates or arguments that have sibling relations, such as ⟨bei"
W15-0813,D13-1065,0,\N,Missing
W15-1701,D11-1145,1,0.689984,"tion. They judged whether the tweet was posted just after an earthquake using a support vector machine (SVM), and determined the seismic center from the formatted tweets. In addition, they developed a system that raises the alarm about an earthquake from the predicted results. Bollen et al. (2011) extracted the social mood, and predicted the stock price fluctuation N days from the day of observation by using evaluated data of the ’mood-related’ dictionary. As a result, they concluded that they could show the 3 days from the ’calm-mood’ day might be able to predict the stock price fluctuation. Aramaki et al. (2011) predicted an influenza epidemic from tweets. They showed the possibility of information extraction from the tweets that reflects the actual world’s situation by using language processing technologies. Boyd et al. (2010) examined a practice of retweeting as a way by which participants can be “in a conversation.” Paul and Dredze (2011) considered a broader range of public health applications for Twitter and showed quantitative correlations with public health data and qualitative evaluations of model output. Baldwin et al. (2013) explored how linguistically noisy or otherwise it is over a range"
W15-1701,I13-1041,0,0.0132289,"lm-mood’ day might be able to predict the stock price fluctuation. Aramaki et al. (2011) predicted an influenza epidemic from tweets. They showed the possibility of information extraction from the tweets that reflects the actual world’s situation by using language processing technologies. Boyd et al. (2010) examined a practice of retweeting as a way by which participants can be “in a conversation.” Paul and Dredze (2011) considered a broader range of public health applications for Twitter and showed quantitative correlations with public health data and qualitative evaluations of model output. Baldwin et al. (2013) explored how linguistically noisy or otherwise it is over a range of social media sources empirically over popular social media text types, in the form of YouTube comments, Twitter posts, web user forum posts, blog posts and Wikipedia. Yin et al. (2012) constructed a system architecture for leveraging social media to enhance emergency situation awareness with high-speed text streams retrieved from Twitter during natural disasters and crises. In these researches, the location of an SNS document plays an important role in extracting informa3 tion, and in most cases, rely on GPS function connect"
W15-1701,C12-1064,0,0.0329781,"he IP address of past content cannot be accessed, and this approach is becoming increasingly ineffective with the increased use of portable terminals. As a result, location name disambiguation should now focus on procedures that consider the original text. As information references, Web pages and change logs in Wikipedia have been used as the basis of location name disambiguation. These resources are homogeneous and manageable. In contrast, the numerous data on SNS often contain noise, which makes disambiguation unmanageable. A number of studies have investigated location name disambiguation. Han et al. (2012) extracted location-indicative words from tweet data by calculating the information gain ratios. Their paper states that the words improved the estimation performance of the users’ location. They concluded that the procedure requires relatively little memory, is fast, and could potentially be used by lexicographers to extract location-indicative words. Backstrom et al. (2008) developed a probabilistic framework to quantify the spatial variation manifested in search queries. This allowed them to obtain a measure of spatial dispersion that indicates regional information. Adams and Janowicz (2012"
W15-1701,P13-4002,0,0.0143475,"GPS information. 1 “I arrived at Prefectural Office Ave. from Shuri Station!” Introduction As the volume of documents on the Web increases, technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services (SNS) such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document. However, many previous studies on SNS rely only on geo-tagged documents (e.g., (Han et al., 2013; Han et al., 2014)), which include GPS information, In this paper, we propose a method that identifies the locations of location expressions in Twitter tweets on the basis of the following two clues: (1) spatial proximity, and (2) temporal consistency. Spatial proximity assumes that all locations mentioned in a tweet are close to one another. In the above document, for example, we would assume that “Prefectural Office Ave.” is “Prefectural Office Ave. (Okinawa)” using the proximity between “Shuri Station” and “Prefectural Office Ave. (Okinawa)” The other clue is temporal consistency, 1 Semioc"
W15-3119,Y01-1001,0,0.637464,"an event and its participants (i.e., predicates and arguments such as agent, patient, locative, temporal and manner). Unlike syntactic level surface cases (i.e., dependency labels such as subject and object), semantic roles can be regarded as a deep case representation for predicates. Because of its ability to abstract the meaning of a sentence, SRL has been applied to many NLP applications, including information extraction (Christensen et al., 2010), question answering (Pizzato and Moll´a, 2008) and machine translation (Liu and Gildea, 2010). Semantically annotated corpora, such as FrameNet (Fillmore et al., 2001) and PropBank (Kingsbury and Palmer, 2002), make this type of automatic semantic structure analysis feasible by using supervised machine learning methods. Automatic SRL processing has two major drawbacks: 120 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 120–127, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing word similarity learned from unlabeled data as additional features for SRL. Word embeddings have also been used in several NLP tasks including SRL (Collober"
W15-3119,jin-etal-2014-framework,1,0.45784,"dency parsing and word similarity calculation, to multilingual applications, such as machine translation. Japanese case frames have been successfully compiled (Kawahara and Kurohashi, 2006), where each argument is represented as its case marker in Japanese such as ‘ga’, ‘wo’, and ‘ni’. For the case frames of other languages such as English and Chinese, because there are no such case markers that can help clarify syntactic structures, instead of using case markers like in Japanese, syntactic surface cases (i.e., subject, object, prepositional phrase, etc.) are used for argument representation (Jin et al., 2014). Case frames can be automatically acquired using a different method such as Chinese Restaurant Process (CRP) (Kawahara et al., 2014) for different languages. In our work, we employ such syntactic level knowledge, which use surface cases as argument representation, to help SRL task. We refer to this kind of knowledge as syntactic knowledge in this paper. Section 2 contains related work. Section 3 describes the high-quality dependency selection process. Section 4.1 presents a detailed description of our approach, conducted on three languages, along with the results followed by a discussion in S"
W15-3119,H01-1043,1,0.737508,"Missing"
W15-3119,N06-1023,1,0.942083,"onal preferences to improve SRL. This study is similar to our approaches but the quality of selectional preferences was not concerned at all. In syntactic level of NLP, rich knowledge such as predicate-argument structures and case frames are strong backups for various kinds of tasks. A case frame, which clarifies relations between a predicate and its arguments, can support tasks ranging from fundamental analysis, such as syntactic dependency parsing and word similarity calculation, to multilingual applications, such as machine translation. Japanese case frames have been successfully compiled (Kawahara and Kurohashi, 2006), where each argument is represented as its case marker in Japanese such as ‘ga’, ‘wo’, and ‘ni’. For the case frames of other languages such as English and Chinese, because there are no such case markers that can help clarify syntactic structures, instead of using case markers like in Japanese, syntactic surface cases (i.e., subject, object, prepositional phrase, etc.) are used for argument representation (Jin et al., 2014). Case frames can be automatically acquired using a different method such as Chinese Restaurant Process (CRP) (Kawahara et al., 2014) for different languages. In our work,"
W15-3119,W09-1206,0,0.263213,"Missing"
W15-3119,kawahara-kurohashi-2010-acquiring,1,0.835992,"to indicate the proposed features. 3.2 Syntactic knowledge acquisition We constructed two types of syntactic knowledge namely, predicate-argument structures and case frames. set feature of the children of predicate 3.2.1 High-quality predicate-argument structure extraction the concatenation of the dependency labels of predicate’s children Predicate-argument structures (PAS) have been basically acquired from syntactic analyses which varies from phrase chunking to syntactic dependency parsing. For example, English PAS in surface case was acquired in a large scale using a chunking-based system (Kawahara and Kurohashi, 2010). Some phenomena in Chinese, such as omission and complex grammar, make it intractable to automatically extract PAS only using shallow syntactic analysis, such as chunking. Syntactic dependency parsing is applied for Chinese PAS extraction. Arguments are represented by their syntactic dependency labels (i.e., subject, object, etc.) Due to various factors, Chinese syntactic dependency parsing is relatively worse in performance compared to that of English, Japanese, etc. However, using an existing treebank, it is possible to train a classifier to acquire high-quality PAS by only using highly rel"
W15-3119,W10-0907,0,0.320263,"tract a proposition from a sentence about who does what to whom, when, where and why. By using semantic roles, the complex expression of a sentence is then interpreted as an event and its participants (i.e., predicates and arguments such as agent, patient, locative, temporal and manner). Unlike syntactic level surface cases (i.e., dependency labels such as subject and object), semantic roles can be regarded as a deep case representation for predicates. Because of its ability to abstract the meaning of a sentence, SRL has been applied to many NLP applications, including information extraction (Christensen et al., 2010), question answering (Pizzato and Moll´a, 2008) and machine translation (Liu and Gildea, 2010). Semantically annotated corpora, such as FrameNet (Fillmore et al., 2001) and PropBank (Kingsbury and Palmer, 2002), make this type of automatic semantic structure analysis feasible by using supervised machine learning methods. Automatic SRL processing has two major drawbacks: 120 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 120–127, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language"
W15-3119,E14-1007,1,0.770777,"ave been successfully compiled (Kawahara and Kurohashi, 2006), where each argument is represented as its case marker in Japanese such as ‘ga’, ‘wo’, and ‘ni’. For the case frames of other languages such as English and Chinese, because there are no such case markers that can help clarify syntactic structures, instead of using case markers like in Japanese, syntactic surface cases (i.e., subject, object, prepositional phrase, etc.) are used for argument representation (Jin et al., 2014). Case frames can be automatically acquired using a different method such as Chinese Restaurant Process (CRP) (Kawahara et al., 2014) for different languages. In our work, we employ such syntactic level knowledge, which use surface cases as argument representation, to help SRL task. We refer to this kind of knowledge as syntactic knowledge in this paper. Section 2 contains related work. Section 3 describes the high-quality dependency selection process. Section 4.1 presents a detailed description of our approach, conducted on three languages, along with the results followed by a discussion in Section 4.2. Finally, Section 5 contains our conclusions and future work. 2 Related work The CoNLL-2009 shared task (Hajiˇc et al., 20"
W15-3119,D09-1003,0,0.267902,"Missing"
W15-3119,P08-1068,0,0.192546,"Missing"
W15-3119,C10-1081,0,0.156181,"antic roles, the complex expression of a sentence is then interpreted as an event and its participants (i.e., predicates and arguments such as agent, patient, locative, temporal and manner). Unlike syntactic level surface cases (i.e., dependency labels such as subject and object), semantic roles can be regarded as a deep case representation for predicates. Because of its ability to abstract the meaning of a sentence, SRL has been applied to many NLP applications, including information extraction (Christensen et al., 2010), question answering (Pizzato and Moll´a, 2008) and machine translation (Liu and Gildea, 2010). Semantically annotated corpora, such as FrameNet (Fillmore et al., 2001) and PropBank (Kingsbury and Palmer, 2002), make this type of automatic semantic structure analysis feasible by using supervised machine learning methods. Automatic SRL processing has two major drawbacks: 120 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 120–127, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing word similarity learned from unlabeled data as additional features for SRL. Word e"
W15-3119,W09-1203,0,0.37053,"Missing"
W15-3119,W08-1810,0,0.385679,"Missing"
W15-3119,W09-1217,0,0.201904,"s syntactic knowledge in this paper. Section 2 contains related work. Section 3 describes the high-quality dependency selection process. Section 4.1 presents a detailed description of our approach, conducted on three languages, along with the results followed by a discussion in Section 4.2. Finally, Section 5 contains our conclusions and future work. 2 Related work The CoNLL-2009 shared task (Hajiˇc et al., 2009) features a substantial number of studies on SRL that used Propbank as one of the resources. These work can be categorized into two types: joint learning of syntactic parsing and SRL (Tang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but has a much larger search space during the joint model learning. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and"
W15-3119,D14-1041,0,0.391097,"Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but has a much larger search space during the joint model learning. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were also several studies using semisupervised methods for SRL. One basic idea of semi-supervised SRL is to automatically annotate unlabeled data using a simple classifier trained on original training data (F¨urstenau and Lapata, 2009). Since there is a substantial amount of error propagation in SRL frameworks, the additional automatic semantic roles are not guaranteed to be of good quality. Contrary to this approach, we only rely on syntactic level knowledge which does not suffer too much from error propagation. Also, some studies assume that sentences that are syntactically and lexic"
W15-3119,P09-2019,0,0.477444,"Missing"
W15-3119,W09-1208,0,0.236766,"ins our conclusions and future work. 2 Related work The CoNLL-2009 shared task (Hajiˇc et al., 2009) features a substantial number of studies on SRL that used Propbank as one of the resources. These work can be categorized into two types: joint learning of syntactic parsing and SRL (Tang et al., 2009; Morante et al., 2009), which learns a unique model for syntactic parsing and SRL jointly. This type of framework has the ability to use SRL information in syntactic parsing for improvement, but has a much larger search space during the joint model learning. The other type is called SRLonly task (Zhao et al., 2009; Bj¨orkelund et al., 2009), which uses automatic morphological and syntactic information as the input in order to judge which token plays what kind of semantic role. Our work focuses on the second category of SRL. Our framework is based on those used by Bj¨orkelund et al. (2009) and Yang and Zong (2014). There were also several studies using semisupervised methods for SRL. One basic idea of semi-supervised SRL is to automatically annotate unlabeled data using a simple classifier trained on original training data (F¨urstenau and Lapata, 2009). Since there is a substantial amount of error propa"
W15-3119,kingsbury-palmer-2002-treebank,0,\N,Missing
W15-5001,W15-5008,0,0.0448879,"Missing"
W15-5001,W15-5013,0,0.0370801,"Missing"
W15-5001,W15-5002,0,0.0289439,"Missing"
W15-5001,W14-7001,1,0.205867,"ve been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 • Evaluation method Evaluation is done both automatically and manually. For human evaluation, WAT uses crowdsourcing, which is low cost and allows multiple evaluations, as the first-stage evaluation. Also, JPO adequacy evaluation is conducted for the selected submissions according to the crowdsourcing evaluation results. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshop WAT2014(Nakazawa et al., 2014), WAT2015 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 2nd WAT, we adopt new translation subtasks “Chinese-to-Japanese and Koreanto-Japanese patent translation” in addition to the subtasks that were conducted in WAT2014. WAT is unique for the following reasons: 2 Dataset WAT uses the Asian Scientific Paper Excerpt Corpus (ASPEC)1 and JPO Patent Corpus (JPC) 2 as the dataset. 2.1 ASPEC ASPEC is constructed by t"
W15-5001,P11-2093,1,0.853461,"on results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005) 8 9 • Subtask: – Scientific papers subtask (J ↔ E, J ↔ C); – Patents subtask (C → J, K → J); • Method (SMT, RBMT, SMT and RBMT, EBMT, Other); 10 http://code.google.com/p/mecab/downloads/detail? name=mecab-ipadic-2.7.0-20070801.tar.gz 11 http://nlp.stanford.edu/software/segmenter.shtml 12 https://bitbucket.org/eunjeon/mecab-ko/ 13 https://github.com/moses-smt/mosesdecoder/tree/ RELEASE-2.1.1/scripts/tokenizer/tokenizer.perl 14 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html http://w"
W15-5001,P13-2121,0,0.0884737,"Missing"
W15-5001,2009.iwslt-papers.4,0,0.108842,"em as that at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To obtain word alignment"
W15-5001,W15-5003,1,0.888058,"Missing"
W15-5001,D10-1092,0,0.23015,"2 Automatic Evaluation System The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results. Figure 1 shows the submission interface for participants. The system requires participants to provide the following information when they upload translation results: Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005) 8 9 • Subtask: – Scientific paper"
W15-5001,P02-1040,0,0.109103,"the other system parameters. 4 4.2 Automatic Evaluation System The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results. Figure 1 shows the submission interface for participants. The system requires participants to provide the following information when they upload translation results: Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005)"
W15-5001,P06-1055,0,0.112959,"we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To obtain word alignments, GIZA++ and growdiag-final-and heuristics were used. We used 5gram language models with mod"
W15-5001,P07-2045,0,0.0208005,"ich is the same system as that at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To"
W15-5001,W15-5006,1,0.742862,"Missing"
W15-5001,W04-3250,0,0.568022,"Missing"
W15-5001,W15-5010,0,0.0468531,"Missing"
W15-5001,W15-5005,0,0.0362327,"Missing"
W15-5001,W15-5012,0,0.0480478,"Missing"
W15-5001,W15-5009,0,0.0622163,"Missing"
W15-5001,2007.mtsummit-papers.63,0,0.08182,"2015. 2015 Copyright is held by the author(s). LangPair ASPEC-JE ASPEC-JC Train 3,008,500 672,315 Dev 1,790 2,090 DevTest 1,784 2,148 Test 1,812 2,107 LangPair JPC-CJ JPC-KJ Table 1: Statistics for ASPEC. Train 1,000,000 1,000,000 Dev 2,000 2,000 DevTest 2,000 2,000 Test 2,000 2,000 Table 2: Statistics for JPC. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by the NICT from approximately 2 million Japanese-English scientific paper abstracts owned by the JST. Because the abstracts are comparable corpora, the sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score and the field symbol. The similarity scores are calculated by the method from (Utiyama and Isahara, 2007). The field symbols are single letters A-Z and show the scientific field for each document3 . The correspondence between the symbols and field names, along with the frequency and occurrence ratios for the training data, are given in the README file from ASPECJE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts owned by JST that are not contained in the tr"
W15-5001,W15-5011,0,0.0304094,"Missing"
W15-5001,W15-5007,0,0.153198,"Missing"
W15-5006,P05-1022,0,0.129705,"Missing"
W15-5006,N12-1047,0,0.0279359,"2014), which makes use of our dependency parses in order to capture non-local reorderings. 3.2 • Forest parse scores • Number of content/function words aligned to content/function words • Number of times a subtree is inserted in a position (left or right of parent) that is not the most common in the training data • Number of examples sharing the same information used to create an initial hypothesis • Similarity between source and input word embeddings (Mikolov et al., 2013) Forest Input The optimal weights for each feature are as before estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. We found that the quality of the source-side dependency parsing had an important impact 3 http://kheaﬁeld.com/code/kenlm/ 57 3.4 Reranking translation in the n-best list. These features were added to those used in the ﬁrst round of tuning, then one ﬁnal iteration of tuning was run. The tuning algorithm and settings were the same as for standard tuning. This retuning step was added in order to ﬁnd an optimal combination of the additional features with related features such as sentence length and the score given by the 5-gram language model used inside the decoder. A ﬁnal rer"
W15-5006,J07-2003,0,0.0620462,"X2, morphological forms of “be”, and the optional insertion of “at”. simple if i, s and t have the same structure and are perfectly aligned, but again this is not typically the case. A consequence is that we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the"
W15-5006,D11-1047,1,0.850442,"view Figure 1 shows the basic structure of the KyotoEBMT translation pipeline. The training process begins with parsing and aligning parallel sentences from the training corpus. The alignments are then used to build an example database (‘translation mem54 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 54‒60, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and sco"
W15-5006,C12-1120,1,0.883803,"Missing"
W15-5006,P14-2024,0,0.0236533,"ble combination of hypotheses. This linear model is based on a linear combination of both local features (local to each translation hypothesis) and non-local features (such as a 5-gram language model score of the ﬁnal translation). Despite our already relatively large set of dense features, we found there were a number of cases where these features were not enough to diﬀerentiate between good and bad translation hypotheses. This year we have added ten new features, now reaching a total of 52, a selection of which are shown below: Improvements from WAT2014 3.1 Alignment Based on the ﬁndings of Neubig and Duh (2014), we experimented with supervised alignment using Nile (Riesa et al., 2011) as part of our translation framework. We found that using supervised alignments made a considerable improvement to translation quality. Since Nile supports only constituency parses, we also perform constituency parsing for source and target languages for generating bidirectional word alignments. For the initial alignments for Nile, we use the alignments generated from the model described in last year’s system description (Richardson et al., 2014), which makes use of our dependency parses in order to capture non-local r"
W15-5006,W11-2123,0,0.0137398,"”, and the optional insertion of “at”. simple if i, s and t have the same structure and are perfectly aligned, but again this is not typically the case. A consequence is that we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-term"
W15-5006,N15-3009,0,0.0231441,"Missing"
W15-5006,2011.iwslt-evaluation.24,0,0.0153808,"er of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in Cromières and Kurohashi (2011) to perform this step as eﬃciently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it. A translation hypothesis is deﬁned as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence. A translation hypothesis is created from a translation example as follows: Figure 1: The translation pipeline can be roughly divided in 3 steps. Step 1 is t"
W15-5006,P06-1055,0,0.256442,"Missing"
W15-5006,D12-1107,0,0.0117256,"n-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 3). This lattice representation also allows the decoder to make choices between various morphological variations of a wo"
W15-5006,P05-1034,0,0.166664,"Missing"
W15-5006,N06-1023,1,0.726271,"Missing"
W15-5006,W08-0402,0,0.0128267,"hat we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 3). This lattice representation also allows the"
W15-5006,D11-1046,0,0.0924047,"er of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in Cromières and Kurohashi (2011) to perform this step as eﬃciently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it. A translation hypothesis is deﬁned as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence. A translation hypothesis is created from a translation example as follows: Figure 1: The translation pipeline can be roughly divided in 3 steps. Step 1 is t"
W15-5006,Y12-1033,1,0.720835,"Missing"
W15-5006,D07-1104,0,0.0236985,"g. We believe that 2 System Overview Figure 1 shows the basic structure of the KyotoEBMT translation pipeline. The training process begins with parsing and aligning parallel sentences from the training corpus. The alignments are then used to build an example database (‘translation mem54 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 54‒60, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full"
W16-1316,J15-4004,0,0.0235822,"word association game using a dialog system on smartphones as GWAP. We evaluate the quality of automatically acquired Japanese word associations based on logs obtained from game players. For example, if players correctly answer the keyword “glass” for the given associated words “fragile,” “cup,” and “reflect,” these associated words can be regarded as high quality for the keyword. We use such a game to evaluate automatically acquired word associations at no cost. 2 Related Work In recent years, crowdsourcing has been used for data construction and evaluation in NLP (e.g., (Snow et al., 2008; Hill et al., 2015; Schnabel et al., 2015)). Snow et al. (2008) demonstrated that annotations by crowdworkers have almost identical quality with those by experts in various NLP tasks. The motivation of crowdworkers in crowdsourcing is monetary. Another type of wisdom of crowds is GWAP, for which a player’s motivation differs from that of crowdsourcing. Their motivation is “enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 200"
W16-1316,P09-2053,0,0.07251,"Missing"
W16-1316,Q14-1035,0,0.0130811,"“enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 2009), paraphrasing (Poesio et al., 2013), constructing semantic network (Lafourcade, 2007), and word sense disambiguation (Venhuizen et al., 2013). However, they are designed in a text-based style. Text-based games are probably less enjoyable for players. Some studies have specifically examined nontext-based games, i.e., video games (Vannella et al., 2014; Jurgens and Navigli, 2014). Video games are familiar for ordinary people and are much more enjoyable than text-based games. For example, Van87 nella et al. (2014) developed a video game to validate the associations between images and senses. They reported that the annotation quality using the video game is better than crowdsourcing. The enjoyable game design improves the quality of annotations by crowds. However, because playing a video game requires a certain amount of time, it is a bit difficult to play it in one’s spare-time. Furthermore, developing an attractive video game would be a timeconsuming task. Our word as"
W16-1316,C14-1027,1,0.815809,"ave lower precision than manually created ones, which would harm the performance of subsequent NLP applications. To cope with potential difficulties existing both in manual methods and automatic ones, it is necessary to combine the two methods, taking their respective benefits. This paper presents a method for compiling large-scale word association knowledge of high quality by evaluating automatically acquired word associations manually, not by linguistic experts but by the wisdom of crowds. To make use of the wisdom of crowds, crowdsourcing has been employed widely (e.g., (Snow et al., 2008; Kawahara et al., 2014)). Crowdsourcing is a low-cost service that enlists numerous human workers to make judgments that are difficult for computers.3 However, costs are still high when we 2 A keyword can be a word or a phrase, but we call both “keywords.” 3 In this paper, we refer to microtask crowdsourcing as “crowdsourcing.” We present a design for acquiring word association knowledge of high quality on the basis of a game with a purpose (GWAP). We evaluate automatically acquired word associations using a word association game as a GWAP. In the word association game, a player is given a set of associated words as"
W16-1316,P98-2127,0,0.102355,"affiliated with Recruit Lifestyle Co., keyword has an association are designated as associated words. There are manually crafted resources of word associations, such as thesauri and ontologies, which have been compiled by lexicographers and which have contributed to many studies in NLP. However, it takes long times and high costs to create a large thesaurus. Furthermore, it is difficult to update it continuously to adapt to neologisms and the changing use of a word. To reduce the creation cost, methods for automatically acquiring word associations from a large corpus have been studied (e.g., (Lin, 1998; Mikolov et al., 2013)). Although such methods can acquire large-scale resources of word associations, they tend to have lower precision than manually created ones, which would harm the performance of subsequent NLP applications. To cope with potential difficulties existing both in manual methods and automatic ones, it is necessary to combine the two methods, taking their respective benefits. This paper presents a method for compiling large-scale word association knowledge of high quality by evaluating automatically acquired word associations manually, not by linguistic experts but by the wis"
W16-1316,W06-1664,0,0.0422799,"at many players can play easily. Moreover, we need not spend much time to develop it because the game system is simple. 3 Automatic Acquisition of Word Associations We first explain our model for acquiring word associations. Then, we describe a method for clustering the acquired word associations to efficiently make questions for the word association game. 3.1 Definition and Collection of Word Associations In general, many relations exist among words, such as hypernym-hyponym relations and part-whole relations. However, we do not care about the kind of relation but the strength between words. Matsuo et al. (2006) used pointwise mutual information (PMI) and chi-square as the strength measures of relations, and acquired associated words using graph clustering. Our method is based on this method, but uses word frequencies and PMI as the strength measures as proposed by Shin and Kurohashi (2014). We used a Japanese Web corpus of 4.2 million sentences and Japanese Wikipedia to acquire associated words for nouns, verbs, and adjectives. 3.2 Clustering Word Associations Next, we cluster the acquired associated words according to their basic meanings. By clustering associated words, we can not only structure a"
W16-1316,D15-1036,0,0.0160241,"ame using a dialog system on smartphones as GWAP. We evaluate the quality of automatically acquired Japanese word associations based on logs obtained from game players. For example, if players correctly answer the keyword “glass” for the given associated words “fragile,” “cup,” and “reflect,” these associated words can be regarded as high quality for the keyword. We use such a game to evaluate automatically acquired word associations at no cost. 2 Related Work In recent years, crowdsourcing has been used for data construction and evaluation in NLP (e.g., (Snow et al., 2008; Hill et al., 2015; Schnabel et al., 2015)). Snow et al. (2008) demonstrated that annotations by crowdworkers have almost identical quality with those by experts in various NLP tasks. The motivation of crowdworkers in crowdsourcing is monetary. Another type of wisdom of crowds is GWAP, for which a player’s motivation differs from that of crowdsourcing. Their motivation is “enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 2009), paraphrasing (Poesio"
W16-1316,D08-1027,0,0.0179698,"Missing"
W16-1316,P14-1122,0,0.116193,"g. Their motivation is “enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 2009), paraphrasing (Poesio et al., 2013), constructing semantic network (Lafourcade, 2007), and word sense disambiguation (Venhuizen et al., 2013). However, they are designed in a text-based style. Text-based games are probably less enjoyable for players. Some studies have specifically examined nontext-based games, i.e., video games (Vannella et al., 2014; Jurgens and Navigli, 2014). Video games are familiar for ordinary people and are much more enjoyable than text-based games. For example, Van87 nella et al. (2014) developed a video game to validate the associations between images and senses. They reported that the annotation quality using the video game is better than crowdsourcing. The enjoyable game design improves the quality of annotations by crowds. However, because playing a video game requires a certain amount of time, it is a bit difficult to play it in one’s spare-time. Furthermore, developing an attractive video game would be a tim"
W16-1316,W13-0215,0,0.0221177,"with those by experts in various NLP tasks. The motivation of crowdworkers in crowdsourcing is monetary. Another type of wisdom of crowds is GWAP, for which a player’s motivation differs from that of crowdsourcing. Their motivation is “enjoying the game.” Therefore, we need not pay for the players, and can reduce the number of low-quality or dishonest workers. Many approaches using GWAP have been proposed in the field of NLP, such as anaphora resolution (Hladk´a et al., 2009), paraphrasing (Poesio et al., 2013), constructing semantic network (Lafourcade, 2007), and word sense disambiguation (Venhuizen et al., 2013). However, they are designed in a text-based style. Text-based games are probably less enjoyable for players. Some studies have specifically examined nontext-based games, i.e., video games (Vannella et al., 2014; Jurgens and Navigli, 2014). Video games are familiar for ordinary people and are much more enjoyable than text-based games. For example, Van87 nella et al. (2014) developed a video game to validate the associations between images and senses. They reported that the annotation quality using the video game is better than crowdsourcing. The enjoyable game design improves the quality of an"
W16-1316,C98-2122,0,\N,Missing
W16-2201,P06-1121,0,0.0156779,"arser, to improve the isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on"
W16-2201,P05-1033,0,0.0890526,"from the language side that has a high quality parser, to the side that has a low quality parser, to improve the isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source an"
W16-2201,W02-1001,0,0.122912,"ady have a partially projected LQ subtree. Next, we want to project a new dependency in the HQ tree to the LQ side. If adding this newly projected dependency to the partially projected subtree leads to non-projectivity,2 we give up this projection and leave the dependency as null. Many alignment errors can be detected by the property of projectivity. For example, in Figure F ∈Y The score function for each factor is denoted as the inner product of a feature and a weight vector: score(F, X) = w · f (F, X) (3) The weight vector can be learnt by e.g., the averaged structured perceptron algorithm (Collins, 2002) on an annotated treebank. During parsing, the parser would utilize the learnt weight vector to determine the best parse tree. In our partial parsing method, we aim to keep the dependencies in partial projected trees, while 2 Note that not all non-projectivites are caused by alignment errors; a few of them are also due to translation shift. 5 complement the null dependencies to construct a projective tree. To realize this, we set extremely high scores to the projected dependencies to maximize the score(F, X) for these dependencies, while for the null dependencies we set relatively small scores"
W16-2201,D11-1047,1,0.900779,"Missing"
W16-2201,D14-1063,1,0.892513,"Missing"
W16-2201,W04-3250,0,0.11489,"-trained partial parsing denotes the systems that used the Chinese parser re-trained on the projected trees for the partial parsing process. For reference, we also show the MT performance of the phrase based, string-to-tree, and tree-to-string systems, which are based on the open-source GIZA++/Moses pipeline (Koehn et al., 2007). Note that in all of the Moses, string-totree, and tree-to-string settings, Japanese is always in the string format, and Chinese is parsed by the Berkeley parser14 (Petrov and Klein, 2007).15 The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that, the Baseline KyotoEBMT system outperforms the Moses, string-to-tree, and tree-to-string systems, which verifies the effectiveness of the tree-to-tree approach. The performance difference of KyotoEBMT against the other three MT approaches on the Ja-to-Zh direction is much larger than those of the Zh-to-Ja direction. The reason for this is that KyotoEBMT is much more sensitive to the parsing accuracy on the source side, because the source tree is utilized in the ordering of the final translation. Therefore using Chinese as the source side limits the effectiveness Ja-to-Zh 13.1"
W16-2201,P09-1042,0,0.193816,"es. This extremely limits the translation quality of tree-totree MT. In this paper, we present an approach that projects dependency trees from a high quality (HQ) parser to a low quality (LQ) parser using alignment information. The projection could reduce the parsing errors on the LQ side, and address the annotation criterion difference problem. This can make the LQ trees isomorphic to the HQ trees, which can benefit the translation rule extraction in tree-to-tree MT, and thus improve the MT performance. The idea of cross-language projection of parse trees has been proposed previously, e.g., (Ganchev et al., 2009; Jiang et al., 2010; Goto Tree-to-tree machine translation (MT) that utilizes syntactic parse trees on both source and target sides suffers from the non-isomorphism of the parse trees due to parsing errors and the difference of annotation criterion between the two languages. In this paper, we present a method that projects dependency parse trees from the language side that has a high quality parser, to the side that has a low quality parser, to improve the isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then co"
W16-2201,P06-1077,0,0.237594,"e to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 1–11, c Berlin, Germany, August 11-12, 2016. 2016 Associatio"
W16-2201,D08-1022,0,0.0294423,"l parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 1–11, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational L"
W16-2201,W10-1736,0,0.0700554,"Missing"
W16-2201,W15-5001,1,0.774902,"trees with the ratio higher than the threshold for training the parser. We tried several thresholds in our preliminary experiments, and selected the best threshold of 0.78 (170k trees) based on the MT performance.3 4 Experiments We conducted Japanese-Chinese MT experiments to verify the effectiveness of our constrained partial parsing based projection method. 4.1 Settings 3.3 Re-train a New Low Quality Side Parser We conducted experiments on the scientific domain MT task on the Japanese-Chinese paper excerpt corpus (ASPEC-JC),4 which is one subtask of the workshop on Asian translation (WAT)5 (Nakazawa et al., 2015). The ASPEC-JC task uses 672,315, 2,090, and 2,107 sentences for training, development, and testing, respectively. We used the tree-to-tree MT system KyotoEBMT6 (Richardson et al., 2015) for all of our MT experiments. For Chinese, we used the Chinese analyzing tool KyotoMorph7 proposed by Shen et al. (2014) for segmentation and part-of-speech (POS) tagging, and the SKP parser8 (Shen et al., 2012) for parsing. As the baseline Chinese parser, we trained SKP with the Penn Chinese treebank version 5 (CTB5)9 containing 18k sentences in news domain, and an in-house scientific domain treebank of 10k"
W16-2201,D11-1110,0,0.0211491,"trees might only contain a part of dependencies over a threshold. Our proposed method differs from the previous studies in several aspects: we propose the use of the projectivity criterion for partial projection; we utilize the original target parser and propose a constrained partial parsing algorithm; we re-train a target parser on the full trees generated by the partial parsing. To address the annotation criterion difference problem in projection, Hwa et al. (2005) firstly projected the dependency parse trees, and then applied post projection transformations based on manually created rules. Jiang et al. (2011) presented a method that tolerates the syntactic nonisomorphism between languages. This allows the projected parse trees do not have to follow the annotation criterion of the source parse trees. Our proposed method does not adjust the annotation criterion difference between the source and the projected trees, because in our tree-to-tree MT task, we prefer isomorphic trees. Only a few studies have been conducted to improve MT performance via projection. For string-to-string MT (Koehn et al., 2007), Goto et al. (2015) proposed a pre-ordering method that projects target side constituency trees to"
W16-2201,N06-1023,1,0.721456,"ee-to-tree MT system KyotoEBMT6 (Richardson et al., 2015) for all of our MT experiments. For Chinese, we used the Chinese analyzing tool KyotoMorph7 proposed by Shen et al. (2014) for segmentation and part-of-speech (POS) tagging, and the SKP parser8 (Shen et al., 2012) for parsing. As the baseline Chinese parser, we trained SKP with the Penn Chinese treebank version 5 (CTB5)9 containing 18k sentences in news domain, and an in-house scientific domain treebank of 10k sentences. For Japanese, we used JUMAN10 (Kurohashi et al., 1994) for morphological analyzing, and the KNP parser for parsing11 (Kawahara and Kurohashi, 2006). We trained two 5-gram language models for Chinese and Japanese, respectively, on the training data of the ASPEC corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting, and used them for all the experiments. In all of our experiments, we used the discriminative alignment model Nile13 (Riesa et al., 2011) for word alignment; tuning was performed by the k-best batch Re-training a new LQ parser on the projected trees is necessary for two reasons. Initially, we use the original LQ parser for the partial parsing process, because we do not have a better choice; due to the low acc"
W16-2201,P05-1034,0,0.066477,"with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 1–11, c Berlin, Germany, August 11-12, 201"
W16-2201,P07-2045,0,0.0607713,"uage side that has a high quality parser, to the side that has a low quality parser, to improve the isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are di"
W16-2201,D15-1039,0,0.0191895,"vided into two categories: word alignment errors and annotation criterion difference (Ganchev et al., 2009). To address the word alignment error problem, several studies have proposed to train a target parser on high confidence partially projected trees. Ganchev et al. (2009) presented a partial projection method with constraints such as language-specific annotation rules. They then trained a target parser using the partially projected trees. Spreyer and Kuhn (2009) proposed a similar method that trains both graph-based and transition-based dependency parsers on the partially projected trees. Rasooli and Collins (2015) proposed a method to train a target parser on 8 “dense” projected trees. The “dense” projected trees might only contain a part of dependencies over a threshold. Our proposed method differs from the previous studies in several aspects: we propose the use of the projectivity criterion for partial projection; we utilize the original target parser and propose a constrained partial parsing algorithm; we re-train a target parser on the full trees generated by the partial parsing. To address the annotation criterion difference problem in projection, Hwa et al. (2005) firstly projected the dependency"
W16-2201,W15-5006,1,0.861406,"Missing"
W16-2201,D11-1046,0,0.0576875,"Missing"
W16-2201,P08-1066,0,0.0355068,"isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation,"
W16-2201,Y12-1033,1,0.922019,"projected dependency of (13:及其 (extremely), 19:+), which is obviously incorrect. Alignment errors could happen due to many factors, one of which is translation shift. The erroneous alignment in Figure 2 is caused by this. 3.2 Partial Parsing After the partial projection step, we obtain partial projected trees, with null dependencies discussed in Section 3.1.2. We then perform partial parsing to complement these null dependencies. Before the description of the partial parsing method, we first review the formalism of dependency parsing used in many previous studies such as (Kubler et al., 2009; Shen et al., 2012): Because of the existence of the above cases, we only apply the direct mapping method for partial projection. For the (1) and (2) cases, we leave the dependencies for these words as null. For the (3) case, we propose a projectivity criterion to detect the alignment error, and again leave the dependencies as null. Note that all of these three cases are processed during the top-down projection process. Y ∗ = argmaxY ∈Φ(X) score(Y, X) (1) where X = x1 ...xi ...xn is the input sentence, Y is a candidate tree, Φ(X) is a set of all possible dependency trees over X. Y can be denoted as Y = {(m, h) :"
W16-2201,P14-2042,1,0.853752,"strained partial parsing based projection method. 4.1 Settings 3.3 Re-train a New Low Quality Side Parser We conducted experiments on the scientific domain MT task on the Japanese-Chinese paper excerpt corpus (ASPEC-JC),4 which is one subtask of the workshop on Asian translation (WAT)5 (Nakazawa et al., 2015). The ASPEC-JC task uses 672,315, 2,090, and 2,107 sentences for training, development, and testing, respectively. We used the tree-to-tree MT system KyotoEBMT6 (Richardson et al., 2015) for all of our MT experiments. For Chinese, we used the Chinese analyzing tool KyotoMorph7 proposed by Shen et al. (2014) for segmentation and part-of-speech (POS) tagging, and the SKP parser8 (Shen et al., 2012) for parsing. As the baseline Chinese parser, we trained SKP with the Penn Chinese treebank version 5 (CTB5)9 containing 18k sentences in news domain, and an in-house scientific domain treebank of 10k sentences. For Japanese, we used JUMAN10 (Kurohashi et al., 1994) for morphological analyzing, and the KNP parser for parsing11 (Kawahara and Kurohashi, 2006). We trained two 5-gram language models for Chinese and Japanese, respectively, on the training data of the ASPEC corpus using the KenLM toolkit12 wit"
W16-2201,Y15-2010,1,0.739736,"Goto et al. (2015) proposed a pre-ordering method that projects target side constituency trees to the source side, and then generates pre-ordering rules based on the projected trees. For tree-to-string MT, Jiang et al. (2010) combined projection and supervised constituency parsing by guiding the parsing procedure of the supervised parser with the projected parser. They showed that the guided parser achieved comparable MT results on a treeto-string system (Liu et al., 2006), compared to a normal supervised parser trained on thousands of CTB trees. For tree-to-tree MT (Richardson et al., 2015), Shen et al. (2015) proposed a naive projection method. They complemented the remaining dependencies for a partially projected tree with a backtracking method. Namely, they reused the dependencies in the original target tree for the complement without considering the partially projected dependencies. In contrast, in this paper we propose partial parsing for the complement, in which we search for the best parse tree by taking account of the partially projected dependencies. isomorphic parse tree problem in a dependency based tree-to-tree MT system. Experiments verified the effectiveness of our proposed method. As"
W16-2201,W09-1104,0,0.0318536,"(e.g., English) to a low resource language, to improve the parsing accuracy of the low resource language. The difficulties in projection can be mainly divided into two categories: word alignment errors and annotation criterion difference (Ganchev et al., 2009). To address the word alignment error problem, several studies have proposed to train a target parser on high confidence partially projected trees. Ganchev et al. (2009) presented a partial projection method with constraints such as language-specific annotation rules. They then trained a target parser using the partially projected trees. Spreyer and Kuhn (2009) proposed a similar method that trains both graph-based and transition-based dependency parsers on the partially projected trees. Rasooli and Collins (2015) proposed a method to train a target parser on 8 “dense” projected trees. The “dense” projected trees might only contain a part of dependencies over a threshold. Our proposed method differs from the previous studies in several aspects: we propose the use of the projectivity criterion for partial projection; we utilize the original target parser and propose a constrained partial parsing algorithm; we re-train a target parser on the full tree"
W16-2201,P08-1064,0,0.0311285,"arsing constrained by the already projected dependencies. MT experiments verify the effectiveness of our proposed method. 1 Introduction According to how syntactic parse trees are used in machine translation (MT), there are 4 types of MT approaches: string-to-string that does not use parse trees (Chiang, 2005; Koehn et al., 2007), string-to-tree that uses parse trees on the target side (Galley et al., 2006; Shen et al., 2008), treeto-string that uses parse trees on the source side (Quirk et al., 2005; Liu et al., 2006; Mi and Huang, 2008), and tree-to-tree that uses parse trees on both sides (Zhang et al., 2008; Richardson et al., 2015). Intuitively, the tree-to-tree approach seems to be the most appropriate. The reason is that it could preserve the structure information on both sides, which leads to fluent and accurate translations. In practice, however, good quality parsers on both the source and target sides are difficult to ac∗ Corresponding author. 1 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 1–11, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics tative dependency based tree-to-tree MT system. Figure 1 shows"
W16-2201,N07-1051,0,\N,Missing
W16-2201,C10-2059,0,\N,Missing
W16-2201,W14-7001,1,\N,Missing
W16-2201,N12-1047,0,\N,Missing
W16-2349,P14-1129,0,0.0953135,"Missing"
W16-2349,P07-2045,0,0.00486754,"act that are able to beat the state of art systems by a reasonable margin. (Mikolov et al., 2010) showed that the word embeddings obtained using a simple feed-forward neural network give better results for word similarity tasks compared to those given by the embeddings obtained using GLOVE(Pennington et al., 2014). Furthermore, (Devlin et al., 2014) have shown that using a Neural Network based Lexical Translation Model can help boost the quality of Statistical Machine Translation. (Bahdanau et al., 2014) showed that it is possible to perform end to end MT whose quality surpasses that of Moses(Koehn et al., 2007) by using a combination of Recurrent Neural Networks (RNNs) and dictionary based unknown word substitution. In particular we wanted to test the capabilities of Recurrent Neural Networks augmented with an Attention Based Mechanism for this task. They are easy to design, implement and test due to the availability of NN frameworks like Chainer 1 , Torch 2 , Tensorflow 3 etc. Since Chainer provides a lot of useful functionality and enables rapid prototyping we decided to use it to implement our system. In this paper we describe our system we designed and implemented for the crosslingual pronoun pr"
W16-2349,D14-1162,0,0.0853875,"oss-Lingual Pronoun Translation System Raj Dabre Yevgeniy Puzikov Graduate School of Informatics Graduate School of Informatics Kyoto University, Japan Kyoto University, Japan prajdabre@gmail.com puzikov@nlp.ist.i.kyoto-u.ac.jp Fabien Cromieres JST, Japan Kyoto University fabien@nlp.ist.i.kyoto-u.ac.jp Abstract that are able to beat the state of art systems by a reasonable margin. (Mikolov et al., 2010) showed that the word embeddings obtained using a simple feed-forward neural network give better results for word similarity tasks compared to those given by the embeddings obtained using GLOVE(Pennington et al., 2014). Furthermore, (Devlin et al., 2014) have shown that using a Neural Network based Lexical Translation Model can help boost the quality of Statistical Machine Translation. (Bahdanau et al., 2014) showed that it is possible to perform end to end MT whose quality surpasses that of Moses(Koehn et al., 2007) by using a combination of Recurrent Neural Networks (RNNs) and dictionary based unknown word substitution. In particular we wanted to test the capabilities of Recurrent Neural Networks augmented with an Attention Based Mechanism for this task. They are easy to design, implement and test due to"
W16-2349,W15-2501,0,\N,Missing
W16-2349,W16-2345,0,\N,Missing
W16-4402,W15-4656,1,0.500564,"e also been exploited to acquire knowledge in Chinese (Kuo et al., 2009; Kuo and Hsu, 2011) and Japanese (Nakahara, 2011; Nakahara and Yamada, 2013). The collected knowledge was registered in ConceptNet. Although it is generally hard to gather many players, our GWAP can reach many more users than previous studies because it is built on a running spoken dialogue system. Spoken dialogue systems on smartphones have been attracting much industrial and research interest in recent years. Several studies report that enjoyable user interactions are beneficial for dialogue systems (Jiang et al., 2015; Kobayashi et al., 2015; Sano et al., 2016). 3 Rapid Knowledge Acquisition from Quiz Game We use a quiz game on a spoken dialogue system to obtain large-scale, high-quality commonsense knowledge from many human players. 3.1 Japanese ConceptNet Our knowledge acquisition method follows the scheme of ConceptNet (Speer and Havasi, 2012). In ConceptNet, knowledge is expressed as a triple of two concepts and a relation linking them, where a concept is a word or a short phrase, and a relation consists of about 30 relations such as IsA, Causes, or Antonym. We call a triple a fact, and the two concepts are called a head and"
W16-4402,P16-1137,0,0.019872,"entities, we focus on commonsense knowledge, which has a wider range. The Cyc Project (Lenat, 1995), an early work on commonsense knowledge acquisition, recruited a small group of annotators to construct a knowledge base. Manually curated resources are accurate but expensive to build. Several studies have attempted to construct knowledge bases automatically. For example, Tandon et al. (2014) extracted knowledge from WordNet and Web texts. However, commonsense knowledge is likely to be omitted from texts because it is assumed that every person knows such knowledge (Gordon and Van Durme, 2013). Li et al. (2016) addressed this problem using knowledge base completion, in which existing knowledge is used to acquire more knowledge. However, their method needs some amount of existing knowledge as a seed. Thus, manual effort is still required. Crowdsourcing, which is a process that requests various tasks of non-expert workers on the Internet, can be used to reduce the cost of the manual process. The OMCS project (Liu and Singh, 2004; Speer and Havasi, 2012) collected commonsense knowledge by recruiting many volunteers on the Internet. The resulting knowledge base is called ConceptNet,2 which we use and ex"
W16-4402,W16-1316,1,0.835681,"on-expert workers on the Internet, can be used to reduce the cost of the manual process. The OMCS project (Liu and Singh, 2004; Speer and Havasi, 2012) collected commonsense knowledge by recruiting many volunteers on the Internet. The resulting knowledge base is called ConceptNet,2 which we use and extend in our study. Some studies transform the manual acquisition process into an enjoyable game, called a GWAP, to motivate players to participate in knowledge acquisition. GWAPs are a form of crowdsourcing3 and have been used for validating (Herda˘gdelen and Barobni, 2012; Vannella et al., 2014; Machida et al., 2016) and collecting (von Ahn et al., 2006; Lieberman et al., 2007; Kuo et al., 2009; Nakahara, 2011; Kuo and Hsu, 2011; Nakahara and Yamada, 2013) language resources. A word-guessing game was designed by von Ahn et al. (2006) to collect a large amount of knowledge within a short time and at a low cost. GWAPs have also been exploited to acquire knowledge in Chinese (Kuo et al., 2009; Kuo and Hsu, 2011) and Japanese (Nakahara, 2011; Nakahara and Yamada, 2013). The collected knowledge was registered in ConceptNet. Although it is generally hard to gather many players, our GWAP can reach many more user"
W16-4402,D15-1276,1,0.87159,"and von Ahn, 2011). 4 From a snapshot taken on Sept. 10th , 2015 at http://conceptnet5.media.mit.edu/downloads/20150910/. 5 http://compling.hss.ntu.edu.sg/wnja/index.en.html 3 13 words and many relations linking them. Thus, collecting more commonsense facts is essential to making them at least as useful in NLP tasks as WordNet is. We only consider the filtered ConceptNet in the rest of this paper. Note that words in heads and tails are normalized into their representative forms (e.g., { みかん mikan, ミカン mikan, 蜜柑 mikan} (orange) → みかん mikan (orange)) given by the morphological analyzer JUMAN++ (Morita et al., 2015). 3.2 Building a Quiz from using ConceptNet We collect commonsense knowledge from many people. To motivate them, we transform the knowledge acquisition process into an enjoyable quiz game, where human players are given several hints about a certain word to be guessed, and we acquire knowledge from players’ guesses. The hints are easily generated from existing knowledge in ConceptNet. Figure 1 shows examples. “This is at supermarkets” and “this is made of milk” are generated from the facts (cake, AtLocation, supermarket) and (cake MadeOf, milk), respectively. The word to be guessed (hereafter k"
W16-4402,P16-1114,1,0.436906,"acquire knowledge in Chinese (Kuo et al., 2009; Kuo and Hsu, 2011) and Japanese (Nakahara, 2011; Nakahara and Yamada, 2013). The collected knowledge was registered in ConceptNet. Although it is generally hard to gather many players, our GWAP can reach many more users than previous studies because it is built on a running spoken dialogue system. Spoken dialogue systems on smartphones have been attracting much industrial and research interest in recent years. Several studies report that enjoyable user interactions are beneficial for dialogue systems (Jiang et al., 2015; Kobayashi et al., 2015; Sano et al., 2016). 3 Rapid Knowledge Acquisition from Quiz Game We use a quiz game on a spoken dialogue system to obtain large-scale, high-quality commonsense knowledge from many human players. 3.1 Japanese ConceptNet Our knowledge acquisition method follows the scheme of ConceptNet (Speer and Havasi, 2012). In ConceptNet, knowledge is expressed as a triple of two concepts and a relation linking them, where a concept is a word or a short phrase, and a relation consists of about 30 relations such as IsA, Causes, or Antonym. We call a triple a fact, and the two concepts are called a head and tail, respectively."
W16-4402,speer-havasi-2012-representing,0,0.386844,"r, commonsense knowledge is so clear for every person that it is often omitted in a text (Gordon and Van Durme, 2013). For instance, we rarely state in a text that strawberries are stored in refrigerators. Rather, we often talk about a major production region for strawberries. Therefore, manual effort is still required to build commonsense knowledge bases. To reduce the cost of manual knowledge acquisition, some studies explored the use of crowdsourcing, a process that requests various tasks of non-expert workers on the Internet. The Open Mind Common Sense (OMCS) project (Liu and Singh, 2004; Speer and Havasi, 2012) recruited volunteers on the Internet and constructed ConceptNet, a large collection of commonsense knowledge such as (cake, AtLocation, supermarket). Whereas participants in the OMCS projects entered the commonsense knowledge in Web forms, some studies have transformed the knowledge acquisition process into a type of enjoyable game, called games with a purpose (GWAP) (von Ahn et al., 2006; Lieberman et al., 2007; Kuo et al., 2009; Nakahara, 2011; Herda˘gdelen and Barobni, 2012; Kuo and Hsu, 2011). The advantage of a GWAP is that it is more attractive to humans than the standard annotation pro"
W16-4402,P14-1122,0,0.0301215,"ests various tasks of non-expert workers on the Internet, can be used to reduce the cost of the manual process. The OMCS project (Liu and Singh, 2004; Speer and Havasi, 2012) collected commonsense knowledge by recruiting many volunteers on the Internet. The resulting knowledge base is called ConceptNet,2 which we use and extend in our study. Some studies transform the manual acquisition process into an enjoyable game, called a GWAP, to motivate players to participate in knowledge acquisition. GWAPs are a form of crowdsourcing3 and have been used for validating (Herda˘gdelen and Barobni, 2012; Vannella et al., 2014; Machida et al., 2016) and collecting (von Ahn et al., 2006; Lieberman et al., 2007; Kuo et al., 2009; Nakahara, 2011; Kuo and Hsu, 2011; Nakahara and Yamada, 2013) language resources. A word-guessing game was designed by von Ahn et al. (2006) to collect a large amount of knowledge within a short time and at a low cost. GWAPs have also been exploited to acquire knowledge in Chinese (Kuo et al., 2009; Kuo and Hsu, 2011) and Japanese (Nakahara, 2011; Nakahara and Yamada, 2013). The collected knowledge was registered in ConceptNet. Although it is generally hard to gather many players, our GWAP c"
W16-4601,W16-4616,1,0.921374,"n and Communication Technology Waseda University Ehara NLP Research Laboratory NTT Communication Science Laboratories Weblio, Inc. Indian Institute of Technology Bombay Japan Patent Information Organization Indian Institute of Technology Patna University of Tokyo University of Tokyo ASPEC JPC BPPT IITBC pivot JE EJ JC CJ JE EJ JC CJ JK KJ IE EI HE EH HJ JH ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 7: List of participants who submitted translation results to WAT2016 and their participation in each subtasks. Team ID NAIST (Neubig, 2016) Kyoto-U (Cromieres et al., 2016) TMU (Yamagishi et al., 2016) bjtu nlp (Li et al., 2016) Sense (Tan, 2016) NICT-2 (Imamura and Sumita, 2016) WASUIPS (Yang and Lepage, 2016) EHR (Ehara, 2016) ntt (Sudoh and Nagata, 2016) TOKYOMT (Shu and Miura, 2016) IITB-EN-ID (Singh et al., 2016) JAPIO (Kinoshita et al., 2016) IITP-MT (Sen et al., 2016) UT-KAY (Hashimoto et al., 2016) UT-AKY (Eriguchi et al., 2016) 7 Evaluation Results In this section, the evaluation results for WAT2016 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website24 . 7.1 Offi"
W16-4601,W16-4609,0,0.0501384,"Missing"
W16-4601,W16-4617,0,0.0323933,"Missing"
W16-4601,P13-2121,0,0.0165266,"evelopment data. 3.2 Common Settings for Baseline SMT We used the following tools for tokenization. • Juman version 7.08 for Japanese segmentation. • Stanford Word Segmenter version 2014-01-049 (Chinese Penn Treebank (CTB) model) for Chinese segmentation. • The Moses toolkit for English and Indonesian tokenization. • Mecab-ko10 for Korean segmentation. • Indic NLP Library11 for Hindi segmentation. To obtain word alignments, GIZA++ and grow-diag-final-and heuristics were used. We used 5-gram language models with modified Kneser-Ney smoothing, which were built using a tool in the Moses toolkit (Heafield et al., 2013). 3.3 Phrase-based SMT We used the following Moses configuration for the phrase-based SMT system. • distortion-limit – 20 for JE, EJ, JC, and CJ – 0 for JK, KJ, HE, and EH – 6 for IE and EI • msd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuring The default values were used for the other system parameters. 3.4 Hierarchical Phrase-based SMT We used the following Moses configuration for the hierarchical phrase-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-"
W16-4601,2009.iwslt-papers.4,0,0.0367694,"tp://lotus.kuee.kyoto-u.ac.jp/WAT/Hindi-corpus/WAT2016-Ja-Hi.zip 3 LangPair IITB-EH IITB-JH Train 1,492,827 152,692 Dev 520 1,566 Test 2,507 2,000 Monolingual Corpus (Hindi) 45,075,279 - Table 4: Statistics for IITB Corpus. SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page7 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 5. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 7 http://lotus.kuee.kyot"
W16-4601,W16-4611,0,0.032371,"Missing"
W16-4601,D10-1092,0,0.148885,"rser to obtain source language syntax. We used the following Moses configuration for the baseline tree-to-string syntax-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. The default values were used for the other system parameters. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016. All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994"
W16-4601,W16-4612,0,0.0719596,"kyo ASPEC JPC BPPT IITBC pivot JE EJ JC CJ JE EJ JC CJ JK KJ IE EI HE EH HJ JH ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 7: List of participants who submitted translation results to WAT2016 and their participation in each subtasks. Team ID NAIST (Neubig, 2016) Kyoto-U (Cromieres et al., 2016) TMU (Yamagishi et al., 2016) bjtu nlp (Li et al., 2016) Sense (Tan, 2016) NICT-2 (Imamura and Sumita, 2016) WASUIPS (Yang and Lepage, 2016) EHR (Ehara, 2016) ntt (Sudoh and Nagata, 2016) TOKYOMT (Shu and Miura, 2016) IITB-EN-ID (Singh et al., 2016) JAPIO (Kinoshita et al., 2016) IITP-MT (Sen et al., 2016) UT-KAY (Hashimoto et al., 2016) UT-AKY (Eriguchi et al., 2016) 7 Evaluation Results In this section, the evaluation results for WAT2016 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website24 . 7.1 Official Evaluation Results Figures 2, 3, 4 and 5 show the official evaluation results of ASPEC subtasks, Figures 6, 7, 8, 9 and 10 show those of JPC subtasks, Figures 11 and 12 show those of BPPT subtasks and Figures 13 and 14 show those of IITB subtasks. Each figure contains automa"
W16-4601,P07-2045,0,0.0173325,"ee syntax-based 6 http://lotus.kuee.kyoto-u.ac.jp/WAT/Hindi-corpus/WAT2016-Ja-Hi.zip 3 LangPair IITB-EH IITB-JH Train 1,492,827 152,692 Dev 520 1,566 Test 2,507 2,000 Monolingual Corpus (Hindi) 45,075,279 - Table 4: Statistics for IITB Corpus. SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page7 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 5. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 7 h"
W16-4601,W04-3250,0,0.322367,"Missing"
W16-4601,W15-5008,0,0.0203255,"eement between the workers, we calculated the Fleiss’ κ (Fleiss and others, 1971) values. The results are shown in Table 20. We can see that the κ values are larger for X → J translations than for J → X translations. This may be because the majority of the workers are Japanese, and the evaluation of one’s mother tongue is much easier than for other languages in general. 7.3 Chronological Evaluation Figure 15 shows the chronological evaluation results of 4 subtasks of ASPEC and 2 subtasks of JPC. The Kyoto-U (2016) (Cromieres et al., 2016), ntt (2016) (Sudoh and Nagata, 2016) and naver (2015) (Lee et al., 2015) are NMT systems, the NAIST (2015) (Neubig et al., 2015) is a forest-to-string SMT system, Kyoto-U (2015) (Richardson et al., 2015) is a dependency tree-to-tree EBMT system and JAPIO (2016) (Kinoshita et al., 2016) system is a phrase-based SMT system. What we can see is that in ASPEC-JE and EJ, the overall quality is improved from the last year, but the ratio of grade 5 is decreased. This is because the NMT systems can output much fluent translations 24 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html 12 but the adequacy is worse. As for ASPEC-JC and CJ, the quality is very much impro"
W16-4601,W16-4608,0,0.0513487,"Missing"
W16-4601,W14-7001,1,0.885529,"red tasks from the 3rd workshop on Asian translation (WAT2016) including J↔E, J↔C scientific paper translation subtasks, C↔J, K↔J, E↔J patent translation subtasks, I↔E newswire subtasks and H↔E, H↔J mixed domain subtasks. For the WAT2016, 15 institutions participated in the shared tasks. About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014) and WAT2015 (Nakazawa et al., 2015), WAT2016 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 3rd WAT, we adopt new translation subtasks with English-Japanese patent description, Indonesian-English news description and Hindi-English and Hindi-Japanese mixed domain corpus in addition to the subtasks that were conducted in WAT2015. Furthermore, we invited research papers on topics related to the machine translation"
W16-4601,W15-5001,1,0.853738,"sian translation (WAT2016) including J↔E, J↔C scientific paper translation subtasks, C↔J, K↔J, E↔J patent translation subtasks, I↔E newswire subtasks and H↔E, H↔J mixed domain subtasks. For the WAT2016, 15 institutions participated in the shared tasks. About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014) and WAT2015 (Nakazawa et al., 2015), WAT2016 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 3rd WAT, we adopt new translation subtasks with English-Japanese patent description, Indonesian-English news description and Hindi-English and Hindi-Japanese mixed domain corpus in addition to the subtasks that were conducted in WAT2015. Furthermore, we invited research papers on topics related to the machine translation, especially for Asian languages. Th"
W16-4601,P11-2093,1,0.785718,"s et al., 2015). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016. All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 13 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0 14 . For Chinese segmentation we used two different tools: KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 15 (Tseng, 2005). For Korean segmentation we used mecabko 16 . For English and Indonesian segmentations we used tokenizer.perl 17 in the Moses toolkit. For Hindi segmentation we used Indic NLP Library 18 . Detailed procedures for the automatic evaluation are shown on the WAT2016 evaluation web page 19"
W16-4601,W16-4610,1,0.870902,"Missing"
W16-4601,P02-1040,0,0.0994357,"sed SMT We used the Berkeley parser to obtain source language syntax. We used the following Moses configuration for the baseline tree-to-string syntax-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. The default values were used for the other system parameters. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 12 ; AMFM scores were calculated using scripts created by technical collaborators of WAT2016. All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman versi"
W16-4601,P06-1055,0,0.0157429,"Train 1,492,827 152,692 Dev 520 1,566 Test 2,507 2,000 Monolingual Corpus (Hindi) 45,075,279 - Table 4: Statistics for IITB Corpus. SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page7 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 5. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 7 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 4 5 System ID SMT Phrase SMT Hiero SMT S2T SMT T2S RBMT X RBMT X RBMT X RBMT X"
W16-4601,W15-5006,1,0.884753,"Missing"
W16-4601,W16-4622,0,0.0346093,"Missing"
W16-4601,W16-4623,0,0.0353036,"Missing"
W16-4601,W16-4604,0,0.0409014,"Missing"
W16-4601,W16-4621,0,0.131935,"mation Organization Indian Institute of Technology Patna University of Tokyo University of Tokyo ASPEC JPC BPPT IITBC pivot JE EJ JC CJ JE EJ JC CJ JK KJ IE EI HE EH HJ JH ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 7: List of participants who submitted translation results to WAT2016 and their participation in each subtasks. Team ID NAIST (Neubig, 2016) Kyoto-U (Cromieres et al., 2016) TMU (Yamagishi et al., 2016) bjtu nlp (Li et al., 2016) Sense (Tan, 2016) NICT-2 (Imamura and Sumita, 2016) WASUIPS (Yang and Lepage, 2016) EHR (Ehara, 2016) ntt (Sudoh and Nagata, 2016) TOKYOMT (Shu and Miura, 2016) IITB-EN-ID (Singh et al., 2016) JAPIO (Kinoshita et al., 2016) IITP-MT (Sen et al., 2016) UT-KAY (Hashimoto et al., 2016) UT-AKY (Eriguchi et al., 2016) 7 Evaluation Results In this section, the evaluation results for WAT2016 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2016 website24 . 7.1 Official Evaluation Results Figures 2, 3, 4 and 5 show the official evaluation results of ASPEC subtasks, Figures 6, 7, 8, 9 and 10 show those of JPC subtasks, Figures 11 and 12 show those of"
W16-4601,W16-4618,0,0.0231168,"Missing"
W16-4601,2007.mtsummit-papers.63,0,0.716723,"Information and Communications Technology (NICT). It consists of a JapaneseEnglish scientific paper abstract corpus (ASPEC-JE), which is used for J↔E subtasks, and a JapaneseChinese scientific paper excerpt corpus (ASPEC-JC), which is used for J↔C subtasks. The statistics for each corpus are described in Table1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by the NICT from approximately 2 million JapaneseEnglish scientific paper abstracts owned by the JST. Because the abstracts are comparable corpora, the sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score and the field symbol. The similarity scores are calculated by the method from (Utiyama and Isahara, 2007). The field symbols are single letters A-Z and show the scientific field for each document5 . The correspondence between the symbols and field names, along with the frequency and occurrence ratios for the training data, are given in the README file from ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts owned by JST that are not contained in the t"
W16-4601,W16-4620,0,0.0195884,"Missing"
W16-4601,W16-4619,0,0.038307,"Missing"
W16-4616,C16-2064,1,0.550675,"Japanese translation direction of the ASPEC-JC task, and further updated the state-of-the-art result (46.04 → 46.36 BLEU). 3 NMT NMT is a new approach to MT that, although recently proposed, has quickly achieved state-of-the-art results (Bojar et al., 2016). We implemented our own version of the sequence-to-sequence with attention mechanism model, first proposed in (Bahdanau et al., 2015). Our implementation was done using the Chainer3 toolkit (Tokui et al., 2015). We make this implementation available under a GPL license.4 3 4 http://chainer.org/ https://github.com/fabiencro/knmt . See also (Cromieres, 2016) 167 Figure 2: The structure of a NMT system with attention, as described in (Bahdanau et al., 2015) (but with LSTMs instead of GRUs). The notation “&lt;1000&gt;” means a vector of size 1000. The vector sizes shown here are the ones suggested in the original paper. 3.1 Overview of NMT We describe here briefly the (Bahdanau et al., 2015) model. As shown in Figure 2, an input sentence is first converted into a sequence of vector through an embedding layer; these vectors are then fed to two LSTM layers (one going forward, the other going backward) to give a new sequence of vectors that encode the input"
W16-4616,W16-4601,1,0.866497,"Missing"
W16-4616,L16-1350,1,0.87803,"Missing"
W16-4616,W15-5006,1,0.864664,"Missing"
W16-4616,W16-2323,0,0.0339875,"els instead of one. The two “classic” ways of combining the prediction of different systems are to either take the geometric average or the arithmetic average of their predicted probabilities. Interestingly, although it seems other researchers have reported that using the arithmetic average works better (Luong et al., 2016), we actually found that geometric average was giving better results for us. Ensembling usually works best with independently trained parameters. We actually found that even using parameters from a single run could improve results. This had also been previously observed by (Sennrich et al., 2016). Therefore, for the cases when we could only run one training session, we ensembled the three parameters corresponding to the best development loss, the best development BLEU, and the final parameters (obtained after continuing training for some time after the best development BLEU was found). We refer to this as “self-ensembling” in the result section. When we could do n independent training, we kept these three parameters for each of the independent run and ensembled the 3 · n parameters. 3.7 Preprocessing The preprocessing steps were essentially the same as for KyotoEBMT. We lowercased the"
W16-4616,C16-1029,1,0.86956,"Missing"
W16-4616,P16-1008,0,0.0308784,"luency and meaning of the translations. This is due to the reason that the word order of the translations in EBMT depends on the parse trees of the input sentences, but the parsing accuracy is not perfect especially for Chinese. NMT tends to produce fluent translations, however it lacks of adequacy sometimes. The most common problem of NMT is that it could produce under or over translated translations, due to the lack of a way for the attention mechanism to memorize the source words that have been translated during decoding. We plan to address this problem with the coverage model proposed in (Tu et al., 2016). The UNK words are also a big problem. Although we deal with them using the UNK replacement method (Luong et al., 2015), it could simply fail because of errors for finding the corresponding source words using attention. We show a Japanese-to-English translation example by our EBMT and NMT systems in Table 2 to illustrate some of the above problems. The translation produced by the EBMT system has a word order problem that changes the meaning, making “the basic composition, standard” independent from “this flow sensor.” It also has an agreement violation problem between the argument and the pre"
W16-5407,C96-2184,0,0.0597353,"nnotation are selected from the LCAS (National Science Library, Chinese Academy of Sciences) corpus provided by Japan Science and Technology Agency (JST). The LCAS corpus consists of Chinese scientific papers of various scientific subdomains. From this corpus, 780k abstracts were manually translated from Chinese to Japanese by JST (most of them also contain English translations). We randomly selected the raw sentences from the parallel part of the LCAS corpus, aiming for not only improving Chinese analysis but also multilingual NLP. 2.2 Annotation Standard Conventional segmentation standards (Huang et al., 1996; Xia et al., 2000; Duan et al., 2003) define words based on the analysis of morphology, which could lead to two problems: inconsistency and data sparsity. For example, based on the conventional segmentation standards, both “使用 (use)” and “使 用者 (user/use person)” in Figure 1 are one words, because “者 (person)” is a bound morpheme that cannot form a word itself. This leads to the inconsistent segmentation of “使用 (use)”, and also makes 3 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?A%20Chinese%20Treebank%20in%20Scientific%20Domain%20%28SCTB%29 60 both words sparse. In this work, we adopt the Chin"
W16-5407,P07-2045,0,0.00552999,"bowl wall section 26 defines an opening 28 and a leachate chamber 29 is located in the wall 24 beneath the opening 28. annular 碗状壁区 section 26 defines opening 28, leach liquid chamber 29 in the wall 24 opening 28 below. the annular bowl-shaped wall section 26 defines opening 28, leach liquid chamber 29 is positioned below the opening 28 in the wall 24. Table 5: An improved MT example. task at the NTCIR-10 workshop10 (Goto et al., 2013). The NTCIR-CE task uses 1,000,000, 2,000, and 2,000 sentences for training, development, and testing, respectively. We used the Moses tree-to-string MT system (Koehn et al., 2007) for all of our MT experiments. In our experiments, Chinese is in the tree format, and Japanese/English is in the string format. For Chinese, we used KyotoMorph for segmentations and the Berkeley parser for joint POS tagging and parsing. We binarized the parsing results for better translation rule extraction. We compared the MT performance of the “Baseline” and “Baseline+SCTB” settings in Section 3.1. For Japanese, we used JUMAN11 (Kurohashi et al., 1994) for the segmentation. For English, we tokenized the sentences using a script in Moses. For the Chinese-to-Japanese MT task, we trained a 5-g"
W16-5407,W04-3250,0,0.13052,"d a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting. For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method. In all of our experiments, we used the GIZA++ toolkit13 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. Table 4 shows the translation results. The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that the significant improvements on Chinese analysis due to the annotated treebank, also lead to the significant MT performance improvements. Despite the language pair and slight domain difference, similar improvements are observed on both the ASPEC-CJ and NTCIR-CE MT tasks. To further understand the reasons for the improvements, we also investigated the translation results. We found that most of the improvements are due to analysis improvements of the source sentences. Table 5 shows an improved MT example from the NTCIR-CE task. We can see that there is an out-ofvocabulary word"
W16-5407,J93-2004,0,0.0540819,"Missing"
W16-5407,W15-5001,1,0.854089,"ences to the baseline treebanks for training the analyzers. Figure 3 shows the results. We can see that for segmentation and POS tagging, the accuracy improvements slow down when more annotated sentences are used for training the analyzers; while for parsing, there is still a large potential of improvement by annotating more sentences. 3.2 MT Experiments For Chinese-to-Japanese translation, we conducted experiments on the scientific domain MT task on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ)8 (Nakazawa et al., 2016), which is one subtask of the workshop on Asian translation (WAT)9 (Nakazawa et al., 2015). The ASPEC-CJ task uses 672,315, 2,090, and 2,107 sentences for training, development, and testing, respectively. For Chinese-to-English translation, we conducted experiments on the Chinese-English subtask (NTCIR-CE) of the patent MT 8 9 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ http://orchid.kuee.kyoto-u.ac.jp/WAT/ 63 System Baseline Baseline+SCTB ASPEC-CJ 39.12 40.08† NTCIR-CE 33.19 33.90† Table 4: BLEU-4 scores for ASPEC-CJ and NTCIR-CE translation tasks (“†” indicates that the result is significantly better than “Baseline” at p < 0.01). Source Reference Baseline Baseline +SCTB 环形碗状壁区段２６限定了开口"
W16-5407,L16-1262,0,0.021801,"Missing"
W16-5407,P03-1021,0,0.0123856,"d JUMAN11 (Kurohashi et al., 1994) for the segmentation. For English, we tokenized the sentences using a script in Moses. For the Chinese-to-Japanese MT task, we trained a 5-gram language model for Japanese, on the training data of the ASPEC-CJ corpus using the KenLM toolkit12 with interpolated Kneser-Ney discounting. For the Chinese-to-English MT task, we trained a 5-gram language model for English, on the training data of the NTCIR-CE corpus using the same method. In all of our experiments, we used the GIZA++ toolkit13 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. Table 4 shows the translation results. The significance tests were performed using the bootstrap resampling method (Koehn, 2004). We can see that the significant improvements on Chinese analysis due to the annotated treebank, also lead to the significant MT performance improvements. Despite the language pair and slight domain difference, similar improvements are observed on both the ASPEC-CJ and NTCIR-CE MT tasks. To further understand the reasons for the improvements, we also investigated the translation results. We found that most of the improvements"
W16-5407,C14-1026,0,0.0271706,"urce sentence in Table 5 “浸出 (leach ) /液 (liquid) /腔室 (chamber) /２９/位于 (is located) /壁 (wall) /２４/中 (in) /开口 (opening) /２８/之下 (beneath)”. chamber)” into “leach liquid chamber”, this is due to the similar analysis results of both systems, while the correct analysis for this noun phrase should be “(NP (NP 浸出 NN 液 SFN) 腔室 NN) (leachate chamber)”. 4 Related Work Besides the widely used CTB (Xue et al., 2005), there are two other treebanks for Chinese. The Peking University (PKU) annotated a Chinese treebank, firstly only for segmentations and POS tags (Yu et al., 2003), and later also for syntax (Qiu et al., 2014). The Harbin Institute of Technologys (HIT) also annotated a treebank for dependency structures (Che et al., 2012). Besides the difference in annotation standards and syntactic structures, all the three treebanks are in news domain. CTB selected the raw sentences from People’s Daily, Hong Kong newswire, Xinhua newswire etc., and PKU and HIT selected the raw sentences from People’s Daily newswire. To the best of our knowledge, our treebank is the first publicly available Chinese treebank in scientific domain. Three are two types of syntactic grammars for treebanking: phrase structures and depen"
W16-5407,P14-2042,1,0.898261,"Missing"
W16-5407,C16-1029,1,0.912447,"of downstream applications such as text mining and machine translation (MT). Motivated by this, we decide to construct a Chinese treebank in the scientific domain (SCTB) to promote Chinese NLP research in this domain. This paper presents the details of our treebank annotation process and the experiments conducted on the annotated treebank. The raw sentences are selected from Chinese scientific papers. Our annotation process follows that of CTB (Xue et al., 2005) with an exception of the segmentation standard. We apply a Chinese word segmentation standard based on character-level POS patterns (Shen et al., 2016), aiming to circumvent inconsistency and address data 1 2 https://catalog.ldc.upenn.edu/LDC2005T01 Statistics from Japan Patent Office. 59 Proceedings of the 12th Workshop on Asian Language Resources, pages 59–67, Osaka, Japan, December 12 2016. Figure 1: A screenshot of the annotation interface containing an annotation example of a Chinese sentence “烟草 (tobacco) /使用 (use) /是 (is) /当今 (nowadays) /世界 (world) /最大 (biggest) /的 (’s) /可 (can) /预防 (prevention) /死因 (cause of death) /，/烟草 (tobacco) /使用 (use) /者 (person) /中 (among) /近 (about) /一半 (half) /将 (will) /死于 (die) /烟草 (tobacco) /使用 (use) /。” ("
W16-5407,L16-1249,0,0.024799,"rase structures and dependency structures. We adopt the phrase structures used in CTB (Xue et al., 2005), because phrase structures can be converted to dependency structures based on predefined head rules using e.g. the Penn2Malt toolkit.14 Treebanks with multi-view of both phrase structures and dependency structures also have been proposed (Qiu et al., 2014). Recently, with more needs of multilingual NLP, the interests of constructing multilingual treebanks have increased. Multilingual treebanks such as the universal dependency treebank15 (Nivre et al., 2016) and the Asian language treebank (Thu et al., 2016) are being constructed. As the raw sentences of our treebank were selected from parallel data and the translated Japanese and English sentences are available, we leave the potential to develop our treebank to a trilingual one. 5 Conclusion In this paper, we presented the details of the annotation of SCTB: a Chinese treebank in scientific domain. Experiments conducted for Chinese analysis and MT verified the effectiveness of the annotated SCTB. As future work, firstly, we plan to annotate more sentences, and we aim to finish the annotation for 10k sentences within this year. Secondly, we also p"
W16-5407,xia-etal-2000-developing,0,0.591073,"ed from the LCAS (National Science Library, Chinese Academy of Sciences) corpus provided by Japan Science and Technology Agency (JST). The LCAS corpus consists of Chinese scientific papers of various scientific subdomains. From this corpus, 780k abstracts were manually translated from Chinese to Japanese by JST (most of them also contain English translations). We randomly selected the raw sentences from the parallel part of the LCAS corpus, aiming for not only improving Chinese analysis but also multilingual NLP. 2.2 Annotation Standard Conventional segmentation standards (Huang et al., 1996; Xia et al., 2000; Duan et al., 2003) define words based on the analysis of morphology, which could lead to two problems: inconsistency and data sparsity. For example, based on the conventional segmentation standards, both “使用 (use)” and “使 用者 (user/use person)” in Figure 1 are one words, because “者 (person)” is a bound morpheme that cannot form a word itself. This leads to the inconsistent segmentation of “使用 (use)”, and also makes 3 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?A%20Chinese%20Treebank%20in%20Scientific%20Domain%20%28SCTB%29 60 both words sparse. In this work, we adopt the Chinese word segmentat"
W16-5407,zhang-etal-2004-interpreting,0,0.102482,"Missing"
W17-2704,I08-1065,0,0.0378229,"ection 3.1, we first introduce the acquisition of related event pairs, which are the inputs to our shared argument identification model. We introduce the gold dataset used for model learning in Section 3.2. In Section 3.3, we describe the selection of case frames. These case frames will be used to model different meanings of predicates in our model. The remaining of the section will be dedicated to the description of our proposed methods of shared argument identification. There are several works proposed for Japanese event relation knowledge acquisition utilizing the co-occurrences of events. Abe et al. (2008) proposed a pattern-based method which utilized a predefined set of lexico-syntactic co-occurrence patterns to perform bootstrapping for event relation learning. Their work focused on the acquisition of related event pairs, but not the relations between the arguments of the related events. 3.1 Related Event Pairs Our work is based on the two-stage framework of event relation knowledge proposed by Shibata and Kurohashi (2011). We adopt the first stage of related event pair extraction proposed in their work to obtain the related event pairs, which will be the input to our shared argument identif"
W17-2704,P08-1090,0,0.312892,"linguistic properties hinder the performance of Japanese coreference resolution sys1 In this paper we adopt the Japanese case marker, ga, wo, ni, and de, which roughly corresponds to nominative, accusative, dative, and instrumental/locative cases. 21 Proceedings of the Events and Stories in the News Workshop, pages 21–30, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics Figure 2: Two-stage approach for Japanese event relation knowledge acquisition. tems, and make it unsuitable to apply coreferencebased methods of English event relation knowledge acquisition (Chambers and Jurafsky, 2008) directly to Japanese. On the other hand, event relation knowledge can benefit the task of the coreference resolution. The shared arguments within an event relation knowledge provide direct clues that the case slots sharing an argument should hold co-referring arguments. These clues are particularly critical in cases in which selectional preference is not helpful, such as coreference resolution problems presented in Winograd Schema Challenge (Levesque et al., 2012; Rahman and Ng, 2012). Consider the following example: shared arguments in our gold dataset. We designed a richer feature represent"
W17-2704,E14-1024,0,0.0190506,"s2 切手を貼る (stamp-wo paste) → ポストに入れる (mailbox-ni put) 薬を飲む (medicine-wo take) → 症状が軽くなる (symptom-ga alleviate) Support sentences 親宛に書いた葉書きに切手を貼ってポストに入れた。 (I pasted a stamp on the postcard to my parents, and put it into the mailbox.) 手紙を書いて、封をして、切手を貼って、ポストに入れる。 (I write a letter, seal it, paste a stamp, and put it into the mailbox.) 薬を飲み続けていると、アレルギーの症状は大分軽くなってきている。 (Taking the medicine alleviates the allergy symptom significantly.) 抗ヒスタミン系の薬を処方され、飲めば症状は軽くなります。 (I was prescribed antihistamine, the symptom alleviated after taking it.) Table 1: Related Event Pairs. 3 Shared Argument Identification Pichotta and Mooney (2014) used a richer representation of event than in the work of Chambers et al. and achieved an improvement in predicting performance. Instead of representing an event as a (predicate, dependency) pair, they considered an event as a structure of a predicate and arguments with subject, object, direct object relations with the predicate. With this multi-argument event representation, their model performs better in the cases of ambiguous verbs, and is more capable of capturing complex interactions between multiple entities. In this section, we introduce our method of shared argument identification. In"
W17-2704,D12-1071,0,0.0235801,"make it unsuitable to apply coreferencebased methods of English event relation knowledge acquisition (Chambers and Jurafsky, 2008) directly to Japanese. On the other hand, event relation knowledge can benefit the task of the coreference resolution. The shared arguments within an event relation knowledge provide direct clues that the case slots sharing an argument should hold co-referring arguments. These clues are particularly critical in cases in which selectional preference is not helpful, such as coreference resolution problems presented in Winograd Schema Challenge (Levesque et al., 2012; Rahman and Ng, 2012). Consider the following example: shared arguments in our gold dataset. We designed a richer feature representation for shared argument learning, which considers the interaction between shared arguments and the mechanism of argument omission in depth. (2) a. グーグルが モトローラ を買収した。 彼ら が破綻したからだ。 (Google-ga acquired Motorola-wo, because they-ga went bankrupt.) b. A1 -ga go bankrupt → A2 -ga A1 -wo acquire In the example of (2-a), both precedents of ‘they’, ‘Google’ and ‘Motorola’, are of the same category. While selectional preference is not helpful in this case, the event relation knowledge in (2-b)"
W17-2704,I11-1115,1,0.936398,"arning, which considers the interaction between shared arguments and the mechanism of argument omission in depth. (2) a. グーグルが モトローラ を買収した。 彼ら が破綻したからだ。 (Google-ga acquired Motorola-wo, because they-ga went bankrupt.) b. A1 -ga go bankrupt → A2 -ga A1 -wo acquire In the example of (2-a), both precedents of ‘they’, ‘Google’ and ‘Motorola’, are of the same category. While selectional preference is not helpful in this case, the event relation knowledge in (2-b) can help us resolve (2-a) correctly. In this work, we adopted the two-stage framework for Japanese event relation knowledge acquisition (Shibata and Kurohashi, 2011). In the first stage of related event pair extraction, we adopted the method proposed by Shibata and Kurohashi (2011); and in the second stage of shared argument identification, we extended the model of Kohama et al. (2015) to incorporate all types of 22 In addition, we manually constructed a gold dataset for shared argument learning. With the help of linguistic experts, we established an annotation scheme for shared argument. We classified the shared arguments into three types: standard shared argument, quasi shared argument, and multiple shared argument. We evaluated our method of shared arg"
W17-5014,C16-2063,0,0.0311746,"ences which are useless for example sentences, for instance ones that contain random digits or alphabet. 4 be from different domains and cover rare words. Also, the work does not consider sentence difficulty. In the evaluation by language learners we found out that sentence difficulty is a major factor for example sentence quality. Kathuria and Shirai (2012) explore the use of disambiguated example sentences in a reading assistant system for Japanese learners. They create a system that assists reading by showing disambiguated example sentences that have the same sense as the word in the text. Huang et al. (2016) have used neural network models to show example sentences which would help disambiguate close synonyms. However, this work does not try to extract globally diverse example sentences which cover the usage of a target word. The DPP itself (Kulesza and Taskar, 2012) was used for document summarization by selecting sentences from a text and showing a diverse image search result tasks. We use several tricks from the former application. Related Work There exist human-curated databases of example sentences. Dictionaries contain example sentences which explain word usage, but usually those are fragme"
W17-5014,W09-4407,0,0.0685219,"Missing"
W17-5014,N13-1090,0,0.0181321,"btrees. Bunsetsu can contain compound nouns like “参政権” (a right to vote) or “積み上 げる” (to place on top of something) which are analyzed to consist of two lexical units. Grammatically, they are not much different from single unit words. This step ensures that sentences containing both several-unit and single-unit words are still going to be structurally similar. A semantic similarity score should be higher if the target word is used in the same or a close sense. For computing semantic similarity from a context we use prototype projections (Tsubaki et al., 2013) on word2vec word representations (Mikolov et al., 2013). Prototype projections assume that for triples of (A, relation, B) there exist prototypes in the form of frequently occurring and semantically related groups words at the end of each relation. For example, it is possible to run company, business or marathon. The computed representation makes it possible to distinguish between the distant senses. For a given triple (e.g run, object, marathon), you compute frequently occurring words of run and marathon over the same relation and compute SVD in each group. The top n right singular vectors in each end of the relation form a prototype subspace, an"
W17-5014,shinnou-sasaki-2008-division,0,0.0372877,"inition, (c) readable, meaning intelligible to learners, avoiding difficult words, anaphora and other structures that makes it difficult to understand a sentence without access to wider context. Sentence length, word frequency, information about the presence of pronouns and some other heuristics were used to judge the quality of sentences. Subsequently, the final example sentences for the dictionary were manually selected by editors. There are numerous works which approach the problem of selecting example sentences mostly as a word sense disambiguation (WSD) problem (de Melo and Weikum, 2009; Shinnou and Sasaki, 2008; Kathuria and Shirai, 2012). Specifically, de Melo and Weikum (2009) proposed the use of parallel corpora to extract disambiguated sentences from an aligned subtitle database. One more important feature of that work is a concern about diversity of example sentences. They generate a set of 1,2,3-grams for each example sentence and use them for scoring example sentences, setting to zero scores for n-gram for the selected sentences. This approach used aligned corpora for WSD, which usually are small or belong to a specific domain, whereas example sentences should 2 5 Evaluation Evaluating the su"
W17-5014,D13-1014,0,0.0147997,"inally, the feature space is expanded by deriving new subtrees. Bunsetsu can contain compound nouns like “参政権” (a right to vote) or “積み上 げる” (to place on top of something) which are analyzed to consist of two lexical units. Grammatically, they are not much different from single unit words. This step ensures that sentences containing both several-unit and single-unit words are still going to be structurally similar. A semantic similarity score should be higher if the target word is used in the same or a close sense. For computing semantic similarity from a context we use prototype projections (Tsubaki et al., 2013) on word2vec word representations (Mikolov et al., 2013). Prototype projections assume that for triples of (A, relation, B) there exist prototypes in the form of frequently occurring and semantically related groups words at the end of each relation. For example, it is possible to run company, business or marathon. The computed representation makes it possible to distinguish between the distant senses. For a given triple (e.g run, object, marathon), you compute frequently occurring words of run and marathon over the same relation and compute SVD in each group. The top n right singular vectors i"
W17-5701,W17-5715,0,0.0507339,"Missing"
W17-5701,W04-3250,0,0.243434,"Missing"
W17-5701,W17-5714,1,0.815327,"Missing"
W17-5701,P07-2045,0,0.0182137,"f a hierarchical phrase-based SMT system, a string-to-tree syntax-based SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. We also experimentally produced results for the baseline systems that consisted of an neural machine translation system using the implementation of (Vaswani et al., 2017). The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page8 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 6. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 8 h"
W17-5701,W17-5710,0,0.0950012,"Missing"
W17-5701,P13-2121,0,0.0263037,"Missing"
W17-5701,2009.iwslt-papers.4,0,0.017237,"ase-based SMT system, a string-to-tree syntax-based SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. We also experimentally produced results for the baseline systems that consisted of an neural machine translation system using the implementation of (Vaswani et al., 2017). The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page8 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 6. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 8 http://lotus.kuee.kyot"
W17-5701,W17-5709,0,0.0346313,"Missing"
W17-5701,W17-5711,0,0.0304192,"Missing"
W17-5701,W17-5716,0,0.0397813,"Missing"
W17-5701,D10-1092,0,0.0485922,"GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. Phrase-based SMT We used the following Moses configuration for the phrase-based SMT system. The default values were used for the other system parameters. • distortion-limit – 20 for JE, EJ, JC, and CJ – 0 for JK, KJ, HE, and EH – 6 for IE and EI • msd-bidirectional-fe lexicalized reordering 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl which was distributed 9 http://nlp.ist.i.kyotou.ac.jp/EN/index.php?JUMAN 10 http://nlp.stanford.edu/software/segmenter.shtml 11 https://bitbucket.org/eunjeon/mecab-ko/ 12 https://bitbucket.org/anoopk/indic_nlp_library 6 with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4 13 . AMFM scores were calculated using scripts created by the technical collaborators of WAT2017. All scores for each task were calculated using the corresponding reference. Before the calculat"
W17-5701,W17-5706,0,0.0506466,"Missing"
W17-5701,W17-5713,0,0.03618,"Missing"
W17-5701,W14-7001,1,0.898058,"ic evaluation server, and selected submissions were manually evaluated. 1 Sadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. There is no deadline of translation result submission with respect to automatic evaluation of translation quality and WAT receives submissions at any time. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014), WAT2015 (Nakazawa et al., 2015) and WAT2016 (Nakazawa et al., 2016), WAT2017 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 4th WAT, we adopted new translation subtasks with English-Japanese news corpus and English-Japanese recipe corpus in addition to the subtasks at WAT2016 1 . Fur• Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Ko"
W17-5701,2007.mtsummit-papers.63,0,0.586487,"nstitute of Information and Communications Technology (NICT). The corpus consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for J↔E subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for J↔C subtasks. The statistics for each corpus are shown in Table 1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million Japanese-English scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence 2.2 JPC JPC was constructed by the Japan Patent Office (JPO). The corpus consists of ChineseJapanese patent description corpus (JPC-CJ), Korean-Japanese patent description corpus (JPC-KJ) and English-Japanese patent description corpus (JPC-EJ) with the sections of Chemistry, Electricity, Mechanical engineering, and Physics on the basis of International Patent Classification (IPC). Each corpus is partitioned into training, development, development-test and test data. This corpus is used for patent subtasks C↔J, K↔J and E↔J. The statistics for each corpus are shown 2 http://lotus"
W17-5701,W15-5001,1,0.885412,"d submissions were manually evaluated. 1 Sadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. There is no deadline of translation result submission with respect to automatic evaluation of translation quality and WAT receives submissions at any time. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014 (Nakazawa et al., 2014), WAT2015 (Nakazawa et al., 2015) and WAT2016 (Nakazawa et al., 2016), WAT2017 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 4th WAT, we adopted new translation subtasks with English-Japanese news corpus and English-Japanese recipe corpus in addition to the subtasks at WAT2016 1 . Fur• Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Korean↔Japanese language pairs. In"
W17-5701,W17-5707,0,0.110884,"Missing"
W17-5701,W17-5708,0,0.0494718,"Missing"
W17-5701,P11-2093,1,0.766986,"ecab-ko/ 12 https://bitbucket.org/anoopk/indic_nlp_library 6 with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4 13 . AMFM scores were calculated using scripts created by the technical collaborators of WAT2017. All scores for each task were calculated using the corresponding reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 14 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0 15 . For Chinese segmentation, we used two different tools: KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 16 . For Korean segmentation we used mecab-ko 17 . For English segmentation, we used tokenizer.perl 18 in the Moses toolkit. For Hindi segmentation, we used Indic NLP Library 19 . The detailed procedures for the automatic evaluation are shown on the WAT2017 evaluation web page 20 . 4.2 N"
W17-5701,W17-5712,1,0.882844,"Missing"
W17-5701,P02-1040,0,0.105207,"n = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords. Phrase-based SMT We used the following Moses configuration for the phrase-based SMT system. The default values were used for the other system parameters. • distortion-limit – 20 for JE, EJ, JC, and CJ – 0 for JK, KJ, HE, and EH – 6 for IE and EI • msd-bidirectional-fe lexicalized reordering 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl which was distributed 9 http://nlp.ist.i.kyotou.ac.jp/EN/index.php?JUMAN 10 http://nlp.stanford.edu/software/segmenter.shtml 11 https://bitbucket.org/eunjeon/mecab-ko/ 12 https://bitbucket.org/anoopk/indic_nlp_library 6 with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4 13 . AMFM scores were calculated using scripts created by the technical collaborators of WAT2017. All scores for each task were calculated using the corresponding"
W17-5701,P06-1055,0,0.0160054,"d SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. We also experimentally produced results for the baseline systems that consisted of an neural machine translation system using the implementation of (Vaswani et al., 2017). The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page8 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 6. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 8 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 4 5 ✓ ✓ ✓ ✓ ✓ ✓ ✓ JPC IITB JIJI RECIPE EJ JC CJ JK KJHE EH JE EJ JE EJ ✓ ✓ ✓ ✓"
W17-5714,C16-2064,1,0.872421,"Missing"
W17-5714,W16-4616,1,0.879551,"Missing"
W17-5714,W17-3206,0,0.0819212,". Which is why all results presented in this paper are related to the LSTM-based model. Such feed-forward models probably have high potentials for the future, as they are more computationally efficient and do obtain state-of-the-art results on certain language directions. But, currently, we do not find that they should be necessarily preferred to recurrent architectures. 2.2 Direct connection from previous word to attention model There is an interesting flaw in the original architecture of the model (as well as in the model described in (Bahdanau et al., 2015)). This is briefly mentioned3 in (Goto and Tanaka, 2017), but we will expand on the details a bit more here. The attention mechanism computes the current context using only the previous decoder state as input. But the previous decoder state has been itself computed before the previously generated target word was selected. Therefore, when computing the current context, the attention mechanism is totally unaware of the previously generated word. Intuitively, this seems wrong: the attention should certainly depend on the previously generated word. Therefore, we add another input to the attention model: the previous word embedding. To be precise, re-us"
W17-5714,W11-2123,0,0.0728444,"Missing"
W17-5714,W16-2316,0,0.0291316,"semble, having 3 layers on the encoder and 2 on the decoder. 3.5 Averaging and Ensembling It is well known that using an ensemble of several independently trained models can boost NMT performances by several BLEU points. We did this in the same way as was described in (Cromi`eres et al., 2016). On top of ensembling independently trained models, we had found it useful to also make an ensemble with the parameters of the same model corresponding respectively to the best loss, best dev BLEU and last obtained during the training process (a practice which we will call here selfensemble). Following (Junczys-Dowmunt et al., 2016), we tried to compute averaged parameters instead of ensembling models. We found this to work surprisingly well. We observed only nonsignificant BLEU drops (by about 0.1 BLEU). But with the benefit that the averaged model has the same time and space complexity as a single model, while an ensemble of N models has N times the time and space complexity of a single model. We therefore switched to this averaging approach instead of the self-ensemble approach10 . 4 Zh → Ja Submission 1 corresponds to an ensemble of 5 models, three of them having 2 layers for encoders and decoders, and two of them ha"
W17-5714,P15-1002,0,0.0345332,"or sizes shown here are the ones suggested in the original paper. We use this general architecture for our model, but the single LSTMs are replaced by stacks of LSTMs. We also add a connection from the target embedding to the attention model, as suggested by (Goto and Tanaka, 2017), which was not in the original model (see section 2.2) We used subword segmentation for all target languages, so as to reduce the target vocabulary size. This makes the translation process more efficient memory-wise and computation-wise, while mostly avoiding the need for unknown-word replacement tricks such as in (Luong et al., 2015). The subword segmentation was done using the BPE algorithm (Sennrich et al., 2015) 6 . For the Japanese-Chinese language pair, we learned a joint segmentation (as suggested in (Sennrich et al., 2015)). We used a character equivalence map (Chu et al., 2013) to maximize the number of common characters between Japanese and Chinese when learning the joint segmentation. The joint segmentation was aimed at producing a vocabulary size of about 40,000 words for both the source and target vocabulary. For the Japanese-English language pair, we did not use a joint segmentation. We created a BPE model of"
W17-5714,W16-4601,1,0.753152,"ne for NMT. Our Kyoto-NMT system largely relies on an implementation of this model, with small modifications. Kyoto-NMT is implemented using the Chainer1 toolkit (Tokui et al., 2015). We make this implementation available under a GPL license.2 Introduction This paper describes our experiments for the WAT 2017 shared translation task. For more details refer to the overview paper (Nakazawa et al., 2017). This translation task contains several subtasks, but we focused on the ASPEC dataset, for the Japanese-English and Japanese-Chinese language pairs. Following up on our findings during WAT 2016 (Nakazawa et al., 2016) that our Neural Machine Translation system yielded significantly better results than our Example-Based Machine Translation system, we only experimented with NMT this year. Our improvements are actually quite incremental, with only small changes in the model architectures, model sizes, training and decoding approaches. Together, these small changes, however, allow us to improve over our past year’s results by several BLEU points, leading to the best official results for the Japanese-Chinese pair. In terms of pairwise human evaluation scores we have the best official results for all language di"
W17-5714,C16-1029,1,0.799847,"of our models, as well as the pre-processing we applied to the data. where eij is the unnormalized attention coefficient on source word j when decoding target word at step i, si−1 is the decoder state at step i − 1, and hj is the encoding of source word j. The matrices Wa and Ua , and the vector va are the parameters of the alignment model. We replace this equation with: 3.1 Preprocessing As a first preprocessing step, English sentences were tokenized and lowercased. Both Japanese sentences and Chinese sentences were automatically segmented, respectively with JUMAN5 (Kurohashi, 1994) and SKP (Shen et al., 2016). eij = vaT tanh(Wa ·si−1 +Ua ·hj +Xa ·Ey−1 ) (2) 4 3 tion The author had previously mentioned this to us in private communications. 5 147 but see section 4.2.1 for our attempt at system combinahttp://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN Figure 1: The structure of a NMT system with attention, as described in (Bahdanau et al., 2015) (but with LSTMs instead of GRUs). The notation “<1000>” means a vector of size 1000. The vector sizes shown here are the ones suggested in the original paper. We use this general architecture for our model, but the single LSTMs are replaced by stacks of LSTMs."
W17-6301,Q13-1034,0,0.0388365,"Missing"
W17-6301,P12-1110,0,0.0284407,"is organized as follows. Section 2 summarizes previous joint models for morphological and dependency analysis. Section 3 describes our method for constructing lexical knowledge. Section 4 illustrates our idea and describes our joint analysis model in detail. Section 5 is devoted to our experiments. Finally, section 6 gives the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual dec"
W17-6301,D09-1060,0,0.076172,"Missing"
W17-6301,N06-1023,1,0.865037,"with Fujitsu Laboratories Ltd. 3 In this paper, we use the following abbreviations: NOM (nominative), ACC (accusative), DAT (dative), LOC (locative), ABL (ablative), and TOP (topic marker). 1 Proceedings of the 15th International Conference on Parsing Technologies, pages 1–10, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics mentation and POS tagging using recurrent neural network language models, but does not perform dependency parsing. We employ this morphological analyzer, JUMAN++4 , as a pre-processor to generate word lattice (described in Section 4.1). Kawahara and Kurohashi (2006) proposed a probabilistic model for Japanese dependency parsing and PAS analysis based on case frames automatically compiled from a large raw corpus, which are also used as a source of selectional preferences in our model (described in Section 3.1). Kudo and Matsumoto (2002), Sassano (2004), Iwatate (2012) and Yoshinaga and Kitsuregawa (2014) proposed supervised models for Japanese dependency parsing without using external knowledge sources. These models need a 1-best output of segmentation and POS tagging as an input, and are not a joint model of morphological analysis and dependency parsing."
W17-6301,1993.eamt-1.1,0,0.508974,"Missing"
W17-6301,P11-2124,0,0.0194077,"transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified"
W17-6301,kawahara-etal-2002-construction,1,0.312831,"Missing"
W17-6301,E14-1007,1,0.846844,"hich are distinguished for each predicate sense or usage. Although PropBank was elaborated by hand and does not have frequency information, we automatically compile large-scale case frames that reflect real predicate uses. Each predicate has several case frames that are semantically distinguished. Each case frame consists of case slots, each of which consists of word instances that can be filled. Examples of Japanese case frames are shown in Table 1. Case frames are the source of selectional preferences, which are compiled by aggregating PASs for each predicate usage. We adapted the method of Kawahara et al. (2014) to Japanese case frame compilation. They proposed an unsupervised method for compiling English case frames from a large raw corpus. The procedure for inducing case frames is as follows: instances necessity:297865, case:190109, · · · thing:40, me:29, trend:29, · · · <time>:398 interest:34236, confidence:21326, · · · point:702, way:490, me:442, · · · feeling:70, aspect:58, · · · possibility:121867 price:23, myself:20, you:18, · · · step:4, influence:4, · · · .. . person:57, I:13, · · · road:24236, trail:4066, · · · parking:175, station:88, · · · I:35, parade:27, · · · city:13548, town:5336, par"
W17-6301,P08-1043,0,0.0295009,"ts. Finally, section 6 gives the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word"
W17-6301,E09-1038,0,0.0184339,"the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS taggi"
W17-6301,P08-1068,0,0.0355769,"ich does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency parsing models using lexical knowledge, there have been semi-supervised approaches that use knowledge of word classes, lexical preferences or selectional preferences acquired from raw corpora (e.g., (van Noord, 2007; Koo et al., 2008; Chen et al., 2009; Zhou et al., 2011; Bansal and Klein, 2011)). However, these dependency parsing models cannot be applied to joint morphological and dependency analysis. For Japanese, Morita et al. (2015) proposed a morphological analyzer that jointly performs seg3 Lexical Knowledge Acquisition In our joint analysis model, we use the following three types of lexical knowledge automatically acquired from a large raw corpus: case frames, cooccurrence probabilities of noun-noun / predicatepredicate dependencies, and word embeddings. We deeply utilize case frames in our joint model 4 http://nlp"
W17-6301,W02-2016,0,0.0969971,"Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics mentation and POS tagging using recurrent neural network language models, but does not perform dependency parsing. We employ this morphological analyzer, JUMAN++4 , as a pre-processor to generate word lattice (described in Section 4.1). Kawahara and Kurohashi (2006) proposed a probabilistic model for Japanese dependency parsing and PAS analysis based on case frames automatically compiled from a large raw corpus, which are also used as a source of selectional preferences in our model (described in Section 3.1). Kudo and Matsumoto (2002), Sassano (2004), Iwatate (2012) and Yoshinaga and Kitsuregawa (2014) proposed supervised models for Japanese dependency parsing without using external knowledge sources. These models need a 1-best output of segmentation and POS tagging as an input, and are not a joint model of morphological analysis and dependency parsing. We adopt KNP5 and CaboCha6 as baseline dependency parsers, which are implementations of Kawahara and Kurohashi (2006) and Sassano (2004), respectively.7 Tawara et al. (2015) proposed a joint model for Japanese morphological analysis and dependency parsing without lexical kn"
W17-6301,C10-1045,0,0.0413105,"Missing"
W17-6301,P14-1125,0,0.0205822,"ws. Section 2 summarizes previous joint models for morphological and dependency analysis. Section 3 describes our method for constructing lexical knowledge. Section 4 illustrates our idea and describes our joint analysis model in detail. Section 5 is devoted to our experiments. Finally, section 6 gives the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the ra"
W17-6301,J05-1004,0,0.0199068,"technique is complementary to our model and can be incorporated into our model in the future. 2 CS が ある:1 に (exist:1) time が ある:2 に (exist:2) で が ある:3 に (exist:3) で .. .. . . が あるく:1 を (walk:1) から が あるく:2 を (walk:2) で が あるく:3 を (walk:3) で .. .. . . and also consider these resources as features in our scoring function described in Section 4.2. Below, we describe the methods for constructing each of the resources, which are basically based on previous work. 3.1 Case Frames We use case frames to evaluate the plausibility of PASs. Case frames are predicate-specific semantic frames like PropBank (Palmer et al., 2005), which are distinguished for each predicate sense or usage. Although PropBank was elaborated by hand and does not have frequency information, we automatically compile large-scale case frames that reflect real predicate uses. Each predicate has several case frames that are semantically distinguished. Each case frame consists of case slots, each of which consists of word instances that can be filled. Examples of Japanese case frames are shown in Table 1. Case frames are the source of selectional preferences, which are compiled by aggregating PASs for each predicate usage. We adapted the method"
W17-6301,N15-1005,0,0.0130215,"POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency parsing models using lexica"
W17-6301,D12-1046,0,0.0221564,"n proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency parsing models using lexical knowledge, there have been semi-supervised approaches that use knowledge of word classes, lexical preferences or selectional preferences acquired from raw corpora (e.g., (van Noord, 2007; Koo et al., 2008; Chen et al., 2009; Zhou et al., 2011; Bansal and Klein, 2011)). However, these dependency p"
W17-6301,P11-1156,0,0.0181279,"erences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency parsing models using lexical knowledge, there have been semi-supervised approaches that use knowledge of word classes, lexical preferences or selectional preferences acquired from raw corpora (e.g., (van Noord, 2007; Koo et al., 2008; Chen et al., 2009; Zhou et al., 2011; Bansal and Klein, 2011)). However, these dependency parsing models cannot be applied to joint morphological and dependency analysis. For Japanese, Morita et al. (2015) proposed a morphological analyzer that jointly performs seg3 Lexical Knowledge Acquisition In our joint analysis model, we use the following three types of lexical knowledge automatically acquired from a large raw corpus: case frames, cooccurrence probabilities of noun-noun / predicatepredicate dependencies, and word embeddings. We deeply utilize case frames in our joint model 4 http://nlp.ist.i.kyoto-u.ac.jp/EN/?JUMAN++ http:"
W17-6301,C04-1002,0,0.215265,". 2017 Association for Computational Linguistics mentation and POS tagging using recurrent neural network language models, but does not perform dependency parsing. We employ this morphological analyzer, JUMAN++4 , as a pre-processor to generate word lattice (described in Section 4.1). Kawahara and Kurohashi (2006) proposed a probabilistic model for Japanese dependency parsing and PAS analysis based on case frames automatically compiled from a large raw corpus, which are also used as a source of selectional preferences in our model (described in Section 3.1). Kudo and Matsumoto (2002), Sassano (2004), Iwatate (2012) and Yoshinaga and Kitsuregawa (2014) proposed supervised models for Japanese dependency parsing without using external knowledge sources. These models need a 1-best output of segmentation and POS tagging as an input, and are not a joint model of morphological analysis and dependency parsing. We adopt KNP5 and CaboCha6 as baseline dependency parsers, which are implementations of Kawahara and Kurohashi (2006) and Sassano (2004), respectively.7 Tawara et al. (2015) proposed a joint model for Japanese morphological analysis and dependency parsing without lexical knowledge. However"
W17-6301,W07-2201,0,0.0920682,"Missing"
W17-6301,P14-1069,0,0.0278021,"nowledge which includes selectional preferences. pipeline models. The remainder of this paper is organized as follows. Section 2 summarizes previous joint models for morphological and dependency analysis. Section 3 describes our method for constructing lexical knowledge. Section 4 illustrates our idea and describes our joint analysis model in detail. Section 5 is devoted to our experiments. Finally, section 6 gives the conclusions. 2 Related Work Some variants of transition-based parsing methods have been proposed for joint POS tagging and parsing (Bohnet and Nivre, 2012; Bohnet et al., 2013; Wang and Xue, 2014) and joint Chinese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice,"
W17-6301,P13-2110,0,0.0238419,"ese word segmentation, POS tagging, and dependency parsing (Hatori et al., 2012; Zhang et al., 2014). As an external knowledge source, Hatori et al. (2012) used a word dictionary extracted mainly from Wikipedia, but it did not provide lexical knowledge for resolving syntactic ambiguities. Lattice parsing methods have been proposed for Hebrew and Arabic (Goldberg and Tsarfaty, 2008; Goldberg et al., 2009; Green and Manning, 2010; Goldberg and Elhadad, 2011). These methods first generate a word lattice and then apply PCFG parsing to the word lattice. Starting with a word lattice, the methods of Wang et al. (2013) and Zhang et al. (2015) select the best parse using dual decomposition and the randomized greedy algorithm, respectively. Of these methods, Goldberg et al. (2009) incorporated an external morphological lexicon, which does not provide selectional preferences. As a different method from lattice parsing, Qian and Liu (2012) trained separate models for Chinese word segmentation, POS tagging, and constituency parsing. They proposed a unified decoding algorithm that combines the scores from these three models. This is a purely supervised method that does not use lexical knowledge. As dependency par"
W17-6301,C14-1103,0,0.0286068,"Missing"
W17-6301,D12-1133,0,\N,Missing
W17-6301,P11-1070,0,\N,Missing
W17-6301,D15-1276,1,\N,Missing
W17-6301,Y12-1058,1,\N,Missing
W19-5312,E14-2008,0,0.0682749,"Missing"
W19-5312,P16-1009,0,0.0699283,"Missing"
W19-5312,W18-1819,0,0.0295581,"hine Translation system. We make use of the Tensor2Tensor implementation of the Transformer model. After carefully cleaning the data and noting the importance of the good use of recent monolingual data for the task, we obtain our final result by combining the output of a diverse set of trained models through the use of their ”checkpoint agreement”. 1 Sadao Kurohashi Graduate School of Informatics Kyoto University kuro@i.kyoto-u.ac.jp 2 Basic setting All of our experiments are based on the Transformer sequence-to-sequence model (Vaswani et al., 2017). We used the Tensor2Tensor implementation1 (Vaswani et al., 2018). For hyperparameters, we used the predefined ”big” setting of Tensor2Tensor: • 6 layers for the encoder Introduction • 6 layers for the decoder The 2019 edition of WMT’s news translation shared tasks was proposing the German-French pair for the first time. The inclusion of two not-soclosely related languages which both have a richer morphology than English is interesting and can in theory provide additional challenges to the more English-X pairs most frequently used for Machine Translation. Due to the rather large computation time investment required by the training of a modern Neural Machine"
W19-6704,H92-1086,0,0.135738,"-items, we conducted a websurvey and statistical analysis to identify the fivefactor structure, calculate reliability, and examine validity. Exploratory Factor Analysis (EFA) is a statistical approach to extract common factors across measured variables based on correlation coefficients (Fabrigar et al., 1999). In constructing a psychological questionnaire, it is important to evaluate reliability and validity. Reliability indicates how responses are reliably produced. Internal consistency assumes that a person tends to similarly answer items within the same trait, which Cronbach’s α indicates (Cronbach, 1951). Furthermore, a psychological questionnaire must measure the targeted concepts, which is named as validity. One method to assess validity is criterion-validity. Criterion-validity investigates correlations between the latent variables in the newly constructed questionnaire and the corresponding latent variables in a “criterion” questionnaire. The correlations are expected to be high between similar latent variables and low between unrelated latent variables. We use TIPI-J (Oshio et al., 2012) for this validity evaluation. 4.1 Web-Survey Participants: We conducted a web-survey on registrants o"
W98-0605,W97-0109,1,\N,Missing
W98-0701,C96-1005,0,0.0763487,"Missing"
W98-0701,J96-1002,0,0.00360777,"(lo)g i = 2--, h~ L j / L represents the score of the head word sense j based on the matrices Q calculated in the step 1., i.e.: (ll) Lj = ~ maz(zi(j, u)) i=I I where x i ( j , u ) E Qi and m a x ( x i ( j , u ) ) highest score in the line of the matrix Qi corresponds to the head word sense j. n number of modifiers of the head word h current tree level, and is the which is the at the k i Lj = j~l Lj where k is the number of senses of the head word h. I i I I The reason why gj (I0) is calculated as a sum of the best scores (ll), rather than by using the traditional maximum likelihood estimate (Berger et al., 1996)(Gah eta[., 1993), is to minimise the effectof the sparse data problem. Imagine, for example, the phrase VP-- VB NP PP, where the head verb V B is in the object relation with the head of the noun phrase NP and also in the modifying relation with the head of the prepositional p h r ~ e PP. Let us also assume that the correct sense of the verb VB is a. Even if the verb-object relation provided a strong selectional support for the sense a, if there was no example in the training set for the second relation (between VB and PP) which would score a hit for the sense a, multiplying the scores of that"
W98-0701,P96-1025,0,0.0785509,"Missing"
W98-0701,H94-1052,0,0.018234,"Missing"
W98-0701,W96-0104,0,0.022059,"Missing"
W98-0701,P95-1037,0,0.0581033,"Missing"
W98-0701,J93-2004,0,0.028952,"Missing"
W98-0701,H93-1061,0,0.112636,"e # of senses 5.4 10.5 5.5 3.? 5.8 The Task Specification For our work, we used the word sense definitions as given in WordNet (Miller, 1990), which is comparable to a good printed dictionary in its coverage and distinction of senses. Since WordNet only provides definitions for content words (nouns, verbs, adjectives and adverbs), we are only concerned with identifying the correct senses of the content words. Both for the training and for the testing of our algorithm, we used the syntactically analysed sentences of the Brown Corpus (Marcus, 1993), which have been manually semantically tagged (Miller et al., 1993) into semantic concordance files (SemCor). These files combine 103 passages of the Brown Corpus with the WordNet lexical database in such a way that every content word in the text carries both a syntactic tag and a semantic tag pointing to the appropriate sense of that word in WordNet. Passages in the Brown Corpus are approximately 2,000 words long, and each contains approximately 1,000 content words. The percentages of the nouns, verbs, adjectives and adverbs in the semantically tagged corpus, together with theiraverage number of Word Net senses, are given in Table I. Although most of the wor"
W98-0701,H93-1054,0,0.0420515,"Missing"
W98-0701,W95-0105,0,0.0306191,"Missing"
W98-0701,P95-1026,0,0.445582,"Missing"
W98-0701,W97-0109,1,\N,Missing
W98-0701,W96-0208,0,\N,Missing
W98-0701,H94-1046,0,\N,Missing
W98-0701,H93-1052,0,\N,Missing
W98-0701,P94-1020,0,\N,Missing
W98-0701,P92-1032,0,\N,Missing
W98-0701,P91-1034,0,\N,Missing
W98-0701,P91-1017,0,\N,Missing
W98-0701,P95-1025,0,\N,Missing
Y01-1017,J95-2003,0,0.131016,"enerated, the system will go to strategy 3. Entity nodes of the first sentence Tom The second sentence Canada Figure 5: Generation strategy 2. 191 Strategy 3 The system selects an entity node from the unused entity nodes randomly, and uses this node as input to generate the second sentence. At this time, there is no connection between the generated sentence and its preceding one, as Figure 6 shows: Tom Canada Figure 6: Generation strategy 3. 3.2.2 Attentional state of the readers Text structure influences on attentional state of the readers. First, let us analyze the following two discourses (Grosz et al., 1995): (1) a. John went to his favorite music store to buy a piano. b. He had frequented the store for many years. c. He was excited that he could finally buy a piano. d. He arrived just as the store was closing for the day. (2) a. John went to his favorite music store to buy a piano. b. It was a store John had frequented for many years. c. He was excited that he could finally buy a piano. d. It was closing just as John arrived. Discourse(1) is more coherent than Discourse(2), though they convey the same information. The main reason that contributes to the difference between these two discourses is"
Y12-1033,P05-1022,0,0.845516,"plore more expressive features and have proven significant improvement on parsing accuracy. However, the power of higherorder models comes with the cost of expensive computation and sometimes it requires aggressive pruning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007; Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extr"
Y12-1033,W02-1001,0,0.0495527,"a weight vector : ( ) ( ). (3) The feature vector is defined on the factor which means it is only able to capture tree-structure information from a small context. This can be seen as the off-set for performing exact inference. The goal of training a parser is to learn a weight vector that assigns scores to effectively discriminate good parses from bad parses. We use the edge factorization and the sibling factorization models described in (McDonald et al., 2005; McDonald and Pereira, 2006) to construct our base parsers. We learn the weight vector by applying the averaged perceptron algorithm (Collins, 2002) for its efficiency and stable performance. An illustration for generic perceptron algorithm is shown in Pseudocode 1. Pseudocode 1: Generic perceptron learning ) 1 for training data ( 2 for iteration 3  ( ) ( ) 4 if  5 ( ) ( ) 6 end 7 End 3. Parse Reranking In this section, we describe our reranking approach and introduce the feature set consists of three different types. 3.1 Overview of Parse Reranking The task of reranking is similar with that of parsing instead of that the searching of parse tree is performed on a K-best list with selected parse candidates rather than the entire search"
Y12-1033,D09-1060,0,0.0178368,"ollecting linguistic evidence, and efficient feature back-off strategy is proposed to relieve data sparsity. Through experiment we confirmed the effectiveness and efficiency of our method, and observed significant 316 improvement over the base system as well as other known systems. To further improve the proposed method, we mention several possibilities for our future work. An advantage of the reranking framework we used is that it has no overlap with many of the semisupervised parsing methods, such as word clustering (Koo et al., 2008) and subtree features integration using auto-parsed data (Chen et al., 2009). We are interested in the performance of our system when combining with these methods. Another interesting approach is to incorporate information from large-scale structured data, such as case frame (Kawahara and Kurohashi, 2006), which provides lexical predicate-argument selection preference and is an effective way to help to overcome data sparse problem in discriminative learning. While the relatively complex data structure in the case frame prohibits its incorporation in any existing factorization methods, it can be well utilized in the reranking framework with the proposed feature set. Re"
Y12-1033,C96-1058,0,0.00888878,"re unbalanced. Typically, when computing the trimmed subtree features, a candidate parse with most nodes being leaves will provide little information except on the root node, while on another parse that has fewer leaves and more depth we can have a bunch of features that give more information. This defect makes the comparison between candidates be “unfair” and thus less reliable. Therefore, it is natural to raise the question the other way round—whether a node is a good head for a subtree. To answer this question, we consider a dynamic programming structure called complete span introduced in (Eisner, 1996). A complete span consists of a head node and all its descendants on one side, which can also be transfer money from S2 to funds funds … … S1 … … S0 Figure 4. A complete span for the clause “transfer money from the new funds to other investment funds” where we omitted some of the details. This structure functions as a relatively independent and complete component in the entire parse tree. Features are encoded over the tuples: &lt;transfer, ,s2&gt;, &lt;transfer, s2,s1&gt;, &lt;transfer, s1,s0&gt;, &lt;transfer, s0,-&gt;. considered as a head node and sibling subtrees shown in Figure 4. In our observation, a complete"
Y12-1033,P07-1050,0,0.0849741,"uning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007; Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extracted from the candidate parse with arbitrary 308 Copyright 2012 by Mo Shen, Daisuke Kawahara, and Sadao Kurohashi Copyright 2012 by Mo Shen, Daisuke Kawahara, and Sadao Kurohashi 26th Pacific Asia Conf"
Y12-1033,D11-1137,0,0.0381078,"Missing"
Y12-1033,W05-1506,0,0.0384085,"ive computation and sometimes it requires aggressive pruning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007; Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extracted from the candidate parse with arbitrary 308 Copyright 2012 by Mo Shen, Daisuke Kawahara, and Sadao Kurohashi Copyright 2012 by Mo Shen, Daisuke Kawahara,"
Y12-1033,P08-1067,0,0.324314,"es and have proven significant improvement on parsing accuracy. However, the power of higherorder models comes with the cost of expensive computation and sometimes it requires aggressive pruning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007; Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extracted from the"
Y12-1033,kawahara-kurohashi-2006-case,1,0.871017,"Missing"
Y12-1033,P10-1001,0,0.144659,"tructures to perform an efficient dynamic programming search. This treatment however restricts the representation of features to in a local context which can be, for example, single edges or adjacent edges. Such restriction prohibits the model from exploring large or complex structures for linguistic evidence, which can be considered as the major drawback of the graphbased approach. Attempts have been made in developing more complex factorization techniques and corresponding decoding methods. Higher-order models that use grand-child, grand-sibling or trisibling factorization were proposed in (Koo and Collins, 2010) to explore more expressive features and have proven significant improvement on parsing accuracy. However, the power of higherorder models comes with the cost of expensive computation and sometimes it requires aggressive pruning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins,"
Y12-1033,D10-1004,0,0.029001,"Missing"
Y12-1033,P05-1012,0,0.676516,"ubtrees, and can be encoded efficiently. It exhaustively explores a candidate parse tree for features from the most simple to the most expressive while maintaining the efficiency in the sense that it does not add additional complexities over the K-best parsing. We choose the K-best list reranking framework rather than the forest reranking in (Huang, 2008) because an explicit representation of parse trees is needed in order to compute the features for reranking. We implemented an edge-factored parser and a second-order sibling-factored parser which emulate models in the MSTParser described in (McDonald et al., 2005; McDonald and Pereira, 2006) as our base parsers. In the rest part of this paper, we first give a brief description of the dependency parsing, then we describe the feature set for reranking, which is the major contribution of this paper. Finally, we present a set of experiment for the evaluation of our method. 2. Dependency Parsing The task of dependency parsing is to find a tree structure for a sentence in which edges represent the head-modifier relationship between words: each word is linked to a unique “head” such that the link forms a semantic dependency while the main predicate of the se"
Y12-1033,E06-1011,0,0.194514,"oded efficiently. It exhaustively explores a candidate parse tree for features from the most simple to the most expressive while maintaining the efficiency in the sense that it does not add additional complexities over the K-best parsing. We choose the K-best list reranking framework rather than the forest reranking in (Huang, 2008) because an explicit representation of parse trees is needed in order to compute the features for reranking. We implemented an edge-factored parser and a second-order sibling-factored parser which emulate models in the MSTParser described in (McDonald et al., 2005; McDonald and Pereira, 2006) as our base parsers. In the rest part of this paper, we first give a brief description of the dependency parsing, then we describe the feature set for reranking, which is the major contribution of this paper. Finally, we present a set of experiment for the evaluation of our method. 2. Dependency Parsing The task of dependency parsing is to find a tree structure for a sentence in which edges represent the head-modifier relationship between words: each word is linked to a unique “head” such that the link forms a semantic dependency while the main predicate of the sentence is linked to a dummy “"
Y12-1033,W96-0213,0,0.336353,"of branch of a node from its head. 4. Evaluation We present our experimental results on two languages, English and Chinese. For English experiment, we use the Penn Treebank WSJ part. We convert the constituent structure in the Treebank into dependency structure with the tool Penn2Malt and the head-extraction rule identical with that in (Yamada and Matsumoto, 2003). To align with previous work, we use the standard data division: section 02-21 for training, section 24 for development, and section 23 for testing. As our system assumes Part-of-Speech tags as input, we use MXPOST, a MaxEnt tagger (Ratnaparkhi, 1996) to automatically tag the test data. The tagger is trained on the same training data. For Chinese, we use the Chinese Treebank 5.0 with the following data division: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We use Penn2Malt to convert the Treebank into dependency structure and the set of head-extraction rules for Chinese is identical with the one in (Zhang and Clark, 2008). Moreover, for Chinese we use the gold standard Part-of-Speech tags in evaluation. We apply unlabeled attachment score (UAS) to measure the effectiveness of our"
Y12-1033,W09-3839,0,0.0531774,"Missing"
Y12-1033,D09-1058,0,0.013012,"parser in the training. It is much slower than the base parser in parsing new sentences, which is mainly due to the time required for outputting the 50-best candidates list; this can be seen as an unavoidable trade-off to obtain high accuracy in the reranking framework. 5. Related Work McDonald (2005, 2006) proposed an edge-factored parser and a second-order parser that both trained by discriminative online learning methods. Huang (2005) proposed the efficient algorithm for produce K-best list for graph-based parsers, which add a factor of to the parsing complexity of the base parser. Sangati (2009) has shown that a discriminative parser is very effective at filtering out bad parses from a factorized search space which agreed with the conclusion in (Hall, 2007) that an edge-factored model can reach good oracle performance when generating relatively small Kbest list. Successful results have been reported for constituent parse reranking in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008), in which feature sets defined on constituent parses have been proposed that are able to capture rich non-local information. These feature sets, however, cannot be directly applied to parse tree un"
Y12-1033,W03-3023,0,0.0629629,"in each time, while for all nodes we encode their grammatical and positional information. Thus for the subtree (e) in Figure 5, a feature can appear as: 〈( s w )( )( )( )〉 A binary value, here we denote as “left” and “right”, is used to indicate the direction of branch of a node from its head. 4. Evaluation We present our experimental results on two languages, English and Chinese. For English experiment, we use the Penn Treebank WSJ part. We convert the constituent structure in the Treebank into dependency structure with the tool Penn2Malt and the head-extraction rule identical with that in (Yamada and Matsumoto, 2003). To align with previous work, we use the standard data division: section 02-21 for training, section 24 for development, and section 23 for testing. As our system assumes Part-of-Speech tags as input, we use MXPOST, a MaxEnt tagger (Ratnaparkhi, 1996) to automatically tag the test data. The tagger is trained on the same training data. For Chinese, we use the Chinese Treebank 5.0 with the following data division: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We use Penn2Malt to convert the Treebank into dependency structure and the se"
Y12-1033,C08-1132,1,0.900961,"Missing"
Y12-1033,D08-1059,0,0.0428817,"tion 02-21 for training, section 24 for development, and section 23 for testing. As our system assumes Part-of-Speech tags as input, we use MXPOST, a MaxEnt tagger (Ratnaparkhi, 1996) to automatically tag the test data. The tagger is trained on the same training data. For Chinese, we use the Chinese Treebank 5.0 with the following data division: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We use Penn2Malt to convert the Treebank into dependency structure and the set of head-extraction rules for Chinese is identical with the one in (Zhang and Clark, 2008). Moreover, for Chinese we use the gold standard Part-of-Speech tags in evaluation. We apply unlabeled attachment score (UAS) to measure the effectiveness of our method, which is the percentage of words that correctly identified their heads. For all experiments conducted, we use the parameters tuned in the development set. We train two base parsers which are the reimplementation of the first-order and second-order parsers in the MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006) with 10 iterations on English and Chinese training dataset. We use 30way cross-validation on the identica"
Y12-1033,P11-2033,0,\N,Missing
Y12-1033,P08-1068,0,\N,Missing
Y12-1058,W07-1522,0,0.0901418,"pus that consists of document leads, we expect to raise the accuracy of the analysis of both document leads and the document as a whole. In this paper, we describe related work in Section 2. We describe the documents that the corpus consists of in Section 3 and the annotation criteria in Section 4. In Section 5 we discuss the statistics and properties of the corpus and conclude in Section 6. 536 2 Related Work Existing corpora that are annotated with predicateargument structures and anaphoric relations include the Kyoto University Text Corpus (Kawahara et al., 2002) and the Naist Text Corpus (Iida et al., 2007). These corpora are based on Mainich Newspaper articles from 1995 and annotated with predicateargument structures and anaphoric relations. Since there are only reports and editorial articles in the newspaper, the writing styles are consistent, making it difficult to adapt a semantic analysis system based on this corpus to texts other than newspaper articles. Corpora that consist of documents from various genres include the Balanced Corpus of Contemporary Written Japanese (BCCWJ)2 . BCCWJ includes publications such as books and magazines and text from the Internet. BCCWJ has publications form v"
Y12-1058,kawahara-etal-2002-construction,1,0.85845,"agate to the following analyses. By building a corpus that consists of document leads, we expect to raise the accuracy of the analysis of both document leads and the document as a whole. In this paper, we describe related work in Section 2. We describe the documents that the corpus consists of in Section 3 and the annotation criteria in Section 4. In Section 5 we discuss the statistics and properties of the corpus and conclude in Section 6. 536 2 Related Work Existing corpora that are annotated with predicateargument structures and anaphoric relations include the Kyoto University Text Corpus (Kawahara et al., 2002) and the Naist Text Corpus (Iida et al., 2007). These corpora are based on Mainich Newspaper articles from 1995 and annotated with predicateargument structures and anaphoric relations. Since there are only reports and editorial articles in the newspaper, the writing styles are consistent, making it difficult to adapt a semantic analysis system based on this corpus to texts other than newspaper articles. Corpora that consist of documents from various genres include the Balanced Corpus of Contemporary Written Japanese (BCCWJ)2 . BCCWJ includes publications such as books and magazines and text fr"
Y12-1058,rodriguez-etal-2010-anaphoric,0,0.0476145,"Missing"
Y12-1058,M95-1005,0,0.0202762,"phoric relations Table 8: Number of zero anaphora/exophora Author 602 8 78 23 711 Anaphora 2201 3185 757 6143 = NO  Total Author 100 256 31 387 Reader 29 96 24 149 Others 2072 2833 702 5607 Total 2201 3185 757 6143 Table 12: Breakdown of anaphoric relations indicates that most reference relations are anaphoric relations regardless of types. Since NO relations are more than , more bridging references can be rephrased as the form “A ͷ B.” The inter-annotator agreements are shown in Table 14 and Table 157 . Only the agreement of coreference, is annotated by “=,” is calculated by the MUC score (Vilain et al., 1995). For the agreement of other cases, we show only representative cases and “Total” includes cases that are omitted from the table. In Table 15, although the agreements of GA and WO are very high, that of GA2 is very low. It is because that GA2-case sometimes can be rephrased to other cases. For example, since it is possible to rephrase Example(11) to both (12) and (13), there are two annotation candidates, (11a) and (11b). We had set up a criterion that a case marker other than GA2 to which the target expression can be paraphrased is preferred to GA2. However, the judgment on such paraphrasing"
Y14-1032,N13-1075,0,0.210807,"262–271 !262 PACLIC 28 pairs to compare the similarity between them. Data sparseness makes the vector representations sparse (e.g., the vector of a low frequent word tends to have many zero entries), thus they do not always reliably represent the meanings of words. Therefore, the similarity of word pairs can be inaccurate. Smoothing technology has been proposed to address the data sparseness problem for BLE. Pekar et al. (2006) smooth the vectors of words with their distributional nearest neighbors, however distributional nearest neighbors can have different meanings and thus introduce noise. Andrade et al. (2013) use synonym sets in WordNet to smooth the vectors of words, however WordNet is not available for every language. More importantly, both studies work for words, which are not suitable for comparable feature estimation. The reason is that translation pairs can also be phrases (Koehn et al., 2003) or syntactic rules (Galley et al., 2004) etc., depending on what kind of SMT models we use. In this paper, we propose using paraphrases to address the data sparseness problem of BLE for comparable feature estimation. A paraphrase is a restatement of the meaning of a word, phrase or syntactic rule etc.,"
Y14-1032,P05-1074,0,0.0563113,"rase pairs more accurately. Finally, we compute the similarity of phrase pairs based on the smoothed source and target vectors. In this way, we improve the quality of comparable features, which can improve the accuracy of the phrase table thus improve SMT performance. Details of paraphrase generation, comparable feature estimation and vector smoothing with paraphrases will be described in Section 4.1, 4.2 and 4.3 respectively. 4.1 Paraphrase Generation In this paper, we generate both source and target phrasal level paraphrases from the parallel corpus used for SMT4 through bilingual pivoting (Bannard and Callison-Burch, 2005). The idea of this method is that if two source phrases f1 and f2 are translated to the same target phrase e, we can assume that f1 and f2 are a paraphrase pair. Probability of this paraphrase pair can be assigned by marginalizing over 4 Paraphrases also can be generated from external parallel corpora and monolingual corpora, however we leave it as future work. !265 all shared target translations e in the parallel corpus, defined as follows: p(f1 |f2 ) = ! e φ(f1 |e)φ(e|f2 ) (1) where, φ(f1 |e) and φ(e|f2 ) are phrase translation probability. Target paraphrases can be generated in a similar wa"
Y14-1032,N06-1003,0,0.0324651,"udi, 2011; Irvine et al., 2013). Moreover, studies have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a). Our study focuses on addressing the accuracy problem of SMT with BLE. We use paraphrases to address the data sparseness problem of BLE for comparable feature estimation, which makes the comparable features more accurate. 2.2 Paraphrases for SMT Many methods have been proposed to use paraphrases for SMT, mainly for the coverage problem. One method is paraphrasing unknown words or phrases in the translation model (Callison-Burch et al., 2006; Razmara et al., 2013; Marton et al., 2009). PACLIC 28 f 业 业 业 业 e unemployment figures number of unemployed . unemployment was unemployment and bringing φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and th"
Y14-1032,2012.eamt-1.7,1,0.851342,"vectors for the phrase “unemployment figures” before and after smoothing. 5 Experiments In our experiments, we compared our proposed method with (Klementiev et al., 2012). We estimated comparable features from comparable corpora using the method of (Klementiev et al., 2012) and our proposed method respectively. We appended the comparable features to the phrase table, and evaluated the two methods in the perspective of SMT performance. We conducted experiments on Chinese-English data. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012), and tokenizing English sentences. 5.1 Experimental Settings SMT Settings We conducted Chinese-to-English translation experiments. The parallel corpus we used is from Chinese-English NIST open MT.6 The “NIST” column of Table 4 shows the statistics of this parallel corpus. For decoding, we used the state-of-theart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012). We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit7 with interpolated Kneser-Ney discounting"
Y14-1032,P11-2071,0,0.0449146,"Missing"
Y14-1032,D10-1041,0,0.014692,"ng φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and their feature scores are incorrect. Another method is constructing a paraphrase lattice for the tuning and testing data, and performing lattice decoding (Du et al., 2010; Bar and Dershowitz, 2014). Paraphrases also can be incorporated as additional training data, which may improve both coverage and accuracy of SMT (Pal et al., 2014). Previous studies require external data in addition to the parallel corpus used for SMT for paraphrase generation to make their methods effective. These paraphrases can be generated from external parallel corpora (Callison-Burch et al., 2006; Du et al., 2010), or monolingual corpora based on distributional similarity (Marton et al., 2009; Razmara et al., 2013; Pal et al., 2014; Bar and Dershowitz, 2014). Our study differs from pre"
Y14-1032,N04-1035,0,0.0348047,"hnology has been proposed to address the data sparseness problem for BLE. Pekar et al. (2006) smooth the vectors of words with their distributional nearest neighbors, however distributional nearest neighbors can have different meanings and thus introduce noise. Andrade et al. (2013) use synonym sets in WordNet to smooth the vectors of words, however WordNet is not available for every language. More importantly, both studies work for words, which are not suitable for comparable feature estimation. The reason is that translation pairs can also be phrases (Koehn et al., 2003) or syntactic rules (Galley et al., 2004) etc., depending on what kind of SMT models we use. In this paper, we propose using paraphrases to address the data sparseness problem of BLE for comparable feature estimation. A paraphrase is a restatement of the meaning of a word, phrase or syntactic rule etc., therefore it is suitable for the data sparseness problem. We generate paraphrases from the parallel corpus used for translation model learning. Then, we use the paraphrases to smooth the vectors of the translation pairs in the translation model for comparable feature estimation. Smoothing is done by learning vectors that combine the v"
Y14-1032,W12-3134,0,0.0156949,"hrases 6,273 46,191 # paraphrases 39.8 21.6 # Bigrams w/ paraphrases 15,026 223,299 Avg # paraphrases 34.6 17.7 # Trigrams w/ paraphrases 5,419 185,609 # paraphrases 20.0 14.9 Table 6: Statistics the generated paraphrases for the phrases and individual words inside the phrases in the filtered phrase table. tered phrase table. We can see that each Chinese phrase has a large number of translations on average especially for the lower order n-gram phrases, which can indicate the inaccuracy of the filtered phrase table. Our proposed method requires paraphrases for vector smoothing. We used Joshua (Ganitkevitch et al., 2012) to generate both Chinese and English paraphrases from the parallel corpus. We kept the paraphrase pairs that satisfy logp(x1 |x2 ) &gt; −7 and logp(x2 |x1 ) &gt; −7 14 for smoothing, where p(x1 |x2 ) is the probability that x1 is a paraphrase of x2 , and p(x2 |x1 ) is the probability that x2 is a paraphrase of x1 . Table 6 shows the statistics of the paraphrase generation results for the Chinese and English phrases, and individual words inside the phrases in the filtered phrase table. Note that, for some phrase pairs, their comparable feature scores may be 0, because of data sparseness. In that cas"
Y14-1032,W09-1117,0,0.016764,"ntax-based context etc. In this paper, we use window-based context, and leave the comparison of using different definitions of context as future work. Given a phrase, we count all its immediate context words, with a window size of 4 (2 preceding words and 2 following words). We build a context by collecting the counts in a bag of words fashion, namely we do not distinguish the positions that the context words appear in. The number of dimensions of the constructed vector is equal to the vocabulary size. We further reweight each component in the vector by multiplying by the IDF score following (Garera et al., 2009; Chu et al., 2014), which is defined as follows: IDF (t, D) = log |D| 1 + |{d ∈ D : t ∈ d}| (2) where |D |is the total number of documents in the corpus, and |{d ∈ D : t ∈ d} |denotes number of documents where the term t appears.5 We model the source and target vectors using the method described above, and project the source vector onto the vector space of the target language using a seed dictionary. The contextual similarity of the phrase pair is the similarity of the vectors, which is computed using cosine similarity defined as follows: &quot;K Fk × Ek #&quot; K 2 2 k=1 (Fk ) × k=1 (Ek ) Cos(f, e) ="
Y14-1032,P08-1088,0,0.0286005,"ation. The con6 LDC2007T02, LDC2002T01, LDC2003T17, LDC2004T07, HK News part of LDC2004T08, LDC2005T10 and LDC2006T04 7 http://www.speech.sri.com/projects/srilm !268 # Zh articles # En articles # Zh sentences # En sentences # Zh tokens # En tokens NIST N/A N/A 991k 991k 26.1M 27.2M Gigaword 3.6M 4.3M 42.6M 56.9M 1.1B 1.3B Wikipedia 248k 248k 2.8M 10.1M 70.5M 240.5M Table 4: Statistics of the comparable data used for comparable feature estimation. textual feature was estimated on the parallel corpus. We treated the two sides of the parallel corpus as independent monolingual corpora, following (Haghighi et al., 2008; Klementiev et al., 2012). Contextual feature estimation requires a seed dictionary. The seed dictionary we used is NIST ChineseEnglish translation lexicon Version 3.0,8 containing 82k entries. The temporal feature was estimated on Chinese9 and English10 Gigaword version 5.0. We used the afp, cna and xin sections with date range 1994/05-2010/12 of the corpora. The topical feature was estimated on Chinese and English Wikipedia data. We downloaded Chinese11 (2012/09/21) and English12 (2012/10/01) Wikipedia database dumps. We used an open-source Python script13 to extract and clean the text from"
Y14-1032,W13-2233,0,0.23776,"ns.1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate. Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features (Klementiev et al., 2012; Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word 1 Scarceness of parallel corpora al"
Y14-1032,N13-1056,0,0.109849,"ns.1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate. Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features (Klementiev et al., 2012; Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word 1 Scarceness of parallel corpora al"
Y14-1032,D13-1109,0,0.0221904,"Missing"
Y14-1032,P06-1103,0,0.128113,"odels. !263 BLE (Klementiev et al., 2012; Irvine and CallisonBurch, 2013a). The results verify the effectiveness of using BLE together with paraphrases for the accuracy problem of SMT. 2 Related Work 2.1 Bilingual Lexicon Extraction (BLE) for SMT From the pioneering work of (Rapp, 1995), BLE from comparable corpora has been studied for a long time. BLE is based on the distributional hypothesis (Harris, 1954), stating that words with similar meaning have similar distributions across languages. Contextual similarity (Rapp, 1995), topical similarity (Vuli´c et al., 2011) and temporal similarity (Klementiev and Roth, 2006) can be important clues for BLE. Orthographic similarity may also be used for BLE for some similar language pairs (Koehn and Knight, 2002). Moreover, some studies try to use the combinations of different similarities for BLE (Irvine and Callison-Burch, 2013b; Chu et al., 2014). To address the data sparseness problem of BLE, smoothing technology has been proposed (Pekar et al., 2006; Andrade et al., 2013). BLE can be used to address the accuracy problem of SMT, which estimates comparable features for the translation pairs in the translation model (Klementiev et al., 2012). BLE also can be used"
Y14-1032,E12-1014,0,0.0677873,"language pairs and domains.1 Accuracy also can be improved by filtering out the noisy translation pairs from the translation model, however meanwhile we may lose some good translation pairs, thus the coverage of the translation model may decrease. A good solution to improve the accuracy while keeping the coverage is estimating new features for the translation pairs from comparable corpora (which we call comparable features), to make the translation model more discriminative thus more accurate. Previous studies use bilingual lexicon extraction (BLE) technology to estimate comparable features (Klementiev et al., 2012; Irvine and Callison-Burch, 2013a). They extend traditional BLE that estimates similarity for bilingual word pairs on comparable corpora, to translation pairs in the translation model of SMT. The similarity scores of the translation pairs are used as comparable features. These comparable features are combined with the original features used in SMT, which can provide additional information to distinguish good and bad translation pairs. A major problem of previous studies is that they do not deal with the data sparseness problem that BLE suffers from. BLE uses vector representations for word 1"
Y14-1032,W02-0902,0,0.136879,"Missing"
Y14-1032,N03-1017,0,0.120216,"d pairs can be inaccurate. Smoothing technology has been proposed to address the data sparseness problem for BLE. Pekar et al. (2006) smooth the vectors of words with their distributional nearest neighbors, however distributional nearest neighbors can have different meanings and thus introduce noise. Andrade et al. (2013) use synonym sets in WordNet to smooth the vectors of words, however WordNet is not available for every language. More importantly, both studies work for words, which are not suitable for comparable feature estimation. The reason is that translation pairs can also be phrases (Koehn et al., 2003) or syntactic rules (Galley et al., 2004) etc., depending on what kind of SMT models we use. In this paper, we propose using paraphrases to address the data sparseness problem of BLE for comparable feature estimation. A paraphrase is a restatement of the meaning of a word, phrase or syntactic rule etc., therefore it is suitable for the data sparseness problem. We generate paraphrases from the parallel corpus used for translation model learning. Then, we use the paraphrases to smooth the vectors of the translation pairs in the translation model for comparable feature estimation. Smoothing is do"
Y14-1032,P07-2045,0,0.00477633,"e phrase table, and evaluated the two methods in the perspective of SMT performance. We conducted experiments on Chinese-English data. In all our experiments, we preprocessed the data by segmenting Chinese sentences using a segmenter proposed by Chu et al. (2012), and tokenizing English sentences. 5.1 Experimental Settings SMT Settings We conducted Chinese-to-English translation experiments. The parallel corpus we used is from Chinese-English NIST open MT.6 The “NIST” column of Table 4 shows the statistics of this parallel corpus. For decoding, we used the state-of-theart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012). We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit7 with interpolated Kneser-Ney discounting, and used it for all the experiments. We used NIST open MT 2002 and 2003 data sets for tuning and testing, containing 878 and 919 sentence pairs respectively. Note that both MT 2002 and 2003 data sets contain 4 references for each Chinese sentence. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experi"
Y14-1032,W04-3250,0,0.0529101,"“Baseline” denotes the baseline system that does not use comparable features. “Klementiev+” denotes the system that appends the comparable features estimated following (Klementiev et al., 2012) to the phrase table. “Proposed” denotes the system that uses the comparable features estimated by our proposed method. “+Contextual”, “+Topical” and “+Temporal” denote the systems that append contextual, topical and temporal features respectively. “+All” denotes the system that appends all the three types of features. The significance test was performed using the bootstrap resampling method proposed by Koehn (2004). We can see that “Klementiev+” does not always outperform “Baseline”. The reason for this is that the comparable features estimated by (Klementiev et al., 2012) are inaccurate. “Proposed” performs significantly better than both “Baseline” and “Klementiev+”. The reason for this is that “Proposed” deals with the data sparseness problem of BLE for comparable feature estimation, making the features more accurate thus improve the SMT performance. As for different comparable features of “Proposed”, “+Contextual”, “+Topical” and “+Temporal” are all helpful, and combining them can be more effective."
Y14-1032,D09-1040,0,0.125371,"have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a). Our study focuses on addressing the accuracy problem of SMT with BLE. We use paraphrases to address the data sparseness problem of BLE for comparable feature estimation, which makes the comparable features more accurate. 2.2 Paraphrases for SMT Many methods have been proposed to use paraphrases for SMT, mainly for the coverage problem. One method is paraphrasing unknown words or phrases in the translation model (Callison-Burch et al., 2006; Razmara et al., 2013; Marton et al., 2009). PACLIC 28 f 业 业 业 业 e unemployment figures number of unemployed . unemployment was unemployment and bringing φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and their feature scores are incorrect. Another me"
Y14-1032,P03-1021,0,0.0189184,"heart PBSMT toolkit Moses (Koehn et al., 2007) with default options, except for the phrase length limit (7→3) following (Klementiev et al., 2012). We trained a 5-gram language model on the English side of the parallel corpus using the SRILM toolkit7 with interpolated Kneser-Ney discounting, and used it for all the experiments. We used NIST open MT 2002 and 2003 data sets for tuning and testing, containing 878 and 919 sentence pairs respectively. Note that both MT 2002 and 2003 data sets contain 4 references for each Chinese sentence. Tuning was performed by minimum error rate training (MERT) (Och, 2003), and it was re-run for every experiment. Comparable Feature Estimation Settings Table 4 shows the statistics of the comparable data used for comparable feature estimation. The con6 LDC2007T02, LDC2002T01, LDC2003T17, LDC2004T07, HK News part of LDC2004T08, LDC2005T10 and LDC2006T04 7 http://www.speech.sri.com/projects/srilm !268 # Zh articles # En articles # Zh sentences # En sentences # Zh tokens # En tokens NIST N/A N/A 991k 991k 26.1M 27.2M Gigaword 3.6M 4.3M 42.6M 56.9M 1.1B 1.3B Wikipedia 248k 248k 2.8M 10.1M 70.5M 240.5M Table 4: Statistics of the comparable data used for comparable fea"
Y14-1032,P95-1050,0,0.428112,"h Phrase-based SMT (PBSMT) (Koehn et al., 2003).2 Experimental results show that our proposed method can improve SMT performance, compared to the previous studies that estimate comparable features without dealing with the data sparseness problem of 2 Our proposed method can also be applied to other language pairs and SMT models. !263 BLE (Klementiev et al., 2012; Irvine and CallisonBurch, 2013a). The results verify the effectiveness of using BLE together with paraphrases for the accuracy problem of SMT. 2 Related Work 2.1 Bilingual Lexicon Extraction (BLE) for SMT From the pioneering work of (Rapp, 1995), BLE from comparable corpora has been studied for a long time. BLE is based on the distributional hypothesis (Harris, 1954), stating that words with similar meaning have similar distributions across languages. Contextual similarity (Rapp, 1995), topical similarity (Vuli´c et al., 2011) and temporal similarity (Klementiev and Roth, 2006) can be important clues for BLE. Orthographic similarity may also be used for BLE for some similar language pairs (Koehn and Knight, 2002). Moreover, some studies try to use the combinations of different similarities for BLE (Irvine and Callison-Burch, 2013b; C"
Y14-1032,P13-1109,0,0.0524129,"3). Moreover, studies have been conducted to address the accuracy and coverage problems of SMT simultaneously with BLE (Irvine and Callison-Burch, 2013a). Our study focuses on addressing the accuracy problem of SMT with BLE. We use paraphrases to address the data sparseness problem of BLE for comparable feature estimation, which makes the comparable features more accurate. 2.2 Paraphrases for SMT Many methods have been proposed to use paraphrases for SMT, mainly for the coverage problem. One method is paraphrasing unknown words or phrases in the translation model (Callison-Burch et al., 2006; Razmara et al., 2013; Marton et al., 2009). PACLIC 28 f 业 业 业 业 e unemployment figures number of unemployed . unemployment was unemployment and bringing φ(f |e) 0.3 0.1333 0.3333 1 lex(f |e) 0.0037 0.0188 0.0015 0.0029 φ(e|f ) 0.0769 0.1025 0.0256 0.0256 lex(e|f ) 0.0018 0.0041 6.8e-06 5.4e-07 Alignment 0-0 1-1 1-0 1-1 0-2 0-1 1-1 1-2 0-0 1-0 Table 1: An example of the accuracy problem in PBSMT. The correct translations of “ 业 (unemployment) (number of people)” are in bold. The incorrect phrase pairs are extracted because “ (number of people)” is incorrectly aligned to “unemployment”, and their feature scores are"
Y14-1032,P11-2084,0,0.0522009,"Missing"
Y14-1032,J93-2003,0,\N,Missing
Y15-1033,D14-1179,0,0.0191739,"Missing"
Y15-1033,D07-1103,0,0.657787,"ntences and 4.5M terms) parallel data via pivot-based SMT. We generate a large pivot translation model using the Ja-En and En-Zh parallel data. Moreover, a small direct Ja-Zh translation model is generated using small-scale Ja-Zh parallel data. (680k sentences and 561k terms). Both the direct and pivot translation models are used to translate the Ja terms in the Ja-En dictionaries to Zh and the Zh terms in the Zh-En dictionaries to Ja to construct a large-scale Ja-Zh dictionary (about 3.6M terms). We address the noisy nature of pivoting large phrase tables by statistical significance pruning (Johnson et al., 2007). In addition, we exploit linguistic knowledge of common Chinese characters (Chu et al., 2013) shared in Ja-Zh to further improve the translation model. Large-scale experiments on scientific domain data indicate that our proposed method achieves high quality dictionaries which we manually verify to have a high quality. Reranking the n-best list produced by the SMT decoder is known to help improve the translation quality given that good quality features are used (Och et al., 2004). In this paper, we use bilingual neural network language model features for reranking the n-best list produced by t"
Y15-1033,P07-2045,0,0.143247,"o be a possible way of constructing a dictionary for the language pairs that have scarce parallel data (Tsunakawa et al., 2009; Chu et al., 2015). The assumption of this method is that there is a pair of large-scale parallel data: one between the source language and an intermediate resource rich language (henceforth called pivot), and one between that pivot and the target language. We can use the source-pivot and pivot-target parallel data to develop a source-target term1 translation model for dictionary construction. Pivot-based SMT uses the log linear model as conventional phrase-based SMT (Koehn et al., 2007) does. This method can address the data sparseness problem of directly merging the source-pivot and pivot-target terms, because it can use the portion of terms to generate new terms. Small-scale experiments in (Tsunakawa et al., 2009) showed very low 1 In this paper, we call the entries in the dictionary terms. A term consists of one or multiple tokens. accuracy of pivot-based SMT for dictionary construction.2 This paper presents our study to construct a largescale Japanese-Chinese (Ja-Zh) scientific dictionary, using large-scale Japanese-English (Ja-En) (49.1M sentences and 1.4M terms) and En"
Y15-1033,D09-1141,0,0.0502113,"iplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. They used the multiple decoding paths (MDP) feature of the phrase-based SMT toolkit Moses (Koehn et al., 2007) to combine multiple tables which avoids interpolation. The issue of noise introduced by pivoting has not been seriously addressed and although statistical significance pruning (Johnson et al., 2007) has shown to be quite effective in a bilingual scenario, it has never been considered in a pivot language scenario. (Tsunakawa et al., 2009) was the first work that constructs a dictionary"
Y15-1033,P02-1040,0,0.092256,"ioned in Section 4. The following scores are reported: • BS+RRCBLEU: Using character BLEU to rerank the n-best list. • BS+RRWBLEU: Using word BLEU to rerank the n-best list. • BS+RRSVM: Using SVM to rerank the n-best list. This is followed by substituting the OOVs with the character level translations using the learned neural translation models (which we label as +OOVsub). 5.2.3 Evaluation Criteria Following (Tsunakawa et al., 2009), we evaluated the accuracy on the test set using three metrics: 1 best, 20 best and Mean Reciprocal Rank (MRR)(Voorhees, 1999). In addition, we report the BLEU-4 (Papineni et al., 2002) scores that were computed on the word level. 5.2.4 Results of Automatic Evaluation Table 3 shows the evaluation results. We also show the percentage of OOV terms,11 and the accuracy with and without OOV terms respectively. In general, we can see that Pivot performs better than Direct, because the data of Ja-En and En-Zh is larger than that of Ja-Zh. Direct+Pivot shows better performance than either method. Different pruning methods show different performances, where Pr:P-T improves the accuracy, while the other two not. To understand the reason for this, we also investigated the statistics of"
Y15-1033,E12-1055,0,0.0162567,"loped a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. They used the multiple decoding paths (MDP) feature of the phrase-based SMT toolkit Moses (Koehn et al., 2007) to combine multiple tables which avoids interpolation. The issue of noise introduced by pivoting has not been seriously addressed and although statistical significance pruning (Johnson et al., 2007"
Y15-1033,P14-2042,1,0.828997,"AS LCAS title ISTIC pc ASPEC Size 3,588,800 22,610,643 19,905,978 3,013,886 6,090,535 1,070,719 1,562,119 680,193 shows the statistics of the bilingual dictionaries used for training. • Parallel corpora: the scientific Ja-En, En-Zh and Ja-Zh corpora we used were also provided by JST and ISTIC, containing 49.1M , 8.7M and 680k sentence pairs respectively. Table 2 shows the statistics of parallel corpora used for training. Among which ISTIC pc was provided by ISTIC, and the others were provided by JST. 5.2.1 In our experiments, we segmented the Chinese and Japanese data using a tool proposed by Shen et al. (2014) and JUMAN (Kurohashi et al., 1994) respectively. For decoding, we used Moses (Koehn et al., 2007) with the default options. We trained a word 5-gram language model on the Zh side of all the En-Zh and Ja-Zh training data (14.4M sentences) using the SRILM toolkit10 with interpolated Keneser-Ney discounting. Tuning was performed by minimum error rate training which also provides us with the n-best lists used to learn reranking weights. As a baseline, we compared following three methods for training the translation model: • Direct: Only use the Ja-Zh data to train a direct Ja-Zh model. Table 2: S"
Y15-1033,N07-1061,0,0.02745,"character based neural MT to eliminate the out-of-vocabulary (OOV) terms, which further improves the quality. The rest of this paper is structured as follows: Section 2 reviews related work. Section 3 presents our dictionary construction using pivot-based SMT with significance pruning. Section 4 describe the bilingual neural language model features using a parallel corpus and the constructed dictionary for reranking the n-best list. Experiments and results are described in Section 5, and we conclude this paper in Section 6. 2 Related Work Many studies have been conducted for pivot-based SMT. Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weig"
Y15-1033,P07-1108,0,0.0645188,"Missing"
Y15-1033,P09-1018,0,0.0175042,"ot-based SMT with significance pruning. Section 4 describe the bilingual neural language model features using a parallel corpus and the constructed dictionary for reranking the n-best list. Experiments and results are described in Section 5, and we conclude this paper in Section 6. 2 Related Work Many studies have been conducted for pivot-based SMT. Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked we"
Y15-1042,D09-1092,0,0.0869491,"Missing"
Y15-1042,P95-1050,0,0.257109,"ning data. After describing the baseline model (bilingual LDA), we introduce three novel methods of taking advantage of data including a pivot language P , such as SP + P -T and S-P -T data. 370 3.1 Baseline: Bilingual LDA We begin with a baseline non-pivot lexicon extraction model MST : S × T → R that gives a similarity score to a source-target word pair (using S-T training data). The non-pivot lexicon extraction model MST makes use of a bilingual topic similarity measure. We elected to use bilingual topic models rather than the more intuitive method of comparing monolingual context vectors (Rapp, 1995) as we believe topic modelling is more suitable for processing uncommon language pairs. This is because a bilingual seed lexicon is required for methods that learn a mapping between source and target vector spaces, such as Haghighi et al. (2008), in order to match crosslanguage word pairs. This data is unlikely to be available in sufficient quantity for low-resource language pairs, however comparable documents can be found from sources such as Wikipedia. We base our implementation on the state-ofthe-art system of Vulić et al. (2011) for comparison. This method uses the bilingual Latent Dirichl"
Y15-1042,P10-1011,0,0.0195602,"hods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 1996) and ‘non-aligned signatures’ (Shezaf and Rappoport, 2010), a form of word context similarity. Bilingual lexicon mining from non-parallel data has seen much popularity in recent years. Studies have considered a variety of methods such as canonical correlation analysis (Haghighi et al., 2008) and label propagation (Tamura et al., 2012). We use the method of bilingual topic modelling (Vulić et al., 2011), which has been recently applied to a variety of fields such as transliteration mining (Richardson et al., 2013). 3 Model Details We consider the task of translating a source word s from language S to a target word t from language T . The baseline mode"
Y15-1042,C96-2098,0,0.202542,"ama and Isahara (2007) give a comparison of possible methods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 1996) and ‘non-aligned signatures’ (Shezaf and Rappoport, 2010), a form of word context similarity. Bilingual lexicon mining from non-parallel data has seen much popularity in recent years. Studies have considered a variety of methods such as canonical correlation analysis (Haghighi et al., 2008) and label propagation (Tamura et al., 2012). We use the method of bilingual topic modelling (Vulić et al., 2011), which has been recently applied to a variety of fields such as transliteration mining (Richardson et al., 2013). 3 Model Details We consider the task of translating a source word s from languag"
Y15-1042,C94-1048,0,0.0388793,"ting their application to a practical scenario, and compare their effectiveness to mainstream approaches. 2 Related Work The use of pivot models has been a common theme in the development of Natural Language Processing systems that deal with low-resource languages. In the field of Machine Translation, pivot models can be used in both decoding and the construction of parallel training data. Utiyama and Isahara (2007) give a comparison of possible methods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 1996) and ‘non-aligned signatures’ (Shezaf and Rappoport, 2010), a form of word context similarity. Bilingual lexicon mining from non-parallel data has seen much popularity in recent years. Studies have consi"
Y15-1042,N07-1061,0,0.0391231,"ion pages 369 - 377 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by John Richardson, Toshiaki Nakazawa and Sadao Kurohashi PACLIC 29 present a variety of solutions to this problem, demonstrating their application to a practical scenario, and compare their effectiveness to mainstream approaches. 2 Related Work The use of pivot models has been a common theme in the development of Natural Language Processing systems that deal with low-resource languages. In the field of Machine Translation, pivot models can be used in both decoding and the construction of parallel training data. Utiyama and Isahara (2007) give a comparison of possible methods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 19"
Y15-1042,P08-1089,0,0.0352667,"the development of Natural Language Processing systems that deal with low-resource languages. In the field of Machine Translation, pivot models can be used in both decoding and the construction of parallel training data. Utiyama and Isahara (2007) give a comparison of possible methods for integrating a pivot language into phrase-based SMT systems. Bilingual lexicon extraction has had a long history of using pivot languages. Tanaka and Umemura (1994) build a pivot lexicon by combining bilingual dictionaries, and more recently there have been attempts to extract lexicons or paraphrase patterns (Zhao et al., 2008) from bilingual corpora. A common problem with the use of a pivot language is associated noise, leading to a number of studies aiming to improve pivot lexicons, such as by using cross-lingual cooccurrences (Tanaka and Iwasaki, 1996) and ‘non-aligned signatures’ (Shezaf and Rappoport, 2010), a form of word context similarity. Bilingual lexicon mining from non-parallel data has seen much popularity in recent years. Studies have considered a variety of methods such as canonical correlation analysis (Haghighi et al., 2008) and label propagation (Tamura et al., 2012). We use the method of bilingual"
Y15-1042,P13-2050,0,0.0157914,"ge pairs, particularly when one of those languages is English, previous direct approaches fail when there is no such data available. For many language pairs there simply does not exist comparable (and even less so parallel) data. Even for languages with a large volume of available parallel data, most corpora cover only limited domains. There are two natural methods to deal with this problem: constructing or mining new data for the direct approach, and finding new ways to make better use of what data is already available. For an example of the construction of comparable corpora, see Zhu et al. (2013). We take the second approach and design pivot-based models for bilingual lexicon extraction. The major advantage of using a pivot language is that it is possible to take advantage of the large volume of comparable data sharing a common language such as English. In this paper we develop pivot-based approaches to make use of modern bilingual lexicon extraction methods that can be trained on comparable corpora. We present a selection of efficient algorithms using the framework of topic modelling (Blei et al., 2003). Topic modelling has been a popular approach for bilingual lexicon extraction, ho"
Y15-1042,D12-1003,0,\N,Missing
Y15-1042,P08-1088,0,\N,Missing
Y15-1042,P11-2084,0,\N,Missing
Y15-1042,I13-1030,1,\N,Missing
Y15-1042,N15-1125,1,\N,Missing
Y15-2010,P05-1033,0,0.101997,"de which has a high quality parser to the side which has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indica"
Y15-2010,P11-1061,0,0.0167906,"ject modifier. In this paper we consider combining these two trees and get improved results. We show in our experiments that combining the LQ-parsed trees with the transferred trees yield better translation results rather than only using them individually. 2 Related Work 2.1 Syntax Transfer for Non-MT Task There are many previous works describing methods to improve the performance of NLP tasks for a resource poor language by using a related resource rich language (mainly English). Amongst these the ones which employ methods which transfer information perform better than unsupervised methods. (Das and Petrov, 2011) describe an approach for inducing unsupervised part-of-speech tags for languages that have no labeled training data. (Jiang et al., 2010) show a transfer strategy to construct a constituency parser. (Ganchev et al., 2009) present a partial, approximate transfer through linear expectation constraints to project only parts of the parse trees to the low resource language side. However, improving monolingual parsing accuracy does not directly lead to higher MT performance, as it does not address the annotation criteria difference problem. 2.2 Syntax Transfer for MT Task For MT tasks, most transfe"
Y15-2010,P06-1121,0,0.0248273,"ser to the side which has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which ha"
Y15-2010,P09-1042,0,0.603471,"about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be referred to as the LQ or LQ side since the Chinese parser is of a relatively lower quality and makes a number of parsing mistakes. One advantage is that the transferred parse information will possibly be more similar to the other side’s parse. This will also reduce the parsing error on the LQ side and unify the syntactic annotation on both sides. This idea has been proposed before, but not much has been done in the case of d"
Y15-2010,N06-1023,1,0.774953,"ti ,tj and undo the modification if so. We run several iterations of the procedure mentioned above till it reaches the worst case which means reverse T reenew is the same as T reeold T T . It is not so simple to test the effectiveness of projectivity on tasks other than MT. We manually check the percentage of errors our method has solved by manually evaluating 50 sentences (Section 7.2). tific domain, with default parameters. The new combined parser that we proposed used the training data obtained from the ASPEC Ja-Zh parallel corpus,3 containing 670k sentences. We used a Japanese parser KNP (Kawahara and Kurohashi, 2006)4 and the baseline SKP to automatically parse these sentences. We then created combined Chinese dependency trees.5 Finally, we trained a new parser using these combined Chinese dependency trees with the same parameters. As test data we used an additional 1k sentences from our in-house treebank. Table 1 shows the results of these two parsers. Parser Baseline Combined UAS 0.7433 0.5890 Root-Accuracy 0.6950 0.6140 Table 1: Parsing accuracy 6 Re-train a New LQ Side Parser Using the word alignments and original monolingual dependency trees, we successfully create combined trees using the parallel t"
Y15-2010,P07-2045,0,0.00835035,"from the language side which has a high quality parser to the side which has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or"
Y15-2010,P06-1077,0,0.148062,"transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be re"
Y15-2010,D08-1022,0,0.192001,"ne the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be referred to as the LQ or LQ side since the"
Y15-2010,P05-1034,0,0.223368,"trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be referred to as the LQ"
Y15-2010,P14-5014,1,0.878661,"Missing"
Y15-2010,P08-1066,0,0.104227,"has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality par"
Y15-2010,Y12-1033,1,0.438635,"Missing"
Y15-2010,P08-1064,0,0.223917,"original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 1 Introduction Depending on whether or not monolingual parsing is utilized, there are about 4 types of machine translation (MT) methods. string-to-string (Koehn et al., 2007; Chiang, 2005), string-to-tree (Galley et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Quirk et al., 2005; Mi and Huang, 2008), and tree-to-tree (Zhang et al., 2008; Richardson et al., 2014). Though the tree-to-tree system that employs syntactic analysis for both source and target sides seems In this paper, we explore a method which relies on using parallel text for transferring syntactic knowledge from a high quality (HQ) parser to a low quality (LQ) parser using alignment information(Ganchev et al., 2009; Hwa et al., 2005). Henceforth we shall refer to Japanese as HQ or HQ side, indicating that it is the language which has a high quality parser. Conversely Chinese will be referred to as the LQ or LQ side since the Chinese parser is of a relatively lowe"
Y15-2010,C10-2059,0,\N,Missing
Y18-1026,P16-1101,0,0.0300583,"s feelings of being understood (Oishi et al., 2012). Drivers’ feelings of being understood by the system enhance the reliability and trustworthiness of the system, and improve the likelihood of following the system’s messages or cautions. For our future work, we are currently working on (1) expanding the corpus size with human annotations, (2) conducting further CRF experiments with more features such as modality, polarity, pronouns, zero-anaphora and emotions, and (3) conducting experiments with more recent machine learning methods such as Bidirectional LSTM-CRF (e.g., Huang, Xu, & Yu, 2015; Ma & Hovy, 2016; Reimers & Gurevych, 2017). The errors suggest that simple sequential tagging is not good enough to represent contexts that are located a far distance away. 6 References Conclusions and Future Work We constructed a Driving Experience Corpus by annotating behaviors and subjectivity. With the tagged textual data, we constructed a model that sought to understand driving scenes, behaviors as actors or observers and their subjectivity as humans recognize them, which helps the system to communicate with its users in a more colloquial and human-like manner. The annotation scheme and guidelines refle"
Y18-1026,P06-2059,0,0.0526234,"rs, including the present and the past, the date, the time, and the place. This tag refers to the evaluation of the behaviors. It refers to the emotions, cognitions, thoughts, judgments, and predictions held by the author as a result of actions. Subjectivity (SJ) Table 1: Definitions of the tags Polarity Evaluation (polarity per sentence)2, (2) the ACP corpus, (3) the Kyoto University and NTT blog corpus and (4) the Phrase Polarity Corpus. The Tsukuba Corpus consists of 4,309 sentences in the travel services domain with the polarity per sentence1. The ACP corpus was automatically constructed (Kaji & Kitsuregawa, 2006). This corpus is large (126,610 sentences), but it was not filtered by humans. Hashimoto et al. (2011) constructed the Kyoto University and NTT Blog Corpus of 249 blog articles (4,186 sentences) with sentiment information. These blogs were written by 81 university students in the following four themes: sight-seeing in Kyoto, cellphones, sports or gourmet food. Nakazawa et al. (2018) constructed a phrase-sentiment review corpus (59,758 phrases) with all manually annotated polarity information from the Tsukuba Corpus. These corpora do not focus on driving, thinking and cognition. Our corpus is d"
Y18-1026,D17-1035,0,0.0116143,"ng understood (Oishi et al., 2012). Drivers’ feelings of being understood by the system enhance the reliability and trustworthiness of the system, and improve the likelihood of following the system’s messages or cautions. For our future work, we are currently working on (1) expanding the corpus size with human annotations, (2) conducting further CRF experiments with more features such as modality, polarity, pronouns, zero-anaphora and emotions, and (3) conducting experiments with more recent machine learning methods such as Bidirectional LSTM-CRF (e.g., Huang, Xu, & Yu, 2015; Ma & Hovy, 2016; Reimers & Gurevych, 2017). The errors suggest that simple sequential tagging is not good enough to represent contexts that are located a far distance away. 6 References Conclusions and Future Work We constructed a Driving Experience Corpus by annotating behaviors and subjectivity. With the tagged textual data, we constructed a model that sought to understand driving scenes, behaviors as actors or observers and their subjectivity as humans recognize them, which helps the system to communicate with its users in a more colloquial and human-like manner. The annotation scheme and guidelines reflect the ideas and knowledge"
Y18-1026,C08-1103,0,0.0412832,"on and Driving Memo Previous studies related to driving in Japan basically focused only on driving actions, traffic rules and ontologies (e.g., Takayama et al., 2017; Kawabe et al., 2015; Suzuki et al., 2015; Taira et al., 2014). These datasets, however, are not publicly available. Furthermore, human-centered or human experiential foci are lacking. 2.3 Annotation 1/1 Regarding subjectivity, subjective analysis is one important and active domain. Subjectivity in this analysis refers to “information about any attitudes, beliefs, emotions, opinions, evaluations and sentiment expressed in texts” (Stoyanov & Cardie, 2008, p. 817). The previous studies of subjectivity analysis include emotions (e.g., Laskowski & Burger, 2006; (Tokuhisa et al., 2008, 2009), opinions (e.g., Jakob & Guvebychi, 2008), and sentiments (e.g., Hashimoto et al., 2011; Nakazawa et al., 2018). Regarding annotated corpora in this domain, four Japanese corpora have been constructed: (1) the University of Tsukuba Corpus Tagged with 1/1 223 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors 2 PACLIC 32 Tags Driving Experience (DE) Definitions A car driving experien"
Y18-3001,Y18-3013,1,0.888887,"Missing"
Y18-3001,Y18-3003,1,0.889659,"Missing"
Y18-3001,Y18-3005,0,0.0459044,"Missing"
Y18-3001,P17-4012,0,0.0426364,". 3 each participant’s system. That is, the specific baseline system was the standard for human evaluation. At WAT 2018, we adopted a neural machine translation (NMT) with attention mechanism as a baseline system except for the IITB tasks. We used a phrasebased statistical machine translation (SMT) system, which is the same system as that at WAT 2017, as the baseline system for the IITB tasks. The NMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page.5 We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems. In addition to the NMT baseline systems, we have SMT baseline systems for the tasks that started at last year or before last year. The baseline systems are shown in Tables 8, 9, and 10. SMT baseline systems are described in the previous WAT overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online tra"
Y18-3001,Y18-3002,0,0.0375601,"Missing"
Y18-3001,P07-2045,0,0.0120426,"in frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeC"
Y18-3001,W04-3250,0,0.312277,"Missing"
Y18-3001,Y18-3007,0,0.0326187,"Missing"
Y18-3001,Y18-3014,0,0.0304859,"Missing"
Y18-3001,W14-7001,1,0.458489,"ion (WAT2018) including Ja↔En, Ja↔Zh scientific paper translation subtasks, Zh↔Ja, K↔Ja, En↔Ja patent translation subtasks, Hi↔En, My↔En mixed domain subtasks and Bn/Hi/Ml/Ta/Te/Ur/Si↔En Indic languages multilingual subtasks. For the WAT2018, 17 teams participated in the shared tasks. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2017 (Nakazawa et al., 2014; Nakazawa et al., 2015; Nakazawa et al., 2016; Nakazawa et al., 2017), WAT2018 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 5th WAT, we adopted new translation subtasks with Myanmar ↔ EnSadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp glish mixed domain corpus1 and Bengali/Hindi/Malayalam/Tamil/Telugu/Urdu/Sinhalese ↔ English OpenSubtitles corpus2 in addition to the subtasks at WAT2017. WAT is the uniq"
Y18-3001,W16-4601,1,0.938773,"Missing"
Y18-3001,P11-2093,0,0.0504246,"leu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.14 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.15 For Korean segmentation, we 11 http://www.kecl.ntt.co.jp/icl/lirg/ ribes/index.html 12 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2018/ 13 http://www.phontron.com/kytea/model. html 14 http://code.google.com/p/mecab/ downloads/detail?name=mecab-ipadic-2.7. 0-20070801.tar.gz 15 http://nlp.stanford.ed"
Y18-3001,Y18-3011,0,0.141475,"Missing"
Y18-3001,P02-1040,0,0.119424,"://bitbucket.org/anoopk/indic_nlp_ library 10 https://github.com/rsennrich/ subword-nmt • tgt vocab size = 100000 • src words min frequency = 1 • tgt words min frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanes"
Y18-3001,Y18-3010,0,0.0585011,"Missing"
Y18-3001,Y18-3012,0,0.0444067,"Missing"
Y18-3001,2007.mtsummit-papers.63,0,0.0425147,"itute of Information and Communications Technology (NICT). The corpus consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for ja↔en subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for ja↔zh subtasks. The statistics for each corpus are shown in Table 1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million JapaneseEnglish scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score that are calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the frequency and occurrence ratios for the training data, are described in the README file of ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts that exclude the sentences in the training data. Each dataset consists of 400 documents and contains sentences in each field at the same rate. The document ali"
Y18-3001,Y18-3017,0,0.0591037,"Missing"
Y18-3001,Y18-3006,1,0.868302,"Missing"
