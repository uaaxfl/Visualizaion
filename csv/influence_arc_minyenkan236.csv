2020.acl-main.100,C18-1057,1,0.889971,"Missing"
2020.acl-main.100,D18-1021,1,0.877217,"Missing"
2020.acl-main.100,D19-1025,1,0.824252,"Missing"
2020.acl-main.100,P17-1149,1,0.869886,"Missing"
2020.acl-main.100,E99-1042,0,0.410389,"expertise level of text, which is also a key difference from conventional styles. We identify two major types of knowledge gaps in MSD: terminology, e.g., dyspnea in the first example; and empirical evidence. As shown in the third pair, doctors prefer to use statistics (About 1/1000), while laymen do not (quite small). Lexical & Structural Modification. Fu et al. (2019) has indicated that most ST models only perform lexical modification, while leaving structures unchanged. Actually, syntactic structures play a significant role in language styles, especially regarding complexity or simplicity (Carroll et al., 1999). As shown in the last example, a complex sentence can be expressed with several simple sentences by appropriately splitting content. However, available datasets rarely contain such cases. Our main contributions can be summarized as: • We propose the new task of expertise style transfer, which aims to facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models a"
2020.acl-main.100,W11-1601,0,0.0153766,"ne a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particularly important. Terminologi"
2020.acl-main.100,P19-1601,0,0.0500939,"oder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similarity (Jin et al., 2019), or cyclic reconstruction (Dai et al., 2019), marked with Translation methods. The third group is Manipulation methods. Li et al. (2018) first identify the style words by their statistics, then replace them with similar retrieved sentences with a target style. Xu et al. (2018) jointly train the two steps with a neutralization module and a stylization module based on reinforcement learning. For better stylization, Zhang et al. (2018b) introduce a learned sentiment memory network, while John et al. (2019) utilize hierarchical reinforcement learning. 2.2 Zweigenbaum (2008) detect paraphrases from comparable medical corpora of specialized a"
2020.acl-main.100,P15-2011,0,0.0195333,"ed targets) with respect to both model training and testing. Besides, it is usually ignored that the opposite direction of TS — improving the expertise levels of layman language for accuracy and professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance"
2020.acl-main.100,W17-4902,0,0.0470796,"techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm for unsupervised methods without parallel data. There are three groups. The first group is Disentanglement methods that learn disentangled representations of style and content, and then directly manipulating these latent representations t"
2020.acl-main.100,D19-1306,0,0.0944358,"Missing"
2020.acl-main.100,P19-1041,0,0.0278074,"gma and Welling, 2013), to represent the content as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similarity (Jin et al., 2019), or cyclic reconstruction (Dai et al., 2019), marked with Translation methods. The third group is Manipulation methods. Li et al. (2018) first identify the style words by their statistics, then re"
2020.acl-main.100,E17-2068,0,0.0261161,"incorporates a phrase table into OpenNMT (Klein et al., 2017), which provides guidance for replacing complex words with their simple synonym (Shardlow and Nawaz, 2019); and (2) Unsupervised model UNTS that utilizes adversarial learning (Surya et al., 2019). The models for ST task selected are: (1) Disentanglement method ControlledGen (Hu et al., Following Dai et al. (2019), we make an automatic evaluation on three aspects: Style Accuracy (marked as Acc) aims to measure how accurate the model controls sentence style. We train two classifiers on the training set of each dataset using fasttext (Joulin et al., 2017). Fluency (marked as PPL) is usually measured by the perplexity of the transferred sentence. We fine-tune the state-of-the-art pretrained language model, Bert (Devlin et al., 2019), on the training set of each dataset for each style. Content Similarity measures how much content is preserved during style transfer. We calculate 4-gram BLEU (Papineni et al., 2002) between model outputs and inputs (marked as self-BLEU), and between outputs and gold human references (marked as ref-BLEU). Automatic metrics for content similarity are arguably unreliable, since the original inputs usually achieve the"
2020.acl-main.100,P17-4012,0,0.0742569,"professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning betwe"
2020.acl-main.100,N19-1423,0,0.0221769,"supervised model UNTS that utilizes adversarial learning (Surya et al., 2019). The models for ST task selected are: (1) Disentanglement method ControlledGen (Hu et al., Following Dai et al. (2019), we make an automatic evaluation on three aspects: Style Accuracy (marked as Acc) aims to measure how accurate the model controls sentence style. We train two classifiers on the training set of each dataset using fasttext (Joulin et al., 2017). Fluency (marked as PPL) is usually measured by the perplexity of the transferred sentence. We fine-tune the state-of-the-art pretrained language model, Bert (Devlin et al., 2019), on the training set of each dataset for each style. Content Similarity measures how much content is preserved during style transfer. We calculate 4-gram BLEU (Papineni et al., 2002) between model outputs and inputs (marked as self-BLEU), and between outputs and gold human references (marked as ref-BLEU). Automatic metrics for content similarity are arguably unreliable, since the original inputs usually achieve the highest scores (Fu et al., 2019). We 5 We only report TS models for expertise to laymen language, since they do not claim the opposite direction. 6 https://github.com/senisioi/ Neu"
2020.acl-main.100,W19-8604,0,0.171898,"rs to annotate the parallel sentences between the two versions (examples shown in Figure 1). Compared with both ST and TS datasets, MSD is more challenging from two aspects: Knowledge Gap. Domain knowledge is the key factor that influences the expertise level of text, which is also a key difference from conventional styles. We identify two major types of knowledge gaps in MSD: terminology, e.g., dyspnea in the first example; and empirical evidence. As shown in the third pair, doctors prefer to use statistics (About 1/1000), while laymen do not (quite small). Lexical & Structural Modification. Fu et al. (2019) has indicated that most ST models only perform lexical modification, while leaving structures unchanged. Actually, syntactic structures play a significant role in language styles, especially regarding complexity or simplicity (Carroll et al., 1999). As shown in the last example, a complex sentence can be expressed with several simple sentences by appropriately splitting content. However, available datasets rarely contain such cases. Our main contributions can be summarized as: • We propose the new task of expertise style transfer, which aims to facilitate communication between experts and lay"
2020.acl-main.100,D19-1325,0,0.0648545,"ontent as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similarity (Jin et al., 2019), or cyclic reconstruction (Dai et al., 2019), marked with Translation methods. The third group is Manipulation methods. Li et al. (2018) first identify the style words by their statistics, then replace them with similar retrieved senten"
2020.acl-main.100,N18-1169,0,0.329339,"the other hand, it also aims to improve the expertise level based on context, so that laymen’s expressions can be more accurate and professional. For example, in the second pair, causing further damage is not as accurate as ulcerates, omitting the important mucous and disintegrative conditions of the sores. There are two related tasks, but neither serve as suitable prior art. The first is text style transfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without c"
2020.acl-main.100,W16-4912,0,0.0454583,"but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medical domain, named MSD, and conduct deep analysis by implementing state-"
2020.acl-main.100,P02-1040,0,0.110222,"i et al. (2019), we make an automatic evaluation on three aspects: Style Accuracy (marked as Acc) aims to measure how accurate the model controls sentence style. We train two classifiers on the training set of each dataset using fasttext (Joulin et al., 2017). Fluency (marked as PPL) is usually measured by the perplexity of the transferred sentence. We fine-tune the state-of-the-art pretrained language model, Bert (Devlin et al., 2019), on the training set of each dataset for each style. Content Similarity measures how much content is preserved during style transfer. We calculate 4-gram BLEU (Papineni et al., 2002) between model outputs and inputs (marked as self-BLEU), and between outputs and gold human references (marked as ref-BLEU). Automatic metrics for content similarity are arguably unreliable, since the original inputs usually achieve the highest scores (Fu et al., 2019). We 5 We only report TS models for expertise to laymen language, since they do not claim the opposite direction. 6 https://github.com/senisioi/ NeuralTextSimplification/ 4.1 Baselines 1066 E2L L2E Dataset Metrics OpenNMT+PT UNTS ControlledGen DeleteAndRetrieve StyleTransformer Gold ControlledGen DeleteAndRetrieve StyleTransforme"
2020.acl-main.100,D14-1162,0,0.0827572,"Missing"
2020.acl-main.100,P18-1080,0,0.179009,"experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm for unsupervised methods without parallel data. There are three groups. T"
2020.acl-main.100,N18-1012,0,0.40865,"improve the expertise level based on context, so that laymen’s expressions can be more accurate and professional. For example, in the second pair, causing further damage is not as accurate as ulcerates, omitting the important mucous and disintegrative conditions of the sores. There are two related tasks, but neither serve as suitable prior art. The first is text style transfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. Howev"
2020.acl-main.100,P18-2031,0,0.0241445,"s: • We propose the new task of expertise style transfer, which aims to facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More p"
2020.acl-main.100,N16-1005,0,0.030617,"pertise style transfer, which aims to facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm f"
2020.acl-main.100,P19-1037,0,0.0531654,"8), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particularly important. Terminologies are one of the main obstacles to understanding, and extracting their explanations could be helpful for TS (Shardlow and Nawaz, 2019). Del´eger and Discussion To sum up, both tasks lack parallel data for training and evaluation. This prevents researcher"
2020.acl-main.100,D18-1081,0,0.279357,"ransfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medic"
2020.acl-main.100,P08-1040,0,0.027235,"es) and inadequate (instances having non-simplified targets) with respect to both model training and testing. Besides, it is usually ignored that the opposite direction of TS — improving the expertise levels of layman language for accuracy and professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Sh"
2020.acl-main.100,D11-1038,0,0.0297356,"usually ignored that the opposite direction of TS — improving the expertise levels of layman language for accuracy and professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burd"
2020.acl-main.100,P12-1107,0,0.0302466,"Missing"
2020.acl-main.100,Q15-1021,0,0.0221396,"ational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medical domain, named MSD, and conduct deep analysis by implementing state-of-the-art (SOTA) TS and ST models. The dataset is derived from human-written medical references, The Merck Manuals1 , which include two parallel versions of texts, one tailored for consumers and the other for healthcare professionals. For automatic evaluation, we hire doctors to annotate the parallel sentences between the two versions (examples"
2020.acl-main.100,Q16-1029,0,0.0438076,"Missing"
2020.acl-main.100,C12-1177,0,0.367398,"o facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm for unsupervised methods withou"
2020.acl-main.100,D17-1062,0,0.0260756,"words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particularly important. Terminologies are one of the main obstacles to understandi"
2020.acl-main.100,N18-1138,0,0.0276499,"autoencoder that learns a shared latent content space between true samples and generated samples through an adversarial classifier. Hu et al. (2017) utilize neural generative model, Variational Autoencoders (VAEs) (Kingma and Welling, 2013), to represent the content as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similar"
2020.acl-main.100,D18-1138,0,0.0168144,"autoencoder that learns a shared latent content space between true samples and generated samples through an adversarial classifier. Hu et al. (2017) utilize neural generative model, Variational Autoencoders (VAEs) (Kingma and Welling, 2013), to represent the content as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similar"
2020.acl-main.100,C10-1152,0,0.011789,"implification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particula"
2020.acl-main.100,P18-1016,0,0.139175,"ransfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medic"
2020.acl-main.100,P19-1198,0,0.306698,"cs ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medical domain, named MSD, and conduct deep analysis by implementing state-of-the-art (SOTA) TS and ST models. The dataset is derived from human-written medical references, The Merck Manuals1 , which include two parallel versions of texts, one tailored for consumers and the other for healthcare professionals. For automatic evaluation, we hire doctors to annotate the parallel sentences between the two versions (examples shown in Figure 1)."
2020.acl-main.100,P18-1090,0,\N,Missing
2020.acl-main.135,W07-0734,0,0.04282,"th the answer as inputs to generate the question. However, state-of-the-art semantic parsing models have difficulty in producing accurate semantic graphs for very long documents. We therefore pre-process the original dataset to select relevant sentences, i.e., the evidence statements and the sentences that overlap with the ground-truth question, as the input document. We follow the original data split of HotpotQA to pre-process the data, resulting in 90,440 / 6,072 examples for training and evaluation, respectively. Following previous works, we employ BLEU 1–4 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004) as automated evaluation metrics. BLEU measures the average n-gram overlap on a set of reference sentences. Both METEOR and ROUGE-L specialize BLEU’s n-gram overlap idea for machine translation and text summarization evaluation, respectively. Critically, we also conduct human evaluation, where annotators evaluate the generation quality from three important aspects of deep questions: fluency, relevance, and complexity. 4.2 Baselines We compare our proposed model against several strong baselines on question generation. • Seq2Seq + Attn (Bahdanau et al., 2014): the basic"
2020.acl-main.135,D18-1362,0,0.0304405,"east two potential future directions. First, graph structure that can accurately represent the semantic meaning of the document is crucial for our model. Although DP-based and SRL-based semantic parsing are widely used, more advanced semantic representations could also be explored, such as discourse structure representation (van Noord et al., 2018; Liu et al., 2019b) and knowledge graph-enhanced text representations (Cao et al., 2017; Yang et al., 2019). Second, our method can be improved by explicitly modeling the reasoning chains in generation of deep questions, inspired by related methods (Lin et al., 2018; Jiang and Bansal, 2019) in multi-hop question answering. Acknowledgments This research is supported by the National Research Foundation, Singapore under its International Research Centres in Singapore Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. 1471 References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473. Nicola De Cao, Wilker Aziz"
2020.acl-main.135,P19-1629,0,0.325959,"ling better language flexibility, compared against rule-based methods. A comprehensive survey of QG can be found in Pan et al. (2019). Many improvements have been proposed since the first Seq2Seq model of Du et al. (2017): applying various techniques to encode the answer information, thus allowing for better quality answerfocused questions (Zhou et al., 2017; Sun et al., 2018; Kim et al., 2019); improving the training via combining supervised and reinforcement learning to maximize question-specific rewards (Yuan et al., 2017); and incorporating various linguistic features into the QG process (Liu et al., 2019a). However, these approaches only consider sentence-level QG. In contrast, our work focus on the challenge of generating deep questions with multi-hop reasoning over document-level contexts. Recently, work has started to leverage paragraphlevel contexts to produce better questions. Du and Cardie (2018) incorporated coreference knowledge to better encode entity connections across documents. Zhao et al. (2018) applied a gated selfattention mechanism to encode contextual information. However, in practice, semantic structure is difficult to distil solely via self-attention over the entire documen"
2020.acl-main.135,J08-2001,0,0.053577,"Missing"
2020.acl-main.135,Q18-1043,0,0.0602566,"Missing"
2020.acl-main.135,P02-1040,0,0.107103,"he supporting documents along with the answer as inputs to generate the question. However, state-of-the-art semantic parsing models have difficulty in producing accurate semantic graphs for very long documents. We therefore pre-process the original dataset to select relevant sentences, i.e., the evidence statements and the sentences that overlap with the ground-truth question, as the input document. We follow the original data split of HotpotQA to pre-process the data, resulting in 90,440 / 6,072 examples for training and evaluation, respectively. Following previous works, we employ BLEU 1–4 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004) as automated evaluation metrics. BLEU measures the average n-gram overlap on a set of reference sentences. Both METEOR and ROUGE-L specialize BLEU’s n-gram overlap idea for machine translation and text summarization evaluation, respectively. Critically, we also conduct human evaluation, where annotators evaluate the generation quality from three important aspects of deep questions: fluency, relevance, and complexity. 4.2 Baselines We compare our proposed model against several strong baselines on question generation. • Seq2Seq + Attn ("
2020.acl-main.135,D14-1162,0,0.0837784,"Missing"
2020.acl-main.135,P19-1487,0,0.0222493,", What-if, which requires an in-depth understanding of the input source and the ability to reason over disjoint relevant contexts; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. Learning to ask such deep questions has intrinsic research value concerning how human intelligence embodies the skills of curiosity and integration, and will have broad application in future intelligent systems. Despite a clear push towards answering deep questions (exemplified by multi-hop reading comprehension (Cao et al., 2019) and commonsense QA (Rajani et al., 2019)), generating deep questions remains un-investigated. There is thus a clear need to push QG research towards generating deep questions that demand higher cognitive skills. In this paper, we propose the problem of Deep Question Generation (DQG), which aims to generate questions that require reasoning over multiple pieces of information in the passage. Figure 1 b) shows an example of deep question which requires a comparative reasoning over two disjoint pieces of evidences. DQG introduces three additional challenges that are not captured by traditional QG systems. First, unlike generating questi"
2020.acl-main.135,W17-2603,0,0.0409227,"Missing"
2020.acl-main.135,D18-1424,0,0.249997,"xample of Deep Question Generation Figure 1: Examples of shallow/deep QG. The evidence needed to generate the question are highlighted. Introduction Question Generation (QG) systems play a vital role in question answering (QA), dialogue system, and automated tutoring applications – by enriching the training QA corpora, helping chatbots start conversations with intriguing questions, and automatically generating assessment questions, respectively. Existing QG research has typically focused on generating factoid questions relevant to one fact obtainable from a single sentence (Duan et al., 2017; Zhao et al., 2018; Kim et al., 2019), as exemplified in Figure 1 a). However, less explored has been the comprehension and reasoning aspects of questioning, resulting in questions that are shallow and not reflective of the true creative human process. People have the ability to ask deep questions about events, evaluation, opinions, synthesis, or reasons, usually in the form of Why, Why-not, How, What-if, which requires an in-depth understanding of the input source and the ability to reason over disjoint relevant contexts; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy no"
2020.acl-main.135,P17-1099,0,0.0242224,"2 as an example); therefore its constituting words are aligned with the same node representation; (3) keyword information: a word (e.g., a preposition) not appearing in the semantic graph is aligned with the special node vector mentioned before, indicating the word does not carry important information. 3.4 Joint Task Question Generation Based on the semantic-rich input representations, we generate questions via jointly training on two tasks: Question Decoding and Content Selection. Question Decoding. We adopt an attention-based GRU model (Bahdanau et al., 2014) with copying (Gu et al., 2016; See et al., 2017) and coverage mechanisms (Tu et al., 2016) as the question decoder. The decoder takes the semantic-enriched representations ED = {ei , ∀wi ∈ D} from the encoders as the attention memory to generate the output sequence one word at a time. To make the decoder aware of the answer, we use the average word embeddings in the answer to initialize the decoder hidden states. At each decoding step t, the model learns to attend over the input representations ED and compute a context vector ct based on ED and the current decoding state st . Next, the copying probability Pcpy ∈ [0, 1] is calculated from th"
2020.acl-main.135,D18-1427,0,0.0727727,"Missing"
2020.acl-main.135,P16-1008,0,0.0231203,"ng words are aligned with the same node representation; (3) keyword information: a word (e.g., a preposition) not appearing in the semantic graph is aligned with the special node vector mentioned before, indicating the word does not carry important information. 3.4 Joint Task Question Generation Based on the semantic-rich input representations, we generate questions via jointly training on two tasks: Question Decoding and Content Selection. Question Decoding. We adopt an attention-based GRU model (Bahdanau et al., 2014) with copying (Gu et al., 2016; See et al., 2017) and coverage mechanisms (Tu et al., 2016) as the question decoder. The decoder takes the semantic-enriched representations ED = {ei , ∀wi ∈ D} from the encoders as the attention memory to generate the output sequence one word at a time. To make the decoder aware of the answer, we use the average word embeddings in the answer to initialize the decoder hidden states. At each decoding step t, the model learns to attend over the input representations ED and compute a context vector ct based on ED and the current decoding state st . Next, the copying probability Pcpy ∈ [0, 1] is calculated from the context vector ct , the decoder state st"
2020.acl-main.135,D18-1259,0,0.200913,"ncorporates an attention mechanism into the Gated Graph Neural Network (GGNN) (Li et al., 2016), to dynamically model the interactions between different semantic relations; (2) enhancing the word-level passage embeddings and the node-level semantic graph representations to obtain an unified semantic-aware passage representations for question decoding; and (3) introducing an auxiliary content selection task that jointly trains with question decoding, which assists the model in selecting relevant contexts in the semantic graph to form a proper reasoning chain. We evaluate our model on HotpotQA (Yang et al., 2018), a challenging dataset in which the questions are generated by reasoning over text from separate Wikipedia pages. Experimental results show that our model — incorporating both the use of the semantic graph and the content selection task — improves performance by a large margin, in terms of both automated metrics (Section 4.3) and human evaluation (Section 4.5). Error analysis (Section 4.6) validates that our use of the semantic graph greatly reduces the amount of semantic errors in generated questions. In summary, our contributions are: (1) the very first work, to the best of our knowledge, t"
2020.acl-main.135,C12-1030,0,\N,Missing
2020.acl-main.135,P16-1154,0,\N,Missing
2020.acl-main.135,D16-1264,0,\N,Missing
2020.acl-main.135,P17-1149,0,\N,Missing
2020.acl-main.135,D17-1090,0,\N,Missing
2020.acl-main.135,P18-1177,0,\N,Missing
2020.acl-main.135,N19-1240,0,\N,Missing
2020.acl-main.263,S13-1004,0,0.0679212,"Missing"
2020.acl-main.263,D18-1316,0,0.182867,"in the correct answer sentence. Belinkov and Bisk (2018) followed by demonstrating the brittleness of neural machine translation systems against character-level perturbations like randomly swapping/replacing characters. However, these attacks are not optimized on the target models, unlike Ebrahimi et al. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to disrupt the sentence’s semantics, Ribeiro et al. (2018) and Michel et al. (2019) propose searching for adversaries that preserve semantic content. Alzantot et al. (2018) and Jin et al. (2019) explore the use of synonym substitution to create adversarial examples, using word embeddings to find the n nearest words. Eger et al. (2019) take a different approach, arguing that adding visual noise to characters leaves their semantic content undisturbed. Iyyer et al. (2018) propose to create paraphrase adversaries by conditioning their generation on a syntactic template, while Zhang et al. (2019b) swap key entities in the sentences. Zhang et al. (2019a) provide a comprehensive survey of this topic. Adversarial training. In order to ensure our NLP systems are not left"
2020.acl-main.263,N19-3002,0,0.0256124,"bustness, while preserving performance on clean examples (Table 5). To the best of our knowledge, we are the first to investigate the robustness of NLP models to inflectional perturbations and its ethical implications. 2 Related Work Fairness in NLP. It is crucial that NLP systems do not amplify and entrench social biases (Hovy and Spruit, 2016). Recent research on fairness has primarily focused on racial and gender biases within distributed word representations (Bolukbasi et al., 2016), coreference resolution (Rudinger et al., 2018), sentence encoders (May et al., 2019), and language models (Bordia and Bowman, 2019). However, we posit that there exists a significant potential for linguistic bias that has yet to be investigated, which is the motivation for our work. Adversarial attacks in NLP. First discovered in computer vision by Szegedy et al. (2014), adversarial examples are data points crafted with the intent of causing a model to output a wrong prediction. In NLP, this could take place at the character, morphological, lexical, syntactic, or semantic level. Jia and Liang (2017) showed that question answering models could be misled into choosing a distractor sentence in the passage that was created by"
2020.acl-main.263,P16-2096,0,0.0396823,"eness on multiple machine comprehension and translation models, including BERT and Transformer (Tables 1 & 2). • Show that adversarially fine-tuning the model on an adversarial training set generated via weighted random sampling is sufficient for it to acquire significant robustness, while preserving performance on clean examples (Table 5). To the best of our knowledge, we are the first to investigate the robustness of NLP models to inflectional perturbations and its ethical implications. 2 Related Work Fairness in NLP. It is crucial that NLP systems do not amplify and entrench social biases (Hovy and Spruit, 2016). Recent research on fairness has primarily focused on racial and gender biases within distributed word representations (Bolukbasi et al., 2016), coreference resolution (Rudinger et al., 2018), sentence encoders (May et al., 2019), and language models (Bordia and Bowman, 2019). However, we posit that there exists a significant potential for linguistic bias that has yet to be investigated, which is the motivation for our work. Adversarial attacks in NLP. First discovered in computer vision by Szegedy et al. (2014), adversarial examples are data points crafted with the intent of causing a model"
2020.acl-main.263,N18-1170,0,0.0348915,"l. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to disrupt the sentence’s semantics, Ribeiro et al. (2018) and Michel et al. (2019) propose searching for adversaries that preserve semantic content. Alzantot et al. (2018) and Jin et al. (2019) explore the use of synonym substitution to create adversarial examples, using word embeddings to find the n nearest words. Eger et al. (2019) take a different approach, arguing that adding visual noise to characters leaves their semantic content undisturbed. Iyyer et al. (2018) propose to create paraphrase adversaries by conditioning their generation on a syntactic template, while Zhang et al. (2019b) swap key entities in the sentences. Zhang et al. (2019a) provide a comprehensive survey of this topic. Adversarial training. In order to ensure our NLP systems are not left vulnerable to powerful attacks, most existing work make use of adversarial training to improve the model’s robustness (Goodfellow et al., 2015). This involves augmenting the training data either by adding the adversaries to or replacing the clean examples in the training set. Summary. Existing work"
2020.acl-main.263,D17-1215,0,0.0316738,"t al., 2016), coreference resolution (Rudinger et al., 2018), sentence encoders (May et al., 2019), and language models (Bordia and Bowman, 2019). However, we posit that there exists a significant potential for linguistic bias that has yet to be investigated, which is the motivation for our work. Adversarial attacks in NLP. First discovered in computer vision by Szegedy et al. (2014), adversarial examples are data points crafted with the intent of causing a model to output a wrong prediction. In NLP, this could take place at the character, morphological, lexical, syntactic, or semantic level. Jia and Liang (2017) showed that question answering models could be misled into choosing a distractor sentence in the passage that was created by replacing key entities in the correct answer sentence. Belinkov and Bisk (2018) followed by demonstrating the brittleness of neural machine translation systems against character-level perturbations like randomly swapping/replacing characters. However, these attacks are not optimized on the target models, unlike Ebrahimi et al. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to"
2020.acl-main.263,N19-1063,0,0.0496558,"g performance on clean data.1 1 Introduction In recent years, Natural Language Processing (NLP) systems have gotten increasingly better at learning complex patterns in language by pretraining large language models like BERT, GPT-2, and CTRL (Devlin et al., 2019; Radford et al., 2019; Keskar et al., 2019), and fine-tuning them on taskspecific data to achieve state of the art results has become a norm. However, deep learning models are only as good as the data they are trained on. Existing work on societal bias in NLP primarily focuses on attributes like race and gender (Bolukbasi et al., 2016; May et al., 2019). In contrast, we investigate a uniquely NLP attribute that has been largely ignored: linguistic background. Current NLP models seem to be trained with the implicit assumption that everyone speaks fluent (often U.S.) Standard English, even though twothirds (&gt;700 million) of the English speakers in the world speak it as a second language (L2) (Eberhard et al., 2019). Even among native speakers, a significant number speak a dialect like African American Vernacular English (AAVE) rather than Standard English (Crystal, 2003). In addition, these 1 Code and adversarially fine-tuned models available"
2020.acl-main.263,N19-1314,0,0.136714,"hoosing a distractor sentence in the passage that was created by replacing key entities in the correct answer sentence. Belinkov and Bisk (2018) followed by demonstrating the brittleness of neural machine translation systems against character-level perturbations like randomly swapping/replacing characters. However, these attacks are not optimized on the target models, unlike Ebrahimi et al. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to disrupt the sentence’s semantics, Ribeiro et al. (2018) and Michel et al. (2019) propose searching for adversaries that preserve semantic content. Alzantot et al. (2018) and Jin et al. (2019) explore the use of synonym substitution to create adversarial examples, using word embeddings to find the n nearest words. Eger et al. (2019) take a different approach, arguing that adding visual noise to characters leaves their semantic content undisturbed. Iyyer et al. (2018) propose to create paraphrase adversaries by conditioning their generation on a syntactic template, while Zhang et al. (2019b) swap key entities in the sentences. Zhang et al. (2019a) provide a comprehensive su"
2020.acl-main.263,N19-4009,0,0.0397215,"Missing"
2020.acl-main.263,W18-6301,0,0.0191745,"ed solution. To solve this problem, we propose M ORPHEUS (Algorithm 1), an approach that greedily searches for the inflectional form of each noun, verb, or adjective in x that maximally increases f ’s loss (Eq. 1). For each token in x, M ORPHEUS calls M AX I NFLECTED to find the inflected form that caused the greatest increase in f ’s loss.3 Table 1 presents some adversarial examples obtained by running M ORPHEUS on state-of-theart machine reading comprehension and translation models: namely, BERT (Devlin et al., 2019), SpanBERT (Joshi et al., 2019), and Transformer-big (Vaswani et al., 2017; Ott et al., 2018). 3 A task-specific evaluation metric may be used instead of the loss in situations where it is unavailable. However, as we discuss later, the choice of metric is important for optimal performance and should be chosen wisely. Require: Original instance x, Label y, Model f Ensure: Adversarial example x0 T ← TOKENIZE(x) for all i = 1, . . . , |T |do if POS(Ti ) ∈ {NOUN, VERB, ADJ} then I ← G ET I NFLECTIONS(Ti ) Ti ← M AX I NFLECTED(I, T, y, f ) end if end for x0 ← DETOKENIZE(T ) return x0 There are two possible approaches to implementing M AX I NFLECTED: one is to modify each token independentl"
2020.acl-main.263,P02-1040,0,0.108717,"Missing"
2020.acl-main.263,N18-1202,0,0.0583613,"Missing"
2020.acl-main.263,W15-3049,0,0.0418897,"Missing"
2020.acl-main.263,W18-6319,0,0.0326363,"Missing"
2020.acl-main.263,P18-2124,0,0.0425504,"Missing"
2020.acl-main.263,D16-1264,0,0.016695,"cts each eligible word in each original example. Measures. In addition to the raw scores, we also report the relative decrease for easier comparison across models since they perform differently on the clean dataset. Relative decrease (dr ) is calculated using the following formula: dr = 4.1 scoreoriginal − scoreadversarial scoreoriginal (2) Extractive Question Answering Given a question and a passage containing spans corresponding to the correct answer, the model is expected to predict the span corresponding to the answer. Performance for this task is computed using exact match or average F1 (Rajpurkar et al., 2016). We evaluate the effectiveness of our attack using average F1 , which is more forgiving (for the target model). From our experiments, the exact match score is usually between 3-9 points lower than the average F1 score. SQuAD 1.1 and 2.0. The Stanford Question Answering Dataset (SQuAD) comprises over 100,000 question–answer pairs written by crowdworkers 5 https://github.com/bjascob/LemmInflect 2923 https://github.com/alvations/sacremoses Dataset Model Clean Random M ORPHEUS SQuAD 2.0 Answerable Questions (F1 ) GloVe-BiDAF ELMo-BiDAF BERTSQuAD 1.1 SpanBERTSQuAD 1.1 BERTSQuAD 2 SpanBERTSQuAD 2 7"
2020.acl-main.263,P18-1079,0,0.219105,"chances of propagating linguistic discrimination. Hence, in this paper, we: 2 Inflections convey tense, quantity, etc. See Appendix A for dialectal examples. 2920 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2920–2935 c July 5 - 10, 2020. 2020 Association for Computational Linguistics • Propose M ORPHEUS, a method for generating plausible and semantically similar adversaries by perturbing the inflections in the clean examples (Figure 1). In contrast to recent work on adversarial examples in NLP (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018), we exploit morphology to craft our adversaries. • Demonstrate its effectiveness on multiple machine comprehension and translation models, including BERT and Transformer (Tables 1 & 2). • Show that adversarially fine-tuning the model on an adversarial training set generated via weighted random sampling is sufficient for it to acquire significant robustness, while preserving performance on clean examples (Table 5). To the best of our knowledge, we are the first to investigate the robustness of NLP models to inflectional perturbations and its ethical implications. 2 Related Work Fairness in NLP"
2020.acl-main.263,N18-2002,0,0.0526665,"Missing"
2020.acl-main.263,P16-1162,0,0.110478,"Missing"
2020.acl-main.263,N19-1131,0,0.0227152,"Missing"
2020.acl-main.263,W17-1606,0,0.0225226,"izes the target model’s loss. To maximize semantic preservation, M ORPHEUS only considers inflections belonging to the same universal part of speech as the original word. World Englishes exhibit variation at multiple levels of linguistic analysis (Kachru et al., 2009). Therefore, putting these models directly into production without addressing this inherent bias puts them at risk of committing linguistic discrimination by performing poorly for many speech communities (e.g., AAVE and L2 speakers). This could take the form of either failing to understand these speakers (Rickford and King, 2016; Tatman, 2017), or misinterpreting them. For example, the recent mistranslation of a minority speaker’s social media post resulted in his wrongful arrest (Hern, 2017). Since L2 (and many L1 dialect) speakers often exhibit variability in their production of inflectional morphology2 (Lardiere, 1998; Pr´evost and White, 2000; Haznedar, 2002; White, 2003; Seymour, 2004), we argue that NLP models should be robust to inflectional perturbations in order to minimize their chances of propagating linguistic discrimination. Hence, in this paper, we: 2 Inflections convey tense, quantity, etc. See Appendix A for dialect"
2020.acl-main.263,P18-2006,0,\N,Missing
2020.acl-main.263,N19-1165,0,\N,Missing
2020.acl-main.263,N19-1423,0,\N,Missing
2020.acl-main.263,W19-4406,0,\N,Missing
2020.coling-main.238,D15-1109,0,0.0417366,"Missing"
2020.coling-main.238,L16-1432,0,0.0312328,"Missing"
2020.coling-main.238,D18-1241,0,0.0475096,"Missing"
2020.coling-main.238,N19-1423,0,0.0529108,"prehension for multiparty dialogs Methods. SQuAD 2.0 is an MRC dataset that adopts a passage as the input and the answer is a span from input passage (Rajpurkar et al., 2018). We adopt the following existing methods for SQuAD 2.0 on our dataset. In this paper, we use three different kinds of settings of BERT: BERT-base, BERT-large, and BERT-whole word masking (BERT-wwm). We concatenate all utterances from input dialog as a passage, and each utterance includes speaker and text. We used the open-source code of BERT to perform our experiments3 . BERT is a bidirectional encoder from transformers (Devlin et al., 2019). To learn better representations for text, BERT adopts two objectives: masked language modeling and the next sentence prediction during pretraining. In the BERT-wwm, if a part of a complete word WordPiece is replaced by [mask], the other parts of the same word will also be replaced by mask, which is the whole word mask. 3 https://github.com/google-research/bert 2648 Table 7: Results of machine reading comprehension for multiparty dialogs. Method BERT-base BERT-large BERT-wwm Human performance Human-machine gap EM Squad 2.0 73.1 80.0 86.7 86.8 0.1 Our 45.3 51.8 54.7 64.3 9.6 F1 Squad 2.0 76.2"
2020.coling-main.238,P17-1147,0,0.0218738,"in–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structur"
2020.coling-main.238,Q18-1023,0,0.0494099,"Missing"
2020.coling-main.238,D17-1082,0,0.168593,"y chat dialogs (Asher et al., 2016). The corpus derives from the online version of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et"
2020.coling-main.238,P19-1210,0,0.0607923,"Missing"
2020.coling-main.238,W15-4640,0,0.0772636,"r speakers converse over seven utterances. We additionally employ annotators to read the passage and contribute questions: in the example, the annotators propose three questions: two answerable and one unanswerable. We observe that adjacent utterance pairs can be incoherent, illustrating the key challenge. It is non-trivial to detect discourse relations between non-adjacent utterances; and crucially, difficult to correctly interpret a multiparty dialog without a proper understanding of the input’s complex structure. We derived Molweni from the large-scale multiparty dialog Ubuntu Chat Corpus (Lowe et al., 2015). We chose the name Molweni, as it is the plural form of “Hello” in the Xhosa language, representing multiparty dialog in the same language as Ubuntu. Our dataset contains 10,000 dialogs with 88,303 utterances and 30,066 questions including answerable and unanswerable questions. All answerable questions are extractive questions whose answer is a span in the source dialog. For unanswerable questions, we annotate their plausible answers from dialog. Most questions in Molweni are 5W1H questions – Why, What, Who, Where, When, and How. For each dialog in the corpus, annotators propose three questio"
2020.coling-main.238,N18-1185,0,0.141448,"et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multiparty dialogs and unanswerable questions. 3 The Molweni corpus Our dataset derives from the large scale multiparty dialogs dataset — the Ubuntu Chat Corpus (Lowe et al., 2015). We list our three reasons in choosing the Ubuntu Chat Corpus as the base corpus for annotation. • First, the Ubuntu dataset is a large multiparty dataset. After filtering the dataset by only retaining all utterances wi"
2020.coling-main.238,N16-1013,0,0.0594168,"Missing"
2020.coling-main.238,prasad-etal-2008-penn,0,0.129569,"Missing"
2020.coling-main.238,D16-1264,0,0.0434744,"game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes th"
2020.coling-main.238,P18-2124,0,0.204274,"course relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multipa"
2020.coling-main.238,Q19-1016,0,0.0616554,"Missing"
2020.coling-main.238,D13-1020,0,0.0464644,"urse parsing on multiparty chat dialogs (Asher et al., 2016). The corpus derives from the online version of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2"
2020.coling-main.238,Q19-1014,0,0.222945,"2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multiparty dialogs and unanswerable questions. 3 The Molweni corpus Our dataset derives from the large scale multiparty dialogs dataset — the Ubuntu Chat Corpus (Lowe et al., 2015). We list our three reasons in choosing the Ubuntu Chat Corpus as the base corpus for annotation. • First, the Ubuntu dataset is a large multiparty dataset. After filtering the dataset by only retaining all utterances with response relations, the"
2020.coling-main.238,W17-2623,0,0.022661,"roduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additi"
2020.coling-main.238,D07-1003,0,0.0332586,"from the online version of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Differ"
2020.coling-main.238,W19-5923,0,0.0645038,"datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multiparty dialogs and unanswerable questions. 3 The Molweni corpus Our dataset derives from the large scale multiparty dialogs dataset — the Ubuntu Chat Corpus (Lowe et al., 2015). We list our three reasons in choosing the Ubuntu Chat Corpus as the base corpus for annotation. • First, the Ubuntu dataset is a large multiparty dataset. After filtering the dataset by only retaining all utterances with response relations, there are still over 380K sessions and"
2020.coling-main.238,D15-1237,0,0.0235004,"rsion of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three"
2020.coling-main.238,P19-1374,0,\N,Missing
2020.coling-main.513,C18-1139,0,0.0129003,"idely-used approach is Sequence Labelling. These tasks involve prediction of a label for each of the tokens present in the textual content. Contiguous chunks of text are then treated as the prediction target. In the Named Entity Recognition (NER) task, entities such as organization, person, location are extracted from the text. Typically these methods employ Bidirectional Long Short Term Memory (BiLSTM) stacked with Conditional Random Field (CRF) layers to decode the sequence (Huang et al., 2015; Ma and Hovy, 2016). Context-sensitive character embedding methods also enhance the effectiveness (Akbik et al., 2018). Although our task can be formulated as a sequence labeling task, we take the multi-label classification perspective for two reasons. First, the label space is finite: most job portal interfaces allow the user to choose a subset from a predefined list of skills in the system. Second, to the best of our knowledge, there is no existing dataset with manually-annotated labels from textual JDs, making it sub-optimal to train a sequence labeling model. The existing approaches do not address all the unique challenges associated with our skill extraction problem. Alternate techniques of exploiting la"
2020.coling-main.513,N19-1423,0,0.252369,"rs, such as the GRU (Chung et al., 2014). These methods have been used in downstream applications such as tag prediction on Wikipedia articles and news articles, as well as recommending textual articles to potentially interested users. Language Modeling: Recent developments in language modeling are also relevant to our task. The objective here is to model the syntactic and semantic structure of input language utterances through model training to predict tokens, based on the available contextual information. Models such as ELMo (Peters et al., 2018), Transformer and BERT (Vaswani et al., 2017; Devlin et al., 2019) have significantly improved a bevy of NLP tasks through their unsupervised pre-trained representations. Another relevant and widely-used approach is Sequence Labelling. These tasks involve prediction of a label for each of the tokens present in the textual content. Contiguous chunks of text are then treated as the prediction target. In the Named Entity Recognition (NER) task, entities such as organization, person, location are extracted from the text. Typically these methods employ Bidirectional Long Short Term Memory (BiLSTM) stacked with Conditional Random Field (CRF) layers to decode the s"
2020.coling-main.513,D14-1181,0,0.120513,"matrix. They reliably learn embeddings for a lower-dimensional label space, then use suitable decompression techniques to map them back to the original label space (Bhatia et al., 2015; Cisse et al., 2013). More recently, methods have been proposed to reduce the information loss during the decompression phase, such as LPSR (Weston et al., 2013) and MLRF (Agrawal et al., 2013). Following the success of Computer Vision community, deep learning-based approaches have led to a surge of performance in multiple natural language modeling tasks. Neural approaches to natural language processing (e.g., (Kim, 2014)) have also been applied to XMLC tasks (Halder et al., 2018). Methods such as XML-CNN have been proposed, which uses a bottleneck layer to reduce the number of learnable parameters (Liu et al., 2017). A cluster sensitive attention mechanism has also been explored to capture the correlation between labels (Halder et al., 2018), in conjunction with RNN-based text encoders, such as the GRU (Chung et al., 2014). These methods have been used in downstream applications such as tag prediction on Wikipedia articles and news articles, as well as recommending textual articles to potentially interested u"
2020.coling-main.513,P16-1101,0,0.0153263,"a bevy of NLP tasks through their unsupervised pre-trained representations. Another relevant and widely-used approach is Sequence Labelling. These tasks involve prediction of a label for each of the tokens present in the textual content. Contiguous chunks of text are then treated as the prediction target. In the Named Entity Recognition (NER) task, entities such as organization, person, location are extracted from the text. Typically these methods employ Bidirectional Long Short Term Memory (BiLSTM) stacked with Conditional Random Field (CRF) layers to decode the sequence (Huang et al., 2015; Ma and Hovy, 2016). Context-sensitive character embedding methods also enhance the effectiveness (Akbik et al., 2018). Although our task can be formulated as a sequence labeling task, we take the multi-label classification perspective for two reasons. First, the label space is finite: most job portal interfaces allow the user to choose a subset from a predefined list of skills in the system. Second, to the best of our knowledge, there is no existing dataset with manually-annotated labels from textual JDs, making it sub-optimal to train a sequence labeling model. The existing approaches do not address all the un"
2020.coling-main.513,N18-1202,0,0.24043,"s (Halder et al., 2018), in conjunction with RNN-based text encoders, such as the GRU (Chung et al., 2014). These methods have been used in downstream applications such as tag prediction on Wikipedia articles and news articles, as well as recommending textual articles to potentially interested users. Language Modeling: Recent developments in language modeling are also relevant to our task. The objective here is to model the syntactic and semantic structure of input language utterances through model training to predict tokens, based on the available contextual information. Models such as ELMo (Peters et al., 2018), Transformer and BERT (Vaswani et al., 2017; Devlin et al., 2019) have significantly improved a bevy of NLP tasks through their unsupervised pre-trained representations. Another relevant and widely-used approach is Sequence Labelling. These tasks involve prediction of a label for each of the tokens present in the textual content. Contiguous chunks of text are then treated as the prediction target. In the Named Entity Recognition (NER) task, entities such as organization, person, location are extracted from the text. Typically these methods employ Bidirectional Long Short Term Memory (BiLSTM)"
2020.emnlp-main.455,P18-2006,0,0.0422403,"ction by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 2018; Cheng et al., 2019). However, this generally involves retraining the model with the adversarial data, which is computationally expensive and time-consuming. Tan et al. (2020) showed that simply fine-tuning a trained model for a single epoch on appropriately generated adversarial training data is sufficient to harden the model against inflectional adversaries. Instead of adversarial training, Piktus et al. (2019) train word embeddings to be robust to misspellings, while Zhou et al. (2019b) propose using a BERT-based model to d"
2020.emnlp-main.455,D18-1366,0,0.0307226,"as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 2018; Cheng et al., 2019). However, this generally involves retraining the model with the adversarial data, which is computationally expensive and time-consuming. Tan et al. (2020) showed that simply fi"
2020.emnlp-main.455,P19-1425,0,0.0331759,"pose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 2018; Cheng et al., 2019). However, this generally involves retraining the model with the adversarial data, which is computationally expensive and time-consuming. Tan et al. (2020) showed that simply fine-tuning a trained model for a single epoch on appropriately generated adversarial training data is sufficient to harden the model against inflectional adversaries. Instead of adversarial training, Piktus et al. (2019) train word embeddings to be robust to misspellings, while Zhou et al. (2019b) propose using a BERT-based model to detect adversaries and recover clean examples. Jia et al. (2019) and Huang et al. (2019)"
2020.emnlp-main.455,W02-1001,0,0.0335196,"arn each inflection’s grammatical role more quickly. This is because the model does not need to first learn that the same grammatical category can manifest in orthographically different forms. Crucially, the original sentence can usually be reconstructed from the base forms and grammatical information preserved by the inflection symbols, except in cases of overabundance (Thornton, 2019). Implementation details. We use the BertPreTokenizer from the tokenizers6 library for whitespace and punctuation splitting. We use the NLTK (Bird et al., 2009) implementation of the averaged perceptron tagger (Collins, 2002) with greedy decoding to generate POS tags, which serve to improve lemmatization accuracy and as inflection symbols. For lemmatization and reinflection, we use lemminflect7 , which uses a dictionary look-up together with rules for lemmatizing and inflecting words. A benefit of this approach is that the neural network can now generate orthographically appropriate inflected forms by generating the base form and the corresponding inflection symbol. 3.2 Compatibility with Data-Driven Methods Although BITE has the numerous advantages outlined above, it suffers from the same weakness as regular word"
2020.emnlp-main.455,W02-0603,0,0.0426279,"en subword tokenizer such as byte pair encoding (BPE; Sennrich et al. (2016)). However, a purely data-driven approach may fail to find the optimal encoding, both in terms of vocabulary efficiency and cross-dialectal generalization. This could make the neural model more vulnerable to inflectional perturbations. Hence, we: • Propose Base-InflecTion Encoding (BITE), which uses morphological information to help the data-driven tokenizer use its vocabulary efficiently and generate robust symbol3 sequences. In contrast to morphological segmentors such as Linguistica (Goldsmith, 2000) and Morfessor (Creutz and Lagus, 2002), we reduce inflected forms to their base forms before reinjecting the inflection information into the encoded sequence as special symbols. This approach gracefully handles the canonicalization of words with nonconcatenative morphology while generally allowing the original sentence to be reconstructed. • Demonstrate BITE’s effectiveness at making neural NLP systems robust to non-standard inflection use while preserving performance on Standard English examples. Crucially, simply fine-tuning the pretrained model for the downstream task after adding BITE is sufficient. Unlike adversarial training"
2020.emnlp-main.455,D19-1423,0,0.0676907,"Missing"
2020.emnlp-main.455,P16-2090,0,0.0626025,"Missing"
2020.emnlp-main.455,D07-1091,0,0.120448,"19) reduce vocabulary sizes by operating on bytes instead of characters (as in standard BPE). To make subword regularization more tractable, Kudo (2018) proposed an alternative method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples,"
2020.emnlp-main.455,W17-3204,0,0.0141092,"tion of subwords in the vocabulary. Wang et al. (2019) reduce vocabulary sizes by operating on bytes instead of characters (as in standard BPE). To make subword regularization more tractable, Kudo (2018) proposed an alternative method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To"
2020.emnlp-main.455,W18-6301,0,0.025453,"with nonconcatenative morphology while generally allowing the original sentence to be reconstructed. • Demonstrate BITE’s effectiveness at making neural NLP systems robust to non-standard inflection use while preserving performance on Standard English examples. Crucially, simply fine-tuning the pretrained model for the downstream task after adding BITE is sufficient. Unlike adversarial training, BITE does not enlarge the dataset and is more computationally efficient. • Show that BITE helps BERT (Devlin et al., 2019) generalize to dialects unseen during training and also helps Transformer-big (Ott et al., 2018) converge faster for the WMT’14 En-De task. • Propose metrics like symbol complexity to operationalize and evaluate the vocabulary efficiency of an encoding scheme. Our metrics are generic and can be used to evaluate any tokenizer. 2 Related Work Subword tokenization. Before neural models can learn, raw text must first be encoded into symbols with the help of a fixed-size vocabulary. Early 3 Following Sennrich et al. (2016), we use symbol instead of token to avoid confusion with the unencoded word token. models represented each word as a single symbol in the vocabulary (Bengio et al., 2001; Co"
2020.emnlp-main.455,P02-1040,0,0.107697,"Missing"
2020.emnlp-main.455,N19-1326,0,0.0299045,"versarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 2018; Cheng et al., 2019). However, this generally involves retraining the model with the adversarial data, which is computationally expensive and time-consuming. Tan et al. (2020) showed that simply fine-tuning a trained model for a single epoch on appropriately generated adversarial training data is sufficient to harden the model against inflectional adversaries. Instead of adversarial training, Piktus et al. (2019) train word embeddings to be robust to misspellings, while Zhou et al. (2019b) propose using a BERT-based model to detect adversaries and recover clean examples. Jia et al. (2019) and Huang et al. (2019) use Interval Bound Propagation to train provably robust pre-Transformer models, while Shi et al. (2020) propose an efficient algorithm for training certifiably robust Transformer architectures. Summary. Popular subword tokenizers operate on surface forms in a purely data-driven manner. Existing adversarial robustness methods for largescale Transformers are computationally expensive, while prov"
2020.emnlp-main.455,P18-2124,0,0.0283581,"+ WP (+1 epoch) 79.07 75.46 72.21 72.56 74.45 73.69 68.23 70.66 83.86 82.21 83.87 81.05 83.86 83.36 75.77 81.04 Table 1: BERTbase results on the clean and adversarial MultiNLI and SQuAD 2.0 examples. We compare BITE+WordPiece to both WordPiece alone and with one epoch of adversarial fine-tuning. For fair comparison with adversarial fine-tuning, we trained the BITE+WordPiece model for an extra epoch (bottom) on clean data. 4.1 Adversarial Robustness (Classification) We evaluate BITE’s ability to improve model robustness for question answering and natural language understanding using SQuAD 2.0 (Rajpurkar et al., 2018) and MultiNLI (Williams et al., 2018), respectively. We use M ORPHEUS (Tan et al., 2020), an adversarial attack targeting inflectional morphology, to test the overall system’s robustness to non-standard inflections. They previously demonstrated M ORPHEUS’s ability to generate plausible and semantically equivalent adversarial examples resembling L2 English sentences. We attack each BERTbase model separately and report F1 scores on the answerable questions and the full SQuAD 2.0 dataset, following Tan et al. (2020). In addition, for MNLI, we report scores for both the in-domain (MNLI) and out-of"
2020.emnlp-main.455,P18-1079,0,0.19352,"odels, such as BERTScore (Zhang et al., 2020). In particular, Tan et al. (2020) show that current question answering and machine translation systems are overly sensitive to non-standard inflections—a common feature of dialects such as Colloquial Singapore English (CSE) and African American Vernacular English (AAVE).2 Since people naturally correct for or ignore non-standard inflection use (Foster and Wigglesworth, 2016), we should expect NLP systems to be equally robust. Existing work on adversarial robustness for NLP primarily focuses on adversarial training methods (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Tan et al., 2020) or classifying and correcting adversarial examples (Zhou et al., 2019a). However, this effectively increases the size of the training dataset by including adversarial examples or training a new model to identify and correct perturbations, thereby significantly increasing the overall computational cost of creating robust models. These approaches also only operate on either raw text or the model, ignoring tokenization—an operation that transforms raw text into a form that the neural network can learn from. We introduce a 2 Examples in Appendix A. new representation for word t"
2020.emnlp-main.455,P06-1001,0,0.0716823,"cabulary. Wang et al. (2019) reduce vocabulary sizes by operating on bytes instead of characters (as in standard BPE). To make subword regularization more tractable, Kudo (2018) proposed an alternative method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems agains"
2020.emnlp-main.455,2020.acl-main.240,0,0.044966,"Missing"
2020.emnlp-main.455,P16-1162,0,0.287998,"te on either raw text or the model, ignoring tokenization—an operation that transforms raw text into a form that the neural network can learn from. We introduce a 2 Examples in Appendix A. new representation for word tokens that separates base from inflection. This improves both model robustness and vocabulary efficiency by explicitly inducing linguistic structure in the input to the NLP system (Erdmann et al., 2019; Henderson, 2020). Many extant NLP systems use a combination of a whitespace and punctuation tokenizer followed by a data-driven subword tokenizer such as byte pair encoding (BPE; Sennrich et al. (2016)). However, a purely data-driven approach may fail to find the optimal encoding, both in terms of vocabulary efficiency and cross-dialectal generalization. This could make the neural model more vulnerable to inflectional perturbations. Hence, we: • Propose Base-InflecTion Encoding (BITE), which uses morphological information to help the data-driven tokenizer use its vocabulary efficiently and generate robust symbol3 sequences. In contrast to morphological segmentors such as Linguistica (Goldsmith, 2000) and Morfessor (Creutz and Lagus, 2002), we reduce inflected forms to their base forms befor"
2020.emnlp-main.455,2020.acl-main.263,1,0.815286,"er to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.1 1 Introduction Large-scale neural models have proven successful at a wide range of natural language processing (NLP) tasks but are susceptible to amplifying discrimination against minority linguistic communities (Hovy and Spruit, 2016; Tan et al., 2020) due to selection bias in the training data and model overamplification (Shah et al., 2019). Most datasets implicitly assume a distribution of error-free Standard English speakers, but this does not accurately reflect the majority of the global English speaking population who are either second language (L2) or non-standard dialect speakers (Crystal, 2003; Eberhard et al., 2019). These World Englishes differ at lexical, morphological, and syntactic levels (Kachru et al., 2009); sensitivity to 1 Code will be available at github.com/salesforce/bite. Figure 1: Base-Inflection Encoding reduces infl"
2020.emnlp-main.455,W17-1606,0,0.0164059,"ither second language (L2) or non-standard dialect speakers (Crystal, 2003; Eberhard et al., 2019). These World Englishes differ at lexical, morphological, and syntactic levels (Kachru et al., 2009); sensitivity to 1 Code will be available at github.com/salesforce/bite. Figure 1: Base-Inflection Encoding reduces inflected words to their base forms, then reinjects the grammatical information into the sentence as inflection symbols. these variations predisposes English NLP systems to discriminate against speakers of World Englishes by either misunderstanding or misinterpreting them (Hern, 2017; Tatman, 2017). Left unchecked, these biases could inadvertently propagate to future models via metrics built around pretrained models, such as BERTScore (Zhang et al., 2020). In particular, Tan et al. (2020) show that current question answering and machine translation systems are overly sensitive to non-standard inflections—a common feature of dialects such as Colloquial Singapore English (CSE) and African American Vernacular English (AAVE).2 Since people naturally correct for or ignore non-standard inflection use (Foster and Wigglesworth, 2016), we should expect NLP systems to be equally robust. Existing"
2020.emnlp-main.455,W19-2304,0,0.0234626,", where this is often the case. In contrast, BITE performs equally well on both in- and out-of-domain data, demonstrating its applicability to practical scenarios where the training and testing domains may not match. This is the result of preserving the base forms, which we investigate further in §5.2. 4.3 Dialectal Variation Apart from second languages, dialects are another common source of non-standard inflections. However, there is a dearth of task-specific datasets in English dialects like AAVE and CSE. Therefore, in this section’s experiments, we use the model’s pseudo perplexity (pPPL) (Wang and Cho, 2019) on monodialectal corpora as a proxy for its performance on downstream tasks in the corresponding dialect. The pPPL measures how certain the pretrained model is about its prediction and reflects its generalization ability on the dialectal datasets. To ensure fair comparisons across different subword segmentations, we normalize the pseudo log-likelihoods by the number of word tokens fed into the WordPiece component of each tokenization pipeline (Mielke, 2019). This avoids unfairly penalizing BITE for inevitably generating longer sequences. Finally, we scale the pseudo loglikelihoods by the mask"
2020.emnlp-main.455,D19-5214,0,0.0238724,"ry (Bengio et al., 2001; Collobert et al., 2011) and uncommon words were represented by an unknown symbol. However, such a representation is unable to adequately deal with words absent in the training vocabulary. Therefore, subword representations like WordPiece (Schuster and Nakajima, 2012) and BPE (Sennrich et al., 2016) were proposed to encode out-of-vocabulary (OOV) words by segmenting them into subwords and encoding each subword as a separate symbol. This way, less information is lost in the encoding process since OOV words are approximated as a combination of subwords in the vocabulary. Wang et al. (2019) reduce vocabulary sizes by operating on bytes instead of characters (as in standard BPE). To make subword regularization more tractable, Kudo (2018) proposed an alternative method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koe"
2020.emnlp-main.455,N18-1101,0,0.0187221,"6 74.45 73.69 68.23 70.66 83.86 82.21 83.87 81.05 83.86 83.36 75.77 81.04 Table 1: BERTbase results on the clean and adversarial MultiNLI and SQuAD 2.0 examples. We compare BITE+WordPiece to both WordPiece alone and with one epoch of adversarial fine-tuning. For fair comparison with adversarial fine-tuning, we trained the BITE+WordPiece model for an extra epoch (bottom) on clean data. 4.1 Adversarial Robustness (Classification) We evaluate BITE’s ability to improve model robustness for question answering and natural language understanding using SQuAD 2.0 (Rajpurkar et al., 2018) and MultiNLI (Williams et al., 2018), respectively. We use M ORPHEUS (Tan et al., 2020), an adversarial attack targeting inflectional morphology, to test the overall system’s robustness to non-standard inflections. They previously demonstrated M ORPHEUS’s ability to generate plausible and semantically equivalent adversarial examples resembling L2 English sentences. We attack each BERTbase model separately and report F1 scores on the answerable questions and the full SQuAD 2.0 dataset, following Tan et al. (2020). In addition, for MNLI, we report scores for both the in-domain (MNLI) and out-of-domain dev. set (MNLI-MM). BITE+Word"
2020.emnlp-main.455,P15-2111,0,0.0316112,"e method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 20"
2020.emnlp-main.455,D19-1496,0,0.163056,"rrent question answering and machine translation systems are overly sensitive to non-standard inflections—a common feature of dialects such as Colloquial Singapore English (CSE) and African American Vernacular English (AAVE).2 Since people naturally correct for or ignore non-standard inflection use (Foster and Wigglesworth, 2016), we should expect NLP systems to be equally robust. Existing work on adversarial robustness for NLP primarily focuses on adversarial training methods (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Tan et al., 2020) or classifying and correcting adversarial examples (Zhou et al., 2019a). However, this effectively increases the size of the training dataset by including adversarial examples or training a new model to identify and correct perturbations, thereby significantly increasing the overall computational cost of creating robust models. These approaches also only operate on either raw text or the model, ignoring tokenization—an operation that transforms raw text into a form that the neural network can learn from. We introduce a 2 Examples in Appendix A. new representation for word tokens that separates base from inflection. This improves both model robustness and vocabu"
2020.emnlp-main.455,P16-2096,0,\N,Missing
2020.emnlp-main.455,N18-1170,0,\N,Missing
2020.emnlp-main.455,N19-1423,0,\N,Missing
2020.emnlp-main.455,D19-1419,0,\N,Missing
2020.emnlp-main.564,D14-1179,0,0.0128654,"Missing"
2020.emnlp-main.564,N19-1423,0,0.038154,"tion, we use M LP to mean multilayer perceptron, ⊕ to represent the concatenation operation, and bold symbols to denote dense representations. Base Model We now detail the base model which consists of 1 an encoder and a decoder. The encoder (part in Figure 2) processes the input (i.e., Q and E) into hidden representation (denoted as h) and the 2 in Figure 2) generates the SQL decoder (part query (i.e, S) accordingly. Encoder: Following (Hwang et al., 2019; Guo et al., 2019; Zhang et al., 2019), we concatenate the input query Q and database schema E to an integrated sequence as input for BERT (Devlin et al., 2019) to generate embeddings for each question token and element in the schema (namely Q = {qi }|Q| i=1 and E = {ei }|E| i=1 ) and the overall representation for the input as h. Here, E consists of embeddings of all the columns/tables and the special token [none]. The embedding of the special token [CLS] in BERT is taken as h. Formally, we have: |Q| |E| {[CLS], Q, E} → h, {qi }i=1 , {ei }i=1 . (1) Note that, in this representation, the schema linking information has also been captured by the multilayer self-attention implicitly. However, we argue the explicit supervisions are required. While a plau"
2020.emnlp-main.564,P16-1004,0,0.0162855,"es and the special token [none]. The embedding of the special token [CLS] in BERT is taken as h. Formally, we have: |Q| |E| {[CLS], Q, E} → h, {qi }i=1 , {ei }i=1 . (1) Note that, in this representation, the schema linking information has also been captured by the multilayer self-attention implicitly. However, we argue the explicit supervisions are required. While a plausible solution is to use the relation-aware encoding proposed by Wang et al. (2020) to do this, we later propose a simpler solution to facilitate our analytical study. Decoder: Inspired by the prior work (Yin and Neubig, 2017; Dong and Lapata, 2016, 2018; Zhang et al., 2019), we adopt a two-step decoder to generate the SQL query from the hidden representation h. We first generate a coarse SQL query S 0 , namely a SQL sequence without aggregate functions, using a GRU network (Cho et al., 2014). We then synthesize the final SQL query S based on S 0 . 2 part in Figure 2 illustrates the generation The of aggregate functions for the column budget during the decoding process. 4.2 Schema Linking Extension To study the role of schema linking, we extend the encoder to explicitly capture the schema linking information. It works in two steps: in s"
2020.emnlp-main.564,P18-1068,0,0.0255061,"research on the areas of problem identification, dataset construction and model evaluation. 2 Related Work Text-to-SQL Parsing: Text-to-SQL parsing has been long studied in past decades (Finegan-Dollak et al., 2018; Yu et al., 2018c). Early text-to-SQL systems rely heavily on complicated rules and handcrafted feature engineering (Zhong et al., 2017; Finegan-Dollak et al., 2018). Fortunately, the research progress has been largely accelerated in recent years thanks to both large-scale text-to-SQL datasets (Zhong et al., 2017; Yu et al., 2018c) and interests in neural modeling (Xu et al., 2017; Dong and Lapata, 2018; Sun et al., 2018; Yu et al., 2018b; Guo et al., 2019; Wang et al., 2020). With years of studies, current research on this task focuses on addressing cross-domain generalizability and generating complex SQL queries. To improve cross-domain generalizability, advanced representations of the schema and the queries are explored, e.g., graph-based schema representations (Bogin et al., 2019b,a), contextualized question representations (Hwang et al., 2019; Guo et al., 2019) and relation-aware self-attention (Wang et al., 2020). As for the complex SQL query generation, approaches are proposed to cons"
2020.emnlp-main.564,2020.emnlp-main.521,0,0.0284301,"proposed to constrain the output with SQL grammar, e.g., modular decoders for separate SQL clauses (Yu et al., 2018b), intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pr"
2020.emnlp-main.564,P19-1444,0,0.0722991,"linking for future developments of text-to-SQL tasks.1 1 Introduction Structured Query Language (SQL), while exact and powerful, suffers from a complex grammar presenting significant challenges for laymen to write queries. Automatically parsing natural language into SQL (text-to-SQL) thus has huge potential, as it would enable lay users to mine the world’s structured data using natural language queries. To achieve practical text-to-SQL workflow, a model needs to correlate natural language queries with the given database. Therefore, schema linking is considered helpful for text-to-SQL parsing (Guo et al., 2019; Bogin et al., 2019b; Dong et al., 2019; Wang et al., 2020). Here, schema linking means identifying references of columns, tables and condition values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the"
2020.emnlp-main.564,D18-1190,0,0.0119239,"e idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string matching between natural language utterances and column/table names (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019). As discussed in Dong et al. (2019), such simple heuristics are difficult to accurately identify columns/tables involved in a natural language utterance and well understand the relation between an utterance and the corresponding database schema. Therefore, they make the first step towards treating sch"
2020.emnlp-main.564,D19-1624,0,0.0124777,"plex SQL queries. To improve cross-domain generalizability, advanced representations of the schema and the queries are explored, e.g., graph-based schema representations (Bogin et al., 2019b,a), contextualized question representations (Hwang et al., 2019; Guo et al., 2019) and relation-aware self-attention (Wang et al., 2020). As for the complex SQL query generation, approaches are proposed to constrain the output with SQL grammar, e.g., modular decoders for separate SQL clauses (Yu et al., 2018b), intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annota"
2020.emnlp-main.564,J82-2003,0,0.69635,"Missing"
2020.emnlp-main.564,P19-1335,0,0.0220677,"modular decoders for separate SQL clauses (Yu et al., 2018b), intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string mat"
2020.emnlp-main.564,D15-1166,0,0.059739,"Missing"
2020.emnlp-main.564,P14-5010,0,0.00440625,"Missing"
2020.emnlp-main.564,D18-1299,0,0.0236196,"Missing"
2020.emnlp-main.564,D17-1127,0,0.016122,"Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string matching between natural language utterances and column/table names (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019). As discussed in Dong et al. (2019), such simple heuristics are difficult to accurately identify columns/tables involved in a natural language utterance and well understand the relation between an utterance and the corresponding database schema. Therefore, they make the first"
2020.emnlp-main.564,P18-1034,0,0.0162153,"f problem identification, dataset construction and model evaluation. 2 Related Work Text-to-SQL Parsing: Text-to-SQL parsing has been long studied in past decades (Finegan-Dollak et al., 2018; Yu et al., 2018c). Early text-to-SQL systems rely heavily on complicated rules and handcrafted feature engineering (Zhong et al., 2017; Finegan-Dollak et al., 2018). Fortunately, the research progress has been largely accelerated in recent years thanks to both large-scale text-to-SQL datasets (Zhong et al., 2017; Yu et al., 2018c) and interests in neural modeling (Xu et al., 2017; Dong and Lapata, 2018; Sun et al., 2018; Yu et al., 2018b; Guo et al., 2019; Wang et al., 2020). With years of studies, current research on this task focuses on addressing cross-domain generalizability and generating complex SQL queries. To improve cross-domain generalizability, advanced representations of the schema and the queries are explored, e.g., graph-based schema representations (Bogin et al., 2019b,a), contextualized question representations (Hwang et al., 2019; Guo et al., 2019) and relation-aware self-attention (Wang et al., 2020). As for the complex SQL query generation, approaches are proposed to constrain the output w"
2020.emnlp-main.564,2020.acl-main.677,0,0.116538,"ntroduction Structured Query Language (SQL), while exact and powerful, suffers from a complex grammar presenting significant challenges for laymen to write queries. Automatically parsing natural language into SQL (text-to-SQL) thus has huge potential, as it would enable lay users to mine the world’s structured data using natural language queries. To achieve practical text-to-SQL workflow, a model needs to correlate natural language queries with the given database. Therefore, schema linking is considered helpful for text-to-SQL parsing (Guo et al., 2019; Bogin et al., 2019b; Dong et al., 2019; Wang et al., 2020). Here, schema linking means identifying references of columns, tables and condition values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the SQL query. Existing solutions largely treat schema linking"
2020.emnlp-main.564,P18-1134,0,0.0208586,"intermediate language representation (Guo et al., 2019), recursive decoding for nested queries (Lee, 2019), schemadependent grammar for SQL decoding (Lin et al., 6944 2019), etc. Unlike their perspective, this work calls attention to schema linking, which we consider is the crux for the text-to-SQL task and yet to be sufficiently studied. Schema Linking: The idea of schema linking has been broadly studied in similar tasks like entity linking in the field of knowledge graphs (Fu et al., 2020; Wu et al., 2019; Rijhwani et al., 2019; Logeswaran et al., 2019) and slot filling in dialogue systems (Xu and Hu, 2018; Ren et al., 2018; Nouri and Hosseini-Asl, 2018; Rastogi et al., 2017), where ample annotated data and models have been proposed to address their specific properties. In the general domain of semantic parsing, it has been demonstrated that decoupling underlying structure with lexicon benefits cross-domain semantic parsing (Su and Yan, 2017; Herzig and Berant, 2018). However, when it comes to the text-to-SQL problem, many existing approaches treat schema linking as a minor pre-processing procedure using simple heuristics, such as string matching between natural language utterances and column/t"
2020.emnlp-main.564,P17-1041,0,0.0399859,"f all the columns/tables and the special token [none]. The embedding of the special token [CLS] in BERT is taken as h. Formally, we have: |Q| |E| {[CLS], Q, E} → h, {qi }i=1 , {ei }i=1 . (1) Note that, in this representation, the schema linking information has also been captured by the multilayer self-attention implicitly. However, we argue the explicit supervisions are required. While a plausible solution is to use the relation-aware encoding proposed by Wang et al. (2020) to do this, we later propose a simpler solution to facilitate our analytical study. Decoder: Inspired by the prior work (Yin and Neubig, 2017; Dong and Lapata, 2016, 2018; Zhang et al., 2019), we adopt a two-step decoder to generate the SQL query from the hidden representation h. We first generate a coarse SQL query S 0 , namely a SQL sequence without aggregate functions, using a GRU network (Cho et al., 2014). We then synthesize the final SQL query S based on S 0 . 2 part in Figure 2 illustrates the generation The of aggregate functions for the column budget during the decoding process. 4.2 Schema Linking Extension To study the role of schema linking, we extend the encoder to explicitly capture the schema linking information. It w"
2020.emnlp-main.564,N18-2093,0,0.272729,"tion values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the SQL query. Existing solutions largely treat schema linking as a minor component implemented with simple string matching (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019) heuristics to support sophisticated textto-SQL models. An exception is Dong et al. (2019), which framed schema linking as a task to be solved by sequential tagging. While they did show the importance of schema linking, how it contribute to text-to-SQL task performance remains unanswered as there is no annotated corpus to analyze. To address these shortcomings, we perform an indepth study on the role of schema linking in text-toSQL parsing. Intuitively, schema linking helps both cross-domain generalizability and complex SQL generation, which have been identified as the curr"
2020.emnlp-main.564,D18-1193,0,0.254085,"tion values in natural language queries. For example, for the question “Find the names of schools that have a donation with amount * Equal contribution. Our code and annotation are available at https:// github.com/WING-NUS/slsql. 1 above 8.5” (shown with relevant tables in Figure 1), “name” is a column reference to school.name, “donation” a table reference to endowment, and “8.5” and “sale” are value references, corresponding to the condition values in the SQL query. Existing solutions largely treat schema linking as a minor component implemented with simple string matching (Guo et al., 2019; Yu et al., 2018a; Lin et al., 2019) heuristics to support sophisticated textto-SQL models. An exception is Dong et al. (2019), which framed schema linking as a task to be solved by sequential tagging. While they did show the importance of schema linking, how it contribute to text-to-SQL task performance remains unanswered as there is no annotated corpus to analyze. To address these shortcomings, we perform an indepth study on the role of schema linking in text-toSQL parsing. Intuitively, schema linking helps both cross-domain generalizability and complex SQL generation, which have been identified as the curr"
2020.emnlp-main.564,D18-1425,0,0.139882,"Missing"
2020.sdp-1.13,N19-1361,0,0.0313506,"Missing"
2020.sdp-1.13,N18-2097,0,0.0245529,"tc.) can be performed. SciWING incorporates ready-to-use web and terminal-based applications and demonstrations to aid adoption and development. The toolkit is available from http://sciwing.io and the demos are available at http://rebrand. ly/sciwing-demo1 . 1 Figure 1: SciWING Components: Text classification and Sequence labelling Datasets, Models composed from low-level Modules, and Engine to train and record experiment parameters. Infer middleware does the inference and high-level functionality (e.g., developing APIs; low-level and web applications). et al., 2013; Cohan and Goharian, 2015; Cohan et al., 2018; Cohan and Goharian, 2018). SDP tasks, in turn, help downstream systems and assist scholars in finding relevant documents and manage their knowledge discovery and utilization workflows. Next-generation introspective digital libraries such as Semantic Scholar (Ammar et al., 2018), Google Scholar and Microsoft Academic have begun to incorporate such services. While NLP, in general, has seen tremendous progress with the introduction of neural network architectures and general toolkits and datasets to leverage them, their deployment for SDP is still limited. Over the past few years, many opensour"
2020.sdp-1.13,C18-1139,0,0.0126732,"ering approach, leading to performance losses for many SDP tasks, and difficulties in retrofitting neural models into its framework. SciSpaCy (Neumann et al., 2019) focuses on biomedical related tasks such as POS-tagging, syntactic parsing and biomedical span extraction. However, SciSpaCy primarily caters for practitioners; it does not easily allow for the development and testing of new models and architectures. Task- and domain-agnostic frameworks also exist. NCRF++ (Yang and Zhang, 2018) is a tool for performing sequence tagging using Neural Networks and Conditional Random Fields and FLAIR (Akbik et al., 2018) is a framework for generalpurpose NLP and mainly provide access to different embeddings and ways to combine them. 4 5 # C o n c a t e n a t e t h e embeddings embedder = C o n c a t E m b e d d e r s ( [ word embedder , c h a r e m b e d d e r , elmo embedder ] ) # LSTM c h a r a c t e r embedder c h a r e m b e d d e r = CharEmbedder ( char embedding dimension = 10 , hidden dimension = 25 , ) 11 118 SciWING is actively being developed. We consider the following improvements in our roadmap: • SciWING has yet to incorporate natural language generation related models. We would like to consider"
2020.sdp-1.13,D15-1045,0,0.025277,"rs (i.e., BERT, SciBERT, etc.) can be performed. SciWING incorporates ready-to-use web and terminal-based applications and demonstrations to aid adoption and development. The toolkit is available from http://sciwing.io and the demos are available at http://rebrand. ly/sciwing-demo1 . 1 Figure 1: SciWING Components: Text classification and Sequence labelling Datasets, Models composed from low-level Modules, and Engine to train and record experiment parameters. Infer middleware does the inference and high-level functionality (e.g., developing APIs; low-level and web applications). et al., 2013; Cohan and Goharian, 2015; Cohan et al., 2018; Cohan and Goharian, 2018). SDP tasks, in turn, help downstream systems and assist scholars in finding relevant documents and manage their knowledge discovery and utilization workflows. Next-generation introspective digital libraries such as Semantic Scholar (Ammar et al., 2018), Google Scholar and Microsoft Academic have begun to incorporate such services. While NLP, in general, has seen tremendous progress with the introduction of neural network architectures and general toolkits and datasets to leverage them, their deployment for SDP is still limited. Over the past few"
2020.sdp-1.13,N18-3011,0,0.036618,"Missing"
2020.sdp-1.13,N19-1423,0,0.109512,"of SciWING with popular frameworks. Pre-trained models: availability of pretrained models. SOTA: state-of-the-art or comparably performing models for SDP. Neural-Networks first: supports endto-end neural network development and training. Extensible: easily incorporates new datasets and architectures. ing NLP frameworks. SciWING incorporates AllenNLP, the generic NLP pipeline (Gardner et al., 2018), developing models on top of it when necessary, while using the transformers package (Wolf et al., 2019) to enable transfer learning via its pretrained general-purpose representations such as BERT (Devlin et al., 2019) and SDP-specific ones like SciBERT (Beltagy et al., 2019). Fig. 1 shows SciWING’s Dataset, Model and Engine components facilitating flexible re-configuration. We now describe these components. Second, they do not provide deployable, SOTA models for SDP. Most provide limited or no means for researchers to train models on their datasets, or experiment with model architectures. A key barrier to entry is accessibility: a nontrivial level of expertise in NLP and machine learning is a prerequisite. Practitioners who wish to deploy SDP on their field’s literature may lack knowledge and motivation to"
2020.sdp-1.13,S17-2097,0,0.0273012,"tunately, due to the high training expense of their 10-fold cross validation, we are not able to obtain directly comparable results to their model’s performance. • ScienceIE identifies typed keyphrases, originally from chemical documents: Task keyphrases that denote the end task or goal, Material keyphrases indicate any chemical, and Dataset that is being used by the scientific work and the process includes any scientific model or algorithm. The state-of-the-art system from 2017 includes a word and character embeddings and a bidirectional LSTM with CRF and uses language model (LM) embeddings (Ammar et al., 2017). SciWING includes a reference implementation without using LM embeddings and the results are comparable. We use the same dataset used by (Luong et al., 2012) for training the neural networks. • Logical Structure Recovery identifies the logical sections of a document: introduction, related work, methodology, and experiments. This drives the relevant, targeted text to downstream tasks such as summarization, citation intent classification, among others. Currently, there are no neural network methods for this task, so SciWING’s models can serve as strong baselines. • Citation Intent Classificatio"
2020.sdp-1.13,W18-2501,0,0.167183,"oi.org/10.18653/v1/P17 Framework Pre-trained models SOTA Neural first Extensible Language/Framework SciSpaCy 4 AllenNLP 4 4 4 Py(Torch) FLAIR 4 4 4 Py(Torch) Grobid 4 SciWING 4 Py Java 4 4 4 Py(Torch) Table 1: Comparison of SciWING with popular frameworks. Pre-trained models: availability of pretrained models. SOTA: state-of-the-art or comparably performing models for SDP. Neural-Networks first: supports endto-end neural network development and training. Extensible: easily incorporates new datasets and architectures. ing NLP frameworks. SciWING incorporates AllenNLP, the generic NLP pipeline (Gardner et al., 2018), developing models on top of it when necessary, while using the transformers package (Wolf et al., 2019) to enable transfer learning via its pretrained general-purpose representations such as BERT (Devlin et al., 2019) and SDP-specific ones like SciBERT (Beltagy et al., 2019). Fig. 1 shows SciWING’s Dataset, Model and Engine components facilitating flexible re-configuration. We now describe these components. Second, they do not provide deployable, SOTA models for SDP. Most provide limited or no means for researchers to train models on their datasets, or experiment with model architectures. A"
2020.sdp-1.13,D19-1371,0,0.0653433,"availability of pretrained models. SOTA: state-of-the-art or comparably performing models for SDP. Neural-Networks first: supports endto-end neural network development and training. Extensible: easily incorporates new datasets and architectures. ing NLP frameworks. SciWING incorporates AllenNLP, the generic NLP pipeline (Gardner et al., 2018), developing models on top of it when necessary, while using the transformers package (Wolf et al., 2019) to enable transfer learning via its pretrained general-purpose representations such as BERT (Devlin et al., 2019) and SDP-specific ones like SciBERT (Beltagy et al., 2019). Fig. 1 shows SciWING’s Dataset, Model and Engine components facilitating flexible re-configuration. We now describe these components. Second, they do not provide deployable, SOTA models for SDP. Most provide limited or no means for researchers to train models on their datasets, or experiment with model architectures. A key barrier to entry is accessibility: a nontrivial level of expertise in NLP and machine learning is a prerequisite. Practitioners who wish to deploy SDP on their field’s literature may lack knowledge and motivation to learn it for the sake of deployment. Thus, there is a cle"
2020.sdp-1.13,N16-1030,0,0.131462,"Missing"
2020.sdp-1.13,P18-4013,0,0.0228245,"lassification, reference string parsing, among other tasks. But Grobid is architected in the traditional, manual feature engineering approach, leading to performance losses for many SDP tasks, and difficulties in retrofitting neural models into its framework. SciSpaCy (Neumann et al., 2019) focuses on biomedical related tasks such as POS-tagging, syntactic parsing and biomedical span extraction. However, SciSpaCy primarily caters for practitioners; it does not easily allow for the development and testing of new models and architectures. Task- and domain-agnostic frameworks also exist. NCRF++ (Yang and Zhang, 2018) is a tool for performing sequence tagging using Neural Networks and Conditional Random Fields and FLAIR (Akbik et al., 2018) is a framework for generalpurpose NLP and mainly provide access to different embeddings and ways to combine them. 4 5 # C o n c a t e n a t e t h e embeddings embedder = C o n c a t E m b e d d e r s ( [ word embedder , c h a r e m b e d d e r , elmo embedder ] ) # LSTM c h a r a c t e r embedder c h a r e m b e d d e r = CharEmbedder ( char embedding dimension = 10 , hidden dimension = 25 , ) 11 118 SciWING is actively being developed. We consider the following improve"
2020.sdp-1.13,W19-5034,0,0.0178567,"ls, datasets and experiment parameters in a single configuration file. ... w o r d e m b e d d e r = WordEmbedder ( embedding type = ” glove 6B 100 ” ) 5 6 Related Work Grobid (GRO, 2008–2020) is the closest to a general workbench for scientific document processing. Similarly to SciWING, Grobid also performs document structure classification, reference string parsing, among other tasks. But Grobid is architected in the traditional, manual feature engineering approach, leading to performance losses for many SDP tasks, and difficulties in retrofitting neural models into its framework. SciSpaCy (Neumann et al., 2019) focuses on biomedical related tasks such as POS-tagging, syntactic parsing and biomedical span extraction. However, SciSpaCy primarily caters for practitioners; it does not easily allow for the development and testing of new models and architectures. Task- and domain-agnostic frameworks also exist. NCRF++ (Yang and Zhang, 2018) is a tool for performing sequence tagging using Neural Networks and Conditional Random Fields and FLAIR (Akbik et al., 2018) is a framework for generalpurpose NLP and mainly provide access to different embeddings and ways to combine them. 4 5 # C o n c a t e n a t e t"
2020.sdp-1.13,W19-5035,0,0.012028,"al attention, as most are missing from pre-trained embeddings. SciWING includes task-specific trained embeddings for reference strings (Prasad et al., 2018). SciWING also supports contextual word embeddings: ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), SciBERT (Beltagy et al., 2019), etc. SOTA embedding models are built by concatenating multiple representations, via SciWING’s ConcatEmbedders module. As an example, word and character embeddings are combined in NER models (Lample et al., 2016), and multiple contextual word embeddings are combined in various clinical and BioNLP tasks (Zhai et al., 2019). Neural Network Encoders: SciWING ports commonly-used neural network components that can be composed to form neural architectures for different tasks. For example in text classification, encoding input sentence as a vector using an LSTM is a common task (SciWING’s seq2vecencoder). Another common operation is obtaining a sequence of hidden states for a set of tokens, often used in sequence labelling tasks and SciWING’s Lstm2seq achieves this. Further, it also includes attention based modules. SciWING builds in generic linear classification and sequence labelling with CRF heads that can be atta"
2020.sdp-1.13,D14-1162,0,0.083546,"e of Weights and Biases2 , with the integration of alternative logging services on the way. Metrics: Different SDP tasks require their respective metrics. SciWING abstracts a separate Metrics module to select appropriate metrics for each task. SciWING includes PrecisionRecallFMeasure suitable for text classification tasks, TokenClassificationAccuracy, and the official CONLL2003 shared task evaluation metric suitable for sequence labelling. ural language tokens as continuous vectors – embeddings. SciWING abstracts this concept via Embedders. Generic (non-SDP specific) embeddings such as GlovE (Pennington et al., 2014) are natively provided. Tokens in scientific documents can benefit from special attention, as most are missing from pre-trained embeddings. SciWING includes task-specific trained embeddings for reference strings (Prasad et al., 2018). SciWING also supports contextual word embeddings: ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), SciBERT (Beltagy et al., 2019), etc. SOTA embedding models are built by concatenating multiple representations, via SciWING’s ConcatEmbedders module. As an example, word and character embeddings are combined in NER models (Lample et al., 2016), and multiple c"
2020.sdp-1.13,N18-1202,0,0.0118869,"on tasks, TokenClassificationAccuracy, and the official CONLL2003 shared task evaluation metric suitable for sequence labelling. ural language tokens as continuous vectors – embeddings. SciWING abstracts this concept via Embedders. Generic (non-SDP specific) embeddings such as GlovE (Pennington et al., 2014) are natively provided. Tokens in scientific documents can benefit from special attention, as most are missing from pre-trained embeddings. SciWING includes task-specific trained embeddings for reference strings (Prasad et al., 2018). SciWING also supports contextual word embeddings: ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), SciBERT (Beltagy et al., 2019), etc. SOTA embedding models are built by concatenating multiple representations, via SciWING’s ConcatEmbedders module. As an example, word and character embeddings are combined in NER models (Lample et al., 2016), and multiple contextual word embeddings are combined in various clinical and BioNLP tasks (Zhai et al., 2019). Neural Network Encoders: SciWING ports commonly-used neural network components that can be composed to form neural architectures for different tasks. For example in text classification, encoding input sentence as a"
2020.sdp-1.13,C08-1087,0,0.0506298,"ocessing (NLP) on scholarly articles. As scholarly articles are long-form, complex documents with conventional structure and cross-reference to external resources, they require specialized treatment and have specialized tasks. Representative SDP tasks include parsing embedded reference strings (Prasad et al., 2018; Thai et al., 2020); identifying the importance, sentiment and provenance for citations (Cohan et al., 2019; Su et al., 2019); identifying logical sections and markup (Luong et al., 2012); parsing of equations, figures and tables (Clark and Divvala, 2016); and article summarization (Qazvinian and Radev, 2008; Qazvinian 1 Watch our demo video at https://rebrand.ly/ sciwing-video 113 Proceedings of the First Workshop on Scholarly Document Processing, pages 113–120 c Online, November 19, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Framework Pre-trained models SOTA Neural first Extensible Language/Framework SciSpaCy 4 AllenNLP 4 4 4 Py(Torch) FLAIR 4 4 4 Py(Torch) Grobid 4 SciWING 4 Py Java 4 4 4 Py(Torch) Table 1: Comparison of SciWING with popular frameworks. Pre-trained models: availability of pretrained models. SOTA: state-of-the-art or comparably performi"
2021.acl-long.321,D18-1316,0,0.0133931,"in a controlled manner via reliability tests can be a complementary method of evaluating the system’s out-of-distribution generalization ability. 4 Adversarial Attacks as Reliability Tests We first give a brief introduction to adversarial attacks in NLP before showing how they can be used for reliability testing. We refer the reader to Zhang et al. (2020b) for a comprehensive survey. Existing work on NLP adversarial attacks perturbs the input at various levels of linguistic analysis: phonology (Eger and Benz, 2020), orthography (Ebrahimi et al., 2018), morphology (Tan et al., 2020), lexicon (Alzantot et al., 2018; Jin et al., 2020), and syntax (Iyyer et al., 2018). Early work did not place any constraints on the attacks and merely used the degradation to a tar3 Dua et al. (2019) reports a cost of 60k USD for 96k question–answer pairs. Algorithm 1 General Reliability Test Require: Data distribution Dd = {X , Y} modeling the dimension of interest d, NLP system M, Source dataset X ∼ X , Desired labels Y 0 ∼ Y, Scoring function S. Ensure: Average- or worst-case examples X 0 , Result r. 1: X 0 ← {∅}, r ← 0 2: for x, y 0 in X, Y 0 do 3: C ← S AMPLE C ANDIDATES(X ) 4: switch TestType do 5: case AverageCaseTe"
2021.acl-long.321,2020.acl-main.485,0,0.0293702,"s. Expert. An actor who has specialized knowledge, such as ethicists, linguists, domain experts, social scientists, or NLP practitioners. 3 The Case for Reliability Testing in NLP The accelerating interest in building NLP-based products that impact many lives has led to urgent questions of fairness, safety, and accountability (Hovy and Spruit, 2016; Bender et al., 2021), 2 The “degree to which a system or component can function correctly in the presence of invalid inputs or stressful environmental conditions” (IEEE, 2017). 4154 prompting research into algorithmic bias (Bolukbasi et al., 2016; Blodgett et al., 2020), explainability (Ribeiro et al., 2016; Danilevsky et al., 2020), robustness (Jia and Liang, 2017), etc. Research is also emerging on best practices for productizing ML: from detailed dataset documentation (Bender and Friedman, 2018; Gebru et al., 2018), model documentation for highlighting important but often unreported details such as its training data, intended use, and caveats (Mitchell et al., 2019), and documentation best practices (Partnership on AI, 2019), to institutional mechanisms such as auditing (Raji et al., 2020) to enforce accountability and red-teaming (Brundage et al., 2020)"
2021.acl-long.321,2020.aacl-main.46,0,0.0290211,"icists, linguists, domain experts, social scientists, or NLP practitioners. 3 The Case for Reliability Testing in NLP The accelerating interest in building NLP-based products that impact many lives has led to urgent questions of fairness, safety, and accountability (Hovy and Spruit, 2016; Bender et al., 2021), 2 The “degree to which a system or component can function correctly in the presence of invalid inputs or stressful environmental conditions” (IEEE, 2017). 4154 prompting research into algorithmic bias (Bolukbasi et al., 2016; Blodgett et al., 2020), explainability (Ribeiro et al., 2016; Danilevsky et al., 2020), robustness (Jia and Liang, 2017), etc. Research is also emerging on best practices for productizing ML: from detailed dataset documentation (Bender and Friedman, 2018; Gebru et al., 2018), model documentation for highlighting important but often unreported details such as its training data, intended use, and caveats (Mitchell et al., 2019), and documentation best practices (Partnership on AI, 2019), to institutional mechanisms such as auditing (Raji et al., 2020) to enforce accountability and red-teaming (Brundage et al., 2020) to address developer blind spots, not to mention studies on the"
2021.acl-long.321,W19-3504,0,0.0608711,"Missing"
2021.acl-long.321,N19-1423,0,0.0535642,"ting distributions, may inadvertently result in systems that discriminate against minorities, who are often underrepresented in the training data. This can take ∗ Correspondence to: samson.tan@salesforce.com the form of misrepresentation of or poorer performance for people with disabilities, specific gender, ethnic, age, or linguistic groups (Hovy and Spruit, 2016; Crawford, 2017; Hutchinson et al., 2020). Amongst claims of NLP systems achieving human parity in challenging tasks such as question answering (Yu et al., 2018), machine translation (Hassan et al., 2018), and commonsense inference (Devlin et al., 2019), research has demonstrated these systems’ fragility to natural and adversarial noise (Goodfellow et al., 2015; Belinkov and Bisk, 2018) and out-of-distribution data (Fisch et al., 2019). It is also still common practice to equate “testing” with “measuring held-out accuracy”, even as datasets are revealed to be harmfully biased (Wagner et al., 2015; Geva et al., 2019; Sap et al., 2019). Many potential harms can be mitigated by detecting them early and preventing the offending model from being put into production. Hence, in addition to being mindful of the biases in the NLP pipeline (Bender and"
2021.acl-long.321,2020.coling-main.76,0,0.0429937,"rectness, we would expect the ATS system to ignore linguistic variation and sensitive attributes as long as they do not affect the answer’s validity. Hence, we would expect variation in these dimensions to have no effect on scores: answer length, language/vocabulary simplicity, alternative spellings/misspellings of non-keywords, grammatical variation, syntactic variation (especially those resembling transfer from a first language), and proxies for sensitive attributes. On the other hand, the system should be able to differentiate proper answers from those aimed at gaming the test (Chin, 2020; Ding et al., 2020). When grading students on language skills, however, we would expect ATS systems to be only sensitive to the relevant skill. For example, when assessing grammar use, we would expect the system to be sensitive to grammatical errors (from the perspective of the language variety the student is expected to use), but not to the other dimensions mentioned above (e.g., misspellings). Actors. Relevant experts include teachers of the subjects where the ATS systems will be deployed, linguists, and computer scientists. The stakeholders (students) may be represented by student unions (at the university le"
2021.acl-long.321,2020.emnlp-main.500,0,0.0981568,"d for r 13: r ← |X| 14: return X 0 , r get model’s accuracy as the measure of success. However, this often resulted in the semantics and expected prediction changing, leading to an overestimation of the attack’s success. Recent attacks aim to preserve the original input’s semantics. A popular approach has been to substitute words with their synonyms using word embeddings or a language model as a measure of semantic similarity (Alzantot et al., 2018; Ribeiro et al., 2018; Michel et al., 2019; Ren et al., 2019; Zhang et al., 2019a; Li et al., 2019; Jin et al., 2020; Garg and Ramakrishnan, 2020; Li et al., 2020a). Focusing on maximally degrading model accuracy overlooks the key feature of adversarial attacks: the ability to find the worst-case example for a model from an arbitrary distribution. Many recent attacks perturb the input across multiple dimensions at once, which may make the result unnatural. By constraining our sample perturbations to a distribution modeling a specific dimension of interest, the performance on the generated adversaries is a valid lower bound performance for that dimension. Said another way, adversarial attacks can be reframed as interpretable reliability tests if we cons"
2021.acl-long.321,D17-1153,0,0.0219433,"aluation metrics for natural language generation (Novikova et al., 2017; Reiter, 2018; Kryscinski et al., 2019): due to the tests’ synthetic nature they may not fully capture the nuances of reality. For example, if a test’s objective were to test an NLP system’s reliability when interacting with African American English (AAE) speakers, would it be possible to guarantee (in practice) that all generated examples fall within the distribution of AAE texts? Potential research directions would be to design adversary generation techniques that can offer such guarantees or incorporate human feedback (Nguyen et al., 2017; Kreutzer et al., 2018; Stiennon et al., 2020). 9 Conclusion Once language technologies leave the lab and start impacting real lives, concerns around safety, fairness, and accountability cease to be thought experiments. While it is clear that NLP can have a positive impact on our lives, from typing autocompletion to revitalizing endangered languages (Zhang et al., 2020a), it also has the potential to perpetuate harmful stereotypes (Bolukbasi et al., 2016; Sap et al., 2019), perform disproportionately poorly for underrepresented groups (Hern, 2017; Bridgeman et al., 2012), and even erase alrea"
2021.acl-long.321,2020.acl-main.441,0,0.24244,"et al., 2019; 4153 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4153–4169 August 1–6, 2021. ©2021 Association for Computational Linguistics Waseem et al., 2021) and holding creators accountable via audits (Raji et al., 2020; Brundage et al., 2020), we argue for the need to evaluate an NLP system’s reliability in diverse operating conditions. Initial research on evaluating out-of-distribution generalization involved manually-designed challenge sets (Jia and Liang, 2017; Nie et al., 2020; Gardner et al., 2020), counterfactuals (Kaushik et al., 2019; Khashabi et al., 2020; Wu et al., 2021), biased sampling (Søgaard et al., 2021) or toolkits for testing if a system has specific capabilities (Ribeiro et al., 2020) or robustness to distribution shifts (Goel et al., 2021). However, most of these approaches inevitably overestimate a given system’s worst-case performance since they do not mimic the NLP system’s adversarial distribution1 . A promising technique for evaluating worst-case performance is the adversarial attack. However, although some adversarial attacks explicitly focus"
2021.acl-long.321,D17-1238,0,0.1766,"of operationalizing potential dimensions: “What is the system’s performance when exposed to variation along dimension d?”. For example, rather than simply “gender”, a better-defined dimension would be “gender pronouns”. With this understanding, experts and policymakers can then create a set of reliability requirements, comprising the testing dimensions, performance metric(s), and passing thresholds. Next, we recommend using the same metrics for held-out, average-case, and worst-case performance for easy comparison. These often vary from task to task and are still a subject of active research (Novikova et al., 2017; Reiter, 2018; Kryscinski et al., 2019), hence the question of the right metric to use is beyond the scope of this paper. Finally, ethicists, in consultation with the other aforementioned experts and stakeholders, will determine acceptable thresholds for worst-case performance. The system under test must perform above said thresholds when exposed to variation along those dimensions in order to pass. For worst-case performance, we recommend reporting thresholds as relative differences (δ) between the average-case and worst-case performance. These questions may help in applying this step and de"
2021.acl-long.321,J18-3002,0,0.0876608,"ential dimensions: “What is the system’s performance when exposed to variation along dimension d?”. For example, rather than simply “gender”, a better-defined dimension would be “gender pronouns”. With this understanding, experts and policymakers can then create a set of reliability requirements, comprising the testing dimensions, performance metric(s), and passing thresholds. Next, we recommend using the same metrics for held-out, average-case, and worst-case performance for easy comparison. These often vary from task to task and are still a subject of active research (Novikova et al., 2017; Reiter, 2018; Kryscinski et al., 2019), hence the question of the right metric to use is beyond the scope of this paper. Finally, ethicists, in consultation with the other aforementioned experts and stakeholders, will determine acceptable thresholds for worst-case performance. The system under test must perform above said thresholds when exposed to variation along those dimensions in order to pass. For worst-case performance, we recommend reporting thresholds as relative differences (δ) between the average-case and worst-case performance. These questions may help in applying this step and deciding if spec"
2021.acl-long.321,P19-1103,0,0.0196223,"8: case WorstCaseTest 9: x0 , s ← arg minxc ∈C S(y 0 , M(xc )) 10: X 0 ← X 0 ∪ {x0 } 11: r ←r+s 12: end for r 13: r ← |X| 14: return X 0 , r get model’s accuracy as the measure of success. However, this often resulted in the semantics and expected prediction changing, leading to an overestimation of the attack’s success. Recent attacks aim to preserve the original input’s semantics. A popular approach has been to substitute words with their synonyms using word embeddings or a language model as a measure of semantic similarity (Alzantot et al., 2018; Ribeiro et al., 2018; Michel et al., 2019; Ren et al., 2019; Zhang et al., 2019a; Li et al., 2019; Jin et al., 2020; Garg and Ramakrishnan, 2020; Li et al., 2020a). Focusing on maximally degrading model accuracy overlooks the key feature of adversarial attacks: the ability to find the worst-case example for a model from an arbitrary distribution. Many recent attacks perturb the input across multiple dimensions at once, which may make the result unnatural. By constraining our sample perturbations to a distribution modeling a specific dimension of interest, the performance on the generated adversaries is a valid lower bound performance for that dimensio"
2021.acl-long.321,P18-1079,0,0.0243937,"6: s ← M EAN(S(y 0 , M(C))) 7: X0 ← X0 ∪ C 8: case WorstCaseTest 9: x0 , s ← arg minxc ∈C S(y 0 , M(xc )) 10: X 0 ← X 0 ∪ {x0 } 11: r ←r+s 12: end for r 13: r ← |X| 14: return X 0 , r get model’s accuracy as the measure of success. However, this often resulted in the semantics and expected prediction changing, leading to an overestimation of the attack’s success. Recent attacks aim to preserve the original input’s semantics. A popular approach has been to substitute words with their synonyms using word embeddings or a language model as a measure of semantic similarity (Alzantot et al., 2018; Ribeiro et al., 2018; Michel et al., 2019; Ren et al., 2019; Zhang et al., 2019a; Li et al., 2019; Jin et al., 2020; Garg and Ramakrishnan, 2020; Li et al., 2020a). Focusing on maximally degrading model accuracy overlooks the key feature of adversarial attacks: the ability to find the worst-case example for a model from an arbitrary distribution. Many recent attacks perturb the input across multiple dimensions at once, which may make the result unnatural. By constraining our sample perturbations to a distribution modeling a specific dimension of interest, the performance on the generated adversaries is a valid lo"
2021.acl-long.321,2020.acl-main.442,0,0.269267,"Association for Computational Linguistics Waseem et al., 2021) and holding creators accountable via audits (Raji et al., 2020; Brundage et al., 2020), we argue for the need to evaluate an NLP system’s reliability in diverse operating conditions. Initial research on evaluating out-of-distribution generalization involved manually-designed challenge sets (Jia and Liang, 2017; Nie et al., 2020; Gardner et al., 2020), counterfactuals (Kaushik et al., 2019; Khashabi et al., 2020; Wu et al., 2021), biased sampling (Søgaard et al., 2021) or toolkits for testing if a system has specific capabilities (Ribeiro et al., 2020) or robustness to distribution shifts (Goel et al., 2021). However, most of these approaches inevitably overestimate a given system’s worst-case performance since they do not mimic the NLP system’s adversarial distribution1 . A promising technique for evaluating worst-case performance is the adversarial attack. However, although some adversarial attacks explicitly focus on specific linguistic levels of analysis (Belinkov and Bisk, 2018; Iyyer et al., 2018; Tan et al., 2020; Eger and Benz, 2020), many often simply rely on word embeddings or language models for perturbation proposal (see §4). Wh"
2021.acl-long.321,P19-1163,0,0.094836,"son et al., 2020). Amongst claims of NLP systems achieving human parity in challenging tasks such as question answering (Yu et al., 2018), machine translation (Hassan et al., 2018), and commonsense inference (Devlin et al., 2019), research has demonstrated these systems’ fragility to natural and adversarial noise (Goodfellow et al., 2015; Belinkov and Bisk, 2018) and out-of-distribution data (Fisch et al., 2019). It is also still common practice to equate “testing” with “measuring held-out accuracy”, even as datasets are revealed to be harmfully biased (Wagner et al., 2015; Geva et al., 2019; Sap et al., 2019). Many potential harms can be mitigated by detecting them early and preventing the offending model from being put into production. Hence, in addition to being mindful of the biases in the NLP pipeline (Bender and Friedman, 2018; Mitchell et al., 2019; 4153 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4153–4169 August 1–6, 2021. ©2021 Association for Computational Linguistics Waseem et al., 2021) and holding creators accountable via audits (Raji et al., 2020; Brundage et"
2021.acl-long.321,2020.acl-main.468,0,0.031264,"ial examples and out-of-distribution generalization has found ML systems to be particularly vulnerable to slight perturbations in the input (Goodfellow et al., 2015) and natural distribution shifts (Fisch et al., 2019). While these perturbations are often chosen to maximize model failure, they highlight serious reliability issues for putting ML models into production since they show that these models could fail catastrophically in naturally noisy, diverse, real-world environments (Saria and Subbaswamy, 2019). Additionally, bias can seep into the system at multiple stages of the NLP lifecycle (Shah et al., 2020), resulting in discrimination against minority groups (O’Neil, 2016). The good news, however, is that rigorous testing can help to highlight potential issues before the systems are deployed. The need for rigorous testing in NLP is reflected in ACL 2020 giving the Best Paper Award to CheckList (Ribeiro et al., 2020), which applied the idea of behavior testing from software engineering to testing NLP systems. While invaluable as a first step towards the development of comprehensive testing methodology, the current implementation of CheckList may still overestimate the reliability of NLP systems"
2021.acl-long.321,2021.eacl-main.156,0,0.0394858,"nt Conference on Natural Language Processing, pages 4153–4169 August 1–6, 2021. ©2021 Association for Computational Linguistics Waseem et al., 2021) and holding creators accountable via audits (Raji et al., 2020; Brundage et al., 2020), we argue for the need to evaluate an NLP system’s reliability in diverse operating conditions. Initial research on evaluating out-of-distribution generalization involved manually-designed challenge sets (Jia and Liang, 2017; Nie et al., 2020; Gardner et al., 2020), counterfactuals (Kaushik et al., 2019; Khashabi et al., 2020; Wu et al., 2021), biased sampling (Søgaard et al., 2021) or toolkits for testing if a system has specific capabilities (Ribeiro et al., 2020) or robustness to distribution shifts (Goel et al., 2021). However, most of these approaches inevitably overestimate a given system’s worst-case performance since they do not mimic the NLP system’s adversarial distribution1 . A promising technique for evaluating worst-case performance is the adversarial attack. However, although some adversarial attacks explicitly focus on specific linguistic levels of analysis (Belinkov and Bisk, 2018; Iyyer et al., 2018; Tan et al., 2020; Eger and Benz, 2020), many often sim"
2021.acl-long.321,2021.naacl-main.282,1,0.78736,"Missing"
2021.acl-long.321,2020.acl-main.263,1,0.838983,"et al., 2021), biased sampling (Søgaard et al., 2021) or toolkits for testing if a system has specific capabilities (Ribeiro et al., 2020) or robustness to distribution shifts (Goel et al., 2021). However, most of these approaches inevitably overestimate a given system’s worst-case performance since they do not mimic the NLP system’s adversarial distribution1 . A promising technique for evaluating worst-case performance is the adversarial attack. However, although some adversarial attacks explicitly focus on specific linguistic levels of analysis (Belinkov and Bisk, 2018; Iyyer et al., 2018; Tan et al., 2020; Eger and Benz, 2020), many often simply rely on word embeddings or language models for perturbation proposal (see §4). While the latter may be useful to evaluate a system’s robustness to malicious actors, they are less useful for dimension-specific testing (e.g., reliability when encountering grammatical variation). This is because they often perturb the input across multiple dimensions at once, which may make the resulting adversaries unnatural. Hence, in this paper targeted at NLP researchers, practitioners, and policymakers, we make the case for reliability testing and reformulate adversa"
2021.acl-long.321,D19-1221,0,0.0261856,"Missing"
2021.acl-long.321,2020.acl-main.540,0,0.036226,"Missing"
2021.acl-long.321,P19-1559,0,0.0758545,"§5). Finally, we note that reliabil4155 ity testing and standards are established practices in engineering industries (e.g., aerospace (Nelson, 2003; Wilkinson et al., 2016)) and advocate for NL engineering to be at parity with these fields. 3.2 Evaluating worst-case performance in a label-scarce world A proposed approach for testing robustness to natural and adverse distribution shifts is to construct test sets using data from different domains or writing styles (Miller et al., 2020; Hendrycks et al., 2020), or to use a human vs. model method of constructing challenge sets (Nie et al., 2020; Zhang et al., 2019b). While they are the gold standard, such datasets are expensive to construct,3 making it infeasible to manually create worst-case test examples for each NLP system being evaluated. Consequently, these challenge sets necessarily overestimate each system’s worst-case performance when the inference distribution differs from the training one. Additionally, due to their crowdsourced nature, these challenge sets inevitably introduce distribution shifts across multiple dimensions at once, and even their own biases (Geva et al., 2019), unless explicitly controlled for. Building individual challenge"
2021.acl-long.321,2020.emnlp-main.43,0,0.0589549,"ension would be prohibitively expensive due to combinatorial explosion, even before having to account for concept drift (Widmer and Kubat, 1996). This coupling complicates efforts to design a nuanced and comprehensive testing regime. Hence, simulating variation in a controlled manner via reliability tests can be a complementary method of evaluating the system’s out-of-distribution generalization ability. 4 Adversarial Attacks as Reliability Tests We first give a brief introduction to adversarial attacks in NLP before showing how they can be used for reliability testing. We refer the reader to Zhang et al. (2020b) for a comprehensive survey. Existing work on NLP adversarial attacks perturbs the input at various levels of linguistic analysis: phonology (Eger and Benz, 2020), orthography (Ebrahimi et al., 2018), morphology (Tan et al., 2020), lexicon (Alzantot et al., 2018; Jin et al., 2020), and syntax (Iyyer et al., 2018). Early work did not place any constraints on the attacks and merely used the degradation to a tar3 Dua et al. (2019) reports a cost of 60k USD for 96k question–answer pairs. Algorithm 1 General Reliability Test Require: Data distribution Dd = {X , Y} modeling the dimension of intere"
2021.acl-short.61,P19-1620,0,0.0222415,"fact verification model. We propose a simple yet general framework Question Answering for Claim Generation (QACG) to generate three types of claims from any given evidence: 1) claims that are supported by the evidence, 2) claims that are refuted by the evidence, and 3) claims that the evidence does Not have Enough Information (NEI) to verify. To generate claims, we utilize Question Generation (QG) (Zhao et al., 2018; Liu et al., 2020a; Pan et al., 2020), which aims to automatically ask questions from textual inputs. QG has been shown to benefit various NLP tasks, such as enriching QA corpora (Alberti et al., 2019), checking factual consistency for summarization (Wang et al., 2020), and data augmentation for semantic parsing (Guo et al., 2018). To the best of our knowledge, we are the first to employ QG for fact verification. As illustrated in Figure 1, given a passage P as the evidence, we first employ a Question Generator to generate a question–answer pair (Q, A) for the evidence. We then convert (Q, A) into a claim C (QA-to-Claim) based on the following logical assumptions: a) if P can answer Q and A is the correct answer, then C is a supported claim; b) if P can answer Q but A is an incorrect answer"
2021.acl-short.61,2020.emnlp-main.256,0,0.0354021,"atasets. We evaluate fact verification on three different test sets based on FEVER: 1) FEVER-S/R: Since only the supported and refuted claims are labeled with gold evidence in FEVER, we take the claim–evidence pairs of these two classes from the FEVER test set for evaluation. 2) FEVER-Symmetric: this is a carefullydesigned unbiased test set designed by Schuster et al. (2019) to detect the robustness of the fact verification model. Note that only supported and refuted claims are present in this test set. 3) FEVER-S/R/N: The full FEVER test set are used for a three-class verification. We follow Atanasova et al. (2020) to use the system of Malon (2019) to retrieve evidence sentences for NEI claims. NEI claim generation. We need to generate a question q 0 which is relevant but cannot be answered by P. To this end, we link P back to its original Wikipedia article W and expand the evidence with additional contexts Pext , which are five randomly-retrieved sentences from W that are not present in P. In our example in Figure 1, one additional context retrieved is “By the time the riots ended, 63 people had been killed”. We then concatenate P and Pext as the expanded evidence, based on which we generate a supporte"
2021.acl-short.61,2021.ccl-1.108,0,0.0945055,"Missing"
2021.acl-short.61,N18-1074,0,0.0516085,"Missing"
2021.acl-short.61,2020.acl-main.450,0,0.0268532,"tion Answering for Claim Generation (QACG) to generate three types of claims from any given evidence: 1) claims that are supported by the evidence, 2) claims that are refuted by the evidence, and 3) claims that the evidence does Not have Enough Information (NEI) to verify. To generate claims, we utilize Question Generation (QG) (Zhao et al., 2018; Liu et al., 2020a; Pan et al., 2020), which aims to automatically ask questions from textual inputs. QG has been shown to benefit various NLP tasks, such as enriching QA corpora (Alberti et al., 2019), checking factual consistency for summarization (Wang et al., 2020), and data augmentation for semantic parsing (Guo et al., 2018). To the best of our knowledge, we are the first to employ QG for fact verification. As illustrated in Figure 1, given a passage P as the evidence, we first employ a Question Generator to generate a question–answer pair (Q, A) for the evidence. We then convert (Q, A) into a claim C (QA-to-Claim) based on the following logical assumptions: a) if P can answer Q and A is the correct answer, then C is a supported claim; b) if P can answer Q but A is an incorrect answer, then C is a refuted claim; c) if P cannot answer Q, then C is a NE"
2021.acl-short.61,2020.acl-main.549,0,0.0486278,"Missing"
2021.adaptnlp-1.23,2020.acl-main.692,0,0.094304,"tvya.malik, abdulwaheed1513}@gmail.com {rajivratn}@iiitd.ac.in Analysis (Maas et al., 2011), Question Answering (Zhang et al., 2018), among others. However, they do not investigate the domain invariance of PLM representations from different layers when presented with data from distinct domains. Studying the invariance of PLM representations has been useful in advancing methods for unsupervised domain adaptation. For example, in building domain adaptation models that explicitly reduce the divergence between layers of a neural network (Long et al., 2015; Shen et al., 2018a), for data selection (Aharoni and Goldberg, 2020; Ma et al., 2019) et cetera. Given the importance of PLMs, a glass-box study of the internal robustness of PLM representations is overdue. We thus study these representations, dissecting them layer by layer, to uncover their internal contributions in domain adaptation. Firstly, we use the tools of domain divergence and domain invariance, without subscribing to the performance of a model on any end task. The theory of domain adaptation (Ben-David et al., 2010), shows that reducing H-divergence between two domains results in higher performance in the target domain. Many works have since adopted"
2021.adaptnlp-1.23,W18-0701,0,0.0234151,"Missing"
2021.adaptnlp-1.23,D15-1075,0,0.0357068,"using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain. 1 Introduction Pretrained Language Models (PLMs) have improved the downstream performance of many natural language understanding tasks on standard data (Devlin et al., 2019).1 Recent works attest to the surprising out-of-the-box robustness of PLMs on out-of-distribution tasks (Hendrycks et al., 2020; Brown et al., 2020; Miller et al., 2020). These works measure robustness in terms of the performance invariance of PLMs on end tasks like Natural Language Inference (Bowman et al., 2015; Williams et al., 2018), Sentiment 1 We borrow the term standard data from (Plank, 2016) to refer to news and web-like text and non-standard data to refer to other text like biomedical and Twitter. 222 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 222–244 April 20, 2021. ©2021 Association for Computational Linguistics • Further, we analyze the robustness in terms of the syntactic and semantic information encoded in the representations across unseen domains and find that similar layers have similar amounts of linguistic information, which is a preliminary exposition of"
2021.adaptnlp-1.23,2020.findings-emnlp.222,0,0.042037,"Missing"
2021.adaptnlp-1.23,W19-1909,0,0.0541812,"Missing"
2021.adaptnlp-1.23,P18-1198,0,0.0575978,"Missing"
2021.adaptnlp-1.23,D11-1033,0,0.0575404,"e divergence between representations from different layers are reduced (Long et al., 2015). Recently, Aharoni and Goldberg (2020) show the final transformer layer representations cluster while Ma et al. (2019) consider penultimate layer representations. The high domain divergence of the upper layers is a plausible explanation for the clustering (Figs. 8 to 13 in Appendix E.). Clustering of representations plays a key role in downstream applications, such as data selection for machine translation and curriculum learning, data points in the source domain closest to the target domain are chosen (Axelrod et al., 2011; Moore and Lewis, 2010). BERT vs. RoBERTa: Compared to BERT, RoBERTa has uniform divergence across layers (c.f. Fig. 1 ). RoBERTa is similar to BERT, but a major difference is the amount of pre-training data used (one magnitude; 160GB vs. 16GB). We speculate that the domain-invariance is because the pretraining data is an unintended mixture of different domains. Recent works have shown the impact of training models with large and diverse datasets on the robustness of image classification models (Taori et al., 2020) and text classification models (Tu et al., 2020) with similar trends observed"
2021.adaptnlp-1.23,R13-1026,0,0.0116282,"framework (Wang et al., 2019) . Data: Following (Tenney et al., 2019b) we use the OntoNotes 5.0 corpus (Weischedel, Ralph et al., 2013) for probing. Since they are from newswire and web text, which is similar to the pretraining corpus of BERT (Devlin et al., 2019), we consider this dataset as standard data (source domain). We choose Twitter to represent non-standard data (target domain) for the probing task since our previous experiments showed a greater divergence, and thus are significantly different from the pretraining corpus used in BERT. For POS tagging, we use the dataset described by (Derczynski et al., 2013). We remove the following POS tags from the dataset: USR, URL, HT, RT, “(”, and “)”, to normalize the labels across the domains. For NER, we use the dataset released for the shared task of the Workshop on Noisy Usergenerated Text (W-NUT) (Baldwin et al., 2015). For coreference resolution, we use the dataset presented in (Aktas¸ et al., 2018), whose annotations were later modified by (Aktas¸ et al., 2020) so that they were conceptually parallel to OntoNotes 5.0 corpus (Weischedel, Ralph et al., 2013). The size of the datasets across train, development and test splits were kept similar for both"
2021.adaptnlp-1.23,W15-4319,0,0.0355881,"Missing"
2021.adaptnlp-1.23,N19-1423,0,0.518877,"main-specific data (DAPT) (Gururangan et al., 2020) exhibit more variance than their pretrained PLM counterparts; and that iii) Distilled models (e.g.,DistilBERT) also show greater domain variance. Second, we investigate the robustness of representations by analyzing the encoded syntactic and semantic information using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain. 1 Introduction Pretrained Language Models (PLMs) have improved the downstream performance of many natural language understanding tasks on standard data (Devlin et al., 2019).1 Recent works attest to the surprising out-of-the-box robustness of PLMs on out-of-distribution tasks (Hendrycks et al., 2020; Brown et al., 2020; Miller et al., 2020). These works measure robustness in terms of the performance invariance of PLMs on end tasks like Natural Language Inference (Bowman et al., 2015; Williams et al., 2018), Sentiment 1 We borrow the term standard data from (Plank, 2016) to refer to news and web-like text and non-standard data to refer to other text like biomedical and Twitter. 222 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 222–244 Apri"
2021.adaptnlp-1.23,2020.findings-emnlp.148,0,0.0131892,"esentations. Next, to analyze whether training with larger data scale aids in robustness, we consider RoBERTa (Liu et al., 2019b), which is similar to BERT, but trained on a magnitude larger standard data. Further, we check the effect of distillation on domaininvariance and hence, consider DistilBERT (Sanh et al., 2019). Finally, training of models on domain-specific data is known to increase their performance on domain-specific tasks. To analyze the effect of continued fine-tuning on invariance, we consider RoBERTa pretrained on non-standard Biomedical (Gururangan et al., 2020), and Twitter (Barbieri et al., 2020) domain data. We refer to this as DAPT-biomed and DAPT-tweet, respectively. For our experiments, we use the models hosted on the huggingface-transformer library (Wolf et al., 2020). Divergence Measures. We consider three different divergence measures that are widely used in the unsupervised domain adaptation literature. Correlation Alignment (CORAL) measures the difference between covariance of features – a second-order moment. Sun and Saenko (2016) reduce the distributional distance between features for unsupervised domain adaptation (UDA) in computer vision models. In contrast to CORAL, Cent"
2021.adaptnlp-1.23,D19-1371,0,0.0149487,"is non-trivial (Gretton et al., 2012b). We confirm this by plotting the PCA representations of these data points (Figs. 8 to 13 in Appendix E.), which show that the representations from the two domains are interspersed in the lower layers and separated in the upper layers, as done in many previous works (Ganin et al., 2016; Long et al., 2015). We further quantify this by performing k-means clustering where 3.3 What happens to the domain-invariance of DAPT models? To create domain-specific PLM, the simplest methods train models from scratch on domainspecific data like scientific publications (Beltagy et al., 2019), BioBERT (Lee et al., 2019), ClinicalBERT (Alsentzer et al., 2019) among others. In 225 (a) CORAL (b) CMD Figure 2: Comparing CORAL and CMD divergences for roberta-base and DAPT-biomed (Gururangan et al., 2020). Considers standard and biomedical samples. main). DistilBERT contains half the number of layers compared to BERT. At a comparable layers, DistilBERT always has higher divergence values for both CMD and CORAL. Sanh et al. (2019) show that distillation loss that mimics the teacher’s output and cosine embedding loss which aligns the student and teacher hidden states vectors, are the majo"
2021.adaptnlp-1.23,P18-1031,0,0.0281242,"8 0.092 0.695 9 0.164 0.753 10 0.312 0.772 11 0.63 0.762 12 0.588 0.68 Table 1: NMI values measuring the clustering performance at different layers of bert-base-uncased. k = 2 (the number of domains). We evaluate the clusters using Normalized Mutual Information (c.f. Table 1). Clustering quality is higher for upper layers compared to lower layers where representations are interspersed. The increasing divergence across layers has plausible implications in making decisions in many scenarios. For example, in deciding the number of layers in the gradual unfreezing of layers in transfer learning (Howard and Ruder, 2018), in unsupervised domain adaptation where divergence between representations from different layers are reduced (Long et al., 2015). Recently, Aharoni and Goldberg (2020) show the final transformer layer representations cluster while Ma et al. (2019) consider penultimate layer representations. The high domain divergence of the upper layers is a plausible explanation for the clustering (Figs. 8 to 13 in Appendix E.). Clustering of representations plays a key role in downstream applications, such as data selection for machine translation and curriculum learning, data points in the source domain c"
2021.adaptnlp-1.23,P19-1356,0,0.0793364,"also consider only one target domain — Twitter — and analyze bert-base-uncased for our probes. The availability and varying characteristics of the dataset across domains dictates our choice. For example, compared to standard coreference, biomedical text exhibits co-referring terms across sentences in long documents. Concerning which part of the model encodes syntactic information required for POS and NER, we observe that the middle layers perform the best for both tasks, invariant of domain. This result is consistent with the results reported for nondomain adaptation work (Liu et al., 2019a; Jawahar et al., 2019). For Coref, the upper layers perform better on the task for the source domain. This indicates that the models store the information required for Coref (Liu et al., 2019a), but that the lower layers perform better when it comes to the target domain. We speculate that this is due to the nature of Twitter-coref dataset (target domain). For tasks like coreference resolution, there is a need for the presence of semantic information to identify the co-referring entities. But as tweets are naturally shorter, they contain co-referring entities that are close to each other, and do not require long-ran"
2021.adaptnlp-1.23,2020.acl-main.740,0,0.0296054,"Missing"
2021.adaptnlp-1.23,2020.findings-emnlp.372,0,0.0369331,"d (Gururangan et al., 2020). Considers standard and biomedical samples. main). DistilBERT contains half the number of layers compared to BERT. At a comparable layers, DistilBERT always has higher divergence values for both CMD and CORAL. Sanh et al. (2019) show that distillation loss that mimics the teacher’s output and cosine embedding loss which aligns the student and teacher hidden states vectors, are the major contributors to the student’s performance. Yet, we find that DistilBERT still has greater variance which may affect downstream tasks like text classification. Although a few models (Jiao et al., 2020; Sanh et al., 2019) reduce some notion of geometric distance between the intermediate representations of the student and the teacher, it does not guarantee that the entire linguistic knowledge and the domain-invariance of the teacher are transferred to the student model. Recent work in NLP have tried to incorporate rich information from teacher networks using contrastive learning (Tian et al., 2020; Sun et al., 2020) and by reducing the Earth Mover’s distance between the hidden representations in the transformer architecture (Li et al., 2020). Related computer vision work also to impart adver"
2021.adaptnlp-1.23,2020.tacl-1.40,0,0.157226,"target domain are chosen (Axelrod et al., 2011; Moore and Lewis, 2010). BERT vs. RoBERTa: Compared to BERT, RoBERTa has uniform divergence across layers (c.f. Fig. 1 ). RoBERTa is similar to BERT, but a major difference is the amount of pre-training data used (one magnitude; 160GB vs. 16GB). We speculate that the domain-invariance is because the pretraining data is an unintended mixture of different domains. Recent works have shown the impact of training models with large and diverse datasets on the robustness of image classification models (Taori et al., 2020) and text classification models (Tu et al., 2020) with similar trends observed where RoBERTa is more robust. We make forward passes of 1000 samples from one pair of domains (standard–biomedical / standard–twitter) separately through the transformers, obtaining two sets of representations. We then use these to calculate divergence measures. We consider the representations of [CLS] token as the representation of a sentence, as done in other works. Note that we do not fine-tune any of our models on the non-standard data. 3.2 Results Across Layers: Overall, the divergence measures increase from the lower layers to the upper layers (Figure 1). CO"
2021.adaptnlp-1.23,2020.emnlp-main.36,0,0.0353442,"tributors to the student’s performance. Yet, we find that DistilBERT still has greater variance which may affect downstream tasks like text classification. Although a few models (Jiao et al., 2020; Sanh et al., 2019) reduce some notion of geometric distance between the intermediate representations of the student and the teacher, it does not guarantee that the entire linguistic knowledge and the domain-invariance of the teacher are transferred to the student model. Recent work in NLP have tried to incorporate rich information from teacher networks using contrastive learning (Tian et al., 2020; Sun et al., 2020) and by reducing the Earth Mover’s distance between the hidden representations in the transformer architecture (Li et al., 2020). Related computer vision work also to impart adversarial robustness, even in the student network (Goldblum et al., 2020). The benefits of such enhanced distillation techniques on the robustness of the model is an under-explored area. contrast, instead of pretraining from scratch, recent work shows impressive benefits of continuing to pretrain on domain-specific data — termed domain adaptive pretraining (DAPT) (Gururangan et al., 2020). Although there are improvements"
2021.adaptnlp-1.23,2020.findings-emnlp.125,0,0.0226931,"on PLM robustness use the notion of performance drop in a new target domain (Hendrycks et al., 2020; Tu et al., 2020; Miller et al., 2020). However, analyzing the robustness and invariance of the representations under data from different domains or adversarial examples (Zhu et al., 2020) has not received much attention thus far in domain adaptation. Concerning the robustness of linguistic information stored in representations, Merchant et al. (2020) analyze the syntactic and semantic information preserved by PLMs, both before and after fine-tuning the models on task-specific data. Similarly, Tamkin et al. (2020) analyze the role of different layers in transfer learning on end tasks. Different from their study, we are interested in the intrinsic invariance of the PLM representations under data from different domains. 6.2 Unsupervised Domain Adaptation For unsupervised domain adaptation, a popular method is use the adversarial training between a domain and a task classifier (DANN) (Ganin et al., 2016). Compared to DANN, where domainspecific peculiarities are lost, (Bousmalis et al., 2016) introduce domain-specific networks, which where domain-specific and domain-invariant representations are formed in"
2021.adaptnlp-1.23,N18-1101,0,0.078309,"Missing"
2021.adaptnlp-1.23,P19-1452,0,0.321109,"rained language models still encode for data from a different domain? Here, we evaluate the robustness of representations in bert-base-uncased. Do word-level repre226 (a) CORAL (b) CMD Figure 3: CORAL and CMD divergence measures for standard vs biomedical samples. Two encoders are considered here: bert-base-uncased and distilbert-base. where Coreference resolution (Coref) is considered a semantic task. We chose these tasks guided by the availability of similar datasets in both domains. For all our experiments involving probing, we use the jiant framework (Wang et al., 2019) . Data: Following (Tenney et al., 2019b) we use the OntoNotes 5.0 corpus (Weischedel, Ralph et al., 2013) for probing. Since they are from newswire and web text, which is similar to the pretraining corpus of BERT (Devlin et al., 2019), we consider this dataset as standard data (source domain). We choose Twitter to represent non-standard data (target domain) for the probing task since our previous experiments showed a greater divergence, and thus are significantly different from the pretraining corpus used in BERT. For POS tagging, we use the dataset described by (Derczynski et al., 2013). We remove the following POS tags from the"
2021.adaptnlp-1.23,2020.emnlp-main.239,0,0.0302919,"PubMed abstracts2 and for the Twitter domain, we sample 5000 tweets from the year 2011 made available by the archive team.3 We follow the same procedure as Nguyen et al. (2020) to preprocess tweets: we use fastText (Joulin et al., 2017) to consider only English tweets and use the emoji package 4 to translate emojis into text strings, normalize all the user mentions to @USER and URLs to HTTPURL. How Domain-Invariant are PLM Representations? Most of the current techniques in unsupervised domain adaptation explicitly reduce the divergence between different layer representations during training (Yu et al., 2020). A common posthoc analysis from such works shows the reduction of domain invariance at different layers. However, they do not pay much heed to the domaininvariance of representations that already exist in such models prior to domain-adapted training. 2 https://www.nlm.nih.gov/databases/download/ pubmed medline.html 3 https://archive.org/details/twitterstream 4 https://pypi.org/project/emoji 224 Layer NMI (standard-biomed) NMI (standard-twitter) 1 0.004 0.256 2 0.365 0.252 3 0.046 0.215 4 0.63 0.233 5 0.722 0.698 6 0.596 0.699 7 0.245 0.727 8 0.092 0.695 9 0.164 0.753 10 0.312 0.772 11 0.63 0."
2021.naacl-main.147,2020.tacl-1.33,0,0.0177573,"n find PAD performs better than MMD. Here, we target domain. We observe that information theo- limit our review to works utilising divergence mearetic measures and geometric measures based on sures. We exclude feature-based UDA methods frequency distributions and continuous representa- such as Structural Corresponding Learning (SCL) tions are common for text and structured prediction (Blitzer et al., 2006), Autoencoder-SCL and pivot tasks (cf. Table 1 in the appendix). The effective- based language models (Ziser and Reichart, 2017, ness of higher order measures for these tasks are 2018, 2019; Ben-David et al., 2020). yet to be ascertained. Obtaining domain invariant representations is deFurther, we find that for SMT data selection, sirable for many different NLP tasks, especially for variants of Cross Entropy (CE) measures are used sequence labelling where annotating large amounts extensively. However, the conclusions of van der of data is hard. They are typically used when there Wees et al. (2017) are more measured regarding is a single source domain and a single target do1834 main — for sentiment analysis (Ganin et al., 2016), NER (Zhou et al., 2019), stance detection (Xu et al., 2019), machine transla"
2021.naacl-main.147,D08-1078,0,0.0427849,"ce of a 1835 machine learning model in new domains. Information theoretic measures like Renyi-Div and KL-Div has been used for predicting performance drops in POS (Van Asch and Daelemans, 2010) and CrossEntropy based measure has been used for dependency parsing (Ravi et al., 2008). Prediction of performance can also be useful for machine translation where obtaining parallel data is hard. Based on distance between languages, (Xia et al., 2020) predict performance of the model on new languages for MT, among other tasks. Such performance prediction models have also been done in the past for SMT (Birch et al., 2008; Specia et al., 2013). However, Ponomareva and Thelwall (2012) argue that predicting drops in performance is more appropriate compared to raw performance. They find that JS-Div effective for predicting performance drop of Sentiment Analysis systems. Only recently, predicting model failures in practical deployments from an empirical viewpoint has regained attention. Elsahar and Gallé (2019) find the efficacy of higher-order measures to predict the drop in performance for POS and SA and do not rely on hand crafted measures as in previous works. However, analysing performance drops using CWR is"
2021.naacl-main.147,W06-1615,0,0.305684,"r, this is not captured in a shared–private network. DSN is flexalways possible, especially in unsupervised domain ible in its choice of divergence measures and they adaptation where there is no supervised data in find PAD performs better than MMD. Here, we target domain. We observe that information theo- limit our review to works utilising divergence mearetic measures and geometric measures based on sures. We exclude feature-based UDA methods frequency distributions and continuous representa- such as Structural Corresponding Learning (SCL) tions are common for text and structured prediction (Blitzer et al., 2006), Autoencoder-SCL and pivot tasks (cf. Table 1 in the appendix). The effective- based language models (Ziser and Reichart, 2017, ness of higher order measures for these tasks are 2018, 2019; Ben-David et al., 2020). yet to be ascertained. Obtaining domain invariant representations is deFurther, we find that for SMT data selection, sirable for many different NLP tasks, especially for variants of Cross Entropy (CE) measures are used sequence labelling where annotating large amounts extensively. However, the conclusions of van der of data is hard. They are typically used when there Wees et al. (2"
2021.naacl-main.147,C18-1111,0,0.0187575,"llenges researchers in choosing an appropriate sures to predict drop in performance – an immeasure for a given application. portant aspect of Decisions in the Wild, we To help guide best practices, we first comprehenperform correlation analysis spanning 130 dosively review the NLP literature on domain divermain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures identified from gences. Unlike previous surveys, which focus on our survey. To calculate these divergences, domain adaptation for specific tasks such as mawe consider the current contextual word reprechine translation (Chu and Wang, 2018) and statistisentations (CWR) and contrast with the older cal (non-neural network) models (Jiang, 2007; Mardistributed representations. We find that tragolis, 2011), our work takes a different perspective. ditional measures over word distributions still We study domain adaptation through the vehicle serve as strong baselines, while higher-order of domain divergence measures. First, we develop measures with CWR are effective. a taxonomy of divergence measures consisting of 1 Introduction three groups: Information-Theoretic, Geometric, Standard machine learning models do not perform and Higher-O"
2021.naacl-main.147,P13-2119,0,0.198941,"o other tasks. This can be attributed context of use. In contrast, contextual word repreto its popularity in real-world applications and the sentations (CWR) — mostly derived from neural difficulty of obtaining parallel sentences for every networks (Devlin et al., 2019; Peters et al., 2018) 1832 the prevalence of the divergence measures in NLP. Paper Task(s) Information-Theoretic KL (Plank and van Noord, 2011) (Dai et al., 2019) (Ruder and Plank, 2017) (Ruder et al., 2017) (Remus, 2012) (Lü et al., 2007) (Zhao et al., 2004) (Yasuda et al., 2008) (Moore and Lewis, 2010) (Axelrod et al., 2011) (Duh et al., 2013) (Liu et al., 2014) (van der Wees et al., 2017) (Silva et al., 2018) (Aharoni and Goldberg, 2020) (Wang et al., 2017) (Carpuat et al., 2017) (Vyas et al., 2018) (Chen and Huang, 2016) (Chen et al., 2017) Par, POS NER SA, NER, Par SA SA SMT SMT SMT SMT SMT SMT SMT NMT NMT NMT 4 JS Renyi CE Wass. DATA S ELECTION 4 4 4 Geometric Cos Higher-Order CMD MMD Others - P-Norm PAD 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 NMT NMT NMT SMT NMT 4 4 4 4 LEARNING REPRESENTATIONS (Ganin et al., 2015) (Kim et al., 2017) (Liu et al., 2017) (Li et al., 2018) (Chen and Cardie, 2018) (Zellinger et al., 2017) (P"
2021.naacl-main.147,2020.acl-main.292,1,0.830945,"017; Zeng et al., 2018). The application of DANN and DSN to a variety of tasks are testament of their generality. DANN and DSN are applied in other innovative situations. Text from two different periods of time can be considered as two different domains for intent classification (Kim et al., 2017). Gui et al. (2017) consider clean formal newswire data as source domain and noisy, colloquial, unlabeled Twitter data as the target domain and use adversarial learning to learn robust representations for POS. Commonsense knowledge graphs can help in learning domain-invariant representations as well. Ghosal et al. (2020) condition DANN with an external commonsense knowledge graph using graph convolutional neural networks for sentiment analysis. In contrast, Wang et al. (2018) use MMD outside the adversarial learning framework. They use MMD to learn to reduce the discrepancy between neural network representations belonging to two domains. Such concepts have been explored in computer vision (Tzeng et al., 2014). While single source and target domains are common, complementary information available in multiple domains can help to improve performance in a target domain. This is especially helpful when there is no"
2021.naacl-main.147,D17-1256,0,0.028781,"f data is hard. They are typically used when there Wees et al. (2017) are more measured regarding is a single source domain and a single target do1834 main — for sentiment analysis (Ganin et al., 2016), NER (Zhou et al., 2019), stance detection (Xu et al., 2019), machine translation (Britz et al., 2017; Zeng et al., 2018). The application of DANN and DSN to a variety of tasks are testament of their generality. DANN and DSN are applied in other innovative situations. Text from two different periods of time can be considered as two different domains for intent classification (Kim et al., 2017). Gui et al. (2017) consider clean formal newswire data as source domain and noisy, colloquial, unlabeled Twitter data as the target domain and use adversarial learning to learn robust representations for POS. Commonsense knowledge graphs can help in learning domain-invariant representations as well. Ghosal et al. (2020) condition DANN with an external commonsense knowledge graph using graph convolutional neural networks for sentiment analysis. In contrast, Wang et al. (2018) use MMD outside the adversarial learning framework. They use MMD to learn to reduce the discrepancy between neural network representations"
2021.naacl-main.147,D18-1498,0,0.0538334,"Missing"
2021.naacl-main.147,2020.acl-main.740,0,0.0714902,"Missing"
2021.naacl-main.147,D19-1433,0,0.0204053,"18), and Wasserstein distance has been used for duplicate question detection (Shah et al., 2018) and to learn domain-invariant attention distributions for emotional regression (Zhu et al., 2019). The review shows that most works extend the DSN framework to learn domain invariant representations in different scenarios (cf. Table 1, in the appendix). The original work from (Bousmalis et al., 2016) includes MMD divergence besides PAD, which is not adopted in subsequent works, possibly due to the reported poor performance. Most works require careful balancing between multiple objective functions (Han and Eisenstein, 2019), which can affect the stability of training. The stability of training can be improved by selecting appropriate divergence measures like CMD (Zellinger et al., 2017) and Wasserstein Distance (Arjovsky et al., 2017). We believe additional future works will adopt such measures. 3.3 Decisions in the Wild Models can perform poorly when they are deployed in the real world. The performance degrades due to the difference in distribution between training and test data. Such performance degradation can be alleviated by large-scale annotation in the new DANN and DSN can also help in multitask domain. H"
2021.naacl-main.147,N06-2015,0,0.0272913,"nce drops using CWR is still lacking. We tackle this in the next section. 4 Experiments the target domain, to best mimic realistic settings. 4.1 Experimental Setup Datasets: For POS, we select 5 different corpora from the English Word Tree Bank of Universal Dependency corpus (Nivre et al., 2016)1 and also include the GUM, Lines, and ParTUT datasets. We follow Elsahar and Gallé (2019) and consider these as 8 domains. For NER, we consider CONLL 2003 (Tjong Kim Sang and De Meulder, 2003), Emerging and Rare Entity Recognition Twitter (Derczynski et al., 2017) and all 6 categories in OntoNotes v5 (Hovy et al., 2006)2 , resulting in 8 domains. For SA, we follow Guo et al. (2020), selecting the same 5 categories3 for experiments (Liu et al., 2017). Divergence Measures: We consider 12 divergences. For Cos, we follow the instance based calculation (Ruder et al., 2017). For MMD, Wasserstein and CORAL, we randomly sample 1000 sentences and average the results over 3 runs. For MMD, we experiment with different kernels (cf. Appendix A) and use default values of σ from the GeomLoss package (Feydy et al., 2019). For TVO, KL-div, JS-div, Renyi-div, based on word frequency distribution we remove stop-words and consi"
2021.naacl-main.147,Q13-1035,0,0.056707,"Missing"
2021.naacl-main.147,P17-1119,0,0.151723,"Zhao et al., 2004) (Yasuda et al., 2008) (Moore and Lewis, 2010) (Axelrod et al., 2011) (Duh et al., 2013) (Liu et al., 2014) (van der Wees et al., 2017) (Silva et al., 2018) (Aharoni and Goldberg, 2020) (Wang et al., 2017) (Carpuat et al., 2017) (Vyas et al., 2018) (Chen and Huang, 2016) (Chen et al., 2017) Par, POS NER SA, NER, Par SA SA SMT SMT SMT SMT SMT SMT SMT NMT NMT NMT 4 JS Renyi CE Wass. DATA S ELECTION 4 4 4 Geometric Cos Higher-Order CMD MMD Others - P-Norm PAD 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 NMT NMT NMT SMT NMT 4 4 4 4 LEARNING REPRESENTATIONS (Ganin et al., 2015) (Kim et al., 2017) (Liu et al., 2017) (Li et al., 2018) (Chen and Cardie, 2018) (Zellinger et al., 2017) (Peng et al., 2018) (Wu and Guo, 2020) (Ding et al., 2019) (Shah et al., 2018) (Zhu et al., 2019) (Gui et al., 2017) (Zhou et al., 2019) (Cao et al., 2018) (Wang et al., 2018) (Gu et al., 2019) (Britz et al., 2017) (Zeng et al., 2018) (Wang et al., 2019) 4 4 4 4 SA Intent-clf SA Lang-ID SA SA SA SA Intent-Clf Question sim EmoRegress POS NER NER NER NMT NMT NMT NMT 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 DECISIONS IN THE WILD (Ravi et al., 2008) (Elsahar and Gallé, 2019) (Ponomareva and Thelwall, 2012) (Van Asch and"
2021.naacl-main.147,N18-2076,0,0.064207,"Missing"
2021.naacl-main.147,P14-2093,0,0.0605956,"Missing"
2021.naacl-main.147,P17-1001,0,0.226877,"(Yasuda et al., 2008) (Moore and Lewis, 2010) (Axelrod et al., 2011) (Duh et al., 2013) (Liu et al., 2014) (van der Wees et al., 2017) (Silva et al., 2018) (Aharoni and Goldberg, 2020) (Wang et al., 2017) (Carpuat et al., 2017) (Vyas et al., 2018) (Chen and Huang, 2016) (Chen et al., 2017) Par, POS NER SA, NER, Par SA SA SMT SMT SMT SMT SMT SMT SMT NMT NMT NMT 4 JS Renyi CE Wass. DATA S ELECTION 4 4 4 Geometric Cos Higher-Order CMD MMD Others - P-Norm PAD 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 NMT NMT NMT SMT NMT 4 4 4 4 LEARNING REPRESENTATIONS (Ganin et al., 2015) (Kim et al., 2017) (Liu et al., 2017) (Li et al., 2018) (Chen and Cardie, 2018) (Zellinger et al., 2017) (Peng et al., 2018) (Wu and Guo, 2020) (Ding et al., 2019) (Shah et al., 2018) (Zhu et al., 2019) (Gui et al., 2017) (Zhou et al., 2019) (Cao et al., 2018) (Wang et al., 2018) (Gu et al., 2019) (Britz et al., 2017) (Zeng et al., 2018) (Wang et al., 2019) 4 4 4 4 SA Intent-clf SA Lang-ID SA SA SA SA Intent-Clf Question sim EmoRegress POS NER NER NER NMT NMT NMT NMT 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 DECISIONS IN THE WILD (Ravi et al., 2008) (Elsahar and Gallé, 2019) (Ponomareva and Thelwall, 2012) (Van Asch and Daelemans, 2010) 4"
2021.naacl-main.147,D14-1162,0,0.0870654,"Missing"
2021.naacl-main.147,N18-1202,0,0.0907908,"imilar in serves as supervised data to train models in the tar- tenor (the participants in a discourse, the relationget domain. We note that the literature pays closer ships between them, etc.) to the source data. But attention to data selection for machine translation static embeddings do not change according to the compared to other tasks. This can be attributed context of use. In contrast, contextual word repreto its popularity in real-world applications and the sentations (CWR) — mostly derived from neural difficulty of obtaining parallel sentences for every networks (Devlin et al., 2019; Peters et al., 2018) 1832 the prevalence of the divergence measures in NLP. Paper Task(s) Information-Theoretic KL (Plank and van Noord, 2011) (Dai et al., 2019) (Ruder and Plank, 2017) (Ruder et al., 2017) (Remus, 2012) (Lü et al., 2007) (Zhao et al., 2004) (Yasuda et al., 2008) (Moore and Lewis, 2010) (Axelrod et al., 2011) (Duh et al., 2013) (Liu et al., 2014) (van der Wees et al., 2017) (Silva et al., 2018) (Aharoni and Goldberg, 2020) (Wang et al., 2017) (Carpuat et al., 2017) (Vyas et al., 2018) (Chen and Huang, 2016) (Chen et al., 2017) Par, POS NER SA, NER, Par SA SA SMT SMT SMT SMT SMT SMT SMT NMT NMT NM"
2021.naacl-main.147,D14-1104,0,0.0692119,"Missing"
2021.naacl-main.147,P11-1157,0,0.615774,"Missing"
2021.naacl-main.147,2020.coling-main.603,0,0.551483,"it the scope our survey to works that focus on divergence measures. We only consider unsu- quently used for NER. Dai et al. (2019) use Term Vocabulary Overlap for selecting data for pretrainpervised domain adaptation (UDA) – where there is no annotated data available in the target domain. ing language models. Geometric and Informationtheoretic measures with word level distributions It is more practical yet more challenging. For a are inexpensive to calculate. However, the distribucomplete treatment of neural networks and UDA in tions are sparse and continuous word distributions NLP, refer to (Ramponi and Plank, 2020). Also, we help in learning denser representations. do not treat multilingual work. While cross-lingual transfer can be regarded as an extreme form of doContinuous or distributed representations of main adaptation, measuring the distance between words, such as CBOW, Skip-gram (Mikolov et al., languages requires different divergence measures, 2013) and GloVe (Pennington et al., 2014), adoutside our purview. dress shortcomings of representing text as sparse, frequency-based probability distributions by trans3.1 Data Selection forming them into dense vectors learned from freeDivergence measures a"
2021.naacl-main.147,D08-1093,0,0.137799,"Missing"
2021.naacl-main.147,D17-1038,0,0.0173165,"14). classifiers require supervised target domain data The representation learner is not only trained to which is not always available. As an alternative, minimise a task loss on source domain, but also Chen et al. (2017) train a classifier and selector in maximise a discriminator’s loss, by reversing the an alternating optimisation manner. gradients calculated for the discriminator. Note that From this literature review, we find that dis- this does not require any supervised data for target tinct measures are effective for different NLP tasks. domain. In later work, Bousmalis et al. (2016) arRuder and Plank (2017) argue that owing to their gue that domain-specific peculiarities are lost in a varying task characteristics, different measures DANN, and propose Domain Separation Networks should apply. They show that learning a linear (DSN) to address this shortcoming. In DSN, both combination of measures is useful for NER, pars- domain-specific and -invariant representations are ing and sentiment analysis. However, this is not captured in a shared–private network. DSN is flexalways possible, especially in unsupervised domain ible in its choice of divergence measures and they adaptation where there is no su"
2021.naacl-main.147,D18-1131,0,0.405811,"al., 2018) (Aharoni and Goldberg, 2020) (Wang et al., 2017) (Carpuat et al., 2017) (Vyas et al., 2018) (Chen and Huang, 2016) (Chen et al., 2017) Par, POS NER SA, NER, Par SA SA SMT SMT SMT SMT SMT SMT SMT NMT NMT NMT 4 JS Renyi CE Wass. DATA S ELECTION 4 4 4 Geometric Cos Higher-Order CMD MMD Others - P-Norm PAD 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 NMT NMT NMT SMT NMT 4 4 4 4 LEARNING REPRESENTATIONS (Ganin et al., 2015) (Kim et al., 2017) (Liu et al., 2017) (Li et al., 2018) (Chen and Cardie, 2018) (Zellinger et al., 2017) (Peng et al., 2018) (Wu and Guo, 2020) (Ding et al., 2019) (Shah et al., 2018) (Zhu et al., 2019) (Gui et al., 2017) (Zhou et al., 2019) (Cao et al., 2018) (Wang et al., 2018) (Gu et al., 2019) (Britz et al., 2017) (Zeng et al., 2018) (Wang et al., 2019) 4 4 4 4 SA Intent-clf SA Lang-ID SA SA SA SA Intent-Clf Question sim EmoRegress POS NER NER NER NMT NMT NMT NMT 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 DECISIONS IN THE WILD (Ravi et al., 2008) (Elsahar and Gallé, 2019) (Ponomareva and Thelwall, 2012) (Van Asch and Daelemans, 2010) 4 Parsing SA, POS SA 4 POS 4 4 4 4 4 4 4 4 4 Table 1: Prior works using divergence measures for Data Selection, Learning Representations and Decisio"
2021.naacl-main.147,D17-1147,0,0.0331321,"Missing"
2021.naacl-main.147,N19-1019,0,0.0576422,"Missing"
2021.naacl-main.147,W10-2605,0,0.648997,"Missing"
2021.naacl-main.147,N18-1136,0,0.0626967,"Missing"
2021.naacl-main.147,2020.emnlp-main.639,0,0.0264279,"ging to two domains. Such concepts have been explored in computer vision (Tzeng et al., 2014). While single source and target domains are common, complementary information available in multiple domains can help to improve performance in a target domain. This is especially helpful when there is no large-scale labelled data in any one domain, but where smaller amounts are available in several domains. DANN and DSN have been extended to such multi-source domain adaptation: for intent classification (Ding et al., 2019), sentiment analysis (Chen and Cardie, 2018; Li et al., 2018; Guo et al., 2018; Wright and Augenstein, 2020) and machine translation (Gu et al., 2019; Wang et al., 2019). learning (Chen et al., 2018; Zou et al., 2018; Yasunaga et al., 2018). Most works that adopt DANN and DSN framework reduce either the PAD or MMD divergence. However, reducing the divergences, combined with other auxiliary task specific loss functions, can result in training instabilities and vanishing gradients when the domain discriminator becomes increasingly accurate (Shen et al., 2018). Using other higher order measures can result in more stable learning. In this vein, CMD has been used for sentiment analysis (Zellinger et al.,"
2021.naacl-main.147,P17-2089,0,0.161277,"rld applications and the sentations (CWR) — mostly derived from neural difficulty of obtaining parallel sentences for every networks (Devlin et al., 2019; Peters et al., 2018) 1832 the prevalence of the divergence measures in NLP. Paper Task(s) Information-Theoretic KL (Plank and van Noord, 2011) (Dai et al., 2019) (Ruder and Plank, 2017) (Ruder et al., 2017) (Remus, 2012) (Lü et al., 2007) (Zhao et al., 2004) (Yasuda et al., 2008) (Moore and Lewis, 2010) (Axelrod et al., 2011) (Duh et al., 2013) (Liu et al., 2014) (van der Wees et al., 2017) (Silva et al., 2018) (Aharoni and Goldberg, 2020) (Wang et al., 2017) (Carpuat et al., 2017) (Vyas et al., 2018) (Chen and Huang, 2016) (Chen et al., 2017) Par, POS NER SA, NER, Par SA SA SMT SMT SMT SMT SMT SMT SMT NMT NMT NMT 4 JS Renyi CE Wass. DATA S ELECTION 4 4 4 Geometric Cos Higher-Order CMD MMD Others - P-Norm PAD 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 NMT NMT NMT SMT NMT 4 4 4 4 LEARNING REPRESENTATIONS (Ganin et al., 2015) (Kim et al., 2017) (Liu et al., 2017) (Li et al., 2018) (Chen and Cardie, 2018) (Zellinger et al., 2017) (Peng et al., 2018) (Wu and Guo, 2020) (Ding et al., 2019) (Shah et al., 2018) (Zhu et al., 2019) (Gui et al., 2017) (Z"
2021.naacl-main.147,2020.acl-main.764,0,0.0195829,"stic representations for text classiSimple measures based on word level features fication and structured prediction in multilingual have been used to predict the performance of a 1835 machine learning model in new domains. Information theoretic measures like Renyi-Div and KL-Div has been used for predicting performance drops in POS (Van Asch and Daelemans, 2010) and CrossEntropy based measure has been used for dependency parsing (Ravi et al., 2008). Prediction of performance can also be useful for machine translation where obtaining parallel data is hard. Based on distance between languages, (Xia et al., 2020) predict performance of the model on new languages for MT, among other tasks. Such performance prediction models have also been done in the past for SMT (Birch et al., 2008; Specia et al., 2013). However, Ponomareva and Thelwall (2012) argue that predicting drops in performance is more appropriate compared to raw performance. They find that JS-Div effective for predicting performance drop of Sentiment Analysis systems. Only recently, predicting model failures in practical deployments from an empirical viewpoint has regained attention. Elsahar and Gallé (2019) find the efficacy of higher-order"
2021.naacl-main.147,I08-2088,0,0.181576,"anslation static embeddings do not change according to the compared to other tasks. This can be attributed context of use. In contrast, contextual word repreto its popularity in real-world applications and the sentations (CWR) — mostly derived from neural difficulty of obtaining parallel sentences for every networks (Devlin et al., 2019; Peters et al., 2018) 1832 the prevalence of the divergence measures in NLP. Paper Task(s) Information-Theoretic KL (Plank and van Noord, 2011) (Dai et al., 2019) (Ruder and Plank, 2017) (Ruder et al., 2017) (Remus, 2012) (Lü et al., 2007) (Zhao et al., 2004) (Yasuda et al., 2008) (Moore and Lewis, 2010) (Axelrod et al., 2011) (Duh et al., 2013) (Liu et al., 2014) (van der Wees et al., 2017) (Silva et al., 2018) (Aharoni and Goldberg, 2020) (Wang et al., 2017) (Carpuat et al., 2017) (Vyas et al., 2018) (Chen and Huang, 2016) (Chen et al., 2017) Par, POS NER SA, NER, Par SA SA SMT SMT SMT SMT SMT SMT SMT NMT NMT NMT 4 JS Renyi CE Wass. DATA S ELECTION 4 4 4 Geometric Cos Higher-Order CMD MMD Others - P-Norm PAD 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 NMT NMT NMT SMT NMT 4 4 4 4 LEARNING REPRESENTATIONS (Ganin et al., 2015) (Kim et al., 2017) (Liu et al., 2017) (Li"
2021.naacl-main.147,N18-1089,0,0.0134731,"common, complementary information available in multiple domains can help to improve performance in a target domain. This is especially helpful when there is no large-scale labelled data in any one domain, but where smaller amounts are available in several domains. DANN and DSN have been extended to such multi-source domain adaptation: for intent classification (Ding et al., 2019), sentiment analysis (Chen and Cardie, 2018; Li et al., 2018; Guo et al., 2018; Wright and Augenstein, 2020) and machine translation (Gu et al., 2019; Wang et al., 2019). learning (Chen et al., 2018; Zou et al., 2018; Yasunaga et al., 2018). Most works that adopt DANN and DSN framework reduce either the PAD or MMD divergence. However, reducing the divergences, combined with other auxiliary task specific loss functions, can result in training instabilities and vanishing gradients when the domain discriminator becomes increasingly accurate (Shen et al., 2018). Using other higher order measures can result in more stable learning. In this vein, CMD has been used for sentiment analysis (Zellinger et al., 2017; Peng et al., 2018), and Wasserstein distance has been used for duplicate question detection (Shah et al., 2018) and to learn"
2021.naacl-main.469,2020.acl-main.135,1,0.841429,"dge Generate a comparison-type multi-hop question QC by fusing two single-hop questions Selection Generation Fusion Table 1: The 8 basic operators for MQA-QG, categorized into 3 groups. Selection: retrieve relevant information from contexts. Generation: generate information from a single context. Fusion: fuse retrieved/generated information to construct multi-hop questions. Each operator is defined as a function mapping f (X) → Y . have largely been addressed. QG research has started to generate more complex questions that require deep comprehension and multi-hop reasoning (Tuan et al., 2020; Pan et al., 2020; Xie et al., 2020; Yu et al., 2020). For example, Tuan et al. (2020) proposed a multi-state attention mechanism to mimic the multi-hop reasoning process. Pan et al. (2020) parsed the input passage as a semantic graph to facilitate the reasoning over different entities. However, these supervised methods require large amounts of human-written multi-hop questions as training data. Instead, we propose the first unsupervised QG system to generate multi-hop questions without the need to access those annotated data. 3 Methodology nents: operators, reasoning graphs, and question filtration. Operators"
2021.naacl-main.469,P02-1040,0,0.116995,"Missing"
2021.naacl-main.469,2020.emnlp-main.89,0,0.0222683,"able T into a document PT and then respectively. The remaining reasoning graphs defeed PT to the pre-trained GPT-2 model (Radford fine four types of multi-hop questions. 1) Table-toet al., 2019) to generate the output sentence Y . To avoid irrelevant information in PT , we apply a tem- Text: bridge-type question between table and text, where the answer comes from the text. 2) Text-toplate that only describes the row where the target Table: bridge-type question between table and text, entity locates. We then finetune the model on the where the answer comes from the table. 3) TextToTTo dataset (Parikh et al., 2020), a large-scale to-Text: bridge-type question between two texts. dataset of controlled table-to-text generation, by 4) Comparison: comparison-type question based maximizing the likelihood of p(Y |PT ; β), with β denoting the model parameters. The implementa- on two passages. These four reasoning chains can tion details and the model evaluation are in Ap- cover a large portion of questions in existing multihop QA datasets, such as HotpotQA and HybridQA. pendix A.1. We generate QA pairs by executing each reasoning • QuesToSent: This operator convert a question graph. Our framework can easily ext"
2021.naacl-main.469,2020.emnlp-main.713,0,0.0193627,"ltiple data sources in order to propose a reasonable question. Extractive Question Answering (EQA) is the task of answering questions by selecting a span from To address the above problem, we pursue a more the given context document. Works on EQA can realistic setting, i.e., unsupervised multi-hop QA, be divided into the single-hop (Rajpurkar et al., in which we assume no human-labeled multi-hop 2016, 2018; Kwiatkowski et al., 2019) and multi- question is available for training, and we explore hop cases (Yang et al., 2018; Welbl et al., 2018; the possibility of generating human-like multi-hop Perez et al., 2020). Unlike single-hop QA, which question–answer pairs to train the QA model. We assumes the question can be answered with a sin- study multi-hop QA for both the homogeneous gle sentence or document, multi-hop QA requires case where relevant evidence is in the textual combining disjoint pieces of evidence to answer forms (Yang et al., 2018) and the heterogeneous a question. Though different well-designed neu- case where evidence is manifest in both tabular ral models (Qiu et al., 2019; Fang et al., 2020) and textual forms (Chen et al., 2020b). Though have achieved near-human performance on the su"
2021.naacl-main.469,2020.emnlp-main.468,0,0.0268573,"Unsupervised / Zero-Shot QA has been proposed to train question answering models without any humanlabeled training data. Lewis et al. (2019) proposed the first unsupervised QA model which generates synthetic (context, question, answer) triples to train the QA model using unsupervised machine translation. However, the generated questions are unlike human-written questions and tend to have a lot of lexical overlaps with the context. To address this, followup works utilized the Wikipedia cited documents (Li et al., 2020), predefined templates (Fabbri et al., 2020), or pretrained language model (Puri et al., 2020) to produce more natural questions resembling the human-annotated ones. However, all the existing studies are focused on the SQuAD (Rajpurkar et al., 2016) dataset to answer single-hop and text-only questions. These methods do not generalize to multi-hop QA because they lack integrating and reasoning over disjoint pieces of evidence. Furthermore, they are restricted to text-based QA without considering structured or semi-structured data sources such as KB and Table. In contrast, we propose the first framework for unsupervised multi-hop QA, which can reason over disjoint structured or unstructu"
2021.sigdial-1.14,2020.acl-main.12,0,0.0223081,"tion process. By running this process many times, we can create an arbitrarily-large dataset that can be used to train a dialogue comprehension model. 4 Experiments To evaluate we need a dataset for dialogue comprehension. Unfortunately, no suitable dataset exists for this purpose, so we pick an existing dialogue dataset and retrofit it for our evaluation purposes. We start with the MultiWOZ dataset, commonly used for DST. We choose a range of domains from MultiWOZ to work with to showcase domain agnostic feature of our framework. We leave hospital and police domains out, following past work (Campagna et al., 2020) since they lack correct annotations and validation and test sets. From the remaining five we choose restaurant, hotel, and taxi domains as their pools of slot labels show very few overlaps thus resulting in a diverse dataset. The resulting corpus contains 2,409 dialogues, averaging 8.92 turns per dialogue, and 12.2 tokens per turn. But since MultiWOZ does not come with dialogue comprehension questions natively, we supply our our own hand annotated questions as detailed next. 137 Speaker Turn Speaker Turn User Hello, I would like to find a place to dine in, there will be restaurant bookpeople"
2021.sigdial-1.14,D18-1241,0,0.122889,"ctions to accomplish common tasks: work email threads, nurse– patient conversations, customer service conversations, etc. (cf. Table 1). Systems that can comprehend and answer key questions about these dialogues can significantly speed up information extraction from such documents. However, studies in machine reading comprehension (MRC) largely focus on the written form of text, such as news articles, Wikipedia documents, etc. These are not directly applicable to dialogue comprehension. While there are datasets that incorporate dialogue components in MRC (Sun et al., 2020; Reddy et al., 2020; Choi et al., 2018), they are not representative of task-oriented dialogue. Such dialogue comprehension systems are currently constrained by the lack of annotated data. A task-oriented dialogue is a form of information exchange where the system obtains user preferences (i.e. slot values for attributes) by conversation. The dynamic flow between speakers in these dialogues introduces additional challenges such as: (1) Mind change: Speakers might state their preference over some attribute/slot two or more times (cf. Table 1 U3&U4: Italian → − British food); (2) Topic drift: Speakers might change the topic of the co"
2021.sigdial-1.14,N19-1423,0,0.630336,"split are identical with MultiWOZ. Note specifically that this generation process is completely separate from the dialogue augmentation in Velocidapter that we evaluate. We also randomly sample a small development set, vel dev containing few dialogue (e.g. 2–10 dialogues) from the training set of each domain to extract turn templates for Velocidapter. During sampling, we ensure that the final set of dialogues cover all possible slots encountered in the test set so that the trained model will be exposed to each slot at least once (e.g. food type, booking day, etc.). We fine-tune the BERT-Base (Devlin et al., 2019a) and BiDAF models (Seo et al., 2016) in experiments representing three different scenarios/datasets: (1) In a high-resource scenario on MRCWOZ, which serves as an upper bound for our experimental setup. We term the models that are fine-tuned with this dataset WOZ Large; (2) In a low resource setting on the small vel dev set, which uses only a handful of dialogues. We term models fine-tuned with this other training set WOZ Small. (3) In our proposed setting on our framework’s synthetic dataset. We term models trained with this set as Velocidapter. Considering that our synthetic data is genera"
2021.sigdial-1.14,D17-1087,0,0.0223682,"ient for use in our setting, as they all assume at least an unlabeled dataset in the domain to generate the synthetic data. Domain adaptation (DA). With the recent increase in the number of large corpora, DA has attracted the attention of many MRC researchers. Zhao and Liu (2018) and Wiese et al. (2017) use models pretrained with the SQuAD dataset to increase performance in the target domain, utilizing small amounts of labeled data. In (Hazen et al., 2019), the authors pretrain models over the many large MRC corpora (SQuAD, NewsQA, etc.), then fine-tune them on the associated development set. Golub et al. (2017) and Wang et al. (2019) both use a data-driven approach generating synthetic questions on target unlabeled data and fine-tuning models on this synthetic data. In a variant, Li et al. (2019) instead ensemble pretrained language models, before appropriate fine-tuning. 3 Velocidapter: Data Generation Framework Let us first formalize our task. Our goal is to create a task-oriented dialogue-augmentation framework F , that given a list of dialogue turn templates T , a slot label-value list SV , and finally a slot labelquestion list SQ , can generate a large dialogue comprehension dataset D. F create"
2021.sigdial-1.14,W18-2605,0,0.0168644,"thus conclude that this approach potentially can greatly facilitate the rapid advancement of understudied task-oriented dialogue areas, which lack sufficient corpora. 1 Framework and experimental data available at https: //github.com/cuthalionn/Velocidapter 134 2 Related Work Reading Comprehension. Corpora on reading comprehension are largely limited to written text, e.g., SQuAD (Rajpurkar et al., 2018b), MARCO (Nguyen et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and many others (Hermann et al., 2015; Hill et al., 2016; Richardson et al., 2013; Kocisk´y et al., 2017; He et al., 2018). These datasets are all collections of written passages: SQuAD collects Q–A pairs for Wikipedia articles; MARCO collects pairs from Bing, along with context passages; RACE from English exams; and TriviaQA collects pairs with evidence documents. A few incorporate a conversational component to the MRC task. DREAM (Sun et al., 2020), FriendsQA (Yang and Choi, 2019) and a study by Ma et al. (2018) are all dialogue comprehension datasets. Although a valuable source, these do not apply to taskoriented dialogue comprehension, as all three are open-domain and multi-party. In contrast, CoQa and QuAC d"
2021.sigdial-1.14,P17-1147,0,0.026455,"o the first to address dialoguespecific challenges that span over several turns within a machine comprehension perspective. We thus conclude that this approach potentially can greatly facilitate the rapid advancement of understudied task-oriented dialogue areas, which lack sufficient corpora. 1 Framework and experimental data available at https: //github.com/cuthalionn/Velocidapter 134 2 Related Work Reading Comprehension. Corpora on reading comprehension are largely limited to written text, e.g., SQuAD (Rajpurkar et al., 2018b), MARCO (Nguyen et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and many others (Hermann et al., 2015; Hill et al., 2016; Richardson et al., 2013; Kocisk´y et al., 2017; He et al., 2018). These datasets are all collections of written passages: SQuAD collects Q–A pairs for Wikipedia articles; MARCO collects pairs from Bing, along with context passages; RACE from English exams; and TriviaQA collects pairs with evidence documents. A few incorporate a conversational component to the MRC task. DREAM (Sun et al., 2020), FriendsQA (Yang and Choi, 2019) and a study by Ma et al. (2018) are all dialogue comprehension datasets. Although a valuable source, these do n"
2021.sigdial-1.14,D19-5828,0,0.013759,"rge corpora, DA has attracted the attention of many MRC researchers. Zhao and Liu (2018) and Wiese et al. (2017) use models pretrained with the SQuAD dataset to increase performance in the target domain, utilizing small amounts of labeled data. In (Hazen et al., 2019), the authors pretrain models over the many large MRC corpora (SQuAD, NewsQA, etc.), then fine-tune them on the associated development set. Golub et al. (2017) and Wang et al. (2019) both use a data-driven approach generating synthetic questions on target unlabeled data and fine-tuning models on this synthetic data. In a variant, Li et al. (2019) instead ensemble pretrained language models, before appropriate fine-tuning. 3 Velocidapter: Data Generation Framework Let us first formalize our task. Our goal is to create a task-oriented dialogue-augmentation framework F , that given a list of dialogue turn templates T , a slot label-value list SV , and finally a slot labelquestion list SQ , can generate a large dialogue comprehension dataset D. F creates individual synthetic dialogues in D by composing turn templates from T , filling these turn templates with values from SV , and finally matching these to questions from SQ . D then can be"
2021.sigdial-1.14,N18-1185,0,0.053551,"Missing"
2021.sigdial-1.14,2020.findings-emnlp.17,0,0.0233373,"low within the generated dialogue. Shah et al. (2018) also use a template-based approach: they simulate dialogue templates with a rule-based system and then use crowdsourced workers to fill in the templates, generating a dialogue corpus. This process requires manual work for each dialogue created. Data-driven approaches largely lack the transparent controllability and diversity provided by a template-based approach (Duˇsek et al., 2020). There are, however, studies that tackle this problem. Wiseman et al. (2018); Ye et al. (2020) try to learn templates from data and use them to generate text. Peng et al. (2020) uses few-shot learning to train models that can be easily adapted to new domains. However, these are not convenient for use in our setting, as they all assume at least an unlabeled dataset in the domain to generate the synthetic data. Domain adaptation (DA). With the recent increase in the number of large corpora, DA has attracted the attention of many MRC researchers. Zhao and Liu (2018) and Wiese et al. (2017) use models pretrained with the SQuAD dataset to increase performance in the target domain, utilizing small amounts of labeled data. In (Hazen et al., 2019), the authors pretrain model"
2021.sigdial-1.14,P18-2124,0,0.187552,"nversations to augment a large set of instantiated dialogue datasets. Our framework is also the first to address dialoguespecific challenges that span over several turns within a machine comprehension perspective. We thus conclude that this approach potentially can greatly facilitate the rapid advancement of understudied task-oriented dialogue areas, which lack sufficient corpora. 1 Framework and experimental data available at https: //github.com/cuthalionn/Velocidapter 134 2 Related Work Reading Comprehension. Corpora on reading comprehension are largely limited to written text, e.g., SQuAD (Rajpurkar et al., 2018b), MARCO (Nguyen et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and many others (Hermann et al., 2015; Hill et al., 2016; Richardson et al., 2013; Kocisk´y et al., 2017; He et al., 2018). These datasets are all collections of written passages: SQuAD collects Q–A pairs for Wikipedia articles; MARCO collects pairs from Bing, along with context passages; RACE from English exams; and TriviaQA collects pairs with evidence documents. A few incorporate a conversational component to the MRC task. DREAM (Sun et al., 2020), FriendsQA (Yang and Choi, 2019) and a study by Ma et al."
2021.sigdial-1.14,D13-1020,0,0.0344875,"s within a machine comprehension perspective. We thus conclude that this approach potentially can greatly facilitate the rapid advancement of understudied task-oriented dialogue areas, which lack sufficient corpora. 1 Framework and experimental data available at https: //github.com/cuthalionn/Velocidapter 134 2 Related Work Reading Comprehension. Corpora on reading comprehension are largely limited to written text, e.g., SQuAD (Rajpurkar et al., 2018b), MARCO (Nguyen et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and many others (Hermann et al., 2015; Hill et al., 2016; Richardson et al., 2013; Kocisk´y et al., 2017; He et al., 2018). These datasets are all collections of written passages: SQuAD collects Q–A pairs for Wikipedia articles; MARCO collects pairs from Bing, along with context passages; RACE from English exams; and TriviaQA collects pairs with evidence documents. A few incorporate a conversational component to the MRC task. DREAM (Sun et al., 2020), FriendsQA (Yang and Choi, 2019) and a study by Ma et al. (2018) are all dialogue comprehension datasets. Although a valuable source, these do not apply to taskoriented dialogue comprehension, as all three are open-domain and"
2021.sigdial-1.14,K17-1029,0,0.0215261,"ach (Duˇsek et al., 2020). There are, however, studies that tackle this problem. Wiseman et al. (2018); Ye et al. (2020) try to learn templates from data and use them to generate text. Peng et al. (2020) uses few-shot learning to train models that can be easily adapted to new domains. However, these are not convenient for use in our setting, as they all assume at least an unlabeled dataset in the domain to generate the synthetic data. Domain adaptation (DA). With the recent increase in the number of large corpora, DA has attracted the attention of many MRC researchers. Zhao and Liu (2018) and Wiese et al. (2017) use models pretrained with the SQuAD dataset to increase performance in the target domain, utilizing small amounts of labeled data. In (Hazen et al., 2019), the authors pretrain models over the many large MRC corpora (SQuAD, NewsQA, etc.), then fine-tune them on the associated development set. Golub et al. (2017) and Wang et al. (2019) both use a data-driven approach generating synthetic questions on target unlabeled data and fine-tuning models on this synthetic data. In a variant, Li et al. (2019) instead ensemble pretrained language models, before appropriate fine-tuning. 3 Velocidapter: Da"
2021.sigdial-1.14,D18-1356,0,0.0259426,"n dialogues. However, their system is confined to turn-level transformations, limiting the information flow within the generated dialogue. Shah et al. (2018) also use a template-based approach: they simulate dialogue templates with a rule-based system and then use crowdsourced workers to fill in the templates, generating a dialogue corpus. This process requires manual work for each dialogue created. Data-driven approaches largely lack the transparent controllability and diversity provided by a template-based approach (Duˇsek et al., 2020). There are, however, studies that tackle this problem. Wiseman et al. (2018); Ye et al. (2020) try to learn templates from data and use them to generate text. Peng et al. (2020) uses few-shot learning to train models that can be easily adapted to new domains. However, these are not convenient for use in our setting, as they all assume at least an unlabeled dataset in the domain to generate the synthetic data. Domain adaptation (DA). With the recent increase in the number of large corpora, DA has attracted the attention of many MRC researchers. Zhao and Liu (2018) and Wiese et al. (2017) use models pretrained with the SQuAD dataset to increase performance in the target"
2021.sigdial-1.14,W19-5923,0,0.0265476,"ten text, e.g., SQuAD (Rajpurkar et al., 2018b), MARCO (Nguyen et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and many others (Hermann et al., 2015; Hill et al., 2016; Richardson et al., 2013; Kocisk´y et al., 2017; He et al., 2018). These datasets are all collections of written passages: SQuAD collects Q–A pairs for Wikipedia articles; MARCO collects pairs from Bing, along with context passages; RACE from English exams; and TriviaQA collects pairs with evidence documents. A few incorporate a conversational component to the MRC task. DREAM (Sun et al., 2020), FriendsQA (Yang and Choi, 2019) and a study by Ma et al. (2018) are all dialogue comprehension datasets. Although a valuable source, these do not apply to taskoriented dialogue comprehension, as all three are open-domain and multi-party. In contrast, CoQa and QuAC do employ two-party dialogue; however, their task is to conversationally answer questions about a passage, diverging from our task definition (Reddy et al., 2020; Choi et al., 2018). Synthetic Text Generation. Natural language generation (NLG) systems are basic components of text generation. These systems can be classified into three different categories by their"
2021.sigdial-1.14,2020.nlp4convai-1.13,0,0.226782,"u with? U6: No, that’s all I need. Thanks for your help! We introduce a synthetic dialogue generation framework, Velocidapter, which addresses the corpus availability problem for dialogue comprehension. Velocidapter augments datasets by simulating synthetic conversations for a task-oriented dialogue domain, requiring a small amount of bootstrapping work for each new domain. We evaluate the efficacy of our framework on a task-oriented dialogue comprehension dataset, MRCWOZ, which we curate by annotating questions for slots in the restaurant, taxi, and hotel domains of the MultiWOZ 2.2 dataset (Zang et al., 2020). We run experiments within a low-resource setting, where we pretrain a model on SQuAD, fine-tuning it on either a small original data or on the synthetic data generated by our framework. Velocidapter shows significant improvements using both the transformer-based BERTBase and BiDAF as base models. We further show that the framework is easy to use by novice users and conclude that Velocidapter can greatly help training over task-oriented dialogues, especially for low-resourced emerging domains. 1 Q1: A1: Q2: A2: Q3: A3: Q4: A4: Q5: A5: What type of food does the user want to have? British What"
bird-etal-2008-acl,D07-1089,0,\N,Missing
bird-etal-2008-acl,W06-1613,0,\N,Missing
bird-etal-2008-acl,N04-1042,0,\N,Missing
bird-etal-2008-acl,radev-etal-2004-mead,1,\N,Missing
C10-1065,W08-0312,0,0.0183476,"Missing"
C10-1065,W97-0703,0,0.0194392,"is found to achieve the highest correlation with human annotations. We also provide evidence that the degree of semantic similarity varies with the location of the partially-matching component words. 1 Introduction Keyphrases are noun phrases (NPs) that are representative of the main content of documents. Since they represent the key topics in documents, extracting good keyphrases benefits various natural language processing (NLP) applications such as summarization, information retrieval (IR) and question-answering (QA). Keyphrases can also be used in text summarization as semantic metadata (Barzilay and Elhadad, 1997; Lawrie et al., 2001; D’Avanzo and Magnini, 2005). In search engines, keyphrases supplement full-text indexing and assist users in creating good queries. In the past, a large body of work on keyphrases has been carried out as an extraction task, utilizing three types of cohesion: (1) document cohesion, i.e. cohesion between documents and keyphrases (Frank et al., 1999; Witten et al., 1999; Despite recent successes in keyphrase extraction (Frank et al., 1999; Turney, 2003; Park et al., 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), current work is hampered by the inflexibility of stan"
C10-1065,S10-1004,1,0.888563,"Missing"
C10-1065,N03-1020,0,0.0479685,"etween the reference and candidate translations. The difference is that it allows for more match flexibility, including stem variation and WordNet synonymy. The basic metric is based on the number of mapped unigrams found between the two strings, the total number of unigrams in the translation, and the total number of unigrams in the reference. NIST (Martin and Przybocki, 1999) is once again similar to BLEU, but integrates a proportional difference in the co-occurrences for all ngrams while weighting more heavily n-grams that occur less frequently, according to their information value. ROUGE (Lin and Hovy, 2003) — and its variants including ROUGE-N and ROUGE-L — is similarly based on n-gram overlap between the candidate and reference summaries. For example, ROUGE-N is based on co-occurrence statistics, using higher-order n-grams (n > 1) to estimate the fluency of summaries. ROUGE-L uses longest common subsequence (LCS)-based statistics, based on the assumption that the longer the substring overlap between the two strings, the greater the similar Saggion et al. (2002). ROUGEW is a weighted LCS-based statistic that prioritizes consecutive LCSes. In this research, we experiment exclusively with the basi"
C10-1065,W09-0404,0,0.0450082,"Missing"
C10-1065,2001.mtsummit-papers.68,0,0.0658224,"Missing"
C10-1065,C08-2021,0,0.031443,"Missing"
C10-1065,C02-1073,0,0.0347311,"-occurrences for all ngrams while weighting more heavily n-grams that occur less frequently, according to their information value. ROUGE (Lin and Hovy, 2003) — and its variants including ROUGE-N and ROUGE-L — is similarly based on n-gram overlap between the candidate and reference summaries. For example, ROUGE-N is based on co-occurrence statistics, using higher-order n-grams (n > 1) to estimate the fluency of summaries. ROUGE-L uses longest common subsequence (LCS)-based statistics, based on the assumption that the longer the substring overlap between the two strings, the greater the similar Saggion et al. (2002). ROUGEW is a weighted LCS-based statistic that prioritizes consecutive LCSes. In this research, we experiment exclusively with the basic ROUGE metric, and unigrams (i.e. ROUGE-1). 3.2 R-precision In order to analyze near-misses in keyphrase extraction evaluation, Zesch and Gurevych (2009) proposed R-precision, an n-gram-based evaluation metric for keyphrase evaluation.3 R-precision contrasts with the majority of previous work on keyphrase extraction evaluation, which has used semantic similarity based on external resources 3 Zesch and Gurevych’s R-precision has nothing to do with the informat"
C10-1065,C08-1122,0,0.0279896,"Missing"
C10-1065,R09-1086,0,0.579699,"rithm. Also, computing algorithm is closer than effective grid to the same goldstandard keyphrase. From these observations, we infer that n-gram-based evaluation metrics can be applied to evaluating keyphrase extraction, but also that candidates with the same relative n-gram overlap are not necessarily equally good. Our primary goal is to test the utility of n-gram based evaluation metrics to the task of keyphrase extraction evaluation. We test the following evaluation metrics: (1) evaluation metrics from MT and multi-document summarization (BLEU, NIST, METEOR and ROUGE); and (2) R-precision (Zesch and Gurevych, 2009), an n-gram-based evaluation metric developed specifically for keyphrase extraction evaluation which has yet to be evaluated against humans at the extraction task. Secondarily, we attempt to shed light on the bigger question of whether it is feasible to expect that n-gram-based metrics without access to external resources should be able to capture subtle semantic differences in keyphrase candidates. To this end, we experimentally verify the impact of lexical overlap of different types on keyphrase similarity, and use this as the basis for proposing a variant of R-precision. In the next section"
C10-1065,W04-3252,0,\N,Missing
C10-1065,C02-1142,0,\N,Missing
C10-1065,P02-1040,0,\N,Missing
C10-1084,J96-1002,0,0.0157508,"espond. However, in a sentence pair there can be many instances of both the source and target morphemes. In our example, the the–n pair corresponds to definite nouns; there are two the and three -n instances, yielding 2 × 3=6 possible links. Deciding which instances are aligned is a decision problem. To solve this, we inspect the IBM Model 4 morpheme alignment to construct a CCM alignment model. The CCM model labels whether an instance of a CCM pair is deemed semantically related (linked). We cast the modeling problem as supervised learning, where we choose a maximum entropy (ME) formulation (Berger et al., 1996). We first discuss sample selection from the IBM Model 4 morpheme alignment, and then give details on the features extracted. The processes described below are done per sentence pair with f1m , 2 nPMI has a bounded range of [−1, 1] with values 1 and 0 indicating perfect positive and no correlation, respectively. en1 and U denoting the source, target sentences and the union alignments, respectively. Class labels. We base this on the initial IBM Model 4 alignment to label each CCM pair instance as a positive or negative example: Positive examples are simply CCM pairs in U. To be precise, links j"
C10-1084,W07-0403,0,0.0215308,"fter reviewing related work, we give a case study for morpheme alignment in Section 3. Section 4 presents our four-step approach to construct and incorporate our CCM alignment model into the grow-diag process. Section 5 describes experiments, while Section 6 analyzes the system merits. We conclude with suggestions for future work. 2 Related Work MT alignment has been an active research area. One can categorize previous approaches into those that use language-specific syntactic information and those that do not. Syntactic parse trees have been used to enhance alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; DeNero and Klein, 2007; Zhang et al., 2008; Haghighi et al., 2009). With syntactic knowledge, modeling long distance reordering is possible as the search space is confined to plausible syntactic variants. However, they generally require language-specific tools and annotated data, making such approaches infeasible for many languages. Works that follow non-syntactic approaches, such as (Matusov et al., 743 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 743–751, Beijing, August 2010 i1 declare2 resumed3 the4 session5 of6 the7 european8 parliam"
C10-1084,P05-1066,0,0.101571,"Missing"
C10-1084,P07-1003,0,0.0179658,"work, we give a case study for morpheme alignment in Section 3. Section 4 presents our four-step approach to construct and incorporate our CCM alignment model into the grow-diag process. Section 5 describes experiments, while Section 6 analyzes the system merits. We conclude with suggestions for future work. 2 Related Work MT alignment has been an active research area. One can categorize previous approaches into those that use language-specific syntactic information and those that do not. Syntactic parse trees have been used to enhance alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; DeNero and Klein, 2007; Zhang et al., 2008; Haghighi et al., 2009). With syntactic knowledge, modeling long distance reordering is possible as the search space is confined to plausible syntactic variants. However, they generally require language-specific tools and annotated data, making such approaches infeasible for many languages. Works that follow non-syntactic approaches, such as (Matusov et al., 743 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 743–751, Beijing, August 2010 i1 declare2 resumed3 the4 session5 of6 the7 european8 parliament9 adjourned10 on11 13"
C10-1084,P08-1112,0,0.0346843,"Missing"
C10-1084,W08-2118,0,0.0165484,"321 yli-‡391 Table 1: English(en)-Finnish(fi) Bilingual CCM pairs (N =128). Shown are the top 19 and last 10 of 168 bilingual CCM pairs extracted. Subscript i indicates the ith most frequent morpheme in each language. ‡ marks exact correspondence linguistically, whereas † suggests rough correspondence w.r.t http://en.wiktionary.org/wiki/. in common at the morpheme level. The manyto-one relationships among words on both sides is often captured better by one-to-one correspondences among morphemes. We wish to model such bilingual correspondence in terms of closedclass morphemes (CCM), similar to Nguyen and Vogel (2008)’s work that removes nonaligned affixes during the alignment process. Let us now formally define CCM and an associative measure to gauge such correspondence. Definition 1. Closed-class Morphemes (CCM) are a fixed set of stems and affixes that exhibit grammatical functions just like closed-class words. In highly inflected languages, we observe that grammatical meanings may be encoded in morphological stems and affixes, rather than separate words. While we cannot formally identify valid CCMs in a language-independent way (as by definition they manifest language-dependent grammatical functions),"
C10-1084,J03-1002,0,0.00874209,"rian are highly inflected languages, with numerous verbal and nominal cases, exhibiting agreement. Dataset statistics are given in Table 3. Train LM Dev Test en-fi Europarl-v1 Europarl-v1-fi wpt05-dev wpt05-test # 714K 714K 2000 2000 en-hu Europarl-v4 News-hu news-dev2009 news-test2009 # 1,510K 4,209K 2051 3027 Table 3: Dataset Statistics: the numbers of parallel sentences for training, LM training, development and test sets. We use the Moses SMT framework for our work, creating both our CCM-based systems and the baselines. In all systems built, we obtain the IBM Model 4 alignment via GIZA++ (Och and Ney, 2003). Results are reported using caseinsensitive BLEU (Papineni et al., 2001). Baselines. We build two SMT baselines: w-system: This is a standard phrase-based SMT, which operates at the word level. The system extracts phrases of maximum length 7 words, and uses a 4-gram word-based LM. wm -system: This baseline works at the word level just like the w-system, but differs at the alignment stage. Specifically, input to the IBM Model 4 training is the morpheme-level corpus, segmented by Morfessor and augmented with “+” to provide word-boundary information (§4.1). Using such information, we constrain t"
C10-1084,J04-4002,0,0.0755395,"d better word alignments that otherwise would be missed. Let us make this idea concrete with a case study of the benefits of morpheme based alignment. We show the intersecting alignments of an actual English (source) → Finnish (target) sentence pair in Figure 1, where (a) word-level and (b) morphemelevel alignments are shown. The morphemelevel alignment is produced by automatically segmenting words into morphemes and running IBM Model 4 on the resulting token stream. Intersection links (i.e., common to both direct and inverse alignments) play an important role in creating the final alignment (Och and Ney, 2004). While there are several heuristics used in the symmetrizing process, the grow-diag(onal) process is common and prevalent in many SMT systems, such as Moses (Koehn et al., 2007). In the growdiag process, intersection links are used as seeds to find other new alignments within their neighborhood. The process continues iteratively, until no further links can be added. In our example, the morpheme-level intersection alignment is better as it has no misalignments and adds new alignments. However it misses some key links. In particular, the alignments of closed-class morphemes (CCMs; later formall"
C10-1084,2001.mtsummit-papers.68,0,0.0106003,"cases, exhibiting agreement. Dataset statistics are given in Table 3. Train LM Dev Test en-fi Europarl-v1 Europarl-v1-fi wpt05-dev wpt05-test # 714K 714K 2000 2000 en-hu Europarl-v4 News-hu news-dev2009 news-test2009 # 1,510K 4,209K 2051 3027 Table 3: Dataset Statistics: the numbers of parallel sentences for training, LM training, development and test sets. We use the Moses SMT framework for our work, creating both our CCM-based systems and the baselines. In all systems built, we obtain the IBM Model 4 alignment via GIZA++ (Och and Ney, 2003). Results are reported using caseinsensitive BLEU (Papineni et al., 2001). Baselines. We build two SMT baselines: w-system: This is a standard phrase-based SMT, which operates at the word level. The system extracts phrases of maximum length 7 words, and uses a 4-gram word-based LM. wm -system: This baseline works at the word level just like the w-system, but differs at the alignment stage. Specifically, input to the IBM Model 4 training is the morpheme-level corpus, segmented by Morfessor and augmented with “+” to provide word-boundary information (§4.1). Using such information, we constrain the alignment symmetrization to extract phrase pairs of 7 words or less in"
C10-1084,P07-1090,1,0.934339,"the alignment process. Let us now formally define CCM and an associative measure to gauge such correspondence. Definition 1. Closed-class Morphemes (CCM) are a fixed set of stems and affixes that exhibit grammatical functions just like closed-class words. In highly inflected languages, we observe that grammatical meanings may be encoded in morphological stems and affixes, rather than separate words. While we cannot formally identify valid CCMs in a language-independent way (as by definition they manifest language-dependent grammatical functions), we can devise a good approximation. Following Setiawan et al. (2007), we induce the set of CCMs for a language as the top N frequent stems together with all affixes1 . Definition 2. Bilingual Normalized PMI (biPMI) is the averaged normalized PMI computed on the asymmetric morpheme alignments. Here, normalized PMI (Bouma, 2009), known to be less biased towards low-frequency data, is dep(x,y) fined as: nPMI(x, y) = ln p(x)p(y) )/- ln p(x, y), where p(x), p(y), and p(x, y) follow definitions in the standard PMI formula. In our case, we only 1 Note that we employ length and vowel sequence heuristics to filter out corpus-specific morphemes. 745 compute the scores f"
C10-1084,H05-1085,0,0.0705064,"words. Our languageindependent model recovers important links missing in the IBM Model 4 alignment and demonstrates improved end-toend translations for English-Finnish and English-Hungarian. 1 Introduction Modern statistical machine translation (SMT) systems, regardless of whether they are word-, phrase- or syntax-based, typically use the word as the atomic unit of translation. While this approach works when translating between languages with limited morphology such as English and French, it has been found inadequate for morphologicallyrich languages like Arabic, Czech and Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). As a result, a line of SMT research has worked to incorporate morphological analysis to gain access to information encoded within individual words. In a typical MT process, word aligned data is fed as training data to create a translation model. In cases where a highly inflected language is involved, the current word-based alignment approaches produce low-quality alignment, as the statistical correspondences between source and ∗ This work was supported by a National Research Foundation grant “Interactive Media Search” (grant # R-252-000325-279) target words are dif"
C10-1084,2007.mtsummit-papers.65,0,0.0642037,"orphemes before training the IBM Model 4 morpheme alignment. Second, from the morpheme alignment, we induce automatically bilingual CCM pairs. The core of our approach is in the third and fourth steps. In Step 3, we construct a CCM alignment model, and apply it on the segmented input corpus to obtain an automatic CCM alignment. Finally, in Step 4, we incorporate the CCM alignment into the symmetrizing process via our modified grow-diag process. 4.1 Step 1: Morphological Analysis The first step presupposes morphologically segmented input to compute the IBM Model 4 morpheme alignment. Following Virpioja et al. (2007), we use Morfessor, an unsupervised analyzer which learns morphological segmentation from raw tokenized text (Creutz and Lagus, 2007). The tool segments input words into labeled morphemes: PRE (prefix), STM (stem), and SUF (suffix). Multiple affixes can be proposed for each word; word compounding is allowed as well, e.g., uncarefully is analyzed as un/PRE+ care/STM+ ful/SUF+ ly/SUF. We append a “+” sign to each nonfinal tag to distinguish wordinternal morphemes from word-final ones, e.g., “x/STM” and “x/STM+” are considered different tokens. The “+” annotation enables the restoration of the or"
C10-1084,P09-1104,0,0.0125845,"gnment in Section 3. Section 4 presents our four-step approach to construct and incorporate our CCM alignment model into the grow-diag process. Section 5 describes experiments, while Section 6 analyzes the system merits. We conclude with suggestions for future work. 2 Related Work MT alignment has been an active research area. One can categorize previous approaches into those that use language-specific syntactic information and those that do not. Syntactic parse trees have been used to enhance alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; DeNero and Klein, 2007; Zhang et al., 2008; Haghighi et al., 2009). With syntactic knowledge, modeling long distance reordering is possible as the search space is confined to plausible syntactic variants. However, they generally require language-specific tools and annotated data, making such approaches infeasible for many languages. Works that follow non-syntactic approaches, such as (Matusov et al., 743 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 743–751, Beijing, August 2010 i1 declare2 resumed3 the4 session5 of6 the7 european8 parliament9 adjourned10 on11 1312 december13 199614 (a) -1 julistan2 euroop"
C10-1084,E06-1006,0,0.0639167,"model recovers important links missing in the IBM Model 4 alignment and demonstrates improved end-toend translations for English-Finnish and English-Hungarian. 1 Introduction Modern statistical machine translation (SMT) systems, regardless of whether they are word-, phrase- or syntax-based, typically use the word as the atomic unit of translation. While this approach works when translating between languages with limited morphology such as English and French, it has been found inadequate for morphologicallyrich languages like Arabic, Czech and Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). As a result, a line of SMT research has worked to incorporate morphological analysis to gain access to information encoded within individual words. In a typical MT process, word aligned data is fed as training data to create a translation model. In cases where a highly inflected language is involved, the current word-based alignment approaches produce low-quality alignment, as the statistical correspondences between source and ∗ This work was supported by a National Research Foundation grant “Interactive Media Search” (grant # R-252-000325-279) target words are diffused over many morphologic"
C10-1084,W09-0429,0,0.0219337,"er direction as future work. Though modest, the best improvement for en-hu is statistical significant at p&lt;0.01 according to Collins’ sign test (Collins et al., 2005). System w-system wm -system wm -system + CCM wm -system + CCM + wg = 1 wm -system + CCM + wg = 2 BLEU 9.63 9.47 9.82 +0.35 9.91 +0.44 9.87 Table 5: English/Hungarian results. Subscripts indicate absolute improvements with respect to the wm -system. We note that MT experiments for en/hu 4 are very limited, especially for the en to hu direction. Nov´ak (2009) obtained an improvement of 0.22 BLEU with no distortion penalty; whereas Koehn and Haddow (2009) enhanced by 0.5 points using monotone-at-punctuation reordering, minimum Bayes risk and larger beam size decoding. While not directly comparable in the exact settings, these systems share the same data source and splits similar to ours. In view of these community results, we conclude that our CCM model does perform competitively in the en-hu task, and indeed seems to be language independent. 6 Detailed Analysis The macroscopic evaluation validates our approach as improving BLEU over both baselines, 4 Hungarian was used in the ACL shared task 2008, 2009. but how do the various components contr"
C10-1084,P05-1059,0,0.0334055,"d translation quality. After reviewing related work, we give a case study for morpheme alignment in Section 3. Section 4 presents our four-step approach to construct and incorporate our CCM alignment model into the grow-diag process. Section 5 describes experiments, while Section 6 analyzes the system merits. We conclude with suggestions for future work. 2 Related Work MT alignment has been an active research area. One can categorize previous approaches into those that use language-specific syntactic information and those that do not. Syntactic parse trees have been used to enhance alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; DeNero and Klein, 2007; Zhang et al., 2008; Haghighi et al., 2009). With syntactic knowledge, modeling long distance reordering is possible as the search space is confined to plausible syntactic variants. However, they generally require language-specific tools and annotated data, making such approaches infeasible for many languages. Works that follow non-syntactic approaches, such as (Matusov et al., 743 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 743–751, Beijing, August 2010 i1 declare2 resumed3 the4 session5 of6"
C10-1084,P07-2045,0,0.00416687,"lignments of an actual English (source) → Finnish (target) sentence pair in Figure 1, where (a) word-level and (b) morphemelevel alignments are shown. The morphemelevel alignment is produced by automatically segmenting words into morphemes and running IBM Model 4 on the resulting token stream. Intersection links (i.e., common to both direct and inverse alignments) play an important role in creating the final alignment (Och and Ney, 2004). While there are several heuristics used in the symmetrizing process, the grow-diag(onal) process is common and prevalent in many SMT systems, such as Moses (Koehn et al., 2007). In the growdiag process, intersection links are used as seeds to find other new alignments within their neighborhood. The process continues iteratively, until no further links can be added. In our example, the morpheme-level intersection alignment is better as it has no misalignments and adds new alignments. However it misses some key links. In particular, the alignments of closed-class morphemes (CCMs; later formally defined) as indicated by the dotted lines in (b) are overlooked in the IBM Model 4 alignment. This difficulty in aligning CCMs is due to: 1. Occurrences of garbage-collector wo"
C10-1084,P08-1012,0,0.0151102,"udy for morpheme alignment in Section 3. Section 4 presents our four-step approach to construct and incorporate our CCM alignment model into the grow-diag process. Section 5 describes experiments, while Section 6 analyzes the system merits. We conclude with suggestions for future work. 2 Related Work MT alignment has been an active research area. One can categorize previous approaches into those that use language-specific syntactic information and those that do not. Syntactic parse trees have been used to enhance alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; DeNero and Klein, 2007; Zhang et al., 2008; Haghighi et al., 2009). With syntactic knowledge, modeling long distance reordering is possible as the search space is confined to plausible syntactic variants. However, they generally require language-specific tools and annotated data, making such approaches infeasible for many languages. Works that follow non-syntactic approaches, such as (Matusov et al., 743 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 743–751, Beijing, August 2010 i1 declare2 resumed3 the4 session5 of6 the7 european8 parliament9 adjourned10 on11 1312 december13 199614"
C10-1084,N04-4015,0,0.201605,"on nearby words. Our languageindependent model recovers important links missing in the IBM Model 4 alignment and demonstrates improved end-toend translations for English-Finnish and English-Hungarian. 1 Introduction Modern statistical machine translation (SMT) systems, regardless of whether they are word-, phrase- or syntax-based, typically use the word as the atomic unit of translation. While this approach works when translating between languages with limited morphology such as English and French, it has been found inadequate for morphologicallyrich languages like Arabic, Czech and Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). As a result, a line of SMT research has worked to incorporate morphological analysis to gain access to information encoded within individual words. In a typical MT process, word aligned data is fed as training data to create a translation model. In cases where a highly inflected language is involved, the current word-based alignment approaches produce low-quality alignment, as the statistical correspondences between source and ∗ This work was supported by a National Research Foundation grant “Interactive Media Search” (grant # R-252-00"
C10-1084,N06-1014,0,0.0323519,"2-3 5-4 9-5 8-6 10-7 10-8 11-9 0-10 7-11 14-12 15-13 15-14 16-15 11-16 11-17 11-18 11-19 11-20 11-21 0-22 11-23 Figure 1: Sample English-Finnish IBM Model 4 alignments: (a) word-level and (b) morpheme-level. Solid lines indicate intersection alignments, while the exhaustive asymmetric alignments are listed below. In (a), translation glosses for Finnish are given; the dash-dot line is the incorrect alignment. In (b), bolded texts are closed-class morphemes (CCM), while bolded indices indicate alignments involving CCMs. The dotted lines are correct CCM alignments not found by IBM Model 4. 2004; Liang et al., 2006; Ganchev et al., 2008), which aim to achieve symmetric word alignment during training, though good in many cases, are not designed to tackle highly inflected languages. Our work differs from these by taking a middle road. Instead of modifying the alignment algorithm directly, we preprocess asymmetric alignments to improve the input to the symmetrizing process later. Also, our approach does not make use of specific language resources, relying only on unsupervised morphological analysis. 3 A Case for Morpheme Alignment The notion that morpheme based alignment would be useful in highly inflected"
C10-1084,C04-1032,0,0.0711529,"Missing"
C10-1084,P02-1040,0,\N,Missing
C10-1084,W09-0428,0,\N,Missing
C10-1084,P04-1066,0,\N,Missing
C10-2049,C08-1062,0,0.012262,"s on differing values based on the presence of keywords in the sentence. Q, QA, and QS denote keywords from current, ancestor and sibling nodes. If the sentence contains keywords from other sibling nodes, we assign a penalty of 0.1. Otherwise, we assign a weight of 1.0, 0.5, or 0.25, based on whether keywords are present from both the ancestor node and current node, just the current node or just the ancestor node. To build the final summary, ReWoS selects the top scoring sentence and iteratively adds the next most highly ranked sentence, until the n sentence budget is reached. We use SimRank (Li et al., 2008) to remove the next sentence to be added, if it is too similar to the sentences already in the summary. 4.3 Figure 3: A context modeling example. motivates us to choose nearby sentences within a contextual window after the agent-based sentence to represent the topic. We set the contextual window to 5 and extract a maximum of 2 additional sentences. These additions are chosen based on their relevance scores to the topic, using Equation (3). Sentences with non-zero scores are then added as contexts of the anchor agent-based sentence, otherwise they are excluded. As a result, some topics may cont"
C10-2049,W04-1013,0,0.00502509,"t features that can be parameterized to create resulting summaries. We conducted an internal tuning of MEAD to maximize its performance on the RWSData. The optimal configuation uses just two tuned features of centroid and cosine similarity. Note that the MEAD baseline does use the topic tree keywords in computing cosine similarity score. Our ReWoS system is the only system that leverages the topic tree structure which is central to our approach. In our experiments, we used MEAD toolkit4 to produce the summaries for LEAD and MEAD baseline systems. Automatic evaluation was performed with ROUGE (Lin, 2004), a widely used and recognized automated summarization evaluation method. We employed a number of ROUGE variants, which have been proven to correlate with human judgments in multi-document summarization (Lin, 2004). However, given the small size of our summarization dataset, we can only draw notional 4 http://www.summarization.com/mead/ evidence from such an evaluation; it is not possible to find statistically significant conclusion from our evaluation. To partially address this, we also conducted a human evaluation to assess more fine-grained qualities of our system. We asked 11 human judges"
C10-2049,P08-1093,0,0.0826756,"n (Nakov et al., 2004), which showed that the citing sentences in other papers can give a useful description of a target work. Other studies focus mainly on single-document scientific article summarization. The pioneers of automated summarization (Luhn, 1958; Baxendale, 1958; Edmundson, 1969) had envisioned their approaches being used for the automatic creation of scientific summaries. They examined various features specific to scientific texts (e.g., frequency-based, sentence position, or rhetorical clues features) which were proven effective for domain-specific summarization tasks. Further, Mei and Zhai (2008) and Qazvinian and Radev (2008) utilized citation information in creating summaries for a single scientific article in computational linguistics domain. Also, Schwartz and Hearst (2006) also utilized the citation sentences to summarize the key concepts and entities in bioscience texts, and argued that citation sentences may contain informative contributions of a paper that complement its original abstract. 2 http://clair.si.umich.edu/clair/iopener/ These works all center on the role of citations and their contexts in creating a summary, using citation information to rank content for extraction"
C10-2049,D07-1040,0,0.0121328,"ing discourse work. We see our differentiation between general and detailed topics as a natural parallel to their notion of integrative and descriptive strategies. To introspect on these findings further, we created a related work data set (called RWSData3 ), which includes 20 articles from well-respected venues in NLP and IR, namely SIGIR, ACL, NAACL, EMNLP and COLING. We extracted the related work sections directly from those research articles as well as references the sections cited. References to books and Ph.D. theses were removed, as their verbosity would change the problem drastically (Mihalcea and Ceylan, 2007). Since we view each related work summary as a topic-biased summary originating from a topic hierarchy tree, annotation of such topical information for our data set is necessary. Each article’s data consists of the reference related work summary, the collection of the input research articles 3 To be made available at http://wing.comp.nus. edu.sg/downloads/rwsdata. 429 SbL−RW 17.9 7.9 6 40 average stdev min max WbL−RW 522.4 216.5 179 922 No−RAs 10.9 5.6 2 26 SbL−RA 2386.0 1306.7 348 5549 WbL−RA 51739.6 26682.3 8580 112267 TS 3.3 1.7 1 7 TD 1.8 0.6 1 3 Table 1: The demographics of RWSData. No, R"
C10-2049,N09-1066,0,0.430866,"ng the individual components will help bring us towards an eventual solution. In fact, existing works in the NLP and recommendation systems communities have already begun work that fits towards the completion of the first two tasks. Citation prediction (Nallapati et al., 2008) is a growing research area that has aimed both at predicting citation growth over time within a community and at individual paper citation patterns. This year, an automatic keyphrase extraction task from scientific articles was first fielded in SemEval-2, partially addressing Task 11 . Also, automatic survey generation (Mohammad et al., 2009) is becoming a growing field within the summarization community. However, to date, we have not yet seen any work that examines topic-biased summarization of multiple scientific articles. For these reasons, we focus on Task 3 here – the creation of a related work section, given a structured input of the topics for summary.. The remaining contributions of our paper 1 http://semeval2.fbk.eu/semeval2.php 427 Coling 2010: Poster Volume, pages 427–435, Beijing, August 2010 consists of work towards this goal: • We conduct a study of the argumentative patterns used in related work sections, to describ"
C10-2049,H05-1115,0,0.0186545,"ative way using the following linear combination: Q QR (1) scoreS → scoreQA S + scoreS − scoreS where scoreS is the final relevance score, and QR Q are the composcoreQA S , scoreS , and scoreS nent scores of the sentence S with respect to the ancestor, current or other remaining nodes. We give positive credit to a sentence that contains keywords from an ancestor node, but penalize sentences with keywords from other topics (as such sentences would be better descriptors for those other topics). Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005): Q scoreS = = P P rel(S, Q) (2) ′ Q′ rel(S, Q ) S Q w∈Q log(tfw + 1) × log(tfw + 1) × isfw N orm where rel(S, Q) is the relevance of S with respect to topic Q, N orm is a normalization factor of rel(S, Q) over all input sentences, tfwS and tfwQ are the term frequencies of token w within S or sentences that discuss topic Q, respectively. isfw is the inverse sentence frequency of w. 4.2 (S)pecific (C)ontent (Sum)marization SCSum aims to extract sentences that contain detailed information about a specific author’s work that is relevant to the input leaf node’s topic from the set of sentences tha"
C10-2049,C08-1087,0,0.164299,"which showed that the citing sentences in other papers can give a useful description of a target work. Other studies focus mainly on single-document scientific article summarization. The pioneers of automated summarization (Luhn, 1958; Baxendale, 1958; Edmundson, 1969) had envisioned their approaches being used for the automatic creation of scientific summaries. They examined various features specific to scientific texts (e.g., frequency-based, sentence position, or rhetorical clues features) which were proven effective for domain-specific summarization tasks. Further, Mei and Zhai (2008) and Qazvinian and Radev (2008) utilized citation information in creating summaries for a single scientific article in computational linguistics domain. Also, Schwartz and Hearst (2006) also utilized the citation sentences to summarize the key concepts and entities in bioscience texts, and argued that citation sentences may contain informative contributions of a paper that complement its original abstract. 2 http://clair.si.umich.edu/clair/iopener/ These works all center on the role of citations and their contexts in creating a summary, using citation information to rank content for extraction. However, they did not study t"
C10-2049,W06-3326,0,0.156155,"tific article summarization. The pioneers of automated summarization (Luhn, 1958; Baxendale, 1958; Edmundson, 1969) had envisioned their approaches being used for the automatic creation of scientific summaries. They examined various features specific to scientific texts (e.g., frequency-based, sentence position, or rhetorical clues features) which were proven effective for domain-specific summarization tasks. Further, Mei and Zhai (2008) and Qazvinian and Radev (2008) utilized citation information in creating summaries for a single scientific article in computational linguistics domain. Also, Schwartz and Hearst (2006) also utilized the citation sentences to summarize the key concepts and entities in bioscience texts, and argued that citation sentences may contain informative contributions of a paper that complement its original abstract. 2 http://clair.si.umich.edu/clair/iopener/ These works all center on the role of citations and their contexts in creating a summary, using citation information to rank content for extraction. However, they did not study the rhetorical structure of the intended summaries, targeting more on deriving useful content. For working along this vein, we turn to studies on the rheto"
C10-2049,J02-4002,0,0.0470906,"ce texts, and argued that citation sentences may contain informative contributions of a paper that complement its original abstract. 2 http://clair.si.umich.edu/clair/iopener/ These works all center on the role of citations and their contexts in creating a summary, using citation information to rank content for extraction. However, they did not study the rhetorical structure of the intended summaries, targeting more on deriving useful content. For working along this vein, we turn to studies on the rhetorical structure of scientific articles. Perhaps the most relevant is work by (Teufel, 1999; Teufel and Moens, 2002) who defined and studied argumentative zoning of texts, especially ones in computational linguistics. While they studied the structure of an entire article, it is clear from their studies that a related work section would contain general background knowledge (BACKGROUND zone) as well as specific information credited to others (OTHER and BASIS zones). This vein of work has been followed by many, including Teufel et al. (2009; Angrosh et al. (2010). 3 Structure of Related Work Section We first extend the work on rhetorical analysis, concentrating on related work sections. By studying examples in"
C10-2049,D09-1155,0,0.0559716,"useful content. For working along this vein, we turn to studies on the rhetorical structure of scientific articles. Perhaps the most relevant is work by (Teufel, 1999; Teufel and Moens, 2002) who defined and studied argumentative zoning of texts, especially ones in computational linguistics. While they studied the structure of an entire article, it is clear from their studies that a related work section would contain general background knowledge (BACKGROUND zone) as well as specific information credited to others (OTHER and BASIS zones). This vein of work has been followed by many, including Teufel et al. (2009; Angrosh et al. (2010). 3 Structure of Related Work Section We first extend the work on rhetorical analysis, concentrating on related work sections. By studying examples in detail, we gain insight on how to approach related work summarization. We focus on a concrete related work example for illustration, an excerpt of which is shown in Figure 1a. Focusing on the argumentative progression of the text, we note the flow through different topics is hierarchical and can be represented as a topic tree as in Figure 1b. This summary provides background knowledge for a paper on text classification, wh"
C10-2049,W05-1612,0,\N,Missing
C10-2049,N03-2024,0,\N,Missing
C10-2049,J05-3002,0,\N,Missing
C10-2049,N04-4001,0,\N,Missing
C10-2049,J04-4001,0,\N,Missing
C10-2049,C08-1023,0,\N,Missing
C10-2049,W08-1301,0,\N,Missing
C10-2049,P07-1083,0,\N,Missing
C10-2049,J02-4006,0,\N,Missing
C10-2049,J95-2003,0,\N,Missing
C10-2049,P08-1118,0,\N,Missing
C10-2049,J08-1001,0,\N,Missing
C10-2049,P09-1022,0,\N,Missing
C10-2049,P10-1057,0,\N,Missing
C10-2049,P06-1070,0,\N,Missing
C10-2049,P09-1024,0,\N,Missing
C10-2049,W01-0100,0,\N,Missing
C10-2049,radev-etal-2004-mead,0,\N,Missing
C10-2049,W09-3603,0,\N,Missing
C10-2049,X98-1025,0,\N,Missing
C10-2049,I05-2009,0,\N,Missing
C12-1128,R09-1002,0,0.0360699,"Missing"
C12-1128,J93-1003,0,0.0452279,"Missing"
C12-1128,N09-1041,0,0.0351669,"gories. Note that aspects are not specific to a topic; rather, they are associated with the category to which the topic belongs. A summary for a topic should cater to all the aspects of its associated template. Such guided summarization can be usefully applied to product opinion summarization, personalization of summaries for users, and improving user experience in question answering scenarios. Category Topic Topic Articles Articles Aspects Template Topic ................. Articles WHAT…. WHO….. ………… ............. Figure 1: How articles, topics, categories and aspects come together. Recently, Haghighi and Vanderwende (2009) proposed several content models for summarization. Their models find aspects within a topic which are subsequently combined using KL-divergence as a criterion for selecting relevant sentences. Conroy et al. (2010) augmented their CLASSY system with a query generation component that expands query terms for each aspect of the associated category by performing searches over Google, dictionaries, thesauri and authored world knowledge. Steinberger et al. (2010) generated guided summaries by framing the problem as an information extraction task. Aspect information extracted from an entity extractor"
C12-1128,W04-1013,0,0.0651584,"fety is shown in Table 2 as an example. Aspect WHAT WHO_AFFECTED HOW WHY COUNTERMEASURES Description what is the issue who are affected by the issue how are they affected why the health/safety issue occurs prevention efforts Table 2: Template of aspects for the Health and Safety category. Four human-written model summaries are provided per topic for each set. These summaries are used as a gold standard for evaluating machine generated summaries. Both automatic and manual measures were utilized by the TAC organizers to evaluate summaries. Automatic evaluation is commonly performed using ROUGE (Lin, 2004), and was used in TAC. ROUGE determines the quality of a summary through overlapping units such as n-grams, word sequences, and word pairs with human written summaries. Manual measures adopted by TAC organizers included pyramid scoring (Nenkova et al., 2007) and subjective assessments about the quality of the summaries. Since the original TAC manual evaluation team is not known or available, manual evaluation of new summarization systems is not possible. As such, we need a fair, objective comparison of our results with previously published results, and can only adopt automated methods. For thi"
C12-1128,N03-1020,0,0.125829,"ies. Manual measures adopted by TAC organizers included pyramid scoring (Nenkova et al., 2007) and subjective assessments about the quality of the summaries. Since the original TAC manual evaluation team is not known or available, manual evaluation of new summarization systems is not possible. As such, we need a fair, objective comparison of our results with previously published results, and can only adopt automated methods. For this reason, we adopt the automatic ROUGE-2 and ROUGE-SU4 measures. While not ideal, these measures have been found to generally correlate well with manual judgments (Lin and Hovy, 2003). 3 Methodology Our system, SWING, is a sentence-extractive summarizer that is designed to be an easy-to-use and an effective testbed for comparative evaluation of summarization methods. Input data is pre-processed using standard techniques, incorporating stop word removal and stemming for better computation of relevance. Our summarization system is fundamentally based on a supervised learning framework. A set of features is derived for each sentence in the input documents to measure their importance. We compute two classes of features, at the topic and category levels. We first discuss a set"
C12-1128,P08-2052,0,0.0144008,"hy information. Accordingly, the score of this feature is gradually decreased from the first sentence to the last sentence in a document based on its position. Sentence length is a binary feature that helps in avoiding noisy short text in the summary. The value of this feature is 1 if the length of sentence is at least 10, and zero otherwise. The value 10 is empirically determined in our system tuning. Interpolated N-gram Document Frequency (INDF) is an extended formulation of the popular document frequency (DF) measure. The efficacy of DF in summarization has been previously demonstrated by (Schilder and Kondadadi, 2008; Bysani et al., 2009). It computes the importance of a token as the ratio of the number of documents in which it occurred to the total number of documents within a topic. We extend the use of DF from unigrams to bigrams. INDF is the weighted linear combination of the DF for unigrams and bigrams of a sentence. Since bigrams encompass richer information and unigrams avoid problems with data sparseness, we choose a combination of both. The INDF of a sentence s, is computed as: P P α( wu ∈s DF (wu )) + (1 − α)( w b ∈s DF (w b )) I N DF (s) = |s| where wu are the unigram and w b are the bigram tok"
C12-1128,P06-1039,0,\N,Missing
C12-1129,W10-0701,0,0.0123159,"2009). However we are unsure if these results are applicable to a temporal dataset which is inherently complex (Setzer and Gaizauskas, 2000) and require a better understanding of the target language. To find out, we set up a crowdsourcing task on CrowdFlower. We chose CrowdFlower as our crowdsourcing platform, as it has access to a large user base (it uses Amazon Mechanical Turk to find workers), but adds an extra validation layer to attempt to address quality concerns, as this has been an issue in many applications of crowdsourcing in natural language annotation tasks (Mason and Watts, 2009; Callison-Burch and Dredze, 2010). 4.1 Task Setup Each annotation instance consists of a single sentence. We pre-process the sentence to highlight one event expression and one time expression found within it. Annotators were tasked to choose from five (OVERLAP, AFTER, BEFORE, NOT-RELATED, BAD-SENTENCE) possible temporal relationships between the marked event and time expression. An additional choice for BAD-SENTENCE was included to allow annotators to indicate if there had been problems with our automatic pre-processing. Such instances (67 in our study) were discarded. We required that at least 3 judgments be made for each an"
C12-1129,S10-1075,0,0.0466152,"in recent shared tasks in TempEval-1 (Verhagen et al., 2007) and TempEval-2 (Verhagen et al., 2010). In fact, the event-temporal relationship classification problem we focus on is exactly Task C in TempEval-2. Other temporal information extraction tasks include eventevent relationship classification, which involves determining which of two events took place earlier. Top performing teams attempting Task C made use of supervised machine-learning, including conditional random fields (Kolya et al., 2010), Markov logic (UzZaman and Allen, 2010; Ha et al., 2010), and maximum entropy classification (Derczynski and Gaizauskas, 2010). Enlarging the problem, Yoshikawa et al. (2009) proposed building an inference model which jointly predicts event-temporal, temporal-document creation time and event-event temporal 2111 relationships within a given sentence. While the system performed comparably to other systems in TempEval-1, such a joint model greatly enlarges the problem complexity as the search space is a Cartesian product of the three separate problems. All of the above systems employed similar features which we categorize into three feature types: 1) lexical cues, such as signal words and part-of-speech tags, 2) context"
C12-1129,S10-1076,0,0.0584417,"on temporal relationship classification, culminating in recent shared tasks in TempEval-1 (Verhagen et al., 2007) and TempEval-2 (Verhagen et al., 2010). In fact, the event-temporal relationship classification problem we focus on is exactly Task C in TempEval-2. Other temporal information extraction tasks include eventevent relationship classification, which involves determining which of two events took place earlier. Top performing teams attempting Task C made use of supervised machine-learning, including conditional random fields (Kolya et al., 2010), Markov logic (UzZaman and Allen, 2010; Ha et al., 2010), and maximum entropy classification (Derczynski and Gaizauskas, 2010). Enlarging the problem, Yoshikawa et al. (2009) proposed building an inference model which jointly predicts event-temporal, temporal-document creation time and event-event temporal 2111 relationships within a given sentence. While the system performed comparably to other systems in TempEval-1, such a joint model greatly enlarges the problem complexity as the search space is a Cartesian product of the three separate problems. All of the above systems employed similar features which we categorize into three feature types: 1)"
C12-1129,W09-1904,0,0.0353526,"le Table 2: Performance on TempEval-2 testing set. Results for TempEval-2 systems are cited from Verhagen et al. (2010). 4 Enlarging the Dataset through Crowdsourcing Our next proposal to solve the data sparsity problem is to enlarge the dataset available. Developing a large, suitably annotated dataset is expensive – both in terms of time and monetary cost. Recent work in natural language processing suggests that crowdsourcing annotations from the untrained public can provide annotated data at similar annotation quality as expert annotators, but for a fraction of the cost (Sheng et al., 2008; Hsueh et al., 2009). However we are unsure if these results are applicable to a temporal dataset which is inherently complex (Setzer and Gaizauskas, 2000) and require a better understanding of the target language. To find out, we set up a crowdsourcing task on CrowdFlower. We chose CrowdFlower as our crowdsourcing platform, as it has access to a large user base (it uses Amazon Mechanical Turk to find workers), but adds an extra validation layer to attempt to address quality concerns, as this has been an issue in many applications of crowdsourcing in natural language annotation tasks (Mason and Watts, 2009; Calli"
C12-1129,P03-1054,0,0.00356814,"rder O t on εs . O t is defined by arranging each w ∈ εs in ascending order of their respective distances from the time expression vertex. Distance in this case is defined as the number of edges that needs to be traversed to reach the time expression vertex. Imposing the total order O t on εs gives us a totally ordered set, i.e. (O t , εs ) = {e0 , e1 , . . .}. From this, for every input sentence s and its associated event (e) and time expressions (t), we can place the tuple < s, e, t > into a partition Pi , where i is the index of e within (O t , εs ). 4 We make use of Stanford dependencies (Klein and Manning, 2003). 2117 Referring to Sentence 1 as an illustration, we thus place <“left”, “Sunday”> in P0 because “left” is the nearest event expression to “Sunday” in the dependency parse in Figure 2. <“said”,“Sunday”> will be placed in P1 as “said” is the next nearest event expression. For convenience, we will also be referring to Pi as the set of Level-i instances. This partitioning scheme is premised on the intuition that it requires more effort to understand the temporal relationship between event and time expressions which are both structurally and semantically further away from each other. Higher level"
C12-1129,S10-1077,0,0.0924589,"ing. 2 Related Work There has been a good amount of research on temporal relationship classification, culminating in recent shared tasks in TempEval-1 (Verhagen et al., 2007) and TempEval-2 (Verhagen et al., 2010). In fact, the event-temporal relationship classification problem we focus on is exactly Task C in TempEval-2. Other temporal information extraction tasks include eventevent relationship classification, which involves determining which of two events took place earlier. Top performing teams attempting Task C made use of supervised machine-learning, including conditional random fields (Kolya et al., 2010), Markov logic (UzZaman and Allen, 2010; Ha et al., 2010), and maximum entropy classification (Derczynski and Gaizauskas, 2010). Enlarging the problem, Yoshikawa et al. (2009) proposed building an inference model which jointly predicts event-temporal, temporal-document creation time and event-event temporal 2111 relationships within a given sentence. While the system performed comparably to other systems in TempEval-1, such a joint model greatly enlarges the problem complexity as the search space is a Cartesian product of the three separate problems. All of the above systems employed similar f"
C12-1129,E06-1015,0,0.0275609,"n the other hand model this form of structure similarity well. Convolution tree kernels take as input tree structures and calculate a degree of similarity between the two trees. In its simplest form, similarity is computed by recursively counting the number of identical subtrees that appear in both input instances. With this structural similarity measure, we can do away with the need to “flatten” the structure with hand-devised representations. For these reasons, we use a support vector machine (SVM) as our supervised classification model, adopting a convolution kernel as its kernel function (Moschitti, 2006). A second issue is then to decide the type of parse tree to employ. Most previous work centered around the use of constituent grammar parses. In fact, the only other work that also adopts a convolution kernel approach to temporal relation classification (Mirroshandel et al., 2011) also uses constituent grammar parses. However, as constituent parses create internal phrasal nodes for every semantic constituent, such parse trees are often deep and overly detailed. Paths in such trees are fine-grained, capturing nuances (e.g., intervening finite verb phrase nodes), and as such, may not generalize"
C12-1129,W10-0719,0,0.0322676,"Missing"
C12-1129,S10-1062,0,0.0491298,"a good amount of research on temporal relationship classification, culminating in recent shared tasks in TempEval-1 (Verhagen et al., 2007) and TempEval-2 (Verhagen et al., 2010). In fact, the event-temporal relationship classification problem we focus on is exactly Task C in TempEval-2. Other temporal information extraction tasks include eventevent relationship classification, which involves determining which of two events took place earlier. Top performing teams attempting Task C made use of supervised machine-learning, including conditional random fields (Kolya et al., 2010), Markov logic (UzZaman and Allen, 2010; Ha et al., 2010), and maximum entropy classification (Derczynski and Gaizauskas, 2010). Enlarging the problem, Yoshikawa et al. (2009) proposed building an inference model which jointly predicts event-temporal, temporal-document creation time and event-event temporal 2111 relationships within a given sentence. While the system performed comparably to other systems in TempEval-1, such a joint model greatly enlarges the problem complexity as the search space is a Cartesian product of the three separate problems. All of the above systems employed similar features which we categorize into three"
C12-1129,S07-1014,0,0.228272,"Missing"
C12-1129,P09-1046,0,0.0403395,"07) and TempEval-2 (Verhagen et al., 2010). In fact, the event-temporal relationship classification problem we focus on is exactly Task C in TempEval-2. Other temporal information extraction tasks include eventevent relationship classification, which involves determining which of two events took place earlier. Top performing teams attempting Task C made use of supervised machine-learning, including conditional random fields (Kolya et al., 2010), Markov logic (UzZaman and Allen, 2010; Ha et al., 2010), and maximum entropy classification (Derczynski and Gaizauskas, 2010). Enlarging the problem, Yoshikawa et al. (2009) proposed building an inference model which jointly predicts event-temporal, temporal-document creation time and event-event temporal 2111 relationships within a given sentence. While the system performed comparably to other systems in TempEval-1, such a joint model greatly enlarges the problem complexity as the search space is a Cartesian product of the three separate problems. All of the above systems employed similar features which we categorize into three feature types: 1) lexical cues, such as signal words and part-of-speech tags, 2) context, including the attributes of the event and time"
C12-1129,S10-1010,0,\N,Missing
C18-1022,D08-1038,0,0.0275884,"target dataset. This results in our final embedded trend detection model – ETD (MGR), or “ETD-M” for short. 2 Prior Work Previous studies have addressed both of our tasks of obtaining keyphrases and identifying research fronts from scientific publications. We review this area first, then review mutually recursive algorithms used in practice. Research Trend Detection. Wang (2017) classified research trend detection models into two categories, namely, text- and bibliometric-based approaches. In text-based approaches, keywords and terms representing the core topics have been the focus of study. Hall et al. (2008) performed post-hoc analysis by applying the LDA model to the batch of publications in the same year; top keywords generated in different years are compared in an effort to identify the transition of topics. M¨orchen et al. (2008) attempts to combine keywords and the time of their appearances: the variation of term frequencies in two different timestamps provides the insights on how significant the term is becoming. Sessions within conferences are another perspective that have been investigated. Furukawa et al. (2014) constructed tf-idf vectors (Salton et al., 1983) from publications’ abstract"
C18-1022,D14-1181,0,0.00418789,"Missing"
C98-1108,W97-0703,0,0.0180665,"ormation extraction systems. For example, given the noun phrases fares and US Air that occur within a particular article, the reader will know what the story is about, i.e. fares and US Air. However, the reader will not know the [EVENT], i.e. what happened to the fares or to US Air. Did airfare prices rise, fall or stabilize? These are the verbs most typically applicable to prices, and which embody the event. 1.1 Focus on t h e N o u n Many natural language analysis systems focus on nouns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task p"
C98-1108,P94-1002,0,0.0227587,"Missing"
C98-1108,C94-2174,0,0.0144944,"a breaking news article would feature a higher percentage of motion verbs rather than verbs of commmfication. 1.3 On Genre Detection Verbs are an important factor in providing an event profile, which in turn might be used in categorizing articles into different genres, f l e m i n g to the literature in genre classification, Biber (1989) outlines five dimensions which can be used to characterize genre. Properties for distinguishing dimensions include verbal features such as tense, agentless passives and infinitives. Biber also refers to three verb classes: private, public, and suasive verbs. Karlgren and Cutting (1994) take a comtmtationally tractable set of these properties and use them to compute a score to recognize text genre using discriminant analysis. Tlm only verbal feature used in their study is present-tense verb count. As Karlgren and Cutting show, their techniques are effective in genre categorization, but, they do not claim to show how genres differ. Kessler et al. (1997) discuss some of the complexities in automatic detection of genre using a set of computationally efficient cues, such as punctuation, abbreviations, or presence of Latinate suffixes. The taxonomy of genres and facets developed"
C98-1108,P97-1005,0,0.07283,"used to characterize genre. Properties for distinguishing dimensions include verbal features such as tense, agentless passives and infinitives. Biber also refers to three verb classes: private, public, and suasive verbs. Karlgren and Cutting (1994) take a comtmtationally tractable set of these properties and use them to compute a score to recognize text genre using discriminant analysis. Tlm only verbal feature used in their study is present-tense verb count. As Karlgren and Cutting show, their techniques are effective in genre categorization, but, they do not claim to show how genres differ. Kessler et al. (1997) discuss some of the complexities in automatic detection of genre using a set of computationally efficient cues, such as punctuation, abbreviations, or presence of Latinate suffixes. The taxonomy of genres and facets developed in Kessler et al. is useful for a wide range of types, such as found in the Brown corpus. Although some of their discriminators could be useful for news articles (e.g. presence of second person pronoun tends to indicate a letter to the editor), the indicators do not appear to be directly applicable to a finer classification of&apos; news articles. News articles can be divided"
C98-1108,A97-1042,0,0.0142071,"example, given the noun phrases fares and US Air that occur within a particular article, the reader will know what the story is about, i.e. fares and US Air. However, the reader will not know the [EVENT], i.e. what happened to the fares or to US Air. Did airfare prices rise, fall or stabilize? These are the verbs most typically applicable to prices, and which embody the event. 1.1 Focus on t h e N o u n Many natural language analysis systems focus on nouns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task provides no information o"
C98-1108,M93-1022,0,0.0134553,"fares or to US Air. Did airfare prices rise, fall or stabilize? These are the verbs most typically applicable to prices, and which embody the event. 1.1 Focus on t h e N o u n Many natural language analysis systems focus on nouns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task provides no information on w h a t h a p p e n e d in the document, i.e. the e v e n t or a c t i o n . Less progress has been made on ways to utilize verbal information efficiently. In earlier systems with stemming, many of the verbal and nonfinal"
C98-1108,J91-1002,0,0.0246762,"n document type discrimination; we show how article types can be successfully classified within the news domain using verb semantic classes. 2 Verb T y p e communication support remainder and EVCA Since our first intuition of the data suggested that articles with a preponderance of verbs of 682 a certain semantic type might reveal aspects of document type, we tested the hypothesis that verbs could be used as a predictor in providing an event profile. We developed two algo~ rithms to: (1) explore WordNet (WN-Yerber) to cluster related verbs and build a set of verb chains in a document, much as Morris and Hirst (1991) used Roget&apos;s Thesaurus or like Hirst and St. Onge (1998) used WordNet to build noun chains; (2) classify verbs according to a semantic classification system, in this case, using Levin&apos;s (1993) English Verb Classes and Alternations (EVCA-Verber) as a basis. For source material, we used the manually-parsed Linguistic Data Consortium&apos;s Wall Street Journal (WSJ) corpus from which we extracted main and complement of communication verbs to test the algorithms on. Using WordNet. Our first technique was to use WordNet to build links between verbs and to provide a semantic profile of the document. Wor"
C98-1108,A97-1031,0,0.0126875,"ns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task provides no information on w h a t h a p p e n e d in the document, i.e. the e v e n t or a c t i o n . Less progress has been made on ways to utilize verbal information efficiently. In earlier systems with stemming, many of the verbal and nonfinal forms were conflated, sometimes erroneously. With the development of more sophisticated tools, such as part of speech taggers, more accurate verb phrase identification is possible. We present in this paper an effective way to utilize ve"
C98-1108,A97-1028,0,0.0387044,"Missing"
C98-1108,A97-1030,0,0.00771795,"tural language analysis systems focus on nouns and noun phrases in order to identify information on who, what, and where. For example, in summarization, Barzilay and Elhadad (1997) and Lin and Hovy (1997) focus oll multiword noun phrases. For information extraction tasks, such as the DARPA-sponsored Message Understanding Conferences (1992), only a few projects use verb phrases (events), e.g. Appelt et al. (1993), Lin (1993). In contrast, the named entity task, which identifies nouns and noun phrases, has generated numerous projects as evidenced by a host of papers in recent conferences, (e.g. Wacholder et al. 1997, Pahner and Day 1997, Neumann et al. 1997). Although rich infbrmation on nominal participants, actors, and other entities is provided, the nmned entity task provides no information on w h a t h a p p e n e d in the document, i.e. the e v e n t or a c t i o n . Less progress has been made on ways to utilize verbal information efficiently. In earlier systems with stemming, many of the verbal and nonfinal forms were conflated, sometimes erroneously. With the development of more sophisticated tools, such as part of speech taggers, more accurate verb phrase identification is possible. We present i"
C98-1108,W98-1123,1,\N,Missing
C98-1108,H94-1020,0,\N,Missing
councill-etal-2008-parscit,D07-1089,0,\N,Missing
councill-etal-2008-parscit,W06-1613,0,\N,Missing
councill-etal-2008-parscit,N04-1042,0,\N,Missing
D09-1036,de-marneffe-etal-2006-generating,0,0.007779,"Missing"
D09-1036,C08-2022,0,0.643572,"can assist in answering why questions. Detecting contrast and restatements is useful for paraphrasing and summarization systems. While different discourse frameworks have been proposed from different perspectives (Mann and Thompson, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1998; Webber, 2004), most admit these basic types of discourse relationships between textual units. When there is a discourse connective (e.g., because) between two text spans, it is often easy to recognize the relation between the spans, as most connectives are unambiguous (Miltsakaki et al., 2005; Pitler et al., 2008). On the other hand, it is difficult to recognize the discourse relations when there are no explicit textual cues. We term these cases explicit and implicit relations, respectively. 2 Related Work One of the first works that use statistical methods to detect implicit discourse relations is that of Marcu and Echihabi (2002). They showed that word pairs extracted from two text spans provide clues for detecting the discourse relation between 343 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 343–351, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP t"
D09-1036,P09-1077,0,0.586044,"ourse relations in the Discourse Graphbank (Wolf and Gibson, 2005). Their experiments show that discourse connectives and the distance between the two text spans have the most impact, and event-based features also contribute to the performance. However, their system may not work well for implicit relations alone, as the two most prominent features only apply to explicit relations: implicit relations do not have discourse connectives and the two text spans of an implicit relation are usually adjacent to each other. The work that is most related to ours is the forthcoming paper of Pitler et al. (2009) on implicit relation classification on the second version of the PDTB. They performed classification of implicit discourse relations using several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. 3 Overview of the Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is a discourse level annotation (Prasad et al., 2008) over the one million word Wall Street Journal corpus. The PDTB adopts the predicate-argument view of discourse relations, where a discourse connective (e.g., because"
D09-1036,W06-1617,1,0.216405,"f Example 4. Fully embedded argument: prev embedded in curr.Arg1 next embedded in curr.Arg2 curr embedded in prev.Arg2 curr embedded in next.Arg1 Shared argument: prev.Arg2 = curr.Arg1 curr.Arg2 = next.Arg1 S-TPC-1 NP-SBJ VP PRP Table 2: Six contextual features derived from two discourse dependency patterns. curr is the relation we want to classify. We VBD NP had ADVP NP Constituent Parse Features. Research work from other NLP areas, such as semantic role labeling, has shown that features derived from syntactic trees are useful in semantic understanding. Such features include syntactic paths (Jiang and Ng, 2006) and tree fragments (Moschitti, 2004). From our observation of the PDTB relations, syntactic structure within one argument may constrain the relation type and the syntactic structure of the other argument. For example, the constituent parse structure in Figure 2(a) usually signals an Asynchronous relation when it appears in Arg2, as shown in Example 3, while the structure in Figure 2(b) usually acts as a clue for a Cause relation when it appears in Arg1, as shown in Example 4. In both examples, the lexicalized parts of the parse structure are bolded. DT NN NNS no operating problems IN DT at al"
D09-1036,prasad-etal-2008-penn,0,0.73134,"urse connectives and the two text spans of an implicit relation are usually adjacent to each other. The work that is most related to ours is the forthcoming paper of Pitler et al. (2009) on implicit relation classification on the second version of the PDTB. They performed classification of implicit discourse relations using several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. 3 Overview of the Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is a discourse level annotation (Prasad et al., 2008) over the one million word Wall Street Journal corpus. The PDTB adopts the predicate-argument view of discourse relations, where a discourse connective (e.g., because) is treated as a predicate that takes two text spans as its arguments. The argument that the discourse connective structurally attaches to is called Arg2, and the other argument is called Arg1. The PDTB provides annotations for explicit and implicit discourse relations. By definition, an explicit relation contains an explicit discourse connective. In the PDTB, 100 explicit connectives are annotated. Example 1 shows an explicit Co"
D09-1036,N06-2034,0,0.0171112,"lation between 343 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 343–351, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP the text spans. They used a set of textual patterns to automatically construct a large corpus of text span pairs from the web. These text spans were assumed to be instances of specific discourse relations. They removed the discourse connectives from the pairs to form an implicit relation corpus. From this corpus, they collected word pair statistics, which were used in a Na¨ıve Bayes framework to classify discourse relations. Saito et al. (2006) extended this theme, to show that phrasal patterns extracted from a text span pair provide useful evidence in the relation classification. For example, the pattern “... should have done ...” usually signals a contrast. The authors combined word pairs with phrasal patterns, and conducted experiments with these two feature classes to recognize implicit relations between adjacent sentences in a Japanese corpus. Both of these previous works have the shortcoming of downgrading explicit relations to implicit ones by removing the explicit discourse connectives. While this is a good approach to autom"
D09-1036,P02-1047,0,0.487895,"most admit these basic types of discourse relationships between textual units. When there is a discourse connective (e.g., because) between two text spans, it is often easy to recognize the relation between the spans, as most connectives are unambiguous (Miltsakaki et al., 2005; Pitler et al., 2008). On the other hand, it is difficult to recognize the discourse relations when there are no explicit textual cues. We term these cases explicit and implicit relations, respectively. 2 Related Work One of the first works that use statistical methods to detect implicit discourse relations is that of Marcu and Echihabi (2002). They showed that word pairs extracted from two text spans provide clues for detecting the discourse relation between 343 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 343–351, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP the text spans. They used a set of textual patterns to automatically construct a large corpus of text span pairs from the web. These text spans were assumed to be instances of specific discourse relations. They removed the discourse connectives from the pairs to form an implicit relation corpus. From this corpus, they colle"
D09-1036,W06-1317,0,0.0957313,"entences in a Japanese corpus. Both of these previous works have the shortcoming of downgrading explicit relations to implicit ones by removing the explicit discourse connectives. While this is a good approach to automatically create large corpora, natively implicit relations may be signaled in different ways. The fact that explicit relations are explicitly signaled indicates that such relations need a cue to be unambiguous to human readers. Thus, such an artificial implicit relation corpus may exhibit marked differences from a natively implicit one. We validate this claim later in this work. Wellner et al. (2006) used multiple knowledge sources to produce syntactic and lexico-semantic features, which were then used to automatically identify and classify explicit and implicit discourse relations in the Discourse Graphbank (Wolf and Gibson, 2005). Their experiments show that discourse connectives and the distance between the two text spans have the most impact, and event-based features also contribute to the performance. However, their system may not work well for implicit relations alone, as the two most prominent features only apply to explicit relations: implicit relations do not have discourse conne"
D09-1036,W98-0301,0,0.105757,"Missing"
D09-1036,J05-2005,0,0.138204,"arge corpora, natively implicit relations may be signaled in different ways. The fact that explicit relations are explicitly signaled indicates that such relations need a cue to be unambiguous to human readers. Thus, such an artificial implicit relation corpus may exhibit marked differences from a natively implicit one. We validate this claim later in this work. Wellner et al. (2006) used multiple knowledge sources to produce syntactic and lexico-semantic features, which were then used to automatically identify and classify explicit and implicit discourse relations in the Discourse Graphbank (Wolf and Gibson, 2005). Their experiments show that discourse connectives and the distance between the two text spans have the most impact, and event-based features also contribute to the performance. However, their system may not work well for implicit relations alone, as the two most prominent features only apply to explicit relations: implicit relations do not have discourse connectives and the two text spans of an implicit relation are usually adjacent to each other. The work that is most related to ours is the forthcoming paper of Pitler et al. (2009) on implicit relation classification on the second version o"
D09-1036,P04-1043,0,0.00818605,"ev embedded in curr.Arg1 next embedded in curr.Arg2 curr embedded in prev.Arg2 curr embedded in next.Arg1 Shared argument: prev.Arg2 = curr.Arg1 curr.Arg2 = next.Arg1 S-TPC-1 NP-SBJ VP PRP Table 2: Six contextual features derived from two discourse dependency patterns. curr is the relation we want to classify. We VBD NP had ADVP NP Constituent Parse Features. Research work from other NLP areas, such as semantic role labeling, has shown that features derived from syntactic trees are useful in semantic understanding. Such features include syntactic paths (Jiang and Ng, 2006) and tree fragments (Moschitti, 2004). From our observation of the PDTB relations, syntactic structure within one argument may constrain the relation type and the syntactic structure of the other argument. For example, the constituent parse structure in Figure 2(a) usually signals an Asynchronous relation when it appears in Arg2, as shown in Example 3, while the structure in Figure 2(b) usually acts as a clue for a Cause relation when it appears in Arg1, as shown in Example 4. In both examples, the lexicalized parts of the parse structure are bolded. DT NN NNS no operating problems IN DT at all Figure 3: A gold standard subtree f"
D09-1036,miltsakaki-etal-2004-penn,0,\N,Missing
D09-1036,C04-1020,0,\N,Missing
D10-1015,P08-1087,0,0.380254,"ith a word-based language model. These systems, however, do not attempt to incorporate their analysis as part of the decoding process, but rather rely on models designed for word-token translation. We should also note the importance of the translation direction: it is much harder to translate from a morphologically poor to a morphologically rich language, where morphological distinctions not present in the source need to be generated in the target language. Research in translating into morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into"
D10-1015,P09-1106,0,0.0190146,"m . Second, we re-tokenize P Tw at the morpheme level, thus obtaining a new phrase table P Tw→m , which is of the same granularity as P Tm . Finally, we merge P Tw→m and P Tm , and we input the resulting phrase table to the decoder. Word Morpheme GIZA++ GIZA++ Word alignment Morpheme alignment Phrase Extrac""on Enriching the Translation Model Phrase Extrac""on PTw Another general strategy for combining evidence from the word-token and the morpheme-token representations is to build two separate SMT systems and then combine them. This can be done as a post-processing system combination step; see (Chen et al., 2009a) for an overview of such approaches. 3 We use the term “hypothesis” to collectively refer to the following (Koehn, 2003): the source phrase covered, the corresponding target phrase, and most importantly, a reference to the previous hypothesis that it extends. 151 PTm Morphological segmenta""on PTw→m PT merging Decoding Figure 3: Building a twin phrase table (PT). First, separate PTs are generated for different input granularities: word-token and morpheme-token. Second, the wordtoken PT is retokenized at the morpheme-token level. Finally, the two PTs are merged and used by the decoder. 4.2 Mer"
D10-1015,P05-1066,0,0.121879,"Missing"
D10-1015,W09-0430,0,0.0116041,"generated for different input granularities: word-token and morpheme-token. Second, the wordtoken PT is retokenized at the morpheme-token level. Finally, the two PTs are merged and used by the decoder. 4.2 Merging and Normalizing Phrase Tables Below we first describe the two general phrase table combination strategies used in previous work: (1) direct merging using additional feature functions, and (2) phrase table interpolation. We then introduce our approach. Add-feature methods. The first line of research on phrase table merging is exemplified by (Niehues et al., 2009; Chen et al., 2009b; Do et al., 2009; Nakov and Ng, 2009). The idea is to select one of the phrase tables as primary and to add to it all nonduplicating phrase pairs from the second table together with their associated scores. For each entry, features can be added to indicate its origin (whether from the primary or from the secondary table). Later in our experiments, we will refer to these baseline methods as add-1 and add-2, depending on how many additional features have been added. The values we used for these features in the baseline are given in Section 5.4; their weights in the log-linear model were set in the standard way"
D10-1015,H05-1085,0,0.182633,"lish to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments. 1 Introduction The fast progress of statistical machine translation (SMT) has boosted translation quality significantly. While research keeps diversifying, the word remains the atomic token-unit of translation. This is fine for languages with limited morphology like English and French, or no morphology at all like Chinese, but it is inadequate for morphologically rich languages like Arabic, Czech or Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). ∗ This research was sponsored in part by CSIDM (grant # 200805) and by a National Research Foundation grant entitled “Interactive Media Search” (grant # R-252-000-325-279). 2 Related Work Most previous work on morphology-aware approaches relies heavily on language-specific tools, e.g., the TreeTagger (Schmid, 1994) or the Buckwalter Arabic Morphological Analyzer (Buckwalter, 2004), which hampers their portability to other languages. Moreover, the prevalent method for incorporating morphological information is by heuristically-driven pre- or post-processing. For exa"
D10-1015,W09-0429,0,0.0117414,"er, do not attempt to incorporate their analysis as part of the decoding process, but rather rely on models designed for word-token translation. We should also note the importance of the translation direction: it is much harder to translate from a morphologically poor to a morphologically rich language, where morphological distinctions not present in the source need to be generated in the target language. Research in translating into morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into morphemes annotated with the following labels: PR"
D10-1015,D07-1091,0,0.33608,"igure 2. The word-token LM can capture much longer phrases and more complete contexts such as “, ep¨ademokraattisen maahanmuuttopolitiikan” compared to the morpheme-token LM. Note that scoring with two LMs that see the output sequence as different numbers of tokens is not readily offered by the existing SMT decoders. For example, the phrase-based model in Moses (Koehn et al., 2007) allows scoring with multiple LMs, but assumes they use the same token granularity, which is useful for LMs trained on different monolingual corpora, but cannot handle our case. While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. Note that scoring with twin LMs is conceptually superior to n-best re-scoring with a word-token LM, e.g., (Oflazer and El-Kahlout, 2007), since it is tightly integrated into decoding: it scores partial hypotheses and influenced the search process directly. 4 However, for phrase-based SMT systems, it is theoretically more appealing to combine their phrase tables since this allows the"
D10-1015,W05-0820,0,0.0288671,"t (f¯m ,¯ em ) into a word-token pair (f¯m→w ,¯ em→w ), and then induce a corresponding word alignment from the morpheme-token alignment of (f¯m ,¯ em ). We then estimate a lexicalized phrase score using the original formula given in (Koehn et al., 2003), where we plug this induced word alignment and word-token lexical translation probabilities estimated from the word-token dataset The case when (f¯w , e¯w ) is present in P Tw , but (f¯m , e¯m ) is not, is solved similarly. 5 Experiments and Evaluation 5.1 Datasets In our experiments, we use the English-Finnish data from the 2005 shared task (Koehn and Monz, 2005), which is split into training, development, and test portions; see Table 1 for details. We further split the training dataset into four subsets T1 , T2 , T3 , and T4 of sizes 40K, 80K, 160K, and 320K parallel sentence pairs, which we use for studying the impact of training data size on translation performance. Sent. Train Dev Test 714K 2K 2K Avg. words en fi 21.62 15.80 29.33 20.99 28.98 20.72 Avg. morph. en fi 24.68 26.15 33.40 34.94 33.10 34.47 Table 1: Dataset statistics. Shown are the number of parallel sentences, and the average number of words and Morfessor morphemes on the English and"
D10-1015,N03-1017,0,0.379639,"tput can be described by the following regular expression: WORD = ( PRE* STM SUF* )+ For example, uncarefully is analyzed as un/PRE+ care/STM+ ful/SUF+ ly/SUF The above token sequence forms the input to our system. We keep the PRE/STM/SUF tags as part of the tokens, and distinguish between care/STM+ and care/STM. Note also that the “+” sign is appended to each nonfinal tag so that we can distinguish word-internal from word-final morphemes. 3.2 Word Boundary-aware Phrase Extraction 3 Morphological Enhancements We present a morphologically-enhanced version of the classic phrase-based SMT model (Koehn et al., 2003). We use a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process. This is in contrast with previous work, where morphological enhancements are typically performed as pre-/postprocessing steps only. In addition to changing the basic translation token unit from a word to a morpheme, our model extends the phrase-based SMT model with the following: 1. word boundary-aware morpheme-level phrase extraction; 2. minimum error-rate training for a morphemelevel model using word-level BLEU; 3. joi"
D10-1015,P07-2045,0,0.0111836,"n LM, the morpheme-token sequence is concatenated into word-tokens before scoring. it can be enhanced with an appropriate word-token “view” on the partial morpheme-level hypotheses3 . The interaction of the twin LMs is illustrated in Figure 2. The word-token LM can capture much longer phrases and more complete contexts such as “, ep¨ademokraattisen maahanmuuttopolitiikan” compared to the morpheme-token LM. Note that scoring with two LMs that see the output sequence as different numbers of tokens is not readily offered by the existing SMT decoders. For example, the phrase-based model in Moses (Koehn et al., 2007) allows scoring with multiple LMs, but assumes they use the same token granularity, which is useful for LMs trained on different monolingual corpora, but cannot handle our case. While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. Note that scoring with twin LMs is conceptually superior to n-best re-scoring with a word-token LM, e.g., (Oflazer and El-Kahlout, 2007), since it i"
D10-1015,N04-4015,0,0.0175227,"tion on English to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments. 1 Introduction The fast progress of statistical machine translation (SMT) has boosted translation quality significantly. While research keeps diversifying, the word remains the atomic token-unit of translation. This is fine for languages with limited morphology like English and French, or no morphology at all like Chinese, but it is inadequate for morphologically rich languages like Arabic, Czech or Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). ∗ This research was sponsored in part by CSIDM (grant # 200805) and by a National Research Foundation grant entitled “Interactive Media Search” (grant # R-252-000-325-279). 2 Related Work Most previous work on morphology-aware approaches relies heavily on language-specific tools, e.g., the TreeTagger (Schmid, 1994) or the Buckwalter Arabic Morphological Analyzer (Buckwalter, 2004), which hampers their portability to other languages. Moreover, the prevalent method for incorporating morphological information is by heuristically-driven pr"
D10-1015,D09-1141,1,0.438212,"fferent input granularities: word-token and morpheme-token. Second, the wordtoken PT is retokenized at the morpheme-token level. Finally, the two PTs are merged and used by the decoder. 4.2 Merging and Normalizing Phrase Tables Below we first describe the two general phrase table combination strategies used in previous work: (1) direct merging using additional feature functions, and (2) phrase table interpolation. We then introduce our approach. Add-feature methods. The first line of research on phrase table merging is exemplified by (Niehues et al., 2009; Chen et al., 2009b; Do et al., 2009; Nakov and Ng, 2009). The idea is to select one of the phrase tables as primary and to add to it all nonduplicating phrase pairs from the second table together with their associated scores. For each entry, features can be added to indicate its origin (whether from the primary or from the secondary table). Later in our experiments, we will refer to these baseline methods as add-1 and add-2, depending on how many additional features have been added. The values we used for these features in the baseline are given in Section 5.4; their weights in the log-linear model were set in the standard way using MERT. Interpola"
D10-1015,W09-0413,0,0.0189108,"phrase table (PT). First, separate PTs are generated for different input granularities: word-token and morpheme-token. Second, the wordtoken PT is retokenized at the morpheme-token level. Finally, the two PTs are merged and used by the decoder. 4.2 Merging and Normalizing Phrase Tables Below we first describe the two general phrase table combination strategies used in previous work: (1) direct merging using additional feature functions, and (2) phrase table interpolation. We then introduce our approach. Add-feature methods. The first line of research on phrase table merging is exemplified by (Niehues et al., 2009; Chen et al., 2009b; Do et al., 2009; Nakov and Ng, 2009). The idea is to select one of the phrase tables as primary and to add to it all nonduplicating phrase pairs from the second table together with their associated scores. For each entry, features can be added to indicate its origin (whether from the primary or from the secondary table). Later in our experiments, we will refer to these baseline methods as add-1 and add-2, depending on how many additional features have been added. The values we used for these features in the baseline are given in Section 5.4; their weights in the log-linea"
D10-1015,J04-4002,0,0.0270322,"um error-rate training for a morphemelevel model using word-level BLEU; 3. joint scoring with morpheme- and word-level language models. We first introduce our morpheme-level representation, and then describe our enhancements. 1 Avramidis and Koehn (2008) improved by 0.15 BLEU over a 18.05 English-Greek baseline; Toutanova et al. (2008) improved by 0.72 BLEU over a 36.00 English-Russian baseline. 149 The core translation structure of a phrase-based SMT model is the phrase table, which is learned from a bilingual parallel sentence-aligned corpus, typically using the alignment template approach (Och and Ney, 2004). It contains a set of bilingual phrase pairs, each associated with five scores: forward and backward phrase translation probabilities, forward and backward lexicalized translation probabilities, and a constant phrase penalty. The maximum phrase length n is normally limited to seven words; higher values of n increase the table size exponentially without actually yielding performance benefit (Koehn et al., 2003). However, things are different when translating with morphemes, for two reasons: (1) morpheme-token phrases of length n can span less than n words; and (2) morphemetoken phrases may onl"
D10-1015,P03-1021,0,0.0375837,"i.e., morphemetoken phrases span a sequence of whole words. This is a fair extension of the morpheme-token system with respect to a word-token one since both are restricted to span up to n word-tokens. 3.3 Morpheme-Token MERT Optimizing Word-Token BLEU Modern phrase-based SMT systems use a log-linear model with the following typical feature functions: language model probabilities, word penalty, distortion cost, and the five parameters from the phrase table. Their weights are set by optimizing BLEU score (Papineni et al., 2001) directly using minimum error rate training (MERT), as suggested by Och (2003). In previous work, phrase-based SMT systems using morpheme-token input/output naturally per2 This means that we miss the opportunity to generate new wordforms for known baseforms, but removes the problem of proposing nonwords in the target language. 150 formed MERT at the morpheme-token level as well. This is not optimal since the final expected system output is a sequence of words, not morphemes. The main danger is that optimizing a morpheme-token BLEU score could lead to a suboptimal weight for the word penalty feature function: this is because the brevity penalty of BLEU is calculated with"
D10-1015,W07-0704,0,0.482548,"process, but rather rely on models designed for word-token translation. We should also note the importance of the translation direction: it is much harder to translate from a morphologically poor to a morphologically rich language, where morphological distinctions not present in the source need to be generated in the target language. Research in translating into morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into morphemes annotated with the following labels: PRE (prefix), STM (stem), SUF (suffix). Multiple prefixes and suffixes can be pr"
D10-1015,2001.mtsummit-papers.68,0,0.193817,"ong as they span n words or less. We further require that word boundaries be respected2 , i.e., morphemetoken phrases span a sequence of whole words. This is a fair extension of the morpheme-token system with respect to a word-token one since both are restricted to span up to n word-tokens. 3.3 Morpheme-Token MERT Optimizing Word-Token BLEU Modern phrase-based SMT systems use a log-linear model with the following typical feature functions: language model probabilities, word penalty, distortion cost, and the five parameters from the phrase table. Their weights are set by optimizing BLEU score (Papineni et al., 2001) directly using minimum error rate training (MERT), as suggested by Och (2003). In previous work, phrase-based SMT systems using morpheme-token input/output naturally per2 This means that we miss the opportunity to generate new wordforms for known baseforms, but removes the problem of proposing nonwords in the target language. 150 formed MERT at the morpheme-token level as well. This is not optimal since the final expected system output is a sequence of words, not morphemes. The main danger is that optimizing a morpheme-token BLEU score could lead to a suboptimal weight for the word penalty fe"
D10-1015,P06-1001,0,0.0483452,"and Kirchhoff, 2006). ∗ This research was sponsored in part by CSIDM (grant # 200805) and by a National Research Foundation grant entitled “Interactive Media Search” (grant # R-252-000-325-279). 2 Related Work Most previous work on morphology-aware approaches relies heavily on language-specific tools, e.g., the TreeTagger (Schmid, 1994) or the Buckwalter Arabic Morphological Analyzer (Buckwalter, 2004), which hampers their portability to other languages. Moreover, the prevalent method for incorporating morphological information is by heuristically-driven pre- or post-processing. For example, Sadat and Habash (2006) use different combinations of Arabic pre-processing schemes 148 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 148–157, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics for Arabic-English SMT, whereas Oflazer and ElKahlout (2007) post-processes Turkish morphemelevel translations by re-scoring n-best lists with a word-based language model. These systems, however, do not attempt to incorporate their analysis as part of the decoding process, but rather rely on models designed for word-token translation. We"
D10-1015,P08-1059,0,0.250483,"their analysis as part of the decoding process, but rather rely on models designed for word-token translation. We should also note the importance of the translation direction: it is much harder to translate from a morphologically poor to a morphologically rich language, where morphological distinctions not present in the source need to be generated in the target language. Research in translating into morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into morphemes annotated with the following labels: PRE (prefix), STM (stem), SUF (suffi"
D10-1015,2007.mtsummit-papers.65,0,0.36438,"morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into morphemes annotated with the following labels: PRE (prefix), STM (stem), SUF (suffix). Multiple prefixes and suffixes can be proposed for each word; word compounding is allowed as well. The output can be described by the following regular expression: WORD = ( PRE* STM SUF* )+ For example, uncarefully is analyzed as un/PRE+ care/STM+ ful/SUF+ ly/SUF The above token sequence forms the input to our system. We keep the PRE/STM/SUF tags as part of the tokens, and distinguish between car"
D10-1015,P07-1108,0,0.0204132,"e not normalized any more. Theoretically, this is not necessarily a problem since the log-linear model used by the decoder does not assume that the scores for the feature functions come from a normalized probability distribution. While it is possible to re-normalize the scores to convert them into probabilities, this is rarely done; it also does not solve the problem with the dropped scores for the duplicated phrases. Instead, the conditional probabilities in the two phrase tables are often interpolated directly, e.g., using linear interpolation. Representative work adopting this approach is (Wu and Wang, 2007). We refer to this method as interpolation. Our method. The above phrase merging approaches have been proposed for phrase tables derived from different sources. This is in contrast with our twin translation scenario, where the morphemetoken phrase tables are built from the same training dataset; the main difference being that word alignments and phrase extraction were performed at the 152 word-token level for P Tw→m and at the morphemetoken level for P Tm . Thus, we propose different merging approaches for the phrase translation probabilities and for the lexicalized probabilities. In phrase-ba"
D10-1015,E06-1006,0,0.0173467,"(714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments. 1 Introduction The fast progress of statistical machine translation (SMT) has boosted translation quality significantly. While research keeps diversifying, the word remains the atomic token-unit of translation. This is fine for languages with limited morphology like English and French, or no morphology at all like Chinese, but it is inadequate for morphologically rich languages like Arabic, Czech or Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). ∗ This research was sponsored in part by CSIDM (grant # 200805) and by a National Research Foundation grant entitled “Interactive Media Search” (grant # R-252-000-325-279). 2 Related Work Most previous work on morphology-aware approaches relies heavily on language-specific tools, e.g., the TreeTagger (Schmid, 1994) or the Buckwalter Arabic Morphological Analyzer (Buckwalter, 2004), which hampers their portability to other languages. Moreover, the prevalent method for incorporating morphological information is by heuristically-driven pre- or post-processing. For example, Sadat and Habash (200"
D10-1015,P02-1040,0,\N,Missing
D10-1015,W09-0401,0,\N,Missing
D10-1015,P08-2039,0,\N,Missing
D10-1015,W09-0428,0,\N,Missing
D13-1002,S07-1025,0,0.587881,"and show that these discourse features are superior. While we are just focusing on E-E temporal classification, our work can complement other approaches such as the joint inference approach proposed by Do et al. (2012) and Yoshikawa et al. (2009) which builds on top of event-timex (E-T) and E-E temporal classification systems. We believe that improvements to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CRF) based learner making use of similar features. Other researchers including Uzzaman and Allen (2010) and Ha et al. (2010) made use of Markov Logic Networks (MLN). By leveraging on the transitivity properties of temporal relationships (Setzer et al., 2003), they found that MLNs are useful in inferring new temporal relationships from known ones. Recognizing that the temporal r"
D13-1002,I11-1012,1,0.868561,"Missing"
D13-1002,W04-3205,0,0.0658355,"4) relative position of events in article, (1) (2) (3) (4) (5) System D O 2012 BASE BASE + R ST + P DTB + T OPIC S EG BASE + R ST + P DTB + T OPIC S EG + C OREF BASE + O-R ST + P DTB + O-T OPIC S EG + O-C OREF Precision 43.86 59.55 71.89 75.23 78.35 Recall 52.65 38.14 41.99 43.58 54.24 F1 47.46 46.50 53.01 55.19 64.10 Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive row is statistically significant, but a comparison is not possible between rows (1) and (2). 5) the number of sentences between the target events and 6) VerbOcean (Chklovski and Pantel, 2004) relations between events. This baseline system, and the subsequent systems we will describe, comprises of three separate one-vs-all classifiers for each of the temporal classes. The result obtained by our baseline is shown in Row 2 (i.e. BASE) in Table 2. We note that our baseline is competitive and performs similarly to the results obtained by Do et al. (2012). However as we do not have the raw judgements from Do’s system, we cannot test for statistical significance. We also implemented our proposed features and show the results obtained in the remaining rows of Table 2. In Row 3, R ST denot"
D13-1002,D12-1062,0,0.350012,"s happened before reports from the officials. Being able to infer these temporal relationships allows us to build up a better understanding of the text in question, and can aid several natural language understanding tasks such as information extraction and text summarization. For example, we can build up a temporal characterization of an article by constructing a temporal graph denoting the relationships between all events within an article (Verhagen et al., 2009). This can then be used to help construct an event timeline which layouts sequentially event mentions in the order they take place (Do et al., 2012). The temporal graph can also be used in text summarization, where temporal order can be used to improve sentence ordering and thereby the eventual generated summary (Barzilay et al., 2002). Given the importance and value of temporal relations, the community has organized shared tasks 1 From article AFP ENG 20030304.0250 of the ACE 2005 corpus (ACE, 2005). 12 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 12–23, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics to spur research efforts in this area, inc"
D13-1002,N13-1112,0,0.176803,"Missing"
D13-1002,P12-1007,0,0.02672,"eneralized. Convolution kernels had also previously been shown to work well for the related problem of E-T temporal classification (Ng and Kan, 2012), where the features adopted are similarly structural in nature. We now describe our use of the discourse analysis frameworks to generate appropriate representations for input to the convolution kernel. RST Discourse Framework. Recall that the RST framework provides us with a discourse tree for an entire input article. In recent years several automatic RST discourse parsers have been made available. In our work, we first make use of the parser by Feng and Hirst (2012) to obtain a discourse tree representation of our input. To represent the meaningful portion of the resultant tree, we encode path information between the two sentences of interest. We illustrate this procedure using the example discourse tree illustrated in Figure 3. EDUs including EDU 1 to EDU 3 form the vertices while discourse relations r1 and r2 between the EDUs form the edges. For a E-E pair, {A, B}, we can obtain a feature structure by first locating the EDUs within which A and B are found. A is found inside EDU 1 and B is found within EDU 3. We trace the shortest path between EDU 1 and"
D13-1002,S10-1076,0,0.0175359,"ments to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CRF) based learner making use of similar features. Other researchers including Uzzaman and Allen (2010) and Ha et al. (2010) made use of Markov Logic Networks (MLN). By leveraging on the transitivity properties of temporal relationships (Setzer et al., 2003), they found that MLNs are useful in inferring new temporal relationships from known ones. Recognizing that the temporal relationships between event pairs and time expressions are related, Yoshikawa et al. (2009) proposed the use of a joint inference model and showed that improvements in performance are obtained. However this gain is attributed to the joint inference model they had developed, making use of similar surface features. To the best of our knowledge,"
D13-1002,P94-1002,0,0.212361,"tence pairs. This is a problem when we need to do an article-wide analysis. The RST framework does not suffer from this limitation however as we can build up a discourse 15 tree connecting all the text within a given article. Topical Text Segmentation. A third complementary type of inter-sentential analysis is topical text segmentation. This form of segmentation separates a piece of text into non-overlapping segments, each of which can span several sentences. Each segment represents passages or topics, and provides a coarsegrained study of the linear structure of the text (Skorochod’Ko, 1972; Hearst, 1994). The transition between segments can represent possible topic shifts which can provide useful information about temporal relationships. Referring to Example 32 , we have delimited the different lines of text into segments with parentheses along with a subscript. Segment (1) talks about the casualty numbers seen at a medical centre, while Segment (2) provides background information that informs us a bomb explosion had taken place. The segment boundary signals to us a possible temporal shift and can help us to infer that the bombing event took place BEFORE the deaths and injuries had occurred."
D13-1002,D11-1026,0,0.0175528,"eans...”, and its discourse structure is shown in the bottom half of Figure 9. As with the previous example, the fragment suggests (correctly) that there should be a OVERLAP relationship for the “requested – said” event pair. [A] Milosevic and his wife wielded enormous power in Yugoslavia for more than a decade before he was swept out of power after a popular revolt in October 2000. [B] The court order was requested by Jack Welch’s attorney, Daniel K. Webb, who said Welch would likely be asked about his business dealings, his health and entries in his personal diary. (4) favoured over recall (Kazantseva and Szpakowicz, 2011, p. 292). As such there is just an average of between two to three identified segments per article. This makes the feature more generalizable despite making use of actual segment numbers. 2. The style of writing in newswire articles which we are experimenting on generally follows common journalistic guidelines. The semantics behind the transitions across the coarse-grained segments that were identified are thus likely to be of a similar nature across many different articles. temporal Milosevic … wielded… a decade temporal before.. swept out.. power after a… October 2000. manner-means The cour"
D13-1002,S10-1077,0,0.01603,"ach proposed by Do et al. (2012) and Yoshikawa et al. (2009) which builds on top of event-timex (E-T) and E-E temporal classification systems. We believe that improvements to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CRF) based learner making use of similar features. Other researchers including Uzzaman and Allen (2010) and Ha et al. (2010) made use of Markov Logic Networks (MLN). By leveraging on the transitivity properties of temporal relationships (Setzer et al., 2003), they found that MLNs are useful in inferring new temporal relationships from known ones. Recognizing that the temporal relationships between event pairs and time expressions are related, Yoshikawa et al. (2009) proposed the use of a joint inference model and showed that improvements in performance ar"
D13-1002,P06-1095,0,0.0720035,"ss based on sentence gap. Experiments. The work done in Do et al. (2012) is highly related to our experiments, and so we have reported the relevant results for local E-E classification in Row 1 of Table 2 as a reference. While largely comparable, note that a direct comparison is not possible because 1) the number of E-E instances we have is slightly different from what was reported, and 2) we do not have access to the exact partitions they have created for 5-fold cross-validation. As such, we have implemented a baseline adopting similar surface lexico-syntactic features used in previous work (Mani et al., 2006; Bethard and Martin, 2007; Ng and Kan, 2012; Do et al., 2012), including 1) part-of-speech tags, 2) tenses, 3) dependency parses, 4) relative position of events in article, (1) (2) (3) (4) (5) System D O 2012 BASE BASE + R ST + P DTB + T OPIC S EG BASE + R ST + P DTB + T OPIC S EG + C OREF BASE + O-R ST + P DTB + O-T OPIC S EG + O-C OREF Precision 43.86 59.55 71.89 75.23 78.35 Recall 52.65 38.14 41.99 43.58 54.24 F1 47.46 46.50 53.01 55.19 64.10 Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive row is statistically significant,"
D13-1002,W97-0713,0,0.112293,"Missing"
D13-1002,C12-1129,1,0.906833,"ropose the use of support vector machines (SVM), adopting a convolution kernel (Collins and Duffy, 2001) for its kernel function (Vapnik, 1999; Moschitti, 2006). The use of convolution kernels allows us to do away with the extensive feature engineering typically required to generate flat vectorized representations of features. This process is time consuming and demands specialized knowledge to achieve representations that are discriminative, yet are sufficiently generalized. Convolution kernels had also previously been shown to work well for the related problem of E-T temporal classification (Ng and Kan, 2012), where the features adopted are similarly structural in nature. We now describe our use of the discourse analysis frameworks to generate appropriate representations for input to the convolution kernel. RST Discourse Framework. Recall that the RST framework provides us with a discourse tree for an entire input article. In recent years several automatic RST discourse parsers have been made available. In our work, we first make use of the parser by Feng and Hirst (2012) to obtain a discourse tree representation of our input. To represent the meaningful portion of the resultant tree, we encode pa"
D13-1002,W10-2926,0,0.0157175,"Table 1. However we see that many of the relations do not follow this distribution. For example, we observe that several relations such as the RST “Condition” and PDTB “Cause” relations are almost exclusively found within AFTER and BEFORE event pairs only, while the RST “Manner-means” and PDTB “Synchrony” relations occur in a disproportionately large number of OVERLAP event pairs. These relations are likely useful in disambiguating between the different temporal classes. To verify this, we examine the convolution tree fragments that lie on the support vector of our SVM classifier. The work of Pighin and Moschitti (2010) 19 in linearizing kernel functions allows us to take a look at these tree fragments. Applying the linearization process leads to a different classifier from the one we have used. The identified tree fragments are therefore just an approximation to those actually employed by our classifier. However, this analysis still offers an introspection as to what relations are most influential for classification. B1 B2 B3 B4 B5 BEFORE (Temporal ... (Temporal (Elaboration ... (Condition (Explanation ... (Condition (Attribution ... (Elaboration (Bckgrnd ... O1 OVERLAP (Manner-means ... Table 4: Subset of"
D13-1002,prasad-etal-2008-penn,0,0.0380832,"they do not explain how sentences are combined together, and thus are unable to properly differentiate between the different temporal classifications. Supporting our argument is the work of Smith (2010), where she argued that syntax cannot fully account for the underlying semantics beneath surface text. D’Souza and Ng (2013) found out as much, and showed that adopting richer linguistic features such as lexical relations from curated dictionaries (e.g. Webster and WordNet) as well as discourse relations help temporal classification. They had shown that the Penn Discourse TreeBank (PDTB) style (Prasad et al., 2008) discourse relations are useful. We expand on their study to assess the utility of adopting additional discourse frameworks as alternative and complementary views. 3 Making Use of Discourse To highlight the deficiencies of surface features, we quote here an example from Lascarides and Asher (1993): [A] Max opened the door. The room was pitch dark. [B] Max switched off the light. The room was pitch dark. (2) The two lines of text A and B in Example 2 have similar syntactic structure. Given only syntactic features, we may be drawn to conclude that they share similar temporal relationships. Howev"
D13-1002,S10-1062,0,0.0190375,"tems. We believe that improvements to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CRF) based learner making use of similar features. Other researchers including Uzzaman and Allen (2010) and Ha et al. (2010) made use of Markov Logic Networks (MLN). By leveraging on the transitivity properties of temporal relationships (Setzer et al., 2003), they found that MLNs are useful in inferring new temporal relationships from known ones. Recognizing that the temporal relationships between event pairs and time expressions are related, Yoshikawa et al. (2009) proposed the use of a joint inference model and showed that improvements in performance are obtained. However this gain is attributed to the joint inference model they had developed, making use of similar surface features. To the be"
D13-1002,P09-1046,0,0.0211269,"n. That is, we want to be able to determine the temporal relationship between two events located anywhere within an article. The main contribution of our work is going beyond the surface lexical and syntactic features commonly adopted by existing state-of-the-art approaches. We suggest making use of semantically 13 motivated features derived from discourse analysis instead, and show that these discourse features are superior. While we are just focusing on E-E temporal classification, our work can complement other approaches such as the joint inference approach proposed by Do et al. (2012) and Yoshikawa et al. (2009) which builds on top of event-timex (E-T) and E-E temporal classification systems. We believe that improvements to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CR"
D13-1002,S10-1010,0,\N,Missing
D13-1073,bird-etal-2008-acl,1,0.862844,"he meaning of the sentence, we may well conclude that it is a definition sentence because of the cue phrase “is a”. But clearly, the whole sentence is not a definition sentence. More sophisticated features based on the sentence parse tree have to be exploited to detect such false positive examples. 5 Insights from the Definitions Extracted from the ACL ARC In this second half of the paper, we apply DefMiner to gain insights on the distributional and lexical properties of terms and definitions that appear in the large corpus of computational linguistics publications represented by the ACL ARC (Bird et al., 2008). The ARC consists of 10,921 scholarly publications from ACL venues, of which our earlier W00 corpus is a subset (n.b., as such, there is a small amount of overlap). We trained a model using the whole of the W00 corpus and used the obtained classifier to identify a list of terms and definitions for each publication in the ACL ARC. Inspecting such output gives us an understanding of the properties of definition components, eventually helping the community to define better features to capture them, as well as intrinsically deepening our knowledge of the natural language of definitions and the st"
D13-1073,W09-4405,0,0.644,"Missing"
D13-1073,cer-etal-2010-parsing,0,0.0256504,"Missing"
D13-1073,W06-2609,0,0.55565,"Missing"
D13-1073,muresan-klavans-2002-method,0,0.619418,"Missing"
D13-1073,P10-1134,0,0.487698,"ut word wi . We post-process our labeler’s results to achieve parity with the simplified definition sentence task: When we detect both a term’s and definition’s presence in a sentence, we deem the sentence a definition sentence. To be clear, this is a requirement; when we detect only either a term or a definition, we filter these out as false positives and do not include them as system output – by definition in DefMiner, terms must appear within the same sentence as their definitions. To train our classifier, we need a corpus of definition sentences where all terms and definitions are 1 While Navigli and Velardi (2010) tagged terms and definitions explicitly in their corpus, their evaluation restricts itself to the task of definition sentence identification. 782 annotated. While Navigli and Velardi (2010) compiled the WCL definition corpus from the English Wikipedia pages, we note that Wikipedia has stylistic conventions that make detection of definitions much easier than in the general case (i.e., “The first paragraph defines the topic with a neutral point of view”2 ). This makes it unsuitable for training a general extraction system from scholarly text. As such, we choose to construct our own dataset from"
D13-1073,W12-3206,0,0.630252,"onal Linguistics 2 Related Work The task of definition mining has attracted a fair amount of research interest. The output of such systems can be used to produce glossaries or answer definition questions. The primary model for this task in past work has been one of binary classification: does a sentence contain a definition or not? Existing methods can be cast into three main categories, namely rule-based (Muresan and Klavans, 2002; Westerhout and Monachesi, 2007), supervised machine learning (Fahmi and Bouma, 2006; Westerhout, 2009), and semi-supervised approaches (Navigli and Velardi, 2010; Reiplinger et al., 2012). Rule-based approaches are intuitive and efficient, and were adopted in early research. Here, system performance is largely governed by the quality of the rules. Muresan and Klavans (2002) developed a rule-based system to extract definitions from online medical articles. The system first selects candidates using hand-crafted cue-phrases (e.g. is defined as, is called; analogous to “IS-A” phrases), further filtering the candidates with grammar analysis. Westerhout and Monachesi (2007) augmented the set of rules with part-of-speech (POS) tag patterns, achieving an F2 of 0.43. While such manuall"
D13-1073,W09-4410,0,0.665463,"Missing"
D19-5227,N15-1078,0,0.095172,"can infer the corresponding sentiment and translation of the ambiguous word based on the given sentiment label. VSE achieves the overall highest performance across all metrics on the ambiguous test set. This suggests that learning different sentiment meanings of the ambiguous word by two separate embedding vectors is more effective than using a single embedding vector. Even in the contextual test set, VSE’s slight increase in precision, recall and F1 indicates that sentiment label helps translation even in the presence of context, with little impact on BLEU. Our results are also in line with (Salameh et al., 2015), which showed that sentiment from source sentences can be preserved by NMT. The slight decrease in BLEU scores when incorporating the sentiment labels may be caused by the fact that the trained sentiment classifier is not perfectly accurate and there are examples where the sentiment labels are wrongly annotated and hence affect the translation quality, although such cases are relatively rare and the impact is rather small. We illustrate some example translations, generated by our methods when given different source sentiment labels in Table 7, together with baseline Seq2Seq translations and r"
D19-5227,W10-4116,0,0.035914,"on sense embeddings from the document and integrating it into the NMT model. Their method improved lexical choice, especially for rare word senses, but did not improve the overall translation performance as measured by BLEU. Pu et al. (2018) incorporate weakly supervised word sense disambiguation into NMT to improve translation quality and accuracy of ambiguous words. However, these works focused on cases where there is only one correct sense for the source sentences. This differs from our goal, which is to tackle cases where both sentiments are correct interpretations of the source sentence. He et al. (2010) used machine translation to learn lexical prior knowledge of English sentiment lexicons and incorporated the prior knowledge into latent Dirichlet allocation (LDA), where sentiment labels are considered as topics for sentiment analysis. In contrast, our work incorporates lexical information from sentiment analysis directly into the NMT process. Sennrich et al. (2016) attempt to control politeness of the translations via incorporating side constraints. Similar to our approach, they also have a two-stage pipeline where they first automatically annotate the T–V distinction of the target sentence"
D19-5227,N16-1005,0,0.0412148,"words. However, these works focused on cases where there is only one correct sense for the source sentences. This differs from our goal, which is to tackle cases where both sentiments are correct interpretations of the source sentence. He et al. (2010) used machine translation to learn lexical prior knowledge of English sentiment lexicons and incorporated the prior knowledge into latent Dirichlet allocation (LDA), where sentiment labels are considered as topics for sentiment analysis. In contrast, our work incorporates lexical information from sentiment analysis directly into the NMT process. Sennrich et al. (2016) attempt to control politeness of the translations via incorporating side constraints. Similar to our approach, they also have a two-stage pipeline where they first automatically annotate the T–V distinction of the target sentences in the training set and then they add the annotations as special tokens at the end of the source text. The attentional encoder-decoder framework is then trained to learn to pay attention to the side constraints during training. However, there are several differences between our work and theirs: 1) instead of politeness, we control the sentiment of the translations;"
D19-5227,Q17-1024,0,0.128286,"Missing"
D19-5227,D13-1170,0,0.00626733,"the corresponding translation. 4 Experiments and Results We use the OpenNMT (Klein et al., 2017) implementation of the Seq2Seq model, consisting of a 2-layer LSTM with 500 hidden units for both encoder and decoder. We use the Adam optimizer with a learning rate 0.001, batch size 64 and train for 100K steps. This same setting is used for all the experiments in this paper. 4.1 Sentiment Analysis We experiment with English-to-Chinese translation, although our proposed methods also apply to other language pairs. For sentiment classification in English, we use binary movie review datasets: SST-2 (Socher et al., 2013) and IMDB (Maas et al., 2011), as well as the binary Yelp review dataset (Zhang et al., 2015) to train our sentiment classifier. The sentiment classifier is trained by fine-tuning the BERTLARGE (Devlin et al., 2018) model on the combined training set. 4.3 train 6.9K 560K 25K Contextual Test Set The above ambiguous corpus contains sentence pairs containing sentiment-bearing context within the sentences. We create a hold-out test set from that ambiguous corpus such that the test set has an equal number of sentences for each sentiment of each sentiment-ambiguous word. This contextual test set con"
D19-5227,P17-4012,0,0.0547638,"rallel text is necessary to ensure that there are sufficient examples for learning the rare sentiment. A total of 210K English–Chinese sentence pairs containing ambiguous words are extracted from three publicly available corpora: MultiUN (Eisele and Chen, 2010), TED (Cettolo et al., 2012) and AI Challenger.1 We annotate the sentiment the English source sentences of the resultant corpus with the trained sentiment classifier. This forms the ambiguous corpus for our sentiment-aware NMT. sired sentiment label to generate the corresponding translation. 4 Experiments and Results We use the OpenNMT (Klein et al., 2017) implementation of the Seq2Seq model, consisting of a 2-layer LSTM with 500 hidden units for both encoder and decoder. We use the Adam optimizer with a learning rate 0.001, batch size 64 and train for 100K steps. This same setting is used for all the experiments in this paper. 4.1 Sentiment Analysis We experiment with English-to-Chinese translation, although our proposed methods also apply to other language pairs. For sentiment classification in English, we use binary movie review datasets: SST-2 (Socher et al., 2013) and IMDB (Maas et al., 2011), as well as the binary Yelp review dataset (Zha"
D19-5227,N18-1121,0,0.0212499,"00 Proceedings of the 6th Workshop on Asian Translation, pages 200–206 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2 Related Work Original He is so proud that nobody likes him. AddLabel h neg i He is so proud that nobody likes him. InsertLabel He is so h neg i proud that nobody likes him. There are several previous attempts of incorporating knowledge from other NLP tasks into NMT. Early work incorporated word sense disambiguation (WSD) into existing machine translation pipelines (Chan et al., 2007; Carpuat and Wu, 2007; Vickrey et al., 2005). Recently, Liu et al. (2018) demonstrated that existing NMT systems have significant problems properly translating ambiguous words. They proposed to use WSD to enhance the system’s ability to capture contextual knowledge in translation. Their work showed improvement on sentences with contextual information, but this method does not apply to sentences which do not have strong contextual information. Rios et al. (2017) pass sense embeddings as additional input to NMT, extracting lexical chains based on sense embeddings from the document and integrating it into the NMT model. Their method improved lexical choice, especially"
D19-5227,H05-1097,0,0.0700866,"e author was an intern at I2 R. 200 Proceedings of the 6th Workshop on Asian Translation, pages 200–206 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2 Related Work Original He is so proud that nobody likes him. AddLabel h neg i He is so proud that nobody likes him. InsertLabel He is so h neg i proud that nobody likes him. There are several previous attempts of incorporating knowledge from other NLP tasks into NMT. Early work incorporated word sense disambiguation (WSD) into existing machine translation pipelines (Chan et al., 2007; Carpuat and Wu, 2007; Vickrey et al., 2005). Recently, Liu et al. (2018) demonstrated that existing NMT systems have significant problems properly translating ambiguous words. They proposed to use WSD to enhance the system’s ability to capture contextual knowledge in translation. Their work showed improvement on sentences with contextual information, but this method does not apply to sentences which do not have strong contextual information. Rios et al. (2017) pass sense embeddings as additional input to NMT, extracting lexical chains based on sense embeddings from the document and integrating it into the NMT model. Their method improv"
D19-5227,P19-1441,0,0.0153707,"ning sentence pairs in total. Furthermore, a development set of 3.9K sentence pairs is extracted from this corpus and excluded from the training. The sentiment analysis dataset statistics are shown in Table 2. We fine-tune BERTLARGE for the sentiment classifier with batch size 16, initial learning rate of 1e-5 and train for 16K steps. Dataset SST-2 Yelp IMDB Corpus with Sentiment Ambiguous Words test 1.8K 38K 25K Table 2: Sentiment Analysis Datasets. The performance of the trained classifier on the test sets are shown in Table 3. The BERTLARGE model achieves close to state-of-the-art results (Liu et al., 2019; Ruder and Howard, 2018). 1 202 Available at: https://challenger.ai/dataset/ectd2018 4.4 Ambiguous Test Set To examine the effectiveness of our proposed methods on achieving sentiment-aware translation, we manually construct an ambiguous test set. Sentences in this test set do not contain sentimentbearing context and can be interpreted in both sentiments. We ask two different bilingual annotators to write two different English sentences containing an ambiguous word for every word in our 110-word list. They were asked to write sentences that can be interpreted with both positive and negative v"
I08-1021,P06-2027,0,0.500206,"age had weakened by the time it passed over Japan’s capital, Tokyo, where it left little damage before moving out to sea. Figure 2: Contextual evidence of similarity. Curved lines indicate similar contexts, providing evidence that “land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion used is pattern overlap: for example, (Collier"
I08-1021,harabagiu-etal-2002-multidocument,0,0.0241081,"land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion used is pattern overlap: for example, (Collier, 1998; Harabagiu and Lacatusu, 2005) judge text spans to be similar if they have similar verbs and share the same verb arguments. Working with tree structures, Sudo et al. and Filatova et al. instead require shared subtrees. Calculating t"
I08-1021,W04-2013,0,0.0325754,"Missing"
I08-1021,W02-1006,0,0.0482409,"Missing"
I08-1021,P98-2127,0,0.060898,"Missing"
I08-1021,N04-1030,0,0.065701,"Missing"
I08-1021,W06-1603,1,0.86546,"h T [i] do for each cluster ccand , any candidate cluster that contextually related with tcontxt .cluster do P (T [i] ∈ ccand ) = comb(Ps , Pc ) likelihood = log(P (T [i] ∈ ccand )) if likelihood &gt; aBestLikelihood then aBestLikelihood = likelihood T [i].cluster = ccand tupleReassigned = true until tupleReassigned == f alse /*alignment stable*/ return During initialization, tuples whose pairwise similarity higher than a threshold τ are merged to form highly cohesive seed clusters. To compute a continuous similarity Sim(ta , tb ) of tuples ta and tb , we use the similarity measure described in (Qiu et al., 2006), which linearly combines similarities between the semantic roles shared by the two tuples. Some other tuples are related to these seed clusters by argument-similarity. These related tuples are temporarily put into a special “other” cluster. The cluster membership of these related tuples, together with those currently in the seed clusters, are to be further adjusted. The “other” cluster is so called because a tuple will end up being assigned to it if it is not found to be similar to any other tuple. Tuples that are neither similar to nor contextually related by argument-similarity to another t"
I08-1021,W98-1106,0,0.0448113,"nes indicate similar contexts, providing evidence that “land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion used is pattern overlap: for example, (Collier, 1998; Harabagiu and Lacatusu, 2005) judge text spans to be similar if they have similar verbs and share the same verb arguments. Working with tree structures, Sudo et al. and Filatov"
I08-1021,P03-1029,0,0.113039,"esday. .... But Tokage had weakened by the time it passed over Japan’s capital, Tokyo, where it left little damage before moving out to sea. Figure 2: Contextual evidence of similarity. Curved lines indicate similar contexts, providing evidence that “land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion used is pattern overla"
I08-1021,A00-1039,0,0.0242911,"er the storm hit on Wednesday. .... But Tokage had weakened by the time it passed over Japan’s capital, Tokyo, where it left little damage before moving out to sea. Figure 2: Contextual evidence of similarity. Curved lines indicate similar contexts, providing evidence that “land” and “hit” from two articles are semantically similar. action clustering accordingly. While the target application varies, most systems that need to group text spans by similarity measures are verb-centric. In addition to the verb, many systems expand their representation by including named entity tags (Collier, 1998; Yangarber et al., 2000; Sudo et al., 2003; Filatova et al., 2006), as well as restricting matches (using constraints on subtrees (Sudo et al., 2003; Filatova et al., 2006), predicate argument structures (Collier, 1998; Riloff and Schmelzenbach, 1998; Yangarber et al., 2000; Harabagiu and Maiorano, 2002) or semantic roles). Given these representations, systems then cluster similar text spans. To our knowledge, all current systems use a binary notion of similarity, in which pairs of spans are either similar or not. How they determine similarity is tightly coupled with their text span representation. One criterion use"
I08-1021,C98-2122,0,\N,Missing
I13-1015,P11-1041,0,0.015978,"ain. To our best knowledge, this is the first work to systematically explore the informal word phenomenon in Chinese microtext. By using a formal domain corpus, we introduce a method that effectively normalizes Chinese informal words through different, independent channels. 2 Related Work Previous works that address a similar task includes the study on abbreviations with their definitions (e.g., (Park and Byrd, 2001; Chang and Teng, 2006; Li and Yarowsky, 2008b)), abbreviations and acronyms in medical domain (Pakhomov, 2002), and transliteration (e.g., (Wu and Chang, 2007; Zhang et al., 2010; Bhargava and Kondrak, 2011)). These works dealt with such relations in formal text, but as we earlier argued, similar processing in the informal domain is quite different. Probably the most related work to our method is Li and Yarowsky (2008a)’s work. They tackle the problem of identifying informal–formal Chinese word pairs in the Web domain. They employ the Baidu1 search engine to obtain definition sentences – sentences that define or explain Chinese informal words with formal ones – from which the pairs are extracted and further ranked using a conditional log-linear model. Their method only works for definition senten"
I13-1015,W11-0704,0,0.0868259,"Missing"
I13-1015,D12-1039,0,0.159589,"Missing"
I13-1015,D08-1108,0,0.490125,"nformal words that have been defined on the web, relying heavily on the quality of Baidu’s index. In addition, the features they proposed are limited to rule-based features and ngram frequency, which does not permit their system to explain how the informal–formal word pair is related (i.e., derived by which channel). Normalizing informal words is another focus area in related work. An important channel for informal–formal mapping (as we review in detail later) is phonetic substitution. In work on Chinese, this is often done by measuring the Pinyin similarity 2 between an informal–formal pair. Li and Yarowsky (2008a) computed the Levenshtein distance (LD) on the Pinyin of the two words in 1 www.baidu.com Pinyin is the official phonetic system for transcribing the sound of Chinese characters into Latin script. P Y Sim(x, y) is used to denote the similarity between two Pinyin string “x” and “y” hereafter. 2 the pair to reflect the phonetic similarity. However, as a general string metric, LD does not capture the (dis-)similarity between two Pinyin pronunciations well as it is too coarse-grained. To overcome this shortcoming, Xia et al. (2008) propose a source channel model that is extended with phonetic ma"
I13-1015,P08-1049,0,0.224241,"nformal words that have been defined on the web, relying heavily on the quality of Baidu’s index. In addition, the features they proposed are limited to rule-based features and ngram frequency, which does not permit their system to explain how the informal–formal word pair is related (i.e., derived by which channel). Normalizing informal words is another focus area in related work. An important channel for informal–formal mapping (as we review in detail later) is phonetic substitution. In work on Chinese, this is often done by measuring the Pinyin similarity 2 between an informal–formal pair. Li and Yarowsky (2008a) computed the Levenshtein distance (LD) on the Pinyin of the two words in 1 www.baidu.com Pinyin is the official phonetic system for transcribing the sound of Chinese characters into Latin script. P Y Sim(x, y) is used to denote the similarity between two Pinyin string “x” and “y” hereafter. 2 the pair to reflect the phonetic similarity. However, as a general string metric, LD does not capture the (dis-)similarity between two Pinyin pronunciations well as it is too coarse-grained. To overcome this shortcoming, Xia et al. (2008) propose a source channel model that is extended with phonetic ma"
I13-1015,P02-1021,0,0.0206235,"d can be effectively adapted to tackle the synonym acquisition task in the formal domain. To our best knowledge, this is the first work to systematically explore the informal word phenomenon in Chinese microtext. By using a formal domain corpus, we introduce a method that effectively normalizes Chinese informal words through different, independent channels. 2 Related Work Previous works that address a similar task includes the study on abbreviations with their definitions (e.g., (Park and Byrd, 2001; Chang and Teng, 2006; Li and Yarowsky, 2008b)), abbreviations and acronyms in medical domain (Pakhomov, 2002), and transliteration (e.g., (Wu and Chang, 2007; Zhang et al., 2010; Bhargava and Kondrak, 2011)). These works dealt with such relations in formal text, but as we earlier argued, similar processing in the informal domain is quite different. Probably the most related work to our method is Li and Yarowsky (2008a)’s work. They tackle the problem of identifying informal–formal Chinese word pairs in the Web domain. They employ the Baidu1 search engine to obtain definition sentences – sentences that define or explain Chinese informal words with formal ones – from which the pairs are extracted and f"
I13-1015,W01-0516,0,0.115819,"Missing"
I13-1015,P11-1027,0,0.0231372,"Missing"
I13-1015,P13-1072,1,0.610199,"e several different equivalent normalizations T (here, T for target). This occurs in the abbreviation (L 8 as (L b or L ) 8 ) and paraphrase (Ù› ˆÒ or ˆ } or ‰) channels, where synonymous formal words are equivalent. In the case where an informal word is explanable as a phonetic substitution, only one formal form is viable. Our classification model caters for these multiple explanations. Figure 1 illustrates the framework of the proposed approach. Given an input Chinese microblog post, we first segment the sentences into words and recognize informal words leveraging the approach proposed in (Wang and Kan, 2013). For each recognized informal word O, we search the Chinese portion of the Google Web1T corpus using lexical patterns, obtaining n potential formal (normalized) candidates. Taking the informal word O, its occurrence context C(O), and the formal candidate T together, we generate feature vectors for each three-tuple, i.e., < O, C(O), T &gt;6 , consisting of both rule-based and statistical features. These features are used in a supervised binary classifier to render the final yes (informal– informal pair) or no (not an appropriate formal word explanation for the given informal word) decision. 4.1 P"
I13-1015,W12-2106,1,0.367906,"ieves significant improvement over the previous state-of-the-art. 3 Data Analysis To bootstrap our work, we analyzed sample Chinese microtext, hoping to gain insight on how informal words relate to their formal counterparts. To do this, we first needed to compile a corpus of microtext and annotate them. We utilized the Chinese social media archive, PrEV (Cui et al., 2012), to obtain Chinese microblog posts from the public timeline of Sina Weibo3 , the most popular Chinese microtext site with over half a billion users. To assemble a corpus for annotation, we first followed the convention from (Wang et al., 2012) to preprocess and label URLs, emoticons, “@usernames” and Hashtags as pre-defined words. We then employed 3 http://open.weibo.com Zhubajie4 , one of China’s largest crowdsourcing platforms to obtain third-party (i.e., not by the original author of the microtext) annotations for any informal words, as well as their normalization, sentiment and motivation for its use (Wang et al., 2010). Our coarse-grained sentiment annotations use the three categories of “positive”, “neutral” and “negative”. Motivation is likewise annotated with the seven categories listed in Table 1: to avoid (politically) se"
I13-1015,I05-3013,0,0.496623,"Missing"
I13-1015,P06-1125,0,0.776797,"Missing"
I13-1015,C10-2165,0,0.0185992,"sk in the formal domain. To our best knowledge, this is the first work to systematically explore the informal word phenomenon in Chinese microtext. By using a formal domain corpus, we introduce a method that effectively normalizes Chinese informal words through different, independent channels. 2 Related Work Previous works that address a similar task includes the study on abbreviations with their definitions (e.g., (Park and Byrd, 2001; Chang and Teng, 2006; Li and Yarowsky, 2008b)), abbreviations and acronyms in medical domain (Pakhomov, 2002), and transliteration (e.g., (Wu and Chang, 2007; Zhang et al., 2010; Bhargava and Kondrak, 2011)). These works dealt with such relations in formal text, but as we earlier argued, similar processing in the informal domain is quite different. Probably the most related work to our method is Li and Yarowsky (2008a)’s work. They tackle the problem of identifying informal–formal Chinese word pairs in the Web domain. They employ the Baidu1 search engine to obtain definition sentences – sentences that define or explain Chinese informal words with formal ones – from which the pairs are extracted and further ranked using a conditional log-linear model. Their method onl"
I13-1015,J90-2002,0,\N,Missing
kan-etal-2002-using,J98-3005,1,\N,Missing
kan-etal-2002-using,P96-1025,0,\N,Missing
N19-1182,S17-2097,0,0.0285639,"one of which is topic distribution. They consider the prominence of the tokens per topic as the surrogate for ranking, utilized for model training, by minimizing the difference in predicted and gold-standard rank between iterations. MIKE can be employed only when topic information is available, but unfortunately, does not generalize to the more common case where only gold-standard keyphrases are available for training. In Augenstein et al.(2017) benchmarking, stateof-the-art rich semantic embeddings deep learning and handcrafted feature–based statistical sequential labeling models used LSTM (Ammar et al., 2017) and CRF (Prasad and Kan, 2017) models respectively. Meng et al.(2017) uses an encoder–decoder model with a copy mechanism for keyphrase extraction (as a special case of 1838 Figure 1: Graph Convolution Model Architectures. Illustrations of the Graph Convolution Network (GCN, left), Graph Attention Network (GAT, center) and our proposed Glocal technique (right), centered on node h1 within its 1−hop neighborhood. TextRank on the complete graph is used to compute parameter βi . GAT parameterizes the edge weights based on gradients (αi , also represented by the differing edge widths). Our Glocal"
N19-1182,S17-2091,0,0.115209,"such important graph-based task is keyphrase extraction. In this task, individual words or phrases serve as graph nodes, and edges represent some form of co-occurrence. Keyphrase extraction has been extensively studied, in both supervised (classification) and unsupervised (ranking) modes. Depending on the length of the text and the final application of the task, solutions can be sample-based classification, pairwise ranking or sequential labeling. For example, Kim et al.(2010) explore the case of extracting top keyphrases from complete documents for downstream indexing, while Augenstein et al.(2017) connects its usage for knowledge base generation, aiming to extract all plausible keyphrases within a short excerpt. Treating a full-text scenario is arguably more challenging than the treatment of an excerpt scenario, as it requires the understanding of the much larger scale of text and extracting its most salient aspects. Traditional supervised models employ a host of hand-engineered features — tf.idf , candidate length, POS tags, sectional information, frequency, among others (Kim et al., 2013; Hasan and Ng, 2010) — trained with a wide range of classifiers. As they typically model the task"
N19-1182,D18-1439,0,0.0134313,"scenario of keyphrase extraction from long documents but only for short excerpts (namely, the abstract). This assumption reduces the complexity of the problem for sequential models that can effectively encode short text spans but may be ineffective on full-text. We suspect this is a current limitation of the encoder–decoder based models, which necessarily reduces the entire textual sequence into a single vector during the encoding stage – making it susceptible to the vanishing gradient and representation underfitting on large text. Further advances using the encoder–decoder framework such as (Chen et al., 2018) further explore the sequential modeling architectures by improving the attention mechanism with traditional features like title guidance. Note that many forms of such structural information — such as sectional information and citation graph co-occurrence — can enhance basic models, however without loss of generality, in this work we consider only text-based features for all the models. 3 as our baseline model. • For incorporating global importance (ranking), we use TextRank as our baseline model. We will first introduce the preliminaries: e.g., Graph Convolution Network (GCN), followed by the"
N19-1182,P17-1102,0,0.0881589,"eapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics tion among most plausible candidates. Unsupervised methods use co-occurrence as a signal for the labels. Under the hypothesis that keyphrase saliency is strongly correlated with repetition, graphical methods for unsupervised keyphrase extraction employ centrality measures and random walk techniques to rank prospective keyphrases (Mihalcea and Tarau, 2004). This hypothesis is widely exploited, with proposed extensions further enriching the graph by incorporating topic, section and/or position information (Florescu and Caragea, 2017b,a; Jiang et al., 2018), among other forms of side information. With these in mind, we make two important observations about the existing keyphrase extraction techniques: • In the supervised setting, word importance is captured in metrics and engineered features, as is local random walk scores. However, the structure of the graph formed by the text is not exploited. • In the unsupervised setting, most techniques do not tightly incorporate the rich semantic features common in the supervised setting. Furthermore, random walk scores are used as-is, without the capability of being finetuned by do"
N19-1182,C10-2042,0,0.0351736,"ng top keyphrases from complete documents for downstream indexing, while Augenstein et al.(2017) connects its usage for knowledge base generation, aiming to extract all plausible keyphrases within a short excerpt. Treating a full-text scenario is arguably more challenging than the treatment of an excerpt scenario, as it requires the understanding of the much larger scale of text and extracting its most salient aspects. Traditional supervised models employ a host of hand-engineered features — tf.idf , candidate length, POS tags, sectional information, frequency, among others (Kim et al., 2013; Hasan and Ng, 2010) — trained with a wide range of classifiers. As they typically model the task as a binary classification task (i.e., keyphrase, ¬keyphrase), they suffer severely from class imbalance as keyphrases are the excep1837 Proceedings of NAACL-HLT 2019, pages 1837–1846 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics tion among most plausible candidates. Unsupervised methods use co-occurrence as a signal for the labels. Under the hypothesis that keyphrase saliency is strongly correlated with repetition, graphical methods for unsupervised keyphrase extract"
N19-1182,C18-1022,1,0.797875,"ne 7, 2019. 2019 Association for Computational Linguistics tion among most plausible candidates. Unsupervised methods use co-occurrence as a signal for the labels. Under the hypothesis that keyphrase saliency is strongly correlated with repetition, graphical methods for unsupervised keyphrase extraction employ centrality measures and random walk techniques to rank prospective keyphrases (Mihalcea and Tarau, 2004). This hypothesis is widely exploited, with proposed extensions further enriching the graph by incorporating topic, section and/or position information (Florescu and Caragea, 2017b,a; Jiang et al., 2018), among other forms of side information. With these in mind, we make two important observations about the existing keyphrase extraction techniques: • In the supervised setting, word importance is captured in metrics and engineered features, as is local random walk scores. However, the structure of the graph formed by the text is not exploited. • In the unsupervised setting, most techniques do not tightly incorporate the rich semantic features common in the supervised setting. Furthermore, random walk scores are used as-is, without the capability of being finetuned by downstream supervision. Fr"
N19-1182,S10-1004,1,0.770433,"o keep the feature inventory constant; i.e. textual and statistical features. Closely related work discussed earlier — such as MIKE (Zhang et al., 2017) — are not directly comparable, since such works may use many other orthogonal features. Similarly, many supervised techniques use additional features. The best reported SemEval-2010 systems show very close performance to our proposed model, but they take advantages of other sources of side information, such as Wikipedia term acquisition, logical section information, bagging, and ensembles; hence, they are not directly comparable (reported in (Kim et al., 2010)). We argue that our main contribution is in the capacity of modeling; other features utilized in prior work can enhance Glocal’s performance and suitable incorporation is future work. In a similar vein, PositionRank (Florescu and Caragea, 2017b) enhances the random walk itself which can again act as a replacement for the TextRank in our model and is thus not strictly a comparable method. We argued earlier that the recently proposed supervised encoder–decoder based CopyRNN (Meng et al., 2017) deep learning models trained using word embedding do not scale well to the full-text setup. In their w"
N19-1182,P17-1054,0,0.0589445,"learning rate of 0.01 for 200 epochs using an early stopping strategy with patience set to 20 epochs. In both evaluation and training, as gold standard keyphrases have multiple tokens, we use each token of the gold keyphrase as the true label for each token. 3. Post-processing. This step reconstructs the multi-token keyphrase from the probability scores as generated by the Glocal model. This formation step then requires a re-ranking (calculating R(p)) of the resultant phrase as: X R(p) = len(p) ∗ r(wi ) (14) 1841 wi p Model tf.idf TextRank (Mihalcea and Tarau, 2004) RNN GRU CopyRNN CopyRNN† (Meng et al., 2017) GCN (Kipf and Welling, 2016) GAT (Veliˇckovi´c et al., 2018) Glocal F1 @5 11.3 10.2 3.2 3.8 5.8 29.3 16.7 25.2 30.7∗ Schutz F1 @10 13.7 12.4 3.6 3.2 6.2 30.2 17.8 28.1 30.3 F1 @15 15.2 14.9 4.0 3.9 7.5 32.2 19.2 29.3 33.9∗ F1 @5 6.9 7.6 2.6 3.1 6.6 26.2 19.2 21.1 24.7 Krapivin F1 @10 7.3 9.3 2.9 3.4 6.9 25.3 19.8 23.1 25.6 F1 @15 9.4 9.9 3.6 5.1 7.1 27.1 20.9 24.2 27.1 F1 @5 9.1 11.2 3.0 2.6 5.4 28.7 18.7 22.5 28.9 SemEval F1 @10 12.2 14.4 3.2 2.8 5.6 29.4 19.5 26.8 29.8 F1 @15 13.5 15.2 3.7 3.9 6.1 31.1 21.4 25.9 33.5∗ Table 2: Main comparative system evaluations on keyphrase extraction. All"
N19-1182,W04-3252,0,0.391053,"classification task (i.e., keyphrase, ¬keyphrase), they suffer severely from class imbalance as keyphrases are the excep1837 Proceedings of NAACL-HLT 2019, pages 1837–1846 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics tion among most plausible candidates. Unsupervised methods use co-occurrence as a signal for the labels. Under the hypothesis that keyphrase saliency is strongly correlated with repetition, graphical methods for unsupervised keyphrase extraction employ centrality measures and random walk techniques to rank prospective keyphrases (Mihalcea and Tarau, 2004). This hypothesis is widely exploited, with proposed extensions further enriching the graph by incorporating topic, section and/or position information (Florescu and Caragea, 2017b,a; Jiang et al., 2018), among other forms of side information. With these in mind, we make two important observations about the existing keyphrase extraction techniques: • In the supervised setting, word importance is captured in metrics and engineered features, as is local random walk scores. However, the structure of the graph formed by the text is not exploited. • In the unsupervised setting, most techniques do n"
N19-1182,D14-1162,0,0.103038,"Missing"
N19-1182,S17-2170,1,0.862346,"bution. They consider the prominence of the tokens per topic as the surrogate for ranking, utilized for model training, by minimizing the difference in predicted and gold-standard rank between iterations. MIKE can be employed only when topic information is available, but unfortunately, does not generalize to the more common case where only gold-standard keyphrases are available for training. In Augenstein et al.(2017) benchmarking, stateof-the-art rich semantic embeddings deep learning and handcrafted feature–based statistical sequential labeling models used LSTM (Ammar et al., 2017) and CRF (Prasad and Kan, 2017) models respectively. Meng et al.(2017) uses an encoder–decoder model with a copy mechanism for keyphrase extraction (as a special case of 1838 Figure 1: Graph Convolution Model Architectures. Illustrations of the Graph Convolution Network (GCN, left), Graph Attention Network (GAT, center) and our proposed Glocal technique (right), centered on node h1 within its 1−hop neighborhood. TextRank on the complete graph is used to compute parameter βi . GAT parameterizes the edge weights based on gradients (αi , also represented by the differing edge widths). Our Glocal technique adds the TextRank sco"
N19-1318,D14-1181,0,0.0065824,"al., 2016): a stack of two layers of LSTM encoders on the post text. 3. LSTM with Attention (Rockt¨aschel et al., 2016): an LSTM layer with hierarchical attention. 4. Answer Sentence Selection (Yu et al., 2014): a CNN model pioneered in a TREC QA11 task. 5. Our Model (Relevance based): only the relevance component of our model. 6. Our Model (Novelty based): only the novelty component of our model. We do not include traditional feature-based models as part of our reported baseline portfolio, as in our study, neural models have outperformed them as well, which is corroborated in recent studies (Kim, 2014). Additionally, such approaches are fragile, as we experiment with datasets from multiple domains with various discussion styles, and extracting hand crafted features for each is non-trivial and labour intensive. As a preliminary experiment, we tried with a traditional bagof-words based model. However, we do not include it in the baseline portfolio given its poor performance on our datasets. 4.4 Training We used the Keras12 library with TensorFlow as the backend for model implementation. We split the dataset 80:10:10 for train, validation, and test, respectively, and perform 5-fold cross valid"
N19-1318,D16-1176,0,0.148732,"e. Since our goal is to predict the helpful posts and the class distribution is inherently skewed from our definition, we evaluate the model performance in terms of prediction accuracy for only the positive, helpful class. We evaluate using standard precision, recall, and F1 score across all datasets. 4.3 Baselines Code for our model is publicly available10 to aid the reproduction of our results. We experiment with the following state-of-the-art neural text classification methods: 1. BiLSTM (Sun et al., 2017): a stack of two layers of Bidirectional LSTM encoders on post text. 2. Stacked LSTM (Liu et al., 2016): a stack of two layers of LSTM encoders on the post text. 3. LSTM with Attention (Rockt¨aschel et al., 2016): an LSTM layer with hierarchical attention. 4. Answer Sentence Selection (Yu et al., 2014): a CNN model pioneered in a TREC QA11 task. 5. Our Model (Relevance based): only the relevance component of our model. 6. Our Model (Novelty based): only the novelty component of our model. We do not include traditional feature-based models as part of our reported baseline portfolio, as in our study, neural models have outperformed them as well, which is corroborated in recent studies (Kim, 2014)"
N19-1318,H05-1014,0,0.0306382,"ping it from being applied to use cases where scalability and automation are key. In the CQA answer quality evaluation literature, quality is often measured through the human evaluators’ annotations during experimentation (Shah and Pomerantz, 2010; Oh et al., 2012; Omari et al., 2016). However, we are interested in modeling the “helpfulness” for actual discussion forum users (in term of “Upvotes”) and not annotators following guidelines to mark answer quality, which might present other forms of bias. Modeling Novelty in IR, such as search result diversification (Carbonell and Goldstein, 1998; Soboroff and Harman, 2005; Ziegler et al., 2005; Clarke et al., 2008), also constitutes prior art. Carbonell and Goldstein (1998) proposed maximal marginal relevance (MMR) to diversify the set of documents returned for a search query. Similar approaches were also used later in MultiDocument Summarization (MDS) tasks (Nallapati et al., 2017). These approaches address the problem either as a ranking task (ordering search results) or as a subset selection problem (MDS), where all documents are simultaneously made available. In contrast, in our discussion thread scenario, we need to model the discussion posts’ sequential"
N19-1318,P08-1082,0,0.0591684,"However, these techniques are tightly coupled with the target domain, and may not be generalizable to new domains. CQA Answer Quality: Past work has also addressed the evaluation of answer quality in CQA sites (Jeon et al., 2006; Hong and Davison, 2009; Shah and Pomerantz, 2010; Yao et al., 2015; Omari et al., 2016; Li et al., 2015). Typically posed as a classification problem, these use both textual and non-textual feature-based approaches. Since it is quite common for popular questions to attract many potential answers, answer ranking based on perceived quality is another line of approach (Surdeanu et al., 2008; Bian et al., 2008; Wang et al., 2009). Closer to our approach, Omari 5 https://www.nlm.nih.gov/research/ umls/ et al. (2016) proposed a novelty-based greedy ranking algorithm that depends on a pre-trained parser to identify different propositions, useful for predicting helpfulness. Li et al. (2015) propose a few features for answer quality detection from academic QA sites such as ResearchGate6 . However this approach does not generalize well since the method uses many website-specific signals such as reputation scores for users and their institutions. Additionally, their approach relies on h"
N19-1318,P15-2116,0,0.0840211,"Missing"
P07-1090,P05-1069,0,0.0476481,"Missing"
P07-1090,P05-1033,0,0.158491,"del (Nagata et al., 2006) and distortion model (Al-Onaizan and Papineni, 2006). However, these models are often fully lexicalized and sensitive to individual phrases. As a result, they are not robust to unseen phrases. A careful approximation is vital to avoid data sparseness. Proposals to alleviate this problem include utilizing bilingual phrase cluster or words at the phrase boundary (Nagata et al., 2006) as the phrase identity. The benefit of introducing lexical evidence without being fully lexicalized has been demonstrated by a recent state-of-the-art formally syntax-based model1 , Hiero (Chiang, 2005). Hiero performs phrase ordering by using linked non-terminal symbols in its synchronous CFG production rules coupled with lexical evidence. However, since it is difficult to specify a well-defined rule, Hiero has to rely on weak heuristics (i.e., length-based thresholds) to extract rules. As a result, Hiero produces grammars of enormous size. Watanabe et al. (2006) further reduces the grammar’s size by enforcing all rules to comply with Greibach Normal Form. Taking the lexicalization an intuitive a step forward, we propose a novel, finer-grained solution which models the content and context i"
P07-1090,J97-3002,0,0.911836,"nce pair. greatly reducing the computational overhead that arises when moving from phrase-based to syntaxbased approach. Furthermore, by modeling only high frequency words, we are able to obtain reliable statistics even in small datasets. Second, as opposed to Hiero, where phrase ordering is done implicitly alongside phrase translation and lexical weighting, we directly model the reordering process using orientation statistics. The FWS approach is also akin to (Xiong et al., 2006) in using a synchronous grammar as a reordering constraint. Instead of using Inversion Transduction Grammar (ITG) (Wu, 1997) directly, we will discuss an ITG extension to accommodate gapping. 3 Phrase Ordering around Function Words We use the following Chinese (c) to English (e) translation in Fig.1 as an illustration to conduct an inquiry to the problem. Note that the sentence translation requires some translations of English words to be ordered far from their original position in Chinese. Recovering the correct English ordering requires the inversion of the Chinese postpositional phrase, followed by the inversion of the first smaller noun phrase, and finally the inversion of the second larger noun phrase. Neverth"
P07-1090,P06-1066,0,0.0846159,"P (  ` ( `P  P (   `P ( ( ` ```  ( (((  P a form is a coll. of data entry fields on a page Figure 1: A Chinese-English sentence pair. greatly reducing the computational overhead that arises when moving from phrase-based to syntaxbased approach. Furthermore, by modeling only high frequency words, we are able to obtain reliable statistics even in small datasets. Second, as opposed to Hiero, where phrase ordering is done implicitly alongside phrase translation and lexical weighting, we directly model the reordering process using orientation statistics. The FWS approach is also akin to (Xiong et al., 2006) in using a synchronous grammar as a reordering constraint. Instead of using Inversion Transduction Grammar (ITG) (Wu, 1997) directly, we will discuss an ITG extension to accommodate gapping. 3 Phrase Ordering around Function Words We use the following Chinese (c) to English (e) translation in Fig.1 as an illustration to conduct an inquiry to the problem. Note that the sentence translation requires some translations of English words to be ordered far from their original position in Chinese. Recovering the correct English ordering requires the inversion of the Chinese postpositional phrase, fol"
P07-1090,J04-4002,0,0.666333,"ems use statistical knowledge obtained from corpora in favor of rich natural language knowledge. Instead of using syntactic knowledge to determine function words, we approximate this by equating the most frequent words as function words. By explicitly modeling phrase ordering around these frequent words, we aim to capture the most important and prevalent ordering productions. 2 Related Work A good translation should be both faithful with adequate lexical choice to the source language and fluent in its word ordering to the target language. In pursuit of better translation, phrase-based models (Och and Ney, 2004) have significantly improved the quality over classical word-based models (Brown et al., 1993). These multiword phrasal units contribute to fluency by inherently capturing intra-phrase reordering. However, despite this progress, interphrase reordering (especially long distance ones) still poses a great challenge to statistical machine translation (SMT). The basic phrase reordering model is a simple unlexicalized, context-insensitive distortion penalty model (Koehn et al., 2003). This model assumes little or no structural divergence between language pairs, preferring the original, translated or"
P07-1090,P06-1090,0,0.335556,"the 45th Annual Meeting of the Association of Computational Linguistics, pages 712–719, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics model, but is otherwise impoverished without any lexical evidence to characterize the reordering. To address this, lexicalized context-sensitive models incorporate contextual evidence. The local prediction model (Tillmann and Zhang, 2005) models structural divergence as the relative position between the translation of two neighboring phrases. Other further generalizations of orientation include the global prediction model (Nagata et al., 2006) and distortion model (Al-Onaizan and Papineni, 2006). However, these models are often fully lexicalized and sensitive to individual phrases. As a result, they are not robust to unseen phrases. A careful approximation is vital to avoid data sparseness. Proposals to alleviate this problem include utilizing bilingual phrase cluster or words at the phrase boundary (Nagata et al., 2006) as the phrase identity. The benefit of introducing lexical evidence without being fully lexicalized has been demonstrated by a recent state-of-the-art formally syntax-based model1 , Hiero (Chiang, 2005). Hiero perf"
P07-1090,J93-2003,0,0.00931495,"ge. Instead of using syntactic knowledge to determine function words, we approximate this by equating the most frequent words as function words. By explicitly modeling phrase ordering around these frequent words, we aim to capture the most important and prevalent ordering productions. 2 Related Work A good translation should be both faithful with adequate lexical choice to the source language and fluent in its word ordering to the target language. In pursuit of better translation, phrase-based models (Och and Ney, 2004) have significantly improved the quality over classical word-based models (Brown et al., 1993). These multiword phrasal units contribute to fluency by inherently capturing intra-phrase reordering. However, despite this progress, interphrase reordering (especially long distance ones) still poses a great challenge to statistical machine translation (SMT). The basic phrase reordering model is a simple unlexicalized, context-insensitive distortion penalty model (Koehn et al., 2003). This model assumes little or no structural divergence between language pairs, preferring the original, translated order by penalizing reordering. This simple model works well when properly coupled with a well-t"
P07-1090,N03-1017,0,0.139444,"Missing"
P07-1090,P03-1019,0,0.0261056,"ed. Step 2 reorders phrases according to knowledge embedded in function words. A new indexed symbol is introduced to indicate previously reordered phrases for conciseness. Step 3 finally maps Chinese phrases to their English translation. 4 The FWS Model We first discuss the extension of standard ITG to accommodate gapping and then detail the statistical components of the model later. 4.1 Single Gap ITG (SG-ITG) The FWS model employs a synchronous grammar to describe the admissible orderings. The utility of ITG as a reordering constraint for most language pairs, is well-known both empirically (Zens and Ney, 2003) and analytically (Wu, 1997), however ITG’s straight (monotone) and inverted (reverse) rules exhibit strong cohesiveness, which is inadequate to express orientations that require gaps. We propose SG-ITG that follows Wellington et al. (2006)’s suggestion to model at most one gap. We show the rules for SG-ITG below. Rules 13 are identical to those defined in standard ITG, in which monotone and reverse orderings are represented by square and angle brackets, respectively. Rank 1 2 3 4 5 6 7 8 9 10 21 37 - Word d d d d d d dd dd d d d d U unigram 0.0580 0.0507 0.0550 0.0155 0.0153 0.0138 0.0123 0.0"
P07-1090,P06-1098,0,0.0231154,"words at the phrase boundary (Nagata et al., 2006) as the phrase identity. The benefit of introducing lexical evidence without being fully lexicalized has been demonstrated by a recent state-of-the-art formally syntax-based model1 , Hiero (Chiang, 2005). Hiero performs phrase ordering by using linked non-terminal symbols in its synchronous CFG production rules coupled with lexical evidence. However, since it is difficult to specify a well-defined rule, Hiero has to rely on weak heuristics (i.e., length-based thresholds) to extract rules. As a result, Hiero produces grammars of enormous size. Watanabe et al. (2006) further reduces the grammar’s size by enforcing all rules to comply with Greibach Normal Form. Taking the lexicalization an intuitive a step forward, we propose a novel, finer-grained solution which models the content and context information encoded by function words - approximated by high frequency words. Inspired by the success of syntaxbased approaches, we propose a synchronous grammar that accommodates gapping production rules, while focusing on the statistical modeling in relation to function words. We refer to our approach as the Function Word-centered Syntax-based approach (FWS). Our F"
P07-1090,P06-2013,0,0.0611208,"Missing"
P07-1090,P06-1067,0,0.0305046,"of Computational Linguistics, pages 712–719, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics model, but is otherwise impoverished without any lexical evidence to characterize the reordering. To address this, lexicalized context-sensitive models incorporate contextual evidence. The local prediction model (Tillmann and Zhang, 2005) models structural divergence as the relative position between the translation of two neighboring phrases. Other further generalizations of orientation include the global prediction model (Nagata et al., 2006) and distortion model (Al-Onaizan and Papineni, 2006). However, these models are often fully lexicalized and sensitive to individual phrases. As a result, they are not robust to unseen phrases. A careful approximation is vital to avoid data sparseness. Proposals to alleviate this problem include utilizing bilingual phrase cluster or words at the phrase boundary (Nagata et al., 2006) as the phrase identity. The benefit of introducing lexical evidence without being fully lexicalized has been demonstrated by a recent state-of-the-art formally syntax-based model1 , Hiero (Chiang, 2005). Hiero performs phrase ordering by using linked non-terminal sym"
P07-1090,2004.tmi-1.9,0,0.109466,"Missing"
P07-1090,P06-1123,0,\N,Missing
P09-1037,D08-1024,1,0.851317,"o hierdriven syntactic modeling, we address this archical phrases that violate syntactic boundaries problem by observing the inuential role in the source language, and he explored the use of function words in determining syntacof a constituent feature intended to reward the tic structure, and introducing soft conapplication of hierarchical phrases which respect straints on function word relationships as source language syntactic categories. part of a standard log-linear hierarchithis did not yield signicant improvements, Marcal phrase-based model. Experimentation ton and Resnik (2008) and Chiang et al. (2008) on Chinese-English and Arabic-English extended this approach by introducing soft syntranslation demonstrates that the approach tactic constraints similar to the constituent feature, yields signicant gains in performance. but more ne-grained and sensitive to distinctions Although among syntactic categories; these led to substanIntroduction tial improvements in performance. Zollman et al. Hierarchical phrase-based models (Chiang, 2005; (2006) took a complementary approach, constrainChiang, 2007) offer a number of attractive beneing the application of hierarchical rules to respect ts in stati"
P09-1037,P08-1066,0,0.16649,"Missing"
P09-1037,J07-2003,0,0.343694,"gnicant improvements, Marcal phrase-based model. Experimentation ton and Resnik (2008) and Chiang et al. (2008) on Chinese-English and Arabic-English extended this approach by introducing soft syntranslation demonstrates that the approach tactic constraints similar to the constituent feature, yields signicant gains in performance. but more ne-grained and sensitive to distinctions Although among syntactic categories; these led to substanIntroduction tial improvements in performance. Zollman et al. Hierarchical phrase-based models (Chiang, 2005; (2006) took a complementary approach, constrainChiang, 2007) offer a number of attractive beneing the application of hierarchical rules to respect ts in statistical machine translation (SMT), while syntactic boundaries in the target language synmaintaining the strengths of phrase-based systems tax. Whether the focus is on constraints from the (Koehn et al., 2003). The most important of these source language or the target language, the main is the ability to model long-distance reordering efingredient in both previous approaches is the idea ciently. of constraining the spans of hierarchical phrases to To model such a reordering, a hierarrespect syntac"
P09-1037,N03-1017,0,0.133033,"Missing"
P09-1037,2003.mtsummit-papers.51,0,0.0330186,"s signicant gains in performance. but more ne-grained and sensitive to distinctions Although among syntactic categories; these led to substanIntroduction tial improvements in performance. Zollman et al. Hierarchical phrase-based models (Chiang, 2005; (2006) took a complementary approach, constrainChiang, 2007) offer a number of attractive beneing the application of hierarchical rules to respect ts in statistical machine translation (SMT), while syntactic boundaries in the target language synmaintaining the strengths of phrase-based systems tax. Whether the focus is on constraints from the (Koehn et al., 2003). The most important of these source language or the target language, the main is the ability to model long-distance reordering efingredient in both previous approaches is the idea ciently. of constraining the spans of hierarchical phrases to To model such a reordering, a hierarrespect syntactic boundaries. chical phrase-based system demands no additional parameters, since long and short distance reorderIn this paper, we pursue a different approach ings are modeled identically using synchronous to improving reordering choices in a hierarchical context free grammar (SCFG) rules. The same phras"
P09-1037,P02-1038,0,0.163241,"model, we used a 5- the baseline, but without further improvements gram model with modied Kneser-Ney smoothing over (Kneser and Ney, 1995) trained on the English side Arabic-to-English experiments. of our training data as well as portions of the Gigamarizes the results of our Arabic-to-English exword v2 English corpus. We used the NIST MT03 periments. This set of experiments shows a pattest set as the development set for optimizing intertern consistent with what we observed in Chinesepolation weights using minimum error rate trainto-English translation, again generally consistent ing (MERT; (Och and Ney, 2002)). We carried out across MT06 and MT08 test sets although modLarger values of N N = 128. Table 3 sumevaluation of the systems on the NIST 2006 evaleling a small number of lexical items (N uation test (MT06) and the NIST 2008 evaluation brings a marginal improvement over the baseline. test (MT08). We segmented Chinese as a preproIn addition, we again nd that the pairwise domcessing step using the Harbin segmenter (Zhao et inance model with al., 2001). signicant gain over the baseline in the MT06, Arabic-to-English experiments. We trained N = 128 = 32) produces the most although, interestingly"
P09-1037,J04-4002,0,0.110896,"r our running example. span of the hierarchical phrases, and (2) the span of a hierarchical phrase at a higher level is al6 ways a superset of the span of all other hierarchical Experimental Setup phrases at the lower level of its substructure. Thus, We tested the effect of introducing the pairwise to establish soft estimates of dominance counts, dominance model into hierarchical phrase-based we utilize alignment information available in the translation on Chinese-to-English and Arabic-torule together with the consistent alignment heurisEnglish translation tasks, thus studying its effect tic (Och and Ney, 2004) traditionally used to guess in two languages where the use of function words phrase alignments. differs signicantly. Following Setiawan et al. (2007), we identify function words as the Specically, we dene the span of a function N most word as a maximal, consistent alignment in the frequent words in the corpus, rather than identifysource language that either starts from or ends ing them according to linguistic criteria; this apwith the function word. (Requiring that spans be proximation removes the need for any additional maximal ensures their uniqueness.) language-specic resources. We wil"
P09-1037,E09-1044,0,\N,Missing
P09-1037,P02-1040,0,\N,Missing
P09-1037,W08-0403,0,\N,Missing
P09-1037,P08-1114,1,\N,Missing
P09-1037,P05-1033,0,\N,Missing
P09-1037,2008.iwslt-papers.7,0,\N,Missing
P09-1037,J97-3002,0,\N,Missing
P09-1037,P07-1090,1,\N,Missing
P09-1037,W06-3119,0,\N,Missing
P09-1037,W04-3250,0,\N,Missing
P11-1100,P05-1018,0,0.806228,"implement and validate our model on three data sets, which show robust improvements over the current state-of-the-art for coherence assessment. We also provide the first assessment of the upper-bound of human performance on the standard task of distinguishing coherent from incoherent orderings. To the best our knowledge, this is also the first study in which we show output from an automatic discourse parser helps in coherence modeling. 2 Related Work The study of coherence in discourse has led to many linguistic theories, of which we only discuss algorithms that have been reduced to practice. Barzilay and Lapata (2005; 2008) proposed an entity-based model to represent and assess local textual coherence. The model is motivated by Centering Theory (Grosz et al., 1995), which states that subsequent sentences in a locally coherent text are likely to continue to focus on the same entities as in previous sentences. Barzilay and Lapata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HM"
P11-1100,J08-1001,0,0.827349,"Missing"
P11-1100,N04-1015,0,0.311771,"ve been reduced to practice. Barzilay and Lapata (2005; 2008) proposed an entity-based model to represent and assess local textual coherence. The model is motivated by Centering Theory (Grosz et al., 1995), which states that subsequent sentences in a locally coherent text are likely to continue to focus on the same entities as in previous sentences. Barzilay and Lapata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. The global coherence of a text can then be summarized by the overall probability of topic shift from the first sentence to the last. Following these two directions, Soricut and Marcu (2006) and Elsner et al. (2007) combined the entity-based and HMM-based models and demonstrated that these two models are complementary to each other in coherence assessment. Our approach differs from these models in that it introduces and operationalizes another in"
P11-1100,N07-1055,0,0.765675,"ata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. The global coherence of a text can then be summarized by the overall probability of topic shift from the first sentence to the last. Following these two directions, Soricut and Marcu (2006) and Elsner et al. (2007) combined the entity-based and HMM-based models and demonstrated that these two models are complementary to each other in coherence assessment. Our approach differs from these models in that it introduces and operationalizes another indicator of discourse coherence, by modeling a text’s discourse relation transitions. Karamanis (2007) has tried to integrate local discourse relations into the Centering-based coherence metrics for the task of information ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller data set and the way the dis"
P11-1100,J95-2003,0,0.847669,"de the first assessment of the upper-bound of human performance on the standard task of distinguishing coherent from incoherent orderings. To the best our knowledge, this is also the first study in which we show output from an automatic discourse parser helps in coherence modeling. 2 Related Work The study of coherence in discourse has led to many linguistic theories, of which we only discuss algorithms that have been reduced to practice. Barzilay and Lapata (2005; 2008) proposed an entity-based model to represent and assess local textual coherence. The model is motivated by Centering Theory (Grosz et al., 1995), which states that subsequent sentences in a locally coherent text are likely to continue to focus on the same entities as in previous sentences. Barzilay and Lapata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. The global coherence of a text"
P11-1100,D09-1036,1,0.530866,"Missing"
P11-1100,J91-1002,0,0.753052,"ananea” in S3 participates in the second, third, and fourth relations). Given these discourse relations, building the matrix is straightforward: we note down the relations that a term Ti from a sentence Sj participates in, and record its discourse roles in the respective cell. We hypothesize that the sequence of discourse role transitions in a coherent text provides clues that distinguish it from an incoherent text. The discourse role matrix thus provides the foundation for computing such role transitions, on a per term basis. In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. The key differences from the traditional lexical chains are that our chain nodes’ entities are simplified (they share the same stemmed form, instead being connected by WordNet relations), but are further enriched by being typed with discourse relations. We compile the set of sub-sequences of discourse role transitions for every term in the matrix. These transitions tell us how the discourse role of a term varies through the progression of the text. For instance, “cananea” functions as Comp.Arg1 in S1 and Comp.Arg2 in S3 , and plays the role of Exp."
P11-1100,P09-2004,0,0.0265124,"tion ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we first apply automatic discourse parsing on the input text. While any discourse framework, su"
P11-1100,C08-2022,0,0.0581557,"ewording (i.e., if the two sentences are swapped), it produces an incoherent text (Marcu, 1996). In addition to the intra-relation ordering, such preferences also extend to inter-relation ordering: (2) [ The Constitution does not expressly give the president such power. ]S1 [ However, the president does have a duty not to violate the Constitution. ]S2 [ The question is whether his only means of defense is the veto. ]S3 The second sentence above provides a contrast to the previous sentence and an explanation for the next one. This pattern of Contrast-followed-by-Cause is rather common in text (Pitler et al., 2008). Ordering the three sentences differently results in incoherent, cryptic text. Thus coherent text exhibits measurable preferences for specific intra- and inter-discourse relation ordering. Our key idea is to use the converse of this phenomenon to assess the coherence of a text. In this paper, we detail our model to capture the coherence of a text based on the statistical distribution of the discourse structure and relations. Our method specifically focuses on the discourse relation transitions between adjacent sentences, modeling them in a discourse role matrix. Proceedings of the 49th Annual"
P11-1100,P09-1077,0,0.0380552,"r the task of information ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we first apply automatic discourse parsing on the input text. While a"
P11-1100,prasad-etal-2008-penn,0,0.100595,"to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we first apply automatic discourse parsing on the input text. While any discourse framework, such as the Rhetorical Structure Theory (RST), could be applied in our work to encode discourse in"
P11-1100,P06-2103,0,0.768546,"s sentences. Barzilay and Lapata operationalized Centering Theory by creating an entity grid model to capture discourse entity transitions at the sentence-to-sentence level, and demonstrated their model’s ability to discern coherent texts from incoherent ones. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. The global coherence of a text can then be summarized by the overall probability of topic shift from the first sentence to the last. Following these two directions, Soricut and Marcu (2006) and Elsner et al. (2007) combined the entity-based and HMM-based models and demonstrated that these two models are complementary to each other in coherence assessment. Our approach differs from these models in that it introduces and operationalizes another indicator of discourse coherence, by modeling a text’s discourse relation transitions. Karamanis (2007) has tried to integrate local discourse relations into the Centering-based coherence metrics for the task of information ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller dat"
P11-1100,P10-1073,0,0.0164132,"provement over the baseline method, which is partly due to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we first apply automatic discourse parsing on the input text. While any discourse framework, such as the Rhetorical Structure Theory"
P11-1100,D07-1010,0,0.0354183,"grate local discourse relations into the Centering-based coherence metrics for the task of information ordering, but was not able to obtain improvement over the baseline method, which is partly due to the much smaller data set and the way the discourse relation information is utilized in heuristic 998 constraints and rules. To implement our proposal, we need to identify the text’s discourse relations. This task, discourse parsing, has been a recent focus of study in the natural language processing (NLP) community, largely enabled by the availability of large-scale discourse annotated corpora (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Lin et al., 2009; Pitler et al., 2009; Pitler and Nenkova, 2009; Lin et al., 2010; Wang et al., 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicateargument approach (Webber, 2004). Crucially, the PDTB provides annotations not only on explicit (i.e., signaled by discourse connectives such as because) discourse relations, but also implicit (i.e., inferred by readers) ones. 3 Using Discourse Relations To utilize discourse relations of a text, we"
P12-1106,J08-1001,0,0.104986,"course Coherence Model First, a free text in Figure 2 is parsed by a discourse parser to derive its discourse relations, which are shown in Figure 3. Lin et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using such patterns to measure the coherence of a text can result in feature sparseness. To solve this problem, they expand the relation sequence into a discourse role matrix, as shown in Table 2. The matrix essentially captures term occurrences in the sentence-to-sentence relation sequences. This model is motivated by the entity-based model (Barzilay and Lapata, 2008) which captures sentence-to-sentence entity transitions. Next, the discourse role transition probabilities of lengths 2 and 3 (e.g., Temp.Arg1→Exp.Arg2 and Comp.Arg1→nil→Temp.Arg1) are calculated with respect to the matrix. For example, the probability of Comp.Arg2→Exp.Arg2 is 2/25 = 0.08 in Table 2. Lin et al. applied their model on the task of discerning an original text from a permuted ordering of its sentences. They modeled it as a pairwise ranking model (i.e., original vs. permuted), and trained a SVM preference ranking model with discourse role 1010 S# S1 S2 copper nil Comp.Arg2 Comp.Arg"
P12-1106,C08-1019,0,0.0618063,"BE and Pyramid, DUC and TAC also asked human judges to score every candidate summary with regard to its content, readability, and overall responsiveness. DUC and TAC defined linguistic quality to cover several aspects: grammaticality, non-redundancy, referential clarity, focus, and structure/coherence. Recently, Pitler et al. (2010) conducted experiments on various metrics designed to capture these aspects. Their experimental results on DUC 2006 and 2007 show that grammaticality can be measured by a set of syntactic features, while the last three aspects are best evaluated by local coherence. Conroy and Dang (2008) combined two manual linguistic scores – grammaticality and focus – with various ROUGE/BE metrics, and showed this helps better predict the responsiveness of the summarizers. Since 2009, TAC introduced the task of Automatically Evaluating Summaries of Peers (AESOP). AESOP 2009 and 2010 focused on two summary qualities: content and overall responsiveness. Summary content is measured by comparing the output of an automatic metric with the manual Pyramid score. Overall responsiveness measures a combination of content and linguistic quality. In AESOP 2011 (Owczarzak and Dang, 2011), automatic metr"
P12-1106,hovy-etal-2006-automated,0,0.31698,"n evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics. 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al., 2006; Zhou et al., 2006). However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from source articles. If no post-processing is performe"
P12-1106,W04-3250,0,0.0407039,"listic similarity score may be problematic when evaluating abstract-based systems, the experimental results support our choice of the similarity function. This reflects a major difference between MT and summarization evaluation: while MT systems always generate new sentences, most summarization systems focus on locating existing salient sentences. Like in TESLA, function words (words in closed POS categories, such as prepositions and articles) have their weights reduced by a factor of 0.1, thus placing more emphasis on the content words. We found this useful empirically. 3.3 Significance Test Koehn (2004) introduced a bootstrap resampling method to compute statistical significance of the difference between two machine translation systems with regard to the BLEU score. We adapt this method to compute the difference between two evaluation metrics in summarization: 1. Randomly choose n topics from the n given topics with replacement. 2. Summarize the topics with the list of machine summarizers. 3. Evaluate the list of summaries from Step 2 with the two evaluation metrics under comparison. 4. Determine which metric gives a higher correlation score. 5. Repeat Step 1 – 4 for 1,000 times. As we have"
P12-1106,N03-1020,0,0.621348,"hole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics. 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al., 2006; Zhou et al., 2006). However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from s"
P12-1106,P11-1100,1,0.673499,", 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from source articles. If no post-processing is performed to the generated summaries, the presentation of the extracted sentences may confuse readers. Knott (1996) argued that when the sentences of a text are randomly ordered, the text becomes difficult to understand, as its discourse structure is disturbed. Lin et al. (2011) validated this argument by using a trained model to differentiate an original text from a randomlyordered permutation of its sentences by looking at their discourse structures. This prior work leads us to believe that we can apply such discourse models to evaluate the readability of extract-based summaries. We will discuss the application of Lin et al.’s discourse coherence model to evaluate readability of machine generated summaries. We also introduce two new feature sources to enhance the model with hierarchical and Explicit/Non-Explicit information, and demonstrate that they improve the or"
P12-1106,W10-1754,1,0.90386,"machine generated summaries. We also introduce two new feature sources to enhance the model with hierarchical and Explicit/Non-Explicit information, and demonstrate that they improve the original model. There are parallels between evaluations of machine translation (MT) and summarization with respect to textual content. For instance, the widely used ROUGE (Lin and Hovy, 2003) metrics are influenced by BLEU (Papineni et al., 2002): both look at surface n-gram overlap for content coverage. Motivated by this, we will adapt a state-of-theart, linear programming-based MT evaluation metric, TESLA (Liu et al., 2010), to evaluate the content coverage of summaries. TAC’s overall responsiveness metric evaluates the 1006 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1006–1014, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics quality of a summary with regard to both its content and readability. Given this, we combine our two component coherence and content models into an SVM-trained regression model as our surrogate to overall responsiveness. Our experiments show that the coherence model significantly outperforms all AES"
P12-1106,N04-1019,0,0.305873,"SOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics. 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al., 2006; Zhou et al., 2006). However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from source articles. If no post-pro"
P12-1106,P02-1040,0,0.0970871,"can apply such discourse models to evaluate the readability of extract-based summaries. We will discuss the application of Lin et al.’s discourse coherence model to evaluate readability of machine generated summaries. We also introduce two new feature sources to enhance the model with hierarchical and Explicit/Non-Explicit information, and demonstrate that they improve the original model. There are parallels between evaluations of machine translation (MT) and summarization with respect to textual content. For instance, the widely used ROUGE (Lin and Hovy, 2003) metrics are influenced by BLEU (Papineni et al., 2002): both look at surface n-gram overlap for content coverage. Motivated by this, we will adapt a state-of-theart, linear programming-based MT evaluation metric, TESLA (Liu et al., 2010), to evaluate the content coverage of summaries. TAC’s overall responsiveness metric evaluates the 1006 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1006–1014, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics quality of a summary with regard to both its content and readability. Given this, we combine our two component cohere"
P12-1106,P10-1056,0,0.0439133,"Missing"
P12-1106,N06-1057,0,0.0115881,"ies on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics. 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al., 2006; Zhou et al., 2006). However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. Most of the state-of-the-art summarization systems (Ng et al., 2011; Zhang et al., 2011; Conroy et al., 2011) are extraction-based. They extract the most content-dense sentences from source articles. If no post-processing is performed to the generated s"
P12-1106,prasad-etal-2008-penn,0,\N,Missing
P13-1072,O98-3002,0,0.0219188,"Missing"
P13-1072,C08-1056,0,0.024301,"ge, microtext processing is difficult to process. One key reason for this difficulty is the ubiquitous presence of informal words – anomalous terms that manifest as ad hoc abbreviations, neologisms, unconventional spellings and phonetic substitutions. Such informality is often present in oral conversation, and user-generated microblogs reflect this informality. Natural language processing (NLP) tools largely fail to work properly on microtext, as they have largely been trained on formally written text (i.e., newswire). Recent work has started to address these shortcomings (Xia and Wong, 2006; Kobus et al., 2008; Han and Baldwin, 2011). Informal words and their usage in microtext evolves quickly, following social trends and news events. ∗ This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. 1 http://www.ictclas.org/index.html 731 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731–741, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Table 1: Our classification of Chinese informal words"
P13-1072,C02-1049,0,0.0283723,"Missing"
P13-1072,J90-1003,0,0.177928,"Missing"
P13-1072,D08-1108,0,0.167633,"complementary task. These experiments further verify the necessity and effectiveness of modeling the two tasks jointly, and point to the possibility of even better performance with improved per-task performance. Analyzing the classes of errors made by our system, we identify a promising future work topic to handle errors arising from partially observed informal words – where parts of a multi-character informal word have been observed before. We believe incorporating informal word normalization into the inference process may help address this important source of error. 6 More closely related, Li and Yarowsky (2008) tackle Chinese IWR. They bootstrap 500 informal/formal word pairs by using manually-tuned queries to find definition sentences on the Web. The resulting noisy list is further re-ranked based on n-gram co-occurrence. However, their method makes a basic assumption that informal/formal word pairs co-occur within a definition sentence (i.e., “&lt;informal word> means &lt;formal word>”) may not hold in microblog data, as microbloggers largely do not define the words they use. Conclusion Acknowledgments We would like to thank the anonymous reviewers for their valuable comments. We also appreciate the pro"
P13-1072,J05-4005,0,0.0343536,"s exemplified in Table 6; i.e., person names in the form of user IDs and nicknames, that have less constraint on form in terms of length, canonical structure (not surnames with given names; as is standard in Chinese names) and may mix alphabetic characters. Most of these belong to the category of Person Name (PER), as defined in CoNLL-200311 Named Entity Recognition shared task. Such freestyle entities are often misrecognized as informal words, as they share some of the same stylistic markings, and are not marked by features used to recognize previous Chinese named entity recognition methods (Gao et al., 2005; Zhao and Kit, 2008) that work on news or general domain text. We recognize this as a challenge in Chinese microtext, but beyond the scope of our current work. 5 Related Work In English, IWR has typically been investigated alongside normalization. Several recent works (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) aim to produce informal/formal word lexicons and mappings. These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling. These methods propose two-step unsupervised approaches to first detect and then"
P13-1072,I05-3025,0,0.0672483,"ì bs` eL L8 &apos; •g g † Ù›J ¦Ò Ò@ ƒ 2.2.1 Linear-Chain CRF A linear-chain CRF (LCRF; Figure 2a) predicts the output label based on feature functions provided by the scientist on the input. In fact, the LCRF has been used for the exact problem of CWS (Sun and Xu, 2011), garnering state-of-theart performance, and as such, validate it as a strong baseline for comparison. Problem Formalization The two tasks are simple to formalize. The IWR task labels each Chinese character with either an F (part of a formal word) or IF (informal word). For the CWS task, we follow the widely-used BIES coding scheme (Low et al., 2005; Hai et al., 2006), where B, I, E and S stand for beginning of a word, inside a word, end of a word and singlecharacter word, respectively. As a result, we have two (hidden) labels to associate with each (observable) character. Figure 1 illustrates an example microblog post graphically, where the labels are in circles and the observations are in squares. The two informal words in the example post are “( ” (normalized form: “¡ ”; English gloss: “no”) and “rp” (“ºÁ&lt;”; “luck”). 2.2.2 Factorial CRF To properly model the interplay between the two sub-problems, we employ the factorial CRF (FCRF) mo"
P13-1072,W11-0704,0,0.488416,"(PER), as defined in CoNLL-200311 Named Entity Recognition shared task. Such freestyle entities are often misrecognized as informal words, as they share some of the same stylistic markings, and are not marked by features used to recognize previous Chinese named entity recognition methods (Gao et al., 2005; Zhao and Kit, 2008) that work on news or general domain text. We recognize this as a challenge in Chinese microtext, but beyond the scope of our current work. 5 Related Work In English, IWR has typically been investigated alongside normalization. Several recent works (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) aim to produce informal/formal word lexicons and mappings. These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling. These methods propose two-step unsupervised approaches to first detect and then normalize detected informal words using dictionaries. In processing Chinese informal language, work conducted by Xia and Wong address the problem 11 http://www.cnts.ua.ac.be/conll2003/ ner/ 738 of in bulletin board system (BBS) chats. They employ pattern matching and SVM-based classification to recognize Chine"
P13-1072,C04-1081,0,0.148978,"Missing"
P13-1072,Y06-1012,0,0.0131166,"† Ù›J ¦Ò Ò@ ƒ 2.2.1 Linear-Chain CRF A linear-chain CRF (LCRF; Figure 2a) predicts the output label based on feature functions provided by the scientist on the input. In fact, the LCRF has been used for the exact problem of CWS (Sun and Xu, 2011), garnering state-of-theart performance, and as such, validate it as a strong baseline for comparison. Problem Formalization The two tasks are simple to formalize. The IWR task labels each Chinese character with either an F (part of a formal word) or IF (informal word). For the CWS task, we follow the widely-used BIES coding scheme (Low et al., 2005; Hai et al., 2006), where B, I, E and S stand for beginning of a word, inside a word, end of a word and singlecharacter word, respectively. As a result, we have two (hidden) labels to associate with each (observable) character. Figure 1 illustrates an example microblog post graphically, where the labels are in circles and the observations are in squares. The two informal words in the example post are “( ” (normalized form: “¡ ”; English gloss: “no”) and “rp” (“ºÁ&lt;”; “luck”). 2.2.2 Factorial CRF To properly model the interplay between the two sub-problems, we employ the factorial CRF (FCRF) model, which is based"
P13-1072,D11-1090,0,0.196881,"table framework for adapting to perform joint inference. Methodology Given an input Chinese microblog post, our method simultaneously segments the sentences into words (the Chinese Word Segmentation, CWS, task), and marks the component words as informal or formal ones (the Informal Word Recongition, IWR, task). 2.1 Example Sentence Ñ:( ( úßf wŠ†i i¸ ì bs` eL L8 &apos; •g g † Ù›J ¦Ò Ò@ ƒ 2.2.1 Linear-Chain CRF A linear-chain CRF (LCRF; Figure 2a) predicts the output label based on feature functions provided by the scientist on the input. In fact, the LCRF has been used for the exact problem of CWS (Sun and Xu, 2011), garnering state-of-theart performance, and as such, validate it as a strong baseline for comparison. Problem Formalization The two tasks are simple to formalize. The IWR task labels each Chinese character with either an F (part of a formal word) or IF (informal word). For the CWS task, we follow the widely-used BIES coding scheme (Low et al., 2005; Hai et al., 2006), where B, I, E and S stand for beginning of a word, inside a word, end of a word and singlecharacter word, respectively. As a result, we have two (hidden) labels to associate with each (observable) character. Figure 1 illustrates"
P13-1072,P12-1027,0,0.00917333,"Missing"
P13-1072,P11-1038,0,0.174245,"sing is difficult to process. One key reason for this difficulty is the ubiquitous presence of informal words – anomalous terms that manifest as ad hoc abbreviations, neologisms, unconventional spellings and phonetic substitutions. Such informality is often present in oral conversation, and user-generated microblogs reflect this informality. Natural language processing (NLP) tools largely fail to work properly on microtext, as they have largely been trained on formally written text (i.e., newswire). Recent work has started to address these shortcomings (Xia and Wong, 2006; Kobus et al., 2008; Han and Baldwin, 2011). Informal words and their usage in microtext evolves quickly, following social trends and news events. ∗ This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. 1 http://www.ictclas.org/index.html 731 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731–741, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Table 1: Our classification of Chinese informal words as originating from thr"
P13-1072,D12-1039,0,0.0798449,"Missing"
P13-1072,W12-2106,1,0.321355,"Missing"
P13-1072,W00-1207,0,0.0838607,"Missing"
P13-1072,I05-3013,0,0.613883,"recognition in Chinese microblogs. A key problem is the lack of word delimiters in Chinese. We exploit this reliance as an opportunity: recognizing the relation between informal word recognition and Chinese word segmentation, we propose to model the two tasks jointly. Our joint inference method significantly outperforms baseline systems that conduct the tasks individually or sequentially. 1 Introduction User generated content (UGC) – including microblogs, comments, SMS, chat and instant messaging – collectively referred to as microtext by Gouwset et al. (2011) or network informal language by Xia et al. (2005), is the hallmark of the participatory Web. While a rich source that many applications are interested in mining for knowledge, microtext processing is difficult to process. One key reason for this difficulty is the ubiquitous presence of informal words – anomalous terms that manifest as ad hoc abbreviations, neologisms, unconventional spellings and phonetic substitutions. Such informality is often present in oral conversation, and user-generated microblogs reflect this informality. Natural language processing (NLP) tools largely fail to work properly on microtext, as they have largely been tra"
P13-1072,O03-4002,0,0.0412838,"dicators in Chinese interrogative sentences. (e.g., “/ /” (“whether or not”), “} }” (“OK or not”)) • Whether Ck and Ck+1 are identical, for i − 4 &lt; k &lt; i + 3. This feature is used to capture the words of employing character doubling in Chinese. (e.g., “Ü Ü” (“see you”), “) )” (“every day”)) CRF Features We use three broad feature classes – lexical, dictionary-based and statistical features – aiming to distinguish the output classes for the CWS and IRW problems. Character-based sequence labeling is employed for word segmentation due to its simplicity and robustness to the unknown word problem (Xue, 2003). A key contribution of our work is also to propose novel features for joint inference. We Dictionary-based Features. We use features that indicate whether the input character sequence 2 For notational convenience, we denote a candidate character token Ci as having a context ...Ci−1 Ci Ci+1 .... We use Cm:n to express a subsequence starting at the position m and ending at n. len stands for the length of the subsequence, and of f set denotes the position offset of Cm:n from the current character Ci . We use b (beginning), m (middle) and e (ending) to indicate the position of Ck (m ≤ k ≤ n) with"
P13-1072,C00-2137,0,0.0208546,"Missing"
P13-1072,W03-1730,0,0.0140688,"es). We train the SVM classifier on the same set of features as the FCRF, by providing the cross-product of two layer labels as gold labels. This system (hereafter denoted as SVM-JC) was implemented using the LibSVM package (Chang and Lin, 2011). Baseline Systems We implemented several baseline systems to compare with proposed FCRF joint inference method. Existing Systems. We re-implemented Xia and Wong (2008)’s extended Support Vector Machine (SVM) based microtext IWR system to compare with our method. Their system only does IWR, using the CWS and POS tagging otuput of the ICTCLAS segmenter (Zhang et al., 2003) as input. To compare our joint inference versus other learning models, we also employed a decision tree (DT) learner, equipped with the same feature set as our FCRF. Both the SVM and DT models are provided by the Weka3 (Hall et al., 2009) toolkit, using its default configuration. To evaluate CWS performance, we compare with two recent segmenters. Sun and Xu (2011)’s 3.3 Evaluation Metrics We use the standard metrics of precision, recall and F1 for the IWR task. Only words that exactly match the manually-annotated labels are considered correct. For example given the sentence “ HË ËH} b” (“ HÙ"
P13-1072,I08-4017,0,0.0102241,"able 6; i.e., person names in the form of user IDs and nicknames, that have less constraint on form in terms of length, canonical structure (not surnames with given names; as is standard in Chinese names) and may mix alphabetic characters. Most of these belong to the category of Person Name (PER), as defined in CoNLL-200311 Named Entity Recognition shared task. Such freestyle entities are often misrecognized as informal words, as they share some of the same stylistic markings, and are not marked by features used to recognize previous Chinese named entity recognition methods (Gao et al., 2005; Zhao and Kit, 2008) that work on news or general domain text. We recognize this as a challenge in Chinese microtext, but beyond the scope of our current work. 5 Related Work In English, IWR has typically been investigated alongside normalization. Several recent works (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) aim to produce informal/formal word lexicons and mappings. These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling. These methods propose two-step unsupervised approaches to first detect and then normalize detected i"
P14-1087,W09-2812,0,0.0199543,"set. T is the proportion of events in s which happen in the same time span as another event in any other sentence in S. Two events are said to be in the same time span if one happens within the time period the other happens in. For example, an event that takes place in “2014 June” is said to take place within the year “2014”. While T IME MMR is proposed here as an improvement over MMR, the premise is that incorporating temporal information can be helpful to minimize redundancy in summaries. In future work, one could apply it to other state-of-the-art lexical-based approaches including that of Hendrickx et al. (2009) and Celikyilmaz and HakkaniTur (2010). We also believe the same idea can be transplanted even to non-lexical motivated techniques such as the corpus-based similarity measure proposed by Xie and Liu (2008). We chose to use MMR here as a proof-of-concept to demonstrate the viability of such a technique, and to easily integrate our work into SWING. (4) where |Es |is the number of events found in s, and |T Sn − T S1 |is the temporal coverage of s. 3.3 Enhancing MMR with TimeMMR In the sentence re-ordering stage of the SWING pipeline, the iterative MMR algorithm is used to adjust the score of a ca"
P14-1087,P99-1071,0,0.251602,"Missing"
P14-1087,N03-1020,0,0.28595,"lue are the timelines used. 4 Experiments and Results The proposed timeline features and T IME MMR were implemented on top of SWING, and evaluated on the test documents from TAC-2011 (Owczarzak and Dang, 2011). SWING makes use of three generic features and two features targeted specifically at guided summarization. Since the focus of this paper is on multi-document summarization, we employ only the three generic features, i.e., 1) sentence position, 2) sentence length, and 3) interpolated n-gram document frequency in our experiments below. Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003), as it has previously been shown to correlate well with human assessment (Lin, 2004) and is often used to evaluate automatic text summarization. The results obtained are shown in Table 1. In the table, each row refers to a specific summarization system configuration. We also show the results of two reference systems, CLASSY (Conroy et al., 2011) and POLYCOM (Zhang et al., 2011), as benchmarks. CLASSY and POLYCOM are top performing systems at TAC-2011 (ranked 2nd and 3rd by R-2 in TAC 2011, respectively; the full version of SWING was ranked 1st with a R-2 score of 0.1380). From these results,"
P14-1087,P10-1084,0,0.0422125,"Missing"
P14-1087,C12-1129,1,0.852519,"in the standardized T IME ML annotation (Pustejovsky et al., 2003a). An event refers to an eventuality, a situation that occurs or an action; while a timex is a reference to a particular date or time (e.g. “2013 December 31”). Following the “divide-and-conquer” approach described in Verhagen et al. (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain timelines (top half of Figure 3). We tap on existing systems for each of these steps (Ng and Kan, 2012; Str¨otgen and Gertz, 2013; Ng et al., 2013). Summarization. We make use of a state-ofthe-art summarization system, SWING (Ng et al., 2012) (bottom half of Figure 3). SWING is a supervised, extractive summarization system which ranks sentences based on scores computed using a set of features in the Sentence Scoring phase. The Maximal Marginal Relevance (MMR) algorithm is then used in the Sentence Re-ordering phase to re-order and select sentences to form the final summary. The timelines built in the earlier temporal processing can be incorporated into this pipeline by deriving a set of featur"
P14-1087,C12-1128,1,0.897655,"Missing"
P14-1087,D12-1062,0,0.0239019,"tion (TmpSum) track at the Text Retrieval Conference (Aslam et al., 2013). Given a large stream of data in real-time, the purpose of the TmpSum track is to look out for a query event, and retrieve specific details about the event over a period of time. Systems are also expected to identify the source sentences from which these details are retrieved. This is not the same as our approach here, which makes use of temporal information encoded in timelines to generate prose summaries. 3 well-understood constructs which have often been used to represent temporal information (Denis and Muller, 2011; Do et al., 2012). They indicate the temporal relationships between two basic temporal units: 1) events, and 2) time expressions (or timexes for short). In this work, we adopt the definitions proposed in the standardized T IME ML annotation (Pustejovsky et al., 2003a). An event refers to an eventuality, a situation that occurs or an action; while a timex is a reference to a particular date or time (e.g. “2013 December 31”). Following the “divide-and-conquer” approach described in Verhagen et al. (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationsh"
P14-1087,D13-1002,1,0.847029,"tejovsky et al., 2003a). An event refers to an eventuality, a situation that occurs or an action; while a timex is a reference to a particular date or time (e.g. “2013 December 31”). Following the “divide-and-conquer” approach described in Verhagen et al. (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain timelines (top half of Figure 3). We tap on existing systems for each of these steps (Ng and Kan, 2012; Str¨otgen and Gertz, 2013; Ng et al., 2013). Summarization. We make use of a state-ofthe-art summarization system, SWING (Ng et al., 2012) (bottom half of Figure 3). SWING is a supervised, extractive summarization system which ranks sentences based on scores computed using a set of features in the Sentence Scoring phase. The Maximal Marginal Relevance (MMR) algorithm is then used in the Sentence Re-ordering phase to re-order and select sentences to form the final summary. The timelines built in the earlier temporal processing can be incorporated into this pipeline by deriving a set of features used to score sentences in Sentence Scorin"
P14-1087,W00-0405,0,0.250071,"lone made landfall. (3) The storm matched one in 1991 that sparked a tidal wave that killed an estimated 138,000 people, Karmakar told AFP. Figure 1: Modified extract from a news article which describes a cyclone landfall. Several events which appear in Figure 2 are bolded. Storm in 1991 set of heuristics, and also made use of lexical patterns to perform basic time normalization on terms like “today” relative to the document creation time. The induced ordering is used to present the selected summary content, following the chronological order in the original documents. In another line of work, Goldstein et al. (2000) made use of the temporal ordering of documents to be summarized. In computing the relevance of a passage for inclusion into the final summary, they considered the recency of the passage’s source document. Passages from more recent documents are deemed to be more important. Wan (2007) and Demartini et al. (2010) made similar assumptions in their work on T IMED T EXT R ANK and entity summarization, respectively. Instead of just considering the notion of recency, Liu et al. (2009) proposed an interesting approach using a temporal graph. Events within a document set correspond to vertices in thei"
P14-1087,S13-2001,0,0.0227806,"Missing"
P14-1087,S10-1010,0,\N,Missing
P18-1133,P17-1045,0,0.0425788,"mplishment. They model the probability distribution of values over each slot (Lee, 2013). Dialogue policy makers then generate the next available system action. Recent experiments suggest that reinforcement learning is a promising paradigm to accomplish this task (Young et al., 1 http://github.com/WING-NUS/sequicity 2013a; Cuay´ahuitl et al., 2015; Liu and Lane, 2017), when state and action spaces are carefully designed (Young et al., 2010). Finally, in the response generation stage, pipeline designs usually pre-define fixed templates where placeholders are filled with slot values at runtime (Dhingra et al., 2017; Williams et al., 2017; Henderson et al., 2014b,a). However, this causes rather static responses that could lower user satisfaction. Generating a fluent, human-like response is considered a separate topic, typified by the topic of conversation systems (Li et al., 2015). 3 3.1 Preliminaries Encoder-Decoder Seq2seq Models Current seq2seq models adopt encoder–decoder structures. Given a source sequence of tokens X = x1 x2 ...xn , an encoder network represent X (x) (x) (x) as hidden states: H(x) = h1 h2 ...hn . Based (x) on H , a decoder network generates a target sequence of tokens Y = y1 y2 ..."
P18-1133,E17-2075,0,0.349759,".com. slot (See §3.2) which can suffer from high complexity, especially when the number of slots and their values grow. Since all the possible slot values have to be pre-defined as classification labels, such trackers also cannot handle the requests that have out-of-vocabulary (OOV) slot values. Moreover, the belief tracker requires delexicalization, i.e., replacing slot values with their slot names in utterances (Mrkˇsi´c et al., 2017). It does not scale well, due to the lexical diversity. The belief tracker also needs to be pre-trained, making the models unrealistic for end-to-end training (Eric and Manning, 2017a). While Eric and Manning (2017a,b) investigated building task-oriented dialogue systems by using a seq2seq model, unfortunately, their methods are rather preliminary and do not perform well in either task completion or response generation, due to their omission of a belief tracker. Questioning the basic pipeline architecture, in this paper, we re-examine the tenets of belief tracking in light of advances in deep learning. We introduce the concept of a belief span (bspan), a text span that tracks the belief states at each turn. This leads to a new framework, named Sequicity, with a single seq"
P18-1133,W17-5506,0,0.358609,".com. slot (See §3.2) which can suffer from high complexity, especially when the number of slots and their values grow. Since all the possible slot values have to be pre-defined as classification labels, such trackers also cannot handle the requests that have out-of-vocabulary (OOV) slot values. Moreover, the belief tracker requires delexicalization, i.e., replacing slot values with their slot names in utterances (Mrkˇsi´c et al., 2017). It does not scale well, due to the lexical diversity. The belief tracker also needs to be pre-trained, making the models unrealistic for end-to-end training (Eric and Manning, 2017a). While Eric and Manning (2017a,b) investigated building task-oriented dialogue systems by using a seq2seq model, unfortunately, their methods are rather preliminary and do not perform well in either task completion or response generation, due to their omission of a belief tracker. Questioning the basic pipeline architecture, in this paper, we re-examine the tenets of belief tracking in light of advances in deep learning. We introduce the concept of a belief span (bspan), a text span that tracks the belief states at each turn. This leads to a new framework, named Sequicity, with a single seq"
P18-1133,P17-1012,0,0.0142254,"zation-based belief tracker, Sequicity achieves much less train1437 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1437–1447 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing time, better performance on larger a dataset and an exceptional ability to handle OOV cases. Furthermore, Sequicity is a theoretically and aesthetically appealing framework, as it achieves true end-to-end trainability using only one seq2seq model. As such, Sequicity leverages the rapid development of seq2seq models (Gehring et al., 2017; Vaswani et al., 2017; Yu et al., 2017) in developing solutions to task-oriented dialogue scenarios. In our implementation, we improve on CopyNet (Gu et al., 2016) to instantiate Sequicity framework in this paper, as key words present in bspans and machine responses recur from previous utterances. Extensive experiments conducted on two benchmark datasets verify the effectiveness of our proposed method. Our contributions are fourfold: (1) We propose the Sequicity framework, which handles both task completion and response generation in a single seq2seq model; (2) We present an implementation of"
P18-1133,P16-1154,0,0.43609,"), pages 1437–1447 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing time, better performance on larger a dataset and an exceptional ability to handle OOV cases. Furthermore, Sequicity is a theoretically and aesthetically appealing framework, as it achieves true end-to-end trainability using only one seq2seq model. As such, Sequicity leverages the rapid development of seq2seq models (Gehring et al., 2017; Vaswani et al., 2017; Yu et al., 2017) in developing solutions to task-oriented dialogue scenarios. In our implementation, we improve on CopyNet (Gu et al., 2016) to instantiate Sequicity framework in this paper, as key words present in bspans and machine responses recur from previous utterances. Extensive experiments conducted on two benchmark datasets verify the effectiveness of our proposed method. Our contributions are fourfold: (1) We propose the Sequicity framework, which handles both task completion and response generation in a single seq2seq model; (2) We present an implementation of the Sequicity framework, called Two Stage CopyNet (TSCP), which has fewer number of parameters and trains faster than state-of-the-art baselines (Wen et al., 2017b"
P18-1133,W14-4337,0,0.366672,"e community to explore Sequicity1 . 2 Related Work Historically, task-oriented dialog systems have been built as pipelines of separately trained modules. A typical pipeline design contains four components: 1) a user intent classifier, 2) a belief tracker, 3) a dialogue policy maker and a 4) response generator. User intent detectors classify user utterances to into one of the pre-defined intents. SVM, CNN and RNN models (Silva et al., 2011; Hashemi et al., 2016; Shi et al., 2016) perform well for intent classification. Belief trackers, which keep track of user goals and constraints every turn (Henderson et al., 2014a,b; Kim et al., 2017) are the most important component for task accomplishment. They model the probability distribution of values over each slot (Lee, 2013). Dialogue policy makers then generate the next available system action. Recent experiments suggest that reinforcement learning is a promising paradigm to accomplish this task (Young et al., 1 http://github.com/WING-NUS/sequicity 2013a; Cuay´ahuitl et al., 2015; Liu and Lane, 2017), when state and action spaces are carefully designed (Young et al., 2010). Finally, in the response generation stage, pipeline designs usually pre-define fixed"
P18-1133,W13-4073,0,0.0276085,". (2)). The summed result ˜ (x) concatenates h(y) as a single vector which is h j j mapped into an output space for a sof tmax operation (Eq. (3)) to decode the current token: (x) uij = vT tanh(W1 hi n X euij P u h(x) i ij ie i=1 &quot; (x) # ˜ h j ) yj = sof tmax(O (y) hj ˜ (x) = h j (y) + W2 hj ) (1) (2) (3) where v ∈ R1×l ; W1 , W2 ∈ Rl×d and O ∈ R|V |×d . d is embedding size and V is vocabulary set and |V |is its size. 1438 3.2 Belief Trackers In the multi-turn scenarios, a belief tracker is the key component for task completion as it records key information from past turns (Wen et al., 2017b; Henderson et al., 2013, 2014a,b). Early belief trackers are designed as Bayesian networks where each node is a dialogue belief state (Paek and Horvitz, 2000; Young et al., 2013b). Recent work successfully represents belief trackers as discriminative classifiers (Henderson et al., 2013; Williams, 2012; Wen et al., 2017b). Wen et al. (2017b) apply discrimination approaches (Henderson et al., 2013) to build one classifier for each slot in their belief tracker. Following the terminology of (Wen et al., 2017b), a slot can be either informable or requestable, which have been annotated in CamRes676 and KVRET. Individually"
P18-1133,W13-4069,0,0.0211411,"ine design contains four components: 1) a user intent classifier, 2) a belief tracker, 3) a dialogue policy maker and a 4) response generator. User intent detectors classify user utterances to into one of the pre-defined intents. SVM, CNN and RNN models (Silva et al., 2011; Hashemi et al., 2016; Shi et al., 2016) perform well for intent classification. Belief trackers, which keep track of user goals and constraints every turn (Henderson et al., 2014a,b; Kim et al., 2017) are the most important component for task accomplishment. They model the probability distribution of values over each slot (Lee, 2013). Dialogue policy makers then generate the next available system action. Recent experiments suggest that reinforcement learning is a promising paradigm to accomplish this task (Young et al., 1 http://github.com/WING-NUS/sequicity 2013a; Cuay´ahuitl et al., 2015; Liu and Lane, 2017), when state and action spaces are carefully designed (Young et al., 2010). Finally, in the response generation stage, pipeline designs usually pre-define fixed templates where placeholders are filled with slot values at runtime (Dhingra et al., 2017; Williams et al., 2017; Henderson et al., 2014b,a). However, this c"
P18-1133,N16-1086,0,0.0131619,". Afterwards, the seq2seq model continues to the second decoding stage, where Rt is generated on the additional conditions of Bt and kt (Eq. 4b). Bt = seq2seq(Bt−1 Rt−1 Ut |0, 0) (4a) Rt = seq2seq(Bt−1 Rt−1 Ut |Bt , kt ) (4b) Sequicity is a general framework suitably implemented by any of the various seq2seq models. The additional modeling effort beyond a general seq2seq model is to add the conditioning on Bt and kt to decode the machine response Rt . Fortunately, natural language generation with specific conditions has been extensively studied (Wen et al., 2016b; Karpathy and Fei-Fei, 2015; Mei et al., 2016) which can be employed within this framework. 4.3 Sequicity Instantiation: A Two Stage CopyNet Although there are many possible instantiations, in this work we purposefully choose a simplistic architecture, leaving more sophisticated modeling for future work. We term our instantiated model a Two Stage CopyNet (TSCP). We denote the first m0 tokens of target sequence Y are Bt and the rests are Rt , i.e. Bt = y1 ...ym0 and Rt = ym0 +1 ...ym . Two-Stage CopyNet. We choose to improve upon CopyNet (Gu et al., 2016) as our seq2seq model. This is a natural choice as we observe that target sequence gen"
P18-1133,P17-1163,0,0.284923,"Missing"
P18-1133,P02-1040,0,0.1034,"j+1) + λ2 r(j+2) + ... + λm−j+1 r(m) . To encourage our generated response to answer the user requested information but avoid long-winded response, we set the reward at each step r(j) as follows: once the placeholder of requested slot has been decoded, the reward for current step is 1; otherwise, current step’s reward is -0.1. λ is a decay parameter. Sec 5.2 for λ settings. 5 Experiments We assess the effectiveness of Sequicity in three aspects: the task completion, the language quality, and the efficiency. The evaluation metrics are listed as follows: · BLEU to evaluate the language quality (Papineni et al., 2002) of generated responses (hence top-1 candidate in (Wen et al., 2017b)). · Entity match rate evaluates task completion. According to (Wen et al., 2017b), it determines if a system can generate all correct constraints to search the indicated entities of the user. This metric is either 0 or 1 for each dialogue. · Success F1 evaluates task completion and is modified from the success rate in (Wen et al., 2017b, 2016a, 2017a). The original success rate measures if the system answered all the requested information (e.g. address, phone number). However, this metric only evaluates recall. A system can"
P18-1133,N16-1176,0,0.0503755,"Missing"
P18-1133,D16-1233,0,0.340452,"ledge base search based on Bt , resulting in kt . Afterwards, the seq2seq model continues to the second decoding stage, where Rt is generated on the additional conditions of Bt and kt (Eq. 4b). Bt = seq2seq(Bt−1 Rt−1 Ut |0, 0) (4a) Rt = seq2seq(Bt−1 Rt−1 Ut |Bt , kt ) (4b) Sequicity is a general framework suitably implemented by any of the various seq2seq models. The additional modeling effort beyond a general seq2seq model is to add the conditioning on Bt and kt to decode the machine response Rt . Fortunately, natural language generation with specific conditions has been extensively studied (Wen et al., 2016b; Karpathy and Fei-Fei, 2015; Mei et al., 2016) which can be employed within this framework. 4.3 Sequicity Instantiation: A Two Stage CopyNet Although there are many possible instantiations, in this work we purposefully choose a simplistic architecture, leaving more sophisticated modeling for future work. We term our instantiated model a Two Stage CopyNet (TSCP). We denote the first m0 tokens of target sequence Y are Bt and the rests are Rt , i.e. Bt = y1 ...ym0 and Rt = ym0 +1 ...ym . Two-Stage CopyNet. We choose to improve upon CopyNet (Gu et al., 2016) as our seq2seq model. This is a natur"
P18-1133,E17-1042,0,0.0749915,"Based on this, we propose a simplistic Two Stage CopyNet instantiation which demonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by an order of magnitude. It significantly outperforms state-of-the-art pipeline-based methods on two datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail. 1 Introduction The challenge of achieving both task completion and human-like response generation for taskoriented dialogue systems is gaining research interest. Wen et al. (2017b, 2016a, 2017a) pioneered a set of models to address this challenge. Their proposed architectures follow traditional pipeline designs, where the belief tracking component is the key component (Chen et al., 2017). In the current paradigm, such a belief tracker builds a complex multi-class classifier for each * Work performed during an internship at Data Science Lab, JD.com. slot (See §3.2) which can suffer from high complexity, especially when the number of slots and their values grow. Since all the possible slot values have to be pre-defined as classification labels, such trackers also cannot"
P18-1133,W13-4065,0,0.175161,"wever, NDM employs a belief tracker which dominates its model size. The belief tracker is sensitive to the increase of distinct slot values because it employs complex structures to model each slot and corresponding values. Here, we only perform empirical evaluation, leaving theoretically complexity analysis for future works. 5.7 Discussions In this section we discuss if Sequicity can tackle inconsistent user requests , which happens when users change their minds during a dialogue. Inconsistent user requests happen frequently and are dif1444 ficult to tackle in belief tracking (Williams, 2012; Williams et al., 2013). Unlike most of previous pipeline-based work that explicitly defines model actions for each situation, Sequicity is proposed to directly handle various situations from the training data with less manual intervention. Here, given examples about restaurant reservation, we provide three different scenarios to discuss: • A user totally changes his mind. For example, the user request a Japanese restaurant first and says “I dont want Japanese food anymore, I’d like French now.” Then, all the slot activated before should be invalid now. The slot annotated for this turn is only French. Sequicity can"
P18-1133,W12-1812,0,0.165116,"W2 hj ) (1) (2) (3) where v ∈ R1×l ; W1 , W2 ∈ Rl×d and O ∈ R|V |×d . d is embedding size and V is vocabulary set and |V |is its size. 1438 3.2 Belief Trackers In the multi-turn scenarios, a belief tracker is the key component for task completion as it records key information from past turns (Wen et al., 2017b; Henderson et al., 2013, 2014a,b). Early belief trackers are designed as Bayesian networks where each node is a dialogue belief state (Paek and Horvitz, 2000; Young et al., 2013b). Recent work successfully represents belief trackers as discriminative classifiers (Henderson et al., 2013; Williams, 2012; Wen et al., 2017b). Wen et al. (2017b) apply discrimination approaches (Henderson et al., 2013) to build one classifier for each slot in their belief tracker. Following the terminology of (Wen et al., 2017b), a slot can be either informable or requestable, which have been annotated in CamRes676 and KVRET. Individually, an informable slot, specified by user utterances in previous turns, is set to a constraint for knowledge base search; whereas a requestable slot records the user’s need in the current dialogue. As an example of belief trackers in CamRes676, food type is an informable slot, and"
P18-1133,P17-1062,0,0.0780341,"the probability distribution of values over each slot (Lee, 2013). Dialogue policy makers then generate the next available system action. Recent experiments suggest that reinforcement learning is a promising paradigm to accomplish this task (Young et al., 1 http://github.com/WING-NUS/sequicity 2013a; Cuay´ahuitl et al., 2015; Liu and Lane, 2017), when state and action spaces are carefully designed (Young et al., 2010). Finally, in the response generation stage, pipeline designs usually pre-define fixed templates where placeholders are filled with slot values at runtime (Dhingra et al., 2017; Williams et al., 2017; Henderson et al., 2014b,a). However, this causes rather static responses that could lower user satisfaction. Generating a fluent, human-like response is considered a separate topic, typified by the topic of conversation systems (Li et al., 2015). 3 3.1 Preliminaries Encoder-Decoder Seq2seq Models Current seq2seq models adopt encoder–decoder structures. Given a source sequence of tokens X = x1 x2 ...xn , an encoder network represent X (x) (x) (x) as hidden states: H(x) = h1 h2 ...hn . Based (x) on H , a decoder network generates a target sequence of tokens Y = y1 y2 ...ym whose likelihood sho"
P98-1112,W97-0703,0,0.0422981,"Missing"
P98-1112,P94-1002,0,0.0244337,"Missing"
P98-1112,C94-2174,0,0.0153799,"sults, a breaking news article would feature a higher percentage of motion verbs rather than verbs of communication. 1.3 On Genre Detection Verbs are an important factor in providing an event profile, which in turn might be used in categorizing articles into different genres. Turning to the literature in genre classification, Biber (1989) outlines five dimensions which can be used to characterize genre. Properties for distinguishing dimensions include verbal features such as tense, agentless passives and infinitives. Biber also refers to three verb classes: private, public, and suasive verbs. Karlgren and Cutting (1994) take a computationally tractable set of these properties and use them to compute a score to recognize text genre using discriminant analysis. The only verbal feature used in their study is present-tense verb count. As Karlgren and Cutting show, their techniques are effective in genre categorization, but they do not claim to show how genres differ. Kessler et al. (1997) discuss some of the complexities in automatic detection of genre using a set of computationally efficient cues, such as punctuation, abbreviations, or presence of Latinate suffixes. The taxonomy of genres and facets developed i"
P98-1112,P97-1005,0,0.081528,"used to characterize genre. Properties for distinguishing dimensions include verbal features such as tense, agentless passives and infinitives. Biber also refers to three verb classes: private, public, and suasive verbs. Karlgren and Cutting (1994) take a computationally tractable set of these properties and use them to compute a score to recognize text genre using discriminant analysis. The only verbal feature used in their study is present-tense verb count. As Karlgren and Cutting show, their techniques are effective in genre categorization, but they do not claim to show how genres differ. Kessler et al. (1997) discuss some of the complexities in automatic detection of genre using a set of computationally efficient cues, such as punctuation, abbreviations, or presence of Latinate suffixes. The taxonomy of genres and facets developed in Kessler et al. is useful for a wide range of types, such as found in the Brown corpus. Although some of their discriminators could be useful for news articles (e.g. presence of second person pronoun tends to indicate a letter to the editor), the indicators do not appear to be directly applicable to a finer classification of news articles. News articles can be divided"
P98-1112,A97-1042,0,0.0254018,"Missing"
P98-1112,M93-1022,0,0.0332853,"Missing"
P98-1112,J91-1002,0,0.0217012,"in document type discrimination; we show how article types can be successfully classified within the news domain using verb semantic classes. 2 Verb T y p e communication support remainder and EVCA Since our first intuition of the data suggested that articles with a preponderance of verbs of 682 a certain semantic type might reveal aspects of document type, we tested the hypothesis that verbs could be used as a predictor in providing an event profile. We developed two algorithms to: (1) explore WordNet (WN-Verber) to cluster related verbs and build a set of verb chains in a document, much as Morris and Hirst (1991) used Roget's Thesaurus or like Hirst and St. Onge (1998) used WordNet to build noun chains; (2) classify verbs according to a semantic classification system, in this case, using Levin's (1993) English Verb Classes and Alternations (EVCA-Yerber) as a basis. For source material, we used the manually-parsed Linguistic Data Consortium's Wall Street Journal (WSJ) corpus from which we extracted main and complement of communication verbs to test the algorithms on. Using WordNet. Our first technique was to use WordNet to build links between verbs and to provide a semantic profile of the document. Wor"
P98-1112,A97-1031,0,0.0174454,"Missing"
P98-1112,A97-1028,0,0.0374578,"Missing"
P98-1112,A97-1030,0,0.0180049,"Missing"
P98-1112,W98-1123,1,\N,Missing
P98-1112,H94-1020,0,\N,Missing
qiu-etal-2004-public,A00-2018,0,\N,Missing
qiu-etal-2004-public,C96-1021,0,\N,Missing
qiu-etal-2004-public,J94-4002,0,\N,Missing
qiu-etal-2004-public,mitkov-2000-towards,0,\N,Missing
S07-1058,S07-1012,0,0.11097,"Missing"
S07-1058,P05-1045,0,0.0266584,"between the feature vectors for each pair of clusters to determine which clusters to merge. We now review the inventory of features studied in our work. Tokens (T). Identical to the task baseline by (Artiles et al., 2005), we stemmed the words in the web pages using the Porter stemmer (Porter, 1980), to conflate semantically similar English words with the stem. Each stemmed word is considered to be a feature and weighted by its Term Frequency × Inverse Document Frequency (TF×IDF). Named Entities (NE). We extract the named entities from the web pages using the Stanford Named Entity Recognizer (Finkel et al., 2005). This tagger identifies and labels names of places, organizations and people in the input. Each named entity token is treated as a separate feature, again weighted by TF×IDF. We do not perform stemming for NE features. We also consider a more target-centric form of the NE feature, motivated by the observation that person names can be differentiated using their middle names or titles. We first discard all named entities that do not contain any token of the search target, and then discard any token from the remaining named entities that appears in the search target. The remaining tokens are the"
S10-1004,W03-1028,0,0.955241,"g, National University of Singapore, Singapore sunamkim@gmail.com, medelyan@gmail.com, kanmy@comp.nus.edu.sg, tb@ldwin.net Abstract have showcased the potential benefits of keyphrase extraction to downstream NLP applications. In light of these developments, we felt that this was an appropriate time to conduct a shared task for keyphrase extraction, to provide a standard assessment to benchmark current approaches. A second goal of the task was to contribute an additional public dataset to spur future research in the area. Currently, there are several publicly available data sets.2 For example, Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. The data set contains keyphrases (i.e. controlled and uncontrolled terms) assigned by professional indexers — 1,000 for training, 500 for validation and 500 for testing. Nguyen and Kan (2007) collected a dataset containing 120 computer science articles, ranging in length from 4 to 12 pages. The articles contain author-assigned keyphrases as well as reader-assigned keyphrases contributed by undergraduate CS students. In the general newswire domain, Wan and Xiao (2008) developed a dataset of 308 d"
S10-1004,N04-4005,0,0.0188339,"tions such as summarization, information retrieval and question-answering. In summarization, keyphrases can be used as a form of semantic metadata (Barzilay and Elhadad, 1997; Lawrie et al., 2001; D’Avanzo and Magnini, 2005). In search engines, keyphrases can supplement full-text indexing and assist users in formulating queries. Recently, a resurgence of interest in keyphrase extraction has led to the development of several new systems and techniques for the task (Frank et al., 1999; Witten et al., 1999; Turney, 1999; Hulth, 2003; Turney, 2003; Park et al., 2004; Barker and Corrnacchia, 2000; Hulth, 2004; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007; Wan and Xiao, 2008; Liu et al., 2009; Medelyan, 2009; Nguyen and Phan, 2009). These 1 We use “keyphrase” and “keywords” interchangeably to refer to both single words and phrases. ♦ Min-Yen Kan’s work was funded by National Research Foundation grant “Interactive Media Search” (grant # R-252000-325-279). 2 All data sets listed below are available for download from http://github.com/snkim/ AutomaticKeyphraseExtraction 3 http://bit.ly/maui-datasets 21 Proceedings of the 5th International Worksho"
S10-1004,C08-1122,0,0.704332,"available data sets.2 For example, Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. The data set contains keyphrases (i.e. controlled and uncontrolled terms) assigned by professional indexers — 1,000 for training, 500 for validation and 500 for testing. Nguyen and Kan (2007) collected a dataset containing 120 computer science articles, ranging in length from 4 to 12 pages. The articles contain author-assigned keyphrases as well as reader-assigned keyphrases contributed by undergraduate CS students. In the general newswire domain, Wan and Xiao (2008) developed a dataset of 308 documents taken from DUC 2001 which contain up to 10 manually-assigned keyphrases per document. Several databases, including the ACM Digital Library, IEEE Xplore, Inspec and PubMed provide articles with authorassigned keyphrases and, occasionally, readerassigned ones. Medelyan (2009) automatically generated a dataset using tags assigned by the users of the collaborative citation platform CiteULike. This dataset additionally records how many people have assigned the same keyword to the same publication. In total, 180 full-text publications were annotated by over 300"
S10-1004,R09-1086,0,0.312048,"tual F-scores 2.2 Evaluation Method and Baseline Traditionally, automatic keyphrase extraction systems have been assessed using the proportion of top-N candidates that exactly match the goldstandard keyphrases (Frank et al., 1999; Witten et al., 1999; Turney, 1999). In some cases, inexact matches, or near-misses, have also been considered. Some have suggested treating semanticallysimilar keyphrases as correct based on similarities computed over a large corpus (Jarmasz and Barriere, 2004; Mihalcea and Tarau, 2004), or using semantic relations defined in a thesaurus (Medelyan and Witten, 2006). Zesch and Gurevych (2009) compute near-misses using an ngram based approach relative to the gold standard. For our shared task, we follow the traditional exact match evaluation metric. That is, we match the keyphrases in the answer set with those the systems provide, and calculate micro-averaged precision, recall and F-score (β = 1). In the evaluation, we check the performance over the top 5, 10 and 15 candidates returned by each system. We rank the participating systems by F-score over the top 15 candidates. Participants were required to extract existing phrases from the documents. Since it is theoretically possible"
S10-1004,P09-2046,0,0.0298245,"1997; Lawrie et al., 2001; D’Avanzo and Magnini, 2005). In search engines, keyphrases can supplement full-text indexing and assist users in formulating queries. Recently, a resurgence of interest in keyphrase extraction has led to the development of several new systems and techniques for the task (Frank et al., 1999; Witten et al., 1999; Turney, 1999; Hulth, 2003; Turney, 2003; Park et al., 2004; Barker and Corrnacchia, 2000; Hulth, 2004; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007; Wan and Xiao, 2008; Liu et al., 2009; Medelyan, 2009; Nguyen and Phan, 2009). These 1 We use “keyphrase” and “keywords” interchangeably to refer to both single words and phrases. ♦ Min-Yen Kan’s work was funded by National Research Foundation grant “Interactive Media Search” (grant # R-252000-325-279). 2 All data sets listed below are available for download from http://github.com/snkim/ AutomaticKeyphraseExtraction 3 http://bit.ly/maui-datasets 21 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 21–26, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics search areas in each case. Note that the trial d"
S10-1004,W04-3252,0,\N,Missing
S10-1004,W97-0703,0,\N,Missing
S10-1004,D09-1027,0,\N,Missing
S10-1004,C02-1142,0,\N,Missing
S17-2170,councill-etal-2008-parscit,1,0.741521,"hich performs a relatively complex job. These additional models are: Feature Ablation To assess the importance of the component features, we perform ablation testing over the Dev (Table 1). We see that the setup of using all features is largely validated, with only a slight 0.01 F1 loss on Subtask A alone. The decay in performance due to ablation is stable: showing that sequential dependencies on the previous output (O), the token identity, prefixes and suffixes matter the most. These results are expected and validate earlier sequence labeling work for parsing bibliographic reference strings (Councill et al., 2008). As we are dealing with document excerpts taken from random sections, and possibly due to the sparsity of the feature, title occurrence (F18, Row 6) played the least role in performance and could have been omitted. However, it helps in Subtask B, making the overall Subtask B perform slightly better (F18, Row 1). For simplicity, we use all features (Row 1) for our subsequent CRF models. 3.2 Precision 0.55 0.49 • Unified: We collapse all three keyphrase types into a single type. This system acts as an expert for identifying keyphrases (Subtask A only). • Individual: We use each of the three typ"
S17-2170,W14-4807,0,0.0281889,"revious token, next two tokens) to incorporate component features from (F0 – F18, O). The template also can direct CRF++ to compute more complex feature– bigrams. Our final model’s expanded feature list includes many of the surrounding tokens features 2. Dataset. We participate in SemEval-2017 Task 10 on science information extraction (Augenstein et al., 2017) using the dataset consisting of 350 training samples (Train), 50 development samples (Dev) and 100 testing samples (Test). Each data sample is an excerpt of a scientific document. Unlike previous work creating similar benchmark dataset (Handschuh and QasemiZadeh, 2014), the dataset excerpt is taken out of a random section of the document. Tasks. The excerpt requires its keyphrases to be identified (Subtask A), typed among one of three types: Materials, Process and Task (Subtask B), then finally followed by Synonym-of and Hyponym-of identification among the extracted keyphrases (Subtask C). Our work focuses specifically only on Subtasks A and B; our effort on Subtask C is minimal. • Part-of-Speech (F10): The part of the speech tag, as obtained from tagging of the complete sequence using the nltk.punkt tagger. • Capitalization (F11): The orthographic case of"
S17-2170,S10-1004,1,0.861789,"hrases of type Materials; Introduction • Determining the keyphrase type depends on the context. For example “oxidation test” and “assessment of the corrosion condition” can potentially be either of Task or Process, depending on the context. Keyphrases are often used for representing the salient concepts of a document. In scientific documents, keyphrase extraction is an important prerequisite task that feeds downstream tasks such as summarization, clustering and indexing, among others. As such, automatic keyphrase extraction has garnered attention and become a focal point for many researchers (Kim et al., 2010). Usually, Considering these differences, we believe that sequence labeling based modeling is more suited than the standard, top K keyphrase extraction. 973 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 973–977, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics • Output for Previous Token (O): The prediction for the previous token, a contextual label feature. Such a model also easily extends to a joint approach for both extraction and classification, which we investigated. 2 Method To achieve Subtask C, we pla"
U09-1013,drouin-2004-detection,0,0.0319108,"the Reuters document collection using term frequency and inverse document frequency. 1 Introduction Automatic domain-specific term extraction is a categorization/classification task where terms are categorized into a set of predefined domains. It has been employed in tasks such as keyphrase extraction (Frank et al., 1999; Witten et al., 1999), word sense disambiguation (Magnini et al., 2002), and query expansion and cross-lingual text categorization (Rigutini et al., 2005). Even though the approach shows promise, relatively little research has been carried out to study its effects in detail (Drouin, 2004; Milne et al., 2006; Rigutini et al., 2006; Kida et al., 2007; Park et al., 2008). Most of the research to date on domainspecific term extraction has employed supervised machine learning, within the fields of term categorization and text mining. However, to date, the only research to approach the task in an unsupervised manner is that of Park et al. (2008). Unsupervised methods have the obvious advantage that they circumvent the need for laborious manual classification of training instances, and are thus readily applicable to arbitrary sets of domains, tasks and languages. In this paper, we p"
U09-1013,P06-1068,0,0.0754223,"Missing"
U09-1013,S01-1027,0,0.0880807,"Missing"
W01-0813,C00-1007,0,0.0268105,"C ENTRIFUSER is fully implemented; it produces the sample summary in Figure 1. We have concentrated on implementing the most commonly occuring document feature, topicality, and have additionally incorporated three other document features into our framework (document-derived Content Types and Special Content and the Title metadata). Future work will include extending our document feature analysis to model context (to model adding features only when appropriate), as well as incorporating additional document features. We are also exploring the use of stochastic corpus modeling (Langkilde, 2000; Bangalore and Rambow, 2000) to replace our template-based realizer with a probabilistic one that can produce felicitous sentence patterns based on contextual analysis. 8 Conclusion We have presented a model for indicative multidocument summarization based on natural language generation. In our model, summary content is based on document features describing topic and structure instead of extracted text. Given these features, a generation model uses a text plan, derived from analysis of naturally occurring indicative summaries plus guidelines for summarization, to guide the system in describing document topics as typical,"
W01-0813,W98-1123,1,0.900195,"Missing"
W01-0813,A00-2023,0,0.0400379,"s and future work C ENTRIFUSER is fully implemented; it produces the sample summary in Figure 1. We have concentrated on implementing the most commonly occuring document feature, topicality, and have additionally incorporated three other document features into our framework (document-derived Content Types and Special Content and the Title metadata). Future work will include extending our document feature analysis to model context (to model adding features only when appropriate), as well as incorporating additional document features. We are also exploring the use of stochastic corpus modeling (Langkilde, 2000; Bangalore and Rambow, 2000) to replace our template-based realizer with a probabilistic one that can produce felicitous sentence patterns based on contextual analysis. 8 Conclusion We have presented a model for indicative multidocument summarization based on natural language generation. In our model, summary content is based on document features describing topic and structure instead of extracted text. Given these features, a generation model uses a text plan, derived from analysis of naturally occurring indicative summaries plus guidelines for summarization, to guide the system in describin"
W01-0813,P97-1013,0,0.0223606,"A topic tree for an article about coronary artery disease from The Merck manual of medical information, constructed automatically from its section headers. care article. Each document in the collection is represented by such a tree, which breaks each document’s topic into subtopics. We build these document topic trees automatically for structured documents using a simple approach that utilizes section headers, which suffices for our current domain and genre. Other methods such as layout identification (Hu et al., 1999) and text segmentation / rhetorical parsing (Yaari, 1999; Kan et al., 1998; Marcu, 1997) can serve as the basis for constructing such trees in both structured and unstructured documents, respectively. 4.1 Normative topicality as composite topic trees As stated in rule 1, the summarizer needs normative values calculated for each document feature to properly compute differences between documents. The composite topic tree embodies this paradigm. It is a data structure that compiles knowledge about all possible topics and their structure in articles of the same intersection of domain and genre, (i.e., rule 1’s notion of “document type”). Figure 4 shows a partial view of such a tree c"
W01-0813,W94-0319,0,0.0106391,"multidocument summary generation. We address the problem of “what to say” in Section 2, by examining what document features are important for indicative summaries, starting from a single document context and generalizing to a multidocument, querybased context. This yields two rules-of-thumb for guiding content calculation: 1) reporting differences from the norm and 2) reporting information relevent to the query. We have implemented these rules as part of the content planning module of our C ENTRIFUSER summarization system. The summarizer’s architecture follows the consensus NLG architecture (Reiter, 1994), including the stages of content calculation and content planning. We follow the generation of a sample indicative multidocument query-based summary, shown in the bottom half of Figure 1, focusing on these two stages in the remainder of the paper. 2 Document features as potential summary content Information about topics and structure of the document may be based on higher-level document features. Such information typically does not occur as strings in the document text. Our approach, therefore, is to identify and extract the document features that are relevant for indicative summaries. These"
W02-2101,C94-2174,0,0.0953842,"Missing"
W02-2101,P97-1005,0,0.0327727,"Missing"
W02-2101,A00-2023,0,0.0474466,"es and their ordering, the task of surface realization is to convey the predicates as natural language. In traditional NLG, surface realization is often broken down into three separate tasks: (1) sentence planning, which takes individual messages or propositions and assigns them to specific sentences and determines the sentences’ basic syntactic structure; (2) lexical choice, which determines the words used, and (3) syntactic realization, which uses a grammar to produce the sentence. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audie"
W02-2101,C00-1007,0,0.066472,"Missing"
W02-2101,P01-1008,1,0.830652,"Missing"
W02-2101,H01-1065,1,0.876325,"Missing"
W02-2101,P96-1025,0,0.0331878,"Missing"
W02-2101,P01-1023,1,0.801212,"hus have different predicate attribute values. In addition, some predicates are present in some summaries and not in others (e.g., author or editor). Tables 1 and 2 list the predicates and their frequency in the training corpus. The presence of the predicate may also be dependent on its value (e.g., Edition only occurs after the first edition). Content ordering. The presence or absence of particular predicates depends greatly on the presence or absence of its peers. Thus it is important to encode content structuring information, represented as local preferences rather than predefined schemas. Duboue and McKeown (2001) detail an approach for this problem which we initially tried that uses techniques from computational biology, but which is best suited for summaries with multiple instances of the same predicate. Instead, we calculated bigram statistics on pairs of adjacent predicates, recording which occurred before another. These statistics are used to find an ordering of the predicates that maximizes agreement with training observations. This approach was also utilized in work done on premodifier ordering (Shaw and Hatzivassiloglou, 1999), in which pairs of premodifiers were observed and used to find order"
W02-2101,W01-0813,1,0.886958,"predicate attribute values. In addition, some predicates are present in some summaries and not in others (e.g., author or editor). Tables 1 and 2 list the predicates and their frequency in the training corpus. The presence of the predicate may also be dependent on its value (e.g., Edition only occurs after the first edition). Content ordering. The presence or absence of particular predicates depends greatly on the presence or absence of its peers. Thus it is important to encode content structuring information, represented as local preferences rather than predefined schemas. Duboue and McKeown (2001) detail an approach for this problem which we initially tried that uses techniques from computational biology, but which is best suited for summaries with multiple instances of the same predicate. Instead, we calculated bigram statistics on pairs of adjacent predicates, recording which occurred before another. These statistics are used to find an ordering of the predicates that maximizes agreement with training observations. This approach was also utilized in work done on premodifier ordering (Shaw and Hatzivassiloglou, 1999), in which pairs of premodifiers were observed and used to find order"
W02-2101,W00-0306,0,0.121094,"Missing"
W02-2101,P98-2176,0,0.0582913,"Missing"
W02-2101,A00-2026,0,0.18552,"Missing"
W02-2101,P99-1018,0,0.0969699,"formation, represented as local preferences rather than predefined schemas. Duboue and McKeown (2001) detail an approach for this problem which we initially tried that uses techniques from computational biology, but which is best suited for summaries with multiple instances of the same predicate. Instead, we calculated bigram statistics on pairs of adjacent predicates, recording which occurred before another. These statistics are used to find an ordering of the predicates that maximizes agreement with training observations. This approach was also utilized in work done on premodifier ordering (Shaw and Hatzivassiloglou, 1999), in which pairs of premodifiers were observed and used to find ordering constraints. The technique is also referred to as Majority Ordering in (Barzilay et al., 2001), in which bigram orderings were elicited from human subjects. (Background j Language) Overview Topic Size Media Types Authority Collection Size (Comparison j Detail j Content Types j Navigation j Query Relevance) Subjective Difficulty Author Purpose Style (Publisher j Award j Readability j Audience j Contributor j Copyright) Figure 2: Highest agreement full orderings of the predicates using harmonic penalties. Predicates are swa"
W02-2101,N01-1001,0,0.17174,"vey the predicates as natural language. In traditional NLG, surface realization is often broken down into three separate tasks: (1) sentence planning, which takes individual messages or propositions and assigns them to specific sentences and determines the sentences’ basic syntactic structure; (2) lexical choice, which determines the words used, and (3) syntactic realization, which uses a grammar to produce the sentence. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audience). We focus on these metadata predicates, as they comprise a large port"
W02-2101,C98-2171,0,\N,Missing
W06-1603,C04-1051,0,0.0917421,"es describing the same event, paraphrases are widely used, possibly with extraneous information. We equate PR with solving these two issues, presenting a natural two-phase architecture. In the first phase, the nuggets shared by the sentences are identified by a pairing process. In the second phase, any unpaired nuggets are classified as significant or not (leading to −pp and +pp classifications, respectively). If the sentences do not contain unpaired nuggets, or if all unpaired nuggets are insignificant, then the sentences are considered paraphrases. Experiments on the widely-used MSR corpus (Dolan et al., 2004) show favorable results. We first review related work in Section 2. We then present the overall methodology and describe the implemented system in Section 3. Sections 4 and 5 detail the algorithms for the two phases respectively. This is followed with our evaluation and discussion of the results. port vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distribu"
W06-1603,P02-1031,0,0.00955003,"wide set of features of unpaired tuples, including internal counts of numeric expressions, named entities, words, semantic roles, whether they are similar to other tuples in the same sentence, and contextual features like source/target sentence length and paired tuple count. Currently, only two features are correlated in improved classification, which we detail now. Syntactic Parse Tree Path: This is a series of features that reflect how the unpaired tuple connects with the context: the rest of the sentence. It models the syntactic connection between the constituents on both ends of the path (Gildea and Palmer, 2002; Pradhan et al., 2004). Here, we model the ends of the path as the unpaired tuple and the paired tuple with the closest shared ancestor, and model the path itself as a sequence of constituent category tags and directions to reach the destination (the paired target) from the source (the 2 Copular constructions are not handled by ASSERT. Such constructions account for a large proportion of the semantic meaning in sentences. Consider the pair “Microsoft rose 50 cents” and “Microsoft was up 50 cents”, in which the second is in copular form. Similarly, NPs can often be equivalent to predicate argu"
W06-1603,P98-2127,0,0.0127747,"ferences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recognition system is shown in Figure 2. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al., 2004)), to label predicate argument tuples. We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple. A thesaurus constructed from corpus statistics (Lin, 1998) is utilized for the content similarity. We utilize this metric to greedily pair together the most similar predicate argument tuples across 1 ASSERT, which is trained on the Propbank, only guarantees consistency of arg0 and arg1 slots, but we have found in practice that aligning arg2 and above arguments do not cause problems. 20 Sentence Modification 1: paraphrase Richard Miller was hurt by a young man. (Paired) Tuples target: arg0: arg1: hurt a young man Richard Miller Model Sentence Authorities said a young man injured Richard Miller. target: said arg0: Authorities arg1: a young man injured"
W06-1603,N04-1030,0,0.0930859,"for the action, concepts and their relationships as a single unit. In comparison, using fine-grained units such as words, including nouns and verbs may result in inaccuracy (sentences that share vocabulary may not be paraphrases), while using coarser-grained units may cause key differences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recognition system is shown in Figure 2. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al., 2004)), to label predicate argument tuples. We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple. A thesaurus constructed from corpus statistics (Lin, 1998) is utilized for the content similarity. We utilize this metric to greedily pair together the most similar predicate argument tuples across 1 ASSERT, which is trained on the Propbank, only guarantees consistency of arg0 and arg1 slots, but we have found in practice that aligning arg2 and above arguments do not cause problems."
W06-1603,W05-1202,0,0.0119731,"red nuggets are insignificant, then the sentences are considered paraphrases. Experiments on the widely-used MSR corpus (Dolan et al., 2004) show favorable results. We first review related work in Section 2. We then present the overall methodology and describe the implemented system in Section 3. Sections 4 and 5 detail the algorithms for the two phases respectively. This is followed with our evaluation and discussion of the results. port vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of a candidate text. Wu (2005)’s approach relies on the generative framework of Inversion Transduction Grammar (ITG) to measure how similar two sentences arrange their words based on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster,"
W06-1603,W05-1205,0,0.0335034,"2. We then present the overall methodology and describe the implemented system in Section 3. Sections 4 and 5 detail the algorithms for the two phases respectively. This is followed with our evaluation and discussion of the results. port vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of a candidate text. Wu (2005)’s approach relies on the generative framework of Inversion Transduction Grammar (ITG) to measure how similar two sentences arrange their words based on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster, presumably similar in structure and content, are then used to construct a lattice with “backbone” nodes corresponding to words shared by the majority and “slots” corresponding to different reali"
W06-1603,N03-1003,0,0.110538,"owed with our evaluation and discussion of the results. port vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of a candidate text. Wu (2005)’s approach relies on the generative framework of Inversion Transduction Grammar (ITG) to measure how similar two sentences arrange their words based on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster, presumably similar in structure and content, are then used to construct a lattice with “backbone” nodes corresponding to words shared by the majority and “slots” corresponding to different realization of arguments. If sentences from different clusters have shared arguments, the associated lattices are claimed to be paraphrase. Likewise, Shinyama et al. (2002) extracted paraphrases f"
W06-1603,I05-5001,0,0.00823651,"araphrases rather than detecting them, and as such have the disadvantage of requiring a certain level of repetition among candidates for paraphrases to be recognized. 2 Related Work Possibly the simplest approach to PR is an information retrieval (IR) based “bag-of-words” strategy. This strategy calculates a cosine similarity score for the given sentence set, and if the similarity exceeds a threshold (either empirically determined or learned from supervised training data), the sentences are paraphrases. PR systems that can be broadly categorized as IR-based include (Corley and Mihalcea, 2005; Brockett and Dolan, 2005). In the former work, the authors defined a directional similarity formula reflecting the semantic similarity of one text “with respect to” another. A word contributes to the directional similarity only when its counterpart has been identified in the opposing sentence. The associated word similarity scores, weighted by the word’s specificity (represented as inverted document frequency, idf ), sum to make up the directional similarity. The mean of both directions is the overall similarity of the pair. Brockett and Dolan (2005) represented sentence pairs as a feature vector, including features ("
W06-1603,A00-2018,0,0.0226765,"is a better choice for the representation of a nugget as it accounts for the action, concepts and their relationships as a single unit. In comparison, using fine-grained units such as words, including nouns and verbs may result in inaccuracy (sentences that share vocabulary may not be paraphrases), while using coarser-grained units may cause key differences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recognition system is shown in Figure 2. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al., 2004)), to label predicate argument tuples. We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple. A thesaurus constructed from corpus statistics (Lin, 1998) is utilized for the content similarity. We utilize this metric to greedily pair together the most similar predicate argument tuples across 1 ASSERT, which is trained on the Propbank, only guarantees consistency of arg0 and arg1 slots, but we have found"
W06-1603,W05-1203,0,0.0190787,"geared towards acquiring paraphrases rather than detecting them, and as such have the disadvantage of requiring a certain level of repetition among candidates for paraphrases to be recognized. 2 Related Work Possibly the simplest approach to PR is an information retrieval (IR) based “bag-of-words” strategy. This strategy calculates a cosine similarity score for the given sentence set, and if the similarity exceeds a threshold (either empirically determined or learned from supervised training data), the sentences are paraphrases. PR systems that can be broadly categorized as IR-based include (Corley and Mihalcea, 2005; Brockett and Dolan, 2005). In the former work, the authors defined a directional similarity formula reflecting the semantic similarity of one text “with respect to” another. A word contributes to the directional similarity only when its counterpart has been identified in the opposing sentence. The associated word similarity scores, weighted by the word’s specificity (represented as inverted document frequency, idf ), sum to make up the directional similarity. The mean of both directions is the overall similarity of the pair. Brockett and Dolan (2005) represented sentence pairs as a feature v"
W06-1603,C98-2122,0,\N,Missing
W06-2407,W05-1005,0,0.265562,"s. They approximate the identification of argument/adjunct structures by using the preposition head of prepositional phrases that occur after the verb or object of interest. Let n be a deverbal noun whose most likely light verb is to be found. Denote its verbal form by v 0 , and let P be the set containing the three most SFN04(v, n) = 2 × I(v, a-n) − I(v, det-n), (3) where higher values indicate a higher likelihood of v-a-n being a light verb construction. Also, they suggested that the determiner “the” be excluded from the development data since it frequently occurred in their data. Recently, Fazly et al. (2005) have proposed a statistical measure for the detection of LVCs. The probability that a verb-object pair v-n (where v is a light verb) is a LVC can be expressed as a product of three probabilities: (1) probability of the object 50 n occurring in the corpus, (2) the probability that n is part of any LVC given n, and (3) the probability of v occurring given n and that v-n is a LVC. Each of these three probabilities can then be estimated by the frequency of occurrence in the corpus, using the assumption that all instances of v 0 -a-n is a LVC, where v 0 is any light verb and a is an indefinite det"
W06-2407,E95-1014,0,0.161725,"is a deverbal noun, a is an indefinite determiner (namely, “a” or “an”), and det is any determiner other than the indefinite. Examples of such constructions are “give a speech” and “take a walk”. They employ mutual information which measures the frequency of co-occurrences of two variables, corrected for random agreement. Let I(x, y) be the mutual information between x and y. Then the following measure can be used: 2 Related Work With the recent availability of large corpora, statistical methods that leverage syntactic features are a current trend. This is the case for LVC detection as well. Grefenstette and Teufel (1995) considered a similar task of identifying the most probable light verb for a given deverbal noun. Their approach focused on the deverbal noun and occurrences of the noun’s verbal form, arguing that the deverbal noun retains much of the verbal characteristics in the LVCs. To distinguish the LVC from other verbobject pairs, the deverbal noun must share similar argument/adjunct structures with its verbal counterpart. Verbs that appear often with these characteristic deverbal noun forms are deemed light verbs. They approximate the identification of argument/adjunct structures by using the preposit"
W06-2407,W04-0401,0,0.481214,"that their subsequent experiments suggested that the filtering step may not be necessary. Whereas the GT95 measure centers on the deverbal object, Dras and Johnson (1996) also consider the verb’s corpus frequency. The use of this complementary information improves LVC identification, as it models the inherent bias of some verbs to be used more often as light verbs than others. Let f (v, n) be the count of verb-object pairs occurring in the corpus, such that v is the verb, n is a deverbal noun. Then, the most probable light verb for n is given by: X DJ96(n) = arg max f (v, n) f (v, n) (2) v n Stevenson et al. (2004)’s research examines evidence from constructions featuring determiners. They focused on expressions of the form v-a-n and v-det-n, where v is a light verb, n is a deverbal noun, a is an indefinite determiner (namely, “a” or “an”), and det is any determiner other than the indefinite. Examples of such constructions are “give a speech” and “take a walk”. They employ mutual information which measures the frequency of co-occurrences of two variables, corrected for random agreement. Let I(x, y) be the mutual information between x and y. Then the following measure can be used: 2 Related Work With the"
W07-0204,C04-1162,0,0.564417,"h particular parameter settings. We apply timestamped graphs on the standard DUC multi-document text summarization task and achieve comparable results to the state of the art. 1 Introduction Graph-based ranking algorithms such as Kleinberg’s HITS (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully applied in citation network analysis and ranking of webpages. These algorithms essentially decide the weights of graph nodes based on global topological information. Recently, a number of graph-based approaches have been suggested for NLP applications. Erkan and Radev (2004) introduced LexRank for multi-document text summarization. Mihalcea and Tarau (2004) introduced TextRank for keyword and sentence extractions. Both LexRank and TextRank assume a fully connected, undirected graph, with text units as nodes and similarity as edges. After graph construction, both algorithms use a random walk on the graph to redistribute the node weights. Many graph-based algorithms feature an evolutionary model, in which the graph changes over timesteps. An example is a citation network whose edges point backward in time: papers (usually) only reference older published works. Refe"
W07-0204,H05-1115,0,0.0694598,"Missing"
W07-0204,W04-3252,0,\N,Missing
W09-2902,P06-1068,0,0.186446,"Missing"
W09-2902,C02-1142,0,0.0679156,"Missing"
W09-2902,C08-2021,0,0.0639913,"Missing"
W09-2902,W97-0703,0,0.286606,"Missing"
W09-2902,C08-1122,0,0.0506999,"erve as a representative summary of the document and also serve as high quality index terms. It is thus no surprise that keyphrases have been utilized to acquire critical information as well as to improve the quality of natural language processing (NLP) applications such as document summarizer(D´avanzo and Magnini, 2005), information retrieval (IR)(Gutwin et al., 1999) and document clustering(Hammouda et al., 2005). In the past, various attempts have been made to boost automatic keyphrase extraction performance based primarily on statistics(Frank et al., 1999; Turney, 2003; Park et al., 2004; Wan and Xiao, 2008) and a rich set of heuristic features(Barker and Corrnacchia, 2000; Medelyan and Witten, 9 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 9–16, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP Textract (Park et al., 2004) also ranks the candidate keyphrases by its judgment of keyphrases’ degree of domain specificity based on subjectspecific collocations(Damerau, 1993), in addition to term cohesion using Dice coefficient(Dice, 1945). Recently, Wan and Xiao (2008) extracts automatic keyphrases from single documents, utilizing document clustering informati"
W09-2902,P89-1010,0,0.175281,"Missing"
W09-2902,councill-etal-2008-parscit,1,0.793074,"rget feature. + indicates an improvement, - indicates a performance decline, and ? indicates no effect or unconfirmed due to small changes of performances. Again, supervised denotes Maximum Entropy training and Unsupervised is our unsupervised approach. To assess the performance of the proposed candidate selection rules and features, we implemented a keyphrase extraction pipe line. We start with raw text of computer science articles converted from PDF by pdftotext. Then, we partitioned the into section such as title and sections via heuristic rules and applied sentence segmenter 2 , ParsCit3 (Councill et al., 2008) for reference collection, part-of-speech tagger4 and lemmatizer5 (Minnen et al., 2001) of the input. After preprocessing, we built both supervised and unsupervised classifiers using Naive Bayes from the WEKA machine learning toolkit(Witten and Frank, 2005), Maximum Entropy6 , and simple weighting. In evaluation, we collected 250 papers from four different categories7 of the ACM digital library. Each paper was 6 to 8 pages on average. In author assigned keyphrase, we found many were missing or found as substrings. To remedy this, we collected reader assigned keyphrase by hiring senior year und"
W09-2905,W02-2001,0,0.108882,"Missing"
W09-2905,P01-1025,0,0.0384995,"annotated in (Baldwin, 2005) and 464 annotated LVC candidates used in (Tan et al., 2006). Both sets of annotations give both positive and negative examples. Our final VPC and LVC evaluation datasets were then constructed by intersecting the goldstandard datasets with our corresponding sets of extracted candidates. We also concatenated both sets of evaluation data for composite evaluation. This set is referred to as “Mixed”. Statistics of our three evaluation datasets are summarized in Table 2. VPC data LVC data Total (freq ≥ 6) Positive instances (the n-best method). However, as discussed in Evert and Krenn (2001), this method depends heavily on the choice of n. In this paper, we opt for average precision (AP), which is the average of precisions at all possible recall values. This choice also makes our results comparable to those of Pecina and Schlesinger (2006). 3.3 Figure 1(a, b) gives the two average precision profiles of the 82 AMs presented in Pecina and Schlesinger (2006) when we replicated their experiments over our English VPC and LVC datasets. We observe that the average precision profile for VPCs is slightly concave while the one for LVCs is more convex. This can be interpreted as VPCs being"
W09-2905,W06-1203,0,0.067321,"Missing"
W09-2905,pearce-2002-comparative,0,0.561572,"Missing"
W09-2905,P06-2084,0,0.597394,"onal University of Singapore hoanghuu@comp.nus.edu.sg snkim@csse.unimelb.edu.au kanmy@comp.nus.edu.sg ratio and the T score. In Pearce (2002), AMs such as Z score, Pointwise MI, cost reduction, left and right context entropy, odds ratio are evaluated. Evert (2004) discussed a wide range of AMs, including exact hypothesis tests such as the binomial test and Fisher’s exact tests, various coefficients such as Dice and Jaccard. Later, Ramisch et al. (2008) evaluated MI, 2 Pearson’s χ and Permutation Entropy. Probably the most comprehensive evaluation of AMs was presented in Pecina and Schlesinger (2006), where 82 AMs were assembled and evaluated over Czech collocations. These collocations contained a mix of idiomatic expressions, technical terms, light verb constructions and stock phrases. In their work, the best combination of AMs was selected using machine learning. While the previous works have evaluated AMs, there have been few details on why the AMs perform as they do. A detailed analysis of why these AMs perform as they do is needed in order to explain their identification performance, and to help us recommend AMs for future tasks. This weakness of previous works motivated us to addres"
W09-2905,J93-1007,0,0.18954,". We note here that the mobility property of both VPC and LVC constituents have been used in the extraction process. For VPCs, we first identify particles using a pre-compiled set of 38 particles based on Baldwin (2005) and Quirk et al. (1985) (Appendix A). Here we do not use the WSJ particle tag to avoid possible inconsistencies pointed out in Baldwin (2005). Next, we search to the left of the located particle for the nearest verb. As verbs and particles in transitive VPCs may not occur contiguously, we allow an intervening NP of up to 5 words, similar to Baldwin and Villavicencio (2002) and Smadja (1993), since longer NPs tend to be located after particles. 32 AM Name M1. Joint Probability Formula AM Name M2. Mutual Information f ( xy ) / N Formula 1 ∑f N log ij i, j M3. Log likelihood ratio M5. Local-PMI M7. PMI 2∑ i, j M4. Pointwise MI (PMI) fij fij log fˆ ij M6. PMI f ( xy ) × PMI 2 log Nf ( xy ) M8. Mutual Dependency 2 M10. Normalized expectation a ( a + b)( a + c ) M12. First Kulczynski a a+b+c M13. Second Sokal-Sneath M17. Hamann M19. Yule’s ω a + 2(b + c ) M16. Rogers-Tanimoto a+d a+b+c+d ( a + d ) − (b + c ) M18. Odds ratio a+b+c+d M20. Yule’s Q ad − bc ad + bc M21. BrawnBlanquet M23."
W09-2905,W06-2407,1,0.816489,"Missing"
W09-3609,councill-etal-2008-parscit,1,0.90765,"Missing"
W09-3609,N04-1042,0,0.0374938,"Missing"
W10-1114,councill-etal-2008-parscit,1,0.85538,"nd biological entities, combining basic orthographic token features with features derived from semantic lexicons such as UMLS, ABGene, SemRep and MetaMap. In a related vein, CRFs have been applied to gene map and relationship identification as well (Bundschus et al., 2008; Talreja et al., 2004). In a different domain, digital library practitioners have also studied how to extract formulaic metadata to enable more comprehensive article indexing. To extract author and title information, systems have used both the Support Vector Machine (SVM) (Han et al., 2003) and CRFs (Peng and McCallum, 2004; Councill et al., 2008). These works have been applied largely to the computer science community and have not yet been extensively tested on biomedical and clinical research articles. Our work differs from the above by making use of CRFs to extract fields in clinical text. Similar lexical-based features are employed, however in addition to regular author metadata, we also attempt to extract domain-specific fields from the body text of the article. 3 Method External to the scope of the research presented here, our wider project goal focuses on constructing a knowledge base of clinical researchers, databases, instrume"
W10-1114,N04-1042,0,0.190372,"suitability of CRFs to find biological entities, combining basic orthographic token features with features derived from semantic lexicons such as UMLS, ABGene, SemRep and MetaMap. In a related vein, CRFs have been applied to gene map and relationship identification as well (Bundschus et al., 2008; Talreja et al., 2004). In a different domain, digital library practitioners have also studied how to extract formulaic metadata to enable more comprehensive article indexing. To extract author and title information, systems have used both the Support Vector Machine (SVM) (Han et al., 2003) and CRFs (Peng and McCallum, 2004; Councill et al., 2008). These works have been applied largely to the computer science community and have not yet been extensively tested on biomedical and clinical research articles. Our work differs from the above by making use of CRFs to extract fields in clinical text. Similar lexical-based features are employed, however in addition to regular author metadata, we also attempt to extract domain-specific fields from the body text of the article. 3 Method External to the scope of the research presented here, our wider project goal focuses on constructing a knowledge base of clinical research"
W12-2106,pak-paroubek-2010-twitter,0,0.0842531,"Missing"
W12-2106,W11-0708,0,0.095764,"Missing"
W12-2106,prasad-etal-2008-penn,0,0.0324566,"Missing"
W12-2106,D09-1026,0,0.0640933,"Missing"
W12-2106,P05-2008,0,0.128166,"Missing"
W12-2106,D11-1141,0,0.047856,"sted to find the best performing feature set. Table 2 quantifies the contribution of each feature and demonstrate the result from the best combination, as measured by Weighted Average F-Measure (WAFM). Compared to the performance of using baseline feature set using tweet content alone, the use of linguistic features improve the performance accordingly, with the exception of the use of named entities which reduced performance slightly, and hence was removed from the final classifier’s feature set. 4 Using the OpenNLP toolkit. Using (Lin et al., 2010)’s parser. 6 Using the UW Twitter NLP tools (Ritter et al., 2011). 7 Defined as Boolean matches to the following regular expressions: “RT @[username]...”, “...via @[username]...”, “Retweeting @[username]...”,“Follow me if...”, “retweet @[username]...”, “...RT if...” and “Retweet if...” 5 Scheme Level-1 Level-2 C .625 .413 CI .642 .422 CT .635 .427 CD .637 .432 CH .629 .415 CE .611 .409 CITDH .670 .451 Table 2: Weighted average F-measure results for the labeled LDA classification. Legend: C: tweet context; I: Interaction; T: Tense; D: Discourse Relations; H: Hashtags; E: Named Entities. Require: Training set L; Test collection C; Evaluation set E; Iteration"
W12-3209,W09-3607,0,0.0689786,"Missing"
W12-3209,P11-4002,0,0.256133,"Missing"
W12-3209,bird-etal-2008-acl,1,\N,Missing
W15-4406,D07-1109,0,0.03399,"log-linear Part-of-Speech tagger (Toutanova et al., 2003) to obtain the POS tag for the English word, whereas the POS tag for target Chinese senses are provided in our dictionary. In cases where multiple candidate Chinese translations fit the same sense, we again break ties using relative frequency of the prospective candidates. This method has complete coverage over the CET 4 list (as the word frequency rule always yields a prospective translation), but as it lacks any context model, it is the least accurate. 3.2 Approach 1: News Category Topic information has been shown to be useful in WSD (Boyd-Graber et al., 2007). For example, consider the English word interest. In finance related articles, “interest” is more likely to carry the sense of “a share, right, or title in the ownership of property” (“利息” in Chinese), over other senses. Therefore, analysing the topic of the original article and selecting the translation with the same topic label might help disambiguate the word sense. For a target English word e, for each prospective Chinese sense c ∈ C, choose the first (in terms of relative frequency) sense that has the same news category as the containing webpage. 3.3 3.4 Approaches 3–5: Machine Translati"
W15-4406,W08-0336,0,0.0240103,"Missing"
W15-4406,volodina-etal-2014-flexible,0,0.0413565,"Missing"
W15-4406,W10-1002,0,0.0198174,"development of the system, we discovered two key components that can be affected by this context modeling. We report on these developments here. In specific, we propose improved algorithms for two components: (i) for translating English words to Chinese from news articles, (ii) for generating distractors for learner assessment. 2 The WordNews Chrome Extension Our method to directly enhance the web browser is inspired by earlier work in the computer-aided language learning community that also uses the web browser as the delivery vehicle for language learning. WERTi (Metcalf and Meurers, 2006; Meurers et al., 2010) was a monolingual, userdriven system that modified web pages in the target language to highlight or remove certain words from specific syntactic patterns to teach difficultto-learn English grammar. Our focus is to help build Chinese vocabulary for second language learners fluent in English. We give a running scenario to illustrate the use of WordNews. When a learner browses to an English webpage on a news website, our extension either selectively replaces certain original English words with their Chinese translation or underlines the English words, based on user configuration (Figure 1, middl"
W15-4406,pho-etal-2014-multiple,0,0.125616,"Missing"
W15-4406,W02-1012,0,0.038179,"rd appears as our context. We then acquire its translation from Microsoft Bing Translator using its API. As we access the translation as a third party, the Chinese translation comes as-is, without the needed explicit word to locate the target English word to translate in the original input sentence. We need to perform alignment of the Chinese and English sentences in order to recover the target word’s translation from the sentence translation. Approach 2: Part-of-Speech Part-of-Speech (POS) tags are also useful for word sense disambiguation (Wilks and Stevenson, 1998) and machine translation (Toutanova et al., 2002; Ueffing and Ney, 2003). For example, the English word “book” can function as a verb or a noun, which gives rise to two differ4 37 Table 3: WSD performance over our test set. Approach 3 – Substring Match. As potential Chinese translations are available in our dictionary, a straightforward use of substring matching recovers a Chinese translation; i.e., check whether the candidate Bing translation is a substring of the Chinese translation. If more than one candidate matches, we use the longest string match heuristic and pick the one with the longest match as the final output. If none matches, t"
W15-4406,N03-1033,0,0.00497122,"es ... verb: 关闭, 合, 关 ... adj: 密切, ... 亲密 ... verb: 停止, 站, 阻止, 停 ... adj: 免费, 自由, 游离, 畅, 空闲的... noun: 旅, 旅程 ... 旅游 ... 关闭 密切 Machine Translation Substring Relax Align 亲密 亲密 亲密 停止 阻止 停止 停止 停止 免费 免费 自由 自由 自由 旅 旅 旅 旅行 旅行 noun: 匹 配, 比 赛, 赛, 敌手, 对手, 火柴 ... noun: 态, 国, 州, ... verb: 声明, 陈述, 述, 申 明 ... 发言 ... adj: 国家的 ... 匹配 匹配 比赛 比赛 比赛 态 态 发言 发言 人 国家 ent dominant senses: “reserve” (“预定” in Chinese) and “printed work” (“书”), respectively. As senses often correspond cross-lingually, knowledge of the English word’s POS can assist disambiguation. We employ the Standford log-linear Part-of-Speech tagger (Toutanova et al., 2003) to obtain the POS tag for the English word, whereas the POS tag for target Chinese senses are provided in our dictionary. In cases where multiple candidate Chinese translations fit the same sense, we again break ties using relative frequency of the prospective candidates. This method has complete coverage over the CET 4 list (as the word frequency rule always yields a prospective translation), but as it lacks any context model, it is the least accurate. 3.2 Approach 1: News Category Topic information has been shown to be useful in WSD (Boyd-Graber et al., 2007). For example, consider the Engl"
W16-1511,W16-1512,0,0.075087,"learn the weights of different features with respect to the relevance of cited text spans and the relevance to a community-based summary. Two runs were submitted, using SUMMA [22] to score and extract all matched sentences and only the top sentences respectively. Lu et al. [13] regarded Task 1a as a ranking problem, applying Learning to Rank strategies. In contrast, the group cast Task 1b as a standard text classification problem, where novel feature engineering was the team’s focus. Along this vein, the group considered features of both citation contexts and cited spans. Aggarwal and Sharma [1] propose several heuristics derived from bigram overlap counts between citances and reference text to identify the reference text span for each citance. This score is used to rank and select sentences from the reference text as output. Baki et al. [18] used SVM with subset tree kernel, a type of convolution kernel. Computed similarities between three tree representations of the citance and reference text formed the convolution kernel. Their set-up scored better than their TF-IDF baseline method. They submitted three system runs with this approach. The PolyU system [2], for Task 1a, use SVM-ran"
W16-1511,W16-1515,0,0.0523171,"pans. Aggarwal and Sharma [1] propose several heuristics derived from bigram overlap counts between citances and reference text to identify the reference text span for each citance. This score is used to rank and select sentences from the reference text as output. Baki et al. [18] used SVM with subset tree kernel, a type of convolution kernel. Computed similarities between three tree representations of the citance and reference text formed the convolution kernel. Their set-up scored better than their TF-IDF baseline method. They submitted three system runs with this approach. The PolyU system [2], for Task 1a, use SVM-rank with lexical and document structural features to rank reference text sentences for every citance. Task 1b is solved using a decision tree classifier. Finally, they model summarization as a query–focussed summarization with citances as queries. They generate summaries (Task 2) by improvising on a Manifold Ranking method (see [2] for details). 97 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Finally, the system submitted by Conroy and Davis [4] attempted to solve Task 2 with an adaptation of a system developed"
W16-1511,W15-1525,0,0.0774646,"ey submitted three system runs with this approach. The PolyU system [2], for Task 1a, use SVM-rank with lexical and document structural features to rank reference text sentences for every citance. Task 1b is solved using a decision tree classifier. Finally, they model summarization as a query–focussed summarization with citances as queries. They generate summaries (Task 2) by improvising on a Manifold Ranking method (see [2] for details). 97 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Finally, the system submitted by Conroy and Davis [4] attempted to solve Task 2 with an adaptation of a system developed for the TAC 2014 BioMedSumm Task 11 . They provided the results from a simple vector space model, wherein they used a TF representation of the text and non- negative matrix factorization (NNMF) to estimate the latent weights of the terms for scientific document summarization. They also provide the results from two language models based on the distribution of words in human-written summaries. 6 System Runs Performance of systems for Task 1a was measured by the number of sentences output by the system that overlap with the sente"
W16-1511,C10-2049,1,0.909157,"ted that these selected facts would closely reflect the most important contributions and applications of the paper. These insights have been explored in a smaller scope by previous work. We propose that further explorations can help to advance the state of the art. Furthermore, we expect that the CL-SciSumm Task could spur the creation of new resources and tools, to automate the synthesis and updating of automatic summaries of CL research papers. Previous work in scientific summarization has attempted to automatically generate multi-document summaries by instantiating a hierarchical topic tree[6], generating model citation sentences[17] or implementing a literature review framework[8]. However, the limited availability of evaluation resources and humancreated summaries constrains research in this area. In 2014, the CL-SciSumm Pilot task was conducted as a part of the larger BioMedSumm Task at TAC 5 . In 2016, our proposal was not successful with ACL; fortunately it was accepted as a part of the BIRNDL workshop [15] at JCDL-20166 . The development and dissemination of the CL-SciSumm dataset and the related Shared Task has been generously supported by the Microsoft Research Asia (MSRA)"
W16-1511,W13-2116,1,0.954571,"reference paper are considered a synopses of its key points and also its key contributions and importance within an academic community [19]. The advantage of using citances is that they are embedded with meta-commentary and offer a contextual, interpretative layer to the cited text. The drawback, however, is that though a collection of citances offers a view of the cited paper, it does not consider the context of the target user [9] [24], verify the claim of the citation or provide context from the reference paper, in terms the type of information cited or where it is in the referenced paper [8]. CL-SciSumm explores summarization of scientific research, for the computational linguistics research domain. It encourages the incorporation of new kinds 93 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries of information in automatic scientific paper summarization, such as the facets of research information being summarized the research paper. Our previous task suggested that scholars in CL typically cite methods information from other papers. CL-SciSumm also encourages the use of citing mini-summaries written in other papers, by other s"
W16-1511,W16-1514,0,0.0369046,"raining data comprising of triples of citance, the true reference and the set of false references for the citance. Sentence selection was based on a dissimilarity score similar to MMR. Mao et al. [11] used an SVM classifier with a topical lexicon to identify the best matching reference spans for a citance, using ifd similarity, Jaccard similarity and context similarity. They finally submitted six system runs, each following a variant of similarities and approaches - the fusion method, the Jaccard Cascade method, the Jaccard Focused method, the SVM method and two voting methods. Klampfl et al. [10] developed three different approaches based on summarization and classification techniques. They applied a modified version of an unsupervised summarization technique, termed it TextSentenceRank, to the reference document. Their second method incorporates similarities of sentences to the citation on a textual level, and employed classification to select from candidates previously extracted through the original TextSentenceRank algorithm. Their third method used unsupervised summarization of the relevant sub-part of the document that was previously selected in a supervised manner. Saggion et al"
W16-1511,W16-1518,0,0.0969034,"ught new information to the summary. 8 9 10 http://knowtator.sourceforge.net/ http://protege.stanford.edu/about.php github.com/WING-NUS/scisumm-corpus 96 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Nomoto [20] proposed a hybrid model for Task 2, comprising TFIDF and a tripartite neural network. Stochastic gradient descent was performed on a training data comprising of triples of citance, the true reference and the set of false references for the citance. Sentence selection was based on a dissimilarity score similar to MMR. Mao et al. [11] used an SVM classifier with a topical lexicon to identify the best matching reference spans for a citance, using ifd similarity, Jaccard similarity and context similarity. They finally submitted six system runs, each following a variant of similarities and approaches - the fusion method, the Jaccard Cascade method, the Jaccard Focused method, the SVM method and two voting methods. Klampfl et al. [10] developed three different approaches based on summarization and classification techniques. They applied a modified version of an unsupervised summarization technique, termed it TextSentenceRank,"
W16-1511,W16-1516,0,0.0269325,"tSentenceRank algorithm. Their third method used unsupervised summarization of the relevant sub-part of the document that was previously selected in a supervised manner. Saggion et al. [23] reported their results for the linear regression implementation of WEKA used together with the GATE system. They trained their model to learn the weights of different features with respect to the relevance of cited text spans and the relevance to a community-based summary. Two runs were submitted, using SUMMA [22] to score and extract all matched sentences and only the top sentences respectively. Lu et al. [13] regarded Task 1a as a ranking problem, applying Learning to Rank strategies. In contrast, the group cast Task 1b as a standard text classification problem, where novel feature engineering was the team’s focus. Along this vein, the group considered features of both citation contexts and cited spans. Aggarwal and Sharma [1] propose several heuristics derived from bigram overlap counts between citances and reference text to identify the reference text span for each citance. This score is used to rank and select sentences from the reference text as output. Baki et al. [18] used SVM with subset tr"
W16-1511,W16-1517,0,0.104492,"orpus. We alerted the participating teams to this mistake and requested them not to use that information for training their systems. 5 Overview of Approaches The following paragraphs discuss the approaches followed by the participating systems, in no particular order. Except for the top performing systems in each of the sub-tasks, we do not provide detailed relative performance information for each system, in this paper. The evaluation scripts have been provided at the CL- SciSumm Github respository 10 where the participants may run their own evaluation and report the results. The approach by [14] used the Transdisciplinary Scientific Lexicon (TSL) developed by [5] to build a profile for each discourse facet in citances and reference spans. Then a similarity function developed by [16] was used to select the best-matching reference span with the same facet as the citance. For Task 2, the authors used Maximal Marginal Relevance [3] to choose sentences so that they brought new information to the summary. 8 9 10 http://knowtator.sourceforge.net/ http://protege.stanford.edu/about.php github.com/WING-NUS/scisumm-corpus 96 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrie"
W16-1511,W16-1500,0,0.0834567,"s of CL research papers. Previous work in scientific summarization has attempted to automatically generate multi-document summaries by instantiating a hierarchical topic tree[6], generating model citation sentences[17] or implementing a literature review framework[8]. However, the limited availability of evaluation resources and humancreated summaries constrains research in this area. In 2014, the CL-SciSumm Pilot task was conducted as a part of the larger BioMedSumm Task at TAC 5 . In 2016, our proposal was not successful with ACL; fortunately it was accepted as a part of the BIRNDL workshop [15] at JCDL-20166 . The development and dissemination of the CL-SciSumm dataset and the related Shared Task has been generously supported by the Microsoft Research Asia (MSRA) Research Grant 2016. 2 Task Given: A topic consisting of a Reference Paper (RP) and up to ten Citing Papers (CPs) that all contain citations to the RP. In each CP, the text spans (i.e., citances) have been identified that pertain to a particular citation to the RP. Task 1A: For each citance, identify the spans of text (cited text spans) in the RP that most accurately reflect the citance. These are of the granularity of a se"
W16-1511,N09-1066,0,0.0253108,"ely reflect the most important contributions and applications of the paper. These insights have been explored in a smaller scope by previous work. We propose that further explorations can help to advance the state of the art. Furthermore, we expect that the CL-SciSumm Task could spur the creation of new resources and tools, to automate the synthesis and updating of automatic summaries of CL research papers. Previous work in scientific summarization has attempted to automatically generate multi-document summaries by instantiating a hierarchical topic tree[6], generating model citation sentences[17] or implementing a literature review framework[8]. However, the limited availability of evaluation resources and humancreated summaries constrains research in this area. In 2014, the CL-SciSumm Pilot task was conducted as a part of the larger BioMedSumm Task at TAC 5 . In 2016, our proposal was not successful with ACL; fortunately it was accepted as a part of the BIRNDL workshop [15] at JCDL-20166 . The development and dissemination of the CL-SciSumm dataset and the related Shared Task has been generously supported by the Microsoft Research Asia (MSRA) Research Grant 2016. 2 Task Given: A topi"
W16-1511,W16-1513,0,0.0702932,"respectively. Lu et al. [13] regarded Task 1a as a ranking problem, applying Learning to Rank strategies. In contrast, the group cast Task 1b as a standard text classification problem, where novel feature engineering was the team’s focus. Along this vein, the group considered features of both citation contexts and cited spans. Aggarwal and Sharma [1] propose several heuristics derived from bigram overlap counts between citances and reference text to identify the reference text span for each citance. This score is used to rank and select sentences from the reference text as output. Baki et al. [18] used SVM with subset tree kernel, a type of convolution kernel. Computed similarities between three tree representations of the citance and reference text formed the convolution kernel. Their set-up scored better than their TF-IDF baseline method. They submitted three system runs with this approach. The PolyU system [2], for Task 1a, use SVM-rank with lexical and document structural features to rank reference text sentences for every citance. Task 1b is solved using a decision tree classifier. Finally, they model summarization as a query–focussed summarization with citances as queries. They g"
W16-1511,W16-1519,0,0.11119,"xicon (TSL) developed by [5] to build a profile for each discourse facet in citances and reference spans. Then a similarity function developed by [16] was used to select the best-matching reference span with the same facet as the citance. For Task 2, the authors used Maximal Marginal Relevance [3] to choose sentences so that they brought new information to the summary. 8 9 10 http://knowtator.sourceforge.net/ http://protege.stanford.edu/about.php github.com/WING-NUS/scisumm-corpus 96 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Nomoto [20] proposed a hybrid model for Task 2, comprising TFIDF and a tripartite neural network. Stochastic gradient descent was performed on a training data comprising of triples of citance, the true reference and the set of false references for the citance. Sentence selection was based on a dissimilarity score similar to MMR. Mao et al. [11] used an SVM classifier with a topical lexicon to identify the best matching reference spans for a citance, using ifd similarity, Jaccard similarity and context similarity. They finally submitted six system runs, each following a variant of similarities and approac"
W16-1511,C08-1087,0,0.0317216,"c-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016), held in New Jersey,USA in June, 2016. The annotated dataset used for this shared task and the scripts used for evaluation can be accessed and used by the community at: https://github.com/WING-NUS/scisumm-corpus. 1 Introduction The CL-SciSumm task provides resources to encourage research in a promising direction of scientific paper summarization, which considers the set of citation sentences (i.e., “citances”) that reference a specific paper as a (community created) summary of a topic or paper [21]. Citances for a reference paper are considered a synopses of its key points and also its key contributions and importance within an academic community [19]. The advantage of using citances is that they are embedded with meta-commentary and offer a contextual, interpretative layer to the cited text. The drawback, however, is that though a collection of citances offers a view of the cited paper, it does not consider the context of the target user [9] [24], verify the claim of the citation or provide context from the reference paper, in terms the type of information cited or where it is in the r"
W16-1511,W16-1520,0,0.196291,"eveloped three different approaches based on summarization and classification techniques. They applied a modified version of an unsupervised summarization technique, termed it TextSentenceRank, to the reference document. Their second method incorporates similarities of sentences to the citation on a textual level, and employed classification to select from candidates previously extracted through the original TextSentenceRank algorithm. Their third method used unsupervised summarization of the relevant sub-part of the document that was previously selected in a supervised manner. Saggion et al. [23] reported their results for the linear regression implementation of WEKA used together with the GATE system. They trained their model to learn the weights of different features with respect to the relevance of cited text spans and the relevance to a community-based summary. Two runs were submitted, using SUMMA [22] to score and extract all matched sentences and only the top sentences respectively. Lu et al. [13] regarded Task 1a as a ranking problem, applying Learning to Rank strategies. In contrast, the group cast Task 1b as a standard text classification problem, where novel feature engineer"
W16-1511,J02-4002,0,0.130508,"nsiders the set of citation sentences (i.e., “citances”) that reference a specific paper as a (community created) summary of a topic or paper [21]. Citances for a reference paper are considered a synopses of its key points and also its key contributions and importance within an academic community [19]. The advantage of using citances is that they are embedded with meta-commentary and offer a contextual, interpretative layer to the cited text. The drawback, however, is that though a collection of citances offers a view of the cited paper, it does not consider the context of the target user [9] [24], verify the claim of the citation or provide context from the reference paper, in terms the type of information cited or where it is in the referenced paper [8]. CL-SciSumm explores summarization of scientific research, for the computational linguistics research domain. It encourages the incorporation of new kinds 93 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries of information in automatic scientific paper summarization, such as the facets of research information being summarized the research paper. Our previous task suggested that sch"
W16-4905,D14-1110,0,0.0601628,"Missing"
W16-4905,W15-4406,1,0.814045,"mpts to use neural networks for WSD may have run into the same problem. Taghipour and Ng (2015b) indicated the need to prevent overfitting while using a neural network to adapt C&W embeddings by omitting a hidden layer and adding a Dropout layer, while K˚ageb¨ack and Salomonsson (2016) developed a new regularization technique in their work. 4 English-Chinese Cross-Lingual Word Sense Disambiguation We now evaluate our proposal on the Cross-Lingual Word Sense Disambiguation task. One key application of such task is to facilitate language learning systems. For example, MindTheWord4 and WordNews (Chen et al., 2015) are two applications that allow users to learn vocabulary of a second language in context, in the form of providing translations of words in an online article. In this work, we model this problem of finding translations of words as a variant of WSD, Cross-Lingual Word Sense Disambiguation, as formalized in (Chen et al., 2015). In the previous section, we have validated and compared enhancements to IMS using word embeddings. These have produced results comparable to, and in some cases, better than state-of-the-art performance on the monolingual WSD tasks. We further evaluate our approach for u"
W16-4905,P16-1085,0,0.230546,"an the Rank 1 systems in many tasks. As word embeddings with higher dimensions increases the feature space of IMS, this may lead to overfitting on some datasets. We believe, this is why a smaller number of dimensions work better in the LS tasks. However, as seen in Table 3, this effect was not observed in the AW task. We also note that relatively poorer performance in the LS tasks may not necessarily result in poor performance in the AW task. We see from the results that the combination of (Taghipour and Ng, 2015b)’s scaling strategy and summation produced results better than the proposal in (Iacobacci et al., 2016) to concatenate and average (0.651 and 0.654), suggesting that the scaling factor is important for the integration of word embeddings for supervised WSD. 3.1 LSTM Network A Long Short Term Memory (LSTM) network is a type of Recurrent Neural Network which has recently been shown to have good performance on many NLP classification tasks. The potential benefit of using an approach using LSTM over our existing approach in IMS is this is that an LSTM network is able to use more information about the sequence of words. For WSD, K˚ageb¨ack & Salomonsson (2016) explored the use of bidirectional LSTMs."
W16-4905,W16-5307,0,0.0238192,"Missing"
W16-4905,S01-1004,0,0.0105118,"ate the performance of our classifiers on monolingual WSD. We do this to first validate that our approach is a sound approach of performing WSD, showing improved or identical scores to state-of-the-art systems in most tasks. Similar to the work by Taghipour (2015b), we experimented with the use of word embeddings as feature types in IMS. However, we did not just experiment using C&W embeddings, as different word embeddings are known to vary in quality when evaluated on different tasks (Schnabel et al., 2015). We performed evaluation on several tasks. For the Lexical Sample (LS) tasks of SE-2 (Kilgarriff, 2001) and SE-3 (Mihalcea et al., 2004), we evaluated our system using fine-grained scoring. For the All Words (AW) tasks, fine-grained scoring is done for SE-2 (Palmer et al., 2001) and SE-3 (Snyder and Palmer, 2004); both the fine (Pradhan et al., 2007) and coarse-grained were used in (Navigli et al., 2007) AW tasks in SE-2007. In order to evaluate our features on the AW task, we trained IMS and the different combinations of features on the One Million Sense-Tagged corpus (Taghipour and Ng, 2015a). To compose word vectors, one method (used as a baseline) is to sum up the word vectors of the words"
W16-4905,S10-1003,0,0.0380591,"as a single sense. In some WSD tasks in SemEval, coarse-grained scoring was done in order to deal with the problem of reliably distinguishing fine-grained senses. 2.1 Cross-Lingual Word Sense Disambiguation Cross-Lingual WSD was, in part, conceived as a further attempt to solve this issue. In Cross-Lingual WSD, the specificity of a sense is determined by its correct translation in another language. The sense inventory is the possible translations of each word in another language. Two instances are said to have the same sense if they map to the same translation in that language. SemEval-2010 (Lefever and Hoste, 2010)2 and SemEval-2013 (Lefever and Hoste, 2013)3 featured iterations of this task. These tasks featured English nouns as the source words, and word senses as translations in Dutch, French, Italian, Spanish and German. Traditional WSD approaches are used in Cross-Lingual WSD, although some approaches leverage statistical machine translation (SMT) methods and features from translation. Cross-Lingual WSD involves training by making use of parallel or multilingual corpora. In the Cross-Lingual WSD task in SemEval-2013, the top performing approaches used either classification or SMT approaches. 2.2 WS"
W16-4905,S13-2029,0,0.0358057,"Eval, coarse-grained scoring was done in order to deal with the problem of reliably distinguishing fine-grained senses. 2.1 Cross-Lingual Word Sense Disambiguation Cross-Lingual WSD was, in part, conceived as a further attempt to solve this issue. In Cross-Lingual WSD, the specificity of a sense is determined by its correct translation in another language. The sense inventory is the possible translations of each word in another language. Two instances are said to have the same sense if they map to the same translation in that language. SemEval-2010 (Lefever and Hoste, 2010)2 and SemEval-2013 (Lefever and Hoste, 2013)3 featured iterations of this task. These tasks featured English nouns as the source words, and word senses as translations in Dutch, French, Italian, Spanish and German. Traditional WSD approaches are used in Cross-Lingual WSD, although some approaches leverage statistical machine translation (SMT) methods and features from translation. Cross-Lingual WSD involves training by making use of parallel or multilingual corpora. In the Cross-Lingual WSD task in SemEval-2013, the top performing approaches used either classification or SMT approaches. 2.2 WSD with Word Embeddings In NLP, words can be"
W16-4905,W04-0807,0,0.0618468,"lassifiers on monolingual WSD. We do this to first validate that our approach is a sound approach of performing WSD, showing improved or identical scores to state-of-the-art systems in most tasks. Similar to the work by Taghipour (2015b), we experimented with the use of word embeddings as feature types in IMS. However, we did not just experiment using C&W embeddings, as different word embeddings are known to vary in quality when evaluated on different tasks (Schnabel et al., 2015). We performed evaluation on several tasks. For the Lexical Sample (LS) tasks of SE-2 (Kilgarriff, 2001) and SE-3 (Mihalcea et al., 2004), we evaluated our system using fine-grained scoring. For the All Words (AW) tasks, fine-grained scoring is done for SE-2 (Palmer et al., 2001) and SE-3 (Snyder and Palmer, 2004); both the fine (Pradhan et al., 2007) and coarse-grained were used in (Navigli et al., 2007) AW tasks in SE-2007. In order to evaluate our features on the AW task, we trained IMS and the different combinations of features on the One Million Sense-Tagged corpus (Taghipour and Ng, 2015a). To compose word vectors, one method (used as a baseline) is to sum up the word vectors of the words in the surrounding context or sen"
W16-4905,S07-1006,0,0.0392206,"ord embeddings as feature types in IMS. However, we did not just experiment using C&W embeddings, as different word embeddings are known to vary in quality when evaluated on different tasks (Schnabel et al., 2015). We performed evaluation on several tasks. For the Lexical Sample (LS) tasks of SE-2 (Kilgarriff, 2001) and SE-3 (Mihalcea et al., 2004), we evaluated our system using fine-grained scoring. For the All Words (AW) tasks, fine-grained scoring is done for SE-2 (Palmer et al., 2001) and SE-3 (Snyder and Palmer, 2004); both the fine (Pradhan et al., 2007) and coarse-grained were used in (Navigli et al., 2007) AW tasks in SE-2007. In order to evaluate our features on the AW task, we trained IMS and the different combinations of features on the One Million Sense-Tagged corpus (Taghipour and Ng, 2015a). To compose word vectors, one method (used as a baseline) is to sum up the word vectors of the words in the surrounding context or sentence. We primarily experimented on this method of composition, due to its good performance and short training time. For this, every word vector for every lemma in the sentence (exclusive of the target word) was summed into a context vector, resulting in d features. Stop"
W16-4905,J03-1002,0,0.00739147,"Missing"
W16-4905,S01-1005,0,0.0386746,"l scores to state-of-the-art systems in most tasks. Similar to the work by Taghipour (2015b), we experimented with the use of word embeddings as feature types in IMS. However, we did not just experiment using C&W embeddings, as different word embeddings are known to vary in quality when evaluated on different tasks (Schnabel et al., 2015). We performed evaluation on several tasks. For the Lexical Sample (LS) tasks of SE-2 (Kilgarriff, 2001) and SE-3 (Mihalcea et al., 2004), we evaluated our system using fine-grained scoring. For the All Words (AW) tasks, fine-grained scoring is done for SE-2 (Palmer et al., 2001) and SE-3 (Snyder and Palmer, 2004); both the fine (Pradhan et al., 2007) and coarse-grained were used in (Navigli et al., 2007) AW tasks in SE-2007. In order to evaluate our features on the AW task, we trained IMS and the different combinations of features on the One Million Sense-Tagged corpus (Taghipour and Ng, 2015a). To compose word vectors, one method (used as a baseline) is to sum up the word vectors of the words in the surrounding context or sentence. We primarily experimented on this method of composition, due to its good performance and short training time. For this, every word vecto"
W16-4905,D14-1162,0,0.0852648,"ingual WSD task in SemEval-2013, the top performing approaches used either classification or SMT approaches. 2.2 WSD with Word Embeddings In NLP, words can be represented with a distributed representation, such as word embeddings, which encodes words into a low dimensional space. In word embeddings, information about a word is distributed across multiple dimensions, and similar words are expected to be close to each other in the vector space. Examples of word embeddings are Continuous Bag of Words (Mikolov et al., 2013), Collobert & Weston’s Embeddings (Collobert and Weston, 2008), and GLoVe (Pennington et al., 2014). We implemented and evaluated the use of word embedding features using these three embeddings in IMS. An unsupervised approach using word embeddings for WSD is described by Chen (2014). This approach finds representation of senses, instead of words, and computes a context vector which is used during disambiguation. A different approach is to work on extending existing WSD systems. Turian (2010) suggests that for any existing supervised NLP system, a general way of improving accuracy would be to use unsupervised word representations as additional features. Taghipour (2015b) used C&W embeddings"
W16-4905,S07-1016,0,0.0300807,"by Taghipour (2015b), we experimented with the use of word embeddings as feature types in IMS. However, we did not just experiment using C&W embeddings, as different word embeddings are known to vary in quality when evaluated on different tasks (Schnabel et al., 2015). We performed evaluation on several tasks. For the Lexical Sample (LS) tasks of SE-2 (Kilgarriff, 2001) and SE-3 (Mihalcea et al., 2004), we evaluated our system using fine-grained scoring. For the All Words (AW) tasks, fine-grained scoring is done for SE-2 (Palmer et al., 2001) and SE-3 (Snyder and Palmer, 2004); both the fine (Pradhan et al., 2007) and coarse-grained were used in (Navigli et al., 2007) AW tasks in SE-2007. In order to evaluate our features on the AW task, we trained IMS and the different combinations of features on the One Million Sense-Tagged corpus (Taghipour and Ng, 2015a). To compose word vectors, one method (used as a baseline) is to sum up the word vectors of the words in the surrounding context or sentence. We primarily experimented on this method of composition, due to its good performance and short training time. For this, every word vector for every lemma in the sentence (exclusive of the target word) was summ"
W16-4905,P15-1173,0,0.0753396,"Missing"
W16-4905,D15-1036,0,0.0269412,"s from Senseval-2 (hereafter SE-2), Senseval-3 (hereafter SE-3) and SemEval-2007 (hereafter SE-2007) to evaluate the performance of our classifiers on monolingual WSD. We do this to first validate that our approach is a sound approach of performing WSD, showing improved or identical scores to state-of-the-art systems in most tasks. Similar to the work by Taghipour (2015b), we experimented with the use of word embeddings as feature types in IMS. However, we did not just experiment using C&W embeddings, as different word embeddings are known to vary in quality when evaluated on different tasks (Schnabel et al., 2015). We performed evaluation on several tasks. For the Lexical Sample (LS) tasks of SE-2 (Kilgarriff, 2001) and SE-3 (Mihalcea et al., 2004), we evaluated our system using fine-grained scoring. For the All Words (AW) tasks, fine-grained scoring is done for SE-2 (Palmer et al., 2001) and SE-3 (Snyder and Palmer, 2004); both the fine (Pradhan et al., 2007) and coarse-grained were used in (Navigli et al., 2007) AW tasks in SE-2007. In order to evaluate our features on the AW task, we trained IMS and the different combinations of features on the One Million Sense-Tagged corpus (Taghipour and Ng, 2015"
W16-4905,W04-0811,0,0.0534986,"stems in most tasks. Similar to the work by Taghipour (2015b), we experimented with the use of word embeddings as feature types in IMS. However, we did not just experiment using C&W embeddings, as different word embeddings are known to vary in quality when evaluated on different tasks (Schnabel et al., 2015). We performed evaluation on several tasks. For the Lexical Sample (LS) tasks of SE-2 (Kilgarriff, 2001) and SE-3 (Mihalcea et al., 2004), we evaluated our system using fine-grained scoring. For the All Words (AW) tasks, fine-grained scoring is done for SE-2 (Palmer et al., 2001) and SE-3 (Snyder and Palmer, 2004); both the fine (Pradhan et al., 2007) and coarse-grained were used in (Navigli et al., 2007) AW tasks in SE-2007. In order to evaluate our features on the AW task, we trained IMS and the different combinations of features on the One Million Sense-Tagged corpus (Taghipour and Ng, 2015a). To compose word vectors, one method (used as a baseline) is to sum up the word vectors of the words in the surrounding context or sentence. We primarily experimented on this method of composition, due to its good performance and short training time. For this, every word vector for every lemma in the sentence ("
W16-4905,K15-1037,0,0.127134,"individual words in the context surrounding the target word, 2) specific ordered sequences of words appearing at specified offsets from the target word, and 3) Part-Of-Speech tags of the surrounding three words. Each of the features are binary features, and IMS trains a model for each word. IMS then uses an support vector machine (SVM) for classification. IMS is open-source, provides state-of-the-art performance, and is easy to extend. As such, our work features IMS and extends off of this backbone. Training data is required to train IMS. We make use of the One-Million Sense-Tagged Instances (Taghipour and Ng, 2015a) dataset, which is the largest dataset we know of for training WSD systems, in training our systems for the All Words tasks. WSD systems can be evaluated using either fine-grained scoring or coarse-grained scoring. Under fine-grained scoring, every sense is equally distinct from each other, and answers must exactly match. WordNet is often used as the sense inventory for monolingual WSD tasks. However, WordNet is a finegrained resource, and even human annotators have trouble distinguishing between different senses of a word (Edmonds and Kilgarriff, 2002). In contrast, under coarse-grained sco"
W16-4905,N15-1035,0,0.0428626,"Missing"
W16-4905,tian-etal-2014-um,0,0.0410587,"Missing"
W16-4905,P10-1040,0,0.0429376,"e word. WSD is often considered a classification task, in which the classifier predicts the sense from a possible set of senses, known as a sense inventory, given the target word and the contextual information of the target word. Existing WSD systems can be categorised into either data-driven supervised or knowledge-rich approaches. Both approaches are considered to be complementary to each other. Word embeddings have become a popular word representation formalism, and many tasks can be done using word embeddings. The effectiveness of using word embeddings has been shown in several NLP tasks (Turian et al., 2010). The goal of our work is to apply and comprehensively compare different uses of word embeddings, solely with respect to WSD. We perform evaluation of the effectiveness of word embeddings on monolingual WSD tasks from Senseval-2 (held in 2001), Senseval-3 (held in 2004), and SemEval-2007. After which, we evaluate our approach on English–Chinese Cross-Lingual WSD using a dataset that we constructed for evaluating our approach on the translation task used in educational applications for language learning. 2 Related Work Word Sense Disambiguation is a well-studied problem, in which many methods h"
W16-4905,P10-4014,0,0.153093,"is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ 1 Licence details: http:// http://wordnetweb.princeton.edu/perl/webwn?s=bank 30 Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications, pages 30–39, Osaka, Japan, December 12 2016. such as WordNet (Miller, 1995). It is noted by Navigli (2009) that supervised approaches using memorybased learning and SVM approaches have worked best. Supervised approaches involve the extraction of features and then classification using machine learning. Zhong and Ng (2010) developed an open-source WSD system, ItMakesSense (hereafter, IMS), which was considered the state-of-the-art at the time it was developed. It is a supervised WSD system, which had to be trained prior to use. IMS uses three feature types: 1) individual words in the context surrounding the target word, 2) specific ordered sequences of words appearing at specified offsets from the target word, and 3) Part-Of-Speech tags of the surrounding three words. Each of the features are binary features, and IMS trains a model for each word. IMS then uses an support vector machine (SVM) for classification."
W17-5217,W14-4012,0,0.0227136,"Missing"
W17-5217,W16-6105,0,0.121769,"Choudhury et al., 2013a). They investigate the use of several linguistic features (choice of negative words in tweet, increased medicinal words), as well as other social features (e.g., egonetwork) to accomplish the task. However such social features are often not available in case of online health forums. In the absence of explicit signals by the users (e.g., ‘mood’), the textual features can be indicative of one’s emotional status. There have been efforts from the intersection of biomedical, and NLP community to understand and analyze the textual contents users post in online health forums (Rey-Villamizar et al., 2016; Gkotsis et al., 2016; Paul and Dredze, 2011; Sadeque et al., 2016). After studying the patient community of dailystrength.org, Rey-Villamizar et al. found that on an average, the anxiety levels of patients in the community lower over time (Rey-Villamizar et al., 2016). Although they spot a global trend at the community level, there is a definite need to model the dynamics of users’ emotional status over time. Sadeque et al. consider a user’s linguistic and timeline features to predict whether a user will withdraw from the forum completely (Sadeque et al., 2016). In con• A systematic investig"
W17-5217,W16-6203,0,0.0797755,"eatures (choice of negative words in tweet, increased medicinal words), as well as other social features (e.g., egonetwork) to accomplish the task. However such social features are often not available in case of online health forums. In the absence of explicit signals by the users (e.g., ‘mood’), the textual features can be indicative of one’s emotional status. There have been efforts from the intersection of biomedical, and NLP community to understand and analyze the textual contents users post in online health forums (Rey-Villamizar et al., 2016; Gkotsis et al., 2016; Paul and Dredze, 2011; Sadeque et al., 2016). After studying the patient community of dailystrength.org, Rey-Villamizar et al. found that on an average, the anxiety levels of patients in the community lower over time (Rey-Villamizar et al., 2016). Although they spot a global trend at the community level, there is a definite need to model the dynamics of users’ emotional status over time. Sadeque et al. consider a user’s linguistic and timeline features to predict whether a user will withdraw from the forum completely (Sadeque et al., 2016). In con• A systematic investigation of the temporal progression of emotional status across users f"
W17-5217,W16-0307,0,0.0546416,"y investigate the use of several linguistic features (choice of negative words in tweet, increased medicinal words), as well as other social features (e.g., egonetwork) to accomplish the task. However such social features are often not available in case of online health forums. In the absence of explicit signals by the users (e.g., ‘mood’), the textual features can be indicative of one’s emotional status. There have been efforts from the intersection of biomedical, and NLP community to understand and analyze the textual contents users post in online health forums (Rey-Villamizar et al., 2016; Gkotsis et al., 2016; Paul and Dredze, 2011; Sadeque et al., 2016). After studying the patient community of dailystrength.org, Rey-Villamizar et al. found that on an average, the anxiety levels of patients in the community lower over time (Rey-Villamizar et al., 2016). Although they spot a global trend at the community level, there is a definite need to model the dynamics of users’ emotional status over time. Sadeque et al. consider a user’s linguistic and timeline features to predict whether a user will withdraw from the forum completely (Sadeque et al., 2016). In con• A systematic investigation of the temporal"
W17-5217,W14-3207,0,\N,Missing
W18-2504,W12-3202,0,0.726029,"handled through volunteer efforts coordinated by the Anthology editor. Running a key service for the computational linguistics community that needs to be continuously available and updated frequently is one of the main issues in administering the Anthology. We discuss this issue along with the challenges of running a large scale project on a volunteer basis and its resulting technical debt. As we look towards the future, previous research has shown that it can also be used as a data source to characterize the work and workings of the ACL community (Bird et al., 2008; Vogel and Jurafsky, 2012; Anderson et al., 2012). Extensions to the Anthology that build on this information could make the Anthology an even more valuable resource for the community. We will discuss two possibl eextensions – anonymous pre-prints and support for finding relevant submission reviewers by linking au2 https://creativecommons.org/licenses/ by-nc-sa/3.0/ 3 https://creativecommons.org/licenses/ by/4.0/ thors in the Anthology with their research interests and community connections. Beyond being useful in itself, work on such challenges has the potential to motivate the ACL community to further support the Anthology. 2 Current State"
W18-2504,bird-etal-2008-acl,1,0.930936,"e maintenance of the code and the website is handled through volunteer efforts coordinated by the Anthology editor. Running a key service for the computational linguistics community that needs to be continuously available and updated frequently is one of the main issues in administering the Anthology. We discuss this issue along with the challenges of running a large scale project on a volunteer basis and its resulting technical debt. As we look towards the future, previous research has shown that it can also be used as a data source to characterize the work and workings of the ACL community (Bird et al., 2008; Vogel and Jurafsky, 2012; Anderson et al., 2012). Extensions to the Anthology that build on this information could make the Anthology an even more valuable resource for the community. We will discuss two possibl eextensions – anonymous pre-prints and support for finding relevant submission reviewers by linking au2 https://creativecommons.org/licenses/ by-nc-sa/3.0/ 3 https://creativecommons.org/licenses/ by/4.0/ thors in the Anthology with their research interests and community connections. Beyond being useful in itself, work on such challenges has the potential to motivate the ACL community"
W18-2504,buitelaar-etal-2014-hot,0,0.0232717,"institution, contributors that work on Anthology-related system administration and development tasks have been recruited in response to calls for volunteers at the main ACL conferences. In contrast, new features have been developed by researchers using the ACL Anthology as a resource in their own work, unconnected with the daily operation of the Anthology. Such research deliverables include, for example, the creation of a corpus of research papers (Bird et al., 2008), an author citation network (Radev et al., 2013) or a 4 https://www.softconf.com/ faceted search engine (Sch¨afer et al., 2012; Buitelaar et al., 2014). These factors, in combination with the multiple, changing responsibilities and shifting research interests of community members, mean that new volunteers join and leave the Anthology team in unpredictable and sporadic patterns. Preserving knowledge about the Anthology’s operational workflow is thus one of the most important challenges for the Anthology. The Anthology editor has played a key role ensuring the continuity of the entire project. This position has so far always been filled for multiple years, longer than the normal time frame for an ACL officer. The role has been critical in ensu"
W18-2504,W12-3209,1,0.776602,"lenge of reviewer matching we encourage the community to rally towards. 1 Introduction The ACL Anthology1 is a service offered by the Association for Computational Linguistics (ACL) allowing open access to the proceedings of all ACL sponsored conferences and journal articles. As a community goodwill gesture, it also hosts third-party computational linguistics literature from sister organizations and their national venues. It offers both text and faceted search of the indexed papers, author-specific pages, and can incorporate third-party metadata and services that can be embedded within pages (Bysani and Kan, 2012). As of this paper, it hosts over 1 https://aclanthology.info/ Min-Yen Kan School of Computing National University of Singapore kanmy@comp.nus.edu.sg 43,000 computational linguistics and natural language processing papers, along with their metadata. Over 4,500 daily requests are served by the Anthology. The code for the Anthology is available at https://github.com/acl-org/ acl-anthology under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License2 . Slightly different from the Anthology source code, ACL also licenses its papers with a more liberal license, supporti"
W18-2504,P11-4002,0,0.0607328,"Missing"
W18-2504,W12-3210,0,0.0498541,"Missing"
W18-2504,W12-3204,0,0.606146,"e code and the website is handled through volunteer efforts coordinated by the Anthology editor. Running a key service for the computational linguistics community that needs to be continuously available and updated frequently is one of the main issues in administering the Anthology. We discuss this issue along with the challenges of running a large scale project on a volunteer basis and its resulting technical debt. As we look towards the future, previous research has shown that it can also be used as a data source to characterize the work and workings of the ACL community (Bird et al., 2008; Vogel and Jurafsky, 2012; Anderson et al., 2012). Extensions to the Anthology that build on this information could make the Anthology an even more valuable resource for the community. We will discuss two possibl eextensions – anonymous pre-prints and support for finding relevant submission reviewers by linking au2 https://creativecommons.org/licenses/ by-nc-sa/3.0/ 3 https://creativecommons.org/licenses/ by/4.0/ thors in the Anthology with their research interests and community connections. Beyond being useful in itself, work on such challenges has the potential to motivate the ACL community to further support the An"
W18-3720,P14-1141,0,0.0702851,"Missing"
W18-5602,D14-1181,0,0.01027,"s a great amount of correlation analysis when applied to a novel problem or dataset. Linguistic features also often fail to fully capture document content, as most do not account for distinctive words in exchange of scalability. Its counter parts, bag of words and per-vocabulary features loosely capture textual content but disregard semantics and suffer scalability with sparsity issues. To address this, state-of-the-art architectures feature complex modeling to model subtle dependencies and rely on word embeddings to address scalability issues, achieving robust results in text classification (Kim, 2014), neural machine translation (Luong et al., 2016), among others. Inspired by the success of their approaches, we adopt the recurrent neural network architecture (RNN) for post content modeling. Coupled with an attention mechanism, our approach adaptively weights the importance of parts in each post (Luong et al., 2015). Thread content modeling. Most research working on thread-level modeling usually obtain thread content representation by aggregating each content of its posts (Yang et al., 2014). However, we hypothesize that each post has different contribution to thread content and should be v"
W18-5602,W10-1915,0,0.0398188,"A common technique is to apply Named Entity Recognition (NER) and Relation Extraction (RE) systems in a supervised manner. (Sampathkumar et al., 2014) demonstrates its effectiveness in detecting drugs and side effects that appear in a target document (in-context), and predicting if they are related. However, in our side effect prediction during treatment, our model is required to cover potentially encountered reactions, many of which are not explicitly mentioned in the given post (outof-context). Hence, we do not identify our task with traditional task of adverse drug side effect extraction (Leaman et al., 2010). Our approach overcomes the limitations of the existing works by modeling user experience, and credibility during post and thread encoding, then subsequently predicting both in- and out-of-context side effects. 3 Task Definition. Drug side effect prediction during treatment is the task of assigning the most relevant subset of side effects to threads discussing certain treatment, from a large collection of potential side effects. We view the drug side effect prediction problem as a multi-label classification task. In our setting, an instance of item–label is a tuple (xt , y) where xt is the fe"
W18-5602,P15-1033,0,0.0185933,"re not included in the pre-trained embeddings but are critical to predict the side effects. We employ Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to encode the textual content. A bi-directional LSTM encodes the word vector sequence, outputting two sequences of hidden states: a forward sequence, H f = hf1 , hf2 , . . . , hfn that starts from the beginning of the text; and a backward sequence, H b = hb1 , hb2 , . . . , hbn that starts from the end of the text. For many sequence encoding tasks, knowing both past (left) and future (right) contexts has proven to be beneficial (Dyer et al., 2015). The states hfi and hbj in the forward and backward sequences are computed as follows: Table 3: Dataset statistics. Cluster-sensitive Attention (CA): Inspired by (Halder et al., 2018), we initialize an attention vector, vai ∈ Re for each cluster ci . Given a forward sequence H f = hf1 , hf2 , · · · , hfn and backward sequence H b = hb1 , hb2 , · · · , hbn of hidden post states p written by user u belonging to cluster ci , the corresponding wj weights each hidden state hfj and hbj of both sequences based on their similarity with the attention vector are: waj = exp(vai hj ) Pn . l=1 exp(vai hj"
W18-5602,D15-1166,0,0.325603,"l content but disregard semantics and suffer scalability with sparsity issues. To address this, state-of-the-art architectures feature complex modeling to model subtle dependencies and rely on word embeddings to address scalability issues, achieving robust results in text classification (Kim, 2014), neural machine translation (Luong et al., 2016), among others. Inspired by the success of their approaches, we adopt the recurrent neural network architecture (RNN) for post content modeling. Coupled with an attention mechanism, our approach adaptively weights the importance of parts in each post (Luong et al., 2015). Thread content modeling. Most research working on thread-level modeling usually obtain thread content representation by aggregating each content of its posts (Yang et al., 2014). However, we hypothesize that each post has different contribution to thread content and should be variously weighted to reflect specific factors, such as its author’s level of credibility. 13 User ID 3690 26521 Post Drug mentioned While my experience of 10 years is with Paxil, I expect that Zoloft will be the same. You should definitely feel better within 2 weeks. One way I found to make it easier to sleep was to ge"
W18-5602,P16-1101,0,0.0145979,"ilar to the attention vector v ai should be paid more attention to; hence are weighted higher during document encoding. v ai is adjusted during training to capture hidden states that are significant in forming the final post representation. waj is then used to compute forward and backward weighted feature vectors: hf = n X j waj hfj , hb = n X j waj hbj . (3) hfi = LST M (hfi−1 , wi ), hbj = LST M (hbj+1 , wj ), We concatenate the forward and backward vectors where hfi , hbj ∈ Re , and e are the number of ento obtain a single vector, following previous bicoder units. directional RNN practice (Ma and Hovy, 2016). 16 Thread Content Encoding with Credibility Weights (CW): For every post–user pair (pi , ui ) of thread t, we first compute post pi feature vector v pi . It is then concatenated with user ui ’s expertise vector v ui to form post–user complex vecp . This user-post complex is weighted by a tor vn i user credibility score wui , which is initially randomized and updated while training, to obtain fip∗ . This folnal post–user pair representation vn i lows the general intuition from the truth discovery literature that users providing high quality answers should assign higher credibility scores, and"
W18-5602,P16-2073,0,0.0159921,"ce, every thread has a list of aggregated side effects defined as St = Sd1 ∪ Sd2 · · · ∪ Sdm , which is also the list of potential side effects experienced during the treatment. User modeling. Statement credibility prediction often represents users by a single scalar that indicates their trustworthiness. The intuition is that users who provide trustworthy information frequently will be assigned high reliability scores (Li et al., 2017). Such representation is effective yet insufficient. Recent work have shown that encoding users into high-dimensional embeddings can improve system performance (Yu et al., 2016), which we have adopted in our model. 2.2 Side Effect Discovery Most drug reaction discovery methods focus on extracting mentioned side effects. A common technique is to apply Named Entity Recognition (NER) and Relation Extraction (RE) systems in a supervised manner. (Sampathkumar et al., 2014) demonstrates its effectiveness in detecting drugs and side effects that appear in a target document (in-context), and predicting if they are related. However, in our side effect prediction during treatment, our model is required to cover potentially encountered reactions, many of which are not explicitl"
W19-2604,L16-1294,0,0.0273022,"with such techniques. 1 Related Work The extraction of important scientific terms within full-text documents has been desiderata of scholarly document analyses extending back decades. In the early 90s, work by Liddy (Liddy, 1991) explored the possibility of promoting key scholarly document metadata into structured abstracts. Generic aspects of scholarly documents have been explored in (Gupta and Manning, 2011) where key aspects of publications namely focus, domain and techniques were identified using linguistic patterns. Domain-specific corpora with complex taxonomies such as the ACL RD-TEC (QasemiZadeh and Schumann, 2016) have also been employed to train systems to identify fine-grained aspects. In the field of nursing and primary care, the key metadata of Patients, Intervention, Condition, and Outcome characterize the acronym “PICO”, which has also been the target of much work (Zhao et al., 2010; Wallace et al., 2016). Recently, shared tasks concerning key generic metadata (inclusive of datasets) have been run in the community. The ScienceIE shared task (Augenstein et al., 2017) benchmarked techniques for identifying predefined entities matching Process, Task and Materials; where the definition of Material en"
W19-2604,S17-2097,0,0.0279076,"ed tasks concerning key generic metadata (inclusive of datasets) have been run in the community. The ScienceIE shared task (Augenstein et al., 2017) benchmarked techniques for identifying predefined entities matching Process, Task and Materials; where the definition of Material entities overlap with that of datasets. The task asked to extract such entities and identify the relations among them on short excerpts of scientific documents. State-of-the-art deep learning and feature-based sequential labeling models set the standard for approaches on such tasks, using Long Short-Term Memory (LSTM) (Ammar et al., 2017) and Conditional Random Field (CRF) (Prasad and Kan, 2017) models, respectively. Though related to a general named entity recognition, we see the problem of dataset mention extraction as having particular challenges. In contrast to the related scientific document processIntroduction The modern scientific method hinges on replicability and falsifiability. Datasets are an essential aspect of enabling such analysis in much of modern empirical studies. Datasets themselves are varied — in size, complexity, substructure, and scope — and references to them are also varied — in naming convention and s"
W19-2604,S17-2091,0,0.0361742,"Missing"
W19-2604,I11-1001,0,0.0294548,"ural baselines and evaluate these model on one-plus and zero-shot classification scenarios. We further explore various joint learning approaches – exploring the synergy between the tasks – and report the issues with such techniques. 1 Related Work The extraction of important scientific terms within full-text documents has been desiderata of scholarly document analyses extending back decades. In the early 90s, work by Liddy (Liddy, 1991) explored the possibility of promoting key scholarly document metadata into structured abstracts. Generic aspects of scholarly documents have been explored in (Gupta and Manning, 2011) where key aspects of publications namely focus, domain and techniques were identified using linguistic patterns. Domain-specific corpora with complex taxonomies such as the ACL RD-TEC (QasemiZadeh and Schumann, 2016) have also been employed to train systems to identify fine-grained aspects. In the field of nursing and primary care, the key metadata of Patients, Intervention, Condition, and Outcome characterize the acronym “PICO”, which has also been the target of much work (Zhao et al., 2010; Wallace et al., 2016). Recently, shared tasks concerning key generic metadata (inclusive of datasets)"
W19-2604,P82-1020,0,0.61415,"Missing"
W19-2604,D14-1181,0,0.00982778,"representation of each dataset’s description text. We then apply convolutions to known mentions of the dataset Di . Both representations are then merged and passed to a dense layer with a binary output such that f (T Fk , Di ) = 1 if T Fk mentions Di , else 0. This step is repeated for all datasets (i ∈ [0, m]) during testing, and a few randomly, sampled datasets per text fragment during training. (1) S = AT H where W ∈ RT ×T is the weight matrix to be trained and represents the Hadamard product. For the third dataset discovery task, we use sentence classification models i.e. BiLSTM and CNN (Kim, 2014) as baselines, replacing the standard sigmoid final binary classification with a softmax layer to enable multilabel multiclass classification. Shared Layer Extraction–Classification (‘SL E–C’). The first joint system selects the best system for each of the individual subtasks, then unifies them by providing a common feature extraction base and optimization using joint losses over both subtasks. We start with the best overall baseline for the mention extraction subtask (cf. Unlike SL E–C, KBSL E–C can incorporate new classes dynamically by creating a new class representation for predicted new c"
W19-2604,D14-1162,0,0.0821663,"Missing"
W19-2604,S17-2170,1,0.808768,"atasets) have been run in the community. The ScienceIE shared task (Augenstein et al., 2017) benchmarked techniques for identifying predefined entities matching Process, Task and Materials; where the definition of Material entities overlap with that of datasets. The task asked to extract such entities and identify the relations among them on short excerpts of scientific documents. State-of-the-art deep learning and feature-based sequential labeling models set the standard for approaches on such tasks, using Long Short-Term Memory (LSTM) (Ammar et al., 2017) and Conditional Random Field (CRF) (Prasad and Kan, 2017) models, respectively. Though related to a general named entity recognition, we see the problem of dataset mention extraction as having particular challenges. In contrast to the related scientific document processIntroduction The modern scientific method hinges on replicability and falsifiability. Datasets are an essential aspect of enabling such analysis in much of modern empirical studies. Datasets themselves are varied — in size, complexity, substructure, and scope — and references to them are also varied — in naming convention and subsequent reference or citation, both within and across do"
W98-1123,P94-1002,0,0.398315,"t, tended to have their own prior segmentation markings consisting of headers or bullets, so these were excluded. We thus concentrated our work on a corpus of shorter articles, averaging roughly 800-1500 words in length: 15 from the Wall Street Journal in the Linguistic Data Consortium's 1988 collection, and 5 from the on-line The Economist from 1997. We constructed an evaluation standard from human segmentation judgments to test our output. 1 SEGMENTER:L i n e a r S e g m e n t a t i o n For the purposes of discourse structure identification, we follow a formulation of the problem similar to Hearst (1994), in which zero or more segment boundaries are found at various paragraph separations, which identify one or more topical text segments. Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments. Our segmentation methodology has three distinct phases (Figure 1), which are executed sequentially. We will describe each of these phases in detail. [. &quot; - &quot; ~ - ~ &quot; - ~ W e i g h :'t [Terms I ITerrn I I [Links ~--~Score I ISegment I:egnts ] [Boundarie] Figure 1. SEGMENTERArchitecture 1,1 E x t r a c"
W98-1123,J97-1003,0,0.395035,"boundary heavily, but instead place the emphasis on the rear. Front: a paragraph in which a link begins. During: a paragraph in which a link occurs, but is not a front paragraph. Rear: a paragraph in which a link just stopped occurring the paragraph before. No link: any remaining paragraphs. paras 1 2 3 4 5 7 8 sents 12345678901234567890123456789012345 wine : Ixxl ix21 t y p e :n f d r n f d Figure 2zt A term &quot;wine&quot;, and its occurrences and type. We also tried to semantically cluster terms by using Miller et al. (1990)'s WordNet 1.5 with edge counting to determine relatedness, as suggested by Hearst (1997). However, results showed only minor improvement in precision and over a tenfold increase in execution time. 199 1.2.3. Zero Sum Normalization When we iterate the weighting process described above over each term, and total the scores assigned, we come up with a numerical score for an indication of which paragraphs are more likely to beh a topical boundary. The higher the numerical score, the higher the likelihood that the paragraph is a beginning of a new topical segment. The question then is what should the threshold be? paras 1 2 3 4 5 7 8 sents 12345678901234567890123456789012345 wine : ixx"
W98-1123,P93-1041,0,0.756536,"Missing"
W98-1123,P97-1013,0,0.0278967,"words in length: 15 from the Wall Street Journal in the Linguistic Data Consortium's 1988 collection, and 5 from the on-line The Economist from 1997. We constructed an evaluation standard from human segmentation judgments to test our output. 1 SEGMENTER:L i n e a r S e g m e n t a t i o n For the purposes of discourse structure identification, we follow a formulation of the problem similar to Hearst (1994), in which zero or more segment boundaries are found at various paragraph separations, which identify one or more topical text segments. Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments. Our segmentation methodology has three distinct phases (Figure 1), which are executed sequentially. We will describe each of these phases in detail. [. &quot; - &quot; ~ - ~ &quot; - ~ W e i g h :'t [Terms I ITerrn I I [Links ~--~Score I ISegment I:egnts ] [Boundarie] Figure 1. SEGMENTERArchitecture 1,1 E x t r a c t i n g Useful T o k e n s The task of determining segmentation b r e ~ s depends fundamentally on extracting useful topic information from the text. We extract three categories of information, which r"
W98-1123,C94-2121,0,0.1839,"Missing"
W98-1123,P93-1020,0,0.0629062,"Missing"
W98-1123,P94-1050,0,0.488043,"Missing"
W98-1123,W97-0304,0,\N,Missing
W98-1123,W97-0703,0,\N,Missing
W98-1123,C94-1042,0,\N,Missing
W98-1123,J96-2004,0,\N,Missing
