2007.jeptalnrecital-long.25,boas-2002-bilingual,0,0.0352586,"Missing"
2007.jeptalnrecital-long.25,C04-1134,0,0.0313517,"Missing"
2007.jeptalnrecital-long.25,J02-3001,0,0.0204752,"Missing"
2007.jeptalnrecital-long.25,P06-1117,0,0.0302329,"Missing"
2007.jeptalnrecital-long.25,P06-2057,0,0.2727,"Missing"
2007.jeptalnrecital-long.25,2005.mtsummit-papers.11,0,0.012598,"Missing"
2007.jeptalnrecital-long.25,C04-1100,0,0.0220152,"Missing"
2007.jeptalnrecital-long.25,J03-1002,0,0.00583962,"Missing"
2007.jeptalnrecital-long.25,H05-1108,1,0.742134,"Missing"
2007.jeptalnrecital-long.25,P06-1146,1,0.896705,"Missing"
2007.jeptalnrecital-long.25,P03-1002,0,0.0328334,"Missing"
2007.jeptalnrecital-long.25,H01-1035,0,0.168381,"Missing"
2020.acl-main.404,P15-2079,0,0.0190611,"n frequency bias, and particularly bad results on the Low band for A DVERSARIAL. 6 Conclusion This paper has discussed the task of political claims analysis as an example of Computational Social Science where NLP methods are finding adoption to scale analysis to large data sets. We have argued that this scenario must be aware of systematic biases in the output of the NLP methods. The NLP community has mostly focused on biases grounded in extralinguistic reality, e.g., gender (Bolukbasi et al., 2016; Rudinger et al., 2018; Stanovsky et al., 2019), race (Kiritchenko and Mohammad, 2018), or age (Hovy and Søgaard, 2015). We identified frequency as a language-internal bias present in a current neural model in political claims analysis. It warrants the same kind of attention as other bias types: lower recall for infrequent actors is inherently unfair, hitting those who can afford least to have their contribution overlooked. We compared two approaches to mitigating frequency bias in political claims detection and tested them on in-domain and out-of-domain settings. We found that a simple data modification strategy does as good as or better than modifying the model objective. Actor masking improves recall for in"
2020.acl-main.404,P16-2096,0,0.162005,"rs, their claims, and polarities (support/opposition). We investigate neural models for the claim identification aspect of political claims analysis trained on a German dataset, MARDY (Pad´o et al., 2019), and find that these models exhibit a strong frequency bias: claims made by frequently occurring actors are retrieved with higher recall than claims by infrequently mentioned actors. This is worrying, because it means that actors who repeat their claims often will now receive ’preferential treatment’ in the aggregated analysis and, arguably, be perceived as even more prominent than they are (Hovy and Spruit, 2016). We interpret these patterns as overfitting of the claim detection model: it relies too much on actor 4385 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4385–4391 c July 5 - 10, 2020. 2020 Association for Computational Linguistics mentions (i.e., either proper names or pronouns) as indicators of claims. To debias the model, we propose three methods: (1) mask the actor information by anonymizing referential expressions in the texts, which masks actor information; (2) train claim detectors adversarially by actor frequency; (3) assign more weight"
2020.acl-main.404,S18-2005,0,0.071668,"ions (Chang et al., 2014). However, this shift comes with new challenges: if the analyses are carried out by a machine, how can we trust that any outcomes really stem from the underlying data, rather than from processing artifacts? Consequently, CSS must be crucially interested in the algorithmic fairness or (absence of) bias of the underlying machine learning methods (e.g., Binns, 2018; Canetti et al., 2019). However, work on this topic in NLP over the last years has found that more applications contain biases than not, including lexical semantics (Bolukbasi et al., 2016), emotion detection (Kiritchenko and Mohammad, 2018), coreference (Zhao et al., 2018), recommendation generation (Chakraborty et al., 2019) and textual inference (Rudinger et al., 2017). It is therefore surprising that, to our knowledge, the bias of NLP methods applied in the CSS domain have found little attention so far. In this paper, we consider the CSS task of political claim analysis (Koopmans and Statham, 2010), an entity and relation extraction task from the domain of argument(ation) mining (Cabrio and Villata, 2018). Its goal is to extract (Actor, Polarity, Claim) tuples from text, as illustrated in Figure 1. This is a structured predic"
2020.acl-main.404,N19-1423,0,0.0357798,"Missing"
2020.acl-main.404,N19-1069,0,0.0256968,"milar to using denoising autoencoders for text representation, which introduce perturbations in the input to encourage models to discover stable latent rather than surface text properties (Glorot et al., 2011). Adversarial Debiasing. Adversarial debiasing forgoes changes in the dataset, preferring to use 2 Source for model and evaluation figures: https:// spacy.io/models/de#de_core_news_sm. Figure 2: Visualization of adversarial debiasing. adversarial training to have the model learn representations of the input that do not exhibit biases (in our case, frequency biases) in any substantial way McHardy et al. (2019). Concretely, we train our model simultaneously to predict whether the given text contains any claim and to prevent the adversarial component from predicting how frequently the claim actor occurs (Figure 2): The adversarial and main components share the feature extractor whose parameters (θf ) are therefore updated by the gradients coming through the objective functions of both model parts. Formally, let Jc and Jf r be the crossentropy loss functions of the main (claim detector) and adversarial (frequency detector) components, let λ be the meta-parameter for the trade-off between the two losse"
2020.acl-main.404,P19-1273,1,0.574148,"Missing"
2020.acl-main.404,W17-1609,0,0.0501081,"Missing"
2020.acl-main.404,N18-2002,0,0.0274486,"Missing"
2020.acl-main.404,P19-1164,0,0.0118591,"models, on the other hand, show an overall low recall with no decrease in frequency bias, and particularly bad results on the Low band for A DVERSARIAL. 6 Conclusion This paper has discussed the task of political claims analysis as an example of Computational Social Science where NLP methods are finding adoption to scale analysis to large data sets. We have argued that this scenario must be aware of systematic biases in the output of the NLP methods. The NLP community has mostly focused on biases grounded in extralinguistic reality, e.g., gender (Bolukbasi et al., 2016; Rudinger et al., 2018; Stanovsky et al., 2019), race (Kiritchenko and Mohammad, 2018), or age (Hovy and Søgaard, 2015). We identified frequency as a language-internal bias present in a current neural model in political claims analysis. It warrants the same kind of attention as other bias types: lower recall for infrequent actors is inherently unfair, hitting those who can afford least to have their contribution overlooked. We compared two approaches to mitigating frequency bias in political claims detection and tested them on in-domain and out-of-domain settings. We found that a simple data modification strategy does as good as or better"
2020.acl-main.404,W15-4631,0,0.0224864,"scourse networks and analyzed for aspects such as discourse coalitions or developments over time (Haunss, 2017; Wang and Wang, 2017). From an NLP perspective, full political claims analysis is a relatively complex process (Pad´o et al., 2019) that involves recognizing entities (actors), opinions (claims), and the relations between them (actor–claim pairs). In this paper, we focus on the task of claims detection in a narrow sense, namely the identification of claim spans in running text (cf. the right-hand markable in Figure 1), a task that is structurally related to (shallow) argument mining (Swanson et al., 2015; Vilares and He, 2017). Dataset. We use the M ARDY dataset, a corpus of articles relevant to the German immigration debate of the year 2015 drawn from the major German newspaper Die Tageszeitung (taz) (Pad´o et al., 2019). The corpus consists of 959 articles with a total of 1841 claims with an average length of 20 tokens. Each claim is associated with an actor. For about half of the claims (879), the actor is local (i.e., inside the claim); for the rest, it is non-local (i.e., somewhere in the document context). Model. We investigate a model inspired by the best claims detection model from Pa"
2020.acl-main.404,D17-1165,0,0.021247,"nalyzed for aspects such as discourse coalitions or developments over time (Haunss, 2017; Wang and Wang, 2017). From an NLP perspective, full political claims analysis is a relatively complex process (Pad´o et al., 2019) that involves recognizing entities (actors), opinions (claims), and the relations between them (actor–claim pairs). In this paper, we focus on the task of claims detection in a narrow sense, namely the identification of claim spans in running text (cf. the right-hand markable in Figure 1), a task that is structurally related to (shallow) argument mining (Swanson et al., 2015; Vilares and He, 2017). Dataset. We use the M ARDY dataset, a corpus of articles relevant to the German immigration debate of the year 2015 drawn from the major German newspaper Die Tageszeitung (taz) (Pad´o et al., 2019). The corpus consists of 959 articles with a total of 1841 claims with an average length of 20 tokens. Each claim is associated with an actor. For about half of the claims (879), the actor is local (i.e., inside the claim); for the rest, it is non-local (i.e., somewhere in the document context). Model. We investigate a model inspired by the best claims detection model from Pad´o et al. (2019). Our"
2020.acl-main.404,N18-2003,0,0.103649,"comes with new challenges: if the analyses are carried out by a machine, how can we trust that any outcomes really stem from the underlying data, rather than from processing artifacts? Consequently, CSS must be crucially interested in the algorithmic fairness or (absence of) bias of the underlying machine learning methods (e.g., Binns, 2018; Canetti et al., 2019). However, work on this topic in NLP over the last years has found that more applications contain biases than not, including lexical semantics (Bolukbasi et al., 2016), emotion detection (Kiritchenko and Mohammad, 2018), coreference (Zhao et al., 2018), recommendation generation (Chakraborty et al., 2019) and textual inference (Rudinger et al., 2017). It is therefore surprising that, to our knowledge, the bias of NLP methods applied in the CSS domain have found little attention so far. In this paper, we consider the CSS task of political claim analysis (Koopmans and Statham, 2010), an entity and relation extraction task from the domain of argument(ation) mining (Cabrio and Villata, 2018). Its goal is to extract (Actor, Polarity, Claim) tuples from text, as illustrated in Figure 1. This is a structured prediction task with the goal of identi"
2020.coling-main.384,H05-1073,0,0.147361,"d). These realizations differ widely across domains and genres (Bostan and Klinger, 2018). To gain a representative picture and investigate the effect of translation on different emotion realizations, we compare four English corpora. ISEAR (Scherer and Wallbott, 1994) includes ≈7k descriptions of events. Each description is labeled with the emotion that it induced in the experiencers (anger, disgust, fear, guilt, joy, sadness and shame). TEC (Mohammad, 2012) contains ≈ 21k tweets associated to the six fundamental Ekman’s emotions (Ekman, 1992). The corpora by Aman and Szpakowicz (2007) and by Alm et al. (2005) are repertoires of ≈5k and ≈15k sentences from a number of Blogs and (fairy-)Tales, respectively (using Ekman+noemo). These corpora differ in labels (see Figure 3 vs. 4), topics, registers and communicative purposes: TEC collects short, spontaneous expressions, ISEAR provides statements that were produced in-lab. Micro F1 Em. ISEAR Blogs Tales TEC A D F G J No Sa Su Sh .51 .58 .70 .55 .72 — .61 — .46 .55 .64 .56 — .69 .88 .49 .41 — .39 .12 .33 — .45 .79 .37 .27 — .37 .26 .55 — .69 — .45 .49 — Table 2: Classification results (“—”: the emotion is not a label in the respective corpus). Emotion C"
2020.coling-main.384,W12-3709,0,0.171736,"al resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is able to mostly preserve author sentiment (Balahur and Turchi, 2012). On the other, it is known that translation obfuscates some socio-demographic characteristics of authors, like gender and personality traits (Mirkin et al., 2015; Rabinovich et al., 2017). In this paper, we investigate the question of how well emotions are preserved in MT. Answering this question and, if necessary, increasing the degree of emotion preservation, is important both theoretically (to inform cross-lingual studies that use translation as part of their experimental setup) and practically (to improve the usefulness of MT). The starting point of our research is a study by Rabinovich e"
2020.coling-main.384,banea-etal-2008-bootstrapping,0,0.0611729,") or colexification phenomena (i.e., naming related emotions with the same word, like grief and regret in Persian) which vary from language to language (Jackson et al., 2019). On the other hand, aesthetic considerations often call for making texts more readable or pleasant. These factors hamper methods that transfer affect or sentiment across languages, as they cause both translation errors (for human and machines alike) and stylistic choices which subvert the sentiment of words (Petrova and Rodionova, 2016). Thus, assessing the quality of sentimentannotated resources produced by translation (Banea et al., 2008; Chen and Skiena, 2014; Buechel et al., 2020, i.a.) is crucial. With this goal, Kajava et al. (2020) compare sentiment and emotion annotations of movie subtitles in English, Finnish, Italian, and French and find that the emotion preservation depends on the language pair. Validating resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation ca"
2020.coling-main.384,P05-1074,0,0.0825639,"g: this would require either manual annotation or highly comparable emotion classifiers for several languages. Manual annotations are costly, and emotion annotation is known to be tricky in terms of intersubjective replicability (Schuff et al., 2017). Neither are we aware of emotion classifiers with evidently similar behavior across languages. To circumvent this problem, our experimental setup uses a back-translation version of the method described above, shown in Figure 2. We compose two translation steps (S→T and T→S) such that the output is a paraphrase of the input in the same language S (Bannard and Callison-Burch, 2005) and the pitfalls of cross-lingual comparability can be avoided. Formally, given an input in S and a target 4343 Translation Input T S Forward Emotion Classification Backward S n S Emotion-Informed Selection Output S n Parameters: Decoding, Data, Target Language Figure 2: Instantiation of the method with back-translation. S is the source language; T is the target. emotion, we generate the best translation in T; this is then translated back into multiple hypotheses, providing a set of n paraphrases for the original input. We acknowledge that this type of setup is a conceptual simplification of"
2020.coling-main.384,C18-1179,1,0.910734,"sian models1 (Ng et al., 2019). These sentence-level models are based on transformers (Vaswani et al., 2017) and pretrained on bitext and back-translated news data, fine-tuned on in-domain data and used for decoding with a noisy channel approach to re-rank the n-best hypotheses. We use these models both with beamsearch and top-k sampling (cf. Section 3). Data Sets: Varying Emotion Realization. Emotions manifest themselves in various linguistic realizations, for instance with direct mentions (sad) or indirect associations (abandoned). These realizations differ widely across domains and genres (Bostan and Klinger, 2018). To gain a representative picture and investigate the effect of translation on different emotion realizations, we compare four English corpora. ISEAR (Scherer and Wallbott, 1994) includes ≈7k descriptions of events. Each description is labeled with the emotion that it induced in the experiencers (anger, disgust, fear, guilt, joy, sadness and shame). TEC (Mohammad, 2012) contains ≈ 21k tweets associated to the six fundamental Ekman’s emotions (Ekman, 1992). The corpora by Aman and Szpakowicz (2007) and by Alm et al. (2005) are repertoires of ≈5k and ≈15k sentences from a number of Blogs and (f"
2020.coling-main.384,2020.acl-main.112,0,0.0440317,"Missing"
2020.coling-main.384,P14-2063,0,0.0238516,"phenomena (i.e., naming related emotions with the same word, like grief and regret in Persian) which vary from language to language (Jackson et al., 2019). On the other hand, aesthetic considerations often call for making texts more readable or pleasant. These factors hamper methods that transfer affect or sentiment across languages, as they cause both translation errors (for human and machines alike) and stylistic choices which subvert the sentiment of words (Petrova and Rodionova, 2016). Thus, assessing the quality of sentimentannotated resources produced by translation (Banea et al., 2008; Chen and Skiena, 2014; Buechel et al., 2020, i.a.) is crucial. With this goal, Kajava et al. (2020) compare sentiment and emotion annotations of movie subtitles in English, Finnish, Italian, and French and find that the emotion preservation depends on the language pair. Validating resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation can corrupt textual senti"
2020.coling-main.384,P18-1082,0,0.0228633,"earch and top-k sampling, which differ in their ability to encourage diversity in the output. Beamsearch searches the space of hypotheses left-to-right, retaining at each time step a number of top-scoring candidates that equals the width of the beam, and expanding on those. Sequences decoded with beamsearch differ on minimal portions (Gimpel et al., 2013), while they are more varied when generated with sampling strategies. Top-k sampling, for instance, does not aim at maximizing the likelihood of text. Instead, it randomly samples words step-wise and outputs from the top-k most probable ones (Fan et al., 2018). Emotion Classification Model. To estimate the probability distribution over emotions for a given text, we use a biLSTM with a self-attention mechanism. This model architecture has been shown to perform close to state-of-the-art in emotion analysis (Baziotis et al., 2018). We treat the output of this emotion classifier as a scoring function emo(t, e) = p(e|t), i.e., the conditional probability of an emotion given a text t, and we assume that it is comparable across languages (see Section 4 for a discussion of this assumption). Translation Candidate Selection. Once the n translation candidates"
2020.coling-main.384,D13-1111,0,0.0293801,"ded. It shows state-of-the-art performance and it was developed with the goal to replicate other model architectures. Therefore, we assume that it is reasonably representative for other models. Importantly, FAIRSEQ supports different search algorithms, like beamsearch and top-k sampling, which differ in their ability to encourage diversity in the output. Beamsearch searches the space of hypotheses left-to-right, retaining at each time step a number of top-scoring candidates that equals the width of the beam, and expanding on those. Sequences decoded with beamsearch differ on minimal portions (Gimpel et al., 2013), while they are more varied when generated with sampling strategies. Top-k sampling, for instance, does not aim at maximizing the likelihood of text. Instead, it randomly samples words step-wise and outputs from the top-k most probable ones (Fan et al., 2018). Emotion Classification Model. To estimate the probability distribution over emotions for a given text, we use a biLSTM with a self-attention mechanism. This model architecture has been shown to perform close to state-of-the-art in emotion analysis (Baziotis et al., 2018). We treat the output of this emotion classifier as a scoring funct"
2020.coling-main.384,N19-1320,0,0.0164461,"towards a specific target attribute. The style transfer challenge is to create a fluent output that is semantically similar to the input, but differs systematically in style. Helbig et al. (2020) control for the balance between content, style and fluency with a dedicated component in their modular pipeline: after a text is re-written in many emotion variations, these are re-ranked by an objective function that measures their perplexity, preservation of content and expression of a target style. Evaluation metrics for these three desiderata are applied in the reinforcement learning approach of Gong et al. (2019) to impose constraints on output generation. Other attempts focus on the explicit separation between content and sentiment style (Li et al., 2018; Wen et al., 2020). Prabhumoye et al. (2018) do so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and"
2020.coling-main.384,guerini-etal-2008-valentino,0,0.0113094,"at investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objective to push the text generated during decoding towards a specific target attribute. The styl"
2020.coling-main.384,2020.socialnlp-1.6,1,0.878518,"nguages. Within this framework, we address three research questions (Table 1 shows motivating examples). We first ask if a state-of-the-art machine translation system, namely FAIRSEQ (Ott et al., 2019), loses emotional information during translation (RQ1). (Yes.) Next, we propose a post-processing step to re-rank n-best translation candidates and evaluate if this improves emotion preservation (RQ2). (It does.) Finally, we exploit the emotional variation in MT output to investigate whether this approach can actively change the input emotion (RQ3), essentially performing emotion style transfer (Helbig et al., 2020). (It can.) The implementation of the pipeline is available at http://www.ims.uni-stuttgart.de/ data/emotion-transfer. 2 Related Work Affect, Sentiment, and Emotion in Translation. Preserving affect in text is an issue for translation and other cross-linguistic studies (Wierzbicka, 2013; Wassmann, 2017; Hubscher-Davidson, 2017). On the one hand, there are linguistic constraints on translation, like the absence of terms for certain states (e.g., Sehnsucht is German for “a longing for some absent thing”) or colexification phenomena (i.e., naming related emotions with the same word, like grief an"
2020.coling-main.384,W17-4902,0,0.0150483,"negative/positive meanings of the ambiguous word as separate embeddings. While these studies gained some insight on translated polarity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change"
2020.coling-main.384,W19-2309,0,0.0198761,"d by learning the negative/positive meanings of the ambiguous word as separate embeddings. While these studies gained some insight on translated polarity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to mod"
2020.coling-main.384,N18-1169,0,0.148648,"iguous word as separate embeddings. While these studies gained some insight on translated polarity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired out"
2020.coling-main.384,P19-1194,0,0.0398724,"Missing"
2020.coling-main.384,E17-1083,0,0.144852,"sfer. An in-depth qualitative analysis reveals that there are recurring linguistic changes through which emotions are toned down or amplified, such as change of modality. 1 Introduction The quality of machine translation (MT) models in some areas follows close behind that of humans (Barrault et al., 2019). MT is deployed widely to support human-to-human communication across languages, e.g., in chat systems, customer support, or (social) media. It is also employed in downstream NLP tasks such as sentence simplification (Xu et al., 2016), error correction (Yuan and Briscoe, 2016), paraphrasing (Mallinson et al., 2017; Wieting and Gimpel, 2018), or cross-lingual resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is"
2020.coling-main.384,P07-1123,0,0.499469,"es, as they cause both translation errors (for human and machines alike) and stylistic choices which subvert the sentiment of words (Petrova and Rodionova, 2016). Thus, assessing the quality of sentimentannotated resources produced by translation (Banea et al., 2008; Chen and Skiena, 2014; Buechel et al., 2020, i.a.) is crucial. With this goal, Kajava et al. (2020) compare sentiment and emotion annotations of movie subtitles in English, Finnish, Italian, and French and find that the emotion preservation depends on the language pair. Validating resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation can corrupt textual sentiment, flattening positive and negative aspects down to neutrality. In MT research, some studies specifically try to incentivize the preservation of sentiment. Lohar et al. (2017) build separate translation models for data coming from each sentiment category. Si et al. (2019) 4341 Input Translation Emotion Classification S T Emotion"
2020.coling-main.384,N19-1049,0,0.0131783,"overgeneration to transfer a target emotion on a text? Having shown that MT prefers to output sentences with a toned-down emotion, and that it is possible to subselect instances with a similar emotional connotation as the input, we now turn to the question if diversity in MT output can be used for the task of emotion style transfer. In this setting, our backtranslation pipeline is used for paraphrasing with style transfer, following Xu et al. (2012) and Prabhumoye et al. (2018). Given an input text t and an emotion e, we want to produce a variation t0 which respects the following desiderata (Mir et al., 2019): it maximizes similarity with t; it is fluent and it expresses emotion e. Backtranslations provide us with a particularly easy setup: since MT systems are trained to maximize the fluency of their output and their faithfulness to the input, we assume that it is sufficient to pay attention to the presence of the target emotion (see Eq. (1)). Forward and backward translation steps alike are carried on through beamsearch or top-k sampling, with k=10, both producing n=50 paraphrases. Since this experiment tries to promote stylistic diversity, n-best lists could have been leveraged also in the targ"
2020.coling-main.384,D15-1130,0,0.0227321,"with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is able to mostly preserve author sentiment (Balahur and Turchi, 2012). On the other, it is known that translation obfuscates some socio-demographic characteristics of authors, like gender and personality traits (Mirkin et al., 2015; Rabinovich et al., 2017). In this paper, we investigate the question of how well emotions are preserved in MT. Answering this question and, if necessary, increasing the degree of emotion preservation, is important both theoretically (to inform cross-lingual studies that use translation as part of their experimental setup) and practically (to improve the usefulness of MT). The starting point of our research is a study by Rabinovich et al. (2017), who show that some semantic nuances tend to vanish in translation. In fact, just like human translation, MT is not guaranteed to preserve any of the"
2020.coling-main.384,S12-1033,0,0.0268988,"Realization. Emotions manifest themselves in various linguistic realizations, for instance with direct mentions (sad) or indirect associations (abandoned). These realizations differ widely across domains and genres (Bostan and Klinger, 2018). To gain a representative picture and investigate the effect of translation on different emotion realizations, we compare four English corpora. ISEAR (Scherer and Wallbott, 1994) includes ≈7k descriptions of events. Each description is labeled with the emotion that it induced in the experiencers (anger, disgust, fear, guilt, joy, sadness and shame). TEC (Mohammad, 2012) contains ≈ 21k tweets associated to the six fundamental Ekman’s emotions (Ekman, 1992). The corpora by Aman and Szpakowicz (2007) and by Alm et al. (2005) are repertoires of ≈5k and ≈15k sentences from a number of Blogs and (fairy-)Tales, respectively (using Ekman+noemo). These corpora differ in labels (see Figure 3 vs. 4), topics, registers and communicative purposes: TEC collects short, spontaneous expressions, ISEAR provides statements that were produced in-lab. Micro F1 Em. ISEAR Blogs Tales TEC A D F G J No Sa Su Sh .51 .58 .70 .55 .72 — .61 — .46 .55 .64 .56 — .69 .88 .49 .41 — .39 .12"
2020.coling-main.384,W19-5333,0,0.0180596,"ent MT settings (which we do in the sections below). In addition, results that would indicate that we can improve emotion preservation in back-translation would conversely be stronger than such results obtained on a single translation step. 4.1 Experimental Setup Details Following the considerations of the previous paragraph, we do not run a single experiment, but instead carry out a series of comparisons, varying the different parameters of the emotion preservation method. NMT Model: Varying target language and sampling method. We use FAIRSEQ with English– German and English–Russian models1 (Ng et al., 2019). These sentence-level models are based on transformers (Vaswani et al., 2017) and pretrained on bitext and back-translated news data, fine-tuned on in-domain data and used for decoding with a noisy channel approach to re-rank the n-best hypotheses. We use these models both with beamsearch and top-k sampling (cf. Section 3). Data Sets: Varying Emotion Realization. Emotions manifest themselves in various linguistic realizations, for instance with direct mentions (sad) or indirect associations (abandoned). These realizations differ widely across domains and genres (Bostan and Klinger, 2018). To"
2020.coling-main.384,P18-2031,0,0.0248135,"rity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objec"
2020.coling-main.384,N19-4009,0,0.0390131,"e→target→source, which we can examine with only one emotion classifier for the source language. We acknowledge that this solution makes a simplifying assumption, namely that experimenting with back-translation can give a realistic picture of what would happen in a source→target setting. Still, adding a translation step seems a reasonable compromise in the absence of comparable emotion classifiers for different languages. Within this framework, we address three research questions (Table 1 shows motivating examples). We first ask if a state-of-the-art machine translation system, namely FAIRSEQ (Ott et al., 2019), loses emotional information during translation (RQ1). (Yes.) Next, we propose a post-processing step to re-rank n-best translation candidates and evaluate if this improves emotion preservation (RQ2). (It does.) Finally, we exploit the emotional variation in MT output to investigate whether this approach can actively change the input emotion (RQ3), essentially performing emotion style transfer (Helbig et al., 2020). (It can.) The implementation of the pipeline is available at http://www.ims.uni-stuttgart.de/ data/emotion-transfer. 2 Related Work Affect, Sentiment, and Emotion in Translation."
2020.coling-main.384,D14-1162,0,0.0847682,"Missing"
2020.coling-main.384,P18-1080,0,0.126853,"t al. (2020) control for the balance between content, style and fluency with a dedicated component in their modular pipeline: after a text is re-written in many emotion variations, these are re-ranked by an objective function that measures their perplexity, preservation of content and expression of a target style. Evaluation metrics for these three desiderata are applied in the reinforcement learning approach of Gong et al. (2019) to impose constraints on output generation. Other attempts focus on the explicit separation between content and sentiment style (Li et al., 2018; Wen et al., 2020). Prabhumoye et al. (2018) do so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and similarity to an input. In line with ours, a few other works have attempted to generate emotionally loaded text for given emotion classes, for instance in dialogue systems (Song et al., 201"
2020.coling-main.384,E17-1101,0,0.0259886,"itional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is able to mostly preserve author sentiment (Balahur and Turchi, 2012). On the other, it is known that translation obfuscates some socio-demographic characteristics of authors, like gender and personality traits (Mirkin et al., 2015; Rabinovich et al., 2017). In this paper, we investigate the question of how well emotions are preserved in MT. Answering this question and, if necessary, increasing the degree of emotion preservation, is important both theoretically (to inform cross-lingual studies that use translation as part of their experimental setup) and practically (to improve the usefulness of MT). The starting point of our research is a study by Rabinovich et al. (2017), who show that some semantic nuances tend to vanish in translation. In fact, just like human translation, MT is not guaranteed to preserve any of the linguistic properties of"
2020.coling-main.384,N15-1078,0,0.222246,"y of sentimentannotated resources produced by translation (Banea et al., 2008; Chen and Skiena, 2014; Buechel et al., 2020, i.a.) is crucial. With this goal, Kajava et al. (2020) compare sentiment and emotion annotations of movie subtitles in English, Finnish, Italian, and French and find that the emotion preservation depends on the language pair. Validating resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation can corrupt textual sentiment, flattening positive and negative aspects down to neutrality. In MT research, some studies specifically try to incentivize the preservation of sentiment. Lohar et al. (2017) build separate translation models for data coming from each sentiment category. Si et al. (2019) 4341 Input Translation Emotion Classification S T Emotion-Informed Selection n Output T Figure 1: Overview of emotion preservation and transfer (method). directly incorporate sentiment in their neural MT system, implementing a Seq2Seq English-to"
2020.coling-main.384,W17-5203,1,0.891722,"Missing"
2020.coling-main.384,N16-1005,0,0.02944,"ined some insight on translated polarity, subjectivity, valence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-en"
2020.coling-main.384,D19-5227,0,0.0219631,"g resources for Romanian created from English ones, Mihalcea et al. (2007) notice that human translation can obscure the subjectivity of a lexicon. A comparable observation is drawn for polarity by Balahur and Turchi (2012) with SMT, and by Salameh et al. (2015) and Mohammad et al. (2016) who find that translation can corrupt textual sentiment, flattening positive and negative aspects down to neutrality. In MT research, some studies specifically try to incentivize the preservation of sentiment. Lohar et al. (2017) build separate translation models for data coming from each sentiment category. Si et al. (2019) 4341 Input Translation Emotion Classification S T Emotion-Informed Selection n Output T Figure 1: Overview of emotion preservation and transfer (method). directly incorporate sentiment in their neural MT system, implementing a Seq2Seq English-to-Chinese translation model that keeps not only the semantics but also the sentiment of input text, both by including the sentiment label in source sentences, and by learning the negative/positive meanings of the ambiguous word as separate embeddings. While these studies gained some insight on translated polarity, subjectivity, valence, dominance and ar"
2020.coling-main.384,P19-1359,0,0.0201472,"ye et al. (2018) do so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and similarity to an input. In line with ours, a few other works have attempted to generate emotionally loaded text for given emotion classes, for instance in dialogue systems (Song et al., 2019; Zhou and Wang, 2018), but they create novel texts rather than re-styling existing ones. 3 A Method for Emotion Preservation in Neural Machine Translation We conceptualize emotion preservation in NMT as a post-processing re-ranking step. As shown in Figure 1, this involves three components: a translation model, an emotion classifier, and a candidate selection procedure. Starting from an input in source language S, we generate the n-best translation candidates in a target language T with an NMT system, which is presumably agnostic to emotion-specific considerations. Then, we re-rank these cand"
2020.coling-main.384,P19-1159,0,0.0129576,"fer, the input presents the action as one that the experiencer had to take. In d., sadness replaces disgust with the use of a softer expression, such as “loathe”. This example also highlights that removing a direct emotion word can determine a switch in connotation. Another reason why the backtranslation in b. is associated to fear could be that silence, in ISEAR, mostly occurs in the description of disruptive, frightening events, similarly to being “approached” by strangers (and hence, the joyful sentence in e. turns into fear). There are also signals that emotion changes show a gender bias (Sun et al., 2019): characterising the subject as a male moves anger to guilt or joy (a.), while we have found that female characters can elicit an association with shame. As for the transfer, it is possible that smaller lexical changes are sufficient to change emotions when the input label and the new emotions can co-occur. For instance, anger and guilt, being negative emotions, are more likely to co-occur than anger and joy, corresponding to output 1 and 3 for the first sentence. These examples also show that transfer can happen without disrupting grammaticality nor content – at least within the relatively sm"
2020.coling-main.384,D19-1365,0,0.0217347,"alence, dominance and arousal, to our knowledge there are no studies that investigate specifically the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objective to push the tex"
2020.coling-main.384,2020.lrec-1.575,0,0.0295226,"in style. Helbig et al. (2020) control for the balance between content, style and fluency with a dedicated component in their modular pipeline: after a text is re-written in many emotion variations, these are re-ranked by an objective function that measures their perplexity, preservation of content and expression of a target style. Evaluation metrics for these three desiderata are applied in the reinforcement learning approach of Gong et al. (2019) to impose constraints on output generation. Other attempts focus on the explicit separation between content and sentiment style (Li et al., 2018; Wen et al., 2020). Prabhumoye et al. (2018) do so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and similarity to an input. In line with ours, a few other works have attempted to generate emotionally loaded text for given emotion classes, for instance in dialogue"
2020.coling-main.384,W10-0211,0,0.0391602,"cally the preservation of emotions in MT. Style Transfer. Related to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objective to push the text generated during decoding towards a specific target attribute. The style transfer challenge is to cr"
2020.coling-main.384,P18-1042,0,0.016262,"ative analysis reveals that there are recurring linguistic changes through which emotions are toned down or amplified, such as change of modality. 1 Introduction The quality of machine translation (MT) models in some areas follows close behind that of humans (Barrault et al., 2019). MT is deployed widely to support human-to-human communication across languages, e.g., in chat systems, customer support, or (social) media. It is also employed in downstream NLP tasks such as sentence simplification (Xu et al., 2016), error correction (Yuan and Briscoe, 2016), paraphrasing (Mallinson et al., 2017; Wieting and Gimpel, 2018), or cross-lingual resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qualities of texts. On the one hand, it is able to mostly preserve au"
2020.coling-main.384,C12-1177,0,0.0264018,"eserving variation further down the space of candidate outputs (at least to a certain point) without sacrificing the system’s performance. 5.3 RQ3: Can we exploit overgeneration to transfer a target emotion on a text? Having shown that MT prefers to output sentences with a toned-down emotion, and that it is possible to subselect instances with a similar emotional connotation as the input, we now turn to the question if diversity in MT output can be used for the task of emotion style transfer. In this setting, our backtranslation pipeline is used for paraphrasing with style transfer, following Xu et al. (2012) and Prabhumoye et al. (2018). Given an input text t and an emotion e, we want to produce a variation t0 which respects the following desiderata (Mir et al., 2019): it maximizes similarity with t; it is fluent and it expresses emotion e. Backtranslations provide us with a particularly easy setup: since MT systems are trained to maximize the fluency of their output and their faithfulness to the input, we assume that it is sufficient to pay attention to the presence of the target emotion (see Eq. (1)). Forward and backward translation steps alike are carried on through beamsearch or top-k sampli"
2020.coling-main.384,Q16-1029,0,0.0182411,"lso be applied to change emotions, obtaining a model for emotion style transfer. An in-depth qualitative analysis reveals that there are recurring linguistic changes through which emotions are toned down or amplified, such as change of modality. 1 Introduction The quality of machine translation (MT) models in some areas follows close behind that of humans (Barrault et al., 2019). MT is deployed widely to support human-to-human communication across languages, e.g., in chat systems, customer support, or (social) media. It is also employed in downstream NLP tasks such as sentence simplification (Xu et al., 2016), error correction (Yuan and Briscoe, 2016), paraphrasing (Mallinson et al., 2017; Wieting and Gimpel, 2018), or cross-lingual resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this,"
2020.coling-main.384,P18-1090,0,0.0263812,"to ours is the task of style transfer. This research direction leverages a variety of methods, from rule-based lexical substitution to sophisticated neural architectures, aiming at retaining the semantics of texts while modifying their linguistic properties, like genre (Lee et al., 2019; Jhamtani et al., 2017), romanticism (Li et al., 2018), politeness/offensiveness and formality (Sennrich et al., 2016; Nogueira dos Santos et al., 2018; Wang et al., 2019) and, importantly for us, affect-related attributes (Guerini et al., 2008; Whitehead and Cavedon, 2010; Shen et al., 2017; Fu et al., 2018; Xu et al., 2018; Smith et al., 2019; Helbig et al., 2020). Yet, only a handful of style transfer studies have considered emotions. Helbig et al. (2020), for instance, propose an interpretable framework based on lexical substitution which sequentially determines the portion of text to modify, performs the change, and filters out undesired output. Smith et al. (2019), instead, leverage a denoising auto-encoder and a back-translation objective to push the text generated during decoding towards a specific target attribute. The style transfer challenge is to create a fluent output that is semantically similar to"
2020.coling-main.384,N16-1042,0,0.0247889,"btaining a model for emotion style transfer. An in-depth qualitative analysis reveals that there are recurring linguistic changes through which emotions are toned down or amplified, such as change of modality. 1 Introduction The quality of machine translation (MT) models in some areas follows close behind that of humans (Barrault et al., 2019). MT is deployed widely to support human-to-human communication across languages, e.g., in chat systems, customer support, or (social) media. It is also employed in downstream NLP tasks such as sentence simplification (Xu et al., 2016), error correction (Yuan and Briscoe, 2016), paraphrasing (Mallinson et al., 2017; Wieting and Gimpel, 2018), or cross-lingual resource creation (Barnes and Klinger, 2019). With the increasing use of MT, however, expectations about output quality also grow, and now that the goal of adequacy with regard to propositional content is met more often than not, more subtle aspects start receiving attention. One such aspect is affective content. Establishing common ground is essential for successful MT-assisted communication (Yamashita et al., 2009), but it is still unclear how well MT promotes this, especially when handling the affective qual"
2020.coling-main.384,P18-1104,0,0.0287804,"so using neural back-translation: in the latent representation of an input text, its stylistic properties are overwritten, which results in a style-specific paraphrase. Like them, we tap on back-translation as a paraphrasing strategy, but we transfer emotions, which we conceptualize as fine-grained styles. Using state-of-the-art off-the-shelf systems, we move from the problem of guaranteeing fluency and similarity to an input. In line with ours, a few other works have attempted to generate emotionally loaded text for given emotion classes, for instance in dialogue systems (Song et al., 2019; Zhou and Wang, 2018), but they create novel texts rather than re-styling existing ones. 3 A Method for Emotion Preservation in Neural Machine Translation We conceptualize emotion preservation in NMT as a post-processing re-ranking step. As shown in Figure 1, this involves three components: a translation model, an emotion classifier, and a candidate selection procedure. Starting from an input in source language S, we generate the n-best translation candidates in a target language T with an NMT system, which is presumably agnostic to emotion-specific considerations. Then, we re-rank these candidates based on probab"
2020.emnlp-main.396,C18-1139,0,0.0197969,"ts us to look more closely at results for individual span types, where we find that BERT+Feat+LSTM+CRF performs best on 16 of the 36 total span types, BERT+CRF on 7 span types, Feat+LSTM+CRF on 7 span types, and BERT+LSTM+CRF on 6 span types. Thus, ‘bespoke’ modeling of span types can evidently improve results. Even though our architectures are task-agnostic, and not tuned to particular tasks or datasets, our best architectures still perform quite competitively. For instance, on CoNLL’00, our BERT+Feat+LSTM+CRF model comes within 0.12 F1 points of the best published model’s F1 score of 97.62 (Akbik et al., 2018). For PARC, existing literature does not report micro-averaged F1 scores, but instead focuses only on F1 scores for content span detection. In this case, we find that our BERT+Feat+LSTM+CRF model beats the existing state of the art on this span type, achieving an F1 score of 78.1, compared to the score of 75 reported in Scheible et al. (2016). 5.3 Meta-learning Results The result of Step 2 is our performance prediction model. Table 3 shows both mean absolute error (MAE), which is directly interpretable as the mean 4885 BERT Feat Feat BERT Feat Feat LSTM LSTM BERT LSTM LSTM Feat BL BL CRF CRF L"
2020.emnlp-main.396,E14-1005,0,0.0264917,"ets. All of them have non-overlapping spans from a closed set of span types. In the following, we discuss (properties of) span types, assuming that each span type maps onto one span ID task. 2.1 Datasets Quotation Detection: PARC and R I Q UA. The Penn Attribution Relation Corpus (PARC) version 3.0 (Pareti, 2016) and the Rich Quotation Attribution Corpus (R I Q UA, Papay and Pad´o, 2020) are two datasets for quotation detection: models must identify direct and indirect quotation spans in text, which can be useful for social network construction (Elson et al., 2010) and coreference resolution (Almeida et al., 2014). The corpora cover articles from the Penn Treebank (PARC) and 19th century English novels (R I Q UA), respectively. Within each text, quotations are identified, along with each quotation’s speaker (or source), and its cue (an introducing word, usually a verb like “said”). We model detection of quotations as well as cues. As speaker and addressee identification are relation extraction tasks, we exclude these span types. Chunking: CoNLL’00. Chunking (shallow parsing) is an important preprocessing step in a number of NLP applications. We use the corpus from the 2000 CoNLL shared task on chunking"
2020.emnlp-main.396,S17-2091,0,0.0651276,"Missing"
2020.emnlp-main.396,J96-1002,0,0.226552,"hing (Pratapa et al., 2018). In terms of complexity, span ID tasks form a middle ground between simpler analysis tasks that predict labels for single linguistic units (such as lemmatization (Porter, 1980) or sentiment polarity classification (Liu, 2012)) and more complex analysis tasks such as relation extraction, which combines span ID with relation identification (Zelenko et al., 2002; Adel et al., 2018). Due to the rapid development of deep learning, an abundance of model architectures is available for the implementation of span ID tasks. These include isolated token classification models (Berger et al., 1996; Chieu and Ng, 2003), probabilistic models such as hidden Markov models (Rabiner, 1989), maximum entropy Markov models (McCallum et al., 2000), and conditional random fields (Lafferty et al., 2001), recurrent neural networks such as LSTMs (Hochreiter and Schmidhuber, 1997), and transformers such as BERT (Devlin et al., 2019). Though we have some understanding what each of these models can and cannot learn, there is, to our knowledge, little work on systematically understanding how different span ID tasks compare: are there model architectures that work well generally? Can we identify properti"
2020.emnlp-main.396,2020.lrec-1.104,1,0.816842,"Missing"
2020.emnlp-main.396,L16-1619,0,0.10946,"pan ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive. 1 Introduction Span identification is a family of analysis tasks that make up a substantial portion of applied NLP. Span identification (or short, span ID) tasks have in common that they identify and classify contiguous spans of tokens within a running text. Examples are named entity recognition (Nadeau and Sekine, 2007), chunking (Tjong Kim Sang and Buchholz, 2000), entity extraction (Etzioni et al., 2005), quotation detection (Pareti, 2016), keyphrase detection (Augenstein et al., 2017), or code switching (Pratapa et al., 2018). In terms of complexity, span ID tasks form a middle ground between simpler analysis tasks that predict labels for single linguistic units (such as lemmatization (Porter, 1980) or sentiment polarity classification (Liu, 2012)) and more complex analysis tasks such as relation extraction, which combines span ID with relation identification (Zelenko et al., 2002; Adel et al., 2018). Due to the rapid development of deep learning, an abundance of model architectures is available for the implementation of span"
2020.emnlp-main.396,D14-1162,0,0.0844584,"Missing"
2020.emnlp-main.396,P18-1143,0,0.014699,"r LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive. 1 Introduction Span identification is a family of analysis tasks that make up a substantial portion of applied NLP. Span identification (or short, span ID) tasks have in common that they identify and classify contiguous spans of tokens within a running text. Examples are named entity recognition (Nadeau and Sekine, 2007), chunking (Tjong Kim Sang and Buchholz, 2000), entity extraction (Etzioni et al., 2005), quotation detection (Pareti, 2016), keyphrase detection (Augenstein et al., 2017), or code switching (Pratapa et al., 2018). In terms of complexity, span ID tasks form a middle ground between simpler analysis tasks that predict labels for single linguistic units (such as lemmatization (Porter, 1980) or sentiment polarity classification (Liu, 2012)) and more complex analysis tasks such as relation extraction, which combines span ID with relation identification (Zelenko et al., 2002; Adel et al., 2018). Due to the rapid development of deep learning, an abundance of model architectures is available for the implementation of span ID tasks. These include isolated token classification models (Berger et al., 1996; Chieu"
2020.emnlp-main.396,P16-1164,1,0.892617,"Missing"
2020.emnlp-main.396,E17-1119,0,0.0210789,"TM+CRF. This architecture combines all components previously mentioned. It first uses a pre-trained BERT encoder to generate a sequence of contextualized embeddings. These embeddings are projected to 300 dimensions using a linear layer, yielding a sequence of vectors, which are then used as input for a LSTM+CRF network. As with BERT+CRF, we first train the non-BERT parameters to convergence while holding BERT’s parameters fixed, and subsequently fine-tune all parameters jointly. Handcrafted Features. Some studies have shown marked increases in performance by adding hand-crafted features (e.g. Shimaoka et al., 2017). We develop such features for our tasks and treat these to be an additional architecture component. For architectures with this component, a bag of features is extracted for each token (the exact features used for each dataset are enumerated in Table 5 in the Appendix). For each feature, we learn a 300-dimensional feature embedding which is averaged with the GloVe or BERT embedding to obtain a token embedding. Handcrafted features can be used with the Baseline, LSTM, LSTM+CRF, and BERT+LSTM+CRF architectures. BERT and BERT+CRF cannot utilize manual features, as they have no way of accepting t"
2020.emnlp-main.396,W00-0726,0,0.659983,"Missing"
2020.emnlp-main.396,2020.emnlp-demos.6,0,0.09424,"Missing"
2020.emnlp-main.396,C18-1182,0,0.0281447,"ocus on learning with very few training examples, while we focus on optimizing performance with normally sized corpora. Additionally, these models selectively train preselected model architectures, while we are concerned with comparisons between architectures. Model and Corpus Comparisons in Survey Papers. In a broad sense, our goal of comparison between existing corpora and modeling approaches is shared with many existing survey papers. Surveys include quantitative comparisons of existing systems’ performances on common tasks, producing a results matrix very similar to ours (Li et al., 2020; Yadav and Bethard, 2018; Bostan and Klinger, 2018, i.a.). However, most of these surveys limit themselves to collecting results across models and datasets without performing a detailed quantitative analysis of these results to identify recurring patterns, as we do with our performance prediction approach. 8 Conclusion In this work, we considered the class of span identification tasks. This class contains a number of widely used NLP tasks, but no comprehensive analysis beyond the level of individual tasks is available. We took a meta-learning perspective, predicting the performance of various architectures on various"
2020.lrec-1.104,E14-1005,0,0.414922,"Missing"
2020.lrec-1.104,J08-4004,0,0.0423877,"t for the corresponding quotation span. (3) While not disallowed by our annotation guidelines, we observed no deeper quotation embeddings. 4.2. P Inter-Annotator Agreement We now assess the quality of the resulting corpus. We quantify agreement with the family of F1 measures (precision, recall, F1 ), computed between sets of spans in an ’exact match’ setup that is often applied to sequence classification tasks like quotation detection – i.e., there is no partial credit for partial match. Note that this evaluation choice does not easily support chance correlation as is usual in classification (Artstein and Poesio, 2008), since it is not obvious what chance agreement on spans is supposed to be. More specifically, we measure agreement between the two initial annotations, and between each initial annotation and the final (merged) annotation. In the first case, we treat one set of initial annotations as a prediction and another as a ground truth. As the F1 measure is symmetric, the choice of which annotator is treated as the ground truth is inconsequential. In the second case, we report precision, recall, and F1 measure, treating the merged annotation as the ground truth. Table 4 shows the results of all levels"
2020.lrec-1.104,P10-1015,0,0.104052,"e. However, the quotation tells us relatively little about the interpersonal structure. The reader requires knowledge of the full sentence to understand which character is speaking ([Scrooge]Speaker ), whom he is addressing ([a boy in Sunday clothes]Addressee ), and the manner of speech (he [cried]Cue ), indicating anxiety. Often, even a full sentence is not enough, and the interpersonal structure must be reconstructed from the larger context. There exist a substantial number of studies which have shown that knowledge about quotations can be useful in tasks such as extracting social networks (Elson et al., 2010), modeling discourse structure (Redeker and Egg, 2006), formality (Faruqui and Padó, 2012), affect (Nalisnick and Baird, 2013; Iosif and Mishra, 2014), or even plain co-reference resolution (Almeida et al., 2014).1 In many of these tasks, the interpersonal structure is as important as the content level, or arguably more so. This mirrors a general tendency in other areas of semantics towards structured (relational) annotations that link states and events with the relevant actors and entities. For example, in sentiment annotation, additional structure can be annotated by linking subjective phras"
2020.lrec-1.104,E12-1064,1,0.787489,". The reader requires knowledge of the full sentence to understand which character is speaking ([Scrooge]Speaker ), whom he is addressing ([a boy in Sunday clothes]Addressee ), and the manner of speech (he [cried]Cue ), indicating anxiety. Often, even a full sentence is not enough, and the interpersonal structure must be reconstructed from the larger context. There exist a substantial number of studies which have shown that knowledge about quotations can be useful in tasks such as extracting social networks (Elson et al., 2010), modeling discourse structure (Redeker and Egg, 2006), formality (Faruqui and Padó, 2012), affect (Nalisnick and Baird, 2013; Iosif and Mishra, 2014), or even plain co-reference resolution (Almeida et al., 2014).1 In many of these tasks, the interpersonal structure is as important as the content level, or arguably more so. This mirrors a general tendency in other areas of semantics towards structured (relational) annotations that link states and events with the relevant actors and entities. For example, in sentiment annotation, additional structure can be annotated by linking subjective phrases with relevant aspects (Liu, 2012) and in emotion annotation, emotion phrases in text ca"
2020.lrec-1.104,W14-0906,0,0.00999379,"rstand which character is speaking ([Scrooge]Speaker ), whom he is addressing ([a boy in Sunday clothes]Addressee ), and the manner of speech (he [cried]Cue ), indicating anxiety. Often, even a full sentence is not enough, and the interpersonal structure must be reconstructed from the larger context. There exist a substantial number of studies which have shown that knowledge about quotations can be useful in tasks such as extracting social networks (Elson et al., 2010), modeling discourse structure (Redeker and Egg, 2006), formality (Faruqui and Padó, 2012), affect (Nalisnick and Baird, 2013; Iosif and Mishra, 2014), or even plain co-reference resolution (Almeida et al., 2014).1 In many of these tasks, the interpersonal structure is as important as the content level, or arguably more so. This mirrors a general tendency in other areas of semantics towards structured (relational) annotations that link states and events with the relevant actors and entities. For example, in sentiment annotation, additional structure can be annotated by linking subjective phrases with relevant aspects (Liu, 2012) and in emotion annotation, emotion phrases in text can be linked with experiencers and causes (Kim and Klinger, 2"
2020.lrec-1.104,C18-1114,0,0.0283683,"and Mishra, 2014), or even plain co-reference resolution (Almeida et al., 2014).1 In many of these tasks, the interpersonal structure is as important as the content level, or arguably more so. This mirrors a general tendency in other areas of semantics towards structured (relational) annotations that link states and events with the relevant actors and entities. For example, in sentiment annotation, additional structure can be annotated by linking subjective phrases with relevant aspects (Liu, 2012) and in emotion annotation, emotion phrases in text can be linked with experiencers and causes (Kim and Klinger, 2018). At the same time, existing corpora with quotation annotation vary greatly in whether, how, and to what extent they cover the interpersonal structure (i.e., who communicates what to whom – see Section 2. for details). In particular, we are not aware of any large-scale, publicly available corpora which mark both speakers and addressees for quotations in literary text. This resource gap makes investigation of conversation structures in text difficult. For example, Elson et al. (2010), lacking full addressee information, must assume that speakers are addressing one another when their quotations"
2020.lrec-1.104,L16-1168,0,0.0190837,"The corpus comprises English texts across three genres: fiction, biography, and newswire. The corpus marks spans which correspond to speech, thought, and writing separately, and further categorizes each such span as either direct, indirect, free-indirect, or reported. This categorization yields a product of twelve distinct span types which are annotated. Each such span is also marked with a speaker (or medium) where possible, but not at the textual level, but relative to a (manually compiled) list of relevant entities. Addressees are not marked. ACDS. The annotated corpus of directed speech (Lee and Yeung, 2016) consists of the four gospels of the New Testament annotated for direct speech. For each quotation, the corpus provides speech verb (cue), speaker, and listener (addressee), when applicable. Speakers and addressees are identified as spans of text, and coreferent speakers and addressees are also marked. The main limitations of this corpus are the use of a single work, and a single type of quotation. Also, as far as we know, this corpus has not been made publicly available. RWG. The Redewiedergabe (’reported speech’) corpus (Brunner, 2013), henceforth RWG, is a Germanlanguage corpus for quotatio"
2020.lrec-1.104,P13-2085,0,0.0166889,"f the full sentence to understand which character is speaking ([Scrooge]Speaker ), whom he is addressing ([a boy in Sunday clothes]Addressee ), and the manner of speech (he [cried]Cue ), indicating anxiety. Often, even a full sentence is not enough, and the interpersonal structure must be reconstructed from the larger context. There exist a substantial number of studies which have shown that knowledge about quotations can be useful in tasks such as extracting social networks (Elson et al., 2010), modeling discourse structure (Redeker and Egg, 2006), formality (Faruqui and Padó, 2012), affect (Nalisnick and Baird, 2013; Iosif and Mishra, 2014), or even plain co-reference resolution (Almeida et al., 2014).1 In many of these tasks, the interpersonal structure is as important as the content level, or arguably more so. This mirrors a general tendency in other areas of semantics towards structured (relational) annotations that link states and events with the relevant actors and entities. For example, in sentiment annotation, additional structure can be annotated by linking subjective phrases with relevant aspects (Liu, 2012) and in emotion annotation, emotion phrases in text can be linked with experiencers and c"
2020.lrec-1.104,L16-1619,0,0.320233,"sting similar corpora. CQSAC. The Columbia Quoted Speech Attribution Corpus (CQSAC) was one of the first corpora to provide annotation of quotations (Elson and McKeown, 2010). The corpus identifies quotation spans in literary text, and for each quotation identifies a noun phrase in the vicinity as a speaker for that quotation, if one exists. Unfortunately, this corpus only considers direct quotations, and not indirect ones. Additionally, quotation spans are detected automatically and not manually annotated, which makes them unreliable. Finally, no addressee information exists. PARC 3. PARC 3 (Pareti, 2016) provides quotation annotation for the more than 2000 news articles in the Wall Street Journal portion of the Penn Treebank. In each article, both direct and indirect quotations are annotated, together with cues and speakers. The annotation assumes that each quotation is introduced by a cue, which is reasonable for newswire, but not for literary text. Again, no addressees are provided. STOP. STOP is a corpus of “speech, thought, and writing presentation” (Semino and Short, 2004), primarily developed to empirically illustrate the authors’ taxonomy of types of language reporting. The corpus comp"
2020.lrec-1.104,W18-4515,0,0.0620464,"Missing"
2020.lrec-1.104,E12-2021,0,0.135042,"Missing"
2020.lrec-1.115,P19-3018,1,0.352915,"nd the construction of border installations as a solution to the immigration problem. The claims are highlighted in colors in the text, and give rise to the corresponding parts of the network representation to the right. The actors are represented by red squares in the discourse network. Blue edges indicate support towards a claim category (Merkel supports the &quot;Refugees Welcome&quot; claim), red edges indicate opposition to it (the demonstrators stand against the claim &quot;Controlling migration with border 919 installations&quot;). Methodologically, this paper follows our previous work (Padó et al., 2019; Blessing et al., 2019). The novel contribution of this paper are: (a) the release and the documentation of the complete dataset, including a quantitative and qualitative analysis with corpus linguistic tools such as keywords and collocations (Baker et al., 2008); and (b) a quantitative/qualitative analysis of the concrete discourse network structures that arise from the annotation as well as the temporal dynamics of these structures. In Section 2, we provide background on the discourse network analysis framework from political science that we build on, and on the role of and challenges for NLP in such a study. In S"
2020.lrec-1.115,P19-1273,1,0.793254,"olation of Europe and the construction of border installations as a solution to the immigration problem. The claims are highlighted in colors in the text, and give rise to the corresponding parts of the network representation to the right. The actors are represented by red squares in the discourse network. Blue edges indicate support towards a claim category (Merkel supports the &quot;Refugees Welcome&quot; claim), red edges indicate opposition to it (the demonstrators stand against the claim &quot;Controlling migration with border 919 installations&quot;). Methodologically, this paper follows our previous work (Padó et al., 2019; Blessing et al., 2019). The novel contribution of this paper are: (a) the release and the documentation of the complete dataset, including a quantitative and qualitative analysis with corpus linguistic tools such as keywords and collocations (Baker et al., 2008); and (b) a quantitative/qualitative analysis of the concrete discourse network structures that arise from the annotation as well as the temporal dynamics of these structures. In Section 2, we provide background on the discourse network analysis framework from political science that we build on, and on the role of and challenges for N"
2020.lrec-1.115,J17-3005,0,0.0205038,"ile the projection on the concept side yields the argumentative clusters present in the debate. Clearly, manual annotation of such claims and claim-actor relations is a resource intensive process. It it therefore natural to ask if Natural Language Processing can help: What are the potentials, limitations, and the practical issues of applying NLP to the automatic construction of discourse networks? At a general level, the NLP take on debate modeling can build on the insights from argumentation mining and subjectivity analysis (Peldszus and Stede, 2013; Ceron et al., 2014; Swanson et al., 2015; Stab and Gurevych, 2017; Vilares and He, 2017). An ideal NLP tool would automatically identify the actors and their contributions to the debate, and analyze such contributions at a structural level (identifying argumentative structure in their statements), at a semantic level (classifying statements into relevant categories), and at a pragmatic level (detecting the polarity of the statements). In our concrete experience (Padó et al., 2019), however, this task cannot be completely automated, at least not if the target is to acquire representations at the level of granularity and at the level of quality which are requ"
2020.lrec-1.115,W15-4631,0,0.0243993,"on the actor side, while the projection on the concept side yields the argumentative clusters present in the debate. Clearly, manual annotation of such claims and claim-actor relations is a resource intensive process. It it therefore natural to ask if Natural Language Processing can help: What are the potentials, limitations, and the practical issues of applying NLP to the automatic construction of discourse networks? At a general level, the NLP take on debate modeling can build on the insights from argumentation mining and subjectivity analysis (Peldszus and Stede, 2013; Ceron et al., 2014; Swanson et al., 2015; Stab and Gurevych, 2017; Vilares and He, 2017). An ideal NLP tool would automatically identify the actors and their contributions to the debate, and analyze such contributions at a structural level (identifying argumentative structure in their statements), at a semantic level (classifying statements into relevant categories), and at a pragmatic level (detecting the polarity of the statements). In our concrete experience (Padó et al., 2019), however, this task cannot be completely automated, at least not if the target is to acquire representations at the level of granularity and at the level"
2020.lrec-1.115,D17-1165,0,0.028332,"concept side yields the argumentative clusters present in the debate. Clearly, manual annotation of such claims and claim-actor relations is a resource intensive process. It it therefore natural to ask if Natural Language Processing can help: What are the potentials, limitations, and the practical issues of applying NLP to the automatic construction of discourse networks? At a general level, the NLP take on debate modeling can build on the insights from argumentation mining and subjectivity analysis (Peldszus and Stede, 2013; Ceron et al., 2014; Swanson et al., 2015; Stab and Gurevych, 2017; Vilares and He, 2017). An ideal NLP tool would automatically identify the actors and their contributions to the debate, and analyze such contributions at a structural level (identifying argumentative structure in their statements), at a semantic level (classifying statements into relevant categories), and at a pragmatic level (detecting the polarity of the statements). In our concrete experience (Padó et al., 2019), however, this task cannot be completely automated, at least not if the target is to acquire representations at the level of granularity and at the level of quality which are required for the political"
2020.nlpcss-1.3,K19-1024,0,0.0204375,"han on the content’s accuracy regarding party positions (Helbling and Tresch, 2011, p. 180). One natural characteristic is that the substantive density (e.g. as a ratio of claims to text) is much higher in electoral programs than in newspaper articles. NLP methods have been developed for automatic topic analysis (Glavaˇs et al., 2017). They have been investigated in a comparative fashion due to the textual and conceptual similarity to parliamentary speeches, which creates fertile grounds for domain adaptation (Daum´e III, 2007) and cross-topic argument mining (Stab et al., 2018). For example, Abercrombie et al. (2019) apply the annotation scheme of the MARPOR project3 to a corpus of parliamentary speeches to automatically label policy preferences. To the best of our knowledge, this work is the first study that attempts to establish a direct comparison between manifestos and newspaper reports of the political debates while being grounded in the application of automatic classification methods. 3 Methodology In what follows, we spell out the two steps of our methodology underlying the experiments presented in the subsequent sections. Step 1: Automatic claim detection For our machine learning experiments, we e"
2020.nlpcss-1.3,P07-1033,0,0.0411835,"Missing"
2020.nlpcss-1.3,2020.lrec-1.115,1,0.837633,"Missing"
2020.nlpcss-1.3,2020.acl-main.404,1,0.842413,"Missing"
2020.nlpcss-1.3,N19-1423,0,0.00810422,"while being grounded in the application of automatic classification methods. 3 Methodology In what follows, we spell out the two steps of our methodology underlying the experiments presented in the subsequent sections. Step 1: Automatic claim detection For our machine learning experiments, we employ the political claim detector from Dayanik and Pad´o (2020). It models claims detection as a binary classification task at the sentence level where the goal is to decide whether each input sentence contains a claim or not. The architecture is shown in Figure 2. It is based on the BERT architecture (Devlin et al., 2019) which is used to generate sentence representations by computing an embedding for the special [CLS] token used to indicate sentence breaks. We use a language specific BERT that is trained on German corpora4 since it is better at finding subword units for German than the multilingual BERT model (R¨onnqvist et al., 2019). A softmax classifier is then placed on top of BERT. It takes the [CLS] embedding performs the claim/no-claim classification. The model is trained using the DEbateNet-mig15 (Lapesa et al., 2020) data set. This data set contains about 2.000 claims from over 450 articles on the do"
2020.nlpcss-1.3,W17-2906,0,0.109856,"Missing"
2020.nlpcss-1.3,P19-1273,1,0.863357,"Missing"
2020.nlpcss-1.3,W19-6204,0,0.020418,"Missing"
2020.nlpcss-1.3,D18-1402,0,0.0275191,"the selection of certain topics than on the content’s accuracy regarding party positions (Helbling and Tresch, 2011, p. 180). One natural characteristic is that the substantive density (e.g. as a ratio of claims to text) is much higher in electoral programs than in newspaper articles. NLP methods have been developed for automatic topic analysis (Glavaˇs et al., 2017). They have been investigated in a comparative fashion due to the textual and conceptual similarity to parliamentary speeches, which creates fertile grounds for domain adaptation (Daum´e III, 2007) and cross-topic argument mining (Stab et al., 2018). For example, Abercrombie et al. (2019) apply the annotation scheme of the MARPOR project3 to a corpus of parliamentary speeches to automatically label policy preferences. To the best of our knowledge, this work is the first study that attempts to establish a direct comparison between manifestos and newspaper reports of the political debates while being grounded in the application of automatic classification methods. 3 Methodology In what follows, we spell out the two steps of our methodology underlying the experiments presented in the subsequent sections. Step 1: Automatic claim detection Fo"
2021.spnlp-1.6,P19-1273,1,0.880202,"Missing"
2021.spnlp-1.6,N19-1423,0,0.00748564,"a single-layer LSTM. The final hidden state is used as input to a fully connected layer. 3 BiLSTM A single-layer Bidirectional LSTM (Graves et al., 2013) traverses the input. The final hidden states in both directions are concatenated and fed to a fully connected layer. BiLSTM+Attention This model combines the BiLSTM architecture with the attention mechanism described in Shimaoka et al. (2017a). The input is fed to a single-layer BiLSTM. Then, the attention-weighted sum of the hidden states corresponding to the input sequence is fed to a fully connected layer. BERT This is a pretrained BERT (Devlin et al., 2019) model trained solely on German corpora 3 and a fully connected layer which is trained while the BERT encoder is fine-tuned. After each input is encoded, we use the final hidden state of the first token, corresponding to the special token [CLS], as the contextualized representation of the input which serves as input to a fully connected layer. Basic Claim Classification Given the properties described above, we model claim classification as multi-label classification. We follow previous work on coarse-grained claim classification (Pad´o et al., 2019) in comparing a set 2 Further details regardi"
2021.spnlp-1.6,C04-1197,0,0.44746,"t developing practically useful models of fine-grained claim classification. Its main proposal is to exploit the hierarchical nature of claim ontologies by jointly predicting (frequent) supercategories and (informative) subcategories. By enforcing consistency between the levels, the predictions can profit off each other. We experiment with two operationalizations of this idea. The first one, Hierarchical Label Encoding (HLE, Shimaoka et al. (2017a)) introduces “soft” constraints through parameter sharing between classes in the classifier. The second one, Integer Linear Programming (ILP, e.g., Punyakanok et al. (2004)) introduces “hard” constraints in a post-processing step. Both methods can be applied to a range of claim classifier architectures. We present experiments with four architectures on a German manually annotated corpus from the so-called refugee crisis in Germany in 2015. We answer the following questions: Do HLE and ILP improve the performance in our experimental setup? (Yes.) Is there complementarity between them? (Yes.) Does the effect depend on the underlying architectures. (Broadly, no.) What types of classes is the improvement most pronounced for. (Low-frequency ones.) The analysis of pub"
2021.spnlp-1.6,W06-1616,0,0.0155168,"i) ≤ 0 The second constraint is that each predicted supercategory is accompanied by at least one if its subcategories. Let subs(i) denote the set of subcategories for supercategory i. The constraint is: X for each supercategory xi : xi − xj ≤ 0 j∈subs(i) ILP has a complementary profile to HLE in enforcing hard constraints on the output, without propagating the errors back to representation learning. Integer Linear Programming (ILP). ILP has been applied to enforce linguistically motivated constraints on predicted structures such as semantic roles (Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), or entailment graphs (Berant et al., 2011). Formally, an integer linear program is an optimization problem over a set of integer variables x, given a linear objective function with a set of coefficients c and a set of linear inequality (and equality) constraints (Schrijver, 1984): max c |x 5 Experimental Evaluation Setup. We remove very infrequent subcategories in the dataset by applying a threshold of 20 instances. Smaller categories are merged with the preexisting subcategory x99, which exists for each supercategory as a ‘catch-all’ category for outlier cases. After filtering, there are 8"
2021.spnlp-1.6,W07-1509,0,0.0627328,"Specifically, HLE+ILP models achieves better Recall scores than HLE models (+7 points on average) and better Precision (+8 points on average) scores than ILP models. The effect is least pronounced for the best architecture (BERT); nevertheless, BERT with HLE and ILP achieves the overall highest Recall (0.59) and F-Score (0.60), corresponding to an improvement of 13 points F1 compared to the ‘plain’ version. The fact that the F1 boost is fueled mainly by Recall is particularly promising because optimizing for Recall is the best strategy when NLP tools are employed in semiautomatic annotation (Ganchev et al., 2007; Ambati et al., 2011). Qualitative Considerations. Finally, we investigate which subcategories benefit most from HLE and ILP in our best model (BERT). Table 4 again shows complementarity between HLE and ILP, indicating that a better combination of the two methods could lead to further improvements. HLE+ILP overlaps largely with HLE, mirroring the larger impact of HLE. Analysis of these classes shows that they belong to the mid and low frequency bands. However, not all low and mid frequency classes profit equally. To explain this, we note that the finegrained classes in the migration ontology"
2021.spnlp-1.6,P19-4004,0,0.0219905,"Missing"
2021.spnlp-1.6,E17-1119,0,0.13595,"he set of classes, the more data would be desirable, while in actuality, the number of instances per class shrinks (Mai et al., 2018; Chang et al., 2020). Our paper aims at developing practically useful models of fine-grained claim classification. Its main proposal is to exploit the hierarchical nature of claim ontologies by jointly predicting (frequent) supercategories and (informative) subcategories. By enforcing consistency between the levels, the predictions can profit off each other. We experiment with two operationalizations of this idea. The first one, Hierarchical Label Encoding (HLE, Shimaoka et al. (2017a)) introduces “soft” constraints through parameter sharing between classes in the classifier. The second one, Integer Linear Programming (ILP, e.g., Punyakanok et al. (2004)) introduces “hard” constraints in a post-processing step. Both methods can be applied to a range of claim classifier architectures. We present experiments with four architectures on a German manually annotated corpus from the so-called refugee crisis in Germany in 2015. We answer the following questions: Do HLE and ILP improve the performance in our experimental setup? (Yes.) Is there complementarity between them? (Yes.)"
2021.wassa-1.5,H05-1073,0,0.269204,"Missing"
2021.wassa-1.5,J11-4004,0,0.0237158,"o raters can be penalized more when the one choosing the emotion label does so by perceiving extreme confidence or intensity – even though we provided evidence that these cases are rare. Future work could explore this direction. In summary, we uphold that disagreements are not necessarily symptomatic of unreliability. This claim has so far not found much attention in emotion annotations, but is in line with a more general body of research dedicated to the reasons and the patterns underlying annotators’ disagreements and to the ways in which their intuitions should be aggregated and evaluated (Bayerl and Paul, 2011; Bhardwaj et al., 2010; Qing et al., 2014; Peldszus and Stede, 2013; Plank et al., 2014; Sommerauer et al., 2020, i.a.). The applicability of these ideas to emotions should not come as a surprise — their assessment can derive from perceptive and metaperceptual phenomena (intensity and confidence, for instance). Therefore, if emotion judgments alone might not be sufficient to measure the quality of annotations, they can be enriched and, eventually, explained by the knowledge of such phenomena. This annotation investigated if the perceived emotion (class), a perceived feature of emotion (intens"
2021.wassa-1.5,2020.coling-main.11,1,0.844011,"er automatic regressors or classifiers actually predict intensity, or rather human’s self-perceived confidence. 1 Introduction A plethora of theories exist on the matter of emotions: the intensity of affective states, their link to cognition, and their arrangement into categories are just a few of the angles from which psychology has tackled this complex phenomenon (Gendron and Feldman Barrett, 2009). Correspondingly, in computational emotion analysis, texts have been associated to values of intensity (Strapparava and Mihalcea, 2007; Mohammad and Bravo-Marquez, 2017), to cognitive components (Hofmann et al., 2020), and discrete classes (Zhang et al., 2018; Zhong et al., 2019, i.a.). In support of these tasks, substantial research effort has been directed to resource construction, which typically relies on the participation of human judges. Yet, emotions are a subjective experience. Their interpretation in text 40 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 40–49 April 19, 2021. ©2021 Association for Computational Linguistics In this paper, we experimentally investigate the relationship between three human judgments: about the"
2021.wassa-1.5,bobicev-sokolova-2017-inter,0,0.0432753,"Missing"
2021.wassa-1.5,I17-1099,0,0.0633886,"Missing"
2021.wassa-1.5,W13-2324,0,0.0340052,"label does so by perceiving extreme confidence or intensity – even though we provided evidence that these cases are rare. Future work could explore this direction. In summary, we uphold that disagreements are not necessarily symptomatic of unreliability. This claim has so far not found much attention in emotion annotations, but is in line with a more general body of research dedicated to the reasons and the patterns underlying annotators’ disagreements and to the ways in which their intuitions should be aggregated and evaluated (Bayerl and Paul, 2011; Bhardwaj et al., 2010; Qing et al., 2014; Peldszus and Stede, 2013; Plank et al., 2014; Sommerauer et al., 2020, i.a.). The applicability of these ideas to emotions should not come as a surprise — their assessment can derive from perceptive and metaperceptual phenomena (intensity and confidence, for instance). Therefore, if emotion judgments alone might not be sufficient to measure the quality of annotations, they can be enriched and, eventually, explained by the knowledge of such phenomena. This annotation investigated if the perceived emotion (class), a perceived feature of emotion (intensity), and self-perception (confidence) are tied together – and can h"
2021.wassa-1.5,P19-1391,1,0.890559,"Missing"
2021.wassa-1.5,P14-2083,0,0.0316962,"g extreme confidence or intensity – even though we provided evidence that these cases are rare. Future work could explore this direction. In summary, we uphold that disagreements are not necessarily symptomatic of unreliability. This claim has so far not found much attention in emotion annotations, but is in line with a more general body of research dedicated to the reasons and the patterns underlying annotators’ disagreements and to the ways in which their intuitions should be aggregated and evaluated (Bayerl and Paul, 2011; Bhardwaj et al., 2010; Qing et al., 2014; Peldszus and Stede, 2013; Plank et al., 2014; Sommerauer et al., 2020, i.a.). The applicability of these ideas to emotions should not come as a surprise — their assessment can derive from perceptive and metaperceptual phenomena (intensity and confidence, for instance). Therefore, if emotion judgments alone might not be sufficient to measure the quality of annotations, they can be enriched and, eventually, explained by the knowledge of such phenomena. This annotation investigated if the perceived emotion (class), a perceived feature of emotion (intensity), and self-perception (confidence) are tied together – and can help understand incon"
2021.wassa-1.5,2020.coling-main.422,0,0.0430629,"or intensity – even though we provided evidence that these cases are rare. Future work could explore this direction. In summary, we uphold that disagreements are not necessarily symptomatic of unreliability. This claim has so far not found much attention in emotion annotations, but is in line with a more general body of research dedicated to the reasons and the patterns underlying annotators’ disagreements and to the ways in which their intuitions should be aggregated and evaluated (Bayerl and Paul, 2011; Bhardwaj et al., 2010; Qing et al., 2014; Peldszus and Stede, 2013; Plank et al., 2014; Sommerauer et al., 2020, i.a.). The applicability of these ideas to emotions should not come as a surprise — their assessment can derive from perceptive and metaperceptual phenomena (intensity and confidence, for instance). Therefore, if emotion judgments alone might not be sufficient to measure the quality of annotations, they can be enriched and, eventually, explained by the knowledge of such phenomena. This annotation investigated if the perceived emotion (class), a perceived feature of emotion (intensity), and self-perception (confidence) are tied together – and can help understand inconsistent annotations. We f"
2021.wassa-1.5,S07-1013,0,0.105532,". This insight is relevant for modelling studies of intensity, as it opens the question wether automatic regressors or classifiers actually predict intensity, or rather human’s self-perceived confidence. 1 Introduction A plethora of theories exist on the matter of emotions: the intensity of affective states, their link to cognition, and their arrangement into categories are just a few of the angles from which psychology has tackled this complex phenomenon (Gendron and Feldman Barrett, 2009). Correspondingly, in computational emotion analysis, texts have been associated to values of intensity (Strapparava and Mihalcea, 2007; Mohammad and Bravo-Marquez, 2017), to cognitive components (Hofmann et al., 2020), and discrete classes (Zhang et al., 2018; Zhong et al., 2019, i.a.). In support of these tasks, substantial research effort has been directed to resource construction, which typically relies on the participation of human judges. Yet, emotions are a subjective experience. Their interpretation in text 40 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 40–49 April 19, 2021. ©2021 Association for Computational Linguistics In this paper, we ex"
2021.wassa-1.6,D18-1002,0,0.113,". More generally, the bias-inducing role of demographic attributes is dangerous for studies that use texts from a multitude of authors – often gathered from social media – to draw inferences about the authors (Sobkowicz et al., 2012; Cheng et al., 2015). In such studies, demographic biases can lead to erroneous causal attributions (as our case will illustrate). To counteract the presence of biases in NLP, researchers have devised debiasing methods. Due to its general applicability and high effectiveness, adversarial debiasing has become one of the most widely used methods for bias mitigation (Elazar and Goldberg, 2018; Zhang et al., 2018; Arduini et al., 2020). Unfortunately, these advances are not accompanied by an analysis of the prerequisites Abstract Text classification is a central tool in NLP, including social media analysis. However, when the target classes are strongly correlated with other textual attributes, text classification models can pick up “wrong” features, leading to bad generalization and biases. In social media analysis, this problem surfaces for demographic user classes such as language, topic, or gender, which influence how an author writes a text to a substantial extent. Adversarial"
2021.wassa-1.6,S19-1010,0,0.0561966,"Missing"
2021.wassa-1.6,P18-2005,0,0.338019,"r bias exhibited by the model. A more fundamental idea is to adopt adversarial training (Goodfellow et al., 2014) to other tasks. For instance, Ganin and Lempitsky (2015) adapted adversarial training to the task of domain adaptation by introducing Gradient Reversal Layer (GRL) which acts as an identity function during forward pass and reverses the gradient by multiplying it by a negative scalar during the backward pass. Elazar and Goldberg (2018) apply the idea to the removal of demographic bias; McHardy et al. (2019) remove publication source as a bias variable from a satire detection model. Li et al. (2018) reported that adversarial training with GRL layer can remove unintended bias from the representations of POS tagging and Sentiment analysis models while maintaining task performance. Zhang et al. (2018) show that adversarial training mitigates the bias in word embeddings while maintaining its performance on word analogies task. Finally, Arduini et al. (2020) demonstrate how adversarial learning can be used for debiasing knowledge graph embeddings. 2 3 Related Work Regarding bias analysis at the representation level, the most important source of bias is arguably formed by corpus-derived embedd"
2021.wassa-1.6,N19-1069,0,0.0921258,"itional term to be used in loss function of language generation model, seeking to reduce the gender bias exhibited by the model. A more fundamental idea is to adopt adversarial training (Goodfellow et al., 2014) to other tasks. For instance, Ganin and Lempitsky (2015) adapted adversarial training to the task of domain adaptation by introducing Gradient Reversal Layer (GRL) which acts as an identity function during forward pass and reverses the gradient by multiplying it by a negative scalar during the backward pass. Elazar and Goldberg (2018) apply the idea to the removal of demographic bias; McHardy et al. (2019) remove publication source as a bias variable from a satire detection model. Li et al. (2018) reported that adversarial training with GRL layer can remove unintended bias from the representations of POS tagging and Sentiment analysis models while maintaining task performance. Zhang et al. (2018) show that adversarial training mitigates the bias in word embeddings while maintaining its performance on word analogies task. Finally, Arduini et al. (2020) demonstrate how adversarial learning can be used for debiasing knowledge graph embeddings. 2 3 Related Work Regarding bias analysis at the repres"
2021.wassa-1.6,D19-1530,0,0.0140093,"dversarial debiasing is due to the fact that feature space for author gender is subsumed the topic feature space for all languages except French, where gender is expressed overtly by morphological cues that can be picked up by the model. 2017), Emotion Intensity Prediction (Kiritchenko and Mohammad, 2018), Coreference Resolution (Rudinger et al., 2018; Zhao et al., 2018) and Text Classification (De-Arteaga et al., 2019). A number of strategies have been explored for bias mitigation. One common strategy for reducing the bias is to target the representational level again, that is, corpora (Hall Maudslay et al., 2019; Zhao et al., 2018) and word embeddings (Kaneko and Bollegala, 2019; Bolukbasi et al., 2016). Other methods target the model architecture in various ways. For example, Qian et al. (2019) introduce an additional term to be used in loss function of language generation model, seeking to reduce the gender bias exhibited by the model. A more fundamental idea is to adopt adversarial training (Goodfellow et al., 2014) to other tasks. For instance, Ganin and Lempitsky (2015) adapted adversarial training to the task of domain adaptation by introducing Gradient Reversal Layer (GRL) which acts as an ide"
2021.wassa-1.6,P15-1073,0,0.0689449,"Missing"
2021.wassa-1.6,P19-1160,0,0.0298358,"Missing"
2021.wassa-1.6,P19-2031,0,0.0224256,"ogical cues that can be picked up by the model. 2017), Emotion Intensity Prediction (Kiritchenko and Mohammad, 2018), Coreference Resolution (Rudinger et al., 2018; Zhao et al., 2018) and Text Classification (De-Arteaga et al., 2019). A number of strategies have been explored for bias mitigation. One common strategy for reducing the bias is to target the representational level again, that is, corpora (Hall Maudslay et al., 2019; Zhao et al., 2018) and word embeddings (Kaneko and Bollegala, 2019; Bolukbasi et al., 2016). Other methods target the model architecture in various ways. For example, Qian et al. (2019) introduce an additional term to be used in loss function of language generation model, seeking to reduce the gender bias exhibited by the model. A more fundamental idea is to adopt adversarial training (Goodfellow et al., 2014) to other tasks. For instance, Ganin and Lempitsky (2015) adapted adversarial training to the task of domain adaptation by introducing Gradient Reversal Layer (GRL) which acts as an identity function during forward pass and reverses the gradient by multiplying it by a negative scalar during the backward pass. Elazar and Goldberg (2018) apply the idea to the removal of d"
2021.wassa-1.6,S18-2005,0,0.290047,"ion of the target classes. 1 Introduction Natural language processing, and machine learning more generally, has recently received a significant deal of criticism because of the frequent presence of bias in the predictions, where we define bias as a systematic difference in system performance on one set of instances compared to another. Such biases have been identified in NLP tasks such as word representation (Bolukbasi et al., 2016), textual inference (Rudinger et al., 2017), coreference resolution (Zhao et al., 2018), text classification (Dixon et al., 2018) and emotion intensity prediction (Kiritchenko and Mohammad, 2018). In text classification tasks, a principal source of such biases are demographic attributes of authors, 1 A subset of these has specific legal protection in many jurisdictions under the name of sensitive or protected attributes. 50 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 50–61 April 19, 2021. ©2021 Association for Computational Linguistics that need to be satisfied for adversarial training to perform successfully. It has been established empirically is that adversarial training works well for many cases in NLP; n"
2021.wassa-1.6,W17-1609,0,0.0578582,"Missing"
2021.wassa-1.6,2005.mtsummit-papers.11,0,0.079339,"ortunately, neither corpus provides topic or gender labels. # TED Talks Author Gender Talk Topic For this reason, we create a new multilingual parallel dataset with these annotations, based on TED talks (http://ted.com/talks). A TED talk is a presentation at the TED conference or one of its international partner events. TED talks are limited to a maximum length of 18 minutes and may be on any topic. TED talks are rehearsed talks and at least semi-formal, while still definitely belonging to the category of spoken language. In this regard, they are comparable to the widely used Europarl corpus (Koehn, 2005). The talks are divided according to the languages, topics and posted dates. All original talks are presented in English, but volunteers provide (and double check) translations into other languages. Authors are identified by name. # Tokens/doc # Sentences/doc 1518 1042 (Male) / 476 (Female) 704 (SciTech) / 814 (Other) DE ES FR TR 2093 115 2110 110 2280 111 1632 114 Table 1: Statistics of TED multilingual corpora. Document Topic Author Gender Male Female SciTech Other 524 180 518 296 Table 2: Topic–gender correlation: Number of documents in TED corpus for each combination mation on the basis of"
2021.wassa-1.6,N18-2002,0,0.0443726,"Missing"
2021.wassa-1.6,N18-2003,0,0.135168,"result in terms of feature space overlap, highlighting the role of linguistic surface realization of the target classes. 1 Introduction Natural language processing, and machine learning more generally, has recently received a significant deal of criticism because of the frequent presence of bias in the predictions, where we define bias as a systematic difference in system performance on one set of instances compared to another. Such biases have been identified in NLP tasks such as word representation (Bolukbasi et al., 2016), textual inference (Rudinger et al., 2017), coreference resolution (Zhao et al., 2018), text classification (Dixon et al., 2018) and emotion intensity prediction (Kiritchenko and Mohammad, 2018). In text classification tasks, a principal source of such biases are demographic attributes of authors, 1 A subset of these has specific legal protection in many jurisdictions under the name of sensitive or protected attributes. 50 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 50–61 April 19, 2021. ©2021 Association for Computational Linguistics that need to be satisfied for adversarial training to perform succes"
2021.wassa-1.6,P19-1164,0,0.044312,"mbeddings. 2 3 Related Work Regarding bias analysis at the representation level, the most important source of bias is arguably formed by corpus-derived embeddings which are used by virtually all current NLP systems. There have been several efforts to investigate the amount of bias within monolingual (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Swinger et al., 2019) and multilingual embeddings. (Lauscher and Glavaˇs, 2019; Zhao et al., 2020). Bias analysis at the system level has investigated a range of applications such as NER (Mehrabi et al., 2020), Machine Translation (Stanovsky et al., 2019), Natural Language Inference (Rudinger et al., Dataset In order to conduct a study on the relationship between topic and author gender in multiple languages, we require a multilingual comparable corpus for which topic and gender information are available. The corpus should be as parallel as possible so that any differences in outcome across languages are not simply due to differences in the evaluation data. Among the available multilingual parallel data sets, arguably the two most prominent ones are WIT3 and OPUS. WIT3 (Cettolo et al., 2012) consists of lecture translations automatically crawl"
2021.wassa-1.6,P19-1161,0,0.017818,"hat when the target attribute and the bias attribute are too strongly correlated – or, indeed, when the target attribute is subsumed by the bias attribute – adversarial debiasing fails: with a small weight on the bias component, no debiasing takes place; with a large weight, target attribute classification deteriorates to baseline level. Furthermore, we find that the linguistic expression of the attributes matters greatly: the only language for which we achieved satisfactory results was French, due to the consistent morphological marking of gender which can be captured independently of topic (Zmigrod et al., 2019). This highlights the importance of understanding the differences between languages regarding how they encode content (Dubossarsky et al., 2019), and underscores the importance of cross-lingual methods. In future work, we plan to develop a diagnostic to recognize potentially problematic constellations of correlated attributes and improve debiasing. and translated talks. In Conference of European Association for Machine Translation, pages 261–268, Trento, Italy. Justin Cheng, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec. 2015. Antisocial behavior in online discussion communities. In Proc"
2021.wassa-1.6,tiedemann-2012-parallel,0,0.0112682,"d author gender in multiple languages, we require a multilingual comparable corpus for which topic and gender information are available. The corpus should be as parallel as possible so that any differences in outcome across languages are not simply due to differences in the evaluation data. Among the available multilingual parallel data sets, arguably the two most prominent ones are WIT3 and OPUS. WIT3 (Cettolo et al., 2012) consists of lecture translations automatically crawled from the TED talks in a variety of languages and was used in the evaluation campaigns 51 IWSLT 2013 and 2014. OPUS (Tiedemann, 2012) is a collection of data from several sources which provides sentence alignments as well as linguistic markup (for some languages). Unfortunately, neither corpus provides topic or gender labels. # TED Talks Author Gender Talk Topic For this reason, we create a new multilingual parallel dataset with these annotations, based on TED talks (http://ted.com/talks). A TED talk is a presentation at the TED conference or one of its international partner events. TED talks are limited to a maximum length of 18 minutes and may be on any topic. TED talks are rehearsed talks and at least semi-formal, while"
2021.wassa-1.6,D19-1599,0,0.0214135,"Missing"
2021.wassa-1.6,N19-4013,0,0.0360553,"Missing"
burchardt-etal-2006-salsa,erk-pado-2006-shalmaneser,1,\N,Missing
burchardt-etal-2006-salsa,burchardt-etal-2006-salto,1,\N,Missing
burchardt-etal-2006-salsa,fliedner-2006-towards,0,\N,Missing
burchardt-etal-2006-salsa,W04-2703,0,\N,Missing
burchardt-etal-2006-salsa,H05-1047,0,\N,Missing
burchardt-etal-2006-salsa,P98-1013,0,\N,Missing
burchardt-etal-2006-salsa,C98-1013,0,\N,Missing
burchardt-etal-2006-salsa,J02-3001,0,\N,Missing
burchardt-etal-2006-salsa,J05-1004,0,\N,Missing
burchardt-etal-2006-salsa,erk-pado-2004-powerful,1,\N,Missing
burchardt-etal-2006-salsa,W04-1906,1,\N,Missing
burchardt-etal-2006-salto,brants-plaehn-2000-interactive,0,\N,Missing
burchardt-etal-2006-salto,P97-1003,0,\N,Missing
burchardt-etal-2006-salto,P98-1013,0,\N,Missing
burchardt-etal-2006-salto,C98-1013,0,\N,Missing
burchardt-etal-2006-salto,P03-1068,1,\N,Missing
burchardt-etal-2006-salto,erk-pado-2004-powerful,1,\N,Missing
C08-1084,W06-1617,0,0.307641,"Missing"
C08-1084,P03-1068,1,0.875425,"Missing"
C08-1084,W03-1007,0,0.0595431,"Missing"
C08-1084,J02-3001,0,0.793508,"less clear how the combined meaning of phrases can be described. Semantic roles describe an important aspect of phrasal meaning by characterising the relationship between predicates and their arguments on a semantic level (e.g., agent, patient). They generalise over surface categories (such as subject, object) and variations (such as diathesis alternations). Two frameworks for semantic roles have found wide use in the community, PropBank (Palmer et al., 2005) and FrameNet (Fillmore et al., 2003). Their corpora are used to train supervised models for semantic role labelling (SRL) of new text (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). The resulting analysis can benefit a number of applications, such c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. as Information Extraction (Moschitti et al., 2003) or Question Answering (Frank et al., 2007). A commonly encountered criticism of semantic roles, and arguably a major obstacle to their adoption in NLP, is their limited coverage. Since manual semantic role tagging is costly, it is hardly conceivable that gold standard anno"
C08-1084,P07-1025,0,0.0844952,"o generalise from seen predicates to unseen predicates for which no training data is available. Techniques for extending the coverage of SRL therefore address an important need. Unfortunately, pioneering work in unsupervised SRL (Swier and Stevenson, 2004; Grenager and Manning, 2006) currently either relies on a small number of semantic roles, or cannot identify equivalent roles across predicates. A promising alternative direction is automatic data expansion, i.e., leveraging existing annotations to classify unseen, but similar, predicates. The feasibility of this approach was demonstrated by Gordon and Swanson (2007) for syntactically similar verbs. However, their approach requires at least one annotated instance of each new predicate, limiting its practicability. In this paper, we present a pilot study on the application of automatic data expansion to event nominalisations of verbs, such as agreement for agree or destruction for destroy. While event nominalisations often afford the same semantic roles as verbs, and often replace them in written language (Gurevich et al., 2006), they have played a largely marginal role in annotation. PropBank has only annotated verbs.1 FrameNet annotates nouns, but covers"
C08-1084,W06-1601,0,0.0172172,"bstacle to their adoption in NLP, is their limited coverage. Since manual semantic role tagging is costly, it is hardly conceivable that gold standard annotation will ultimately be available for every predicate of English. In addition, the lexically specific nature of the mapping between surface syntax and semantic roles makes it difficult to generalise from seen predicates to unseen predicates for which no training data is available. Techniques for extending the coverage of SRL therefore address an important need. Unfortunately, pioneering work in unsupervised SRL (Swier and Stevenson, 2004; Grenager and Manning, 2006) currently either relies on a small number of semantic roles, or cannot identify equivalent roles across predicates. A promising alternative direction is automatic data expansion, i.e., leveraging existing annotations to classify unseen, but similar, predicates. The feasibility of this approach was demonstrated by Gordon and Swanson (2007) for syntactically similar verbs. However, their approach requires at least one annotated instance of each new predicate, limiting its practicability. In this paper, we present a pilot study on the application of automatic data expansion to event nominalisati"
C08-1084,P07-1027,0,0.528749,"undamental intuition is that it is possible to increase the annotation coverage of event nominalisations by data expansion from verbal instances, since the verbal and nominal predicates share a large part of the underlying argument structure. We assume that annotation is available for verbal instances. Then, for a given instance of a nominalisation and its arguments, the aim is to assign semantic role labels to these arguments. We solve this task by constructing mappings between the arguments of the noun and the semantic roles realised by the verb’s arguments. Crucially, unlike previous work (Liu and Ng, 2007), we do not employ a classical supervised approach, and thus do not require any nominal annotations. Structure of the paper. Sec. 2 provides background on nominalisations and SRL. Sec. 3 provides concrete details on our expansion-based approach to SRL for nominalisations. The second part of the paper (Sec. 4–6) provides a first evaluation of different mapping strategies based on syntactic, semantic, and hybrid information. Sec. 8 concludes. 2 Nominalisations Nominalisations (or deverbal nouns) are commonly defined as nouns morphologically derived from verbs, usually by suffixation (Quirk et al"
C08-1084,meyers-etal-2004-annotating,0,0.0617771,"least one annotated instance of each new predicate, limiting its practicability. In this paper, we present a pilot study on the application of automatic data expansion to event nominalisations of verbs, such as agreement for agree or destruction for destroy. While event nominalisations often afford the same semantic roles as verbs, and often replace them in written language (Gurevich et al., 2006), they have played a largely marginal role in annotation. PropBank has only annotated verbs.1 FrameNet annotates nouns, but covers far fewer nouns than verbs. The same 1 A follow-up project, NomBank (Meyers et al., 2004), has since provided annotations for nominal instances, too. 665 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 665–672 Manchester, August 2008 situation holds in other languages (Erk et al., 2003). Our fundamental intuition is that it is possible to increase the annotation coverage of event nominalisations by data expansion from verbal instances, since the verbal and nominal predicates share a large part of the underlying argument structure. We assume that annotation is available for verbal instances. Then, for a given instance of a nominali"
C08-1084,J07-2002,1,0.837428,"Missing"
C08-1084,J05-1004,0,0.0534777,"c properties of individual words, both in the form of hand-built resources like WordNet and data-driven methods like semantic space models. It is still much less clear how the combined meaning of phrases can be described. Semantic roles describe an important aspect of phrasal meaning by characterising the relationship between predicates and their arguments on a semantic level (e.g., agent, patient). They generalise over surface categories (such as subject, object) and variations (such as diathesis alternations). Two frameworks for semantic roles have found wide use in the community, PropBank (Palmer et al., 2005) and FrameNet (Fillmore et al., 2003). Their corpora are used to train supervised models for semantic role labelling (SRL) of new text (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). The resulting analysis can benefit a number of applications, such c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. as Information Extraction (Moschitti et al., 2003) or Question Answering (Frank et al., 2007). A commonly encountered criticism of semantic roles, and arguably a m"
C08-1084,N04-4036,0,0.506059,"Missing"
C08-1084,W04-3212,0,0.0953581,"Missing"
C08-1084,W04-3213,0,\N,Missing
C08-1084,J02-3004,0,\N,Missing
C14-1163,W02-0606,0,0.0485938,"ell as word sense disambiguation to determine the meaning of ambiguous words in context. Other related work comes from two areas: unsupervised morphology induction and semantic clustering. Unsupervised morphology induction is concerned with the automatic identification of morphological relations (cf. Hammarstr¨om and Borin (2011) for an overview). Most approaches in this area do not differentiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception) and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000; Baroni et al., 2002) take distributional information into account. Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational properties in their feature set to learn Catalan adjective classes. However, the input to such studies is almost always a set of words from the same part of speech with no prior morphological constraints, while our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent, and exhibit systematical variation in"
C14-1163,C10-1011,0,0.0137879,"Evert, 2005). The semantic similarity is measured by the cosine similarity between the vectors. Despite the size of the corpus, many lemmas from DE RIV BASE occur very infrequently, and due to the inflection in German, it is important to retrieve as many occurrences of each lemma as possible. 1731 We therefore use a very permissive two-step lemmatization scheme that starts from lemmas from the lexicon-based TreeTagger (Schmid, 1994), which provides reliable lemmas but with relatively low coverage, and supplements them with lemmas and parts of speech produced by the probabilistic MATE toolkit (Bohnet, 2010) when TreeTagger abstains. 3.2 Frequency Considerations The advantage of the string transformation-based construction of DE RIV BASE is its ability to include infrequent lemmas in the lexicon, and in fact DE RIV BASE includes more than 250,000 content lemmas, some of which occur not more than three times in SDeWaC. However, this is a potential problem when we build distributional representations for all lemmas in DE RIV BASE since it is known from the literature that similarity predictions for infrequent lemmas are often unreliable (Bullinaria and Levy, 2007). Our data conform to expectations"
C14-1163,J12-3005,0,0.0267642,"is concerned with the automatic identification of morphological relations (cf. Hammarstr¨om and Borin (2011) for an overview). Most approaches in this area do not differentiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception) and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000; Baroni et al., 2002) take distributional information into account. Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational properties in their feature set to learn Catalan adjective classes. However, the input to such studies is almost always a set of words from the same part of speech with no prior morphological constraints, while our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent, and exhibit systematical variation in parts of speech. To our knowledge, this challenging situation has not been addressed in previous studies. Recent work has also considered the opposite problem, namely using derivational morphology for improving distributio"
C14-1163,1993.eamt-1.1,0,0.384086,"Missing"
C14-1163,W99-0904,0,0.103679,"ch is a very productive process of word formation in Slavic languages but also in languages more closely related to English, like German ˇ (Stekauer and Lieber, 2005). Derivation comprises a large number of distinct patterns, many of which cross part of speech boundaries (nominalization, verbalization, adjectivization), but some of which do not (gender indicators like master / mistress, approximations like red / reddish). A simple way to conceptualize derivation is that it partitions a language’s vocabulary into derivational families of derivationally related lemmas (cf. Zeller et al. (2013), Gaussier (1999)). In WordNet, this type of information has been included to some extent by so-called “morpho-semantic” relations (Fellbaum et al., 2009), and the approach has been applied to languages other This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1728 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1728–1739, Dublin, Ireland, August 23-29 2014. lachen to laugh V sfx ‘er’ N"
C14-1163,N03-1013,0,0.101177,"Missing"
C14-1163,J06-2001,0,0.0368891,"logy induction is concerned with the automatic identification of morphological relations (cf. Hammarstr¨om and Borin (2011) for an overview). Most approaches in this area do not differentiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception) and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000; Baroni et al., 2002) take distributional information into account. Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational properties in their feature set to learn Catalan adjective classes. However, the input to such studies is almost always a set of words from the same part of speech with no prior morphological constraints, while our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent, and exhibit systematical variation in parts of speech. To our knowledge, this challenging situation has not been addressed in previous studies. Recent work has also considered the opposite problem, namely using derivational morphology for"
C14-1163,jacquemin-2010-derivational,0,0.0526293,"Missing"
C14-1163,W13-2608,0,0.0648607,"bove, the standard distributional approach of using plain cosine scores to measure the absolute amount of co-occurrences does not seem very promising, due to the low absolute numbers of shared dimensions of the two lemmas. We expect other similarity measures, e.g., the Lin measure (Lin, 1998), to perform equally poorly since they do not change the fundamental approach. Also, although using a large corpus for semantic space construction might ameliorate the situation, we would prefer to make improvements on the modeling side of semantic validation. We follow the ideas of Hare et al. (2009) and Lapesa and Evert (2013) who propose to consider semantic similarity in terms of ranks rather than absolute values. The advantage of rank-based similarity is that it takes the density of regions in the semantic space into account. That is, a low cosine value does not necessarily indicate low semantic relatedness – provided that the two words are located in a “sparse” region. Conversely, a high cosine value can be meaningless in a densely populated region. A second conceptual benefit of rank-based similarity is that it is directed: It is possible to distinguish the “forward” rank (the rank of l1 in the neighborhood of"
C14-1163,P13-1149,0,0.181468,"d-alone derivational lexicons such as CatVar (Habash and Dorr, 2003) for English, DE RIV BASE (Zeller et al., 2013) for German, or the multilingual CELEX (Baayen et al., 1996). Recent work has demonstrated that NLP can benefit from derivational knowledge. Shnarch et al. (2011) employ derivational knowledge in recognizing English textual entailment to better gauge the semantic similarity of text and hypothesis. Pad´o et al. (2013) improve the prediction of German semantic similarity judgments for lemma pairs by backing off to derivational families for infrequent lemmas. Luong et al. (2013) and Lazaridou et al. (2013) improve distributional semantic representations. Note that all of these applications make use of derivational knowledge to address various semantic tasks, working on the assumption that derivationally related words, as represented in derivational lexicons, are strongly semantically related. This assumption is not completely warranted, though. The development of wide-coverage derivational lexicons is generally driven by morphological information, using for example finite-state technology (Karttunen and Beesley, 2005) to characterize known derivational patterns in terms of string transformation"
C14-1163,W13-3512,0,0.106005,"nal information are stand-alone derivational lexicons such as CatVar (Habash and Dorr, 2003) for English, DE RIV BASE (Zeller et al., 2013) for German, or the multilingual CELEX (Baayen et al., 1996). Recent work has demonstrated that NLP can benefit from derivational knowledge. Shnarch et al. (2011) employ derivational knowledge in recognizing English textual entailment to better gauge the semantic similarity of text and hypothesis. Pad´o et al. (2013) improve the prediction of German semantic similarity judgments for lemma pairs by backing off to derivational families for infrequent lemmas. Luong et al. (2013) and Lazaridou et al. (2013) improve distributional semantic representations. Note that all of these applications make use of derivational knowledge to address various semantic tasks, working on the assumption that derivationally related words, as represented in derivational lexicons, are strongly semantically related. This assumption is not completely warranted, though. The development of wide-coverage derivational lexicons is generally driven by morphological information, using for example finite-state technology (Karttunen and Beesley, 2005) to characterize known derivational patterns in te"
C14-1163,P13-2128,1,0.86164,"Missing"
C14-1163,W07-1710,0,0.0708195,"Missing"
C14-1163,W00-0712,0,0.295011,"for a specific lemma, as well as word sense disambiguation to determine the meaning of ambiguous words in context. Other related work comes from two areas: unsupervised morphology induction and semantic clustering. Unsupervised morphology induction is concerned with the automatic identification of morphological relations (cf. Hammarstr¨om and Borin (2011) for an overview). Most approaches in this area do not differentiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception) and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000; Baroni et al., 2002) take distributional information into account. Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational properties in their feature set to learn Catalan adjective classes. However, the input to such studies is almost always a set of words from the same part of speech with no prior morphological constraints, while our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent, and exhibit syst"
C14-1163,P11-2098,0,0.035295,"Missing"
C14-1163,P13-1118,1,0.905696,"Missing"
C14-1163,faass-etal-2010-design,0,\N,Missing
C14-1163,2003.mtsummit-systems.9,0,\N,Missing
C14-1163,J11-2002,0,\N,Missing
C16-1122,D10-1115,0,0.0881323,"are represented as vectors in some underlying distributional space. The simple additive model predicts the derived word from the base word as d = b + p where p is a vector representing the semantic shift accompanying the derivation pattern. The simple multiplicative model, d = b p is very similar, but uses component-wise multiplication ( ) instead of addition to combine the base and pattern vectors (Mitchell and Lapata, 2010). The third model, the weighted additive model, enables a simple reweighting of the contributions of basis and pattern (d = αb + βp). Finally, the lexical function model (Baroni and Zamparelli, 2010) represents the pattern as a matrix P that is multiplied with the basis vector: d = Pb, essentially modelling derivation as linear mapping. This model is considerably more powerful than the others, however its number of parameters is quadratic in the number of dimensions of the underlying space, whereas the additive and multiplicative models only use a linear number of parameters. For their empirical evaluation, Lazaridou et al. (2013) considered a dataset of 18 English patterns defined as simple affixes – 4 within-POS (such as un-) and 14 across-POS (such as +ment) – and found that the lexica"
C16-1122,P13-4006,0,0.0342734,"hift vector p is computed as the average of the shift vectors across all word pairs from a single pattern, while the weighted additive model additionally optimizes α and β in a subsequent step. Since the lexical function model is more prone to overfitting due to its many parameters, we train it using ridge regression, employing generalized cross-validation to tune the regularization parameter on the training set. As a fifth, baseline model, we use the identity mapping, which simply predicts the basis vector as the vector of the derived word. Our implementation is based on the DISSECT toolkit (Dinu et al., 2013). Evaluation. The performance of the CDSMs is measured by how well the predicted vector aligns with the corpus-observed vector for the derived word. More concretely, we quantify the performance on each word pair by Reciprocal rank (RR), that is, 1 divided by the position of the predicted vector in the similarity-ranked list of the observed vector’s neighbors. Besides being a well-established evaluation measure in Information Retrieval, RR is also more sensitive than the “Recall out of n” measure used previously (Kisselew et al., 2015), which measures the 0–1 loss and also requires fixing a thr"
C16-1122,D08-1094,1,0.843223,"Missing"
C16-1122,P04-1048,0,0.0480596,"(row 1) this means attaching an affix (+it¨at). The other rows show that the pattern can be more complex, involving stem alternation (row 2; note that the infinitive suffix +en is inflectional), deletion of previous affixes (row 3), circumfixation (row 4), or no overt changes, i.e., conversion (row 5).1 Derivation can take place both within parts of speech (row 6) and across parts of speech. Derivation is a very productive process in many languages, notably Slavic languages. Thus, natural language processing (NLP) for these languages can profit from knowledge about derivational relationships (Green et al., 2004; Szpektor and Dagan, 2008; Pad´o et al., 2013). Nevertheless, derivation is a relatively understudied phenomenon in NLP, and few lexicons contain derivational information. For English, there are two main resources. CatVar (Habash and Dorr, 2003) is a database that groups 100K words of all parts of speech into 60K derivational families, i.e., derivationally related sets of words. The other is CELEX (Baayen et al., 1996), a multi-level lexical database for English, German, and Dutch, which covers about 50K English words and contains derivational information in its morphological annotation. For"
C16-1122,N03-1013,0,0.056161,"ation (row 4), or no overt changes, i.e., conversion (row 5).1 Derivation can take place both within parts of speech (row 6) and across parts of speech. Derivation is a very productive process in many languages, notably Slavic languages. Thus, natural language processing (NLP) for these languages can profit from knowledge about derivational relationships (Green et al., 2004; Szpektor and Dagan, 2008; Pad´o et al., 2013). Nevertheless, derivation is a relatively understudied phenomenon in NLP, and few lexicons contain derivational information. For English, there are two main resources. CatVar (Habash and Dorr, 2003) is a database that groups 100K words of all parts of speech into 60K derivational families, i.e., derivationally related sets of words. The other is CELEX (Baayen et al., 1996), a multi-level lexical database for English, German, and Dutch, which covers about 50K English words and contains derivational information in its morphological annotation. For German, DErivBase (Zeller et al., 2013) is a resource focused on derivation that groups 280K lemmas into 17K derivational families. As opposed to CatVar and CELEX, it also provides explicit information about the applicable derivation pattern at t"
C16-1122,P14-1006,0,0.0213232,"istributional semantics, or CDSMs (Mitchell and Lapata, 2010; Erk and Pad´o, 2008; Baroni et al., 2014; Coecke et al., 2010), have established themselves as a standard tool in computational semantics. Building on traditional distributional semantic models for individual words (Turney and Pantel, 2010), they are generally applied to compositionally compute phrase meaning by defining combination operations on the meaning of the phrase’s constituents. CDSMs have also been co-opted by the deep learning community for tasks including sentiment analysis (Socher et al., 2013) and machine translation (Hermann and Blunsom, 2014). A more recent development is the use of CDSMs to model meaning-related phenomena above and below syntactic structure; here, the term “composition” is used more generally to apply to processes of meaning combination from multiple linguistic units, e.g., above and below syntactic structure. Above the sentence level, such models attempt to predict the unfolding of discourse (Kiros et al., 2015). Below the word level, CDSMs have been applied to model word formation processes like compounding (church + tower → church tower) and (morphological) derivation (Lazaridou et al., 2013) (favor + able → f"
C16-1122,W15-0108,1,0.826306,"Missing"
C16-1122,P13-1149,0,0.288633,"e translation (Hermann and Blunsom, 2014). A more recent development is the use of CDSMs to model meaning-related phenomena above and below syntactic structure; here, the term “composition” is used more generally to apply to processes of meaning combination from multiple linguistic units, e.g., above and below syntactic structure. Above the sentence level, such models attempt to predict the unfolding of discourse (Kiros et al., 2015). Below the word level, CDSMs have been applied to model word formation processes like compounding (church + tower → church tower) and (morphological) derivation (Lazaridou et al., 2013) (favor + able → favorable). More concretely, given a distributional representation of a basis and a derivation pattern (typically an affix), the task of the CDSM is to predict a distributional representation of the derived word, without being provided with any additional information. Interest in the use of CDSMs in this context comes from the observation that derived words are often less frequent than their bases (Hay, 2003), and in the extreme case even completely novel; consequently, distributional evidence is often unreliable and sometimes unavailable. This is confirmed by Luong et al. (20"
C16-1122,W13-3512,0,0.128062,"ou et al., 2013) (favor + able → favorable). More concretely, given a distributional representation of a basis and a derivation pattern (typically an affix), the task of the CDSM is to predict a distributional representation of the derived word, without being provided with any additional information. Interest in the use of CDSMs in this context comes from the observation that derived words are often less frequent than their bases (Hay, 2003), and in the extreme case even completely novel; consequently, distributional evidence is often unreliable and sometimes unavailable. This is confirmed by Luong et al. (2013) who compare the performance of different types of word embeddings on a word similarity task and achieve poorer performance on data sets containing rarer and more complex words. Due to the Zipfian distribution there are many more rare than frequent word types in a corpus, which increases the need for methods being able to model derived words. In this paper, we ask to what extent the application of CDSMs to model derivation is a success story. The record is unclear on this point: Lazaridou et al. (2013), after applying a range of CDSMs to an English derivation dataset, report success, while Kis"
C16-1122,N13-1090,0,0.0225336,"ollowing Kisselew et al. (2015), to mitigate sparsity, for out-of-vocabulary words we back off to the lemmas produced by MATE Tools (Bohnet, 2010), which have higher recall but lower precision than TreeTagger. We also use the MATE dependency analysis to reconstruct lemmas for separated prefix verbs. 3 http://goo.gl/tiRJy0 1288 Prediction Models. To obtain the vector representations on which we can train our prediction models, we use CBOW, a state-of-the-art predictive distributional semantics space which been shown particularly effective for modelling word similarity and relational knowledge (Mikolov et al., 2013).4 (Considering the type of semantic space as a parameter is outside the scope of our study.) As both target and context elements, we use all 280K unique POS-disambiguated lemmas (nouns, adjectives, and verbs) from DErivBase. We use a within-sentence context window of size ±2 to either side of the target word, 300 context dimensions, negative sampling set to 15, and no hierarchical softmax. On these vector representations, we train four prediction models (cf. Section 2): the simple additive model, the simple multiplicative model, the weighted additive model, and the lexical function model. Eac"
C16-1122,P13-2128,1,0.862888,"Missing"
C16-1122,D13-1170,0,0.00757296,"rity. 1 Introduction Compositional models of distributional semantics, or CDSMs (Mitchell and Lapata, 2010; Erk and Pad´o, 2008; Baroni et al., 2014; Coecke et al., 2010), have established themselves as a standard tool in computational semantics. Building on traditional distributional semantic models for individual words (Turney and Pantel, 2010), they are generally applied to compositionally compute phrase meaning by defining combination operations on the meaning of the phrase’s constituents. CDSMs have also been co-opted by the deep learning community for tasks including sentiment analysis (Socher et al., 2013) and machine translation (Hermann and Blunsom, 2014). A more recent development is the use of CDSMs to model meaning-related phenomena above and below syntactic structure; here, the term “composition” is used more generally to apply to processes of meaning combination from multiple linguistic units, e.g., above and below syntactic structure. Above the sentence level, such models attempt to predict the unfolding of discourse (Kiros et al., 2015). Below the word level, CDSMs have been applied to model word formation processes like compounding (church + tower → church tower) and (morphological) d"
C16-1122,N10-1091,0,0.0353174,"Missing"
C16-1122,C08-1107,0,0.0148805,"ttaching an affix (+it¨at). The other rows show that the pattern can be more complex, involving stem alternation (row 2; note that the infinitive suffix +en is inflectional), deletion of previous affixes (row 3), circumfixation (row 4), or no overt changes, i.e., conversion (row 5).1 Derivation can take place both within parts of speech (row 6) and across parts of speech. Derivation is a very productive process in many languages, notably Slavic languages. Thus, natural language processing (NLP) for these languages can profit from knowledge about derivational relationships (Green et al., 2004; Szpektor and Dagan, 2008; Pad´o et al., 2013). Nevertheless, derivation is a relatively understudied phenomenon in NLP, and few lexicons contain derivational information. For English, there are two main resources. CatVar (Habash and Dorr, 2003) is a database that groups 100K words of all parts of speech into 60K derivational families, i.e., derivationally related sets of words. The other is CELEX (Baayen et al., 1996), a multi-level lexical database for English, German, and Dutch, which covers about 50K English words and contains derivational information in its morphological annotation. For German, DErivBase (Zeller"
C16-1122,W11-1301,0,0.0173134,"ns. All numeric variables (predictors and dependent variable) are z-scaled; frequency variables are logarithmized. 1289 Baseline Simple Add Weighted Add Mult LexFun Mean Reciprocal Rank 0.271 0.309 0.316 0.272 0.150 # Predictions used by Oracle (Experiment 2) # Predictions used by Regressionbased Ensemble (Experiment 2) 2139 954 1613 532 913 51 2306 3528 190 76 Table 2: Results for individual prediction models on test set • Prediction level predictors describe properties of the vector that the CDSM outputs. Following work on assessing the plausibility of compositionally constructed vectors by Vecchi et al. (2011), we compute the length of the vector (deriv norm) and the similarity of the vector to its nearest neighbors (deriv density), and the similarity between base vector and derived vector (base deriv sim); • Pattern level predictors. We represent the identity of the pattern, which is admissible since we can assume that the DErivBase patterns cover the (vast) majority of German derivation patterns (Clark, 1973). Unfortunately, this excludes a range of other predictors, such as the parts of speech of the base and derived words, due to their perfect collinearity with the pattern predictor. The rest o"
C16-1122,P13-1118,1,0.900912,"Missing"
C16-1122,2003.mtsummit-systems.9,0,\N,Missing
C16-1122,2014.lilt-9.5,0,\N,Missing
D07-1042,J02-2003,0,0.202254,"Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 400–409, Prague, June 2007. 2007 Association for Computational Linguistics generally share two problems: (a), limited coverage; and (b), the resource (at least partially) predetermines the generalisations that they can make. In this pa"
D07-1042,P07-1028,1,0.809619,"or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 400–409, Prague, June 2007. 2007 Association for Computational Linguistics generally share two problems: (a), limited coverage; and (b), the resource (at least partially) predetermines the generalisations that they can make. In this paper, we investigate whether it is possible to predict the plausibility of (v, r, a) triples in a completely corpus-driven way. We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. While that model relies on corpora with semantic role annotation, we show that it is possible to predict plausibility ratings solely on the basis of a parsed corpus, by using shallow cues and a suitable vector space specification. For evaluation, we use two balanced data sets of human plausibility judgements, i.e., datasets where each verb is paired both with a good agent and a good patient, and where both nouns are presented in either semantic relation (as in Table 1). Using balanced test data is a particularly difficult ta"
D07-1042,J02-3001,0,0.452368,"to model this type of data is relevant in a number of ways. From the point of view of psycholinguistics, selectional preferences have an important effect in human sentence processing (e.g., McRae et al. (1998), Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 400–409, Prague, June 2007. 20"
D07-1042,J93-1005,0,0.158514,"l preferences and argument, for such (verb, relation, argument), in short, (v, r, a), triples. Being able to model this type of data is relevant in a number of ways. From the point of view of psycholinguistics, selectional preferences have an important effect in human sentence processing (e.g., McRae et al. (1998), Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in"
D07-1042,P04-1061,0,0.0372102,"ctor space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. The goal of our exposition is thus to develop a model that can use more training data, and represent the corpus information optimally in order to obtain superior coverage. In fact, tasks (a) and (b) can be solved on the basis of unparsed corpora, but we would expect the results to be rather noisy. Fortunately, the state of the art in broad-coverage (Lin, 1993) and unsupervised (Klein and Manning, 2004) dependency parsing allows us to treat dependency parsing merely as a preprocessing step. We therefore describe two instantiations of our model: one based on an unprocessed corpus, and one based on a dependency-based parsed corpus. By comparing the models, we can gauge whether syntactic preprocessing improves model performance. In the following, we describe the strategies the two models adopt for (a) and (b). Example. Figure 1 shows an example vector space. Consider v = “shoot”, r = agent, and a = “hunter”. In order to judge whether a hunter is a plausible agent of “shoot”, the vector space re"
D07-1042,P99-1004,0,0.191718,"Seenr (v) by using all subjects and objects of v as agents and patients, respectively. We then construct a vector space for the experimental arguments and known headwords.2 We use 2,000 dimensions again, but adopt the most frequent (head , grammatical function) pairs in the BNC as basis elements. The context window is formed by subject and object dependencies. All counts are log-likelihood transformed. We experiment with two distance measures to compute vector similarity, namely the Jaccard Coefficient and Cosine Distance, both of which have been shown to yield good performance in NLP tasks (Lee, 1999; McDonald and Lowe, 1998). Evaluation Procedure. We evaluate our models by correlating the predicted plausibility values with the human judgements, which range between 1 and 7. Since the human judgement data is not normally distributed, we use Spearman’s ρ, a non-parametric rank-order test. We determine the statistical significance of differences in correlation strength using the method described in Raghunathan (2003). This method can deal with missing values and thus allows us to compare models with different coverage. It is difficult to specify a straightforward baseline for our correlation"
D07-1042,P93-1016,0,0.367173,"il which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. The goal of our exposition is thus to develop a model that can use more training data, and represent the corpus information optimally in order to obtain superior coverage. In fact, tasks (a) and (b) can be solved on the basis of unparsed corpora, but we would expect the results to be rather noisy. Fortunately, the state of the art in broad-coverage (Lin, 1993) and unsupervised (Klein and Manning, 2004) dependency parsing allows us to treat dependency parsing merely as a preprocessing step. We therefore describe two instantiations of our model: one based on an unprocessed corpus, and one based on a dependency-based parsed corpus. By comparing the models, we can gauge whether syntactic preprocessing improves model performance. In the following, we describe the strategies the two models adopt for (a) and (b). Example. Figure 1 shows an example vector space. Consider v = “shoot”, r = agent, and a = “hunter”. In order to judge whether a hunter is a plau"
D07-1042,J03-4004,0,0.311533,", argument), in short, (v, r, a), triples. Being able to model this type of data is relevant in a number of ways. From the point of view of psycholinguistics, selectional preferences have an important effect in human sentence processing (e.g., McRae et al. (1998), Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural"
D07-1042,P04-1036,0,0.146281,"alled basis elements). The underlying assumption is that words with similar meanings occur in similar contexts, and will be assigned similar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), is a measure of their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton et al., 1975), automatic thesaurus extraction (Grefenstette, 1994), and predominant sense identification (McCarthy et al., 2004). In cognitive science, they have been used to account for the influence of context on human lexical processing (McDonald and Brew, 2004), and to model lexical priming (Lowe and McDonald, 2000). 402 A drawback of vector space models is the difficulty of interpreting what some degree of “generic semantic similarity” between two target words means in linguistic terms. In particular, this similarity is not sensitive to selectional preferences over specific semantic relations, and thus cannot model the plausibility data we are interested in. The next section demonstrates how the integration of ide"
D07-1042,P04-1003,0,0.17007,"milar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), is a measure of their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton et al., 1975), automatic thesaurus extraction (Grefenstette, 1994), and predominant sense identification (McCarthy et al., 2004). In cognitive science, they have been used to account for the influence of context on human lexical processing (McDonald and Brew, 2004), and to model lexical priming (Lowe and McDonald, 2000). 402 A drawback of vector space models is the difficulty of interpreting what some degree of “generic semantic similarity” between two target words means in linguistic terms. In particular, this similarity is not sensitive to selectional preferences over specific semantic relations, and thus cannot model the plausibility data we are interested in. The next section demonstrates how the integration of ideas from selectional preference induction makes this distinction possible. 3 The Vector Similarity Model: Corpus-Based Modelling of Plausi"
D07-1042,J07-2002,1,0.460937,"model, one using unparsed and one parsed data. Both are trained on the complete British National Corpus (Burnard, 1995, BNC) with more than six million sentences. The unparsed model (Unparsed) uses the BNC without any pre-processing. We first construct the set of known headwords, Seenr (v), as follows: All words up to 2 words to the left of instances of v are assumed to be subjects, and thus agents; vice versa for patients to the right. Then, we construct semantic space representations for the experimental arguments and known headwords, adopting optimal parameter settings from the literature (Padó and Lapata, 2007). This means a context window of 5 words to either side and 2,000 basis elements (dimensions), which are formed by the most frequent 1,000 words 1 We are grateful to Ken McRae for his dataset. in the BNC, combined with each of the relations agent and patient. All counts are log-likelihood transformed (Lowe, 2001). To construct the parsed model (Parsed), we dependency-parsed the BNC with Minipar (Lin, 1993). We first obtain the seen headwords Seenr (v) by using all subjects and objects of v as agents and patients, respectively. We then construct a vector space for the experimental arguments and"
D07-1042,N07-1071,0,0.0339889,"view of psycholinguistics, selectional preferences have an important effect in human sentence processing (e.g., McRae et al. (1998), Trueswell et al. (1994)), and models of selectional preferences are therefore necessary to inform models of this process (Padó et al., 2006). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al., 2007). A number of approaches has been proposed to model selectional preference data (Padó et al., 2006; Resnik, 1996; Clark and Weir, 2002; Abe and Li, 1996). These models generally operate by generalising from seen (v, r, a) triples to unseen ones. By relying on resources like corpora with semantic role annotation or the WordNet ontology, these models 400 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 400–409, Prague, June 2007. 2007 Association for Computational Linguistics generally share two proble"
D07-1042,P99-1014,0,0.826006,"n with the most strongly associated WordNet ancestor class of the argument. WordNet-based approaches however face two problems. One is a coverage problem due to the limited size of the resource (see the task-based evaluation in Gildea and Jurafsky (2002)). The other is that the shape of the WordNet hierarchy determines the generalisations that the models make. These are not always intuitive. For example, Resnik (1996) observes that (answer, obj, tragedy) receives a high preference because “tragedy” in WordNet is a type of written communication, which is a preferred argument class of “answer”. Rooth et al. (1999) present a fundamentally different approach to selectional preference induction which uses soft clustering to form classes for generalisation and does not take recourse to any hand-crafted resource. We will argue in Section 6 that our model allows more control over the generalisations made. Modelling Selectional Preferences with Thematic Roles. Padó et al. (2006) present a deeper model for the plausibility of (v, r, a) triples that approximates the relations with thematic roles. It estimates the selectional preferences of a verb-role pair with a generative probability model that equates the pl"
D08-1094,E03-1034,0,0.0271593,"ectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pad´o et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pad´o et al., 2007). We first present the SVS model of word meaning accuse say claim whirl fly provide throw catch organise ... comp-1 catch"
D08-1094,P07-1028,1,0.613356,"ng been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pad´o et al., 2007). We first present the SVS model of word meaning accuse say claim whirl fly provide throw catch organise ... comp-1 catch catch subj he fielder dog obj-1 subj-1 comp-1 subj ball obj cold baseball drift ! throw catch organise obj-1 obj ... subj-1 mod red golf elegant ... Figure 1: Structured meaning representations for noun ball and verb catch : lexical information plus expectations that integrates lexical information with selectional preferences. Then, we show how the SVS model provides a new way of computing meaning in context. Representing lemma meaning. We abandon the t"
D08-1094,J02-3001,0,0.0239697,"al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pad´o et al., 2007). We first present the SVS model of word meaning accuse say claim whirl fly provide throw catch organise ... comp-1 catch catch subj he fielder dog obj-1 subj-1 comp-1 subj ball obj cold baseball drift ! throw catch organise obj-1 obj ... subj-1 mod red golf elegant ... Figure 1: Structured meaning representations for noun ball and verb catch : lexical information plus expectations that in"
D08-1094,J93-1005,0,0.117728,"ticiples (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002; Pad´o et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998; McRae et al., 2005). In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964; Wilks, 1975), and more recently induced from corpora (Resnik, 1996; Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007; Pad´o et al., 2007). We first present the SVS model of word meaning accuse say claim whirl fly provide throw catch organise ... comp-1 catch catch subj he fielder dog obj-1 subj-1 comp-1 subj ball obj cold baseball drift ! throw catch organise obj-1 obj ... subj-1 mod red golf elegant ... Figure 1:"
D08-1094,P93-1016,0,0.0216957,", (Lund and Burgess, 1996)). For each pair of a target word and context word, the B OW space records a function of their co-occurrence frequency within a surface window of size 10. The space is constructed from the British National Corpus (BNC), and uses the 2,000 most frequent context words as dimensions. We also consider a “dependency-based” vector space (S YN, (Pad´o and Lapata, 2007)). In this space, target and context words have to be linked by a “valid” dependency path in a dependency graph to count as co-occurring.2 This space was built from BNC dependency parses obtained from Minipar (Lin, 1993). For both spaces, we used pre-experiments to compare two methods for the computation of vector components, namely raw co-occurrence counts, the standard model, and the pointwise mutual information (PMI) definition employed by M&L. Selectional preferences. We use a simple, knowledge-lean representation for selectional preferences inspired by Erk (2007), who models selectional preference through similarity to seen filler vectors ~va : We compute the selectional preference vector for word b and relation r as the weighted 2 More specifically, we used the minimal context specification and plain we"
D08-1094,P98-2127,0,0.736346,"ces of a noun and (left) its lexical vector and (right) inverse object preferences vector (cosine similarity in S YN space) ρ = 0.2, significantly outperforming both baselines. It is interesting, though, that the subj −1 preference itself (“Selpref only”) is already highly significantly correlated with the human judgments. A comparison of the upper half (B OW) with the lower half (S YN) shows that the dependency-based space generally shows better correlation with human judgements. This corresponds to a beneficial effect of syntactic information found for other applications of semantic spaces (Lin, 1998; Pad´o and Lapata, 2007). All instances of the SELPREF model show highly significant correlations. SELPREF and SELPREF - CUT show very similar performance. They do better than both baselines in the B OW space; however, in the cleaner S YN space, their performance is numerically lower than using selectional preferences only (ρ = 0.13 vs. 0.16). SELPREF - POW is always significantly better than SELPREF and SELPREF - CUT, and shows the best result of all tested models (ρ = 0.27, B OW space). The performance is somewhat lower in the S YN space (ρ = 0.22). However, this difference, and the differe"
D08-1094,J03-4004,0,0.230695,"e to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases. 1 Introduction Semantic spaces are a popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over Sebastian Pad´o Department of Linguistics Stanford University pado@stanford.edu all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the m"
D08-1094,S07-1009,0,0.569077,"nal vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975; McCarthy and Carroll, 2003; Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over Sebastian Pad´o Department of Linguistics Stanford University pado@stanford.edu all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Sch¨utze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) tha"
D08-1094,P04-1003,0,0.150376,"problems (Kilgarriff, 1997; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over Sebastian Pad´o Department of Linguistics Stanford University pado@stanford.edu all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Sch¨utze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a b. The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can influence the interpretation of nouns: In (1a), ball is unde"
D08-1094,P08-1028,0,0.893726,"7; McCarthy and Navigli, 2007). In a default semantic space as described above, each vector represents one lemma, averaging over Sebastian Pad´o Department of Linguistics Stanford University pado@stanford.edu all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context. There have been several approaches in the literature (Smolensky, 1990; Sch¨utze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a b. The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can influence the interpretation of nouns: In (1a), ball is understood as a spherical object"
D08-1094,P08-2029,0,0.0123258,"t paraphrase appropriateness based on the similarity between vectors. This task can also be addressed with kernel methods, which project items into an implicit feature space for efficient similarity computation. Consequently, vector space methods and kernel methods have both been used for NLP tasks based on similarity, notably Information Retrieval and Textual Entailment. Nevertheless, they place their emphasis on different 899 types of information. Current kernels are mostly tree kernels that compare syntactic structure, and use semantic information mostly for smoothing syntactic similarity (Moschitti and Quarteroni, 2008). In contrast, vector-space models focus on the interaction between the lexical meaning of words in composition. 3 A structured vector space model for word meaning in context In this section, we define the structured vector space (SVS) model of word meaning. The main intuition behind our model is to view the interpretation of a word in context as guided by expectations about typical events. For example, in (1a), we assume that upon hearing the phrase “catch a ball”, the hearer will interpret the meaning of “catch” to match typical actions that can be performed with a ball. Similarly, the inter"
D08-1094,J07-2002,1,0.643973,"Missing"
D08-1094,D07-1042,1,0.81262,"Missing"
D08-1094,J98-1004,0,0.949911,"Missing"
D08-1094,P08-1078,0,0.0397749,"Missing"
D08-1094,C00-2137,0,0.0526464,"he selectional preferences of b model the expectations for a, we use b’s selectional preference vector for the given relation as a second baseline, “selpref only”. Since we focus on the size-invariant cosine similarity, the use of this model does not require normalization. 902 landmark slouch decline slouch decline sim high low low high judgment 7 2 3 7 Figure 3: Experiment 1: Human similarity judgements for subject-verb pair with high- and low-similarity landmarks Differences between the performance of models were tested for significance using a stratified shuffling-based randomization test (Yeh, 2000).4 . 5 Exp. 1: Predicting similarity ratings In our first experiment, we attempt to predict human similarity judgments. This experiment is a replication of the evaluation of M&L on their dataset5 . Dataset. The M&L dataset comprises a total of 3,600 human similarity judgements for 120 experimental items. Each item, as shown in Figure 3, consists of an intransitive verb and a subject noun that are combined with a “landmark”, a synonym of the verb that is chosen to be either similar or dissimilar to the verb in the context of the given subject. The dataset was constructed by extracting pairs of"
D08-1094,W07-1401,0,\N,Missing
D08-1094,C98-2122,0,\N,Missing
D15-1002,J10-4006,1,0.736996,"Missing"
D15-1002,J10-4007,1,0.625165,"Missing"
D15-1002,P09-1113,0,0.00429198,"-of::organization with the value world bank results in a binary attribute Referential Representations As our source of referential attributes, we use FreeBase (see footnote 1), a knowledge base of structured information on a wide range of entities of different semantic types (people, geographical entities, etc.). The information in FreeBase comes from various sources, including Wikipedia and domainspecific databases, plus user content generation and correction. FreeBase currently records at least 2 attributes for over 47 million entities, and it has been used fairly extensively in NLP before (Mintz et al., 2009; Socher et al., 2013a, among others). For each entity, FreeBase contains a list of attribute-value tuples (where values can in turn be entities, allowing a graph view of the data that we do not exploit here). Table 1 shows a sample of the attributes that FreeBase records for countries. Note that some attributes are simple (e.g., date founded), while other can be called complex, in the sense that they are attributes of attributes (e.g., geolocation::latitude). We use a double-colon notation to refer to complex attributes. The values of all attributes can be either numeric or categorical. The n"
D15-1002,W15-0107,0,0.0504358,"Missing"
D15-1002,S12-1019,0,0.0125682,"th Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to construct and populate structured knowledge bases (KBs) (e.g., B"
D15-1002,J07-2002,1,0.247538,"Missing"
D15-1002,P15-2119,0,0.0494805,"neric noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to construct and populate structured knowledge bases (KBs) (e.g., Buitelaar and Cimiano (2008) and references therein). This line of work, however, does not attempt to connect entity representations extracted from corpora and fro"
D15-1002,W15-0120,0,0.0479495,"e rely on the same architecture to learn discrete features denoting relations with entities and numerical features, to induce full attribute-based descriptions of entities. Our proposal is only distantly related to methods to embed words tokens and KB entities and relationships in a vector space, e.g., for better relation extraction (see Weston et al. (2013) and references therein). This line of work does not use distributional semantics to induce word vectors, and ignores numerical attributes. The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 201"
D15-1002,Q14-1023,0,0.0142536,"tional semantics to induce word vectors, and ignores numerical attributes. The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a"
D15-1002,P14-1132,1,0.435548,"eferential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to co"
D15-1002,D13-1136,0,0.0119385,"ose in spirit to what we do, except that, given an entity1-relation-entity2 tuple, we treat relation-entity2 as a binary attribute of entity1, and we try to induce such attributes on a larger scale (Socher et al. consider seven relations in total). Moreover, we rely on the same architecture to learn discrete features denoting relations with entities and numerical features, to induce full attribute-based descriptions of entities. Our proposal is only distantly related to methods to embed words tokens and KB entities and relationships in a vector space, e.g., for better relation extraction (see Weston et al. (2013) and references therein). This line of work does not use distributional semantics to induce word vectors, and ignores numerical attributes. The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such"
D15-1002,N13-1090,0,0.154617,"Missing"
D18-2008,W09-1403,0,0.0134523,"f the proof-of-concept system for aspect based sentiment analysis (10-fold crossvalidation on USAGE corpus). ery. The evaluation calculates the F1 scores for the individual frames (events in the BioNLP task) using a soft matching for trigger boundaries and approximate recursive matching. Table 1 provides the results of our simple system on that task. Due to the restriction of our proof of concept to nonrecursive structures (cf. Section 4), we only report on the BioNLP event types where all slots are filled by spans. In comparison to the second-ranked system, which also reports results on dev (Buyko et al., 2009), our performance is slightly lower (1 percentage point less for protein catabolism, 13pp less for gene expression and phosphorylation, but 11pp more for localization). This confirms the general usability of our general method. Correspondingly, Table 2 provides the current results of the same model on the USAGE corpus for aspect based sentiment analysis (Klinger and Cimiano, 2014), with 10-fold crossvalidation on the English subset. In comparison to previous results, our numbers are very low. Previous work showed that this is tackled by joint inference, which we did not implement yet (Klinger"
D18-2008,W02-0109,0,0.194921,"for which we re-use the original evaluation machinWe generate negative examples automatically. 45 Event Class Precision Recall F1 Gene expression Transcription Protein catabolism Phosphorylation Localization 68.12 70.59 64.00 65.85 78.57 57.30 14.63 76.19 57.45 41.51 62.25 24.24 69.57 61.36 54.32 SVT-TOTAL 68.46 50.27 57.97 The choice of Python will also help with future integration of neural network models. For the proofof-concept backend, we use scikit-learn for feature extraction and training (Pedregosa et al., 2011) with crfsuite and liblinear. Tokenization and stemming is done with NLTK (Loper and Bird, 2002), dependency features are extracted with spacy (Honnibal and Johnson, 2015) and dependency graphs are stored and processed using NetworkX (Schult, 2008). The code is available under the Apache 2.0 License.2 Table 1: Performance of the proof-of-concept system for biomedical relation extraction (BioNLP’09 dev set) Sentiment Class Positive Negative Neutral Precision Recall F1 41.07 26.68 5.83 24.19 7.15 4.50 28.57 11.00 5.08 5 Conclusion and Future Work This paper presented D E RE, a general framework for declarative specification and compilation of template-based slot filling. It addresses the n"
D18-2008,clarke-etal-2012-nlp,0,0.0149051,"g (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of transcription factors, of which NFAT1 and NFAT2 are most prominent in peripheral T cells. tems we are aware of for solving these tasks are tailored to specific scenarios (Angeli et al., 2016; Adel et al., 2016, i.a.). As a result, it is not straightforward to apply them to other use cases. In contrast, our framework is designed to be task- and domain-independent. Clarke et al. (2012) develop an NLP component manager which combines several existing NLP tools in a pipeline. Similarly, Curran (2003) aims at a general NLP infrastructure but only reports implementations of non-relational sequence-tagging tasks. Examples of the few available toolkits which are intended to provide users with the possibility of automatically extracting information from text data are Jet (Java Extraction Toolkit), GATE (General Architecture for Text Engineering, Cunningham et al., 2013), UIMA (Unstructured Information Management Architecture, Ferrucci and Lally, 2004), FACTORIE (McCallum et al., 2"
D18-2008,N16-1030,0,0.0229713,"ble to apply D E RE to a large variety of natural language processing tasks, such as unary, binary and n-ary relation extraction, event extraction, semantic role labeling, aspect-based sentiment analysis, etc. As BRAT annotations are not as expressive as our task schema files, we plan to extend the frontend of D E RE by supporting a native, XML-based annotation format in the future. For the backend, our goal is to develop a variety of state-of-the-art models with joint span identification, slot classification, and frame decoding, e.g., neural networks with structured-prediction output layers (Lample et al., 2016; Adel and Sch¨utze, 2017, i.a.). Given a variety of different models and tasks, we will be able to address interesting research questions, such a transfer learning and joint learning across tasks and domains. We plan to further analyze the usage of D E RE and the possibilities it provides for integrating different model types and configurations in a multi-task oriented shared task. Table 2: Performance of the proof-of-concept system for aspect based sentiment analysis (10-fold crossvalidation on USAGE corpus). ery. The evaluation calculates the F1 scores for the individual frames (events in t"
D18-2008,P09-1113,0,0.0786889,"n be instantiated by different models, following different paradigms. The clear separation of frame specification and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Me"
D18-2008,N16-1034,0,0.0302282,"software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of transcription factors, of which NFAT1 and NFAT2 are most prominent in peripheral T cells. tems we are aware of for solving these tasks are tailored"
D18-2008,S10-1057,0,0.0247273,"s transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of t"
D18-2008,doddington-etal-2004-automatic,0,0.146517,"hallenges. The backend can be instantiated by different models, following different paradigms. The clear separation of frame specification and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Confer"
D18-2008,E12-2021,0,0.032324,"Missing"
D18-2008,S10-1006,1,0.839272,"Missing"
D18-2008,D15-1167,0,0.0310034,"able as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of transcription factors, of which NFAT1 and NFAT2 are most prominent in peripheral T cells. tems we are aware of for solving th"
D18-2008,D15-1162,0,0.0244088,"e examples automatically. 45 Event Class Precision Recall F1 Gene expression Transcription Protein catabolism Phosphorylation Localization 68.12 70.59 64.00 65.85 78.57 57.30 14.63 76.19 57.45 41.51 62.25 24.24 69.57 61.36 54.32 SVT-TOTAL 68.46 50.27 57.97 The choice of Python will also help with future integration of neural network models. For the proofof-concept backend, we use scikit-learn for feature extraction and training (Pedregosa et al., 2011) with crfsuite and liblinear. Tokenization and stemming is done with NLTK (Loper and Bird, 2002), dependency features are extracted with spacy (Honnibal and Johnson, 2015) and dependency graphs are stored and processed using NetworkX (Schult, 2008). The code is available under the Apache 2.0 License.2 Table 1: Performance of the proof-of-concept system for biomedical relation extraction (BioNLP’09 dev set) Sentiment Class Positive Negative Neutral Precision Recall F1 41.07 26.68 5.83 24.19 7.15 4.50 28.57 11.00 5.08 5 Conclusion and Future Work This paper presented D E RE, a general framework for declarative specification and compilation of template-based slot filling. It addresses the needs of three groups of users: backend model developers, developers of info"
D18-2008,P13-1161,0,0.0340886,"s slightly lower (1 percentage point less for protein catabolism, 13pp less for gene expression and phosphorylation, but 11pp more for localization). This confirms the general usability of our general method. Correspondingly, Table 2 provides the current results of the same model on the USAGE corpus for aspect based sentiment analysis (Klinger and Cimiano, 2014), with 10-fold crossvalidation on the English subset. In comparison to previous results, our numbers are very low. Previous work showed that this is tackled by joint inference, which we did not implement yet (Klinger and Cimiano, 2013; Yang and Cardie, 2013). However, this proof-of-concept implementation of the same model already shows the reusability of our framework by only changing the task schema specification. It motivates and enables further research on reusable models across tasks with different needs. Technical Details and Availability. The framework is implemented in Python, following an object-oriented design for frontend and backends to support easy interchangeability of components. Acknowledgments This work has been partially funded by the German Research Council (DFG), projects KL 2869/1-1 and PA 1956/4-1. 2 http://www.ims.uni-stuttg"
D18-2008,C14-1220,0,0.0130071,"essment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics IL-4 gene regulation in general involves the nuclear factor of activated T cells (NFAT) family of transcription factors, of which NFAT1 and NFAT2 are most prominent in periph"
D18-2008,W09-1401,0,0.454261,"tion and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. D E RE is available as open-source software. 1 Introduction A large number of tasks in natural language processing (NLP) are information extraction (IE) tasks, such as n-ary relation extraction (Doddington et al., 2004; Mintz et al., 2009; Hendrickx et al., 2010), semantic role labeling (Das et al., 2014) and event extraction (Kim et al., 2009; Doddington et al., 2004). Researchers address these tasks with a variety of different model paradigms, such as support vector machines (Rink and Harabagiu, 2010), convolutional neural networks (Collobert et al., 2011; Zeng et al., 2014) and recurrent neural networks (Tang et al., 2015; Nguyen et al., 2016). This landscape of different tasks and models gives rise to four challenges: (C1) Lack of gener∗ All authors contributed equally. 42 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 42–47 c Brussels, Belgium, October 31–N"
E12-1064,P88-1018,0,0.358685,"arallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and disc"
E12-1064,P11-1078,0,0.0343121,"ra to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using the technique of annotation projection (Yar"
E12-1064,C10-2010,0,0.0180212,"th century) indicated T (“thee”, “didst”). We cleaned the English and German novels manually by deleting the tables of contents, prologues, epilogues, as well as chapter numbers and titles occurring at the beginning of each chapter to obtain properly parallel texts. The files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005). Blank lines were inserted to preserve paragraph boundaries. All novels were lemmatized and POS-tagged using TreeTagger (Schmid, 1994).2 Finally, they were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments, and word-aligned in both directions using Giza++ (Och and Ney, 2003). 3.2 T/V Gold Labels for English Utterances As Figure 1 shows, the automatic construction of T/V labels for English involves two steps. Step 1: Labeling German Pronouns as T/V. German has three relevant personal pronouns for the T/V distinction: du (T), sie (V), and ihr (T/V). However, various ambiguities makes their interpretation non-straightforward. The pronoun ihr can both be used for plural T address or for a somewhat archaic singular or plural V address. In principle, t"
E12-1064,E03-1009,0,0.0189628,"eature selection (Manning et al., 2008) on the training set. Preliminary experiments established that selecting the top 800 word features yielded a model with good generalization. Semantic Class Features. Our second feature type is semantic class features. These can be seen as another strategy to counteract the sparseness at the level of word features. We cluster words into 400 semantic classes on the basis of distributional and morphological similarity features which are extracted from an unlabeled English collection of Gutenberg novels comprising more than 100M tokens, using the approach by Clark (2003). These features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in Named Entity Recognition (Finkel and Manning, 2009). As features in the T/V classification of a given sentence, we simply count for each class the number of tokens in this class present in the current sentence. For illustration, Table 2 shows the three classes most indicative for V, ranked by the ratio of probabilities for T and V, estimated on the training set. Politeness Theory Features. The third feature type is based on the Politeness Theory (Brown and Levinson, 19"
E12-1064,P10-1015,0,0.0655988,"ion projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using"
E12-1064,P11-2082,1,0.151896,"the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using the technique of annotation projection (Yarowsky and Ngai, 2001) sketched in Figure 1: We first identify the T/V status of German pronouns, then copy this T/V information onto the corresponding English sentence. 3.1 Data Selection and Preparation Annotation projection requires a"
E12-1064,D09-1015,0,0.0211312,"eneralization. Semantic Class Features. Our second feature type is semantic class features. These can be seen as another strategy to counteract the sparseness at the level of word features. We cluster words into 400 semantic classes on the basis of distributional and morphological similarity features which are extracted from an unlabeled English collection of Gutenberg novels comprising more than 100M tokens, using the approach by Clark (2003). These features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in Named Entity Recognition (Finkel and Manning, 2009). As features in the T/V classification of a given sentence, we simply count for each class the number of tokens in this class present in the current sentence. For illustration, Table 2 shows the three classes most indicative for V, ranked by the ratio of probabilities for T and V, estimated on the training set. Politeness Theory Features. The third feature type is based on the Politeness Theory (Brown and Levinson, 1987). Brown and Levinson’s prediction is that politeness levels will be detectable in concrete utterances in a number of ways, e.g. a higher use of conjunctive or hedges in polite"
E12-1064,W09-0420,0,0.030874,"s that are expressed overtly 623 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623–633, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics V projection Darf ich Sie etwas fragen? Step 1: German pronoun provides overt T/V label V Please permit me to ask you a question. Step 2: copy T/V class label to English sentence Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy."
E12-1064,C90-3028,0,0.0461537,"sh sentence Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, inve"
E12-1064,W03-1612,0,0.0144459,"label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger"
E12-1064,2005.mtsummit-papers.11,0,0.0148346,"feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using the technique of annotation projection (Yarowsky and Ngai, 2001) sketched in Figure 1: We first identify the T/V status of German pronouns, then copy this T/V information onto the corresponding English sentence. 3.1 Data Selection and Preparation Annotation projection requires a parallel corpus. We found commonly used parallel corpora like EUROPARL (Koehn, 2005) or the JRC Acquis corpus (Steinberger et al., 2006) to be unsuitable for our study since they either contain almost no direct address at all or, if they do, just formal address (V). Fortunately, for many literary texts from the 19th and early 20th century, copyright has expired, and they are freely available in several languages. We identified 110 stories and novels among the texts provided by Project Gutenberg (English) and Project Gutenberg-DE (German)1 that were available in both languages, with a total of 0.5M sentences per language. Examples are Dickens’ David Copperfield or Tolstoy’s An"
E12-1064,D08-1108,0,0.0158898,"ith annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in de"
E12-1064,J03-1002,0,0.00445845,"ts, prologues, epilogues, as well as chapter numbers and titles occurring at the beginning of each chapter to obtain properly parallel texts. The files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005). Blank lines were inserted to preserve paragraph boundaries. All novels were lemmatized and POS-tagged using TreeTagger (Schmid, 1994).2 Finally, they were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments, and word-aligned in both directions using Giza++ (Och and Ney, 2003). 3.2 T/V Gold Labels for English Utterances As Figure 1 shows, the automatic construction of T/V labels for English involves two steps. Step 1: Labeling German Pronouns as T/V. German has three relevant personal pronouns for the T/V distinction: du (T), sie (V), and ihr (T/V). However, various ambiguities makes their interpretation non-straightforward. The pronoun ihr can both be used for plural T address or for a somewhat archaic singular or plural V address. In principle, these usages should be distinguished by capitalization (V pronouns are generally capitalized in German), but many T inst"
E12-1064,W95-0107,0,0.0782785,"Missing"
E12-1064,P98-2193,0,0.395739,"tly 623 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623–633, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics V projection Darf ich Sie etwas fragen? Step 1: German pronoun provides overt T/V label V Please permit me to ask you a question. Step 2: copy T/V class label to English sentence Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studi"
E12-1064,steinberger-etal-2006-jrc,0,0.0403227,"Missing"
E12-1064,N01-1026,0,0.0604276,"guistics V projection Darf ich Sie etwas fragen? Step 1: German pronoun provides overt T/V label V Please permit me to ask you a question. Step 2: copy T/V class label to English sentence Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and"
E12-1064,C98-2188,0,\N,Missing
E14-1057,de-marneffe-etal-2006-generating,0,0.0133487,"Missing"
E14-1057,D10-1113,0,0.0382555,"control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose this corpus because (a)"
E14-1057,N12-1076,1,0.755835,"any contextspecific substitutes outside the common core. 5 Ranking Paraphrases While there are several studies on modelling lexical substitutes, almost all reported results use McCarthy and Navigli’s S EM E VAL 2007 dataset. We now compare the results of three recent computational models on C O I N C O (our work) and on the S EM E VAL 2007 dataset to highlight similarities and differences between the two datasets. Models. We consider the paraphrase ranking models of Erk and Padó (2008, E P 08), Thater et al. (2010, T FP 10) and Thater et al. (2011, T FP 11). These models have been analysed by Dinu et al. (2012) as instances of the same general framework and have been shown to deliver state-of-the-art performance on the S EM E VAL 2007 dataset, with best results for Thater et al. (2011). The three models share the idea to represent the meaning of a target word in a specific context by 545 corpus syntactically structured syntactically filtered bag of words T FP 11 T FP 10 E P 08 T FP 11/E P 08 T FP 10 T FP 11/E P 08 T FP 10 random COINCO context baseline 47.8 46.2 46.0 44.6 47.4 46.2 47.4 45.8 41.9 38.8 46.2 44.7 40.8 37.5 33.0 S EM E VAL 2007 context baseline 52.5 43.7 48.6 42.7 49.4 43.7 50.1 44.4 4"
E14-1057,1993.eamt-1.1,0,0.520679,"Missing"
E14-1057,D09-1046,1,0.592235,"r substitutes? (b) Do parasets resemble word senses? (c) How similar are the parasets that correspond to the same word sense of a target? These questions have not been addressed before, and we would argue that they could not be addressed before, because previous corpora were either too small or were sampled in a way that was not conducive to this analysis. We use WordNet (Fellbaum, 1998), release 3.1, as a source for both lexical relations and word senses. WordNet is the de facto standard in N LP and is used for both W SD and broader investigations of word meaning (Navigli and Ponzetto, 2012; Erk and McCarthy, 2009). Multi-word substitutes are excluded from all analyses.4 4.1 Relating Targets and Substitutes We first look at the most canonical lexical relations between a target and its substitutes. Table 2 lists the percentage of substitutes that are synonyms (syn), direct/transitive (direct-/trans-) hypernyms (hyper) 3 Please see McCarthy and Navigli (2009) for a possible explanation of the generally low I AA numbers in this field. 543 4 All automatic lexical substitution approaches, including Section 5, omit multi-word expressions. Also, they can be expected to have WordNet coverage and normalisation i"
E14-1057,D08-1094,1,0.881083,".org) that is freely available and has (partial) manual annotation. The main advantage of the all-words setting is that it provides a realistic frequency distribution of target words and their senses. We use this to empirically investigate (a) the nature of lexical substitution and (b) the nature of the corpus, seen through the lens of word meaning in context. 2 2.1 3 Lexical Substitution: Data Lexical Substitution: Models The LexSub task at S EM E VAL 2007 (McCarthy and Navigli, 2009) required systems to both determine substitution candidates and choose contextual substitutions in each case. Erk and Padó (2008) treated the gold substitution candidates as given and focused on the context-specific ranking of those candidates. In this form, the task has been addressed through three types of (mostly unsupervised) approaches. The first group computes a single type representation and modifies it according C O I N C O – The M ASC All-Words Lexical Substitution Corpus1 Compared to, e. g., W SD, there still is little goldannotated data for lexical substitution. With the exception of the dataset created by Biemann (2013), all existing lexical substitution datasets are fairly small, covering at most several th"
E14-1057,P10-2017,1,0.861873,"study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose this corpus because (a) our analyses can pro"
E14-1057,P13-2130,0,0.018836,"in two crucial respects: we annotate all instances of each target, and include all targets regardless of frequency or level of lexical ambiguity. We believe that our corpus is considerably more representative of running text. 541 1 Available as X ML-formatted corpus “Concepts in Context” (C O I N C O) from http://goo.gl/5C0jBH. Also scheduled for release as part of M ASC. 3.2 Crowdsourcing We used the Amazon Mechanical Turk (A MT) platform to obtain substitutes by crowdsourcing. Interannotator variability and quality issues due to nonexpert annotators are well-known difficulties (see, e. g., Fossati et al. (2013)). Our design choices were shaped by “best practices in A MT”, including Mason and Suri (2012) and Biemann (2013). Defining H ITs. An A MT task consists of Human Intelligence Tasks (H ITs), each of which is supposed to represent a minimal, self-contained task. In our case, potential H ITs were annotations of (all target words in) one sentence, or just one target word. The two main advantages of annotating a complete sentence at a time are (a) less overhead, because the sentence has only to be read once; (b) higher reliability, since all words within a sentence will be annotated by the same per"
E14-1057,ide-etal-2008-masc,0,0.033068,"Missing"
E14-1057,P10-2013,0,0.0463841,"d in each sentence. In W SD, “lexical sample” datasets contrast with “allwords” annotation, in which all content words in a text are annotated for sense (Palmer et al., 2001). 540 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540–549, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we present the first large “allwords” Lexical Substitution dataset for English. It provides substitutions for more than 30,000 words of running text from two domains of M ASC (Ide et al., 2008; Ide et al., 2010), a subset of the American National Corpus (http://www.anc.org) that is freely available and has (partial) manual annotation. The main advantage of the all-words setting is that it provides a realistic frequency distribution of target words and their senses. We use this to empirically investigate (a) the nature of lexical substitution and (b) the nature of the corpus, seen through the lens of word meaning in context. 2 2.1 3 Lexical Substitution: Data Lexical Substitution: Models The LexSub task at S EM E VAL 2007 (McCarthy and Navigli, 2009) required systems to both determine substitution can"
E14-1057,D11-1097,0,0.0405471,"Missing"
E14-1057,S01-1005,0,0.0600907,"(Mitchell and Lapata, 2010). There are, however, important shortcomings of the work in the Lexical Substitution paradigm. All existing datasets (McCarthy and Navigli, 2009; Sinha and Mihalcea, 2014; Biemann, 2013; McCarthy et al., 2013) are either comparatively small, are “lexical sample” datasets, or both. “Lexical sample” datasets consist of sample sentences for each target word drawn from large corpora, with just one target word substituted in each sentence. In W SD, “lexical sample” datasets contrast with “allwords” annotation, in which all content words in a text are annotated for sense (Palmer et al., 2001). 540 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540–549, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we present the first large “allwords” Lexical Substitution dataset for English. It provides substitutions for more than 30,000 words of running text from two domains of M ASC (Ide et al., 2008; Ide et al., 2010), a subset of the American National Corpus (http://www.anc.org) that is freely available and has (partial) manual annotation. The main advantage of the all-w"
E14-1057,N10-1013,0,0.0305819,"ask bootstrapping design to control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose t"
E14-1057,N13-1133,0,0.373781,"es. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several layers of linguistic phenomena”, created with the purpose of being “free of usage and redistribution restrictions”. We chose this corpus because (a) our analyses can profit from the preexisting annotations and (b) we can release our annotations as part of M ASC. Since we could not annotate the complete M ASC, we selected (complete) text docume"
E14-1057,P10-1097,1,0.962657,"to substitute all content words in presented sentences. Biemann (2013) first investigated the use of crowdsourcing, developing a three-task bootstrapping design to control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotat"
E14-1057,I11-1127,1,0.859251,"tent words in presented sentences. Biemann (2013) first investigated the use of crowdsourcing, developing a three-task bootstrapping design to control for noise. His study covers over 50,000 instances, but these correspond only to 397 targets, all of which are high-frequency nouns. Biemann clusters the resulting substitutes into word senses. McCarthy et al. (2013) applied lexical substitution in a cross-lingual setting, annotating 130 of the original McCarthy and Navigli targets with Spanish substitutions (i. e., translations). 2.2 to sentence context (Erk and Padó, 2008; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011). The second group of approaches clusters instance representations (Reisinger and Mooney, 2010; Dinu and Lapata, 2010; Erk and Padó, 2010; O’Séaghdha and Korhonen, 2011). The third option is to use a language model (Moon and Erk, 2013). Recently, supervised models have emerged (Biemann 2013; Szarvas et al., 2013a,b). 3.1 Source Corpus Choice For annotation, we chose a subset of the “Manually Annotated Sub-Corpus” M ASC (Ide et al., 2008; Ide et al., 2010) which is “equally distributed across 19 genres, with manually produced or validated annotations for several laye"
E14-1057,D11-1094,0,0.0496376,"Missing"
E14-1057,D13-1198,0,\N,Missing
E14-4044,D09-1030,0,0.0420852,"phenomenon of implicit (non-locally realized) semantic roles where annotators are presented with a target sentence in paragraph context, and have to decide for every role whether it is realized in the target sentence, elsewhere in the paragraph, or not at all. Our results shows that implicit roles can be annotated as well as locally realized roles in a crowdsourcing setup, again provided that good design choices are taken. Introduction 2 In the last years, crowdsourcing, e.g., using Amazon’s Mechanical Turk platform, has been used to collect data for a range of NLP tasks, e.g., MT evaluation (Callison-Burch, 2009), sentiment analysis (Mellebeek et al., 2010), and student answer rating (Heilman and Smith, 2010). Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic rol"
E14-4044,S10-1059,0,0.0427422,"important role in discourse comprehension and coherence (Burchardt et al., 2005) and have found increasing attention over the 226 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics last years. The development was kickstarted by the creation of a corpus of non-local frame-semantic roles for the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), which still serves as a de facto standard. A number of systems perform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 3.1 Experimental Setup Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We"
E14-4044,P13-2130,0,0.133912,"quires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame. Hong and Baker (2011) have recently addressed the first step, experimenting with various ways of presenting the task. Fossati et al. (2013) have considered both steps and operationalized them separately and jointly, finding the best results when a single annotation task is presented to turkers (due to the interdependence of the two steps) and when the semantic role description Implicit Semantic Roles Implicit or non-locally realized semantic roles occur when arguments of a predicate are understood although not expressed in its direct syntactic neighborhood. FrameNet (Fillmore et al., 2003) distinguishes between indefinite non-instantiations (INIs), which are interpreted generically; definite non-instantiations (DNIs), which can o"
E14-4044,J12-4003,0,0.123088,"Missing"
E14-4044,W10-0705,0,0.0151373,"h a target sentence in paragraph context, and have to decide for every role whether it is realized in the target sentence, elsewhere in the paragraph, or not at all. Our results shows that implicit roles can be annotated as well as locally realized roles in a crowdsourcing setup, again provided that good design choices are taken. Introduction 2 In the last years, crowdsourcing, e.g., using Amazon’s Mechanical Turk platform, has been used to collect data for a range of NLP tasks, e.g., MT evaluation (Callison-Burch, 2009), sentiment analysis (Mellebeek et al., 2010), and student answer rating (Heilman and Smith, 2010). Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame. Hong and Baker (2011) have recently addressed the first step,"
E14-4044,W11-0404,0,0.738459,"., 2010), and student answer rating (Heilman and Smith, 2010). Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame. Hong and Baker (2011) have recently addressed the first step, experimenting with various ways of presenting the task. Fossati et al. (2013) have considered both steps and operationalized them separately and jointly, finding the best results when a single annotation task is presented to turkers (due to the interdependence of the two steps) and when the semantic role description Implicit Semantic Roles Implicit or non-locally realized semantic roles occur when arguments of a predicate are understood although not expressed in its direct syntactic neighborhood. FrameNet (Fillmore et al., 2003) distinguishes between in"
E14-4044,S12-1048,0,0.025993,"mplexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We focus on verbs from the M OTION and P OSITION frames, which realize a common set of location roles (PLACE OF EVENT, SOURCE , GOAL , PATH). This makes the task more uniform and allows us to skip frame annotation. Information about spatial relations, provided by such verbs, can be useful for many NLP tasks which reason about spatial information, e.g. systems generating textual descriptions from visual data, robot navigation tasks, and geographical information systems or GIS (Kordjamshidi et al., 2012). 3.2 Corpus We chose the novel “Around the World in Eighty Days” by Jules Verne, annotating the ten most frequent predicates meeting the conditions described above for annotation (reach, arrive, descend, rush, follow, approach, send, cross, escape, pass). A post-hoc analysis later showed that each instance of these predicates has on average 0.67 implicit roles identifiable in previous context, which underlines the relevance of annotating such cases. Metaphorical uses were discarded before annotation, which left an average 38.4 instances for each predicate. 4 Annotation and Agreement We decide"
E14-4044,W13-0114,0,0.0573234,"d coherence (Burchardt et al., 2005) and have found increasing attention over the 226 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics last years. The development was kickstarted by the creation of a corpus of non-local frame-semantic roles for the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), which still serves as a de facto standard. A number of systems perform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 3.1 Experimental Setup Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We focus on verbs from the M OTION and P OSITION frame"
E14-4044,W10-0718,0,0.0243957,"d) semantic roles where annotators are presented with a target sentence in paragraph context, and have to decide for every role whether it is realized in the target sentence, elsewhere in the paragraph, or not at all. Our results shows that implicit roles can be annotated as well as locally realized roles in a crowdsourcing setup, again provided that good design choices are taken. Introduction 2 In the last years, crowdsourcing, e.g., using Amazon’s Mechanical Turk platform, has been used to collect data for a range of NLP tasks, e.g., MT evaluation (Callison-Burch, 2009), sentiment analysis (Mellebeek et al., 2010), and student answer rating (Heilman and Smith, 2010). Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame. Hong and"
E14-4044,S10-1008,0,0.112523,"his left foot before his right five hundred and seventy-six times, reached [G OAL the Reform Club]. Implicit roles play an important role in discourse comprehension and coherence (Burchardt et al., 2005) and have found increasing attention over the 226 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics last years. The development was kickstarted by the creation of a corpus of non-local frame-semantic roles for the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), which still serves as a de facto standard. A number of systems perform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 3.1 Experimental Setup Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexit"
E14-4044,S12-1001,0,0.0360645,"discourse comprehension and coherence (Burchardt et al., 2005) and have found increasing attention over the 226 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics last years. The development was kickstarted by the creation of a corpus of non-local frame-semantic roles for the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), which still serves as a de facto standard. A number of systems perform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 3.1 Experimental Setup Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We focus on verbs from the M"
E14-4044,J14-1002,0,\N,Missing
E17-2013,D15-1003,0,0.0190069,"in terms of evaluation metric, dataset, and space, we leave it to future work to examine the influence of these factors. 5 Related Work Recent work has started exploring the representation of instances in distributional space: Herbe82 Figure 1: Performance by frequency bin lot and Vecchi (2015) and Gupta et al. (2015) extract quantified and specific properties of instances (some cats are black, Germany has 80 million inhabitants), and Kruszewski et al. (2015) seek to derive a semantic space where dimensions are sets of entities. We instead analyze instance vectors. A similar angle is taken in Herbelot and Vecchi (2015), for “artificial” entity vectors, whereas we explore “real” instance vectors extracted with standard distributional methods. An early exploration of the properties of instances and concepts, limited to a few manually defined features, is Alfonseca and Manandhar (2002). Some previous work uses distributional representations of instances for NLP tasks: For instance, Lewis and Steedman (2013) use the distributional similarity of named entities to build a type system for a semantic parser, and several works in Knowledge Base completion use entity embeddings (see Wang et al. (2014) and references"
E17-2013,W11-2501,0,0.227477,"ances are semantically more coherent than concepts, at least in our space. We believe a crucial reason for this is that instances share the same specificity, referring to one entity, while concepts are of widely varying specificity and size (compare president of the United States with artifact). Further work is required to probe this hypothesis. It is well established in lexical semantics that cosine similarity does not distinguish between hypernymy and other lexical relations, and in fact hyponyms and hypernyms are usually less similar than co-hyponyms like cat–dog or antonyms like good–bad (Baroni and Lenci, 2011). This result extends to instantiation: The average similarity of each instance to its concept is 0.110 (standard deviation: 0.12), very low compared to the figures in Table 3. The nearest neighbors of instances show a wide range of relations similar to those of concepts, further enriched by the instanceconcept axis: Tyre – Syria (location), Thames river – estuary (“co-hyponym class”), Luciano Pavarotti – soprano (“contrastive class”), Joseph Goebels – bolshevik (“antonym class”), and occasionally true instantiation cases like Sidney Poitier – actor. 4 reused across partitions. We report F-sco"
E17-2013,E12-1004,0,0.026698,"nal properties. We also establish that instantiation detection (“Mozart – composer”) is generally easier than hypernymy detection (“chemist – scientist”), and that results on the influence of input representation do not transfer from hyponymy to instantiation. 1 Introduction 2 Distributional semantics (Turney and Pantel, 2010), and data-driven, continuous approaches to language in general including neural networks (Bengio et al., 2003), are a success story in both Computational Linguistics and Cognitive Science in terms of modeling conceptual knowledge, such as the fact that cats are animals (Baroni et al., 2012), similar to dogs (Landauer and Dumais, 1997), and shed fur (Erk et al., 2010). However, distributional representations are notoriously bad at handling discrete knowledge (Fodor and Lepore, 1999; Smolensky, 1990), such as information about specific instances. For example, Beltagy et al. (2016) had to revert from a distributional to a symbolic knowledge source in an entailment task because the distributional component licensed unwarranted inferences (white man does not entail black man, even though the phrases are distributionally very similar). This partially explains that instances have recei"
E17-2013,Q15-1027,0,0.0133921,"Roller et al. (2014) report 0.85 maximum accuracy on a task analogous to N OT H YP, compared to our 0.57 F-score. Since our results are not directly comparable in terms of evaluation metric, dataset, and space, we leave it to future work to examine the influence of these factors. 5 Related Work Recent work has started exploring the representation of instances in distributional space: Herbe82 Figure 1: Performance by frequency bin lot and Vecchi (2015) and Gupta et al. (2015) extract quantified and specific properties of instances (some cats are black, Germany has 80 million inhabitants), and Kruszewski et al. (2015) seek to derive a semantic space where dimensions are sets of entities. We instead analyze instance vectors. A similar angle is taken in Herbelot and Vecchi (2015), for “artificial” entity vectors, whereas we explore “real” instance vectors extracted with standard distributional methods. An early exploration of the properties of instances and concepts, limited to a few manually defined features, is Alfonseca and Manandhar (2002). Some previous work uses distributional representations of instances for NLP tasks: For instance, Lewis and Steedman (2013) use the distributional similarity of named"
E17-2013,J16-4007,0,0.0627937,"semantics (Turney and Pantel, 2010), and data-driven, continuous approaches to language in general including neural networks (Bengio et al., 2003), are a success story in both Computational Linguistics and Cognitive Science in terms of modeling conceptual knowledge, such as the fact that cats are animals (Baroni et al., 2012), similar to dogs (Landauer and Dumais, 1997), and shed fur (Erk et al., 2010). However, distributional representations are notoriously bad at handling discrete knowledge (Fodor and Lepore, 1999; Smolensky, 1990), such as information about specific instances. For example, Beltagy et al. (2016) had to revert from a distributional to a symbolic knowledge source in an entailment task because the distributional component licensed unwarranted inferences (white man does not entail black man, even though the phrases are distributionally very similar). This partially explains that instances have received much less attention than concepts in distributional semantics. This paper addresses this gap and shows that distributional models can reproduce the age-old Datasets We focus on “public” named entities such as Abraham Lincoln or Vancouver, as opposed to “private” named entities like my neig"
E17-2013,N16-1030,0,0.015071,"andard distributional methods. An early exploration of the properties of instances and concepts, limited to a few manually defined features, is Alfonseca and Manandhar (2002). Some previous work uses distributional representations of instances for NLP tasks: For instance, Lewis and Steedman (2013) use the distributional similarity of named entities to build a type system for a semantic parser, and several works in Knowledge Base completion use entity embeddings (see Wang et al. (2014) and references there). The focus on public, named instances is shared with Named Entity Recognition (NER; see Lample et al. (2016) and references therein); however, we focus on the instantiation relation rather than on recognition per se. Also, in terms of modeling, NER is typically framed as a sequence labeling task to identify entities in text, whereas we do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a compa"
E17-2013,J10-4007,1,0.831074,"Missing"
E17-2013,S12-1012,0,0.0250573,"ities in text, whereas we do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional r"
E17-2013,N15-1098,0,0.0576241,"used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is that instantiation is easier to spot than hypernymy, consisten"
E17-2013,P05-1014,0,0.161924,"ing, NER is typically framed as a sequence labeling task to identify entities in text, whereas we do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and expe"
E17-2013,D15-1002,1,0.904283,"Missing"
E17-2013,D14-1167,0,0.0347687,"aken in Herbelot and Vecchi (2015), for “artificial” entity vectors, whereas we explore “real” instance vectors extracted with standard distributional methods. An early exploration of the properties of instances and concepts, limited to a few manually defined features, is Alfonseca and Manandhar (2002). Some previous work uses distributional representations of instances for NLP tasks: For instance, Lewis and Steedman (2013) use the distributional similarity of named entities to build a type system for a semantic parser, and several works in Knowledge Base completion use entity embeddings (see Wang et al. (2014) and references there). The focus on public, named instances is shared with Named Entity Recognition (NER; see Lample et al. (2016) and references therein); however, we focus on the instantiation relation rather than on recognition per se. Also, in terms of modeling, NER is typically framed as a sequence labeling task to identify entities in text, whereas we do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instanti"
E17-2013,C14-1212,0,0.172613,"do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The"
E17-2013,E14-1054,0,0.0766868,"f previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is t"
E17-2013,D16-1234,0,0.0586403,"sed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is that instantiation is easier to spot than hypernymy, consistent with it lying along a greater ontological"
E17-2013,C14-1097,1,0.918385,"Missing"
E17-2013,E14-4008,0,0.0385562,"n fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is that instantiation is easier to spot than h"
E17-2013,L16-1723,0,0.0132118,"op of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is that instantiation is easier to spot than hypernymy, consistent with it lying along"
E17-2013,P16-1226,0,0.100763,"Missing"
E17-2013,Q13-1015,0,\N,Missing
E17-2013,J06-1001,0,\N,Missing
erk-pado-2004-powerful,P03-1068,1,\N,Missing
erk-pado-2006-shalmaneser,burchardt-etal-2006-salto,1,\N,Missing
erk-pado-2006-shalmaneser,C04-1100,0,\N,Missing
erk-pado-2006-shalmaneser,P97-1003,0,\N,Missing
erk-pado-2006-shalmaneser,W02-0811,0,\N,Missing
erk-pado-2006-shalmaneser,W04-3212,0,\N,Missing
erk-pado-2006-shalmaneser,A00-1031,0,\N,Missing
erk-pado-2006-shalmaneser,P05-1039,0,\N,Missing
erk-pado-2006-shalmaneser,J02-3001,0,\N,Missing
erk-pado-2006-shalmaneser,J05-1004,0,\N,Missing
erk-pado-2006-shalmaneser,P03-1068,1,\N,Missing
erk-pado-2006-shalmaneser,erk-pado-2004-powerful,1,\N,Missing
erk-pado-2006-shalmaneser,N06-1017,1,\N,Missing
erk-pado-2006-shalmaneser,W02-2018,0,\N,Missing
erk-pado-2006-shalmaneser,P93-1016,0,\N,Missing
frontini-etal-2014-polysemy,bel-etal-2000-simple,1,\N,Missing
frontini-etal-2014-polysemy,jezek-quochi-2010-capturing,1,\N,Missing
H05-1084,N03-2008,0,0.0295845,"Missing"
H05-1084,W04-0803,0,0.0724993,"onstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which has been investigated in the context of semantic role assignment. The current paper concentrates on feature engineering, since the feature set is a pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task. We approach feature engineering not by directly optimizing system performance."
H05-1084,W02-2018,0,0.118704,"Missing"
H05-1084,W04-0817,1,0.784531,"with system performance for models based solely on syntactic features. We conclude that syntactic features approximate a description of grammatical functions, but that semantic features model a different aspect of the world. Much of current research in semantic role assignment is centered on the refinement of syntactic features. Our study suggests that it may be worthwhile to explore the refinement of semantic features as well. The most obvious choice is to investigate features related to selectional preferences. Possible features include goodness of fit relative to pre-computed preferences (Baldewein et al., 2004), named entities (Pradhan et al., 2004), or broad ontological classes like “animate” or “artifact”. Following up on this idea, a natural continuation of the present study would be to create a meta-model that subsumes semantic features. Such a model could use optimal selectional restrictions as a predictor. The next step would then be to construct a combined meta-model that describes the behavior of systems with both syntactic and semantic features. Another interesting research direction that our study suggests is the combination of syntactic and semantic models in co-training. Co-training can"
H05-1084,boas-2002-bilingual,0,0.0605161,"his indicates that syntactic features approximate a description of grammatical functions, and that semantic features provide an independent second view on the data. 1 Introduction Semantic roles have become a focus of research in computational linguistics during the recent years. The driving force behind this interest is the prospect that semantic roles, as a shallow meaning representation, can improve many NLP applications, while still being amenable to automatic analysis. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a nu"
H05-1084,W04-2412,0,0.128319,"Missing"
H05-1084,W05-0620,0,0.244124,"umber of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which has been investigated in the context of semantic role assignment. The current paper concentrates on feature engineering, since the feature set is a pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task. We approach feature engineering not by directly optimizing system performance. Instead, we proceed by error"
H05-1084,P96-1025,0,0.0160785,"Missing"
H05-1084,C04-1100,0,0.0916002,"ons, and that semantic features provide an independent second view on the data. 1 Introduction Semantic roles have become a focus of research in computational linguistics during the recent years. The driving force behind this interest is the prospect that semantic roles, as a shallow meaning representation, can improve many NLP applications, while still being amenable to automatic analysis. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework,"
H05-1084,W04-3214,1,0.888328,"Missing"
H05-1084,N04-1030,0,0.323664,"s has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which has been investigated in the context of semantic role assignment. The current paper concentrates on feature engineering, since the feature set is a pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task. We approach feature engineering not by directly optimizing sy"
H05-1084,P03-1002,0,0.0726952,"approximate a description of grammatical functions, and that semantic features provide an independent second view on the data. 1 Introduction Semantic roles have become a focus of research in computational linguistics during the recent years. The driving force behind this interest is the prospect that semantic roles, as a shallow meaning representation, can improve many NLP applications, while still being amenable to automatic analysis. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend them"
H05-1084,W04-3212,0,0.216476,"omatic analysis. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Typically, role assignment has been modeled as a classification task, with models being estimated from large corpora (Gildea and Jurafsky, 2002; Moschitti, 2004; Xue and Palmer, 2004; Surdeanu et al., 2003; Pradhan et al., 2004; Litkowski, 2004; Carreras and Màrquez, 2005). Within this framework, there is a number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which has been investigated in the context of semantic role assignment. The current paper concentrates on feature engineering, since the feature set is a pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task. We approach feat"
H05-1084,P04-1043,0,\N,Missing
H05-1084,J02-3001,0,\N,Missing
H05-1108,boas-2002-bilingual,0,0.0592407,"he house. abandon.v, desert.v, depart.v, departure.n, emerge.v, emigrate.v, emigration.n, escape.v, escape.n, leave.v, quit.v, retreat.v, retreat.n, split.v, withdraw.v, withdrawal.n Table 1: Example of FrameNet frame Introduction Shallow semantic parsing, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention, partly because of its increasing importance for potential applications. For instance, information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. The FrameNet project (Fillmore et al., 2003) has played a central role in this endeavour by providing a large lexical resource based on semantic roles. In FrameNet, meaning is represented by frames, schematic representations of situations. Semantic roles are frame-specific, and are called frame elements. The database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or FEEs), lists the possible syntactic realisations of their semantic roles, and provides annotated examples"
H05-1108,P97-1003,0,0.0949043,"Missing"
H05-1108,W03-1005,0,0.0155244,"eflect genuine structural differences between translated sentences. Consider the following (simplified) example for the S TATEMENT frame (introduced by say) and its semantic role S TATEMENT (introduced by we): (10) We claim X and we say Y Wir behaupten X und — sagen Y The word alignment correctly aligns the German pronoun wir with the first English we and leaves the second occurrence unaligned. Since there is no corresponding German word for the second we, projection of the S PEAKER role fails. In future work, this problem could be handled with explicit identification of empty categories (see Dienes and Dubey, 2003). 6 Conclusions In this paper, we argue that parallel corpora show promise in relieving the lexical acquisition bottleneck for low density languages. We proposed semantic projection as a means of obtaining FrameNet annotations automatically without additional human effort. We examined semantic parallelism, a prerequisite for accurate projection, and showed that semantic roles can be successfully projected for predicate pairs with matching frame assignments. Similarly to previous work (Hwa et al., 2002), we find that some mileage can be gained by assuming direct correspondence between two langu"
H05-1108,P03-1068,1,0.411785,"d, which arguably cannot have a role set identical to finish. Relying solely on the English FrameNet database for sampling would yield many sentence pairs which are either inappropriate for the present study (because they do not evoke the same frames) or simply problematic for annotation since they are outside the 2 See html. http://www.keenage.com/zhiwang/e_zhiwang. present coverage of the database. For the above reasons, our sample selection procedure was informed by two existing resources, the English FrameNet and SALSA, a FrameNetcompatible database for German currently under development (Erk et al., 2003). We first used the publicly available GIZA++ (Och and Ney, 2003) software to induce English-German word alignments. Next, we gathered all German-English sentences in the corpus that had at least one pair of aligned words (we , wg ), which were listed in FrameNet and SALSA, respectively, and had at least one frame in common. These sentences exemplify 83 frame types, 696 lemma pairs, and 265 unique English and 178 unique German lemmas. Sentence pairs were grouped into three bands according to their frame frequency (High, Medium, Low). We randomly selected 380 pairs from each band. The total sam"
H05-1108,C04-1134,0,0.383294,"English and Chinese. Their results show that, although assuming direct correspondence is often too restrictive, syntactic projection yields good enough annotations to train a dependency parser. Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. Previous work has primarily focused on the projection of morphological and grammatico-syntactic information. Inducing semantic resources from low density languages still poses a significant challenge to data-driven methods. The challenge is recognised by Fung and Chen (2004) who construct a Chinese FrameNet by mapping English FrameNet entries to 860 concepts listed in HowNet2 , an on-line ontology for Chinese, however without exploiting parallel texts. The present work extends previous approaches on annotation projection by inducing FrameNet semantic roles from parallel corpora. Analogously to Hwa et al. (2002), we investigate whether there are indeed semantic correspondences between two languages, since there is little hope for projecting meaningful annotations in nonparallel semantic structures. Similarly to Fung and Chen (2004) we automatically induce semantic"
H05-1108,J02-3001,0,0.0124551,"ing is represented by frames, schematic representations of situations. Semantic roles are frame-specific, and are called frame elements. The database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or FEEs), lists the possible syntactic realisations of their semantic roles, and provides annotated examples from the British National Corpus (Burnard, 1995). The availability of rich annotations for the surface realisation of semantic roles has triggered interest in semantic parsing and enabled the development of data-driven models (e.g., Gildea and Jurafsky, 2002). Table 1 illustrates an example from the FrameNet database, the D EPARTING frame. It has two roles, a T HEME which is the moving object and a S OURCE expressing the initial position of the T HEME. The frame elements are realised by different syntactic expressions. For instance, the T HEME is typically an NP, whereas the S OURCE is often expressed by a prepositional phrase (see the expressions in boldface in Table 1). The D EPARTING frame can be evoked by abandon, desert, depart, and several other verbs as well as nouns (see the list of FEEs in Table 1). Although recent advances in semantic pa"
H05-1108,C04-1100,0,0.0318286,"w York. He retreated from his opponent. The woman left the house. abandon.v, desert.v, depart.v, departure.n, emerge.v, emigrate.v, emigration.n, escape.v, escape.n, leave.v, quit.v, retreat.v, retreat.n, split.v, withdraw.v, withdrawal.n Table 1: Example of FrameNet frame Introduction Shallow semantic parsing, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention, partly because of its increasing importance for potential applications. For instance, information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. The FrameNet project (Fillmore et al., 2003) has played a central role in this endeavour by providing a large lexical resource based on semantic roles. In FrameNet, meaning is represented by frames, schematic representations of situations. Semantic roles are frame-specific, and are called frame elements. The database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or FEEs), lists the possible syntactic realisations of their semantic r"
H05-1108,J03-1002,0,0.00595843,"lying solely on the English FrameNet database for sampling would yield many sentence pairs which are either inappropriate for the present study (because they do not evoke the same frames) or simply problematic for annotation since they are outside the 2 See html. http://www.keenage.com/zhiwang/e_zhiwang. present coverage of the database. For the above reasons, our sample selection procedure was informed by two existing resources, the English FrameNet and SALSA, a FrameNetcompatible database for German currently under development (Erk et al., 2003). We first used the publicly available GIZA++ (Och and Ney, 2003) software to induce English-German word alignments. Next, we gathered all German-English sentences in the corpus that had at least one pair of aligned words (we , wg ), which were listed in FrameNet and SALSA, respectively, and had at least one frame in common. These sentences exemplify 83 frame types, 696 lemma pairs, and 265 unique English and 178 unique German lemmas. Sentence pairs were grouped into three bands according to their frame frequency (High, Medium, Low). We randomly selected 380 pairs from each band. The total sample consisted of ,140 sentence pairs. This procedure produces a r"
H05-1108,W04-3207,0,0.0100441,"r resource-rich languages like English are projected onto another language through aligned parallel texts. Yarowsky et al. (2001) propose several projection algorithms for deriving monolingual tools (ranging from part-ofspeech taggers, to chunkers and morphological analysers) without additional annotation cost. Hwa et al. (2002) assess the degree of syntactic parallelism in dependency relations between English and Chinese. Their results show that, although assuming direct correspondence is often too restrictive, syntactic projection yields good enough annotations to train a dependency parser. Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. Previous work has primarily focused on the projection of morphological and grammatico-syntactic information. Inducing semantic resources from low density languages still poses a significant challenge to data-driven methods. The challenge is recognised by Fung and Chen (2004) who construct a Chinese FrameNet by mapping English FrameNet entries to 860 concepts listed in HowNet2 , an on-line ontology for Chinese, however without exploiting parallel texts. The present wor"
H05-1108,P03-1002,0,0.0210233,"ure was delayed. S OURCE We departed from New York. He retreated from his opponent. The woman left the house. abandon.v, desert.v, depart.v, departure.n, emerge.v, emigrate.v, emigration.n, escape.v, escape.n, leave.v, quit.v, retreat.v, retreat.n, split.v, withdraw.v, withdrawal.n Table 1: Example of FrameNet frame Introduction Shallow semantic parsing, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention, partly because of its increasing importance for potential applications. For instance, information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. The FrameNet project (Fillmore et al., 2003) has played a central role in this endeavour by providing a large lexical resource based on semantic roles. In FrameNet, meaning is represented by frames, schematic representations of situations. Semantic roles are frame-specific, and are called frame elements. The database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or FEEs), lists the"
H05-1108,H01-1035,0,0.747038,"parallelism across languages. Then we propose two broad classes of projection models that utilise lexical and syntactic information (Section 4), and show experimentally that roles can be projected from English onto German with high accuracy (Section 5). We conclude the paper by discussing the implications of our results and future work (Section 6). 2 Related work A number of recent studies exploit parallel corpora for cross-linguistic knowledge induction. In this paradigm, annotations for resource-rich languages like English are projected onto another language through aligned parallel texts. Yarowsky et al. (2001) propose several projection algorithms for deriving monolingual tools (ranging from part-ofspeech taggers, to chunkers and morphological analysers) without additional annotation cost. Hwa et al. (2002) assess the degree of syntactic parallelism in dependency relations between English and Chinese. Their results show that, although assuming direct correspondence is often too restrictive, syntactic projection yields good enough annotations to train a dependency parser. Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a wor"
H05-1108,P02-1050,0,\N,Missing
heid-etal-2004-querying,mengel-lezius-2000-xml,0,\N,Missing
heid-etal-2004-querying,bird-etal-2000-towards,0,\N,Missing
heid-etal-2004-querying,P03-1068,1,\N,Missing
heid-etal-2004-querying,milde-gut-2002-tasx,1,\N,Missing
I08-1051,burchardt-etal-2006-salsa,1,0.931521,"ularity offered by a given annotation layer may diverge considerably from the granularity that is needed for the integration of corpus-derived data in large symbolic processing architectures or general lexical resources. This problem is multiplied when more than one layer of annotation is considered, for example in the characterisation of interface phenomena. While it may be possible to obtain coarser-grained representations procedurally by collapsing categories, such procedures are not flexibly configurable. Figure 1 illustrates these difficulties with a sentence from the SALSA/TIGER corpus (Burchardt et al., 2006), a manually annotated German newspaper corpus which contains role-semantic analyses in the FrameNet paradigm (Fillmore et al., 2003) on top of syntactic structure (Brants et al., 2002).1 The se1 While FrameNet was originally developed for English, the majority of frames has been found to generalise well to other 389 which the official Croatia but in significant international-law difficulties bring would Figure 1: Multi-layer annotation of a German phrase with syntax and frame semantics (‘which would bring official Croatia into significant difficulties with international law’) mantic structure"
I08-1051,E03-1068,0,0.0163518,"namely 4 missing metaphor flags and 4 omitted underspecification links. On the semantic level, we extracted annotation instances (in context) for metaphorical vs. nonmetaphorical readings, or frames that are involved in underspecification in certain sentences, but not in others. While the result sets thus obtained still require manual inspection, they clearly illustrate how the detection of inconsistencies can be enhanced by a declarative formalisation of the annotation scheme. Another strategy could be to concentrate on frames or lemmas exhibiting proportionally high variation in annotation (Dickinson and Meurers, 2003). 6 Conclusion In this paper, we have constructed a Description Logics-based lexicon model directly from multi-layer linguistic corpus annotations. We have shown how such a model allows for explicit data modelling, and for flexible and fine-grained definition of various degrees of abstractions over corpus annotations. Furthermore, we have demonstrated that a powerful logical formalisation which integrates an underlying annotation scheme can be used to directly control consistency of the annotations using general KR techniques. It can also overcome limitations of current XML-based search tools"
I08-1051,J02-3001,0,0.00609142,"roblematic are intersecting hierarchies, i.e., tree-shaped analyses on multiple linguistic levels. Introduction Over the years, much effort has gone into the creation of large corpora with multiple layers of linguistic annotation, such as morphology, syntax, semantics, and discourse structure. Such corpora offer the possibility to empirically investigate the interactions between different levels of linguistic analysis. Currently, the most common use of such corpora is the acquisition of statistical models that make use of the “more shallow” levels to predict the “deeper” levels of annotation (Gildea and Jurafsky, 2002; Miltsakaki et al., 2005). While these models fill an important need for practical applications, they fall short of the general task of lexicon modelling, i.e., creating an abstracted and compact representation of the corpus information that lends itself to ’linguistically informed’ usages such as human interpretation or integration with other knowledge sources (e.g., deep grammar resources or ontologies). In practice, this task faces three major problems: ∗ At the time of writing, Sebastian Padó and Dennis Spohr were affiliated with Saarland University, and Anette Frank with DFKI Saarbrücken"
I08-1051,U04-1019,0,0.0261719,"heme. We present a general approach to formally modelling corpora with multi-layered annotation, thereby inducing a lexicon model in a typed logical representation language, OWL DL. This model can be interpreted as a graph structure that offers flexible querying functionality beyond current XML-based query languages and powerful methods for consistency control. We illustrate our approach by applying it to the syntactically and semantically annotated SALSA/TIGER corpus. 1 4 Dept. of Linguistics Stanford University Stanford, CA Querying multiple layers of linguistic annotation. A recent survey (Lai and Bird, 2004) found that currently available XML-based corpus query tools support queries operating on multiple linguistic levels only in very restricted ways. Particularly problematic are intersecting hierarchies, i.e., tree-shaped analyses on multiple linguistic levels. Introduction Over the years, much effort has gone into the creation of large corpora with multiple layers of linguistic annotation, such as morphology, syntax, semantics, and discourse structure. Such corpora offer the possibility to empirically investigate the interactions between different levels of linguistic analysis. Currently, the m"
I08-1051,W06-1007,0,0.0219647,"mantics of the model makes it possible to use general and efficient knowledge representation techniques for consistency control. Finally, we can extract specific subsets from a corpus by defining task-specific views on the graph. After a short discussion of related approaches in languages (Burchardt et al., 2006; Boas, 2005). Section 2, Section 3 provides details on our methodology. Sections 4 and 5 demonstrate the benefits of our strategy on a model of the SALSA/TIGER data. Section 6 concludes. 2 Related Work One recent approach to lexical resource modelling is the Lexical Systems framework (Polguère, 2006), which aims at providing a highly general representation for arbitrary kinds of lexica. While this is desirable from a representational point of view, the resulting models are arguably too generic to support strong consistency checks on the encoded data. A further proposal is the currently evolving Lexical Markup Framework (LMF; Francopoulo et al. (2006)), an ISO standard for lexical resource modelling, and an LMF version of FrameNet exists. However, we believe that our usage of a typed formalism takes advantage of a strong logical foundation and the notions of inheritance and entailment (cf."
I08-1051,W06-0609,0,\N,Missing
J07-2002,C96-1005,0,0.145569,"Missing"
J07-2002,briscoe-carroll-2002-robust,0,0.0259658,"pendencies like the ones in Figure 4 that will form the context over which the semantic space will be constructed. We base our discussion Figure 4 A dependency analysis of the sentence A lorry might carry sweet apples as parse tree (left) and set of head-relation-modifier triples (right). 167 Computational Linguistics Volume 33, Number 2 and experiments on the broad-coverage dependency parser MINIPAR, version 0.5 (Lin 1998a, 2001). However, there is nothing inherent in our formalization that restricts us to this particular parser. Any other parser with broadly similar dependency output (e.g., Briscoe and Carroll 2002) could serve our purposes. In the remainder of this section, we first give a non-technical description of our algorithm for the construction of semantic spaces. Then, we proceed to discuss each construction step (context selection, basis mapping, and quantification of co-occurrences) in more detail. Finally, we show how our framework subsumes existing models. Table 1 lists the notation we use in the rest of the article. 3.1 The Construction Algorithm Our algorithm for creating semantic space models is summarized in Figure 5. Central to the construction process is the notion of paths, namely se"
J07-2002,W01-0514,0,0.133553,"Missing"
J07-2002,P02-1030,0,0.248675,"research in lexical semantics hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995). It is therefore not surprising that there have been efforts to enrich vector-based models with morpho-syntactic information. Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a). In these semantic space models, contexts are defined over words bearing a syntactic relationship to the target words of interest. This makes semantic spaces more flexible; different types of contexts can be selected; words do not have to co-occur within a small, fixed word window; and word order or argument structure differences can be naturally mirrored in the semantic space. This article proposes a general framework for semantic space models which conceptualizes context in terms of syntactic relations. We introduce an algorithm for constructing semantic s"
J07-2002,J93-1003,0,0.077181,"we (2001) notes, raw counts are likely to give misleading results. This is due to the non-uniform distribution of words in corpora which will introduce a frequency bias so that words with similar frequency will be judged more similar than they actually are. It is therefore advisable to use a lexical association function A to factor out chance co-occurrences explicitly. Our definition allows an arbitrary choice of lexical association function (see Manning and Schütze [1999] for an overview). In our experiments, we follow Lowe and McDonald (2000) in using the well-known log-likelihood ratio G2 (Dunning 1993). We can visualize the computation using a two-by-two contingency table whose four cells correspond to four events (Kilgarriff 2001): b ¬b t k m ¬t l n 173 Computational Linguistics Volume 33, Number 2 The top left cell records the frequency k with which t and b co-occur (i.e., k corresponds to raw frequency counts). The top right cell l records how many times b is attested with any word other than t, the bottom left cell m represents the frequency of any word other than b with t, and the bottom right cell n records the frequency of pairs involving neither b nor t. The function G2 : R4 → R is"
J07-2002,C02-1091,0,0.0386089,"picture. On the one hand, Grefenstette compared the performance of the two classes of models on the task of automatic thesaurus extraction and found that a syntactically enhanced model gave significantly better results over a simple word co-occurrence model. A replication of Grefenstette’s study with a more sophisticated parser (Curran and Moens 2002) revealed that additional syntactic information yields further improvements. On the other hand, attempts to generate more meaningful indexing terms for information retrieval (IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson et al. 2002) have been largely unsuccessful. Experimental results show minimal differences in retrieval effectiveness at a substantially greater processing cost (see Voorhees [1999] for details). Impact on cognitive modeling. Despite their widespread use in NLP, syntax-based semantic spaces have attracted little attention in cognitive science and computational psycholinguistics. Wiemer-Hastings and Zipitria (2001) construct a semantic space similar to LSA, but enhanced with part-of-speech tags with the aim of modeling human raters in an intelligent tutoring context. Their results, however, show that the t"
J07-2002,O97-1002,0,0.157944,"Missing"
J07-2002,A97-1025,0,0.0323534,"e of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art. 1. Introduction Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998). The popularity of vector-based models in both fields lies in their ability to represen"
J07-2002,W03-0208,0,0.013113,"Missing"
J07-2002,H05-1053,0,0.0758814,"Missing"
J07-2002,P99-1004,0,0.895987,"ore, much research in lexical semantics hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995). It is therefore not surprising that there have been efforts to enrich vector-based models with morpho-syntactic information. Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a). In these semantic space models, contexts are defined over words bearing a syntactic relationship to the target words of interest. This makes semantic spaces more flexible; different types of contexts can be selected; words do not have to co-occur within a small, fixed word window; and word order or argument structure differences can be naturally mirrored in the semantic space. This article proposes a general framework for semantic space models which conceptualizes context in terms of syntactic relations. We introduce an algorithm for"
J07-2002,P98-2127,0,0.151471,"semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art. 1. Introduction Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998). The popularity of vector-based models in both fields lies in their ability to represent word meaning simply by using distributional statistics. The"
J07-2002,P99-1041,0,0.0443637,"elations. Even in cases where many relations are used (Lin 1998a; Lin and Pantel 2001), only direct relations are taken into account, ignoring potentially important co-occurrence patterns between, for instance, the subject and the object of a verb, or between a verb and its non-local argument (e.g., in control structures). Comparison between model classes. Syntax-based vector space models have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003). Comparisons between word-based and syntax-based models on the same task are rare, and the effect of syntactic knowledge has not been rigorously investigated or quantified. The few studies on this topic reveal an inconclusive picture. On the one hand, Grefenstette compared the performance of the two classes of models on the task of automatic thesaurus extraction and found that a syntactically enhanced model gave significantly better results over a simple word co-occurrence model. A replication of Grefenstette’s study"
J07-2002,H01-1046,0,0.0154629,"Missing"
J07-2002,W03-1810,0,0.0876768,"Missing"
J07-2002,P04-1036,0,0.156238,"f models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art. 1. Introduction Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz,"
J07-2002,P04-1003,0,0.105127,"uch as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998). The popularity of vector-based models in both fields lies in their ability to represent word meaning simply by using distributional statistics. The central assumption here is that the context surrounding a given word provides important information about its meaning (Harris 1968). The semantic properties of words are captured in a multi-dimensional space by vectors that are constructed from large bodies of text by observing the distributional patterns of co-occurrence with their neighboring words. Co-occurre"
J07-2002,J98-1004,0,0.830277,"malization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art. 1. Introduction Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension"
J07-2002,N03-1036,0,0.198133,"itive behavior in sentence priming tasks (Morris 1994). Furthermore, much research in lexical semantics hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995). It is therefore not surprising that there have been efforts to enrich vector-based models with morpho-syntactic information. Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a). In these semantic space models, contexts are defined over words bearing a syntactic relationship to the target words of interest. This makes semantic spaces more flexible; different types of contexts can be selected; words do not have to co-occur within a small, fixed word window; and word order or argument structure differences can be naturally mirrored in the semantic space. This article proposes a general framework for semantic space models which conceptualizes context in"
J07-2002,N04-3012,0,\N,Missing
J07-2002,W03-1809,0,\N,Missing
J07-2002,C98-2122,0,\N,Missing
J07-2002,J09-3004,0,\N,Missing
J10-4007,D08-1007,0,0.396106,"Missing"
J10-4007,E03-1034,0,0.27612,"lity ratings (Experiment 2). Finally, we investigate inverse selectional preferences—preferences of 6 Dagan, Lee, and Pereira (1999) could in principle do the same, but do not explore this option. 734 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences nouns for the predicates that they co-occur with—again using pseudo-disambiguation (Experiment 3). We compare the EPP model to models from the three model categories presented in Section 2: RESNIK as a hierarchical model; ROOTH ET AL . as a distributional model; and PADO ET AL . as a semantic role–based model. As both Brockmann and Lapata (2003) and Padó (2007) have argued, no WordNet-based model systematically outperforms the others, and the RESNIK model shows the most consistent behavior across different scenarios. Among the distributional models, we choose ROOTH ET AL . as a model that performs soft clustering and thus shows a marked difference to the EPP model. To our knowledge, this is the ﬁrst comparison of all three generalization paradigms: semantic hierarchy–based, distributional, and semantic role–based.7 As mentioned earlier, we employ two tasks to evaluate the four models: pseudodisambiguation and the prediction of human"
J10-4007,C00-1028,0,0.0265236,"a high plausibility for words like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, ﬁrst introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selection"
J10-4007,N01-1013,0,0.277821,"ds like the (previously seen) wordlist and surname as well as the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like cow and machine. The predominant approach to generalizing over headwords, ﬁrst introduced by Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characterization of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selectional preferences. Our mo"
J10-4007,P97-1003,0,0.0854983,"mary corpus with role-semantic annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck 2003). FrameNet is a semantic lexicon for English that groups words in semantic classes called frames and lists ﬁne-grained semantic argument roles for each frame. Ambiguity is expressed by membership of a word in multiple frames. Each frame is exempliﬁed with annotated example sentences extracted from the BNC. The FrameNet release 1.2 comprises 131,582 annotated sentences (roughly three million words). To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser. As generalization corpus, we use the Minipar-parsed BNC in both settings. The experimentation with two different primary corpora allows us to directly study the inﬂuence of the disambiguation of predicates and the semantic characterization of argument positions on the performance of selectional preference models. Note, however, that the comparison is complicated by differences between the two corpora: The primary corpus for the SYN PRIMARY setting is parsed automatically, which can introduce noise in the determination of predicates, grammatical functions, and headwords. The primary co"
J10-4007,P08-2008,0,0.0283777,"a pseudodisambiguation task for inverse selectional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Padó 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word sense analysis tasks. At the same time, inverse selectional preferences have been the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processi"
J10-4007,J93-1003,0,0.0402254,"ity measures from Table 2 are applicable to semantic spaces with arbitrary basis elements, with the exception of the Lin measure, whose deﬁnition applies only to dependency-based spaces. The reason is that it decomposes the basis elements into relation–word pairs (r, v). For semantic spaces with words as basis elements, the Lin measure can be adapted by omitting the random variable r (cf. Padó and Lapata 2007). Transformations DTrans and STrans. Next, we come to transformations on counts and vector spaces. Concerning the count transformations DTrans, all counts are log-likelihood transformed (Dunning 1993), a standard procedure for word-based semantic space models which alleviates the problematic effects of the Zipﬁan distribution of lexical items, as proposed by Lowe (2001). As for transformations on the complete space STrans, many studies do not perform dimensionality reduction at all. Others, like the LSA family of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500dimensional spaces created using Principal Component Analysis (PCA), a standard method for dimensionality reduction that"
J10-4007,P07-1028,1,0.930211,"cognitive evidence for the existence of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been investigated systematically in linguistics. However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al. 1999). We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse preferences with our selectional preference model as well. The model that we discuss in this article, EPP, was ﬁrst introduced in Erk (2007) (using a pseudo-disambiguation task for evaluation) and further studied by Padó, Padó, and Erk (2007) (evaluating against human plausibility judgments). In the current text, we perform a more extensive evaluation and analysis, including the new evaluation on inverse preferences, and we introduce a new similarity measure, nGCM, which achieves excellent performance in many settings. 2. Computational Models of Selectional Preferences In this section, we provide an overview of corpus-based models of selectional preferences. See Table 1 for a summary of the notation that we use. 2 As descriptions"
J10-4007,D08-1094,1,0.351849,"sk for inverse selectional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Padó 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word sense analysis tasks. At the same time, inverse selectional preferences have been the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processing is head-ﬁnal wor"
J10-4007,J02-3001,0,0.169582,"ics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ∗∗ E-mail: pado@cl.uni-heidelberg.de. † E-mail: ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sente"
J10-4007,C92-2099,0,0.138643,"ffer from Resnik’s model in the details of how the generalization in the WordNet hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a tree cut through the WordNet noun hierarchy that minimizes tree cut length while maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by ascending the WordNet noun hierarchy as long as the degree of selectional preference among siblings is not signiﬁcantly different. Ciaramita and Johnson (2000) encode WordNet in a Bayesian Network to take advantage of the Bayes nets’ ability to “explain away” ambiguity. Grishman and Sterling (1992) perform generalization on the basis of a manually constructed semantic hierarchy speciﬁcally developed on the same corpus. 2.3 Distributional Models Distributional models do not make use of any lexicon resource for the generalization step. Instead, they use word co-occurrence—typically obtained from the same corpus as the observed headwords—for generalization. This independence from manually constructed resources gives distributional models a good cost–beneﬁt ratio and makes them especially attractive for domain-speciﬁc applications. These models, like the 727 Computational Linguistics Volume"
J10-4007,P90-1034,0,0.202988,"es. We test our model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNetbased and other distributional models on both tasks. Finally, we investigate inverse preferences, that is, preferences that arguments have for their predicates. Although there is ample cognitive evidence for the existence of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been investigated systematically in linguistics. However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al. 1999). We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse preferences with our selectional preference model as well. The model that we discuss in this article, EPP, was ﬁrst introduced in Erk (2007) (using a pseudo-disambiguation task for evaluation) and further studied by Padó, Padó, and Erk (2007) (evaluating against human plausibility judgments). In the current text, we perform a more extensive evaluation and analysis, including the new evaluation on inverse prefere"
J10-4007,J93-1005,0,0.0401984,"ency remains a major inﬂuence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items. 1. Introduction Selectional preferences or selectional constraints describe knowledge about possible and plausible ﬁllers for a predicate’s argument positions. They model the fact that there is often a semantically coherent set of concepts that can ﬁll a given argument position. Selectional preferences can help for many text analysis tasks which involve comparing different attachment decisions. Examples include syntactic disambiguation (Hindle and Rooth 1993; Toutanova et al. 2005), word sense disambiguation (WSD, ∗ Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ∗∗ E-mail: pado@cl.uni-heidelberg.de. † E-mail: ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguis"
J10-4007,P99-1004,0,0.232072,"ntation of selectional preferences in word-dependency-relation spaces (Padó, Padó, and Erk 2007), we use a subject– object context speciﬁcation that only considers co-occurrences between verbs and their subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items as basis elements. Similarity measure sim. In principle, any similarity measure for vectors can be plugged into our model. Previous studies that compared similarity measures came to various conclusions about the usefulness of different measures. Cosine similarity is very popular in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefﬁcient in pseudo-disambiguation. In the synonymy prediction task of Curran (2004), Dice emerged in ﬁrst place. Padó and Lapata (2007) found good results with Lin’s measure for predominant word sense identiﬁcation. Because it is unclear whether the ﬁndings about best similarity measures generalize to new tasks, we will investigate a range of similarity measures shown in Table 2: Cosine, the Dice and Jaccard coefﬁcients, Hindle’s (1990) and Lin’s (1998) mutual information-based metrics, and an adaptation of Nosofsky’s (1986) Generalized Context Model (GC"
J10-4007,P93-1016,0,0.0302579,"(subj). In both settings, the two potential headwords (here called headword and confounder, to be explained in more detail in the next section) to be distinguished in the pseudo-disambiguation task are noun lemmas. The verb–dependency–headword tuples of the SYN PRIMARY setting yield much more coarse-grained and noisy characterizations of selectional preferences; however, they can be extracted from corpora with only syntactic annotation. We are therefore able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this setting by parsing it with the Minipar dependency parser (Lin 1993). Minipar could parse almost all of the corpus, resulting in 6,005,130 parsed sentences. For the SEM PRIMARY setting, we require a primary corpus with role-semantic annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck 2003). FrameNet is a semantic lexicon for English that groups words in semantic classes called frames and lists ﬁne-grained semantic argument roles for each frame. Ambiguity is expressed by membership of a word in multiple frames. Each frame is exempliﬁed with annotated example sentences extracted from the BNC. The FrameNet release 1.2 comprises 131"
J10-4007,P98-2127,0,0.162994,"Missing"
J10-4007,A00-2034,0,0.0392864,"2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sentence reading times (Padó, Crocker, and Keller 2009). All these applications rely on the availability of broad-coverage, reliable selectional preferences for predicates and their argument positions. Given the immense effort necessary for manual semantic lexicon building and its associated reliability problems (see, e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences acquire selectional preferences automat"
J10-4007,J03-4004,0,0.20841,"word sense disambiguation (WSD, ∗ Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712. E-mail: katrin.erk@mail.utexas.edu. ∗∗ E-mail: pado@cl.uni-heidelberg.de. † E-mail: ulrike.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argume"
J10-4007,P04-1036,0,0.0354815,"which goes back to Firth (1957) and Harris (1968), is that words with similar meanings occur in similar contexts and will be assigned similar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), reﬂects their semantic similarity. Vector space models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identiﬁcation (McCarthy et al. 2004). Lexical resources based on distributional similarity (e.g., Lin [1998]’s thesaurus) are used in a wide range of applications that proﬁt from knowledge about word similarity. In cognitive science, they have been used, for example, to account for the inﬂuence of context on human lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald 2000). An idealized example for a semantic space representation of selectional preferences is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the 730 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of S"
J10-4007,D07-1039,0,0.0435715,"Missing"
J10-4007,P04-1003,0,0.0416762,"models are simple to construct, and the semantic similarity they provide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identiﬁcation (McCarthy et al. 2004). Lexical resources based on distributional similarity (e.g., Lin [1998]’s thesaurus) are used in a wide range of applications that proﬁt from knowledge about word similarity. In cognitive science, they have been used, for example, to account for the inﬂuence of context on human lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald 2000). An idealized example for a semantic space representation of selectional preferences is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the 730 Erk, Padó, and Padó A Flexible, Corpus-Driven Model of Selectional Preferences Figure 1 An idealized vector space for the plausibilities of (shoot, agent, hunter) and (shoot, patient, hunter). ﬁllers of the agent and patient position of shoot, respectively. In order to judge whether a hunter is a plausible agent of shoot, the vector space representation of hunter is compared t"
J10-4007,I08-2105,0,0.0114188,"ctional preferences. 7.1 Related Work In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Padó 2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3–7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word sense analysis tasks. At the same time, inverse selectional preferences have been the object of fruitful research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processing is head-ﬁnal word order (as in"
J10-4007,J07-2002,1,0.965308,"rds spaces tend to group words by topics. They ignore the syntactic relation between context items and the target, which is a problem for selectional preference modeling. The top table in Figure 1(b) illustrates the problem: deer and hunter receive identical vectors, even though they show complementary plausibility ratings. The reason is that deer and hunter often co-occur in similar lexical bag-of-words contexts (namely, hunting-related activities). The bottom table in Figure 1(b) indicates a way out of this problem, namely the use of word-relation pairs as basis elements (Grefenstette 1994; Padó and Lapata 2007). This space splits the co-occurrences with context words such as shoot based on the grammatical relation between target and context word, and this split looks different for different words: whereas deer occurs exclusively as the object of shoot, hunter predominantly occurs as the subject. We ﬁnd the reverse pattern for escape. In consequence, 731 Computational Linguistics Volume 36, Number 4 Table 2 Similarity measures explored in this article. Notation: We assume Basis = {b1 , . . . , bn }. We write I for mutual information, and BE(a) for the set of basis elements that co-occur at least once"
J10-4007,D07-1042,1,0.769375,"Missing"
J10-4007,N07-1071,0,0.0329787,"ke.pado@vico-research.com. Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication: 29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P. a visiting scholar at Stanford University. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selectional preferences are also helpful for determining linguistic properties of predicates and predicate–argument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibility judgments for predicate–argument combinations (Resnik 1996) and effects in human sentence reading times (Padó, Crocker, and Keller 2009). All these applications rely on the availability of broad-coverage, reliable selectional preferences"
J10-4007,P93-1024,0,0.212817,"Missing"
J10-4007,P99-1014,0,0.0557299,"eference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information, either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). In this article, we present a lightweight model for the acquisition and representation of selectional preferences. Our model is fully distributional and does not require any knowledge sources beyond a large corpus where subjects and objects can be identiﬁed with reasonable accuracy. Its key point is to use vector space similarity (Lund and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen 1 Some approaches also ﬁx a role and headword list and generalize from seen predicates to other, similar predicates. 724 Erk, Padó, and Padó A Flexib"
J10-4007,schulte-im-walde-2010-comparing,0,0.0178483,"Missing"
J10-4007,P08-1057,0,0.0712579,"eighted by sim(w1 , w). The similarity sim(w1 , w) is computed on vector space representations. Recently, Bergsma, Lin, and Goebel (2008) have adopted a discriminative approach to the prediction of selectional preferences. The features they use are mainly cooccurrence statistics, enriched with morphological context features to alleviate sparse data problems for low-frequency argument heads. They train one SVM per verb– argument position pair, using unobserved verb–argument combinations as negative examples, which makes their approach independent of manually annotated training data. Schulte im Walde et al. (2008) present a model that combines features of the semantic hierarchy–based and the distributional approaches by integrating WordNet into an EM-based clustering model; Schulte im Walde (2010) shows that integrating noun–modiﬁer relations improves the prediction of human plausibility judgments. 2.4 Semantic Role–Based Models The third class of models takes advantage of semantic resources beyond simple semantic hierarchies, notably of corpora with semantic role annotation. Such corpora allow the prediction of selectional preferences for semantic roles rather than grammatical functions. From a lingui"
J10-4007,H93-1052,0,0.0511239,"model to primary corpora with rich information that are too small for efﬁcient generalization, such as domain-speciﬁc corpora or corpora with deeper linguistic analysis, as long as a larger, even if potentially noisier, generalization corpus is available. We empirically demonstrate the beneﬁt of this distinction. We use FrameNet (Fillmore, Johnson, and Petruck 2003) as primary corpus and the BNC as generalization corpus, modeling selectional preferences for semantic roles with nearperfect coverage and low error rate.2 We evaluate our model on two tasks. The ﬁrst task is pseudo-disambiguation (Yarowsky 1993), where the model decides which of two randomly chosen words is a better ﬁller for the given argument position. This task tests model properties that are needed for concrete semantic analysis tasks, most notably word sense disambiguation, but also for semantic role labeling. The second task is the prediction of human plausibility ratings, which is a standard task-independent benchmark for the quality of selectional preferences. We test our model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNetbased and other distributional"
J10-4007,P06-1107,0,0.0171864,"Missing"
J10-4007,P09-2019,0,0.0790797,"Missing"
J10-4007,J13-3006,0,\N,Missing
J10-4007,E09-1094,0,\N,Missing
J10-4007,E06-1044,1,\N,Missing
J10-4007,C98-2122,0,\N,Missing
N10-1135,D08-1007,0,0.0276483,"to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in other languages, where comparable resources are often much smaller or entirely absent. A promising alternative approach is to derive the generalizations from distributional information (Prescher et al., 2000; Padó et al., 2007; Bergsma et al., 2008). For example, the Padó et al. (2007) model computes vector space representations for all head words h and defines the plausibility of the triple (p, a, h) as a weighted mean of the vector space similarities between h and all h0 in Seena (p): P l(p, a, h) = X h0 ∈Seena (p) w(h0 )· sim(h, h0 ) P 0 h0 w(h ) (1) where w(h0 ) is a weight, typically frequency. In this model, the generalization is provided by distributional similarity, which can be computed from a large corpus, without the need for additional lexical resources. Padó et al. found it to outperform Resnik’s approach in an evaluation ag"
N10-1135,E03-1034,0,0.183025,"ng platform (Snow et al., 2008). We asked native speakers of Spanish to rate the plausibility of a simple sentence with the relevant verb-argument combination on a five-point Likert scale, obtaining between 12 and 17 judgments for each triple. For each datapoint, we removed the single lowest and highest judgments and computed the mean. We assessed the reliability of our data by replicating Brockmann’s experiment for German with our AMT setup. With a Spearman ρ of almost .90, our own judgments correlate very well with Brockmann’s original data. Monolingual Prior Work and Baselines. For German, Brockmann and Lapata (2003) evaluated ontology-based models trained on TiGer triples and the GermaNet ontology. The results in Table 2 show that while both models are able to predict the data significantly, neither of the models can predict all of the data. We attribute this to the small size of TiGer.2 To gauge the limits of monolingual knowledgelean approaches, we constructed two monolingual distributional models for German and Spanish according to the Padó et al. (2007) model (Eq. (1)). Recall that this model performs generalization in a syntax-based vector space model. We computed vector spaces from dependency-parse"
N10-1135,P09-1068,0,0.0685775,"Missing"
N10-1135,C02-2020,0,0.0230607,"rget languages. Their construction does not require resources such as parallel corpora or bilingual translation lexicons, which might not be available for resource-poor source languages. Where parallel corpora exist, they often cover specific domains (e.g., politics), while many bilingual lexicons are prone to ambiguity problems. The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using as little knowledge as possible. Most often, such pairs are extracted from small bilingual lexicons (Fung and McKeown, 1997; Rapp, 1999; Chiao and Zweigenbaum, 2002). As mentioned above, such resources might not be available. We thus follow an alternative approach by using frequent cognates, words that are shared between the two languages (Markó et al., 2005). Cognates can be extracted by simple string matching between the corpora, and mostly share their meaning (Koehn and Knight, 2002). However, they account for (at most) a small percentage of all interesting translation pairs. To extend the set of dimensions available for the 923 bilingual space, we use these cognates merely as a starting point for a bootstrapping process: We build a bilingual vector sp"
N10-1135,J02-2003,0,0.0249902,"29, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Selectional Preferences The first broad-coverage model of selectional preferences was developed by Resnik (1996). To estimate the plausibility of a triple (p, a, h), Resnik first extracted all head words seen with predicate p in position a, Seena (p), from a corpus. He then used the WordNet hierarchy to generalize over the head words and to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in other languages, where comparable resources are often much smaller or entirely absent. A promising alternative approach is to derive the generalizations from distributional information (Prescher et al., 2000; Padó et al., 2007; Bergsma et al., 2008). For example, the Padó et al. (2007) model computes vector space representations for all head words h and defines the plausibility of the triple (p, a, h) as a weighted"
N10-1135,P02-1033,0,0.157556,"Missing"
N10-1135,W97-0119,0,0.0132814,"elated corpora for the source and target languages. Their construction does not require resources such as parallel corpora or bilingual translation lexicons, which might not be available for resource-poor source languages. Where parallel corpora exist, they often cover specific domains (e.g., politics), while many bilingual lexicons are prone to ambiguity problems. The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using as little knowledge as possible. Most often, such pairs are extracted from small bilingual lexicons (Fung and McKeown, 1997; Rapp, 1999; Chiao and Zweigenbaum, 2002). As mentioned above, such resources might not be available. We thus follow an alternative approach by using frequent cognates, words that are shared between the two languages (Markó et al., 2005). Cognates can be extracted by simple string matching between the corpora, and mostly share their meaning (Koehn and Knight, 2002). However, they account for (at most) a small percentage of all interesting translation pairs. To extend the set of dimensions available for the 923 bilingual space, we use these cognates merely as a starting point for a bootstrappi"
N10-1135,J02-3001,0,0.0356455,"to a given verb in a particular argument position (Wilks, 1975; Resnik, 1996). For instance, the subjects of the English verb to shoot are generally people, while the direct objects can be people or animals. This is reflected in speakers’ intuitions. Table 1 shows that the combination the hunter shot the deer is judged more plausible than the deer shot the hunter. Selectional preferences do not only play an important role in human sentence processing (McRae et al., 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Computational models of selectional preferences predict such plausibilities for triples of a predicate p, an argument position a, and a head word h, such as (shoot,object,hunter). All recent models take a twostep approach: (1), they extract all triples (p, a, h) from a large corpus; (2), they apply some type of generalization to make predictions for unseen items. Clearly, the accuracy of these models relies crucially on the quality and coverage of the extracted triples, and thus on the syntactic analysis of the corpus. Unfortunately, corpora that are both large enough and have a very good sy"
N10-1135,W02-0902,0,0.0517095,"The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using as little knowledge as possible. Most often, such pairs are extracted from small bilingual lexicons (Fung and McKeown, 1997; Rapp, 1999; Chiao and Zweigenbaum, 2002). As mentioned above, such resources might not be available. We thus follow an alternative approach by using frequent cognates, words that are shared between the two languages (Markó et al., 2005). Cognates can be extracted by simple string matching between the corpora, and mostly share their meaning (Koehn and Knight, 2002). However, they account for (at most) a small percentage of all interesting translation pairs. To extend the set of dimensions available for the 923 bilingual space, we use these cognates merely as a starting point for a bootstrapping process: We build a bilingual vector space with the initial word pairs as dimensions, and identify nearest neighbors between the two languages in the space. These are added as dimensions of the bilingual space, and the process is repeated. Since the focus is on identifying reliable source-target word pairs rather than complete coverage as in Eq. (2), we adopt a s"
N10-1135,P99-1004,0,0.0929625,"xt word pairs (like secretly/heimlich and rifle/Gewehr for German–English) that are mutual translations. By treating such context word pairs as single dimensions, the vector space can represent target words from both languages, counting the target words’ co-occurrences with the context words from the respective language. In other words, a sourcetarget word pair (s, t) will be assigned similar vectors in the semantic space if the context words of s are translations of the context words of t. Cross-lingual semantic similarity between words can be measured using standard vector space similarity (Lee, 1999). Importantly, bilingual vector spaces can be built on the basis of co-occurrences drawn from two unrelated corpora for the source and target languages. Their construction does not require resources such as parallel corpora or bilingual translation lexicons, which might not be available for resource-poor source languages. Where parallel corpora exist, they often cover specific domains (e.g., politics), while many bilingual lexicons are prone to ambiguity problems. The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using"
N10-1135,P93-1016,0,0.0784211,"Missing"
N10-1135,J03-4004,0,0.0301164,"servation that not all words are equally good arguments to a given verb in a particular argument position (Wilks, 1975; Resnik, 1996). For instance, the subjects of the English verb to shoot are generally people, while the direct objects can be people or animals. This is reflected in speakers’ intuitions. Table 1 shows that the combination the hunter shot the deer is judged more plausible than the deer shot the hunter. Selectional preferences do not only play an important role in human sentence processing (McRae et al., 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Computational models of selectional preferences predict such plausibilities for triples of a predicate p, an argument position a, and a head word h, such as (shoot,object,hunter). All recent models take a twostep approach: (1), they extract all triples (p, a, h) from a large corpus; (2), they apply some type of generalization to make predictions for unseen items. Clearly, the accuracy of these models relies crucially on the quality and coverage of the extracted triples, and thus on the syntactic analysis of the corpus. Unfortunately, cor"
N10-1135,D07-1042,1,0.894858,"the head words and to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in other languages, where comparable resources are often much smaller or entirely absent. A promising alternative approach is to derive the generalizations from distributional information (Prescher et al., 2000; Padó et al., 2007; Bergsma et al., 2008). For example, the Padó et al. (2007) model computes vector space representations for all head words h and defines the plausibility of the triple (p, a, h) as a weighted mean of the vector space similarities between h and all h0 in Seena (p): P l(p, a, h) = X h0 ∈Seena (p) w(h0 )· sim(h, h0 ) P 0 h0 w(h ) (1) where w(h0 ) is a weight, typically frequency. In this model, the generalization is provided by distributional similarity, which can be computed from a large corpus, without the need for additional lexical resources. Padó et al. found it to outperform Resnik’s appro"
N10-1135,C00-2094,0,0.042813,"chy to generalize over the head words and to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in other languages, where comparable resources are often much smaller or entirely absent. A promising alternative approach is to derive the generalizations from distributional information (Prescher et al., 2000; Padó et al., 2007; Bergsma et al., 2008). For example, the Padó et al. (2007) model computes vector space representations for all head words h and defines the plausibility of the triple (p, a, h) as a weighted mean of the vector space similarities between h and all h0 in Seena (p): P l(p, a, h) = X h0 ∈Seena (p) w(h0 )· sim(h, h0 ) P 0 h0 w(h ) (1) where w(h0 ) is a weight, typically frequency. In this model, the generalization is provided by distributional similarity, which can be computed from a large corpus, without the need for additional lexical resources. Padó et al. found it to outper"
N10-1135,P99-1067,0,0.0679897,"ource and target languages. Their construction does not require resources such as parallel corpora or bilingual translation lexicons, which might not be available for resource-poor source languages. Where parallel corpora exist, they often cover specific domains (e.g., politics), while many bilingual lexicons are prone to ambiguity problems. The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using as little knowledge as possible. Most often, such pairs are extracted from small bilingual lexicons (Fung and McKeown, 1997; Rapp, 1999; Chiao and Zweigenbaum, 2002). As mentioned above, such resources might not be available. We thus follow an alternative approach by using frequent cognates, words that are shared between the two languages (Markó et al., 2005). Cognates can be extracted by simple string matching between the corpora, and mostly share their meaning (Koehn and Knight, 2002). However, they account for (at most) a small percentage of all interesting translation pairs. To extend the set of dimensions available for the 923 bilingual space, we use these cognates merely as a starting point for a bootstrapping process:"
N10-1135,D08-1027,0,0.0387861,"Missing"
N10-1135,taule-etal-2008-ancora,0,0.0265705,"ingual baselines 2. Spearman correlation and coverage for distributional models. † : p &lt; .1; *: p &lt; .05; **: p &lt; .01. two languages, using the 2,000 most frequent lemmadependency relation pairs as dimensions and adopting the popular pointwise mutual information metric as co-occurrence statistic. For German, we used Schulte im Walde’s verb frame resource (Schulte im Walde et al., 2001), which contains the frequency of triples calculated from probabilistic parses of 30M words from the Huge German Corpus (HGC) of newswire. For Spanish, we consulted two syntactically analyzed corpora: the AnCora (Taulé et al., 2008) and the Encarta corpus (Calvo et al., 2005). At 0.5M words, the AnCora corpus is small, but manually annotated, whereas the larger, automatically parsed Encarta corpus amounts to over 18M tokens. Table 3 shows the results for the distributional monolingual models. For German, we get significant correlations for DOBJ and POBJ, an almost significant correlation for SUBJs, and high significance for the complete dataset (p &lt; 0.01). These figures rival the performance of the ontological models (cf. Table 2), without using ontological information. For Spanish, the only significant correlation with"
N18-2033,W10-2805,0,0.0308562,"the phrase constituents (Baroni et al., 2014). The most basic CDSMs represent words as vectors and compose phrase vectors by component-wise operations of the constituent vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the general limitations of rating scales, such as inconsistencies in annotations, scale region bias, and fixed granularity (Schuman and Presser, 1996). Phrase similarity datasets used for CDSM evaluation demonstrate slight to fair inter-annotator agreement, as well as overlap"
N18-2033,S15-1017,1,0.890939,"Missing"
N18-2033,W13-3513,0,0.0213643,"(Mitchell and Lapata, 2008). Secondly, phrase similarity is a task that is rather difficult to put down precisely, especially for long phrases. Generally, phrases can be (dis)similar in any number of ways. Annotators commonly agree that some sentence pairs are semantically highly similar (private company files annual account and private company registers annual account, Pickering and Frisson 2001), and others are semantically unrelated (man waves hand vs. employee leaves company). In contrast, their assessments become less confident for cases like delegate buys land and agent sells property (Kartsaklis et al., 2013), where there is a semantic relation other than synonymy. Similarity is also arguably not a useful measure when sentences are semantically deviant, as it is often the case in the datasets: how similar are private company files annual account and private company smooths annual account? The third problem is that the most widely used phrase similarity datasets are constructed in a balanced fashion along psycholinguistic principles. For instance, the adjective-noun-verb-adjectivenoun (“ANVAN”) dataset (Pickering and Frisson, 2001; Kartsaklis et al., 2013), from which the examples above are drawn,"
N18-2033,E14-1057,1,0.805026,"Missing"
N18-2033,S14-2001,0,0.0254595,"vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the general limitations of rating scales, such as inconsistencies in annotations, scale region bias, and fixed granularity (Schuman and Presser, 1996). Phrase similarity datasets used for CDSM evaluation demonstrate slight to fair inter-annotator agreement, as well as overlap between groups of items rated as low and high in similarity (Mitchell and Lapata, 2008). Secondly, phrase similarity is a task that is rather difficult to put down precisely, es"
N18-2033,marelli-etal-2014-sick,0,0.0236963,"vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the general limitations of rating scales, such as inconsistencies in annotations, scale region bias, and fixed granularity (Schuman and Presser, 1996). Phrase similarity datasets used for CDSM evaluation demonstrate slight to fair inter-annotator agreement, as well as overlap between groups of items rated as low and high in similarity (Mitchell and Lapata, 2008). Secondly, phrase similarity is a task that is rather difficult to put down precisely, es"
N18-2033,S17-1014,1,0.777868,"Missing"
N18-2033,K16-1006,0,0.0302634,"odel, PLFGupta . All CDSMs rank the four candidates for each target (cf. Table 1) by comparing the vector for the original sentence against four sentences in which the target is replaced by the two correct and two incorrect substitutes. We use the raw dot product as similarity measure, following Roller and Erk (2016), to boost frequent candidates. CDSMs. We consider two component-wise CDSMs: the simple additive and multiplicative models (Mitchell and Lapata, 2008), defined as 208 Lexical Substitution Model. As competitor, we consider a dedicated lexical substitution model, namely context2vec (Melamud et al., 2016). Since it has demonstrated state-of-the-art performance on lexical substitution and word sense disambiguation tasks, it is a suitable competitor model for CDSMs on a similar problem. Context2vec uses word embeddings to compute a set of viable substitutes given a context, using a bidirectional LSTM recurrent neural network to build a sentential context representation. We work with two instantiations: first, using only ANVAN as context (C2VANVAN ), and second, using the full CoInCo sentence from which the ANVAN was extracted (C2VSent ). The first model is directly comparable to the CDSMs in tha"
N18-2033,P08-1028,0,0.23862,"Englishlanguage corpus with manual “all-words” lexical substitution annotation. Our experiments indicate that the Practical Lexical Function CDSM outperforms simple component-wise CDSMs and performs on par with the context2vec lexical substitution model using the same context. 1 Introduction Compositional Distributional Semantics Models (CDSMs) compute phrase meaning in semantic space as a function of the meanings of the phrase constituents (Baroni et al., 2014). The most basic CDSMs represent words as vectors and compose phrase vectors by component-wise operations of the constituent vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentime"
N18-2033,P14-1009,0,0.0732062,"outperforms simple component-wise CDSMs and performs on par with the context2vec lexical substitution model using the same context. 1 Introduction Compositional Distributional Semantics Models (CDSMs) compute phrase meaning in semantic space as a function of the meanings of the phrase constituents (Baroni et al., 2014). The most basic CDSMs represent words as vectors and compose phrase vectors by component-wise operations of the constituent vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the gener"
N18-2033,N16-1131,0,0.0236259,"matrices are learned with ridge regression from unigram and bigram vectors. Gupta et al. (2015) pointed out that the standard PLF “overcounts” the predicate by adding it explicitly, and proposed a rectified variant which simply → − leaves out the function word vector V . We also experiment with this model, PLFGupta . All CDSMs rank the four candidates for each target (cf. Table 1) by comparing the vector for the original sentence against four sentences in which the target is replaced by the two correct and two incorrect substitutes. We use the raw dot product as similarity measure, following Roller and Erk (2016), to boost frequent candidates. CDSMs. We consider two component-wise CDSMs: the simple additive and multiplicative models (Mitchell and Lapata, 2008), defined as 208 Lexical Substitution Model. As competitor, we consider a dedicated lexical substitution model, namely context2vec (Melamud et al., 2016). Since it has demonstrated state-of-the-art performance on lexical substitution and word sense disambiguation tasks, it is a suitable competitor model for CDSMs on a similar problem. Context2vec uses word embeddings to compute a set of viable substitutes given a context, using a bidirectional LS"
N18-2033,D13-1170,0,0.00937345,"x models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the general limitations of rating scales, such as inconsistencies in annotations, scale region bias, and fixed granularity (Schuman and Presser, 1996). Phrase similarity datasets used for CDSM evaluation demonstrate slight to fair inter-annotator agreement, as well as overlap between groups of items rated as low and high in similarity (Mitchell and Lapata, 2008). Secondly, phrase similarity is a task that is rather difficult to put down precisely, especially for long phrases. Generally, phrases ca"
P03-1017,W01-0514,0,0.0275868,"ord co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the c"
P03-1017,J93-1003,0,0.013767,"ts associated with the nodes and edges. Cosine distance ∑i x√ i yi ∑i x2i ∑i y2i xi = ∑i xi log αxi +(1−α)y i cos(~x,~y) = √ Skew divergence sα (~x,~y) Figure 2: Distance measures The dependency-based semantic space was constructed with the word-based path equivalence function from Section 2.3. As basis elements for our semantic space the 1000 most frequent words in the BNC were used. Each element of the resulting vector was replaced with its log-likelihood value (see Definition 10 in Section 2.3) which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is (Dunning, 1993). We experimented with a variety of distance measures such as cosine, Euclidean distance, L 1 norm, Jaccard’s coefficient, Kullback-Leibler divergence and the Skew divergence (see Lee 1999 for an overview). We obtained the best results for cosine (Experiment 1) and Skew divergence (Experiment 2). The two measures are shown in Figure 2. The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed by Lee (1999) as a linguistically motivated distance measure. We use a value of α = .99. We explored in detail the influence of different types and sizes of conte"
P03-1017,A97-1025,0,0.0189986,"sent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important inform"
P03-1017,P99-1004,0,0.710852,"ment of Computer Science University of Sheffield Regent Court, 211 Portobello Street Sheffield S1 4DP, UK mlap@dcs.shef.ac.uk a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context. Contexts are defined as a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000) or as entire paragraphs, even documents (Landauer and Dumais, 1997). Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account (Lin, 1998; Grefenstette, 1994; Lee, 1999). A word can be thus viewed as a point in an n-dimensional semantic space. The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance. In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink ) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface forms (Lowe and McDonald, 2000; McD"
P03-1017,P98-2127,0,0.910708,"lected in Mirella Lapata Department of Computer Science University of Sheffield Regent Court, 211 Portobello Street Sheffield S1 4DP, UK mlap@dcs.shef.ac.uk a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context. Contexts are defined as a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000) or as entire paragraphs, even documents (Landauer and Dumais, 1997). Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account (Lin, 1998; Grefenstette, 1994; Lee, 1999). A word can be thus viewed as a point in an n-dimensional semantic space. The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance. In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink ) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface for"
P03-1017,H01-1046,0,0.0283752,"Missing"
P03-1017,J98-1004,0,0.0858584,"aditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies"
P03-1017,C98-2122,0,\N,Missing
P03-1017,kilgarriff-yallop-2000-whats,0,\N,Missing
P03-1068,P98-1013,0,0.0248009,"e most serious bottlenecks for language technology. To train tools for the acquisition of semantic information for such lexica, large, extensively annotated resources are necessary. In this paper, we present current work of the SALSA (SAarbr¨ucken Lexical Semantics Annotation and analysis) project, whose aim is to provide such a resource and to investigate efficient methods for its utilisation. In the current project phase, the focus of our research and the backbone of the annotation are semantic role relations. More specifically, our role annotation is based on the Berkeley FrameNet project (Baker et al., 1998; Johnson et al., 2002). In addition, we selectively annotate word senses and anaphoric links. The TIGER corpus (Brants et al., 2002), a 1.5M word German newspaper corpus, serves as sound syntactic basis. Besides the sparse data problem, the most serious problem for corpus-based lexical semantics is the lack of specificity of the data: Word meaning is notoriously ambiguous, vague, and subject to contextual variance. The problem has been recognised and discussed in connection with the SENSEVAL task (Kilgarriff and Rosenzweig, 2000). Annotation of frame semantic roles compounds the problem as it"
P03-1068,H94-1020,0,0.0480689,"quisition of word-semantic information, e.g. the construction of domainindependent lexica. The backbone of the annotation are semantic roles in the frame semantics paradigm. We report experiences and evaluate the annotated data from the first project stage. On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation. 1 Introduction Corpus-based methods for syntactic learning and processing are well-established in computational linguistics. There are comprehensive and carefully worked-out corpus resources available for a number of languages, e.g. the Penn Treebank (Marcus et al., 1994) for English or the NEGRA corpus (Skut et al., 1998) for German. In semantics, the situation is different: Semantic corpus annotation is only in its initial stages, and currently only a few, mostly small, corpora are available. Semantic annotation has predominantly concentrated on word senses, e.g. in the SENSEVAL initiative (Kilgarriff, 2001), a notable exception being the Prague Treebank (Hajiˇcov´a, 1998) . As a consequence, most recent work in corpus-based semantics has taken an unsupervised approach, relying on statistical methods to extract semantic regularities from raw corpora, often u"
P03-1068,C98-1013,0,\N,Missing
P06-1146,P05-1066,0,0.0485505,"Missing"
P06-1146,P97-1003,0,0.0317846,"Missing"
P06-1146,P05-1039,0,0.0382399,"Missing"
P06-1146,C04-1134,0,0.0531419,"filters and graph-based algorithms eliminate alignment noise to a large extent. Analysis of the models’ output revealed that the remaining errors are mostly due to incorrect parses (none of the parsers employed in this work were trained on the Europarl corpus) but also to modelling deficiencies. Recall from Section 3 that our global models cannot currently capture one-to-zero correspondences, i.e., deletions and insertions. 7 Related work Previous work has primarily focused on the projection of grammatical (Yarowsky and Ngai, 2001) and syntactic information (Hwa et al., 2002). An exception is Fung and Chen (2004), who also attempt to induce FrameNet-style annotations in Chinese. Their method maps English FrameNet entries to concepts listed in HowNet7 , an on-line ontology for Chinese, without using parallel texts. The present work extends our earlier projection framework (Padó and Lapata, 2005) by proposing global methods for automatic constituent alignment. Although our models are evaluated on the semantic role projection task, we believe they also show promise in the context of statistical machine translation. Especially for systems that use syntactic information to enhance translation quality. For"
P06-1146,J02-3001,0,0.0126883,"d demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic analysis. Examples include question answering, information extraction, and notably machine translation. Our experiments concentrated primarily on the first projection step, i.e., establishing the right level of linguistic analysis for effecting projection. We showed that projection schemes based on constituent alignments significantly outperform schemes that rely exclusively on word alignments. A local optimisation strategy was used to find constituent alignments, while relying on a simple filtering technique to hand"
P06-1146,P03-1011,0,0.0453339,"Missing"
P06-1146,W04-3228,0,0.0394542,"Missing"
P06-1146,H05-1107,0,0.0974989,"its of projection; these are typically words but can also be chunks or syntactic constituents; (b) inducing alignments between the projection units and projecting annotations along these alignments; (c) reducing the amount of noise in the projected annotations, often due to errors and omissions in the word alignment. The degree to which analyses are parallel across languages is crucial for the success of projection approaches. A number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic analysis. Examples"
P06-1146,P02-1050,0,0.0615109,"syntactic constituents; (b) inducing alignments between the projection units and projecting annotations along these alignments; (c) reducing the amount of noise in the projected annotations, often due to errors and omissions in the word alignment. The degree to which analyses are parallel across languages is crucial for the success of projection approaches. A number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic analysis. Examples include question answering, information extraction, and notably machin"
P06-1146,N03-1017,0,0.00195893,"t and its predicate. This definition covers long-distance dependencies such as control constructions for verbs, or support constructions for nouns and adjectives, and can be extended slightly to accommodate coordination. This argument-based filter reduces target trees to a set of likely arguments. In the example in Figure 3, all tree nodes are removed except Kim and pünktlich zu kommen. entire English-German Europarl bitext as training data (20M words). We used the GIZA++ default settings to induce alignments for both directions (source-target, target-source). Following common practise in MT (Koehn et al., 2003), we considered only their intersection (bidirectional alignments are known to exhibit high precision). We also produced manual word alignments for all sentences in our corpus, using the GIZA++ alignments as a starting point and following the Blinker annotation guidelines (Melamed, 1998). Method and parameter choice The constituent alignment models we present are unsupervised in that they do not require labelled data for inferring correct alignments. Nevertheless, our models have three parameters: (a) the similarity measure for identifying semantically equivalent constituents; (b) the filterin"
P06-1146,2005.mtsummit-papers.11,0,0.109912,"Missing"
P06-1146,J03-1002,0,0.010076,"Missing"
P06-1146,H05-1108,1,0.88534,"alignments between the projection units and projecting annotations along these alignments; (c) reducing the amount of noise in the projected annotations, often due to errors and omissions in the word alignment. The degree to which analyses are parallel across languages is crucial for the success of projection approaches. A number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic analysis. Examples include question answering, information extraction, and notably machine translation. Our experiments concentrate"
P06-1146,C04-1073,0,0.00824849,"Missing"
P06-1146,W04-3212,0,0.0110476,"tives, adverbs, verbs, or nouns, from the source and target sen1164 S VP S VP Kim versprach, pünktlich zu kommen. Figure 3: Filtering of unlikely arguments (predicate in boldface, potential arguments in boxes). tences (Padó and Lapata, 2005). We also use a novel filter which removes all words which remain unaligned in the automatic word alignment. Nonterminal nodes whose terminals are removed by these filters, are also pruned. Argument filtering Previous work in shallow semantic parsing has demonstrated that not all nodes in a tree are equally probable as semantic roles for a given predicate (Xue and Palmer, 2004). In fact, assuming a perfect parse, there is a “set of likely arguments”, to which almost all semantic roles roles should be assigned to. This set of likely arguments consists of all constituents which are a child of some ancestor of the predicate, provided that (a) they do not dominate the predicate themselves and (b) there is no sentence boundary between a constituent and its predicate. This definition covers long-distance dependencies such as control constructions for verbs, or support constructions for nouns and adjectives, and can be extended slightly to accommodate coordination. This ar"
P06-1146,H01-1035,0,0.235924,"s: (a) determining the units of projection; these are typically words but can also be chunks or syntactic constituents; (b) inducing alignments between the projection units and projecting annotations along these alignments; (c) reducing the amount of noise in the projected annotations, often due to errors and omissions in the word alignment. The degree to which analyses are parallel across languages is crucial for the success of projection approaches. A number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic"
P09-1034,P06-2003,0,0.0627553,"Missing"
P09-1034,W05-0909,0,0.0605981,"e system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references a"
P09-1034,2005.mtsummit-papers.11,0,0.00383163,"Missing"
P09-1034,C04-1072,0,0.0380796,"y 5–10 points.1 4.3 Baseline Metrics We consider four baselines. They are small regression models as described in Section 2 over component scores of four widely used MT metrics. To alleviate possible nonlinearity, we add all features in linear and log space. Each baselines carries the name of the underlying metric plus the suffix -R.2 B LEU R includes the following 18 sentence-level scores: BLEU-n and n-gram precision scores (1 ≤ n ≤ 4); BLEU brevity penalty (BP); BLEU score divided by BP. To counteract BLEU’s brittleness at the sentence level, we also smooth BLEU-n and n-gram precision as in Lin and Och (2004). N IST R consists of 16 features. NIST-n scores (1 ≤ n ≤ 10) and information-weighted n-gram precision scores (1 ≤ n ≤ 4); NIST brevity penalty (BP); and NIST score divided by BP. 1 Due to space constraints, we only show results for “tieaware” predictions. See Pad´o et al. (2009) for a discussion. 2 The regression models can simulate the behaviour of each component by setting the weights appropriately, but are strictly more powerful. A possible danger is that the parameters overfit on the training set. We therefore verified that the three non-trivial “baseline” regression models indeed confer"
P09-1034,W05-0904,0,0.324729,"translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses. We operationalize meaning equivalence by bidirectional textual entailment (RTE, D"
P09-1034,E06-1032,0,0.104031,"tnesses are terming it terrorism. Introduction Constant evaluation is vital to the progress of machine translation (MT). Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations. The resulting scores are cheap and objective. However, studies such as Callison-Burch et al. (2006) have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be “gamed” by permuting word order; (3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The conte"
P09-1034,C08-1066,1,0.800593,"ile the original RTE task is asymmetric, MT evaluation needs to determine meaning equivalence, which is a symmetric relation. We do this by checking for entailment in both directions (see Figure 1). Operationally, this ensures we detect translations which either delete or insert material. Clearly, there are also differences between the two tasks. An important one is that RTE assumes the well-formedness of the two sentences. This is not generally true in MT, and could lead to degraded linguistic analyses. However, entailment relations are more sensitive to the contribution of individual words (MacCartney and Manning, 2008). In Example 2, the modal modifiers break the entailment between two otherwise identical sentences: (2) HYP: Peter is certainly from Lincolnshire. REF: Peter is possibly from Lincolnshire. This means that the prediction of TE hinges on correct semantic analysis and is sensitive to misanalyses. In contrast, human MT judgments behave robustly. Translations that involve individual errors, like (2), are judged lower than perfect ones, but usually not crucially so, since most aspects are still rendered correctly. We thus expect even noisy RTE features to be predictive for translation quality. This"
P09-1034,W08-0309,0,0.0715095,"Gibbs sampling (see de Marneffe et al. (2007)). Entailment features. In the third stage, the system produces roughly 100 features for each aligned premise-hypothesis pair. A small number of them are real-valued (mostly quality scores), but most are binary implementations of small linguistic theories whose activation indicates syntactic and se4 4.1 Experimental Evaluation Experiments Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five- or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al., 2008). An alternative that has been adopted by the yearly WMT evaluation shared tasks since 2008 is the collection of pairwise preference judgments between pairs of MT hypotheses which can be elicited (somewhat) more reliably. We demonstrate that our approach works well for both types of annotation and different corpora. Experiment 1 models absolute scores on Asian newswire, and Experiment 2 pairwise preferences on European speech and news data. 4.2 Evaluation We evaluate the output of our models both on the sentence and on the system level. At the sentence level, we can correlate predictions in Ex"
P09-1034,N06-1006,1,0.583147,"Missing"
P09-1034,P08-1007,0,0.286549,"ed towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses. We operati"
P09-1034,I08-1042,0,0.0473301,"Missing"
P09-1034,P06-1114,0,0.0466837,". 3 Textual Entailment vs. MT Evaluation Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE). TE was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning patterns than classical, strict logical entailment. Textual entailment is defined informally as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if “a human reading P would infer that H is most likely true”. Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). The relation between textual entailment and MT evaluation is shown in Figure 1. Perfect MT output and the reference translation entail each other (top). Translation problems that impact semantic equivalence, e.g., deletion or addition of material, can break entailment in one or both directions (bottom). On the modelling level, there is common ground between RTE and MT evaluation: Both have to distinguish between valid and invalid variation to determine whether two texts convey the same information or not. For example, to recognize the bidirectional entailment in Ex. (1), RTE must account for"
P09-1034,hovy-etal-2006-automated,0,0.0208131,"a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar ideas have been applied by Owczarzak et al. (2008) to LFG parses, and by Liu and Gildea (2005) to features derived from phrase-structure tress. This approach has also been successful for the related task of summarization evaluation (Hovy et al., 2006). The most comparable work to ours is Gim´enez and M´arquez (2008). Our results agree on the crucial point that the use of a wide range of linguistic knowledge in MT evaluation is desirable and important. However, Gim´enez and M´arquez advocate the use of a bottom-up development process that builds on a set of “heterogeneous”, independent metrics each of which measures overlap with respect to one linguistic level. In contrast, our aim is to provide a “top-down”, integrated motivation for the features we integrate through the textual entailment recognition paradigm. 8 Conclusion and Outlook In"
P09-1034,N06-1058,0,0.0523827,"ependents of predicates is unreliable on the WMT data. Other differences do reflect more fundamental differences between the two tasks (cf. Section 3). For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation. 7 Related Work Researchers have exploited various resources to enable the matching between words or n-grams that are semantically close but not identical. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al. (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. These approaches reduce the risk that a good translation is rated poorly due to lexical deviation, but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar ideas have been applied by Owcza"
P09-1034,P03-1021,0,0.00335176,"Missing"
P09-1034,W09-0404,1,0.763215,"Missing"
P09-1034,P02-1040,0,0.104893,"fferent settings. The combination metric outperforms the individual scores, but is bested by the entailment-based metric. Combining the entailment and traditional features yields further improvements. 1 (1) HYP: However, this was declared terrorism by observers and witnesses. REF: Nevertheless, commentators as well as eyewitnesses are terming it terrorism. Introduction Constant evaluation is vital to the progress of machine translation (MT). Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations. The resulting scores are cheap and objective. However, studies such as Callison-Burch et al. (2006) have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be “gamed” by permuting word order; (3) for some corpora and languages,"
P09-1034,2007.tmi-papers.19,0,0.082174,"ment REF: Three aid workers were kidnapped by pirates. Figure 1: Entailment status between an MT system hypothesis and a reference translation for equivalent (top) and non-equivalent (bottom) translations. 2 Regression-based MT Quality Prediction Current MT metrics tend to focus on a single dimension of linguistic information. Since the importance of these dimensions tends not to be stable across language pairs, genres, and systems, performance of these metrics varies substantially. A simple strategy to overcome this problem could be to combine the judgments of different metrics. For example, Paul et al. (2007) train binary classifiers on a feature set formed by a number of MT metrics. We follow a similar idea, but use a regularized linear regression to directly predict human ratings. Feature combination via regression is a supervised approach that requires labeled data. As we show in Section 5, this data is available, and the resulting model generalizes well from relatively small amounts of training data. 3 Textual Entailment vs. MT Evaluation Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE). TE was introduced by Dagan et al. (2005) as a"
P09-1034,2006.amta-papers.25,0,0.0532152,"(3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quali"
P09-1034,W09-0441,0,0.0147185,"tness and improved correlations for the regression models. An exception is BLEU-1 and NIST-4 on Expt. 1 (Ar, Ch), which perform 0.5–1 point better at the sentence level. T ER R includes 50 features. We start with the standard TER score and the number of each of the four edit operations. Since the default uniform cost does not always correlate well with human judgment, we duplicate these features for 9 non-uniform edit costs. We find it effective to set insertion cost close to 0, as a way of enabling surface variation, and indeed the new TERp metric uses a similarly low default insertion cost (Snover et al., 2009). M ETEOR R 4.4 consists of METEOR v0.7. Combination Metrics The following three regression models implement the methods discussed in Sections 2 and 3. M T R combines the 85 features of the four baseline models. It uses no entailment features. RTE R uses the 70 entailment features described in Section 3.1, but no M T R features. M T +RTE R uses all M T R and RTE R features, combining matching and entailment evidence.3 5 Expt. 1: Predicting Absolute Scores Data. Our first experiment evaluates the models we have proposed on a corpus with traditional annotation on a seven-point scale, namely the"
P09-1034,W06-1610,0,0.214402,"or mismatches between dependents of predicates is unreliable on the WMT data. Other differences do reflect more fundamental differences between the two tasks (cf. Section 3). For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation. 7 Related Work Researchers have exploited various resources to enable the matching between words or n-grams that are semantically close but not identical. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al. (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. These approaches reduce the risk that a good translation is rated poorly due to lexical deviation, but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar"
P09-1034,W07-1401,0,\N,Missing
P10-1123,abad-etal-2010-resource,1,0.723492,"latter is not found in the focus term’s reference chain. In initial analysis we found that the standard substitution operation applied by virtually all previous studies for integrating coreference into entailment is insufficient. We identified three distinct cases for the integration of discourse reference knowledge in entailment, which correspond to different relations between the target component, the focus term and the reference term. This section describes the three cases and characterizes them in terms of tree transformations. An initial version of these transformations is described in (Abad et al., 2010). We assume a transformation-based entailment architecture (cf. Section 2.2), although we believe that the key points of our account are also applicable to alignment-based architecture. Transformations create revised trees that cover previously uncovered target components in H. The output of each transformation, T1 , is comprised of copies of the components used to construct it, and is appended to the discourse forest, which includes the dependency trees of all sentences and their generated consequents. We assume that we have access to a dependency tree for H, a dependency forest for T and its"
P10-1123,W07-1420,0,0.0545505,"Missing"
P10-1123,H05-1079,0,0.0477878,"Missing"
P10-1123,P09-1068,0,0.0157562,"d bridging.1 Another reason is uncertainty about their practical importance. 2.2 2 2.1 Background Discourse in NLP Discourse information plays a role in a range of NLP tasks. It is obviously central to discourse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that discourse provides is coreference, i.e., information that two linguistic expressions refer to the same entity or event. Coreference is particularly important for processing pronouns and other anaphoric expressions, such as he in Example 1. Ability to resolve this reference translates directly into, e.g., a QA system’s ability to answer questions like Who ki"
P10-1123,W07-1427,0,0.0249585,"Missing"
P10-1123,W07-1401,1,0.734314,"e, e.g. (Li et al., 2009; Dali et al., 2009). An important reason is the unavailability of tools to resolve the more complex (and difficult) forms of discourse reference such as Discourse in Textual Entailment Textual Entailment has been introduced in Section 1 as a common-sense notion of inference. It has spawned interest in the computational linguistics community as a common denominator of many NLP tasks including IE, summarization and tutoring (Romano et al., 2006; Harabagiu et al., 2007; Nielsen et al., 2009). Architectures for Textual Entailment. Over the course of recent RTE challenges (Giampiccolo et al., 2007; Giampiccolo et al., 2008), the main benchmark for TE technology, two architectures for modeling TE have emerged as dominant: transformations and alignment. The goal of transformation-based TE models is to determine the entailment relation T ⇒ H by finding a “proof”, i.e., a sequence of consequents, (T, T1 , . . . , Tn ), such that Tn =H (Bar-Haim et al., 2008; Harmeling, 2009), and that in each transformation, Ti → Ti+1 , the consequent Ti+1 is entailed by Ti . These transformations commonly include lexical modifications and the generation of syntactic alternatives. The second major approach"
P10-1123,C96-1079,0,0.00959989,"tion plays a role in a range of NLP tasks. It is obviously central to discourse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that discourse provides is coreference, i.e., information that two linguistic expressions refer to the same entity or event. Coreference is particularly important for processing pronouns and other anaphoric expressions, such as he in Example 1. Ability to resolve this reference translates directly into, e.g., a QA system’s ability to answer questions like Who killed Kennedy?. A second, more complex type of information stems from bridging references, such as in the following discourse (Asher"
P10-1123,J97-1003,0,0.0586295,"209 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1209–1219, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics mechanisms (Section 5). Section 6 presents quantitative findings and further observations. Conclusions are discussed in Section 7. event coreference and bridging.1 Another reason is uncertainty about their practical importance. 2.2 2 2.1 Background Discourse in NLP Discourse information plays a role in a range of NLP tasks. It is obviously central to discourse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that"
P10-1123,N06-2015,0,0.033287,"course references on entailment with an annotation study which removes these limitations. To counteract (1), we use the recent RTE-5 Search dataset (details below). To avoid (2), we perform a manual analysis, assuming discourse references as predicted by an oracle. With regards to (3), our annotation scheme covers coreference and bridging relations of all syntactic categories and classifies them. As for (4), we suggest several operations necessary to integrate the discourse information into an entailment engine. In contrast to the numerous existing datasets annotated for discourse references (Hovy et al., 2006; Strassel et al., 2008), we do not annotate exhaustively. Rather, we are interested specifically in those references instances that impact inference. Furthermore, we analyze each instance from an entailment perspective, characterizing the relevant factors that have an impact on inference. To our knowledge, this is the first such in-depth study.4 The results of our study are of twofold interest. First, they provide guidance for the developers of reference resolvers who might prioritize the scope of their systems to make them more valuable for inference. Second, they point out potential directi"
P10-1123,P09-1047,0,0.0235941,"Missing"
P10-1123,P09-1083,0,0.0409466,"sher and Lascarides, 1998): (2) “I’ve just arrived. The camel is outside.” While coreference indicates equivalence, bridging points to the existence of a salient semantic relation between two distinct entities or events. Here, it is (informally) ‘means of transport’, which would make the discourse (2) relevant for a question like How did I arrive here?. Other types of bridging relations include set-membership, roles in events and consequence (Clark, 1975). Note, however, that text understanding systems are generally limited to the resolution of entity (or even just pronoun) coreference, e.g. (Li et al., 2009; Dali et al., 2009). An important reason is the unavailability of tools to resolve the more complex (and difficult) forms of discourse reference such as Discourse in Textual Entailment Textual Entailment has been introduced in Section 1 as a common-sense notion of inference. It has spawned interest in the computational linguistics community as a common denominator of many NLP tasks including IE, summarization and tutoring (Romano et al., 2006; Harabagiu et al., 2007; Nielsen et al., 2009). Architectures for Textual Entailment. Over the course of recent RTE challenges (Giampiccolo et al., 2007"
P10-1123,W04-0704,0,0.0236046,"n 6 presents quantitative findings and further observations. Conclusions are discussed in Section 7. event coreference and bridging.1 Another reason is uncertainty about their practical importance. 2.2 2 2.1 Background Discourse in NLP Discourse information plays a role in a range of NLP tasks. It is obviously central to discourse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that discourse provides is coreference, i.e., information that two linguistic expressions refer to the same entity or event. Coreference is particularly important for processing pronouns and other anaphoric expressions, such as he in E"
P10-1123,P93-1016,0,0.0375104,"formation from the discourse sentences T 0 / T 00 . Referring terms (in T ) and target terms (in H) are shown in boldface. 4 Analysis Scheme For annotating the RTE-5 data, we operationalize reference relations that are relevant for entailment as those that improve coverage. Recall from Section 2.2 that the concept of coverage is applicable to both transformation and alignment models, all of which aim at maximizing coverage of H by T . We represent T and H as syntactic trees, as common in the RTE literature (Zanzotto et al., 2009; Agichtein et al., 2008). Specifically, we assume MINIPAR-style (Lin, 1993) dependency trees where nodes represent text expressions and edges represent the syntactic relations between them. We use “term” to refer to text expressions, and “components” to refer to nodes, edges, and subtrees. Dependency trees are a popular choice in RTE since they offer a fairly semantics-oriented account of the sentence structure that can still be constructed robustly. In an ideal case of entailment, all nodes and dependency edges of H are covered by T . For each T − H pair, we annotate all relevant discourse references in terms of three items: the target component in H, the focus term"
P10-1123,D08-1084,0,0.0164515,"Missing"
P10-1123,W03-2606,0,0.0307542,". Alignment quality is generally determined based on features that assess the validity of the local replacement of the T entity by the H entity. While transformation- and alignment-based entailment models look different at first glance, they ultimately have the same goal, namely obtaining a maximal coverage of H by T , i.e. to identify matches of as many elements of H within T as possible.2 To do so, both architectures typically make use of inference rules such as ‘Y was purchased by X → X paid for Y’, either by directly applying them as transformations, or by using them 1 Some studies, e.g. (Markert et al., 2003; Poesio et al., 2004), address the resolution of a few specific kinds of bridging relations; yet, wide-scope systems for bridging resolution are unavailable. 2 Clearly, the details of how the final entailment decision is made based on the attained coverage differ substantially among models. 1210 to score alignments. Rules are generally drawn from external knowledge resources, such as WordNet (Fellbaum, 1998) or DIRT (Lin and Pantel, 2001), although knowledge gaps remain a key obstacle (Bos, 2005; Balahur et al., 2008; Bar-Haim et al., 2008). Discourse in previous RTE challenges. The first two"
P10-1123,P04-1019,0,0.080464,"generally determined based on features that assess the validity of the local replacement of the T entity by the H entity. While transformation- and alignment-based entailment models look different at first glance, they ultimately have the same goal, namely obtaining a maximal coverage of H by T , i.e. to identify matches of as many elements of H within T as possible.2 To do so, both architectures typically make use of inference rules such as ‘Y was purchased by X → X paid for Y’, either by directly applying them as transformations, or by using them 1 Some studies, e.g. (Markert et al., 2003; Poesio et al., 2004), address the resolution of a few specific kinds of bridging relations; yet, wide-scope systems for bridging resolution are unavailable. 2 Clearly, the details of how the final entailment decision is made based on the attained coverage differ substantially among models. 1210 to score alignments. Rules are generally drawn from external knowledge resources, such as WordNet (Fellbaum, 1998) or DIRT (Lin and Pantel, 2001), although knowledge gaps remain a key obstacle (Bos, 2005; Balahur et al., 2008; Bar-Haim et al., 2008). Discourse in previous RTE challenges. The first two rounds of the RTE cha"
P10-1123,N06-1025,0,0.027942,"inference. We identified three general cases, and suggested matching operations to obtain the relevant inferences, formulated as tree transformations. Furthermore, our evidence suggests that for practical reasons, the resolution of discourse references should be tightly integrated into entailment systems instead of treating it as a preprocessing step. A particularly interesting result concerns the interplay between discourse references and entailment knowledge. While semantic knowledge (e.g., from WordNet or Wikipedia) has been used beneficially for coreference resolution (Soon et al., 2001; Ponzetto and Strube, 2006), reference resolution has, to our knowledge, not yet been employed to validate entailment rules’ applicability. Our analyses suggest that in the context of deciding textual entailment, reference resolution and entailment knowledge can be seen as complementary ways of achieving the same goal, namely enriching T with additional knowledge to allow the inference of H. Given that both of the technologies are still imperfect, we envisage the way forward as a joint strategy, where reference resolution and entailment rules mutually fill each other’s gaps (cf. Example 3). In sum, our study shows that"
P10-1123,qiu-etal-2004-public,0,0.179043,"Missing"
P10-1123,E06-1052,1,0.762829,"ce (Clark, 1975). Note, however, that text understanding systems are generally limited to the resolution of entity (or even just pronoun) coreference, e.g. (Li et al., 2009; Dali et al., 2009). An important reason is the unavailability of tools to resolve the more complex (and difficult) forms of discourse reference such as Discourse in Textual Entailment Textual Entailment has been introduced in Section 1 as a common-sense notion of inference. It has spawned interest in the computational linguistics community as a common denominator of many NLP tasks including IE, summarization and tutoring (Romano et al., 2006; Harabagiu et al., 2007; Nielsen et al., 2009). Architectures for Textual Entailment. Over the course of recent RTE challenges (Giampiccolo et al., 2007; Giampiccolo et al., 2008), the main benchmark for TE technology, two architectures for modeling TE have emerged as dominant: transformations and alignment. The goal of transformation-based TE models is to determine the entailment relation T ⇒ H by finding a “proof”, i.e., a sequence of consequents, (T, T1 , . . . , Tn ), such that Tn =H (Bar-Haim et al., 2008; Harmeling, 2009), and that in each transformation, Ti → Ti+1 , the consequent Ti+1"
P10-1123,J01-4004,0,0.0259146,"uently relevant for inference. We identified three general cases, and suggested matching operations to obtain the relevant inferences, formulated as tree transformations. Furthermore, our evidence suggests that for practical reasons, the resolution of discourse references should be tightly integrated into entailment systems instead of treating it as a preprocessing step. A particularly interesting result concerns the interplay between discourse references and entailment knowledge. While semantic knowledge (e.g., from WordNet or Wikipedia) has been used beneficially for coreference resolution (Soon et al., 2001; Ponzetto and Strube, 2006), reference resolution has, to our knowledge, not yet been employed to validate entailment rules’ applicability. Our analyses suggest that in the context of deciding textual entailment, reference resolution and entailment knowledge can be seen as complementary ways of achieving the same goal, namely enriching T with additional knowledge to allow the inference of H. Given that both of the technologies are still imperfect, we envisage the way forward as a joint strategy, where reference resolution and entailment rules mutually fill each other’s gaps (cf. Example 3). I"
P10-1123,strassel-etal-2008-linguistic,0,0.122312,"urse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that discourse provides is coreference, i.e., information that two linguistic expressions refer to the same entity or event. Coreference is particularly important for processing pronouns and other anaphoric expressions, such as he in Example 1. Ability to resolve this reference translates directly into, e.g., a QA system’s ability to answer questions like Who killed Kennedy?. A second, more complex type of information stems from bridging references, such as in the following discourse (Asher and Lascarides, 1998): (2) “I’ve just arrived. The camel is outside.”"
P10-1123,W07-1400,0,\N,Missing
P10-2017,W09-0201,0,0.0109048,"one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. 1 Introduction Distributional models are a popular framework for representing word meaning. They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textu"
P10-2017,P08-1078,0,0.101516,"ll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textual entailment (Bar-Haim et al., 2007). However, polysemy is a fundamental problem for distributional models. Typically, distributional models compute a single “type” vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. If the target is polysemous, this vector mixes contextual features for all the senses of the target. For example, among the 2 Related Work Among distributional models of word, there are some approaches that address polysemy, either by inducing a fix"
P10-2017,W09-0208,1,0.834592,"Missing"
P10-2017,W09-2506,0,0.183666,"mong distributional models of word, there are some approaches that address polysemy, either by inducing a fixed clustering of contexts into senses (Sch¨utze, 1998) or by dynamically modi92 Proceedings of the ACL 2010 Conference Short Papers, pages 92–97, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Sentential context After a fire extinguisher is used, it must always be returned for recharging and its use recorded. fying a word’s type vector according to each given sentence context (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). Polysemy-aware approaches also differ in their notion of context. Some use a bag-of-words representation of words in the current sentence (Sch¨utze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. In cognitive science, prototype models predict degree of category membership through similarity to a single prototype, while exemplar theor"
P10-2017,W02-0811,0,0.0271141,"settings involved the activation of only few exemplars, computation with exemplar models still requires the management of large numbers of vectors. The computational overhead can be reduced by using data structures that cut down on the number of vector comparisons, or by decreasing vector dimensionality (Gorman and Curran, 2006). We will experiment with those methods to determine the tradeoff of runtime and accuracy for this task. Another area of future work is to move beyond bag-of-words context: It is known from WSD that syntactic and bag-of-words contexts provide complementary information (Florian et al., 2002; Szpektor et al., 2008), and we hope that they can be integrated in a more sophisticated exemplar model. Finally, we will to explore task-based evaluations. Relation extraction and textual entailment in particular are tasks where similar models have been used before (Szpektor et al., 2008). Acknowledgements. This work was supported in part by National Science Foundation grant IIS0845925, and by a Morris Memorial Grant from the New York Community Trust. TDP09 NA 36.5 actTP 39.9 39.6 Table 4: Comparison to other models on two subsets of LexSub (GAP evaluation) datapoints) and Erk and Pad´o (200"
P10-2017,P06-1046,0,0.025763,"ved over using s itself only for verbs (Tab. 3). This suggests the possibility of considering T ’s activated paraphrase candidates as the representation of T in the context s, rather than some vector of T itself, in the spirit of Kintsch (2001). While it is encouraging that the best parameter settings involved the activation of only few exemplars, computation with exemplar models still requires the management of large numbers of vectors. The computational overhead can be reduced by using data structures that cut down on the number of vector comparisons, or by decreasing vector dimensionality (Gorman and Curran, 2006). We will experiment with those methods to determine the tradeoff of runtime and accuracy for this task. Another area of future work is to move beyond bag-of-words context: It is known from WSD that syntactic and bag-of-words contexts provide complementary information (Florian et al., 2002; Szpektor et al., 2008), and we hope that they can be integrated in a more sophisticated exemplar model. Finally, we will to explore task-based evaluations. Relation extraction and textual entailment in particular are tasks where similar models have been used before (Szpektor et al., 2008). Acknowledgements."
P10-2017,J03-4004,0,0.0184317,"oing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. 1 Introduction Distributional models are a popular framework for representing word meaning. They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor"
P10-2017,P08-1028,0,0.545312,"target. For example, among the 2 Related Work Among distributional models of word, there are some approaches that address polysemy, either by inducing a fixed clustering of contexts into senses (Sch¨utze, 1998) or by dynamically modi92 Proceedings of the ACL 2010 Conference Short Papers, pages 92–97, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Sentential context After a fire extinguisher is used, it must always be returned for recharging and its use recorded. fying a word’s type vector according to each given sentence context (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). Polysemy-aware approaches also differ in their notion of context. Some use a bag-of-words representation of words in the current sentence (Sch¨utze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. In cognitive science, prototype models predict degree of category membership through similarity"
P10-2017,J98-1004,0,0.837204,"Missing"
P10-2017,D08-1094,1,\N,Missing
P11-2082,P88-1018,0,0.656711,"guage Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in particular the conditions governing"
P11-2082,C10-2010,0,0.0533823,"cleaned by deleting the index, prologue, epilogue and Gutenberg license from the beginning and end of the files. To some extent the chapter numbers and titles occurring at the beginning of each chapter were cleared as well. The files were then formatted to contain one sentence per line and a blank line was inserted to preserve the segmentation information. The sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005) were used. We obtained a comparable corpus of English and German novels using the above pre-processing. The files in the corpus were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments. After obtaining the 1 http://www.gutenberg.org and http://gutenberg.spiegel.de/ 468 ID (1) (2) (3) (4) Position any non-initial non-initial non-initial Lemma du sie ihr ihr Cap any yes no yes Category T V T V Table 1: Rules for T/V determination for German personal pronouns. (Cap: Capitalized) sentence aligned corpus we computed word alignments in both English to German and German to English directions using Giza++ (Och and Ney, 2003). The corpus was lemmatized and POS-tagged using TreeTagger (Schmid, 1994). We did not apply a full parser to k"
P11-2082,E03-1009,0,0.101583,"Missing"
P11-2082,P10-1015,0,0.0951839,"al/informal (T/V) address distinction in 5 We experimented with logistic regression models, but were unable to obtain better performance, probably because we introduced a frequency threshold to limit the feature set size. Top 10 words for V P (w|V ) Word w P (w|T ) Fogg 49.7 32.5 Oswald Ma 31.8 25.2 Gentlemen 24.2 Madam Parfenovitch 23.2 Monsieur 22.6 22.5 Fix Permit 22.5 ’am 22.4 Top 10 words for T P (w|T ) Word w P (w|V ) Thee 67.2 Trot 46.8 Bagheera 37.7 Khan 34.7 Mowgli 33.2 Baloo 30.2 Sahib 30.2 Clare 29.7 didst 27.7 Reinhard 27.2 ever, is the induction of social networks in such novels (Elson et al., 2010): Information on the social relationship between a speaker and an addressee should provide global constraints on all instances of communications between them, and predict the form of address much more reliably than word features can. Acknowledgments Manaal Faruqui has been partially supported by a Microsoft Research India Travel Grant. References Table 4: Words that are indicative for T or V modern English, where it is not determined through pronoun choice or other overt means. We see this task as an instance of the general problem of recovering “hidden” information that is not expressed overt"
P11-2082,W09-0420,0,0.0528938,"ddress. Our results could be useful, for example, for MT from English into languages that distinguish T and V, although we did not test this prediction with the limits of a short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June"
P11-2082,C90-3028,0,0.310283,"ction with the limits of a short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science,"
P11-2082,W03-1612,0,0.284331,"short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in parti"
P11-2082,2005.mtsummit-papers.11,0,0.00963744,"lation science, covering in particular the conditions governing T/V use in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and on the difficulties in translating them (Ardila, 2003; Künzli, 2010). However, these studies are generally not computational in nature, and most of their observations and predictions are difficult to operationalize. 2 A Parallel Corpus of Literary Texts 2.1 Data Selection We chose literary texts to build a parallel corpus for the investigation of the T/V distinction. The main reason is that commonly used non-literary collections like EUROPARL (Koehn, 2005) consist almost exclusively of formal interactions and are therefore of no use to us. Fortunately, many 18th and 19th century texts are freely available in several languages. We identified 115 novels among the texts provided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per language.1 Examples include Dickens’ David Copperfield or Tolstoy’s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align. 2.2 Data Preparation As the Ger"
P11-2082,D08-1108,0,0.203039,"Missing"
P11-2082,J03-1002,0,0.0035304,"n novels using the above pre-processing. The files in the corpus were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments. After obtaining the 1 http://www.gutenberg.org and http://gutenberg.spiegel.de/ 468 ID (1) (2) (3) (4) Position any non-initial non-initial non-initial Lemma du sie ihr ihr Cap any yes no yes Category T V T V Table 1: Rules for T/V determination for German personal pronouns. (Cap: Capitalized) sentence aligned corpus we computed word alignments in both English to German and German to English directions using Giza++ (Och and Ney, 2003). The corpus was lemmatized and POS-tagged using TreeTagger (Schmid, 1994). We did not apply a full parser to keep processing as efficient as possible. 2.3 T/V Gold Labels for English Utterances The goal of creating our corpus is to enable the investigation of contextual correlates of T/V in English. In order to do this, we need to decide for as many English utterances in our corpus as possible whether they instantiate formal or informal address. Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and"
P11-2082,P98-2193,0,0.590452,"be useful, for example, for MT from English into languages that distinguish T and V, although we did not test this prediction with the limits of a short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June 19-24, 2011. 2011 Associati"
P11-2082,N01-1026,0,0.154055,"Missing"
P11-2082,C98-2188,0,\N,Missing
P13-1118,J12-1003,0,0.0227636,"understood as groups of derivationally related lemmas (Daille et al., 2002; Milin et al., 2009). The lemmas in CatVar come from various open word classes, and multiple words may be listed for the same POS. The above family lists two nouns: an event noun (asking) and an agentive noun (asker). However, CatVar does not consider prefixation, which is why, e.g., the adjective unasked is missing. CatVar has found application in different areas of English NLP. Examples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applications. Even though the"
P13-1118,W99-0904,0,0.272967,"rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An exception is the work by Gaussier (1999), who applies an unsupervised model to construct derivational families for French. For German, several morphological tools exist. Morphix is a classification-based analyzer and generator of German words on the inflectional level (Finkler and Neumann, 1988). SMOR (Schmid et al., 2004) employs a finite-state transducer to analyze German words at the inflectional, derivational, and compositional level, and has been used in other morphological analyzers, e.g., Morphisto (Zielinski and Simon, 2008). The site canoonet1 offers broad-coverage information about the German language including derivationa"
P13-1118,P04-1048,0,0.129323,"nd multiple words may be listed for the same POS. The above family lists two nouns: an event noun (asking) and an agentive noun (asker). However, CatVar does not consider prefixation, which is why, e.g., the adjective unasked is missing. CatVar has found application in different areas of English NLP. Examples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applications. Even though there are two derivational resources for this language, IMSL EX (Fitschen, 2004) and C ELEX (Baayen et al., 1996), both have shortcomings. The former does not"
P13-1118,C10-1011,0,0.00877358,"ional families of different size (counting 2 up to 114 lemmas), and assigned 55, 79, and 24 rules to L3, L2 and L1, respectively. 4.2 Data and Preprocessing For an accurate application of nominal derivation rules, we need a lemma list with POS and gender information. We POS-tag and lemmatize S DEWAC, a large German-language web corpus from which boilerplate paragraphs, ungrammatical sentences, and duplicate pages were removed (Faaß et al., 2010). For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). We treat proper nouns like common nouns. We apply three language-specific filtering steps based on observations in Section 3.1. First, we discard non-capitalized nominal lemmas. Second, we deleted verbal lemmas not ending in verb suffixes. An additional complication in German concerns prefix verbs, because prefix is separated in tensed instances. For example, the 3rd person male singular of aufhören (to stop) is er hört auf (he stops). Since most prefixes double as prepositions, the correct lemmas can only be reconstructed by parsing. We parse the corpus using the MST parser (McDonald et al."
P13-1118,N03-1013,0,0.229523,"Missing"
P13-1118,H92-1022,0,0.176793,"Missing"
P13-1118,J96-2004,0,0.00789448,"ws us to identify the pairs that are derivationally unrelated, but compositionally related, e.g., EhemannN – EhefrauN (husband – wife). We first carried out a calibration phase in which the annotators double-annotated 200 pairs from each of the two samples and refined the annotation guidelines. In a subsequent validation phase, we computed inter-annotator agreements on the annotations of another 200 pairs each from the P- and the R-samples. Table 4 shows the proportion of identical annotations by both annotators as well as Cohen’s κ score (Cohen, 1968). We achieve substantial agreement for κ (Carletta, 1996). On the P-sample, κ is a little lower because the distribution of the categories is skewed towards R, which makes an agreement by chance more probable. In our opinion, the IAA results were sufficiently high to switch to single annotation for the production phase. Here, each annotator annotated another 1000 pairs from the P-sample and R-sample so that the final test set consists of 2000 pairs from each sample. The P-sample contains 1663 positive (R+M) and 337 negative (N+C+L) pairs, respectively, the R-sample contains 575 positive and 1425 negative pairs. As expected, there are more positive 6"
P13-1118,W98-1239,0,0.0300565,"tion model that is applied to German in Section 4. Sections 5 and 6 present our evaluation setup and results. Section 7 concludes the paper and outlines future work. 2 Related Work Computational models of morphology have a long tradition. Koskenniemi (1983) was the first who analyzed and generated morphological phenomena computationally. His two-level theory has been applied in finite state transducers (FST) for several languages (Karttunen and Beesley, 2005). Many recent approaches automatically induce morphological information from corpora. They are either based solely on corpus statistics (Déjean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precisio"
P13-1118,J11-2002,0,0.0467485,"tational models of morphology have a long tradition. Koskenniemi (1983) was the first who analyzed and generated morphological phenomena computationally. His two-level theory has been applied in finite state transducers (FST) for several languages (Karttunen and Beesley, 2005). Many recent approaches automatically induce morphological information from corpora. They are either based solely on corpus statistics (Déjean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An exception is the work by Gaussier (1999), who applies an unsupervised model to construct derivational families fo"
P13-1118,W06-2932,0,0.0211716,"(Bohnet, 2010). We treat proper nouns like common nouns. We apply three language-specific filtering steps based on observations in Section 3.1. First, we discard non-capitalized nominal lemmas. Second, we deleted verbal lemmas not ending in verb suffixes. An additional complication in German concerns prefix verbs, because prefix is separated in tensed instances. For example, the 3rd person male singular of aufhören (to stop) is er hört auf (he stops). Since most prefixes double as prepositions, the correct lemmas can only be reconstructed by parsing. We parse the corpus using the MST parser (McDonald et al., 2006) and recover prefix verbs by searching for instances of the dependency relation labeled P TKVZ. Since S DEWAC, as a web corpus, still contains errors, we only take into account lemmas that occur three times or more in the corpus. Considering the size of S DEWAC, we consider this as a conservative filtering step that preserves high recall and provides a comprehensive basis for evaluation. After preprocessing and filtering, we run the induction of the derivational families as explained in Section 3 to obtain the DE RIV BASE resource. 4.3 Statistics on DE RIV BASE The preparation of the S DEWAC c"
P13-1118,meyers-etal-2004-annotating,0,0.0174174,"open word classes, and multiple words may be listed for the same POS. The above family lists two nouns: an event noun (asking) and an agentive noun (asker). However, CatVar does not consider prefixation, which is why, e.g., the adjective unasked is missing. CatVar has found application in different areas of English NLP. Examples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applications. Even though there are two derivational resources for this language, IMSL EX (Fitschen, 2004) and C ELEX (Baayen et al., 1996), both have shortcomings"
P13-1118,W07-1710,0,0.196021,"Missing"
P13-1118,piasecki-etal-2012-recognition,0,0.0563221,"rk. 2 Related Work Computational models of morphology have a long tradition. Koskenniemi (1983) was the first who analyzed and generated morphological phenomena computationally. His two-level theory has been applied in finite state transducers (FST) for several languages (Karttunen and Beesley, 2005). Many recent approaches automatically induce morphological information from corpora. They are either based solely on corpus statistics (Déjean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An exception is the work by Gaussier (1999), who applies an unsupervised model to cons"
P13-1118,W11-1606,0,0.0120952,"xamples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applications. Even though there are two derivational resources for this language, IMSL EX (Fitschen, 2004) and C ELEX (Baayen et al., 1996), both have shortcomings. The former does not appear to be publicly available, and the latter has a limited coverage (50k lemmas) and does not explicitly represent derivational relationships within families, which are necessary for fine-grained optimization of families. For this reason, we look into building a novel derivational resource for German. Unf"
P13-1118,schmid-etal-2004-smor,0,0.0231245,"to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An exception is the work by Gaussier (1999), who applies an unsupervised model to construct derivational families for French. For German, several morphological tools exist. Morphix is a classification-based analyzer and generator of German words on the inflectional level (Finkler and Neumann, 1988). SMOR (Schmid et al., 2004) employs a finite-state transducer to analyze German words at the inflectional, derivational, and compositional level, and has been used in other morphological analyzers, e.g., Morphisto (Zielinski and Simon, 2008). The site canoonet1 offers broad-coverage information about the German language including derivational word formation. 3 Framework In this section, we describe our rule-based model of derivation, its operation to define derivational families, and the application of the model to German. We note that the model is purely surface-based, i.e., it does not model any semantic regularities"
P13-1118,W00-0712,0,0.11241,"our evaluation setup and results. Section 7 concludes the paper and outlines future work. 2 Related Work Computational models of morphology have a long tradition. Koskenniemi (1983) was the first who analyzed and generated morphological phenomena computationally. His two-level theory has been applied in finite state transducers (FST) for several languages (Karttunen and Beesley, 2005). Many recent approaches automatically induce morphological information from corpora. They are either based solely on corpus statistics (Déjean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An"
P13-1118,C08-1107,0,0.0204321,"nal families are commonly understood as groups of derivationally related lemmas (Daille et al., 2002; Milin et al., 2009). The lemmas in CatVar come from various open word classes, and multiple words may be listed for the same POS. The above family lists two nouns: an event noun (asking) and an agentive noun (asker). However, CatVar does not consider prefixation, which is why, e.g., the adjective unasked is missing. CatVar has found application in different areas of English NLP. Examples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applica"
P13-1118,faass-etal-2010-design,0,\N,Missing
P13-1118,2003.mtsummit-systems.9,0,\N,Missing
P13-2128,W98-0705,0,0.190944,"Missing"
P13-2128,J10-4006,0,0.157856,"ector spaces have been applied successfully to many problems in NLP (see Turney and Pantel (2010) or Erk (2012) for current overviews). Most distributional models in computational lexical semantics are either (a) bag-of-words models, where the context features are words within a surface window around the target word, or (b) syntactic models, where context features are typically pairs of dependency relations and context words. The advantage of syntactic models is that they incorporate a richer, structured notion of context. This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks. It is also able – at least in principle – to capture more fine-grained types of semantic similarity such as predicateargument plausibility (Erk et al., 2010). At the same time, syntactic spaces are much more prone to sparsity problems, as their contexts are sparser. This leads to reliability and coverage problems. In this paper, we propose a novel strategy for combating sparsity in syntactic vector spaces, derivational smoothing. It follows the intuition that derivationally related words (feed – feeder, blocked – blockage) are, as a rule, semantically h"
P13-2128,D07-1060,0,0.0412258,"Missing"
P13-2128,D08-1007,0,0.0126143,"l Linguistics, pages 731–735, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics off from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of morphology by building language models for stemming-based equivalence classes. Our approach also uses morphological processing, albeit more precise than stemming. 3 A Resource for German Derivation Using derivational knowledge for smoothing raises the question of how semantically similar the lemmas within a family really are. Fortunately, DERIV BASE"
P13-2128,C10-1011,0,0.0285386,"-based smoothing triggers), but they did not yield any improvements over the simpler models we present here. 5 Experimental Evaluation Syntactic Distributional Model. The syntactic distributional model that we use represents target words by pairs of dependency relations and context words. More specifically, we use the W × LW matricization of D M .D E, the German version (Pad´o and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010). D M .D E was created on the basis of the 884M-token SD E WAC web corpus (Faaß et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010). Experiments. We evaluate the impact of smoothing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German G UR 350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict 2 semantic similarity as cosine similarity. We make a prediction for a word pair if both words are represented in the semantic space and their vectors have a non-zero similarity. The second task is synonym choice"
P13-2128,1993.eamt-1.1,0,0.579256,"Missing"
P13-2128,P05-1045,0,0.00756365,"and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731–735, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics off from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of morphology by building language models for stemming-based equivalence classes. Our approach also uses morphological processing, alb"
P13-2128,W05-1516,0,0.0197004,"sion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731–735, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics off from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of mor"
P13-2128,P13-1118,1,0.82069,"Missing"
P13-2128,N07-2052,0,0.0794613,"Missing"
P13-2128,faass-etal-2010-design,0,\N,Missing
P13-2128,J10-4007,1,\N,Missing
P13-2128,N03-1013,0,\N,Missing
P13-2128,2003.mtsummit-systems.9,0,\N,Missing
P13-2137,C12-2001,0,0.0354826,"Missing"
P13-2137,J10-4006,0,0.567318,"re (K¨ubler et al., 2009). Their obvious problem, of course, is that they require a large parsed corpus. In this paper, we describe the construction of a Distributional Memory for Croatian (D M .H R), a free word order language. To do so, we parse hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token Croatian web corpus. We evaluate D M .H R on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs. We report on the first structured distributional semantic model for Croatian, D M .H R. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available. 1 Introduction Most current work in lexical semantics is based on the Distributional Hypothesis (Harris, 1954), which posits a correlation between the degree of words’ semantic similarity and the similarity of the contexts in which they occur. Using this hypothesis, word meaning representations"
P13-2137,broda-etal-2008-corpus,0,0.0212408,"syntax-based (cooccurrence defined syntactically, syntactic objects as dimensions). Syntax-based models have several desirable properties. First, they are model to fine-grained types of semantic similarity such as predicate-argument plausibility (Erk et al., 2010). Second, they are more versatile – Baroni and Lenci (2010) have presented a generic framework, the Distributional Memory (DM), which is applicable 2 Related Work Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrˇz and Rychl´y, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007). Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubeˇsi´c et al., 2008; Jankovi´c et al., 2011) and synonym detection (Karan et al., 2012), however using only wordbased and not syntactic-based models. So far the only DM for a language other than English is the German D M .D E by Pad´o and Utt (2012), who describe the process of building D M .D E and the evaluation on a synonym choice task. Our work is similar, though each language has its own challenges. Croatian,"
P13-2137,1993.eamt-1.1,0,0.518212,"Missing"
P13-2137,P07-2053,0,0.033919,"Missing"
P13-2137,W06-2932,0,0.0123606,". SET IMES .H R consists of 90K tokens and 4K sentences, manually lemmatized and MSD-tagged according to Multext East v4 tagset (Erjavec, 2012), with the help of the Croatian Lemmatization Server (Tadi´c, 2005). It is used also as a basis for a novel formalism for syntactic annotation and dependency parsing of Croatian (Agi´c and Merkler, 2013). On the basis of previous evaluation for Croatian (Agi´c et al., 2008; Agi´c et al., 2009; Agi´c, 2012) and availability and licensing considerations, we chose HunPos tagger (Hal´acsy et al., 2007), CST lemmatizer (Ingason et al., 2008), and MSTParser (McDonald et al., 2006) to process hrWaC. We evaluated the tools on 100-sentence test sets from SET IMES .H R and Wikipedia; performance on Wikipedia should be indicative of the performance on a cross-domain dataset, such as hrWaC. In Table 1 we show lemmatization and tagging accuracy, as well as dependency parsing accuracy in terms of labeled attachment score (LAS). The results show that lemmatization, tagging and parsing accuracy improves on the state of the art for Croatian. The SET IMES .H R dependency parsing models are publicly available.3 Syntactic patterns. We collect the co-occurrence counts of tuples using"
P13-2137,D07-1060,0,0.239074,"Missing"
P13-2137,J10-4007,1,\N,Missing
P14-5008,W07-1401,1,0.865172,"esources for these languages (assuming that the EDAs themselves are largely language-independent). These are provided by the language-independent knowledge acquisition tools which we offer alongside the platform (cf. Section 3.2). EOP Evaluation Results for the three EDAs included in the EOP platform are reported in Table 1. Each line represents an EDA, the language and the dataset on which the EDA was evaluated. For brevity, we omit here the knowledge resources used for each EDA, even though knowledge configuration clearly affects performance. The evaluations were performed on RTE-3 dataset (Giampiccolo et al., 2007), where the goal is to maximize accuracy. We (manually) translated it to German and Italian for evaluations: in both cases the results fix a reference for the two languages. The two new datasets for German and English are available both as part of the EOP distribution and independently5 . The transformation-based EDA was also evaluated on RTE-6 dataset (Bentivogli et al., 2010), in which the goal is to maximize the F1 measure. The results of the included EDAs are higher than median values of participated systems in RTE-3, and they are competing with state-of-the-arts in RTE-6 results. To the b"
P14-5008,P98-2127,0,0.00988239,"otivates our offer of the EOP platform as a library. They also require a system that provides good quality at a reasonable efficiency as well as guidance as to the best choice of parameters. The latter point is realized through our results archive in the official EOP Wiki on the EOP site. Table 1: EDAs results for specific domains. Particularly, the EOP platform includes a language independent tool to build Wikipedia resources (Shnarch et al., 2009), as well as a language-independent framework for building distributional similarity resources like DIRT (Lin and Pantel, 2002) and Lin similarity(Lin, 1998). 3.3 Use Case 2: Textual Entailment Development. This category covers researchers who are interested in Recognizing Textual Entailment itself, for example with the goal of developing novel algorithms for detecting entailment. In contrast to the first category, this group need to look ”under the hood” of the EOP platform and access the source code of the EOP. For this reason, we have spent substantial effort to provide the code in a well-structured and well-documented form. A subclass of this group is formed by researchers who want to set up a RTE infrastructure for languages in which it does"
P14-5008,P09-1051,1,0.820811,"art of or all of the semantic processing, such as Question Answering or Intelligent Tutoring. Such users require a system that is as easy to deploy as possible, which motivates our offer of the EOP platform as a library. They also require a system that provides good quality at a reasonable efficiency as well as guidance as to the best choice of parameters. The latter point is realized through our results archive in the official EOP Wiki on the EOP site. Table 1: EDAs results for specific domains. Particularly, the EOP platform includes a language independent tool to build Wikipedia resources (Shnarch et al., 2009), as well as a language-independent framework for building distributional similarity resources like DIRT (Lin and Pantel, 2002) and Lin similarity(Lin, 1998). 3.3 Use Case 2: Textual Entailment Development. This category covers researchers who are interested in Recognizing Textual Entailment itself, for example with the goal of developing novel algorithms for detecting entailment. In contrast to the first category, this group need to look ”under the hood” of the EOP platform and access the source code of the EOP. For this reason, we have spent substantial effort to provide the code in a well-s"
P14-5008,P12-1030,1,0.845888,"raphrasing patterns at the predicate-argument level that cannot be captured by purely lexical rules. Formally, each syntactic rule consists of two dependency tree fragments plus a mapping from the variables of the LHS tree to the variables of the RHS tree.4 2.3 In the EOP we include a transformation based inference system that adopts the knowledge based transformations of Bar-Haim et al. (2007), while incorporating a probabilistic model to estimate transformation confidences. In addition, it includes a search algorithm which finds an optimal sequence of transformations for any given T/H pair (Stern et al., 2012). Edit distance EDA involves using algorithms casting textual entailment as the problem of mapping the whole content of T into the content of H. Mappings are performed as sequences of editing operations (i.e., insertion, deletion and substitution) on text portions needed to transform T into H, where each edit operation has a cost associated with it. The underlying intuition is that the probability of an entailment relation between T and H is related to the distance between them; see Kouylekov and Magnini (2005) for a comprehensive experimental study. Configuration Files The EC components can b"
P14-5008,P07-2045,0,\N,Missing
P14-5008,C98-2122,0,\N,Missing
P16-1164,W12-2513,0,0.214933,"y is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. 1 Introduction Quotations are occurrences of reported speech, thought, and writing in text. They play an important role in computational linguistics and digital humanities, providing evidence for, e.g., speaker relationships (Elson et al., 2010), inter-speaker sentiment (Nalisnick and Baird, 2013) or politeness (Faruqui and Pado, 2012). Due to a lack of generalpurpose automatic systems, such information is often obtained through manual annotation (e.g., Agarwal et al. (2012)), which is labor-intensive and costly. Thus, models for automatic quotation detection form a growing research area (e.g., Pouliquen et al. (2007); Pareti et al. (2013)). Quotation detection looks deceptively simple, but is challenging, as the following example shows: [The pipeline], the company said, [would be built by a proposed joint venture . . . , and Trunkline . . . will “build and operate” the system . . . ].1 1 Penn Attributions Relation Corpus (PARC), wsj 0260 Note that quotations can (i) be signalled by lexical cues (e.g., communication verbs) without quotation marks, (ii) contain mi"
P16-1164,W02-1001,0,0.728494,"ing In this paper, we compare our new model architectures to the state-of-the-art approach by Pareti (2015), an extension of Pareti et al. (2013). Their system is a pipeline: Its first component is the cue model, a token-level k-NN classifier applied to the syntactic heads of all verb groups. After cues are detected, content spans are localized using the content model, a linear-chain conditional random field (CRF) which makes use of the location of cues in the document through features. As their system is not publicly available, we reimplement it. Our cue classifier is an averaged perceptron (Collins, 2002) which we describe in more detail in the following section. It uses the 3 http://www.ims.uni-stuttgart.de/data/qsample PARC, wsj 2418 Table 1: Cue detection features for a token ti at position i, mostly derived from Pareti (2015) S1. Is a direct or indirect dependency parent of ti classified as a cue, in the cue list, or the phrase “according to”? S2. Was any token in a window of ±5 classified as a cue? S3. Distance to the previous and next cue S4. Does the sentence containing ti have a cue? S5. Conjunction of S4 and all features from C14 Pareti et al. (2013) distinguish three types of quotati"
P16-1164,P10-1015,0,0.151266,"joint decisions about the begin, end, and internal context of a quotation. We perform an extensive analysis with two new model architectures. We find that (a), simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. 1 Introduction Quotations are occurrences of reported speech, thought, and writing in text. They play an important role in computational linguistics and digital humanities, providing evidence for, e.g., speaker relationships (Elson et al., 2010), inter-speaker sentiment (Nalisnick and Baird, 2013) or politeness (Faruqui and Pado, 2012). Due to a lack of generalpurpose automatic systems, such information is often obtained through manual annotation (e.g., Agarwal et al. (2012)), which is labor-intensive and costly. Thus, models for automatic quotation detection form a growing research area (e.g., Pouliquen et al. (2007); Pareti et al. (2013)). Quotation detection looks deceptively simple, but is challenging, as the following example shows: [The pipeline], the company said, [would be built by a proposed joint venture . . . , and Trunkli"
P16-1164,E12-1064,1,0.806361,"extensive analysis with two new model architectures. We find that (a), simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. 1 Introduction Quotations are occurrences of reported speech, thought, and writing in text. They play an important role in computational linguistics and digital humanities, providing evidence for, e.g., speaker relationships (Elson et al., 2010), inter-speaker sentiment (Nalisnick and Baird, 2013) or politeness (Faruqui and Pado, 2012). Due to a lack of generalpurpose automatic systems, such information is often obtained through manual annotation (e.g., Agarwal et al. (2012)), which is labor-intensive and costly. Thus, models for automatic quotation detection form a growing research area (e.g., Pouliquen et al. (2007); Pareti et al. (2013)). Quotation detection looks deceptively simple, but is challenging, as the following example shows: [The pipeline], the company said, [would be built by a proposed joint venture . . . , and Trunkline . . . will “build and operate” the system . . . ].1 1 Penn Attributions Relation Corpus ("
P16-1164,P13-1166,0,0.0578368,"Missing"
P16-1164,P13-2147,1,0.90986,"Missing"
P16-1164,krestel-etal-2008-minding,0,0.215633,"Missing"
P16-1164,P14-5010,0,0.00345942,"(henceforth PARC3), by Pareti (2015).5 It contains AR annotations on the Wall Street Journal part of the Penn Treebank (2,294 5 Note that the data and thus the results differ from those previously published in (Pareti et al., 2013). news documents). As in related work, we use sections 1–22 as training set, section 23 as test set, and section 24 as development set. We perform the same preprocessing as Pareti: We use gold tokenization, lemmatization, part-of-speech tags, constituency parses, gold named entity annotations (Weischedel and Brunstein, 2005), and Stanford parser dependency analyses (Manning et al., 2014). Evaluation We report precision, recall, and micro-averaged F1 , adopting the two metrics introduced by Pareti et al. (2013): Strict match considers cases as correct where the boundaries of the spans match exactly. Partial match measures correctness as the ratio of overlap of the predicted and true spans. In both cases, we report numbers for each of the three quotation types (direct, indirect, mixed) and their micro averages. Like Pareti (2015), we exclude single-token content spans from the evaluation. To test for statistical significance of differences, we use the approximate randomization"
P16-1164,P13-2085,0,0.123561,"rnal context of a quotation. We perform an extensive analysis with two new model architectures. We find that (a), simple boundary classification combined with a greedy prediction strategy is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. 1 Introduction Quotations are occurrences of reported speech, thought, and writing in text. They play an important role in computational linguistics and digital humanities, providing evidence for, e.g., speaker relationships (Elson et al., 2010), inter-speaker sentiment (Nalisnick and Baird, 2013) or politeness (Faruqui and Pado, 2012). Due to a lack of generalpurpose automatic systems, such information is often obtained through manual annotation (e.g., Agarwal et al. (2012)), which is labor-intensive and costly. Thus, models for automatic quotation detection form a growing research area (e.g., Pouliquen et al. (2007); Pareti et al. (2013)). Quotation detection looks deceptively simple, but is challenging, as the following example shows: [The pipeline], the company said, [would be built by a proposed joint venture . . . , and Trunkline . . . will “build and operate” the system . . . ]."
P16-1164,D08-1004,0,0.0688316,"Missing"
P16-1164,N15-1005,0,0.0257517,"ale to our application, since the maximum length of a span factors into the prediction runtime, and quotations can be arbitrarily long. As an alternative, we propose a sampling-based approach: we draw candidate spans (proposals) from an informed, non-uniform distribution of spans. We score these spans to decide whether they should be added to the document (accepted) or not (rejected). This way, we efficiently traverse the space of potential span assignments while still being able to make informed decisions (cf. Wick et al. (2011)). To obtain a distribution over spans, we adapt the approach by Zhang et al. (2015). We introduce two independent probability distributions: Pb is the distribution of probabilities of a token being a begin token; Pe is the distribution of probabilities of a token being an end token. We sample a single content span proposal (D RAW P ROPOSAL) by first sampling the order in which the boundaries are to be determined (begin token or end token first) by sampling a binary variable d ∼ Bernoulli(0.5). If the begin token is to be sampled first, we continue by drawing a begin token tb ∼ Pb and finally draw an end token te ∼ Pe within a window of up to `max tokens to the right of tb ."
P16-1164,D13-1101,0,0.441509,"Missing"
P16-1164,D12-1122,0,0.0767057,"Missing"
P18-2020,benikova-etal-2014-nosta,0,0.0606931,"rporating subword information.3 Datasets For the evaluation, we use two established datasets for NER on contemporary German and two datasets for historical German. Contemporary German. The first large-scale German NER dataset was published as part of the CoNLL 2003 shared task (CoNLL, Tjong Kim Sang and De Meulder, 2003). It consists of about 220k tokens (for training) of annotated newspaper documents. The tagset handles locations (LOC), organizations (ORG), persons (PER) and the remaining entities as miscellaneous (MISC). The second dataset is the GermEval 2014 shared task dataset (GermEval, Benikova et al. (2014)), consisting of some 450k tokens (for training) of Wikipedia articles.4 This dataset has two levels of annotations: outer and inner span named entities. For example, the term Chicago Bulls is tagged as organization in the outer span annotation. The nested term Chicago is annotated as location in the inner span annotation. However, there are only few inner span annotations. In addition to the standard tagsets also used in the CoNLL dataset, fine grained versions of these entities are marked with suffixes: -deriv marks derivations of the named entities (e.g. German actor – German is a derived l"
P18-2020,Q17-1010,0,0.0138267,"on, gazetteer lists). BiLSTM-based Systems. Among the various deep learning architectures applied for NER, the best results have been achieved with bidirectional LSTM methods combined with a top-level CRF model (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017). In this work, we use an implementation that solely uses word and character embeddings. We train the character embeddings while training the model but use pre-trained word embeddings. To alleviate issues with out-of-vocabulary (OOV) words, we use both character- and subwordbased word embeddings computed with fastText (Bojanowski et al., 2017). This method is able to retrieve embeddings for unknown words by incorporating subword information.3 Datasets For the evaluation, we use two established datasets for NER on contemporary German and two datasets for historical German. Contemporary German. The first large-scale German NER dataset was published as part of the CoNLL 2003 shared task (CoNLL, Tjong Kim Sang and De Meulder, 2003). It consists of about 220k tokens (for training) of annotated newspaper documents. The tagset handles locations (LOC), organizations (ORG), persons (PER) and the remaining entities as miscellaneous (MISC). T"
P18-2020,I17-2016,0,0.0363688,"stingly, these benefits also extend to the historical datasets for which the CRF features were presumably not optimized: overall F1-scores are only a few points lower than for the contemporary corpora, and the CRFs significantly outperform the BiLSTM models on ONB and performs comparable on the larger LFT dataset. The type of embeddings used by BiLSTM plays a minor role for the historical corpora (for contemporary corpora, Wikipedia is clearly better). In sum, we conclude that BiLSTM models run into trouble when faced with very small training datasets, while CRF-based methods are more robust (Cotterell and Duh, 2017). 6 Experiment 3: Transfer Learning If the problems of BiLSTM from the last section are in fact due to lack of data, we might be able to obtain an improvement by combining them. A simple way of doing this is transfer learning (Lee et al., 2017): we simply start training on one corpus and at some point switch to another corpus. In our scenario, we start by training on large contemporary “source” corpora until convergence and then train additional 15 epochs on the “target” corpus from the domain on which we evaluate. The 7 Data Analysis Besides OCR errors, the lower F1 scores for the historic da"
P18-2020,P05-1045,0,0.174065,"Named entity recognition and classification (NER) is a central component in many natural language processing pipelines. High-quality NER is crucial for applications like information extraction, question answering, or entity linking. Since the goal of NER is to recognize instances of named entities in running text, it is established practice to treat NER as a “word-by-word sequence labeling task” (Jurafsky and Martin, 2009). There are two families of sequence models that constitute promising candidates. On the one hand, linearchain CRFs, which form the basis for many widely used systems (e.g., Finkel et al., 2005; Benikova et al., 2015), profit from hand-crafted features and can easily incorporate language- and domainspecific knowledge from dictionaries or gazetteers. On the other hand, bidirectional LSTMSs (BiLSTMs, e.g., Reimers and Gurevych, 2017) identify This paper investigates this question empirically on a set of German corpora including two large, contemporary corpora and two small historical corpora. We pit linear-chain CRF- and BiLSTM-based systems against each other and compare to state-ofthe-art models, performing three experiments. Due to these experiments, we get the following results: ("
P18-2020,I17-1042,0,0.0417425,"Missing"
P18-2020,S15-2046,0,0.0533452,"Missing"
P18-2020,N16-1030,0,0.619592,"2003). Benikova et al. (2015) developed G ERMA NER2 , another CRF-based NER system. It was optimized for the GermEval 2014 NER challenge and also uses a set of standard features (word and character n-grams, POS) supplemented by a number of specific information sources (unsupervised parts of speech (Biemann, 2009), distributional semantics and topic cluster information, gazetteer lists). BiLSTM-based Systems. Among the various deep learning architectures applied for NER, the best results have been achieved with bidirectional LSTM methods combined with a top-level CRF model (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017). In this work, we use an implementation that solely uses word and character embeddings. We train the character embeddings while training the model but use pre-trained word embeddings. To alleviate issues with out-of-vocabulary (OOV) words, we use both character- and subwordbased word embeddings computed with fastText (Bojanowski et al., 2017). This method is able to retrieve embeddings for unknown words by incorporating subword information.3 Datasets For the evaluation, we use two established datasets for NER on contemporary German and two datasets for historical"
P18-2020,W17-4421,0,0.0399093,"ry are not entirely clear. For example, the typo “sterreichischen Außenministerlum” (should be “Außenministerium”, Austrian foreign ministry) is manually annotated in the data but not detected by any of the models. However, “tschechoslowakischen Presse” (engl. Czechoslovakian press) is detected as organization by all classifiers but is not manually annotated. 8 Related Work BiLSTMs that combine neural network architectures with CRF-based superstructures yield the highest results on English NER datasets in a number of studies (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Lin et al., 2017). However, only few systems reported results for German NER, and restrict themselves to the “big-data” scenarios of the CoNLL 2003 (Lample et al., 2016; Reimers and Gurevych, 2017) and GermEval (Reimers et al., 2014; Christian Hnig, 2014) datasets.Sutton and McCallum (2005) showed the capability of CRFs for transfer learning by joint decoding two separately trained sequence models. Lee et al. (2017) apply transfer learning using a BiLSTM for medical NER using two similar tasks 123 BiLSTM-WikiEmb Train Transfer CoNLL CoNLL CoNLL CoNLL GermEval GermEval GermEval GermEval LFT ONB CoNLL LFT ONB 78"
P18-2020,P16-1101,0,0.446106,"ang and De Meulder, 2003). Benikova et al. (2015) developed G ERMA NER2 , another CRF-based NER system. It was optimized for the GermEval 2014 NER challenge and also uses a set of standard features (word and character n-grams, POS) supplemented by a number of specific information sources (unsupervised parts of speech (Biemann, 2009), distributional semantics and topic cluster information, gazetteer lists). BiLSTM-based Systems. Among the various deep learning architectures applied for NER, the best results have been achieved with bidirectional LSTM methods combined with a top-level CRF model (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017). In this work, we use an implementation that solely uses word and character embeddings. We train the character embeddings while training the model but use pre-trained word embeddings. To alleviate issues with out-of-vocabulary (OOV) words, we use both character- and subwordbased word embeddings computed with fastText (Bojanowski et al., 2017). This method is able to retrieve embeddings for unknown words by incorporating subword information.3 Datasets For the evaluation, we use two established datasets for NER on contemporary German and two dat"
P18-2020,L16-1689,0,0.0493291,"nd inner spans) in Section 4. As there are only few inner span annotations, we additionally report results based on the outer spans. To be more conform with the tagsets of the CoNLL task, we focus on outer spans and remove the fine-grained tags in the follow-up experiments (see Section 5 and 6). 1 http://stanford.io/2ohopn3 http://github.com/tudarmstadt-lt/ GermaNER 3 The source code and the best performing models are available online: http://www.ims.uni-stuttgart.de/ forschung/ressourcen/werkzeuge/german_ ner.html Historical German. We further consider two datasets based on historical texts (Neudecker, 2016)5 , extracted from the Europeana collection of historical newspapers6 , a standard resource for historical digital humanities. More specifically, our first corpus is the collection of Tyrolean periodicals and newspapers from the Dr Friedrich Temann Library (LFT), covering around 87k tokens from 2 121 4 https://sites.google.com/site/ germeval2014ner/ 5 https://github.com/KBNLresearch/ europeananp-ner/ 6 www.europeana.eu/portal/de Type Model CRF CRF RNN – RNN RNN StanfordNER GermaNER UKP ExB BiLSTM-WikiEmb BiLSTM-EuroEmb Pr R 80.02 81.31 79.54 78.07 81.95 75.50 62.29 68.00 71.10 74.75 78.13 70.7"
P18-2020,D17-1035,0,0.217555,"ince the goal of NER is to recognize instances of named entities in running text, it is established practice to treat NER as a “word-by-word sequence labeling task” (Jurafsky and Martin, 2009). There are two families of sequence models that constitute promising candidates. On the one hand, linearchain CRFs, which form the basis for many widely used systems (e.g., Finkel et al., 2005; Benikova et al., 2015), profit from hand-crafted features and can easily incorporate language- and domainspecific knowledge from dictionaries or gazetteers. On the other hand, bidirectional LSTMSs (BiLSTMs, e.g., Reimers and Gurevych, 2017) identify This paper investigates this question empirically on a set of German corpora including two large, contemporary corpora and two small historical corpora. We pit linear-chain CRF- and BiLSTM-based systems against each other and compare to state-ofthe-art models, performing three experiments. Due to these experiments, we get the following results: (a), the BiLSTM system indeed performs best on contemporary corpora, both within and across domains; (b), the BiLSTM system performs worse than the CRF systems for the smallest historical corpus due to lack of data; (c), by applying transfer l"
P18-2020,H05-1094,0,0.0608226,"hoslovakian press) is detected as organization by all classifiers but is not manually annotated. 8 Related Work BiLSTMs that combine neural network architectures with CRF-based superstructures yield the highest results on English NER datasets in a number of studies (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Lin et al., 2017). However, only few systems reported results for German NER, and restrict themselves to the “big-data” scenarios of the CoNLL 2003 (Lample et al., 2016; Reimers and Gurevych, 2017) and GermEval (Reimers et al., 2014; Christian Hnig, 2014) datasets.Sutton and McCallum (2005) showed the capability of CRFs for transfer learning by joint decoding two separately trained sequence models. Lee et al. (2017) apply transfer learning using a BiLSTM for medical NER using two similar tasks 123 BiLSTM-WikiEmb Train Transfer CoNLL CoNLL CoNLL CoNLL GermEval GermEval GermEval GermEval LFT ONB CoNLL LFT ONB 78.55 62.80 62.05 84.73† 67.77 72.15 GermEval 82.93 58.89 57.19 72.11 69.09 73.18 BiLSTM-EuroEmb LFT ONB CoNLL 55.28 72.90 59.43 54.21 74.33† 62.52 64.93 67.96 76.17 65.95 70.57 76.06 72.23 56.30 55.82 78.41 55.83 64.05 GermEval LFT ONB 75.78 51.25 49.14 63.42 57.71 64.20 51."
P19-1273,D17-1181,0,0.0280569,"Missing"
P19-1273,E14-1005,0,0.0138038,"e. However new actors can appear and take on importance at any time. Discourse context. Tasks 3 and 4 regularly involves coreference resolution: in the example, the expression the amendment can only be mapped to the correct claim if its content can be inferred. Similarly, actors realized as pronouns have to be resolved. Coreference resolution is still a difficult problem (Martschat and Strube, 2014). Dependencies among tasks. The various tasks are clearly not independent of one another, and joint models have been developed for a subset of the tasks, such as coreference and relation detection (Almeida et al., 2014) or entity and relation classification (Miwa and Sasaki, 2014; Adel and Sch¨utze, 2017; Bekoulis et al., 2018). However, state-of-the-art models still struggle with sentence complexity, and there are no comprehensive models of the complete task including aggregation. 2842 C1: C2: C3: C4: C5: C6: C7: C8: Steuerung von Migration (Controlling Migration) Aufenthalt (Residency) Integration (Integration) Innere Sicherheit (Domestic Security) Aussenpolitik (Foreign Policy) ¨ Okonomie, Arbeitsmarkt (Economy, Labor Market) Gesellschaft (Society) Verfahren (Procedures) Table 1: Migration: Main categorie"
P19-1273,W12-4102,0,0.0296046,"n explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliation networks from newspaper articles as introduced in Section 2 represents a task that combines binary relation extraction (Doddington et al., 2004; Hendrickx et al., 2010) with ontologization (Pennacchiotti and Pantel, 2006; Hachey et al., 2013, i.a.). The task can be decomposed conceptually as shown in Figure 2. From bottom to top, the first task is to identify claims and actors in the text (Tasks 1 and 2). Then, they need to be mapped onto entities"
P19-1273,P19-3018,1,0.595542,"Missing"
P19-1273,Q17-1010,0,0.0163882,"Missing"
P19-1273,doddington-etal-2004-automatic,0,0.0207698,"d their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliation networks from newspaper articles as introduced in Section 2 represents a task that combines binary relation extraction (Doddington et al., 2004; Hendrickx et al., 2010) with ontologization (Pennacchiotti and Pantel, 2006; Hachey et al., 2013, i.a.). The task can be decomposed conceptually as shown in Figure 2. From bottom to top, the first task is to identify claims and actors in the text (Tasks 1 and 2). Then, they need to be mapped onto entities that are represented in the affiliation Task 3: actor mapping support Category A13: delay Brexit Task 4: claim mapping Labour has said it will support the amendment Task 2: actor detection Task 1: claim detection Figure 2: Construction of affiliation network construction (top) from text (bo"
P19-1273,P10-1015,0,0.0392302,"tion 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliation networks from newspaper articles as introduced in Section 2 represents a task that combines binary relation extraction (Doddington et al., 2004; Hendrickx et al., 2010) with ontologization (Pennacchiotti and Pantel, 2006; Hachey et al., 2013, i.a.). The task can be decomposed conceptually as shown in Figure 2. From bottom to top, the first task is to identify claims and actors in the text (Tasks 1 and 2). Then, they need to be"
P19-1273,D18-1393,0,0.0139055,"Social Media Analysis using NLP – in particular sentiment analysis (e.g. Ceron et al. 2014), but also going into fine-grained analysis of groups of users/actors (Cesare et al., 2017). Nevertheless, most analyses in social media concern typically relatively broad categories, such as party preferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among"
P19-1273,N16-1180,0,0.022164,"Missing"
P19-1273,D14-1221,0,0.0164094,"n topic (Koopmans and Statham, 2010). We thus build on an expert-defined ontology of claims (cf. Section 5). With regard to actors, the issue is less clear: knowledge bases such as Wikidata cover many persons in the public eye. However new actors can appear and take on importance at any time. Discourse context. Tasks 3 and 4 regularly involves coreference resolution: in the example, the expression the amendment can only be mapped to the correct claim if its content can be inferred. Similarly, actors realized as pronouns have to be resolved. Coreference resolution is still a difficult problem (Martschat and Strube, 2014). Dependencies among tasks. The various tasks are clearly not independent of one another, and joint models have been developed for a subset of the tasks, such as coreference and relation detection (Almeida et al., 2014) or entity and relation classification (Miwa and Sasaki, 2014; Adel and Sch¨utze, 2017; Bekoulis et al., 2018). However, state-of-the-art models still struggle with sentence complexity, and there are no comprehensive models of the complete task including aggregation. 2842 C1: C2: C3: C4: C5: C6: C7: C8: Steuerung von Migration (Controlling Migration) Aufenthalt (Residency) Integ"
P19-1273,mcnamee-etal-2010-evaluation,0,0.0155496,"k 3: actor mapping support Category A13: delay Brexit Task 4: claim mapping Labour has said it will support the amendment Task 2: actor detection Task 1: claim detection Figure 2: Construction of affiliation network construction (top) from text (bottom) as relation extraction graph, that is, discourse referents for actors (Task 3: entity linking) and categories for claims (Task 4). Next, claims need to be attributed to actors and classified as support or opposition (Task 5). Finally, relations need to be aggregated across documents (Task 6). This setup is related to Knowledge Base Population (McNamee et al., 2010) and presents itself as a series of rather challenging tasks: Actor and claim ontologies. The actors and claims can either be known a priori (then Tasks 3 and 4 amount to classification) or can emerge from the data (then they become clustering tasks). We assume that there is a limited set of claims that structures public debates on a given topic (Koopmans and Statham, 2010). We thus build on an expert-defined ontology of claims (cf. Section 5). With regard to actors, the issue is less clear: knowledge bases such as Wikidata cover many persons in the public eye. However new actors can appear an"
P19-1273,W15-4631,0,0.0969719,"eferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational"
P19-1273,P15-1157,0,0.0223801,"nsiderable work in Social Media Analysis using NLP – in particular sentiment analysis (e.g. Ceron et al. 2014), but also going into fine-grained analysis of groups of users/actors (Cesare et al., 2017). Nevertheless, most analyses in social media concern typically relatively broad categories, such as party preferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics"
P19-1273,D17-1165,0,0.157902,"ask 5: claim attribution NLP and Political Science Our analytical goals have connecting points with a range of activities in NLP. There has been considerable work in Social Media Analysis using NLP – in particular sentiment analysis (e.g. Ceron et al. 2014), but also going into fine-grained analysis of groups of users/actors (Cesare et al., 2017). Nevertheless, most analyses in social media concern typically relatively broad categories, such as party preferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., clai"
P19-1273,D14-1200,0,0.0129076,"y time. Discourse context. Tasks 3 and 4 regularly involves coreference resolution: in the example, the expression the amendment can only be mapped to the correct claim if its content can be inferred. Similarly, actors realized as pronouns have to be resolved. Coreference resolution is still a difficult problem (Martschat and Strube, 2014). Dependencies among tasks. The various tasks are clearly not independent of one another, and joint models have been developed for a subset of the tasks, such as coreference and relation detection (Almeida et al., 2014) or entity and relation classification (Miwa and Sasaki, 2014; Adel and Sch¨utze, 2017; Bekoulis et al., 2018). However, state-of-the-art models still struggle with sentence complexity, and there are no comprehensive models of the complete task including aggregation. 2842 C1: C2: C3: C4: C5: C6: C7: C8: Steuerung von Migration (Controlling Migration) Aufenthalt (Residency) Integration (Integration) Innere Sicherheit (Domestic Security) Aussenpolitik (Foreign Policy) ¨ Okonomie, Arbeitsmarkt (Economy, Labor Market) Gesellschaft (Society) Verfahren (Procedures) Table 1: Migration: Main categories in claim ontology 5 Claim Ontology and Corpus Annotation We"
P19-1273,P06-1100,0,0.0116172,"ms that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliation networks from newspaper articles as introduced in Section 2 represents a task that combines binary relation extraction (Doddington et al., 2004; Hendrickx et al., 2010) with ontologization (Pennacchiotti and Pantel, 2006; Hachey et al., 2013, i.a.). The task can be decomposed conceptually as shown in Figure 2. From bottom to top, the first task is to identify claims and actors in the text (Tasks 1 and 2). Then, they need to be mapped onto entities that are represented in the affiliation Task 3: actor mapping support Category A13: delay Brexit Task 4: claim mapping Labour has said it will support the amendment Task 2: actor detection Task 1: claim detection Figure 2: Construction of affiliation network construction (top) from text (bottom) as relation extraction graph, that is, discourse referents for actors ("
P19-1273,D13-1010,0,0.0272451,"alytical goals have connecting points with a range of activities in NLP. There has been considerable work in Social Media Analysis using NLP – in particular sentiment analysis (e.g. Ceron et al. 2014), but also going into fine-grained analysis of groups of users/actors (Cesare et al., 2017). Nevertheless, most analyses in social media concern typically relatively broad categories, such as party preferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we f"
P19-1273,J17-3005,0,0.0562023,"t al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliat"
P19-1391,C16-1152,0,0.0506035,"Missing"
P19-1391,C18-1179,1,0.893782,"Missing"
P19-1391,W18-1808,0,0.0207177,"sses to previous studies (Bostan and Klinger, 2018). Modeling performance and inter-annotator disagreement are correlated: emotions that are difficult to annotate are also difficult to predict (Spearman’s ρ between F1 and the diagonal in Figure 1 is 0.85 for German, p = .01, and 0.75 for English, p = .05). It is notable that results for German are on a level with English despite the translation step and the shorter length of the German descriptions. That goes against our expectations, as previous studies showed that translation is only sentimentpreserving to some degree (Salameh et al., 2015; Lohar et al., 2018). We take this outcome as evidence for the cross-lingual comparability of deISEAR and enISEAR, and our general method. 5 Conclusion We presented (a) deISEAR, a corpus of 1001 event descriptions in German, annotated with seven emotion classes; and (b) enISEAR, a companion English resource build analogously, to disentangle effects of annotation setup and English when comparing to the original ISEAR resource. Our two-phase annotation setup shows that perceived emotions can be different from expressed emotions in such eventfocused corpus, which also affects classification performance. Emotions var"
P19-1391,W11-1514,0,0.0311947,"t which consists of two steps. In step 1, participants create descriptions of emotional events for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop. 1 Introduction Feeling emotions is a central part of the “human condition” (Russell, 1945). While existing studies on automatic recognition of emotions in text have achieved promising results (Pool and Nissim (2016); Mohammad (2011), i.a.), we see two main shortcomings. First, there is shortage of resources for non-English languages, with few exceptions, like Chinese (Li et al., 2017; Odbal and Wang, 2014; Yuan et al., 2002). This hampers the data-driven modeling of emotion recognition that has unfolded, e.g., for the related task of sentiment analysis. Second, emotions can be expressed in language with a wide variety of linguistic devices, from direct mentions (e.g., “I’m angry”) to evocative images (e.g.,“He was petrified”) or prosody. Computational emotion recognition on English has mostly focused on explicit emotion"
P19-1391,W14-6809,0,0.027761,"by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop. 1 Introduction Feeling emotions is a central part of the “human condition” (Russell, 1945). While existing studies on automatic recognition of emotions in text have achieved promising results (Pool and Nissim (2016); Mohammad (2011), i.a.), we see two main shortcomings. First, there is shortage of resources for non-English languages, with few exceptions, like Chinese (Li et al., 2017; Odbal and Wang, 2014; Yuan et al., 2002). This hampers the data-driven modeling of emotion recognition that has unfolded, e.g., for the related task of sentiment analysis. Second, emotions can be expressed in language with a wide variety of linguistic devices, from direct mentions (e.g., “I’m angry”) to evocative images (e.g.,“He was petrified”) or prosody. Computational emotion recognition on English has mostly focused on explicit emotion expressions. Often, however, emotions are merely inferable from world knowledge and experience. For instance, ”I finally found love” presumably depicts a joyful circumstance, w"
P19-1391,W16-4304,0,0.030813,"crowdsourcing experiment which consists of two steps. In step 1, participants create descriptions of emotional events for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop. 1 Introduction Feeling emotions is a central part of the “human condition” (Russell, 1945). While existing studies on automatic recognition of emotions in text have achieved promising results (Pool and Nissim (2016); Mohammad (2011), i.a.), we see two main shortcomings. First, there is shortage of resources for non-English languages, with few exceptions, like Chinese (Li et al., 2017; Odbal and Wang, 2014; Yuan et al., 2002). This hampers the data-driven modeling of emotion recognition that has unfolded, e.g., for the related task of sentiment analysis. Second, emotions can be expressed in language with a wide variety of linguistic devices, from direct mentions (e.g., “I’m angry”) to evocative images (e.g.,“He was petrified”) or prosody. Computational emotion recognition on English has mostly focused on"
P19-1391,W17-0801,0,0.153474,"nal dataset, all the reports were translated to English, and accordingly, the responses of, e.g., German speakers who took part in the survey are not available in their original language. In this paper, we follow Scherer and Wallbott (1997) by re-using their set of seven basic emotions and recreating part of their questionnaire both in English and German. In contrast to ISEAR, we account for the fact that a description can be related to different emotions by its writer and its readers. Affective analyses have rendered evidence that emotional standpoints affect the quality of annotation tasks (Buechel and Hahn, 2017). For instance, annotation results vary depending on whether workers are asked if a text is associated with an emotion and if it evokes an emotion, with the first phrasing downplaying the reader’s perspective and inducing higher inter-annotator agreement (Mohammad and Turney, 2013). We take notice of these findings to design our annotation guidelines. 3 Crowdsourcing-based Corpus Creation We developed a two-phase crowdsourcing experiment: one for generating descriptions, the other for rating the emotions of the descriptions. Phase 1 can be understood as sampling from P (description|emotion), o"
P19-1391,klinger-cimiano-2014-usage,1,0.803168,"/data/emotion, supports the development of emotion classification models in German and English including multilingual aspects. 2 Previous Work For the related but structurally simpler task of sentiment analysis, resources have been created in many languages. For German, this includes dictionaries 4005 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4005–4011 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (Ruppenhofer et al., 2017, i.a.), corpora of newspaper comments (Schabus et al., 2017) and reviews (Klinger and Cimiano, 2014; Ruppenhofer et al., 2014; Boland et al., 2013). Nevertheless, the resource situation leaves much to be desired. The situation is even more difficult for emotion analysis. Emotion annotation is slower and more subjective (Schuff et al., 2017). Further, there is less agreement on the set of classes to use, stemming from alternative psychological theories. These include, e.g., discrete classes vs. multiple continuous dimensions (Buechel and Hahn, 2016). Resources developed by one strand of research can be unusable for the other (Bostan and Klinger, 2018). In German, a few dictionaries have been"
P19-1391,W18-6206,1,0.878082,"Missing"
P19-1391,ruppenhofer-etal-2017-evaluating,0,0.0231514,"rences including a modelling experiment. Our corpus, available at https: //www.ims.uni-stuttgart.de/data/emotion, supports the development of emotion classification models in German and English including multilingual aspects. 2 Previous Work For the related but structurally simpler task of sentiment analysis, resources have been created in many languages. For German, this includes dictionaries 4005 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4005–4011 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (Ruppenhofer et al., 2017, i.a.), corpora of newspaper comments (Schabus et al., 2017) and reviews (Klinger and Cimiano, 2014; Ruppenhofer et al., 2014; Boland et al., 2013). Nevertheless, the resource situation leaves much to be desired. The situation is even more difficult for emotion analysis. Emotion annotation is slower and more subjective (Schuff et al., 2017). Further, there is less agreement on the set of classes to use, stemming from alternative psychological theories. These include, e.g., discrete classes vs. multiple continuous dimensions (Buechel and Hahn, 2016). Resources developed by one strand of resear"
P19-1391,N15-1078,0,0.0250616,"es between emotion classes to previous studies (Bostan and Klinger, 2018). Modeling performance and inter-annotator disagreement are correlated: emotions that are difficult to annotate are also difficult to predict (Spearman’s ρ between F1 and the diagonal in Figure 1 is 0.85 for German, p = .01, and 0.75 for English, p = .05). It is notable that results for German are on a level with English despite the translation step and the shorter length of the German descriptions. That goes against our expectations, as previous studies showed that translation is only sentimentpreserving to some degree (Salameh et al., 2015; Lohar et al., 2018). We take this outcome as evidence for the cross-lingual comparability of deISEAR and enISEAR, and our general method. 5 Conclusion We presented (a) deISEAR, a corpus of 1001 event descriptions in German, annotated with seven emotion classes; and (b) enISEAR, a companion English resource build analogously, to disentangle effects of annotation setup and English when comparing to the original ISEAR resource. Our two-phase annotation setup shows that perceived emotions can be different from expressed emotions in such eventfocused corpus, which also affects classification perf"
P19-1391,W17-5203,1,0.763055,"Missing"
P19-3018,W17-2208,1,0.833045,"as well as to related annotation tasks (see Section 5). The four dashed boxes with labels in italics show the major tasks involved, each of which comes with a number of desiderata. Document Selection. We assume that annotation is performed on the basis of a potentially large overall corpus where full annotation of all documents is not feasible or desired. Thus, the first task is the selection of relevant documents for annotation. This is essentially an information retrieval task, where keyword-based approaches face the typical prob1 Specifically, e-Identity (Blessing et al., 2015) and CRETA (Blessing et al., 2017). lem of resulting in either high recall–low precision scenarios (too few keywords) or low recall–high precision scenarios (too many keywords). Annotation. Since projects typically involve several annotators, the environment should not just support annotation proper, but also user administration (user management, task assignment). Assuming that we annotate relations between actors and their claims, the annotation links markables to external knowledge bases, specifically the actors to a database of persons and other relevant entities (parties, companies, geopolitical entities etc.) and the clai"
P19-3018,P19-1273,1,0.595542,"Missing"
Q14-1020,P98-1013,0,0.0661572,"latedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They use a relation-based pruning scheme that is somewhat comparable to our backtranslation filtering. To our knowledge, the most similar work to ours is (Mohammad et al., 2007), which also considers DSMs, albeit a different variety, namely conceptbased DSMs where targets are characterized in terms of their distribution over categories of Roget’s thesaurus. Like our work, their study creates crosslin255 gual DSMs for German using a translation lexicon. It follows a different strategy, however: it collects cooccurrence counts from a German corpus and tra"
Q14-1020,J10-4006,0,0.357879,"tional Hypothesis (Harris, 1954; Miller and Charles, 1991), which states that words occurring in similar contexts are similar in meaning, distributional semantic models (DSMs) represent a word’s meaning via its occurrence in context in large corpora. Vector spaces, the most widely used type of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many languages. In addition,"
Q14-1020,C10-1011,0,0.0273296,"easure the correlation between the semantic similarities and human judgments for word pairs. Coverage is calculated as the percentage of items with similarity greater 0. Differences between models are tested for significance using bootstrap resampling (Efron and Tibshirani, 1993), always in the “All” condition. 5.3 Models We consider three types of DM models (monolingual, crosslingual and multilingual), bag-of-words models and a set of models proposed in the literature. Monolingual model. We use DM. DE (Padó and Utt, 2012), constructed from a 900M-token web corpus, SD E WAC, parsed with MATE (Bohnet, 2010).6 As discussed in Section 3.5, we consider all links (AllL) for the monolingual model. Crosslingual models. The starting point for the crosslingual models is Baroni and Lenci (2010)’s English TypeDM model extracted from approximately 3B tokens of Wikipedia and web corpus text parsed with MaltParser (Nivre, 2006).7 DM. XL naive implements Eq. (1), and DM. XL filter implements Eq. (3). As our translation lexicon, we use the communitybuilt English–German dict.cc online dictionary.8 5 We note that since the data are not normally distributed, a non-parametric correlation coefficient would be more"
Q14-1020,W06-2920,0,0.03142,"e is a bottleneck in many languages regarding both large, clean corpora as well as processing pipelines that result in high-accuracy dependency parses. To address this problem, we propose to induce Distributional Memories for such languages crosslingually by translating a source language DM into the target language. By adopting English as the source language we can take advantage of the resource gradient, that is, the higher maturity of English NLP techniques, such as parsers, compared to most other languages. For many languages, treebanks have become available only within the last ten years (Buchholz and Marsi, 2006), if at all, while English has been at the forefront of NLP development for several decades, and a number of highly accurate dependency parsers exist (McDonald et al., 2005; Nivre, 2006). At the same time, English arguably possesses the widest range of large and well-cleaned corpora of any language. To make our approaches applicable to as many target languages as possible, we assume in this section that very few resources for the target language are available. The crosslingual methods we develop 247 HainN copseN GehölzN forestN WaldN timberN woodN HolzN Figure 2: Sample of the English-German d"
Q14-1020,P02-1033,0,0.0322838,"en English and other languages, the crosslingual induction of linguistic information has been an active topic of research. Many studies use parallel corpora. Annotation projection (Yarowsky and Ngai, 2001) transfers source language annotation directly onto target language 12 We cannot provide significances for the BOW results because we again do not have per-item predictions. sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studie"
Q14-1020,1993.eamt-1.1,0,0.26435,"the case of zero similarities. For Exp. 1, we report the accuracy (the number of correctly recognized synonyms divided by the number of attempted problems) and coverage (the ratio of items attempted; always 1 for the “All” condition). Items are considered covered if at least one candidate has a non-zero similarity to the target. In Exp. 2, we measure the correlation between the semantic similarities and human judgments for word pairs. Coverage is calculated as the percentage of items with similarity greater 0. Differences between models are tested for significance using bootstrap resampling (Efron and Tibshirani, 1993), always in the “All” condition. 5.3 Models We consider three types of DM models (monolingual, crosslingual and multilingual), bag-of-words models and a set of models proposed in the literature. Monolingual model. We use DM. DE (Padó and Utt, 2012), constructed from a 900M-token web corpus, SD E WAC, parsed with MATE (Bohnet, 2010).6 As discussed in Section 3.5, we consider all links (AllL) for the monolingual model. Crosslingual models. The starting point for the crosslingual models is Baroni and Lenci (2010)’s English TypeDM model extracted from approximately 3B tokens of Wikipedia and web c"
Q14-1020,C04-1134,0,0.0355121,"cture. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They use a relation-based pruning scheme that is somewhat comparable to our backtranslation filtering. To our knowledge, the most similar work to ours is (Mohammad et al., 2007), which also considers DSMs, albeit a different variety, namely conceptbased DSMs where targets are characterized in terms of their distribution over categories of Roget’s thesaurus. Like our work, their study creates crosslin255 gual DSMs for German using a translation lexicon. It follows a different strategy, however: it collects coo"
Q14-1020,J02-3001,0,0.0418964,"Missing"
Q14-1020,H05-1107,0,0.0336891,"onaries, matching previous results from the literature (Peirsman and Padó, 2011). 8 Related Work Given the resource gradient between English and other languages, the crosslingual induction of linguistic information has been an active topic of research. Many studies use parallel corpora. Annotation projection (Yarowsky and Ngai, 2001) transfers source language annotation directly onto target language 12 We cannot provide significances for the BOW results because we again do not have per-item predictions. sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classif"
Q14-1020,C12-1089,0,0.0604714,"s such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly ric"
Q14-1020,P13-1117,0,0.0194533,"e per-item predictions. sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional"
Q14-1020,P98-2127,0,0.421659,"he Distributional Hypothesis (Harris, 1954; Miller and Charles, 1991), which states that words occurring in similar contexts are similar in meaning, distributional semantic models (DSMs) represent a word’s meaning via its occurrence in context in large corpora. Vector spaces, the most widely used type of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many"
Q14-1020,H05-1066,0,0.124728,"Missing"
Q14-1020,P02-1027,0,0.0310357,"butional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Ci"
Q14-1020,D07-1060,0,0.0962687,"translation pairs without translation probabilities, as shown in Figure 2. Translation lexicons of this type are arguably the most common bilingual resource and accurate ones exist for virtually any language pair (Soderland et al., 2009), even for languages with few available corpora. Furthermore, such translation lexicons are often crowdsourced and are available for download. For example, the website dict.cc provides numerous such lexicons for German and English. This approach promises in particular to yield models with a quality-coverage profile complementary to that of monolingual models (Mohammad et al., 2007; Peirsman and Padó, 2011): Crosslingual DMs are extracted from source language corpora which we assume to be parsed more accurately than target language corpora. In addition, the translation process can be designed to act as a further filtering step (cf. Section 3.4 below), thus optimizing crosslingual models for higher quality at the expense of coverage. In contrast, monolingual models – in particular for under-resourced languages – often hit a quality ceiling, but can generally guarantee high coverage. 3.2 Translating DMs with Translation Lexicons We conceptualize DM as a directed graph (se"
Q14-1020,P10-1023,0,0.0373472,"quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They u"
Q14-1020,P06-2111,0,0.0444709,"Missing"
Q14-1020,P99-1067,0,0.0137635,"11) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They use a relation-based pruning scheme that is somewhat comparable to our backtranslation filtering. To our knowledge, the most similar work to ours is (Mohammad et al., 2007), which also considers DSMs, albeit a different variety, namely conceptbased DSMs where targets are characterized in terms of their distribution o"
Q14-1020,J06-2001,0,0.036712,"e of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many languages. In addition, syntax-based DSMs are inherently more sparse than word spaces, which calls for a large corpus of well parsable data. It is thus not surprising that besides English (Baroni and Lenci, 2010), only few other languages possess large-scale syntax-based DSMs (Padó and Utt, 2012; Šnajd"
Q14-1020,P09-1030,0,0.0301236,"or the target language are available. The crosslingual methods we develop 247 HainN copseN GehölzN forestN WaldN timberN woodN HolzN Figure 2: Sample of the English-German dict.cc dictionary; translations shown as dashed lines. here work without any target language corpora, either monolingual or bilingual. The only knowledge we use is a simple translation lexicon, that is, a list of translation pairs without translation probabilities, as shown in Figure 2. Translation lexicons of this type are arguably the most common bilingual resource and accurate ones exist for virtually any language pair (Soderland et al., 2009), even for languages with few available corpora. Furthermore, such translation lexicons are often crowdsourced and are available for download. For example, the website dict.cc provides numerous such lexicons for German and English. This approach promises in particular to yield models with a quality-coverage profile complementary to that of monolingual models (Mohammad et al., 2007; Peirsman and Padó, 2011): Crosslingual DMs are extracted from source language corpora which we assume to be parsed more accurately than target language corpora. In addition, the translation process can be designed t"
Q14-1020,U05-1019,0,0.0126431,"forest sense). The right-hand side shows (part of) the German translations according to Eq. (1): both Holz (timber) and Wald (forest) are linked to both adjectives, leading to spurious edges in the German DM. 3.4 Filtering by Backtranslation Since the nature of the translation is not indicated in the translation lexicon, we exploit typical redundancies in the source DM, which often contains “quasi-synonymous” edges that express the same relation with different words, e.g., hbook obj readi and hnovel obj readi. This allows us to score target edge candidates by how well we can “backtranslate” (Somers, 2005) them into the source language. This idea is illustrated in Figure 4. We still assume, as above, that wood has two translations, but that precut has only one. For the English edge hprecut mod woodi, we obtain two German candidate edges, namely hzugeschnitten mod Holzi and hzugeschnitten mod Waldi. When backtranslating these candidates, the first one, hzugeschnitten mod Waldi, maps only onto the original edge. The second one, hzugeschnitten mod Holzi, is backtranslated into a different source edge, hprecut mod timberi, which makes it more probable. forestN mod timberN WaldN woodN HolzN mod prec"
Q14-1020,P13-2137,1,0.887498,"Missing"
Q14-1020,P12-1068,0,0.0285916,"ults because we again do not have per-item predictions. sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable"
Q14-1020,J06-3003,0,0.0884864,"s in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many languages. In addition, syntax-based DSMs are inherently more sparse than word spaces, which calls for a large corpus of well parsable data. It is thus not surprising that besides English (Baroni and Lenci, 2010), only few other languages possess large-scale syntax-based DSMs (Padó and Utt, 2012; Šnajder et al., 2013). This paper develops"
Q14-1020,E12-1046,0,0.0606594,"Missing"
Q14-1020,N01-1026,0,0.0845646,"taset from machine-readable dictionaries. Overall, the results for Croatian are encouraging. They demonstrate that languages where parsing technology is still developing can in particular profit from cross- and multilingual methods. This is true even for relatively small translation dictionaries, matching previous results from the literature (Peirsman and Padó, 2011). 8 Related Work Given the resource gradient between English and other languages, the crosslingual induction of linguistic information has been an active topic of research. Many studies use parallel corpora. Annotation projection (Yarowsky and Ngai, 2001) transfers source language annotation directly onto target language 12 We cannot provide significances for the BOW results because we again do not have per-item predictions. sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsu"
Q14-1020,N07-2052,0,0.544355,". We build a standard BOW model from the same German corpus SD E WAC used for DM. DE. We assume a window of 10 context words to the left and right. We use the top 10K most frequent content words (nouns, adjectives, verbs and adverbs) as dimensions. Our second BOW model (BOW PCA500 ) was reduced to 500 dimensions by applying principle component analysis, a technique generally used to increase robustness to parameter choice and to combat sparsity.9 Models from the literature. We compare our models against the state of the art, represented by the respective best models from two previous studies (Zesch et al., 2007; Mohammad et al., 2007). They comprise monolingual ontology-based models that use GermaNet, (German) Wikipedia, or both (LinGN , 9 We also built models using smaller context windows and Latent Semantic Analysis (LSA, Landauer, 1997), both with 500 dimensions and with an automatically optimized number of dimensions (Wild et al., 2008). Since these spaces did not consistently yield better results than the reported models using PCA, we do not report the results in detail. All Covered Acc Acc Cov Model Baselines and word-based DSMs 1 Random .25 .25 1 .31 .31 1 2 Frequency 3 BOW .46 .46 .98 .55 .5"
Q14-1020,J10-4007,1,\N,Missing
Q14-1020,C98-1013,0,\N,Missing
Q14-1020,J10-1001,0,\N,Missing
Q14-1020,C98-2122,0,\N,Missing
Q14-1020,P13-2017,0,\N,Missing
R19-1103,W12-2513,0,0.0150578,"b) shows reasonable performance even when using solely word forms, which makes it applicable for non-standard (i.e., historical) corpora. 1 (1) Introduction Quotation is a general notion that covers different kinds of direct and indirect speech, thought, and writing in text (Semino and Short, 2004). Quotations are a prominent linguistic device used to express claims, assessments, or attitudes attributed to speakers. Consequently, the analysis of quotations is gaining traction in computational linguistics and digital humanities, providing evidence for speaker relationships (Elson et al., 2010; Agarwal et al., 2012), inter-speaker sentiment (Nalisnick and Baird, 2013), politeness (Faruqui and Pado, 2012), and narrative structure (Jannidis et al., 2018). As is often the case with semantic phenomena, manual annotation of quotations has shown to be slow and resource-intensive, in particular when undertaken in conjunction with the annotation of speakers and information quality (Brunner, 2013; Pareti, 2015). This provides the rationale for automatic quotation recognition methods. After a first round of rule-based methods (Pouliquen et al., 2007; Brunner, 2013), recent supervised models Hillary Clinton on Satu"
R19-1103,E14-1005,0,0.0607346,"Missing"
R19-1103,P10-1015,0,0.0140775,"d feature sets and (b) shows reasonable performance even when using solely word forms, which makes it applicable for non-standard (i.e., historical) corpora. 1 (1) Introduction Quotation is a general notion that covers different kinds of direct and indirect speech, thought, and writing in text (Semino and Short, 2004). Quotations are a prominent linguistic device used to express claims, assessments, or attitudes attributed to speakers. Consequently, the analysis of quotations is gaining traction in computational linguistics and digital humanities, providing evidence for speaker relationships (Elson et al., 2010; Agarwal et al., 2012), inter-speaker sentiment (Nalisnick and Baird, 2013), politeness (Faruqui and Pado, 2012), and narrative structure (Jannidis et al., 2018). As is often the case with semantic phenomena, manual annotation of quotations has shown to be slow and resource-intensive, in particular when undertaken in conjunction with the annotation of speakers and information quality (Brunner, 2013; Pareti, 2015). This provides the rationale for automatic quotation recognition methods. After a first round of rule-based methods (Pouliquen et al., 2007; Brunner, 2013), recent supervised models"
R19-1103,E12-1064,1,0.8281,"able for non-standard (i.e., historical) corpora. 1 (1) Introduction Quotation is a general notion that covers different kinds of direct and indirect speech, thought, and writing in text (Semino and Short, 2004). Quotations are a prominent linguistic device used to express claims, assessments, or attitudes attributed to speakers. Consequently, the analysis of quotations is gaining traction in computational linguistics and digital humanities, providing evidence for speaker relationships (Elson et al., 2010; Agarwal et al., 2012), inter-speaker sentiment (Nalisnick and Baird, 2013), politeness (Faruqui and Pado, 2012), and narrative structure (Jannidis et al., 2018). As is often the case with semantic phenomena, manual annotation of quotations has shown to be slow and resource-intensive, in particular when undertaken in conjunction with the annotation of speakers and information quality (Brunner, 2013; Pareti, 2015). This provides the rationale for automatic quotation recognition methods. After a first round of rule-based methods (Pouliquen et al., 2007; Brunner, 2013), recent supervised models Hillary Clinton on Saturday [acknowledged]CUE [the state of the economy is good]QUOTE . This assumption is genera"
R19-1103,D17-1206,0,0.014164,"t for the original corpus, models are not transferable to new domains and analysis schemes. In other words, it leads to serious fragmentation. In this paper, we develop and evaluate a corpusagnostic neural model architecture for automatic quotation recognition that makes as few assumptions as possible about the corpus to be modelled but is still expressive enough to deal with the challenge of recognizing quotation spans of essentially arbitrary length (Scheible et al., 2016). In this respect, we see our study as a step towards transfer learning (Pan and Yang, 2009) and task-agnostic learning (Hashimoto et al., 2017). We find that our model can perform reasonably well across corpora differing in genre, language, and structure. 2 (based on the surface form) between direct quotations (starting and ending with quotation marks), indirect quotations (without any quotation marks), and mixed quotations (everything else). Pareti model. Pareti (2015), an extension of Pareti et al. (2013), presents a pipeline architecture for quotation annotation. It first applies a k-NN classifier to identify quotation cues within the corpus. Then, a linear-chain conditional random field (CRF) is used to identify quotation spans i"
R19-1103,P18-1027,0,0.0270365,"quotations. The second model (Brunner ML) is a simple classification model based on random forests. 3 Neural Quotation Detection (NQD) We now define a neural architecture, NQD, with the goal of modeling the quotations in all three corpora described in Section 2. We design our model to leverage the commonalities across datasets, while not depending on the features of any dataset in particular. As all datasets involve long quotation spans with long-distance dependencies, an LSTMbased approach was natural, given such models’ ability to capture very long-distance dependencies of up to 200 tokens (Khandelwal et al., 2018). Conversely, given the structural differences between corpora, we decided against a pipeline model like those employed by Pareti (2015) and Scheible et al. (2016) which predict cues first and then quotation spans. NQD predicts quotations directly without explicitly identifying cues. NQD frames quotation prediction as token classification, classifying each token as either beginning a quotation (BEGIN), ending a quotation (END), or neither (NEITHER). Quotation spans then consist of all tokens starting with a BEGIN tag, up to (but not including) the next END or BEGIN tag, or the end of sequence."
R19-1103,N16-1030,0,0.0228077,"anguage. While the model does not reach the state of the art on any particular corpus, it performs close to it on all of them. Notably, the model is also able to deal relatively graciously with the absence of linguistic information. We will release an implementation with pre-trained models. As NQD makes independent predictions for each token, it cannot model correlations and mutual exclusions between labels, and there is no guarantee for well-formed output class sequences. We investigated a number of extensions, including linearchain CRF layers that are effective for Named Entity Recognition (Lample et al., 2016), but did not obtain improvements. We believe this is due to the unbounded length of quotation spans which is challenging for CRFs (Scheible et al., 2016). The overall greatest challenge that NQD faces is data scarcity — all existing corpora with manual annotation are small, and our results show consistently bad performance for infrequent quotation types. In this situation, transfer learning seems like a natural proposition, and our model makes it possible for the first time to apply straightforward transfer learning to quotation annotation. In future work, we will explore this direction. He r"
R19-1103,P13-2085,0,0.015682,"solely word forms, which makes it applicable for non-standard (i.e., historical) corpora. 1 (1) Introduction Quotation is a general notion that covers different kinds of direct and indirect speech, thought, and writing in text (Semino and Short, 2004). Quotations are a prominent linguistic device used to express claims, assessments, or attitudes attributed to speakers. Consequently, the analysis of quotations is gaining traction in computational linguistics and digital humanities, providing evidence for speaker relationships (Elson et al., 2010; Agarwal et al., 2012), inter-speaker sentiment (Nalisnick and Baird, 2013), politeness (Faruqui and Pado, 2012), and narrative structure (Jannidis et al., 2018). As is often the case with semantic phenomena, manual annotation of quotations has shown to be slow and resource-intensive, in particular when undertaken in conjunction with the annotation of speakers and information quality (Brunner, 2013; Pareti, 2015). This provides the rationale for automatic quotation recognition methods. After a first round of rule-based methods (Pouliquen et al., 2007; Brunner, 2013), recent supervised models Hillary Clinton on Saturday [acknowledged]CUE [the state of the economy is g"
R19-1103,D13-1101,0,0.079286,"Missing"
R19-1103,D14-1162,0,0.08186,"Missing"
R19-1103,P16-1164,1,0.843673,"Missing"
R19-1137,D13-1160,0,0.114068,"Missing"
R19-1137,P10-1133,0,0.0265903,"015; Gupta et al., 2017). These methods, however, overwhelmingly concentrate on categorical attributes, that is, attributes whose values are themselves entities in the knowledge graph. As an example, consider the attribute capital that maps a country onto a city which is itself an entity (Mexico – Mexico City, UK – London). A prominent approach to the prediction of categorical attributes is as an operation in embedding space, which explains the popularity of embedding-based approaches for this task. Much fewer studies has considered the prediction of numerical attributes of entities in CCKBs (Davidov and Rappoport, 2010; Gupta et al., 2015), Collaboratively constructed knowledge bases play an important role in information systems, but are essentially always incomplete. Thus, a large number of models has been developed for Knowledge Base Completion, the task of predicting new attributes of entities given partial descriptions of these entities. Virtually all of these models either concentrate on numeric attributes (&lt;Italy,GDP,2T$&gt;) or they concentrate on categorical attributes (&lt;Tim Cook,chairman,Apple&gt;). In this paper, we propose a simple feedforward neural architecture to jointly predict numeric and categori"
R19-1137,N13-1095,0,0.0143121,"sity of Stuttgart, Germany Abstract et al., 2009). Their collaborative construction importantly enables them to avoid the scaling problems encountered by expert-constructed knowledge bases. Thus, CCKBs have come to play an important role in information systems, forming the basis for a wide range of natural language processing applications (Hovy et al., 2013) such as question answering (Berant et al., 2013; Krishnamurthy and Mitchell, 2015) or representation learning for entities (Toutanova et al., 2015; Yaghoobzadeh et al., 2018). The most crucial shortcoming of CCKBs is their incompleteness (Min et al., 2013; West et al., 2014) – not just with respect to the entities that they cover, but also with respect to the attributes present for entities that are nominally covered. This is not surprising: When a contributor to a knowledge base adds an entity, they will probably concentrate on the most salient attributes (e.g., for a scientist, field or affiliation), while other attributes (such as parents or place of birth) may be added later or never. This realization has led to a large boost to work in the area of knowledge base completion, that is, the prediction of attributes of entities that are curren"
R19-1137,D15-1002,1,0.766765,"Missing"
R19-1137,D15-1174,0,0.379265,"ttributes of Entities in Knowledge Bases V Thejas BITS Pilani, India Abhijeet Gupta and Sebastian Pad´o IMS, University of Stuttgart, Germany Abstract et al., 2009). Their collaborative construction importantly enables them to avoid the scaling problems encountered by expert-constructed knowledge bases. Thus, CCKBs have come to play an important role in information systems, forming the basis for a wide range of natural language processing applications (Hovy et al., 2013) such as question answering (Berant et al., 2013; Krishnamurthy and Mitchell, 2015) or representation learning for entities (Toutanova et al., 2015; Yaghoobzadeh et al., 2018). The most crucial shortcoming of CCKBs is their incompleteness (Min et al., 2013; West et al., 2014) – not just with respect to the entities that they cover, but also with respect to the attributes present for entities that are nominally covered. This is not surprising: When a contributor to a knowledge base adds an entity, they will probably concentrate on the most salient attributes (e.g., for a scientist, field or affiliation), while other attributes (such as parents or place of birth) may be added later or never. This realization has led to a large boost to wor"
R19-1137,S17-1012,1,0.858955,"Missing"
R19-1137,D15-1038,0,0.141672,"with respect to the attributes present for entities that are nominally covered. This is not surprising: When a contributor to a knowledge base adds an entity, they will probably concentrate on the most salient attributes (e.g., for a scientist, field or affiliation), while other attributes (such as parents or place of birth) may be added later or never. This realization has led to a large boost to work in the area of knowledge base completion, that is, the prediction of attributes of entities that are currently missing from the CCKB (Bordes et al., 2013; Socher et al., 2013; Min et al., 2013; Guu et al., 2015; Gupta et al., 2017). These methods, however, overwhelmingly concentrate on categorical attributes, that is, attributes whose values are themselves entities in the knowledge graph. As an example, consider the attribute capital that maps a country onto a city which is itself an entity (Mexico – Mexico City, UK – London). A prominent approach to the prediction of categorical attributes is as an operation in embedding space, which explains the popularity of embedding-based approaches for this task. Much fewer studies has considered the prediction of numerical attributes of entities in CCKBs (Dav"
R19-1137,Q15-1019,0,0.0231611,"g, while this is not true for categorical attributes. 2 Predicting Numeric and Categorical Attributes from Text The majority of methods to predict attributes for entities in CCKBs are based on techniques from representation learning. Specifically, they us distributed representations (i.e., vectors, also called embeddings) to represent the entities, and sometimes also the attributes. Embeddings can be built from different sources, such as the knowledge bases themselves (Bordes et al., 2013; Guu et al., 2015; Lin et al., 2015), from text corpora that mention these entities (Socher et al., 2013; Krishnamurthy and Mitchell, 2015), or from both (Toutanova et al., 2015; Yaghoobzadeh et al., 2018). In this paper, we use two prediction models that build on embeddings that were built from text corpora, following the widely successful assumption that text corpora implicitly contain a large amount of world knowledge that can be extracted by observing the contexts in which words are used (the so-called distributional hypothesis) (Firth, 1957; Miller and Charles, 1991; Turney and Pantel, 2010; Mikolov et al., 2013). The formulation of attribute prediction on top of precomputed embeddings enables us to use rather simple supervi"
S10-1006,W09-1401,0,0.0287746,"for each semantic relation. Here, we describe the general guidelines, which delineate the scope of the data to be collected and state general principles relevant to the annotation of all relations.1 Our objective is to annotate instances of semantic relations which are true in the sense of holding in the most plausible truth-conditional interpretation of the sentence. This is in the tradition of the Textual Entailment or Information Validation paradigm (Dagan et al., 2009), and in contrast to “aboutness” annotation such as semantic roles (Carreras and M`arquez, 2004) or the BioNLP 2009 task (Kim et al., 2009) where negated relations are also labelled as positive. Similarly, we exclude instances of semantic relations which hold only in speculative or counterfactural scenarios. In practice, this means disallowing annotations within the scope of modals or negations, e.g., “Smoking may/may not have caused cancer in this case.” We accept as relation arguments only noun phrases with common-noun heads. This distinguishes our task from much work in Information Extraction, which tends to focus on specific classes of named entities and on considerably more finegrained relations than we do. Named entities ar"
S10-1006,W05-0620,0,\N,Missing
S12-1023,W06-2911,0,0.0195401,"d senses. We cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (Gale et al., 1992). A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular po"
S12-1023,J07-4004,0,0.0367893,"ntifying which lemmas of a given set instantiate a specific meta alternation. We let the model rank the lemmas through the score function (cf. Table (1) and Eq. (5)) and evaluate the ranked list using Average Precision. While an alternative would be to rank meta alternations for a given polysemous lemma, the method chosen here has the benefit of providing data on the performance of individual meta senses and meta alternations. 4.1 Data All modeling and data extraction was carried out on the written part of the British National Corpus (BNC; Burnage and Dunlop (1992)) parsed with the C&C tools (Clark and Curran, 2007). 6 For the evaluation, we focus on disemous words, words which instantiate exactly two meta senses according to WordNet. For each meta alternation (m, m0 ), we evaluate CAM on a set of disemous targets (lemmas that instantiate (m, m0 )) and disemous distractors (lemmas that do not). We define three types of distractors: (1) distractors sharing m with the targets (but not m0 ), (2) distractors sharing m0 with the targets (but not m), and (3) distractors sharing neither. In this way, we ensure that CAM cannot obtain good results by merely modeling the similarity of targets to either m or m0 , w"
S12-1023,P05-1004,0,0.0302659,"ty, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are interested in relations within words, namely between word senses. We cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (Gale et al., 1992). A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et a"
S12-1023,J10-4007,1,0.925057,"Missing"
S12-1023,H92-1045,0,0.231181,"ograph:camera). The framework defined in Section 2 conceptualizes our task in a way parallel to that of analogical reasoning, modeling not “first-order” semantic similarity, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are interested in relations within words, namely between word senses. We cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (Gale et al., 1992). A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also b"
S12-1023,P90-1034,0,0.617696,"been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular polysemy for adjectives. 7 Conclusions and Future Work We have argued that modeling regular polysemy and other analogical processes will help improve current models of word meaning in empirical computational semantics. We have presented a formal framewor"
S12-1023,E09-1045,0,0.0527305,"Missing"
S12-1023,P03-1009,0,0.0709775,"in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular polysemy for adjectives. 7 Conclusions and Future Work We have argued that modeling regular polysemy and other analogical processes will help improve current models of word meaning in empirical computational semantics. We have presented a formal framework to represent and operate with regular sense alternations, as well as a first simple instantiation of the framework. We have conducted an evaluation of different implementations of this model in the new task of determining whether words match a given sense alternation. All models signifi"
S12-1023,P99-1004,0,0.154155,"fore averaging. This gives equal weight to the each lemma or meta sense, respectively. Macro-averaging in repA thus assumes that senses are equally distributed, which is an oversimplification, as word senses are known to present skewed distributions (McCarthy et al., 2004) and vectors for words with a predominant sense will be similar to the dominant meta sense vector. Micro-averaging partially models sense skewedness under the assumption that word frequency correlates with sense frequency. Similarity measure. As the vector similarity measure in Eq. (5), we use the standard cosine similarity (Lee, 1999). It ranges between −1 and 1, with 1 denoting maximum similarity. In the current model where the vectors do not contain negative counts, the range is [0; 1]. 5 Results Effect of Parameters The four parameters of Section 4.3 (three space types, macro-/micro-averaging for repM and repA , and log-likelihood transformation) correspond to 24 instantiations of CAM. Figure 1 shows the influence of the four parameters. The only significant difference is tied to the use of lexicalized vector spaces (gramlex / lex are better than gram). The statistical significance of this difference was verified by a t"
S12-1023,W04-0837,0,0.028326,"erage, that is, a simple average over all instances. For repM and repA , there 155 is a design choice: The centroid can be computed by micro-averaging as well, which assigns a larger weight to more frequent lemmas (repM ) or meta senses (repA ). Alternatively, it can be computed by macro-averaging, that is, by normalizing the individual vectors before averaging. This gives equal weight to the each lemma or meta sense, respectively. Macro-averaging in repA thus assumes that senses are equally distributed, which is an oversimplification, as word senses are known to present skewed distributions (McCarthy et al., 2004) and vectors for words with a predominant sense will be similar to the dominant meta sense vector. Micro-averaging partially models sense skewedness under the assumption that word frequency correlates with sense frequency. Similarity measure. As the vector similarity measure in Eq. (5), we use the standard cosine similarity (Lee, 1999). It ranges between −1 and 1, with 1 denoting maximum similarity. In the current model where the vectors do not contain negative counts, the range is [0; 1]. 5 Results Effect of Parameters The four parameters of Section 4.3 (three space types, macro-/micro-averag"
S12-1023,J01-3003,0,0.089857,"previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular polysemy for adjectives. 7 Conclusions and Future Work We have argued that modeling regular polysemy and other analogical processes will help improve current models of word meaning in empirical computational semantics. We have presented a formal framework to represent and operate"
S12-1023,J07-2002,1,0.846005,"Missing"
S12-1023,P93-1024,0,0.797098,"Missing"
S12-1023,N10-1013,0,0.0762424,"of CAM is that it avoids word sense disambiguation, although it still relies on a predefined sense inventory (WordNet, through CoreLex). Our use of monosemous words to represent meta senses and meta alternations goes beyond previous work which uses monosemous words to disambiguate polysemous words in context (Izquierdo et al., 2009; Navigli and Velardi, 2005). Because of its focus on avoiding disambiguation, CAM simplifies the representation of meta alternations and polysemous words to single centroid vectors. In the future, we plan to induce word senses (Sch¨utze, 1998; Pantel and Lin, 2002; Reisinger and Mooney, 2010), which will allow for more flexible and realistic models. abs act agt anm art atr cel chm com con A BSTRACTION ACT AGENT A NIMAL A RTIFACT ATTRIBUTE C ELL C HEMICAL C OMMUNICATION C ONSEQUENCE ent evt fod frm grb grp grs hum lfr lme E NTITY E VENT F OOD F ORM B IOLOG . G ROUP G ROUPING S OCIAL G ROUP H UMAN L IVING B EING L INEAR M EASURE loc log mea mic nat phm pho plt pos pro L OCATION G EO . L OCATION M EASURE M ICROORGANISM NATURAL B ODY P HENOMENON P HYSICAL O BJECT P LANT P OSSESSION P ROCESS prt psy qud qui rel spc sta sub tme pro PART P SYCHOL . F EATURE D EFINITE Q UANTITY I NDEFINIT"
S12-1023,P99-1014,0,0.219419,"thing that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular polysemy for adjectives. 7 Conclusions and Future Work We have argued that modeling regular polysemy and other analogical processes will help improve current models of word meaning in empirical computational semantics. We have presented a formal framework to represent and operate with regular sense alternations, as well as a first simple instantiation of the framework. We have conducted an evaluation of different implementations of this model in the new task of determining whether words match a given sense alterna"
S12-1023,J06-2001,0,0.204394,"r more parameters to set. Definition of vector space. We instantiate the vecI function in three ways. All three are based on dependency-parsed spaces, following our intuition that topical similarity as provided by window-based spaces is insufficient for this task. The functions differ in the definition of the space’s dimensions, incorporating different assumptions about distributional differences among meta alternations. The first option, gram, uses grammatical paths of lengths 1 to 3 as dimensions and thus characterizes lemmas and meta senses in terms of their grammatical context (Schulte im Walde, 2006), with a total of 2,528 paths. The second option, lex, uses words as dimensions, treating the dependency parse as a co-occurrence filter (Pad´o and Lapata, 2007), and captures topical distinctions. The third option, gramlex, uses lexicalized dependency paths like obj–see to mirror more fine-grained semantic properties (Grefenstette, 1994). Both lex and gramlex use the 10,000 most frequent items in the corpus. Vector elements. We use “raw” corpus cooccurrence frequencies as well as log-likelihoodtransformed counts (Lowe, 2001) as elements of the co-occurrence vectors. Definition of centroid com"
S12-1023,J98-1004,0,0.778335,"Missing"
S12-1023,P08-1078,0,0.0728258,"Missing"
S12-1023,N01-1010,0,0.0363629,"5 0.10 0.15 0.20 0.25 coherence Figure 2: Average Precision and Coherence (κ) for each meta alternation. Correlation: r = 0.743 (p &lt; 0.001) 6 Related work As noted in Section 1, there is little work in empirical computational semantics on explicitly modeling sense alternations, although the notions that we have formalized here affect several tasks across NLP subfields. Most work on regular sense alternations has focused on regular polysemy. A pioneering study is Buitelaar (1998), who accounts for regular polysemy through the CoreLex resource (cf. Section 3). A similar effort is carried out by Tomuro (2001), but he represents regular polysemy at the level of senses. Recently, Utt and Pad´o (2011) explore the differences between between idiosyncratic and regular polysemy patterns building on CoreLex. Lapata (2000) focuses on the default meaning arising from word combinations, as opposed to the polysemy of single words as in this study. Meta alternations other than regular polysemy, such as metonymy, play a crucial role in Information Extraction. For instance, the meta alternation S OCIAL G ROUP -G EOGRAPHICAL L OCATION corresponds to an ambiguity between the L OCATIONO RGANIZATION Named Entity cl"
S12-1023,D11-1063,0,0.0447519,"CoreLex. Lapata (2000) focuses on the default meaning arising from word combinations, as opposed to the polysemy of single words as in this study. Meta alternations other than regular polysemy, such as metonymy, play a crucial role in Information Extraction. For instance, the meta alternation S OCIAL G ROUP -G EOGRAPHICAL L OCATION corresponds to an ambiguity between the L OCATIONO RGANIZATION Named Entity classes which is known to be a hard problem in Named Entity Recognition and Classification (Markert and Nissim, 2009). Metaphorical meta alternations have also received attention recently (Turney et al., 2011) On a structural level, the prediction of meta alternations shows a clear correspondence to analogy prediction as approached in Turney (2006) (carpenter:wood is analogous to mason:stone, but not to photograph:camera). The framework defined in Section 2 conceptualizes our task in a way parallel to that of analogical reasoning, modeling not “first-order” semantic similarity, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are"
S12-1023,J06-3003,0,0.0254949,"Meta alternations other than regular polysemy, such as metonymy, play a crucial role in Information Extraction. For instance, the meta alternation S OCIAL G ROUP -G EOGRAPHICAL L OCATION corresponds to an ambiguity between the L OCATIONO RGANIZATION Named Entity classes which is known to be a hard problem in Named Entity Recognition and Classification (Markert and Nissim, 2009). Metaphorical meta alternations have also received attention recently (Turney et al., 2011) On a structural level, the prediction of meta alternations shows a clear correspondence to analogy prediction as approached in Turney (2006) (carpenter:wood is analogous to mason:stone, but not to photograph:camera). The framework defined in Section 2 conceptualizes our task in a way parallel to that of analogical reasoning, modeling not “first-order” semantic similarity, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are interested in relations within words, namely between word senses. We cannot expect two different senses of the same noun to co-occur in the"
S12-1023,W11-0128,1,0.8629,"Missing"
S12-1023,C92-2070,0,0.492306,"emantic similarity, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are interested in relations within words, namely between word senses. We cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (Gale et al., 1992). A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 200"
S12-1023,J12-3005,1,\N,Missing
S15-1005,W05-0620,0,0.174968,"Missing"
S15-1005,S10-1059,0,0.398869,"The “full task” includes 42 identification of all (explicit or implicit) semantic roles of the target predicate. The “null instantiation task” is the subtask of the full task concerned only with the identification and labeling of antecedents for implicit roles. It assumes that predicates and overt roles are already available. We follow the lead of almost all models for implicit SRL on the null instantiation task. Structurally, it can be approached similarly to role identification in traditional SRL. The first systems on large-coverage implicit SRL adopted traditional SRL modeling techniques (Chen et al., 2010; Tonelli and Delmonte, 2010). but struggled with the scarcity of training data for the complex task. Work since then has concentrated on tapping into novel knowledge and data sources. There are three main directions. The first one is knowledge about semantic types. This includes Ruppenhofer et al. (2011) who extract semantic types for null instantiations from FrameNet and Laparra and Rigau (2012) who learn distributions over semantic types for each role from explicit role annotations in FrameNet. Similarly, Roth and Frank (2013) retrieve overt instances of implicit roles from comparable corpo"
S15-1005,W10-0907,0,0.0259508,"methods. When we combine the SemEval-2010 Task 10 and Gerber and Chai noun corpora, we obtain substantially improved performance on both corpora, for all roles and parts of speech. We also present new insights into the properties of the implicit semantic role labeling task. 1 Well, sir, it’s [A4 this lonely, silent house] and the queer thing in the kitchen . ... I thought [A1 it] had come again. Introduction Semantic role labeling (SRL) is the task of identifying semantic arguments of predicates in text. It is an important step in text analysis and has applications in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007; Moreda et al., 2011) and machine translation (Wu and Fung, 2009; Xiong et al., 2012) . A large body of work exists on algorithms for SRL (Gildea and Jurafsky, 2002; Srikumar and Roth, 2011). Their success is closely connected to the availability of two large, hand-constructed semantic role resources, FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005). They used to concentrate on overt semantic Implicit SRL is useful to complete predicates’ argument structures for inference (Mirkin et al., 2010) and paraphrasing (Roth and Frank, 2013"
S15-1005,P97-1003,0,0.106072,"mental scenarios (cf. Table 3): (1) The standard “in-domain” setup that only uses S EM E VAL, as assumed by most studies on the dataset. (2) A pure “out-of-domain” setup where we use only G ERBER C HAI as training data. Of course, there is reason to believe that this strategy will perform quite poorly. (3) A simple “concatenation” setup where we train on the union of G ERBER C HAI and the S EM E VAL training corpus. (4) The feature augmentation setting where we train on the combined corpus, but apply Daum´e’s (2007) learning method. Preprocessing. S EM E VAL comes pre-parsed with the Collins (Collins, 1997) parser. We parsed G ER BER C HAI with the same parser, ignoring the Penn Treebank gold trees. Since all datasets are manually annotated with semantic roles, no overt SRL is necessary. Coreference information, which we require for some features, is available from manual annotation in the S EM E VAL test part, but not for the other datasets. We computed coreference chains with the Stanford CoreNLP tools (Manning et al., 2014). Evaluation. We evaluate implicit role predictions with precision, recall, and F1 score, following the official SemEval 2010 Task 10 guidelines. Note that Training Set Pr."
S15-1005,J14-1002,0,0.0585019,"f implicit SRL. Feizabadi & Pad´o (2014) investigated the use of crowdsourcing to create annotations for implicit roles. Both corpora are more restricted in size and scope than the first two. 2.3 Models for Semantic Role Labeling Traditional SRL. A broad range of models have been proposed for “traditional”, i.e., local SRL (Palmer et al., 2010). The task can be seen as a sequence of two classification tasks, predicate disambiguation and role labeling. Earlier models modeled them in a pipeline architecture, but recent works demonstrates the benefits of joint inference (Srikumar and Roth, 2011; Das et al., 2014). SRL models have drawn on a wide variety of features from two main groups: syntactic features describing the structural relation between predicate and argument candidate, and semantic features describing role and candidate. A general observation is that SRL models are lexically specific to a substantial degree, i.e., do not generalize very well between predicates, so that the availability of annotations remains a bottleneck. Implicit SRL was formulated by SemEval 2010 Task 10 in two versions. The “full task” includes 42 identification of all (explicit or implicit) semantic roles of the target"
S15-1005,P07-1033,0,0.272131,"Missing"
S15-1005,1993.eamt-1.1,0,0.560201,"Missing"
S15-1005,E14-4044,1,0.889064,"Missing"
S15-1005,N09-1068,0,0.0363803,"Missing"
S15-1005,J12-4003,0,0.567526,"notated predicates (verbs vs. nouns). As a result, they are generally regarded as incompatible, and previous work has concentrated on getting most out of individual corpora, or spending annotation effort on focused extensions of these corpora. Instead, we will follow the intuition that the performance of implicit SRL can be improved significantly by combining corpora, using simple domain adaptation techniques to bridge the differences between them. We combine the two largest datasets for implicit SRL, the SemEval-2010 Task 10 dataset (Ruppenhofer et al., 2010) and the Gerber and Chai dataset (Gerber and Chai, 2012). This combination achieves improvements across all target and semantic roles despite the differences in genre, domain, and parts of speech. Our analyses indicates that the properties of the implicit SRL task – where syntactic features play a relatively minor role compared to semantic and discourse features – are responsible for this picture, and mean that models can actually profit from complementarity between combined corpora. Plan of the paper. Section 2 summarizes the resource and model situation in SRL. Section 3 defines a simple system for implicit SRL that uses domain adaptation. Sectio"
S15-1005,J02-3001,0,0.0775743,"ts into the properties of the implicit semantic role labeling task. 1 Well, sir, it’s [A4 this lonely, silent house] and the queer thing in the kitchen . ... I thought [A1 it] had come again. Introduction Semantic role labeling (SRL) is the task of identifying semantic arguments of predicates in text. It is an important step in text analysis and has applications in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007; Moreda et al., 2011) and machine translation (Wu and Fung, 2009; Xiong et al., 2012) . A large body of work exists on algorithms for SRL (Gildea and Jurafsky, 2002; Srikumar and Roth, 2011). Their success is closely connected to the availability of two large, hand-constructed semantic role resources, FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005). They used to concentrate on overt semantic Implicit SRL is useful to complete predicates’ argument structures for inference (Mirkin et al., 2010) and paraphrasing (Roth and Frank, 2013), or to assess the coherence of discourse (Burchardt et al., 2005). It however requires (even) more training data than traditional SRL. One reason is that potential arguments come from the whole text rather"
S15-1005,W13-0111,0,0.75925,"the complex task. Work since then has concentrated on tapping into novel knowledge and data sources. There are three main directions. The first one is knowledge about semantic types. This includes Ruppenhofer et al. (2011) who extract semantic types for null instantiations from FrameNet and Laparra and Rigau (2012) who learn distributions over semantic types for each role from explicit role annotations in FrameNet. Similarly, Roth and Frank (2013) retrieve overt instances of implicit roles from comparable corpora. The second direction is discourse level knowledge. Laparra and Rigau (2013) and Gorinski et al. (2013) treat implicit SRL as a task similar to anaphor resolution, which motivates the use of several features of discourse such as distance and salience. A third set of studies concentrated on simply obtaining more annotated instances. Silberer and Frank (2012) use an entity-based coreference resolution model to automatically extended the training set. Moor et al. (2013) and Feizabadi and Pad´o (2014) manually construct focused corpora (cf. Section 2.2). 3 3.1 Combining Corpora for Implicit SRL Rationale and Challenges Despite the progress made by on implicit SRL, as discussed in the previous secti"
S15-1005,N06-2015,0,0.0294921,"The main challenge in this endeavour is (1), Determining a set of implicit roles that should be that these corpora have very different properties (cf. Table 1). Consequently, a number of challenges arise identified in context; (2) Determining the antecedents for data combination. Below we discuss them, our of these missing roles. For the first step, we extract the predominant role set (i.e., most frequently realexpectations, and our strategies to address them. ized set) for each predicate by searching the predicate Challenge: Differences in Role Framework. S E - in a large corpus, OntoNotes (Hovy et al., 2006). We assume that all instances of the predicate realize these M E VAL was annotated with FrameNet roles, while G ERBER C HAI was annotated with PropBank roles. roles and select the subset that is not realized overtly While semi-automatic conversion schemes now exist for inclusion in the second step. We phrase the second step as binary classification. in both directions, we decided to adopt the PropThe items to be classified are triples htarget prediBank paradigm, working on the basis of the semicate, implicit role, candidate realizationi. The set of automatically converted S EM E VAL annotatio"
S15-1005,W13-0114,0,0.0804465,"carcity of training data for the complex task. Work since then has concentrated on tapping into novel knowledge and data sources. There are three main directions. The first one is knowledge about semantic types. This includes Ruppenhofer et al. (2011) who extract semantic types for null instantiations from FrameNet and Laparra and Rigau (2012) who learn distributions over semantic types for each role from explicit role annotations in FrameNet. Similarly, Roth and Frank (2013) retrieve overt instances of implicit roles from comparable corpora. The second direction is discourse level knowledge. Laparra and Rigau (2013) and Gorinski et al. (2013) treat implicit SRL as a task similar to anaphor resolution, which motivates the use of several features of discourse such as distance and salience. A third set of studies concentrated on simply obtaining more annotated instances. Silberer and Frank (2012) use an entity-based coreference resolution model to automatically extended the training set. Moor et al. (2013) and Feizabadi and Pad´o (2014) manually construct focused corpora (cf. Section 2.2). 3 3.1 Combining Corpora for Implicit SRL Rationale and Challenges Despite the progress made by on implicit SRL, as disc"
S15-1005,D09-1133,0,0.0520571,"Missing"
S15-1005,P14-5010,0,0.00623897,"Missing"
S15-1005,W04-2705,0,0.258179,", 2005). It defines a set of general semantic roles named ARG0-ARG5 of which ARG0 and ARG1 are interpreted as proto-agent and proto-patient (Dowty, 1991), respectively. The higher-numbered roles receive more predicate-specific interpretations. These “core” roles are complemented by adjunct roles such as MNR (manner) or TMP (time). For example, Jim Unruh ... said [A1 he] is approaching [A2 next year] [MNR with caution]. PropBank has annotated the WSJ part of the Penn Treebank, i.e., newswire text, exhaustively with semantic roles. While it originally concentrated on verbs, the NomBank project (Meyers et al., 2004) extended the annotation scheme to nouns. PropBank does not have a specific taxonomy of null instantiations like FrameNet, but it can nevertheless be used equally for implicit role annotation. 2.2 Annotated Corpora for Implicit SRL FrameNet and PropBank are both very large corpora, covering tens of thousands of instances. Corpora with implicit role annotation are generally much smaller; the main corpora are summarized in Table 1. Ruppenhofer et al. Arguably the first corpus with a substantial set of annotations for implicit roles was Corpus Ruppenhofer et al. (2010) Gerber & Chai (2012) Moor e"
S15-1005,P10-1123,1,0.899781,"Missing"
S15-1005,W13-0211,0,0.301183,"2004) extended the annotation scheme to nouns. PropBank does not have a specific taxonomy of null instantiations like FrameNet, but it can nevertheless be used equally for implicit role annotation. 2.2 Annotated Corpora for Implicit SRL FrameNet and PropBank are both very large corpora, covering tens of thousands of instances. Corpora with implicit role annotation are generally much smaller; the main corpora are summarized in Table 1. Ruppenhofer et al. Arguably the first corpus with a substantial set of annotations for implicit roles was Corpus Ruppenhofer et al. (2010) Gerber & Chai (2012) Moor et al. (2013) Feizabadi & Pad´o (2014) Scheme FrameNet PropBank FrameNet FrameNet POS V, N N V V Genre Novels Newswire Newswire Novels # predicates 801 10 5 10 # instances 1575 1253 1992 384 # implicit roles 245 1172 242 363 Table 1: Size of available English corpora with implicit semantic role annotation created for SemEval 2010 Task 10 (Ruppenhofer et al., 2010). This dataset covers a number of chapters from Arthur Conan Doyle short stories and provides full-text annotation of both explicit and implicit semantic roles. The texts were annotated manually with FrameNet roles. This dataset is a de-facto stan"
S15-1005,C10-2107,0,0.0185314,"2010) and paraphrasing (Roth and Frank, 2013), or to assess the coherence of discourse (Burchardt et al., 2005). It however requires (even) more training data than traditional SRL. One reason is that potential arguments come from the whole text rather than just the sentence. Another one is that most of the powerful syntactic features that are a staple in traditional SRL are unavailable across sentence boundaries. Unfortunately, existing corpora for implicit SRL are quite small: The task requires full-text annotation, which is time-consuming and pushes semantic role frameworks to their limits (Palmer and Sporleder, 2010). It is also hard to do consistently, and can only be crowdsourced in limited settings (Feizabadi and Pad´o, 2014). Thus, even though multiple systems for implicit SRL exist (among others, Tonelli and Delmonte (2011), Laparra and Rigau (2012), Silberer 40 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 40–50, Denver, Colorado, June 4–5, 2015. and Frank (2012)), results are still relatively poor. In this paper, we focus on the fact that the corpora that exist for implicit SRL differ not only in the semantic role frameworks used (FrameNet vs."
S15-1005,J05-1004,0,0.867291,"ic role labeling (SRL) is the task of identifying semantic arguments of predicates in text. It is an important step in text analysis and has applications in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007; Moreda et al., 2011) and machine translation (Wu and Fung, 2009; Xiong et al., 2012) . A large body of work exists on algorithms for SRL (Gildea and Jurafsky, 2002; Srikumar and Roth, 2011). Their success is closely connected to the availability of two large, hand-constructed semantic role resources, FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005). They used to concentrate on overt semantic Implicit SRL is useful to complete predicates’ argument structures for inference (Mirkin et al., 2010) and paraphrasing (Roth and Frank, 2013), or to assess the coherence of discourse (Burchardt et al., 2005). It however requires (even) more training data than traditional SRL. One reason is that potential arguments come from the whole text rather than just the sentence. Another one is that most of the powerful syntactic features that are a staple in traditional SRL are unavailable across sentence boundaries. Unfortunately, existing corpora for impli"
S15-1005,S13-1043,0,0.555627,"tensen et al., 2010), question answering (Shen and Lapata, 2007; Moreda et al., 2011) and machine translation (Wu and Fung, 2009; Xiong et al., 2012) . A large body of work exists on algorithms for SRL (Gildea and Jurafsky, 2002; Srikumar and Roth, 2011). Their success is closely connected to the availability of two large, hand-constructed semantic role resources, FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005). They used to concentrate on overt semantic Implicit SRL is useful to complete predicates’ argument structures for inference (Mirkin et al., 2010) and paraphrasing (Roth and Frank, 2013), or to assess the coherence of discourse (Burchardt et al., 2005). It however requires (even) more training data than traditional SRL. One reason is that potential arguments come from the whole text rather than just the sentence. Another one is that most of the powerful syntactic features that are a staple in traditional SRL are unavailable across sentence boundaries. Unfortunately, existing corpora for implicit SRL are quite small: The task requires full-text annotation, which is time-consuming and pushes semantic role frameworks to their limits (Palmer and Sporleder, 2010). It is also hard"
S15-1005,S10-1008,0,0.382082,"but also in genre (newswire vs. novels), and classes of annotated predicates (verbs vs. nouns). As a result, they are generally regarded as incompatible, and previous work has concentrated on getting most out of individual corpora, or spending annotation effort on focused extensions of these corpora. Instead, we will follow the intuition that the performance of implicit SRL can be improved significantly by combining corpora, using simple domain adaptation techniques to bridge the differences between them. We combine the two largest datasets for implicit SRL, the SemEval-2010 Task 10 dataset (Ruppenhofer et al., 2010) and the Gerber and Chai dataset (Gerber and Chai, 2012). This combination achieves improvements across all target and semantic roles despite the differences in genre, domain, and parts of speech. Our analyses indicates that the properties of the implicit SRL task – where syntactic features play a relatively minor role compared to semantic and discourse features – are responsible for this picture, and mean that models can actually profit from complementarity between combined corpora. Plan of the paper. Section 2 summarizes the resource and model situation in SRL. Section 3 defines a simple sys"
S15-1005,R11-1046,0,0.392239,"t roles are already available. We follow the lead of almost all models for implicit SRL on the null instantiation task. Structurally, it can be approached similarly to role identification in traditional SRL. The first systems on large-coverage implicit SRL adopted traditional SRL modeling techniques (Chen et al., 2010; Tonelli and Delmonte, 2010). but struggled with the scarcity of training data for the complex task. Work since then has concentrated on tapping into novel knowledge and data sources. There are three main directions. The first one is knowledge about semantic types. This includes Ruppenhofer et al. (2011) who extract semantic types for null instantiations from FrameNet and Laparra and Rigau (2012) who learn distributions over semantic types for each role from explicit role annotations in FrameNet. Similarly, Roth and Frank (2013) retrieve overt instances of implicit roles from comparable corpora. The second direction is discourse level knowledge. Laparra and Rigau (2013) and Gorinski et al. (2013) treat implicit SRL as a task similar to anaphor resolution, which motivates the use of several features of discourse such as distance and salience. A third set of studies concentrated on simply obtai"
S15-1005,D07-1002,0,0.077153,"10 and Gerber and Chai noun corpora, we obtain substantially improved performance on both corpora, for all roles and parts of speech. We also present new insights into the properties of the implicit semantic role labeling task. 1 Well, sir, it’s [A4 this lonely, silent house] and the queer thing in the kitchen . ... I thought [A1 it] had come again. Introduction Semantic role labeling (SRL) is the task of identifying semantic arguments of predicates in text. It is an important step in text analysis and has applications in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007; Moreda et al., 2011) and machine translation (Wu and Fung, 2009; Xiong et al., 2012) . A large body of work exists on algorithms for SRL (Gildea and Jurafsky, 2002; Srikumar and Roth, 2011). Their success is closely connected to the availability of two large, hand-constructed semantic role resources, FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005). They used to concentrate on overt semantic Implicit SRL is useful to complete predicates’ argument structures for inference (Mirkin et al., 2010) and paraphrasing (Roth and Frank, 2013), or to assess the coherence of discourse"
S15-1005,S12-1001,0,0.469713,"null instantiations from FrameNet and Laparra and Rigau (2012) who learn distributions over semantic types for each role from explicit role annotations in FrameNet. Similarly, Roth and Frank (2013) retrieve overt instances of implicit roles from comparable corpora. The second direction is discourse level knowledge. Laparra and Rigau (2013) and Gorinski et al. (2013) treat implicit SRL as a task similar to anaphor resolution, which motivates the use of several features of discourse such as distance and salience. A third set of studies concentrated on simply obtaining more annotated instances. Silberer and Frank (2012) use an entity-based coreference resolution model to automatically extended the training set. Moor et al. (2013) and Feizabadi and Pad´o (2014) manually construct focused corpora (cf. Section 2.2). 3 3.1 Combining Corpora for Implicit SRL Rationale and Challenges Despite the progress made by on implicit SRL, as discussed in the previous section, data sparsity remains the main bottleneck. This has two main reasons. First, the set of constitutents included in the search Challenge: Differences in Genre/Domain. Also, for each role is very large, potentially including the S EM E VAL is based on nov"
S15-1005,D11-1012,0,0.143497,"he implicit semantic role labeling task. 1 Well, sir, it’s [A4 this lonely, silent house] and the queer thing in the kitchen . ... I thought [A1 it] had come again. Introduction Semantic role labeling (SRL) is the task of identifying semantic arguments of predicates in text. It is an important step in text analysis and has applications in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007; Moreda et al., 2011) and machine translation (Wu and Fung, 2009; Xiong et al., 2012) . A large body of work exists on algorithms for SRL (Gildea and Jurafsky, 2002; Srikumar and Roth, 2011). Their success is closely connected to the availability of two large, hand-constructed semantic role resources, FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005). They used to concentrate on overt semantic Implicit SRL is useful to complete predicates’ argument structures for inference (Mirkin et al., 2010) and paraphrasing (Roth and Frank, 2013), or to assess the coherence of discourse (Burchardt et al., 2005). It however requires (even) more training data than traditional SRL. One reason is that potential arguments come from the whole text rather than just the sentence. An"
S15-1005,S10-1065,0,0.381948,"cludes 42 identification of all (explicit or implicit) semantic roles of the target predicate. The “null instantiation task” is the subtask of the full task concerned only with the identification and labeling of antecedents for implicit roles. It assumes that predicates and overt roles are already available. We follow the lead of almost all models for implicit SRL on the null instantiation task. Structurally, it can be approached similarly to role identification in traditional SRL. The first systems on large-coverage implicit SRL adopted traditional SRL modeling techniques (Chen et al., 2010; Tonelli and Delmonte, 2010). but struggled with the scarcity of training data for the complex task. Work since then has concentrated on tapping into novel knowledge and data sources. There are three main directions. The first one is knowledge about semantic types. This includes Ruppenhofer et al. (2011) who extract semantic types for null instantiations from FrameNet and Laparra and Rigau (2012) who learn distributions over semantic types for each role from explicit role annotations in FrameNet. Similarly, Roth and Frank (2013) retrieve overt instances of implicit roles from comparable corpora. The second direction is d"
S15-1005,W11-0908,0,0.276646,"nts come from the whole text rather than just the sentence. Another one is that most of the powerful syntactic features that are a staple in traditional SRL are unavailable across sentence boundaries. Unfortunately, existing corpora for implicit SRL are quite small: The task requires full-text annotation, which is time-consuming and pushes semantic role frameworks to their limits (Palmer and Sporleder, 2010). It is also hard to do consistently, and can only be crowdsourced in limited settings (Feizabadi and Pad´o, 2014). Thus, even though multiple systems for implicit SRL exist (among others, Tonelli and Delmonte (2011), Laparra and Rigau (2012), Silberer 40 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 40–50, Denver, Colorado, June 4–5, 2015. and Frank (2012)), results are still relatively poor. In this paper, we focus on the fact that the corpora that exist for implicit SRL differ not only in the semantic role frameworks used (FrameNet vs. PropBank), but also in genre (newswire vs. novels), and classes of annotated predicates (verbs vs. nouns). As a result, they are generally regarded as incompatible, and previous work has concentrated on getting most"
S15-1005,N09-2004,0,0.0303822,"d performance on both corpora, for all roles and parts of speech. We also present new insights into the properties of the implicit semantic role labeling task. 1 Well, sir, it’s [A4 this lonely, silent house] and the queer thing in the kitchen . ... I thought [A1 it] had come again. Introduction Semantic role labeling (SRL) is the task of identifying semantic arguments of predicates in text. It is an important step in text analysis and has applications in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007; Moreda et al., 2011) and machine translation (Wu and Fung, 2009; Xiong et al., 2012) . A large body of work exists on algorithms for SRL (Gildea and Jurafsky, 2002; Srikumar and Roth, 2011). Their success is closely connected to the availability of two large, hand-constructed semantic role resources, FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005). They used to concentrate on overt semantic Implicit SRL is useful to complete predicates’ argument structures for inference (Mirkin et al., 2010) and paraphrasing (Roth and Frank, 2013), or to assess the coherence of discourse (Burchardt et al., 2005). It however requires (even) more trainin"
S15-1005,P12-1095,0,0.0162811,"th corpora, for all roles and parts of speech. We also present new insights into the properties of the implicit semantic role labeling task. 1 Well, sir, it’s [A4 this lonely, silent house] and the queer thing in the kitchen . ... I thought [A1 it] had come again. Introduction Semantic role labeling (SRL) is the task of identifying semantic arguments of predicates in text. It is an important step in text analysis and has applications in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007; Moreda et al., 2011) and machine translation (Wu and Fung, 2009; Xiong et al., 2012) . A large body of work exists on algorithms for SRL (Gildea and Jurafsky, 2002; Srikumar and Roth, 2011). Their success is closely connected to the availability of two large, hand-constructed semantic role resources, FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005). They used to concentrate on overt semantic Implicit SRL is useful to complete predicates’ argument structures for inference (Mirkin et al., 2010) and paraphrasing (Roth and Frank, 2013), or to assess the coherence of discourse (Burchardt et al., 2005). It however requires (even) more training data than tradition"
S15-1017,D10-1115,0,0.139571,"l contribution, explaining the meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such eled as a matrix p that is applied to a vector for the − a . The meaning of the phrase argument’s meaning, → is then defined as the sum of the lexical meaning of − the predicate, → p , and the contributions of each argument (see Fig. 1). The matrices can be learned in a supervised manner with regression from pairs of corpus-extracted vectors for arguments and phrases. In this paper, we identify an inconsistency between the training and testing phases of the PLF. More specifically, we show that its composition procedu"
S15-1017,P13-4006,0,0.032994,"Missing"
S15-1017,1993.eamt-1.1,0,0.519532,"Missing"
S15-1017,D11-1129,0,0.11413,"Missing"
S15-1017,W10-2805,0,0.38096,"e meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such eled as a matrix p that is applied to a vector for the − a . The meaning of the phrase argument’s meaning, → is then defined as the sum of the lexical meaning of − the predicate, → p , and the contributions of each argument (see Fig. 1). The matrices can be learned in a supervised manner with regression from pairs of corpus-extracted vectors for arguments and phrases. In this paper, we identify an inconsistency between the training and testing phases of the PLF. More specifically, we show that its composition procedure leads to ove"
S15-1017,D13-1176,0,0.022378,"ch leads to an overcounting of the predicate’s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets. 1 arg Introduction Compositional distributional semantic models (CDSMs) make an important theoretical contribution, explaining the meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such eled as a matrix p that is applied to a vector for the − a . The meaning of the phrase argument’s meaning, → is then defined as the sum of the lexica"
S15-1017,W13-3513,0,0.313848,"ice accuse unemployed person armed police bill unemployed person high low Table 1: Example of experimental items in the ANVAN data sets (target verb: charge). S O −−−→ → → charge + charge ×− np subj + charge ×− np obj For transitive sentences (cf. Figure 1), we predict S O − − P(n v n) = v ×→ n + v ×→ n (the sum of the subject and the object contributions), and analogously for other constructions. 3 S O −−−→ → { charge + charge ×− np subj , charge} Experimental Setup Evaluation Datasets. We evaluate the modifications from the last section on three standard benchmarks for CDSMs: ANVAN-1 (Kartsaklis et al., 2013), ANVAN-2 (Grefenstette, 2013) (Paperno et al.’s term) and NVN (Grefenstette and Sadrzadeh, 2011) (our term). As the abbreviations indicate, the two ANVAN datasets contain transitive verbs whose NP arguments are modified by arguments; the NVN dataset contains only bare noun arguments. All three datasets are built around ambiguous target verbs that are combined with two disambiguating contexts (subjects plus objects) and two landmark verbs in a balanced design (cf. Table 1). Each context matches one of the landmark verbs, but not the other. Annotators were asked to rate the similarity between t"
S15-1017,W11-0607,0,0.0309101,"ncy in PLF between the objective function at training and the prediction at testing which leads to an overcounting of the predicate’s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets. 1 arg Introduction Compositional distributional semantic models (CDSMs) make an important theoretical contribution, explaining the meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such eled as a matrix p that is applied to a vec"
S15-1017,P14-1009,0,0.62753,"positional Distributional Semantics Abhijeet Gupta, Jason Utt and Sebastian Pad´o Institut f¨ur Maschinelle Sprachverarbeitung Universit¨at Stuttgart [guptaat|uttjn|pado]@ims.uni-stuttgart.de Abstract as adjectives or intransitive verbs can be modeled as matrices (order-2 tensors), and two-place predicates, e.g., transitive verbs, as order-3 tensors, and so forth. While such tensors enable mathematically elegant accounts of composition, their large degrees of freedom lead to severe sparsity issues when they are learned from corpora. The recently proposed Practical Lexical Function model (PLF; Paperno et al., 2014) represents a compromise between these two extremes by restricting itself to vectors and matrices, effectively reducing sparsity while retaining state-of-the-art performance across multiple datasets. It does away with tensors by ignoring interactions among the arguments of predicates p. Instead, each argument position arg is modThe Practical Lexical Function model (PLF) is a recently proposed compositional distributional semantic model which provides an elegant account of composition, striking a balance between expressiveness and robustness and performing at the state-of-the-art. In this paper"
S15-1017,D12-1110,0,0.0653772,"tion at training and the prediction at testing which leads to an overcounting of the predicate’s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets. 1 arg Introduction Compositional distributional semantic models (CDSMs) make an important theoretical contribution, explaining the meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such eled as a matrix p that is applied to a vector for the − a . The meaning of the phrase a"
S15-1022,P05-1074,0,0.0262158,"consistent semantics (positive and negative). For English, WordNet and VerbOcean were used as lexical resources. Italian WordNet was used for Italian, and GermaNet and German DerivBase (Zeller et al., 2013) were used as lexical resources for German. 1 As a part of Excitement Open Platform for Textual Entailment. https://github.com/hltfbk/EOP-1.2.1/ wiki/AlignmentEDAP1 195 Paraphrase Aligner. The paraphrase aligner concentrates on surface forms rather than lemmas and can align sequences of them rather than just individual tokens. It uses paraphrase tables, e.g. extracted from parallel corpora (Bannard and Callison-Burch, 2005). The alignment process is similar to the lexical aligner: any two sequences of tokens in T and H are aligned if the pair is listed in the resource. The alignment links created by this aligner instantiate only one relation (“paraphrase”) but report the strength of the relation via the translation probability. We used the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), which are available for numerous languages. Lemma Identity Aligner. This aligner does not use any resources. It simply aligns identical lemmas between T and H and plays an important rol"
S15-1022,J12-1003,1,0.930469,"al algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves ar"
S15-1022,W14-5201,0,0.054806,"Missing"
S15-1022,W14-3348,0,0.0536698,"ncentrates on surface forms rather than lemmas and can align sequences of them rather than just individual tokens. It uses paraphrase tables, e.g. extracted from parallel corpora (Bannard and Callison-Burch, 2005). The alignment process is similar to the lexical aligner: any two sequences of tokens in T and H are aligned if the pair is listed in the resource. The alignment links created by this aligner instantiate only one relation (“paraphrase”) but report the strength of the relation via the translation probability. We used the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), which are available for numerous languages. Lemma Identity Aligner. This aligner does not use any resources. It simply aligns identical lemmas between T and H and plays an important role in practice to deal with named entities. 3.3 A Minimal Feature Set Similar to the aligners, we concentrate on a small set of four features in the pilot algorithm. Again, the features are completely language independent, even at the implementation level. This is possible because the linguistic annotations and the alignments, use a language-independent type system (cf. Section 3.1). All current features measur"
S15-1022,E09-1025,0,0.0131132,"gnments T multi-level alignment T-H pair enriched with various levels of alignments H 3-extracting features T-H pair expressed as a data point 4-classifying entailment Entailment Decision Figure 1: Dataflow for TE algorithms based on multi level alignment that alignment strength can be misleading (MacCartney et al., 2006), alignment was understood as an intermediate step whose outcome is a set of correspondences between parts of T and H that can be used to define (mis-)match features. Alignments can be established at the word level, phrase level (MacCartney et al., 2008), or dependency level (Dinu and Wang, 2009). Dagan et al. (2013) generalized this practical use to an architectural principle: They showed that various TE algorithms can be mapped onto a universal alignment-based schema with six steps: preprocessing, enrichment, candidate alignment generation, alignment selection, and classification. Proposal. Our proposal is similar to, but simpler than, Dagan et al.’s. Figure 1 shows the data flow. First, the text and the hypothesis are linguistically pre-processed. Then, the annotated T-H pair becomes 194 the input for various independent aligners, which have access to knowledge resources and can co"
S15-1022,S14-1009,1,0.744724,"Missing"
S15-1022,W07-1401,1,0.817868,"Missing"
S15-1022,P06-1114,0,0.0407371,"ntral, powerful representation for TE algorithms that encourages modular, reusable, multilingual algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge r"
S15-1022,P10-4008,0,0.0227067,"E engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves are generally not designed to be extensible or interoperable. Therefore, changes to the algorithms – like adding support for a new language or for new analysis aspect – are often"
S15-1022,N06-1006,0,0.105121,"Missing"
S15-1022,D08-1084,0,0.0240179,"igner2 aligner3 knowledge resource 2-adding alignments T multi-level alignment T-H pair enriched with various levels of alignments H 3-extracting features T-H pair expressed as a data point 4-classifying entailment Entailment Decision Figure 1: Dataflow for TE algorithms based on multi level alignment that alignment strength can be misleading (MacCartney et al., 2006), alignment was understood as an intermediate step whose outcome is a set of correspondences between parts of T and H that can be used to define (mis-)match features. Alignments can be established at the word level, phrase level (MacCartney et al., 2008), or dependency level (Dinu and Wang, 2009). Dagan et al. (2013) generalized this practical use to an architectural principle: They showed that various TE algorithms can be mapped onto a universal alignment-based schema with six steps: preprocessing, enrichment, candidate alignment generation, alignment selection, and classification. Proposal. Our proposal is similar to, but simpler than, Dagan et al.’s. Figure 1 shows the data flow. First, the text and the hypothesis are linguistically pre-processed. Then, the annotated T-H pair becomes 194 the input for various independent aligners, which ha"
S15-1022,P14-5008,1,0.767679,"Missing"
S15-1022,P12-3013,1,0.852425,"with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves are generally not designed to be extensible or interoperable. Therefore, changes to the algorithms – like adding support for a new language o"
S15-1022,D09-1082,0,0.0349226,"Missing"
S15-1022,P13-1118,1,0.780789,"Missing"
S16-2010,C10-1011,0,0.0229373,"pment empty - emptiness religion - religious sport - sporty dispensable - indispensable write - rewrite familiar - unfamiliar Inst. 227 295 874 103 330 687 294 422 155 172 1,897 215 652 207 454 151 136 178 Table 3: English dataset (Lazaridou et al., 2013). 2.2 Word Embedding Vectors We relied on the German and English COW web corpora2 (Schäfer and Bildhauer, 2012) to obtain vector representations. The corpora contain 20 billion words and 9 billion words, respectively. We parsed the corpora using state-of-the-art pipelines integrating the MarMoT tagger and the MATE parser (Müller et al., 2013; Bohnet, 2010), and induced window co-occurrences for all corpus lemma–POS pairs and co-occurring nouns, verbs and adjectives in a 5-lemma window. We then created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015), with context distribution smoothing of 0.75 and positive point-wise mutual information weighting together with singular value decomposition. The resulting vector space models contain approximately 460 000 lemmas for German and 240 000 lemmas for English. 2.1 Derivation Datasets We created a new collection of German particle verb derivations1 relying on the same"
S16-2010,D13-1032,0,0.0232703,"agonally equip - equipment empty - emptiness religion - religious sport - sporty dispensable - indispensable write - rewrite familiar - unfamiliar Inst. 227 295 874 103 330 687 294 422 155 172 1,897 215 652 207 454 151 136 178 Table 3: English dataset (Lazaridou et al., 2013). 2.2 Word Embedding Vectors We relied on the German and English COW web corpora2 (Schäfer and Bildhauer, 2012) to obtain vector representations. The corpora contain 20 billion words and 9 billion words, respectively. We parsed the corpora using state-of-the-art pipelines integrating the MarMoT tagger and the MATE parser (Müller et al., 2013; Bohnet, 2010), and induced window co-occurrences for all corpus lemma–POS pairs and co-occurring nouns, verbs and adjectives in a 5-lemma window. We then created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015), with context distribution smoothing of 0.75 and positive point-wise mutual information weighting together with singular value decomposition. The resulting vector space models contain approximately 460 000 lemmas for German and 240 000 lemmas for English. 2.1 Derivation Datasets We created a new collection of German particle verb derivations1 relyi"
S16-2010,schafer-bildhauer-2012-building,0,0.0190159,"ss -ly -ment -ness -ous -y inreunbelieve - believable doctor - doctoral repeat - repeater use - useful algorithm - algorithmic erupt - eruption drama - dramatist accessible - accessibility cannibal - cannibalize word - wordless diagonal - diagonally equip - equipment empty - emptiness religion - religious sport - sporty dispensable - indispensable write - rewrite familiar - unfamiliar Inst. 227 295 874 103 330 687 294 422 155 172 1,897 215 652 207 454 151 136 178 Table 3: English dataset (Lazaridou et al., 2013). 2.2 Word Embedding Vectors We relied on the German and English COW web corpora2 (Schäfer and Bildhauer, 2012) to obtain vector representations. The corpora contain 20 billion words and 9 billion words, respectively. We parsed the corpora using state-of-the-art pipelines integrating the MarMoT tagger and the MATE parser (Müller et al., 2013; Bohnet, 2010), and induced window co-occurrences for all corpus lemma–POS pairs and co-occurring nouns, verbs and adjectives in a 5-lemma window. We then created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015), with context distribution smoothing of 0.75 and positive point-wise mutual information weighting together with singul"
S16-2010,W15-0108,1,0.844025,"erns (e.g., use → use + f ul ) as the result of a compositional process, where base term and affix are combined. We exploit such models for German particle verbs (PVs), and focus on the task of learning a mapping function between base verbs and particle verbs. Our models apply particle-verb motivated training-space restrictions relying on nearest neighbors, as well as recent advances from zeroshot-learning. The models improve the mapping between base terms and derived terms for a new PV derivation dataset, and also across existing derivation datasets for German and English. The experiments by Kisselew et al. (2015) were performed over six derivational patterns for German (cf. Table 1), including particle verbs (PVs) with two different particle prefixes (an and durch), which were particularly difficult to predict. PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an. Predicting PV meaning is challenging because German PVs are highly productive (Springorum et al., 2013b; Springorum et al., 2013a), and the particles are notoriously ambiguous (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Furthermore"
S16-2010,P13-1149,0,0.139149,"o predict. PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an. Predicting PV meaning is challenging because German PVs are highly productive (Springorum et al., 2013b; Springorum et al., 2013a), and the particles are notoriously ambiguous (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Furthermore, the particles often trigger meaning shifts when they combine with base verbs (Springorum et al., 2013b), so the resulting PVs represent frequent cases of non-literal meaning. 1 Introduction Lazaridou et al. (2013) were the first to apply distributional semantic models (DSMs) to the task of deriving the meaning of morphologically complex words from their parts. They relied on high-dimensional vector representations to model the derived term (e.g., useful) as a result of a compositional process that combines the meanings of the base term (e.g., to use) and the affix (e.g., ful). For evaluation, they compared the predicted vector of the complex word with the original, corpus-based vector. More recently, Kisselew et al. (2015) put the task of modeling derivation into the perspective of zero-shot-learning:"
S16-2010,W13-0120,1,0.633061,"he mapping between base terms and derived terms for a new PV derivation dataset, and also across existing derivation datasets for German and English. The experiments by Kisselew et al. (2015) were performed over six derivational patterns for German (cf. Table 1), including particle verbs (PVs) with two different particle prefixes (an and durch), which were particularly difficult to predict. PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an. Predicting PV meaning is challenging because German PVs are highly productive (Springorum et al., 2013b; Springorum et al., 2013a), and the particles are notoriously ambiguous (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Furthermore, the particles often trigger meaning shifts when they combine with base verbs (Springorum et al., 2013b), so the resulting PVs represent frequent cases of non-literal meaning. 1 Introduction Lazaridou et al. (2013) were the first to apply distributional semantic models (DSMs) to the task of deriving the meaning of morphologically complex words from their parts. They relied on high-dimensional vector representations to model the"
S16-2010,P15-1027,0,0.0327905,"hinelle Sprachverarbeitung, Universität Stuttgart Pfaffenwaldring 5B, 70569 Stuttgart, Germany {koepermn,schulte,kisselmx,pado}@ims.uni-stuttgart.de Abstract vector was computed, a nearest neighbor search was applied to validate if the prediction corresponded to the derived term. In zero-shotlearning the task is to predict novel values, i.e., values that were never seen in training. More formally, zero-shot-learning trains a classifier f : X → Y that predicts novel values for Y (Palatucci et al., 2009). It is often applied across vector spaces, such as different domains (Mikolov et al., 2013; Lazaridou et al., 2015). Recent models in distributional semantics consider derivational patterns (e.g., use → use + f ul ) as the result of a compositional process, where base term and affix are combined. We exploit such models for German particle verbs (PVs), and focus on the task of learning a mapping function between base verbs and particle verbs. Our models apply particle-verb motivated training-space restrictions relying on nearest neighbors, as well as recent advances from zeroshot-learning. The models improve the mapping between base terms and derived terms for a new PV derivation dataset, and also across ex"
S16-2010,W14-1618,0,0.0952037,"Missing"
S16-2010,P13-1118,1,0.791565,"Missing"
S16-2010,Q15-1016,0,0.0230701,"t (Lazaridou et al., 2013). 2.2 Word Embedding Vectors We relied on the German and English COW web corpora2 (Schäfer and Bildhauer, 2012) to obtain vector representations. The corpora contain 20 billion words and 9 billion words, respectively. We parsed the corpora using state-of-the-art pipelines integrating the MarMoT tagger and the MATE parser (Müller et al., 2013; Bohnet, 2010), and induced window co-occurrences for all corpus lemma–POS pairs and co-occurring nouns, verbs and adjectives in a 5-lemma window. We then created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015), with context distribution smoothing of 0.75 and positive point-wise mutual information weighting together with singular value decomposition. The resulting vector space models contain approximately 460 000 lemmas for German and 240 000 lemmas for English. 2.1 Derivation Datasets We created a new collection of German particle verb derivations1 relying on the same resource as Kisselew et al. (2015), the semiautomatic derivational lexicon for German DErivBase (Zeller et al., 2013). From DErivBase, we induced all pairs of base verbs and particle verbs across seven different particles. Nonexisting"
S17-1012,D15-1002,1,0.906343,"Missing"
S17-1012,D15-1038,0,0.15187,"Missing"
S17-1012,W15-0120,0,0.0313534,"These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example, Donald Trump is an instance of the concept politician. Consequently, entities are generally associated with a rich set of numeric and relational attributes (for politician instances: size, office, etc.). In contrast to concepts, the values of these attributes tend to be discrete (Herbelot, 2015): while the size of politician is best described by a probability distribution, the size of Donald Trump is 1.88m. Since distributional representations are notoriously bad at handling discrete knowledge (Fodor and Lepore, 1999; Smolensky, 1990), this raises the question of how well such models can capture entity-related knowledge. In our previous work (Gupta et al., 2015), we analysed distributional prediction of numeric attributes of entities, found a large variance in quality among attributes, and identified factors determining prediction difficulty. A corresponding analysis for relational ("
S17-1012,W15-0105,0,0.0274374,"Missing"
S17-1012,Q15-1019,0,0.0545797,"we analysed distributional prediction of numeric attributes of entities, found a large variance in quality among attributes, and identified factors determining prediction difficulty. A corresponding analysis for relational (categorial) attributes of entities is still missing, even though entities are highly relevant for NLP. This is evident from the highly active area of knowledge base completion (KBC), the task of extending incomplete entity information in knowledge bases such as Yago or Wikidata (e.g., Bordes et al., 2013; Freitas et al., 2014; Neelakantan and Chang, 2015; Guu et al., 2015; Krishnamurthy and Mitchell, 2015). In this paper, we assess to what extent relational attributes of entities are easily accessible from word embedding space. To this end, we define two models that predict, given a target entity (Star Wars) and a relation (director), a distributed representation for the relatum (George Lucas). We carry out a detailed per-relation analyses of their performance on seven 1 The original dataset by Mikolov et al. (2013) did contain a small number of entity-entity relations. 104 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 104–109, c Vancouver, Ca"
S17-1012,P14-2050,0,0.392568,"entral claim about distributed models of word meaning (e.g., Mikolov et al. (2013)) is that word embedding space provides easy access to semantic relations. E.g., Mikolov et al.’s space was shown to encode the “male-female relation” linearly, as a # » # # » − woman # » = king » ). vector (man − queen The accessibility of semantic relations was subsequently examined in more detail. Rei and Briscoe (2014) and Melamud et al. (2014) reported successful modeling of lexical relations such as hypernymy and synonymy. K¨oper et al. (2015) considered a broader range of relationships,with mixed results. Levy and Goldberg (2014b) developed an improved, nonlinear relation extraction method. These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example, Donald Trump is an instance of the concept politician. Consequently, entities are generally associated with a rich set of numeric and relational attributes (for politician instances: size, office, etc.). In contrast t"
S17-1012,W14-1618,0,0.258101,"entral claim about distributed models of word meaning (e.g., Mikolov et al. (2013)) is that word embedding space provides easy access to semantic relations. E.g., Mikolov et al.’s space was shown to encode the “male-female relation” linearly, as a # » # # » − woman # » = king » ). vector (man − queen The accessibility of semantic relations was subsequently examined in more detail. Rei and Briscoe (2014) and Melamud et al. (2014) reported successful modeling of lexical relations such as hypernymy and synonymy. K¨oper et al. (2015) considered a broader range of relationships,with mixed results. Levy and Goldberg (2014b) developed an improved, nonlinear relation extraction method. These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example, Donald Trump is an instance of the concept politician. Consequently, entities are generally associated with a rich set of numeric and relational attributes (for politician instances: size, office, etc.). In contrast t"
S17-1012,W14-1619,0,0.0160294,"es not result from low frequency, but from (a) one-to-many mappings; and (b) lack of context patterns expressing the relation that are easy to pick up by word embeddings. 1 Introduction A central claim about distributed models of word meaning (e.g., Mikolov et al. (2013)) is that word embedding space provides easy access to semantic relations. E.g., Mikolov et al.’s space was shown to encode the “male-female relation” linearly, as a # » # # » − woman # » = king » ). vector (man − queen The accessibility of semantic relations was subsequently examined in more detail. Rei and Briscoe (2014) and Melamud et al. (2014) reported successful modeling of lexical relations such as hypernymy and synonymy. K¨oper et al. (2015) considered a broader range of relationships,with mixed results. Levy and Goldberg (2014b) developed an improved, nonlinear relation extraction method. These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example, Donald Trump is an instanc"
S17-1012,N15-1054,0,0.0176546,"ge. In our previous work (Gupta et al., 2015), we analysed distributional prediction of numeric attributes of entities, found a large variance in quality among attributes, and identified factors determining prediction difficulty. A corresponding analysis for relational (categorial) attributes of entities is still missing, even though entities are highly relevant for NLP. This is evident from the highly active area of knowledge base completion (KBC), the task of extending incomplete entity information in knowledge bases such as Yago or Wikidata (e.g., Bordes et al., 2013; Freitas et al., 2014; Neelakantan and Chang, 2015; Guu et al., 2015; Krishnamurthy and Mitchell, 2015). In this paper, we assess to what extent relational attributes of entities are easily accessible from word embedding space. To this end, we define two models that predict, given a target entity (Star Wars) and a relation (director), a distributed representation for the relatum (George Lucas). We carry out a detailed per-relation analyses of their performance on seven 1 The original dataset by Mikolov et al. (2013) did contain a small number of entity-entity relations. 104 Proceedings of the 6th Joint Conference on Lexical and Computational"
S17-1012,W14-1608,0,0.0133266,"ifficulty for a relation does not result from low frequency, but from (a) one-to-many mappings; and (b) lack of context patterns expressing the relation that are easy to pick up by word embeddings. 1 Introduction A central claim about distributed models of word meaning (e.g., Mikolov et al. (2013)) is that word embedding space provides easy access to semantic relations. E.g., Mikolov et al.’s space was shown to encode the “male-female relation” linearly, as a # » # # » − woman # » = king » ). vector (man − queen The accessibility of semantic relations was subsequently examined in more detail. Rei and Briscoe (2014) and Melamud et al. (2014) reported successful modeling of lexical relations such as hypernymy and synonymy. K¨oper et al. (2015) considered a broader range of relationships,with mixed results. Levy and Goldberg (2014b) developed an improved, nonlinear relation extraction method. These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example,"
S17-1012,D15-1174,0,0.154722,"Missing"
S17-1014,D10-1115,0,0.504726,"n dataset. We find that the PLF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant. 1 Introduction Compositional distributional semantic models (CDSMs) represent phrase meaning in a vector space by composing the meanings of individual words. Many CDSMs were proposed, ranging from basic ones that use element-wise operations on word vectors to compute phrase vectors (Mitchell and Lapata, 2008), to more complex models that represent predicate arguments as higher-order tensors (Baroni and Zamparelli, 2010; Guevara, 2010). The latter models assume that predicates in a phrase act as functions that act on other phrase components to yield the final representation of the phrase. For example, an adjective acts as a function on the noun in an adjective-noun phrase, while a transitive verb acts as a binary function on its subject and object. However, since the number of parameters in a tensor grows exponentially with the number of arguments of the function that it models, learning full tensors for predicates with many arguments is tedious to impractical (Grefenstette et al., 2012). The Practical Lexic"
S17-1014,W10-2805,0,0.209741,"LF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant. 1 Introduction Compositional distributional semantic models (CDSMs) represent phrase meaning in a vector space by composing the meanings of individual words. Many CDSMs were proposed, ranging from basic ones that use element-wise operations on word vectors to compute phrase vectors (Mitchell and Lapata, 2008), to more complex models that represent predicate arguments as higher-order tensors (Baroni and Zamparelli, 2010; Guevara, 2010). The latter models assume that predicates in a phrase act as functions that act on other phrase components to yield the final representation of the phrase. For example, an adjective acts as a function on the noun in an adjective-noun phrase, while a transitive verb acts as a binary function on its subject and object. However, since the number of parameters in a tensor grows exponentially with the number of arguments of the function that it models, learning full tensors for predicates with many arguments is tedious to impractical (Grefenstette et al., 2012). The Practical Lexical Function mode"
S17-1014,S15-1017,1,0.848946,"ived vectors for predicate-argument combinations are a key part of the PLF, non-adjacency might make it difficult to estimate its parameters reliably for such languages. Secondly, the evaluation method reported by Paperno et al. (2014) uses a somewhat artificial setup by assuming that all phrase pairs, even ill-formed ones, can be graded for similarity. In this work we consider both of these questions. We investigate the application of PLF to Croatian language, a Slavic language with relatively free word order. We compare PLF with other, simpler CDSMs, as well as PLF modifications proposed by Gupta et al. (2015). In contrast to Paperno et al. (2014), we adopt lexical substitution as evaluation, building a new dataset of Croatian ANVAN phrases, 115 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 115–120, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics together with word substitutes for each word. The PLF model for Croatian performs comparably well to English, outperforming simpler CDSMs in particular at the verb position. Gupta et al. (2015) found both modifications to outperform simple baseline CDSMs for English w"
S17-1014,W13-3513,0,0.742936,"voditi suparniˇcka momˇcad (legendary coach lead opponent team) cijenjen (appreciated), izvanredan (outstanding), poznat (famous), uspješan (successful), znamenit (notable) dobar igraˇc dati pobjedniˇcki gol (good player score winning goal) pogoditi (to hit), posti´ci (to achieve), zabiti (to score), zadati (to give) sportski automobil prije´ci velika udaljenost (sports car travel large distance) dionica (section), dužina (length), put (way), razdaljina (distance) Table 1: Examples of ANVAN phrases with manually collected substitutes for boldfaced targets. pairs rated for semantic similarity (Kartsaklis et al., 2013; Grefenstette, 2013). The phrases in each pair differ only in the verb. Annotators rated the similarity on a scale from 1 to 7, and CDSMs were evaluated by correlating the ratings with the similarity of the predicted phrase vectors. The described approach is not appropriate when one or both ANVAN phrases are ungrammatical or nonsensical. Consider the following phrase pair in the ANVAN dataset by Kartsaklis et al. (2013): ‘dental service file false tooth’ – ‘dental service register false tooth’. While the first sentence is plausible, the second one is arguably somewhere between implausible and"
S17-1014,S07-1009,0,0.0492604,"sider the following phrase pair in the ANVAN dataset by Kartsaklis et al. (2013): ‘dental service file false tooth’ – ‘dental service register false tooth’. While the first sentence is plausible, the second one is arguably somewhere between implausible and nonsensical. We believe that semantic similarity is not a reasonable evaluation criterion for such (relatively frequent) cases. For our experiment, we chose a word-choice evaluation setup, which essentially builds on the idea of lexical substitution. Lexical substitution is the task of identifying a substitute for a word in a given context (McCarthy and Navigli, 2007). Typically, a system is presented with a phrase and candidate substitutes for a target word in the phrase and needs to select one or more adequate substitutes. Systems either have to rank the candidates in the appropriate order (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2009), or just choose one best substitute (Melamud et al., 2016). An additional benefit of a lexical substitution setup is that we can evaluate the predictions of the model not just globally, but at the level of individual words. We will exploit that possibility below. Croatian ANVAN dataset. We constructed individual AN"
S17-1014,K16-1006,0,0.206901,"or such (relatively frequent) cases. For our experiment, we chose a word-choice evaluation setup, which essentially builds on the idea of lexical substitution. Lexical substitution is the task of identifying a substitute for a word in a given context (McCarthy and Navigli, 2007). Typically, a system is presented with a phrase and candidate substitutes for a target word in the phrase and needs to select one or more adequate substitutes. Systems either have to rank the candidates in the appropriate order (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2009), or just choose one best substitute (Melamud et al., 2016). An additional benefit of a lexical substitution setup is that we can evaluate the predictions of the model not just globally, but at the level of individual words. We will exploit that possibility below. Croatian ANVAN dataset. We constructed individual ANVAN phrases for Croatian like in prior English work (Kartsaklis et al., 2013; Grefenstette, 2013). We started by choosing six transitive verbs from the list of polysemous verbs on the Croatian language portal.2 We chose verbs with high polysemy level, while avoiding those that overlap in semantic meaning. The list consist of the following v"
S17-1014,P08-1028,0,0.712906,"ver in free word order languages. We evaluate variants of the PLF for Croatian, using a new lexical substitution dataset. We find that the PLF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant. 1 Introduction Compositional distributional semantic models (CDSMs) represent phrase meaning in a vector space by composing the meanings of individual words. Many CDSMs were proposed, ranging from basic ones that use element-wise operations on word vectors to compute phrase vectors (Mitchell and Lapata, 2008), to more complex models that represent predicate arguments as higher-order tensors (Baroni and Zamparelli, 2010; Guevara, 2010). The latter models assume that predicates in a phrase act as functions that act on other phrase components to yield the final representation of the phrase. For example, an adjective acts as a function on the noun in an adjective-noun phrase, while a transitive verb acts as a binary function on its subject and object. However, since the number of parameters in a tensor grows exponentially with the number of arguments of the function that it models, learning full tenso"
S17-1014,P14-1009,0,0.341511,"tter models assume that predicates in a phrase act as functions that act on other phrase components to yield the final representation of the phrase. For example, an adjective acts as a function on the noun in an adjective-noun phrase, while a transitive verb acts as a binary function on its subject and object. However, since the number of parameters in a tensor grows exponentially with the number of arguments of the function that it models, learning full tensors for predicates with many arguments is tedious to impractical (Grefenstette et al., 2012). The Practical Lexical Function model (PLF, Paperno et al. (2014)) strikes a middle ground by breaking down all tensors with ranks higher than two into multiple matrices, each representing the predicate’s composition with a single argument (cf. Section 2 for details). In the experiments of Paperno et al. (2014), PLF has been shown to work better than some other CDSMs in modeling semantic similarity. Particularly good results were obtained on ANVAN (adjective-noun-verb-adjectivenoun) phrases, where PLF outperformed both simple CDSMs (due to its higher expressiveness) as well as the higher-order Lexical Function model (Baroni and Zamparelli, 2010). Although t"
S17-1014,R09-1073,0,0.0182613,"semantic similarity is not a reasonable evaluation criterion for such (relatively frequent) cases. For our experiment, we chose a word-choice evaluation setup, which essentially builds on the idea of lexical substitution. Lexical substitution is the task of identifying a substitute for a word in a given context (McCarthy and Navigli, 2007). Typically, a system is presented with a phrase and candidate substitutes for a target word in the phrase and needs to select one or more adequate substitutes. Systems either have to rank the candidates in the appropriate order (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2009), or just choose one best substitute (Melamud et al., 2016). An additional benefit of a lexical substitution setup is that we can evaluate the predictions of the model not just globally, but at the level of individual words. We will exploit that possibility below. Croatian ANVAN dataset. We constructed individual ANVAN phrases for Croatian like in prior English work (Kartsaklis et al., 2013; Grefenstette, 2013). We started by choosing six transitive verbs from the list of polysemous verbs on the Croatian language portal.2 We chose verbs with high polysemy level, while avoiding those that overl"
S17-1014,P13-2137,1,0.918073,"guistics together with word substitutes for each word. The PLF model for Croatian performs comparably well to English, outperforming simpler CDSMs in particular at the verb position. Gupta et al. (2015) found both modifications to outperform simple baseline CDSMs for English when evaluated on ANVAN datasets, with test adaptation outperforming the original PLF. 2 PLF for Croatian. We implemented the basic PLF and the two above-mentioned modifications for Croatian following the procedure described by Paperno et al. (2014). As a corpus for building word and phrase lexical vectors we used fHrWaC (Šnajder et al., 2013), a filtered version of Croatian web corpus (Ljubeši´c and Erjavec, 2011), totaling 51M sentences and 1.2B tokens. The corpus has been parsed using the MSTParser for Croatian (Agi´c and Merkler, 2013). As a first step in obtaining word vector representations, we extracted a co-occurrence matrix of 30K most frequent lemmas (nouns, verbs, and adjectives) in corpus, using a window of size 3. Next, the vectors contained in the resulting matrix were transformed using Positive Pointwise Mutual Information (PPMI) and reduced to size 300 using Singular Value Decomposition. Finally, all vectors in the"
todirascu-etal-2012-french,C10-1011,0,\N,Missing
todirascu-etal-2012-french,C94-2174,0,\N,Missing
todirascu-etal-2012-french,E06-2001,0,\N,Missing
todirascu-etal-2012-french,J00-4001,0,\N,Missing
W04-0817,W04-2413,1,0.86329,"Missing"
W04-0817,W98-1505,0,0.0255768,"med by off-the-shelf statistical tools for data processing and modelling. After listing our data preparation steps (Sec. 2) and features (Sec. 3), we describe our classification procedure and the learners we used (Sec. 4). Sec. 5 outlines our experiments in similarity-based generalisations, and Section 6 discusses our results. 2 Detlef Prescher University of Amsterdam Amsterdam, The Netherlands prescher@science.uva.nl Data and Instances Parsing. To tag and parse the data, we used LoPar (Schmid, 2000), a probabilistic contextfree parser, which comes with a Head-Lexicalised Grammar for English (Carroll and Rooth, 1998). We considered only the most probable parse for each sentence and simplified parse trees by eliminating unary nodes. The resulting nodes form the instances of our classification. We used the Stuttgart TreeTagger (Schmid, 1994) to lemmatise constituent heads. NP (C OGNIZER) VP (NONE) Peter V (NONE) does VP (NONE) not know NP (C ONTENT) the answer Figure 1: Example parse tree with role labels Semantic clustering. We used clustering to generalise over possible fillers of roles. In a first model, we derived a probability distribution  for pairs     , where  is a target:role co"
W04-0817,W03-1007,0,0.12709,"Missing"
W04-0817,J02-3001,0,0.465326,"Missing"
W04-0817,W02-2018,0,0.0410263,"l, where the probability of a class  given an feature vector ) ( is defined as   & )( $+, * -.!/1032 0 43576 !8 , - where is a normalisation constant, - 9  )   the ) value of feature for class  , and : the weight assigned to 9 - . The model is trained by optimising the weights : subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt. Maximum Entropy (Maxent) models have been successfully applied to semantic role labelling (Fleischman et al., 2003). We used the estimate software for estimation, which implements the LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. Memory-based Learning. Our second learner implements an instance of a memory-based learning (MBL) algorithm, namely the ; -nearest neighbour algorithm. This algorithm classifies test instances by assigning them the label of the most similar examples from the training set. Its parameters are the number of training examples to be considered, the similarity metric, and the feature weighting scheme. We used the implementation provided by TiMBL (Daelemans et al., 2003) with the default parameters, i.e. ; =1 and the weighted overlap similarity metric with gain"
W04-0817,C00-2094,1,0.853748,"t:role pair =@> for a cluster  as follows: <  =@>1$ A BDCFEHG A 2I4 6 JLK 8 MIN 9 OP   & O where <   =?>1 is the total frequency of all head lemmas O that have been seen with =@> , weighted by the class-membership probability of O in  . This appropriateness measure <  =@>1 is built on top of the class-based frequencies 9  O   & O rather than on the frequencies 9  O or the class-membership probabilities   & OP in isolation: For some tasks the combination of lexical and semantic information has been shown to outperform each of the single information sources (Prescher et al., 2000). Our similarity notion is now formalised as follows: With a threshold Q as a parameter, two frame elements =@> , =@>R count as similar if for some class  , <   =@>S $T Q and <   =@>  $T Q . In the syntactic clustering model, a role filler was described as a combination of the path from instance to target, the instance’s preposition, and the target voice. The appropriateness of a target:role pair is defined as for the above model. For time reasons, only verbal targets were considered. Figure 2 shows excerpts of two “syntactic” clusters in the form of target:frame.role members. Group 6"
W04-2413,W04-2412,0,0.0410263,"that employs only shallow information. We use a Maximum Entropy learner, augmented by EM-based clustering to model the fit between a verb and its argument candidate. The instances to be classified are sequences of chunks that occur frequently as arguments in the training corpus. Our best model obtains an F score of 51.70 on the test set. 1 Introduction This paper describes a statistical approach to semantic role labelling addressing the CoNLL shared task 2004, which is based on the the current release of the English PropBank data (Kingsbury et al., 2002). For further details of the task, see (Carreras and Màrquez, 2004). We address the main challenge of the task, the absence of deep syntactic information, with three main ideas: Proper constituents being unavailable, we use chunk sequences as instances for classification. The classification is performed by a maximum entropy model, which can integrate features from heterogeneous data sources. We model the fit between verb and argument candidate by clusters induced with EM on the training data, which we use as features during classification. Sections 2 through 4 describe the systems’ architecture. First, we compute chunk sequences for all sentences (Sec. 2). Th"
W04-2413,W03-1007,0,0.0725491,"NNP POS VBG NN VBZ VBG PRP TO NN NNS [NP ] [NP ] [VP ] [NP] [VP ] [NP ] [S ] Figure 1: Part of a sentence with part of speech, chunk and clause information Frequency in training data 25000 LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. We chose a maximum entropy approach because it can integrate many different sources of information without assuming independence of features. Also, models with minimal commitment are good predictors of future data. Maxent models have found wide application in NLP during the recent years; for semantic role labelling (on FrameNet data) see (Fleischman et al., 2003). Sequence frequencies Divider frequencies 20000 15000 10000 5000 0 3.2 Figure 2: Frequency distribution for the 20 most frequent sequences and dividers in the training data separated from it by e.g. a typical A1 sequence. Generalised divider chunk sequences separating actual arguments and targets in the training set show a Zipfian distribution similar to the chunk sequences (see Fig. 2). As instances to be classified, we consider all sequences whose generalised sequence and divider each appear at least 10 times for an argument in the training corpus, and whose generalised sequence and divider"
W04-2413,W02-2018,0,0.0352341,"VP_PP_NP_NP (in words: a bonus in the form of charitable donations made from an employer ’s treasury). The chunk sequence approach also allows us to consider the divider chunk sequences that separate arguments and targets. For example, A0s are usually divided from the target by the empty divider, while A2 arguments are Britain ’s manufacturing industry is transforming itself to boost exports NNP POS VBG NN VBZ VBG PRP TO NN NNS [NP ] [NP ] [VP ] [NP] [VP ] [NP ] [S ] Figure 1: Part of a sentence with part of speech, chunk and clause information Frequency in training data 25000 LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. We chose a maximum entropy approach because it can integrate many different sources of information without assuming independence of features. Also, models with minimal commitment are good predictors of future data. Maxent models have found wide application in NLP during the recent years; for semantic role labelling (on FrameNet data) see (Fleischman et al., 2003). Sequence frequencies Divider frequencies 20000 15000 10000 5000 0 3.2 Figure 2: Frequency distribution for the 20 most frequent sequences and dividers in the training data separated from it by"
W04-2413,P99-1014,1,0.757592,"get lemma is passive. Divider Features. These are shallow and higher-level features related to the divider sequences: the divider itself, its superchunk, and we state whether, judging by the divider, the sequence is an argument. A similar feature judges this by the combination of divider and sequence. Features based on EM-Based Clustering. We use EM-based clustering to measure the fit between a target verb, an argument position of the verb, and the head lemma (or head named entity) of a sequence. EM-based clustering, originally introduced for the induction of a semantically annotated lexicon (Rooth et al., 1999), regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm. In our application, we aim at deriving a probability distribution +, on verb-argument pairs + from the training data. Using the key idea that + is conditioned on an unobserved class .-0/ , we define the probability of a pair +1 2'+435(+67 -98 3;: 8 6 as: <'+ = >   (+ 2> <  ''+   #@?A @# ?A    B+43   ' '+6   >  #@?A The last line is warranted by the assumption that +C3 and +6 are independent and are only"
W04-3214,P98-1013,0,0.0221795,"r is that this variance is to a large extent a result of differences in the underlying argument structure of the predicates in different frames. In a second experiment, we show that frame uniformity, which measures argument structure variation, correlates well with the performance figures, effectively explaining the variance. 1 Introduction Recent years have witnessed growing interest in corpora with semantic annotation, especially on the semantic role (or argument structure) level. A number of projects are working on producing such corpora through manual annotation, among which are FrameNet (Baker et al., 1998), the Prague Dependency Treebank (Hajiˇcová, 1998), PropBank (Kingsbury et al., 2002), and SALSA (Erk et al., 2003). For semantic role annotation to be widely useful for NLP, however, robust and accurate methods for automatic semantic role assignment are necessary. Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). This year (2004), semantic role labelling served as the shared task at two conferences, CoNLL1 and SENSEVAL2. However, almost all studies have co"
W04-3214,W04-0817,1,0.818377,"e into account that the relevant linking properties differ between individual predicates. Our results suggest that the variance caused by argument structure will not disappear with better classifiers, but that the problem of inadequate generalisations should be addressed in a principled way. There are several possible approaches to do so. First, the classic statistical approach: Combining evidence from different frame-specific roles to alleviate data sparseness. To this end, Gildea and Jurafsky (2002) developed a mapping from framespecific to syntactic roles, but results did not improve much. Baldewein et al. (2004) experiment with EM-driven generalisation, and obtain also only modest improvements. A second approach is to identify other levels, different from frames, at which regularities can be learnt better. One possibility is to identify smaller units within frames which have a more uniform structure and which can be learnt more easily. Since uniformity is defined in terms of a quality function, clustering would be the natural method to employ for this task. However, this method is only viable for frames with a large amount of annotation. A more general idea in this spirit is to construct an independe"
W04-3214,A00-1031,0,0.117153,"Missing"
W04-3214,P97-1003,0,0.159516,"Missing"
W04-3214,P03-1068,1,0.829712,"icates in different frames. In a second experiment, we show that frame uniformity, which measures argument structure variation, correlates well with the performance figures, effectively explaining the variance. 1 Introduction Recent years have witnessed growing interest in corpora with semantic annotation, especially on the semantic role (or argument structure) level. A number of projects are working on producing such corpora through manual annotation, among which are FrameNet (Baker et al., 1998), the Prague Dependency Treebank (Hajiˇcová, 1998), PropBank (Kingsbury et al., 2002), and SALSA (Erk et al., 2003). For semantic role annotation to be widely useful for NLP, however, robust and accurate methods for automatic semantic role assignment are necessary. Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). This year (2004), semantic role labelling served as the shared task at two conferences, CoNLL1 and SENSEVAL2. However, almost all studies have concentrated on the technical aspects of the models – identifying informative feature sets and suitable statistical f"
W04-3214,W03-1007,0,0.182197,"tic role (or argument structure) level. A number of projects are working on producing such corpora through manual annotation, among which are FrameNet (Baker et al., 1998), the Prague Dependency Treebank (Hajiˇcová, 1998), PropBank (Kingsbury et al., 2002), and SALSA (Erk et al., 2003). For semantic role annotation to be widely useful for NLP, however, robust and accurate methods for automatic semantic role assignment are necessary. Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). This year (2004), semantic role labelling served as the shared task at two conferences, CoNLL1 and SENSEVAL2. However, almost all studies have concentrated on the technical aspects of the models – identifying informative feature sets and suitable statistical frameworks – with the goal of optimising the performance of the models on the complete dataset. The only study we are aware of with a more detailed evaluation is Fleischman et al. (2003), who nevertheless come to the conclusion that either “new features”, 1 2 http://www.lsi.upc.es/~conll04st/ http://www.clres.com/SensSemRoles.html Gemma"
W04-3214,P00-1065,0,0.070522,"ely explaining the variance. 1 Introduction Recent years have witnessed growing interest in corpora with semantic annotation, especially on the semantic role (or argument structure) level. A number of projects are working on producing such corpora through manual annotation, among which are FrameNet (Baker et al., 1998), the Prague Dependency Treebank (Hajiˇcová, 1998), PropBank (Kingsbury et al., 2002), and SALSA (Erk et al., 2003). For semantic role annotation to be widely useful for NLP, however, robust and accurate methods for automatic semantic role assignment are necessary. Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). This year (2004), semantic role labelling served as the shared task at two conferences, CoNLL1 and SENSEVAL2. However, almost all studies have concentrated on the technical aspects of the models – identifying informative feature sets and suitable statistical frameworks – with the goal of optimising the performance of the models on the complete dataset. The only study we are aware of with a more detailed evaluation is Fleischman et al. (2003), who"
W04-3214,J02-3001,0,0.373578,"ontact” 3 . (1) a. b. c. [ Impactee His car] was struck [ Impactor by a third vehicle]. [ Impactor The door] slammed [ Result shut]. [ Impactors Their vehicles] collided [ Place at Pond Hill]. Note that the frame-specificity of semantic roles in FrameNet has important consequences for semantic role assignment, since there is no direct way to generalise across frames. Therefore, the learning for automatic assignment of semantic roles has to proceed frame-wise. Thus, the data sparseness problem is especially acute, and automatic assignment for frames with no training data is very difficult (see Gildea and Jurafsky (2002)). 3 Experiment 1: Frame-Wise Evaluation of Semantic Role Assignment In our first experiment, we perform a detailed (frame-wise) evaluation of semantic role assignment to discover general patterns in the data. Our aim is not to outperform existing models, but to replicate the workings of existing models so that our findings are representative for the task as it is currently addressed. To this end, we (a) use a standard dataset, the FrameNet data, (b) model the task with two different statistical frameworks, and (c) keep our models as generic as possible. 3.1 Data and experimental setup For thi"
W04-3214,J03-3005,0,0.0772868,"Missing"
W04-3214,P99-1004,0,0.0465873,", i.e. predicates in  a frame share a common set of arguments. What checks is whether the mapping from semantics to syntax is also similar. 6 The centroid of a cluster is “a point in  -dimensional space found by averaging the measurement values along each dimension” (Kaufman and Rousseeuw, 1990, p. 112), so that it is the point situated at the “center” of the cluster. In order to obtain an actual measure for frame uniformity, we take two further steps. First, we instantiate with the cosine similarity  , which has been found to be appropriate for a wide range of linguistic tasks (see e.g. Lee (1999)) and ranges between 0 (least similar) and 1 (identity):                             Second, we  normalise the values of , which !  , the number of vectors, to  , to grow in  make them interpretable analogously to values of the cosine similarity. Since this is possible in two different ways, we obtain two different measures for frame uniformity. The first one, which we call normalised quality-based  uniformity (""# ), simply divides the values by : ""#                          The second measure, weighted quality-based u"
W04-3214,W02-2018,0,0.312566,"Missing"
W04-3214,C98-1013,0,\N,Missing
W09-0208,D08-1007,0,0.0048007,"get + object selpref target + subject selpref 0.005 0.022 0.024 mean sim(val)−sim(inval) 0.026 0.028 target + object selpref target + subject selpref 0.020 mean sim(val)−sim(inval) 0.025 Figure 4: Scatterplot of &quot;out of ten&quot; accuracy against model discriminativity between valid and invalid paraphrases. Left: L EX S UB -PARA, right: S EM C OR -PARA. 0 5 10 15 20 25 30 0 5 exponent 10 15 20 25 30 exponent Figure 5: Average amount to which predictions are more similar to valid than to invalid paraphrases, for different reweighting values. Left: L EX S UB -PARA, right: S EM C OR -PARA. mulations (Bergsma et al., 2008), or by exemplar-based models that are able to deal better with the ambiguity present in the preferences of very general words. Another important topic for further research is the computation of token vectors that incorporate more than one context word. The current results we obtain for “both ” are promising but limited; it appears that the successful integration of multiple context words requires strategies that go beyond simplistic addition or intersection of observed contexts. in the datasets. We describe an auxiliary quantity, discriminativity, that measures the ability of the model’s pred"
W09-0208,D08-1021,0,0.0131733,"Missing"
W09-0208,P07-1028,1,0.0724908,"well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector space is not straightforward. The second one is reweighting. We obtained the best performance when the context expectations were reweighted by taking each component to a (high) n-th power, which is counterintuitive. Finally, we found subjects to be more informative in judging the appropriateness of paraphrases than objects. This"
W09-0208,J02-3001,0,0.00279255,"ivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector space is not straightforward. The second one is reweighting. We obtained the best performance when the context expectations were reweighted by taking each component to a"
W09-0208,D07-1042,1,0.418701,"lished (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector space is not straightforward. The second one is reweighting. We obtained the best performance when the context expectations were reweighted by taking each component to a (high) n-th power, which is counterintuitive. Finally, we found subjects to be more informative in judging the appropriateness of paraphrases than objects. This appears to contradic"
W09-0208,J93-1005,0,0.176233,"l of word meaning is motivated both on cognitive and linguistic grounds. In cognitive science, the central role of expectations about typical events on almost all aspects of human language processing is well-established (McRae et al., 1998; Narayanan and Jurafsky, 2002). In linguistics, expectations have long been used in semantic theories in the form of selectional restrictions and selectional preferences (Wilks, 1975), and more recently induced from corpora (Resnik, 1996). Attention has mostly been limited to selectional preferences of verbs, which have been used for for a variety of tasks (Hindle and Rooth, 1993; Gildea and Jurafsky, 2002). A recent result that the SVS model builds on is that selectional preferences can be represented as prototype vectors constructed from seen arguments (Erk, 2007; Padó et al., 2007). Impact of parameters. We re-examine three central parameters of SVS. The first one is the choice of vector combination function. Following Mitchell and Lapata (2008), we previously used componentwise multiplication, whose interpretation in vector space is not straightforward. The second one is reweighting. We obtained the best performance when the context expectations were reweighted by"
W09-0208,J98-1004,0,0.362466,"Missing"
W09-0208,P93-1016,0,0.0784823,"an a joint syntactic context preference function because (a) this separation models the conceptual difference between predicates and arguments, and (b) it allows for a simpler, more elegant formulation of the computation of meaning in context in Eq. 3. Vector space. We use a dependency-based vector space that counts a target word and a context word 59 as co-occurring in a sentence if they are connected by an “informative” path in the dependency graph for the sentence.2 We build the space from a Minipar-parsed version of the British National Corpus with dependency parses obtained from Minipar (Lin, 1993). It uses raw co-occurrence counts and 2000 dimensions. Next, we test component-wise multiplication (mult). This operation is more difficult to interpret in terms of vector space, since it does not correspond to the standard inner or outer vector products. The most straightforward interpretation is to reinterpret the second vector as a diagonal matrix, i.e., as a linear transformation of the first vector. Large entries in the second vector increase the weight of the corresponding contexts; small entries decrease it. Mitchell and Lapata (2008) found this method to yield the best results. The th"
W09-0208,S07-1009,0,0.237709,"nce not through a sense label, but through the distance of the token vector to other vectors. A natural question that arises is how vector-based models of token meaning can be evaluated. It is of course possible to apply them to a traditional WSD task. However, this strategy remains vulnerable to all criticism concerning the annotation of categorical word senses, and also does not take advantage of the vector models’ central asset, namely gradedness. Thus, paraphrase-based assessment for models of token meaning was proposed as a representation-neutral disambiguation task that can replace WSD (McCarthy and Navigli, 2007; Mitchell and Lapata, 2008). Given a word token in context and a set of potential paraphrases, the task consists of identifying the subset of valid paraphrases. For example, in the following example, the first paraphrase is appropriate, but the second is not: The appropriateness of paraphrases for words depends often on context: “grab” can replace “catch” in “catch a ball”, but not in “catch a cold”. Structured Vector Space (SVS) (Erk and Padó, 2008) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases. This paper investigates “best-pract"
W09-0208,W06-2503,0,0.0145521,"w occurrences. One prominent approach to this question is the dictionary-based model of token meaning: The different meanings of a word are a set of distinct, disjoint senses enumerated in a lexicon or ontology, such as WordNet. For each new occurrence, determining token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models have been most succe"
W09-0208,P04-1003,0,0.0246093,"token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD). Unfortunately, this task has turned out to be very hard both for human annotators and for machines (Kilgarriff and Rosenzweig, 2000), not at least due to granularity problems with available resources (Palmer et al., 2007; McCarthy, 2006). Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses (Kilgarriff, 1997; Hanks, 2000). An interesting alternative is offered by vector space models of word meaning (Lund and Burgess, 1996; McDonald and Brew, 2004) which characterize the meaning of a word entirely without reference to word senses. Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods. Semantic similarity is then simply distance to vectors of other words. Vector space models have been most successful in modeling the meaning of word types (i.e. in constructing type vectors). The characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, graded, unsupervised measu"
W09-0208,W03-2408,0,0.0281447,"bj subj both 0.5 21.7 20.6 21.1 22.6 21.1 24.5 20.9 20.1 20.1 1 20.7 20.1 20.3 24.8 23.9 24.5 19.5 19.6 19.8 2 23.2 22.9 23.2 25.0 24.4 25.6 23.6 22.5 25.2 5 24.3 24.4 24.4 24.4 24.4 24.3 24.4 24.2 24.5 10 24.2 23.3 23.3 24.2 23.5 20.0 24.3 23.9 24.3 20 21.8 19.7 18.9 21.4 19.8 17.4 21.9 19.6 19.0 Table 2: OOT accuracy on the S EM C OR -PARA dataset across models and reweighting values (best results for each line boldfaced). Random baseline: 19.6. Target type vector baseline: 20.8 need to add direct hypernyms. Direct hypernyms have been used in annotation tasks to characterize WordNet senses (Mihalcea and Chklovski, 2003), an indicator that they are usually close enough in meaning to function as pseudo-paraphrases. Again, we parsed the corpus with Minipar and identified all sense-tagged instances of the verbs from L EX S UB -PARA, to keep the two corpora as comparable as possible. For each instance wi of word w, we collected all synonyms and direct hypernyms of the synset as the set of appropriate paraphrases. The list of synonyms and direct hypernyms of all other senses of w, whether they occur in SemCor or not, were considered inappropriate paraphrases for the instance wi . This method does not provide us wi"
W09-0208,P08-1028,0,0.182264,"l, but through the distance of the token vector to other vectors. A natural question that arises is how vector-based models of token meaning can be evaluated. It is of course possible to apply them to a traditional WSD task. However, this strategy remains vulnerable to all criticism concerning the annotation of categorical word senses, and also does not take advantage of the vector models’ central asset, namely gradedness. Thus, paraphrase-based assessment for models of token meaning was proposed as a representation-neutral disambiguation task that can replace WSD (McCarthy and Navigli, 2007; Mitchell and Lapata, 2008). Given a word token in context and a set of potential paraphrases, the task consists of identifying the subset of valid paraphrases. For example, in the following example, the first paraphrase is appropriate, but the second is not: The appropriateness of paraphrases for words depends often on context: “grab” can replace “catch” in “catch a ball”, but not in “catch a cold”. Structured Vector Space (SVS) (Erk and Padó, 2008) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases. This paper investigates “best-practice” parameter settings for"
W09-0208,D08-1094,1,\N,Missing
W09-0404,W05-0909,0,0.112089,"tion. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which 2 2.1 Textual Entailment for MT Evaluation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between tw"
W09-0404,P03-1021,0,0.00708576,"del. We plan to assess the additional benefit of the full entailment feature set against the T RAD M T feature set extended by a proper lexical similarity metric, such as METEOR. The computation of entailment features is more heavyweight than traditional MT evaluation metrics. We found the speed (about 6 s per hypothesis on a current PC) to be sufficient for easily judging the quality of datasets of the size conventionally used for MT evaluation. However, this may still be too expensive as part of an MT model that directly optimizes some performance measure, e.g., minimum error rate training (Och, 2003). Feature Weights. Finally, we assessed the importance of the different entailment feature groups in the RTE model.1 Since the presence of correlated features makes the weights difficult to interpret, we restrict ourselves to two general observations. First, we find high weights not only for the score of the alignment between hypothesis and reference, but also for a number of syntacto-semantic match and mismatch features. This means that we do get an additional benefit from the presence of these features. For example, features with a negative effect include dropping adjuncts, unaligned root no"
W09-0404,W07-0411,0,0.0306635,"Missing"
W09-0404,E06-1032,0,0.0370947,"quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an M"
W09-0404,P02-1040,0,0.0809369,"combination of lexical and structural features that model the matches and mismatches between system output and reference translation. We use supervised regression models to combine these features and analyze feature weights to obtain further insights into the usefulness of different feature types. Introduction Automatic metrics to assess the quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement"
W09-0404,W08-0309,0,0.108416,"Missing"
W09-0404,2006.amta-papers.25,0,0.157306,"Missing"
W09-0404,P08-1007,0,0.0429762,"of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which 2 2.1 Textual Entailment for MT Evaluation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences ("
W09-0404,P06-1057,0,0.0216052,"valuation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over ∗ This paper is based on work funded by the Defense Advanced Research Pro"
W09-0404,W08-0332,0,0.0303371,"Missing"
W09-0404,P06-1114,0,0.0608647,"tailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. Th"
W09-0404,C08-1066,1,0.823965,"re are also substantial differences between TE and MT evaluation. Crucially, TE assumes the premise and hypothesis to be well-formed sentences, which is not true in MT evaluation. Thus, a possible criticism to the use of TE methods is that the features could become unreliable for ill-formed MT output. However, there is a second difference between the tasks that works to our advantage. Due to its strict compositional nature, TE requires an accurate semantic analysis of all sentence parts, since, for example, one misanalysed negation or counterfactual embedding can invert the entailment status (MacCartney and Manning, 2008). In contrast, human MT judgments behave more additively: failure of a translation with respect to a single semantic dimension (e.g., polarity or tense) degrades its quality, but usually not crucially so. We therefore expect that even noisy entailment features can be predictive in MT evaluation. 2.2 Table 1: Entailment feature groups provided by the Stanford RTE system, with number of features quadratic in the number of systems. On the other hand, it can be trained on more reliable pairwise preference judgments. In a second step, we combine the individual decisions to compute the highest-likel"
W09-0404,N06-1006,1,\N,Missing
W09-1201,burchardt-etal-2006-salsa,1,0.483589,"Missing"
W09-1201,D07-1101,0,0.391748,"Missing"
W09-1201,W09-1202,0,0.0278745,"order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex"
W09-1201,W09-1205,0,0.222475,"al token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Llu´ıs et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen"
W09-1201,W09-1212,1,0.83271,"Missing"
W09-1201,S07-1008,1,0.697359,"Missing"
W09-1201,H05-1066,1,0.168004,"Missing"
W09-1201,W04-2705,1,0.527791,"Missing"
W09-1201,W09-1219,0,0.0294123,"Missing"
W09-1201,H05-1108,1,0.506508,"y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1"
W09-1201,C08-1085,1,0.175934,"Missing"
W09-1201,J05-1004,0,0.213522,"nnotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun"
W09-1201,E09-1087,1,0.646818,"Missing"
W09-1201,W08-2121,1,0.597132,"Missing"
W09-1201,taule-etal-2008-ancora,0,0.543017,"Missing"
W09-1201,cmejrek-etal-2004-prague,1,0.63993,"Missing"
W09-1201,kawahara-etal-2002-construction,1,\N,Missing
W09-1201,J93-2004,0,\N,Missing
W09-1201,D07-1096,1,\N,Missing
W09-2415,W04-3205,0,0.0639941,"unrelated semantic roles. There is a rudimentary frame hierarchy that defines mappings between roles of individual frames,5 but it is far from complete. The situation is similar in PropBank. PropBank does use a small number of semantic roles, but these are again to be interpreted at the level of individual predicates, with little cross-predicate generalization. In contrast, all of the semantic relation inventories discussed in Section 1 contain fewer than 50 types of semantic relations. More generally, semantic relation inventories attempt to generalize relations across wide groups of verbs (Chklovski and Pantel, 2004) and include relations that are not verbcentered (Nastase and Szpakowicz, 2003; Moldovan et al., 2004). Using the same labels for similar semantic relations facilitates supervised learning. For example, a model trained with examples of sell relations should be able to transfer what it has learned to give relations. This has the potential of adding 5 For example, it relates the B UYER role of the C OM frame (verb sell ) to the R ECIPIENT role of the G IVING frame (verb give). MERCE SELL 97 1. People in Hawaii might be feeling &lt;e1>aftershocks&lt;/e1> from that powerful &lt;e2>earthquake&lt;/e2> for weeks"
W09-2415,P08-1027,0,0.0924452,"tical NLP settings, where any relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, som"
W09-2415,J02-3001,0,0.0386032,"This is motivated by modelling considerations. Presumably, the data for OTHER will be very nonhomogeneous. By including it, we force any model of the complete data set to correctly identify the decision boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of mu"
W09-2415,S07-1003,1,0.384989,"Missing"
W09-2415,C92-2082,0,0.060649,"tion will take place in two rounds. In the first round, we will do a coarse-grained search for positive examples for each relation. We will collect data from the Web using a semi-automatic, pattern-based search procedure. In order to ensure a wide variety of example sentences, we will use several dozen patterns per relation. We will also ensure that patterns retrieve both positive and negative example sentences; the latter will help populate the OTHER relation with realistic near-miss negative examples of the other relations. The patterns will be manually constructed following the approach of Hearst (1992) and Nakov and Hearst (2008).6 The example collection for each relation R will be passed to two independent annotators. In order to maintain exclusivity of relations, only examples that are negative for all relations but R will be included as positive and only examples that are negative for all nine relations will be included as OTHER. Next, the annotators will compare their decisions and assess inter-annotator agreement. Consensus will be sought; if the annotators cannot agree on an example it will not be included in the data set, but it will be recorded for future analysis. Finally, two othe"
W09-2415,P08-2047,0,0.0143881,"relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, some subsequent publications tri"
W09-2415,I05-1082,1,0.187851,"nds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relat"
W09-2415,W04-2609,0,0.0615549,"fy noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes c"
W09-2415,P08-1052,1,0.611835,"medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is wh"
W09-2415,C08-1082,1,0.339838,"Missing"
W09-2415,J05-1004,0,0.0280101,"boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of multiple participants and props, while semantic relations are in practice (although not necessarily) binary. The second major difference is the syntactic context. Theories of semantic roles usually d"
W09-2415,P06-1015,1,0.178213,"m of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes clear that context does indeed play a role. Consider, for example, the noun compound wood shed : it may refer either to a shed made of wood, or to a shed of any material used to store wood. This ambiguity is likely to be resolved in particular contexts. In fact, most NLP appli"
W09-2415,D07-1075,0,0.0368294,"annotation, we define a nominal as a noun or a base noun phrase. A base noun phrase is a noun and its pre-modifiers (e.g., nouns, adjectives, determiners). We do not include complex noun phrases (e.g., noun phrases with attached prepositional phrases or relative clauses). For example, lawn is a noun, lawn mower is a base noun phrase, and the engine of the lawn mower is a complex noun phrase. We focus on heads that are common nouns. This emphasis distinguishes our task from much work in IE, which focuses on named entities and on considerably more fine-grained relations than we do. For example, Patwardhan and Riloff (2007) identify categories like Terrorist organization as participants in terror-related semantic relations, which consists predominantly of named entities. We feel that named entities are a specific category of nominal expressions best dealt with using techniques which do not apply to common nouns; for example, they do not lend themselves well to semantic generalization. Figure 1 shows two examples of annotated sentences. The XML tags &lt;e1> and &lt;e2> mark the target nominals. Since all nine proper semantic relations in this task are asymmetric, the ordering of the two nominals must be taken into acco"
W09-2415,W01-0511,0,0.0250487,"CL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one"
W09-2415,P02-1032,0,0.00907258,"stics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and dat"
W09-2415,W09-1401,0,\N,Missing
W09-2415,J02-3004,0,\N,Missing
W09-2415,S10-1006,1,\N,Missing
W09-2415,W04-2412,0,\N,Missing
W09-2501,P05-1074,0,0.0313752,"stitute a major factor in determining entailment. However, we have argued that about half of the true MWEs are decomposable, that is, the part of the alignment that is crucial for entailment can be recovered with a one-to-one alignment link that can be identified even by very limited alignment models. 2 4 We thank Patrick Pantel for granting us access to DIRT. poorly represented represented 0.42 poorly 0.07 rarely 0.06 good 0.05 representatives 0.04 very few 0.04 well 0.02 representative 0.01 Parallel corpora-based paraphrases. An alternative approach to paraphrase acquisition was proposed by Bannard and Callison-Burch (2005). It exploits the variance inherent in translation to extract paraphrases from bilingual parallel corpora. Concretely, it observes translational relationships between a source and a target language and pairs up source language phrases with other source language phrases that translate into the same target language phrases. We applied this method to the large Chinese-English GALE MT evaluation P3/P3.5 corpus (∼2 GB text per language, mostly newswire). The large number of translations makes it impractical to store all observed paraphrases. We therefore filtered the list of paraphrases against the"
W09-2501,W07-1428,0,0.0161426,"of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs and their handling on pr"
W09-2501,W05-1210,0,0.127926,"g et al., 2002). The importance attributed to them is also reflected in a number of workshops (Bond et al., 2003; Tanaka et al., 2004; Moir´on et al., 2006; Gr´egoire et al., 2007). However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications. This paper investigates the impact of MWEs on the “recognition of textual entailment” (RTE) task (Dagan et al., 2006). Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE. A number of papers have suggested that paraphrase knowledge plays a very important role (Bar-Haim et al., 2005; Marsi et al., 2007; Dinu and Wang, 2009). For example, BarHaim et al. (2005) conclude: “Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task.” (1) PRE: He died. HYP: He kicked the bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedin"
W09-2501,W07-1421,0,0.0212269,"to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs and their handling on practical entailment recognition. 2 C"
W09-2501,P98-2127,0,0.0298403,"→ executive director military → naval forces Before we come to actual experiments on the automatic recognition of MWEs in a practical RTE system, we need to consider the prerequisites for this task. As mentioned in Section 2, if an RTE system is to establish multi-word alignments, it requires a knowledge source that provides accurate semantic similarity judgments for “many-to-many” alignments (capital punishment – death penalty) as well as for “one-to-many” alignments (vote – cast ballots). Such similarities are not present in standard lexical resources like WordNet or Dekang Lin’s thesaurus (Lin, 1998). The best class of candidate resources to provide wide-coverage of multi-word similarities seems to be paraphrase resources. In this section, we examine to what extent two of the most widely used paraphrase resource types provide supporting evidence for the true MWEs in the MSR data. We deliberately use corpus-derived, noisy resources, since we are interested in the real-world (rather than idealized) prospects for accurate MWE alignment. In particular when light verbs are involved (file lawsuits) or when modification adds just minor meaning aspects (executive director), we argue that it is su"
W09-2501,N06-1006,1,0.880616,"Missing"
W09-2501,J93-2003,0,0.00952629,"Word Expressions in Alignment Almost all textual entailment recognition models incorporate an alignment procedure that establishes correspondences between the premise and the hypothesis. The computation of word alignments is usually phrased as an optimization task. The search space is based on lexical similarities, but usually extended with structural biases in order to obtain alignments with desirable properties, such as the contiguous alignment of adjacent words, or the mapping of different source words on to different target words. One prominent constraint of the IBM word alignment models (Brown et al., 1993) is functional alignment, that is each target word is mapped onto at most one source word. Other models produce only one-to-one alignments, where both alignment directions must be functional. MWEs that involve many-to-many or one-tomany alignments like Ex. (1) present a problem for such constrained word alignment models. A functional alignment model can still handle cases like Ex. (1) correctly in one direction (from bottom to top), but not in the other one. One-to-one alignments manage neither. Various workarounds have been proposed in the MT literature, such as computing word alignments in b"
W09-2501,D08-1084,1,0.88192,"Missing"
W09-2501,W07-1402,0,0.0213983,"he bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs"
W09-2501,W07-1414,0,0.0204608,"portance attributed to them is also reflected in a number of workshops (Bond et al., 2003; Tanaka et al., 2004; Moir´on et al., 2006; Gr´egoire et al., 2007). However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications. This paper investigates the impact of MWEs on the “recognition of textual entailment” (RTE) task (Dagan et al., 2006). Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE. A number of papers have suggested that paraphrase knowledge plays a very important role (Bar-Haim et al., 2005; Marsi et al., 2007; Dinu and Wang, 2009). For example, BarHaim et al. (2005) conclude: “Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task.” (1) PRE: He died. HYP: He kicked the bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Works"
W09-2501,W07-1412,0,0.0230942,"ch can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs and their handling on practical entailment recognition. 2 CARDINALITY DECOM - POSAB"
W09-2501,E09-1025,0,0.0138437,"to them is also reflected in a number of workshops (Bond et al., 2003; Tanaka et al., 2004; Moir´on et al., 2006; Gr´egoire et al., 2007). However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications. This paper investigates the impact of MWEs on the “recognition of textual entailment” (RTE) task (Dagan et al., 2006). Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE. A number of papers have suggested that paraphrase knowledge plays a very important role (Bar-Haim et al., 2005; Marsi et al., 2007; Dinu and Wang, 2009). For example, BarHaim et al. (2005) conclude: “Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task.” (1) PRE: He died. HYP: He kicked the bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual"
W09-2501,W02-1001,0,\N,Missing
W09-2501,W07-1401,0,\N,Missing
W09-2501,C98-2122,0,\N,Missing
W11-0111,N03-1003,0,0.13299,"Missing"
W11-0111,W05-1209,0,0.163444,"Missing"
W11-0111,E06-1028,0,0.0221759,"rom some corpora is serious exactly because, according to our intuition, the “easier” news agency corpora (like Reuters) are domain102 Corpus Reuters StuttZ D( · |deWac) 0.98 0.93 Die Zeit 0.64 words w with highest P (w)/Q(w) Händler (trader), Börse (exchange), Prozent (per cent), erklärte (stated) DM (German Mark), Prozent (per cent), Millionen (millions), Geschäftsjahr (fiscal year), Milliarden (billions) heißt (means), weiß (knows), läßt (leaves/lets) Table 8: Exp. 2: Domain specificity (KL distance from deWac); typical content words specific. We quantify this intuition with an approach by Ciaramita and Baroni (2006), who propose to model the representativeness of web-crawled corpora as the KL divergence between their Laplacesmoothed unigram distribution P and that of a reference corpus, Q (w ∈ W are vocabulary words): D(P, Q) = X P (w) log w∈W P (w) Q(w) (4) We use the deWac German web corpus (Baroni et al., 2009) as reference, making the idealizing assumption that it is representative for the German language. We interpret a large distance from deWac as domain specificity. The results in Table 8 bear out our hypothesis: Die Zeit is less domain specific than StuttZ, which in turn is less specific than Reu"
W11-0111,P08-1118,0,0.0667294,"Missing"
W11-0111,P10-1122,0,0.0123552,"n and Machine Translation often adopt very different, task-specific semantic processing strategies. Textual entailment (TE) was introduced by Dagan et al. (2006) as a “meta-task” that can subsume a large part of the semantic processing requirements of such applications by providing a generic concept of inference that corresponds to “common sense” reasoning patterns. Textual Entailment is defined as a relation between two natural language utterances (a Premise P and a Hypothesis H) that holds if “a human reading P would infer that H is most likely true”. See, e.g., the ACL “challenge paper” by Sammons et al. (2010) for further details. The successive TE workshops that have taken place yearly since 2005 have produced annotation for English which amount to a total of several thousand entailing Premise-Hypothesis sentence pairs, which we will call entailment pairs: (1) P: Swedish bond yields end 21 basis points higher. H: Swedish bond yields rose further. From the machine learning perspective assumed by many approaches to TE, this is a very small number of examples, given the complex nature of entailment. Given the problems of manual annotation, therefore, Burger and Ferro (2005) proposed to take advantage"
W11-0111,W07-1401,0,\N,Missing
W11-0128,H92-1045,0,0.284768,"estions addressed are the conditions under which polysemy arises, the representation of polysemy in the semantic lexicon, disambiguation mechanisms in the syntax-semantics interface, and subcategories of polysemy. The distinction between polysemy and homonymy also has important potential ramifications for computational linguistics, in particular for Word Sense Disambiguation (WSD). Notably, Ide and Wilks (2006) argue that WSD should focus on modeling homonymous sense distinctions, which are easy to make and provide most benefit. Another case in point is the one-sense-per-discourse hypothesis (Gale et al., 1992), which claims that within a discourse, instances of a word will strongly tend towards realizing the same sense. This hypothesis seems to apply primarily to homonyms, as pointed out by Krovetz (1998). Unfortunately, the distinction between polysemy and homonymy is still very much an unsolved question. The discussion in the theoretical literature focuses mostly on clear-cut examples and avoids the broader issue. Work on WSD, and in computational linguistics more generally, almost exclusively builds on the WordNet (Fellbaum, 1998) word sense inventory, which lists an unstructured set of senses f"
W11-0128,H05-1051,0,0.0723448,"Missing"
W11-0128,P95-1026,0,0.084247,"Missing"
W11-3605,P04-1036,0,0.0132822,"the Google Translate output by replacing them with the English word sharing the same Soundex code that has the highest frequency in the English document collection. 2.4 Disambiguation (GTR+SoExNER+LD(mod)) Generally, a word that has been wrongly transliterated from Urdu maps onto the same Soundex code as several English words. The median number of English words per transliteration is 7. This can be seen as a sort of ambiguity, and the strategy adopted by the previous models is to just choose the most frequent candidate, similar to the “predominant” sense baseline in word sense disambiguation (McCarthy et al., 2004). We found however that the most frequent candidate is often wrong, since Soundex conflates fairly different words (cf. Section 2.2). For example, Subhas, the first name of an Indian freedom fighter, receives the soundex code S120 but it is mapped onto the English term Space (f req=7243) instead of Subhas (f req=2853). We therefore experimented with a more informed strategy that chooses the English candidate based on two variants of Levenshtein distance. The first model, GTR+SoExNER+LD, uses standard Levenshtein distance with a cost of 1 for 1 http://translate.google.com. All queries were tran"
W11-3605,W02-1201,0,0.0157249,"ies in one language and return text documents in a different language. CLIR is of considerable practical importance in countries with many languages like India. One of the most widely used languages is Urdu, the official language of five Indian states as well as the national language of Pakistan. There are around 60 million speakers of Urdu – 48 million in India and 11 million in Pakistan (Lewis, 2009). Despite this large number of speakers, NLP for Urdu is still at a fairly early stage (Hussain, 2008). Studies have been conducted on POS tagging (Sajjad and Schmid, 2009), corpus construction (Becker and Riaz, 2002), word segmentation (Durrani and Hussain, 2010), lexicographic 2 Translation Strategies for Urdu–English We present a series of strategies for translating Urdu queries into English so that they can be pre25 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 25–29, Chiang Mai, Thailand, November 8-12, 2011. 2.3 sented to a monolingual English IR system that works on some English document collection. Inspection of the strategies’ errors led us to develop a hierarchy of increasingly sophisticated strategies. 2.1 An analysis of the output of the GTR+SoEx mo"
W11-3605,N10-1077,0,0.0318057,"ts in a different language. CLIR is of considerable practical importance in countries with many languages like India. One of the most widely used languages is Urdu, the official language of five Indian states as well as the national language of Pakistan. There are around 60 million speakers of Urdu – 48 million in India and 11 million in Pakistan (Lewis, 2009). Despite this large number of speakers, NLP for Urdu is still at a fairly early stage (Hussain, 2008). Studies have been conducted on POS tagging (Sajjad and Schmid, 2009), corpus construction (Becker and Riaz, 2002), word segmentation (Durrani and Hussain, 2010), lexicographic 2 Translation Strategies for Urdu–English We present a series of strategies for translating Urdu queries into English so that they can be pre25 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 25–29, Chiang Mai, Thailand, November 8-12, 2011. 2.3 sented to a monolingual English IR system that works on some English document collection. Inspection of the strategies’ errors led us to develop a hierarchy of increasingly sophisticated strategies. 2.1 An analysis of the output of the GTR+SoEx model showed that the model indeed ensured that a"
W11-3605,W04-2905,0,0.0757525,"Missing"
W11-3605,E09-1079,0,0.0159523,"search is the study of systems that accept queries in one language and return text documents in a different language. CLIR is of considerable practical importance in countries with many languages like India. One of the most widely used languages is Urdu, the official language of five Indian states as well as the national language of Pakistan. There are around 60 million speakers of Urdu – 48 million in India and 11 million in Pakistan (Lewis, 2009). Despite this large number of speakers, NLP for Urdu is still at a fairly early stage (Hussain, 2008). Studies have been conducted on POS tagging (Sajjad and Schmid, 2009), corpus construction (Becker and Riaz, 2002), word segmentation (Durrani and Hussain, 2010), lexicographic 2 Translation Strategies for Urdu–English We present a series of strategies for translating Urdu queries into English so that they can be pre25 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 25–29, Chiang Mai, Thailand, November 8-12, 2011. 2.3 sented to a monolingual English IR system that works on some English document collection. Inspection of the strategies’ errors led us to develop a hierarchy of increasingly sophisticated strategies. 2.1"
W11-3605,I08-7017,0,0.0238511,"roduction ð ð Cross-language information retrieval (CLIR) research is the study of systems that accept queries in one language and return text documents in a different language. CLIR is of considerable practical importance in countries with many languages like India. One of the most widely used languages is Urdu, the official language of five Indian states as well as the national language of Pakistan. There are around 60 million speakers of Urdu – 48 million in India and 11 million in Pakistan (Lewis, 2009). Despite this large number of speakers, NLP for Urdu is still at a fairly early stage (Hussain, 2008). Studies have been conducted on POS tagging (Sajjad and Schmid, 2009), corpus construction (Becker and Riaz, 2002), word segmentation (Durrani and Hussain, 2010), lexicographic 2 Translation Strategies for Urdu–English We present a series of strategies for translating Urdu queries into English so that they can be pre25 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 25–29, Chiang Mai, Thailand, November 8-12, 2011. 2.3 sented to a monolingual English IR system that works on some English document collection. Inspection of the strategies’ errors led u"
W11-3605,C04-1137,0,0.0617671,"Missing"
W11-3605,I08-6010,0,\N,Missing
W11-3605,J98-4003,0,\N,Missing
W12-1707,J10-4006,0,0.705377,"curring in similar contexts are semantically similar. In distributional models, the meaning of a word is represented as a vector whose dimensions represent features of its linguistic context. These features can be chosen in different ways; popular choices are simple words (Schütze, 1992) or lexicalized dependency relations (Lin, 1998; Padó and Lapata, 2007). Semantic similarity can then be approximated by vector similarity using a wide range of similarity metrics (Lee, 1999). 3.1.1 Distributional Memory A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). DM does not immediately construct vectors for words. Instead, it extracts a three-dimensional tensor of weighted wordlink-word tuples each of which is mapped onto a score by a function σ : hw1 l w2 i → R+ . For example, hpencil obj usei has a higher weight than helephant obj usei. The set of links can be defined in different ways, yielding various DM instances. Baroni and Lenci present DepDM (mainly syntactic links such as subj _tr ), LexDM (strongly lexicalized links, e.g., such_as), or TypeDM (syntactic and lexicalized links).3 The benefit of the tensor-based representation is that it is"
W12-1707,D10-1029,0,0.0205102,"o make a prediction. For the distributional models, while both auspacken and verpacken (wrap) are highly associated with Geschenk, the most strongly associated actions of Geburtstagskind are extraordinarily diverse, e.g.: bekommen (receive), sagen (say), auffuttern (eat up), herumkommandieren (boss around), ausblasen (blow out). Neither of the events of interest though were highly associated. 7 Future Work We see a possible improvement in the choice of the number of fillers, with which we construct the prototype vectors. A smaller number might lead to less noisy prototypes. It has been shown (Bergsma et al., 2010) that the meaning of the prefix verb can be accurately predicted using the stem’s vector, when compositionality applies. We suspect covert events that are prefix verbs to suffer from sparser representations than the vectors of their stem. E.g., absaugen (vacuum off ) is much less frequent than the semantically nearly identical saugen (vacuum). Thus, by leveraging the richer representation of the stem, our distributional models could more likely assign the correct event. 8 Conclusions We have presented a contrastive study of two classes of computational models, probabilistic and distributional"
W12-1707,C10-1011,0,0.0354358,"th a low-typicality covert event. This results in 2 hightypicality tuples and 2 low-typicality tuples in each set. Typical events (e) were elicited by 20 participants given the corresponding object o, subjects were elicited by 10 participants as the prototypical agents subjects for each e, o combination. The experiments yielded a main effect of typicality on self-paced reading times (Zarcone and Padó, 2011) 74 DM for German Since DM exists only for English, we constructed a German analog using the 884M word SD E WAC web corpus (Faaß et al., 2010) parsed with the MATE German dependency parser (Bohnet, 2010). From this corpus, we extract 55M instances of simple syntactic relations (subj_tr, subj_intr, obj, iobj, comp, nmod) and 104M instances of lexicalized patterns such as noun–prep–noun e.g. hRecht auf Auskunfti (hright to informationi), or adj–noun-(of)noun such as hstrittig Entscheidung Schiedsrichteri (hcontested decision refereei). These lexicalized patterns make our model roughly similar to the English TypeDM model (Sec. 3.1.1). As for σ, we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). The LMI of a triple is defined as Ow1 lw2 log(Ow1 lw2 /Ew1 lw2 ), where Ow"
W12-1707,P07-1028,0,0.0223308,"itive hypothesis about the form of semantic representations (Lenci, 2008): the distributional behavior of a word reflects its semantic behavior but is also a direct correlate of its semantic content at the cognitive level. Also, similarity-based models are highly compatible with known features of human cognition, such as graded category membership (Rosch, 1975) or multiple sense activation (Erk, 2010). Their cognitive relevance for language has been supported by studies of child lexical development (Li et al., 2004), category-related deficits (Vigliocco et al., 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al. (2007) and Baroni and Lenci (2010) for a review). 3.4 Modeling Logical Metonymy with ECU 3.4.1 Logical Metonymy as Thematic Fit The hypothesis that we follow in this paper is that the ECU model can also be used, with modifications, to predict the interpretation of logical metonymy. The underlying assumption is that the interpretation 73 of logical metonymy is essentially the recovery of a covert event with a maximal thematic fit (hightypicality) and can thus make use of ECU’s mechanisms to treat verb-argument composition. St"
W12-1707,W10-2803,0,0.0190209,"tinguish between the two contexts differing in thematic fit with the object. 3.3 Cognitive relevance Similarity-based models build upon the Distributional Hypothesis, which, in its strong version, is a cognitive hypothesis about the form of semantic representations (Lenci, 2008): the distributional behavior of a word reflects its semantic behavior but is also a direct correlate of its semantic content at the cognitive level. Also, similarity-based models are highly compatible with known features of human cognition, such as graded category membership (Rosch, 1975) or multiple sense activation (Erk, 2010). Their cognitive relevance for language has been supported by studies of child lexical development (Li et al., 2004), category-related deficits (Vigliocco et al., 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al. (2007) and Baroni and Lenci (2010) for a review). 3.4 Modeling Logical Metonymy with ECU 3.4.1 Logical Metonymy as Thematic Fit The hypothesis that we follow in this paper is that the ECU model can also be used, with modifications, to predict the interpretation of logical metonymy. The underlying assumption is that the int"
W12-1707,faass-etal-2010-design,0,0.0134089,"e is matched once with a high-typicality covert event and once with a low-typicality covert event. This results in 2 hightypicality tuples and 2 low-typicality tuples in each set. Typical events (e) were elicited by 20 participants given the corresponding object o, subjects were elicited by 10 participants as the prototypical agents subjects for each e, o combination. The experiments yielded a main effect of typicality on self-paced reading times (Zarcone and Padó, 2011) 74 DM for German Since DM exists only for English, we constructed a German analog using the 884M word SD E WAC web corpus (Faaß et al., 2010) parsed with the MATE German dependency parser (Bohnet, 2010). From this corpus, we extract 55M instances of simple syntactic relations (subj_tr, subj_intr, obj, iobj, comp, nmod) and 104M instances of lexicalized patterns such as noun–prep–noun e.g. hRecht auf Auskunfti (hright to informationi), or adj–noun-(of)noun such as hstrittig Entscheidung Schiedsrichteri (hcontested decision refereei). These lexicalized patterns make our model roughly similar to the English TypeDM model (Sec. 3.1.1). As for σ, we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). The LMI of a"
W12-1707,W11-0115,0,0.0202375,"istic models can account for compositionality by estimating conditional probabilities. Com3 −1 l is used to denote the inverse link of l (i.e., exchanging the positions of w1 and w2 ). 72 positionality is less straightforward in a similaritybased distributional model, because similarity-based distributional models traditionally model meaning at word level. Nevertheless, the last years have seen a wave of distributional models which make progress at building compositional representations of higher-level structures such as noun-adjective or verb-argument combinations (Mitchell and Lapata, 2010; Guevara, 2011; Reddy et al., 2011). 3.2.1 Expectation Composition and Update Lenci (2011) presents a model to predict the degree of thematic fit for verb-argument combinations: the Expectation Composition and Update (ECU) model. More specifically, the goal of ECU is explain how the choice of a specific subject for a given verb impacts the semantic expectation for possible objects. For example, the verb draw alone might have fair, but not very high, expectations for the two possible objects landscape and card. When it is combined with the subject painter, the resulting phrase painter draw the expectation fo"
W12-1707,J03-2004,0,0.458445,"linguistic entities as co-occurrence vectors and phrase interpretation as a vector similarity maximization problem. Distributional models typically do not require any independence assumptions, and include second-order co-occurrences. At the same time, how to integrate context into the vector computation is essentially an open research question (Mitchell and Lapata, 2010). In this paper, we provide the first (to our knowledge) distributional model of logical metonymy by extending the context update of Lenci’s ECU model (Lenci, 2011). We compare this model to a previous probabilistic approach (Lapata and Lascarides, 2003a; Lapata et al., 2003b). In contrast to most experimental studies on logical metonymy, which deal with English data (with the exception of Lapata et al. (2003b)), we focus on German. We estimate our models on a large web corpus and evaluate them on a psycholinguistic dataset (Zarcone and Padó, 2011; Zarcone et al., 2012). The task we use to evaluate our models is to distinguish covert events with a high typicality / thematic fit (e.g. The student finished the beer −→ drinking) from low typicality / thematic fit covert events (−→ brewing). 2.1 Lapata et al. develop a model which we will refer"
W12-1707,P99-1004,0,0.0291138,"enting word meaning. It builds on the Distributional Hypothesis (Harris, 1954; Miller and Charles, 1991) which states that words occurring in similar contexts are semantically similar. In distributional models, the meaning of a word is represented as a vector whose dimensions represent features of its linguistic context. These features can be chosen in different ways; popular choices are simple words (Schütze, 1992) or lexicalized dependency relations (Lin, 1998; Padó and Lapata, 2007). Semantic similarity can then be approximated by vector similarity using a wide range of similarity metrics (Lee, 1999). 3.1.1 Distributional Memory A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). DM does not immediately construct vectors for words. Instead, it extracts a three-dimensional tensor of weighted wordlink-word tuples each of which is mapped onto a score by a function σ : hw1 l w2 i → R+ . For example, hpencil obj usei has a higher weight than helephant obj usei. The set of links can be defined in different ways, yielding various DM instances. Baroni and Lenci present DepDM (mainly syntactic links such as subj _tr ), LexDM (strongl"
W12-1707,W11-0607,0,0.817509,"co-occurrence into account1 . In contrast, distributional models represent linguistic entities as co-occurrence vectors and phrase interpretation as a vector similarity maximization problem. Distributional models typically do not require any independence assumptions, and include second-order co-occurrences. At the same time, how to integrate context into the vector computation is essentially an open research question (Mitchell and Lapata, 2010). In this paper, we provide the first (to our knowledge) distributional model of logical metonymy by extending the context update of Lenci’s ECU model (Lenci, 2011). We compare this model to a previous probabilistic approach (Lapata and Lascarides, 2003a; Lapata et al., 2003b). In contrast to most experimental studies on logical metonymy, which deal with English data (with the exception of Lapata et al. (2003b)), we focus on German. We estimate our models on a large web corpus and evaluate them on a psycholinguistic dataset (Zarcone and Padó, 2011; Zarcone et al., 2012). The task we use to evaluate our models is to distinguish covert events with a high typicality / thematic fit (e.g. The student finished the beer −→ drinking) from low typicality / themat"
W12-1707,P98-2127,0,0.235227,"milarity-based models 3.1 Distributional semantics Distributional or vector space semantics (Turney and Pantel, 2010) is a framework for representing word meaning. It builds on the Distributional Hypothesis (Harris, 1954; Miller and Charles, 1991) which states that words occurring in similar contexts are semantically similar. In distributional models, the meaning of a word is represented as a vector whose dimensions represent features of its linguistic context. These features can be chosen in different ways; popular choices are simple words (Schütze, 1992) or lexicalized dependency relations (Lin, 1998; Padó and Lapata, 2007). Semantic similarity can then be approximated by vector similarity using a wide range of similarity metrics (Lee, 1999). 3.1.1 Distributional Memory A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). DM does not immediately construct vectors for words. Instead, it extracts a three-dimensional tensor of weighted wordlink-word tuples each of which is mapped onto a score by a function σ : hw1 l w2 i → R+ . For example, hpencil obj usei has a higher weight than helephant obj usei. The set of links can be def"
W12-1707,J07-2002,1,0.792983,"sed models 3.1 Distributional semantics Distributional or vector space semantics (Turney and Pantel, 2010) is a framework for representing word meaning. It builds on the Distributional Hypothesis (Harris, 1954; Miller and Charles, 1991) which states that words occurring in similar contexts are semantically similar. In distributional models, the meaning of a word is represented as a vector whose dimensions represent features of its linguistic context. These features can be chosen in different ways; popular choices are simple words (Schütze, 1992) or lexicalized dependency relations (Lin, 1998; Padó and Lapata, 2007). Semantic similarity can then be approximated by vector similarity using a wide range of similarity metrics (Lee, 1999). 3.1.1 Distributional Memory A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). DM does not immediately construct vectors for words. Instead, it extracts a three-dimensional tensor of weighted wordlink-word tuples each of which is mapped onto a score by a function σ : hw1 l w2 i → R+ . For example, hpencil obj usei has a higher weight than helephant obj usei. The set of links can be defined in different ways,"
W12-1707,C00-2094,0,0.105025,"Missing"
W12-1707,I11-1024,0,0.0209367,"n account for compositionality by estimating conditional probabilities. Com3 −1 l is used to denote the inverse link of l (i.e., exchanging the positions of w1 and w2 ). 72 positionality is less straightforward in a similaritybased distributional model, because similarity-based distributional models traditionally model meaning at word level. Nevertheless, the last years have seen a wave of distributional models which make progress at building compositional representations of higher-level structures such as noun-adjective or verb-argument combinations (Mitchell and Lapata, 2010; Guevara, 2011; Reddy et al., 2011). 3.2.1 Expectation Composition and Update Lenci (2011) presents a model to predict the degree of thematic fit for verb-argument combinations: the Expectation Composition and Update (ECU) model. More specifically, the goal of ECU is explain how the choice of a specific subject for a given verb impacts the semantic expectation for possible objects. For example, the verb draw alone might have fair, but not very high, expectations for the two possible objects landscape and card. When it is combined with the subject painter, the resulting phrase painter draw the expectation for the object landscap"
W12-1707,zarcone-lenci-2008-computational,1,0.721901,"e form of semantic representations (Lenci, 2008): the distributional behavior of a word reflects its semantic behavior but is also a direct correlate of its semantic content at the cognitive level. Also, similarity-based models are highly compatible with known features of human cognition, such as graded category membership (Rosch, 1975) or multiple sense activation (Erk, 2010). Their cognitive relevance for language has been supported by studies of child lexical development (Li et al., 2004), category-related deficits (Vigliocco et al., 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al. (2007) and Baroni and Lenci (2010) for a review). 3.4 Modeling Logical Metonymy with ECU 3.4.1 Logical Metonymy as Thematic Fit The hypothesis that we follow in this paper is that the ECU model can also be used, with modifications, to predict the interpretation of logical metonymy. The underlying assumption is that the interpretation 73 of logical metonymy is essentially the recovery of a covert event with a maximal thematic fit (hightypicality) and can thus make use of ECU’s mechanisms to treat verb-argument composition. Strong evidence for this assumption has b"
W12-1707,C98-2122,0,\N,Missing
W13-0116,J05-3002,0,0.0265318,"dination and subordination form distinct patterns of entity mentions which can be used to predict local text structure. Generation and Summarization. We believe that our results may be useful to the natural language generation and summarization communities. In generation, many systems assume overgenerate-and-rank 9 approach to sentence planning (for example, see Stent et al. (2004)). The description of features given in our work may help to create better ranking systems or even direct the generation of complex and compound sentences, in the spirit of Stent and Molina (2009). In summarization, Barzilay and McKeown (2005) present a sentence fusion technique for multidocument summarization which needs to restructure sentences to improve text coherence. Restructuring is currently done without regard to the underlying discourse structure. We believe that the features that we have identified can introduce a bias towards more appropriate structures during sentence fusion. 6 Conclusions In this paper, we have reported on an examination of various semantic and discourse structure-based factors and their effect on the choice of clause combination (subordination vs. coordination) and the direction of relation within su"
W13-0116,W11-2805,0,0.0223615,"in their syntactic position in the sentence (Matthiessen and Thompson, 1988): key events that are necessary for the understanding of the story cannot be expressed as subordinate clauses. If this holds, it could be expected that such events have more salient participants of the discourse as arguments, and that their discourse status is at least partially determined by the salience of their participants. We assess the salience of participants with a total of 20 features, using some of the features used in anaphora resolution tasks: participant frequency and distance to the previous mention (see Chiarcos (2011) and Mitkov (1998), among others). Participant frequency should show how salient the participant is for the overall document. The distance to the previous mention helps to trace down smaller topics and characterize the participant’s role in the local discourse. Adjunct features cover the expression of adjuncts of the predicates. This group is designed to test whether presence of non-clausal modifiers of predicates influence their syntactic combination. The idea behind including these features is two-fold: on the one hand, they might account for the size of the clauses that should be combined."
W13-0116,J97-1003,0,0.149048,"trast, we focus on the correlation between the syntactic structure and the properties of events directly, since both types of clause combination may be used to encode the same rhetoric relation. We think that while the influence of pragmatic factors investigated by Grote et al. (1997) may be significant, we chose to explore other types of features in this study. Lexical Models of Coherence Another direction of research of coherence relations within discourse is represented by Barzilay and Lapata (2008). They show that coherent discourse is characterized by chains of mentions of same entities. Hearst (1997) show that event chains that are formed only by the mentions of the same lexical item mirror the global structure of texts and can be used for discourse segmentation. The “shared participant” features that we use are similar to the approach of these studies. However, our work shows that coordination and subordination form distinct patterns of entity mentions which can be used to predict local text structure. Generation and Summarization. We believe that our results may be useful to the natural language generation and summarization communities. In generation, many systems assume overgenerate-an"
W13-0116,J93-2004,0,0.0467888,"nsiders only the training set, it is amenable to overfitting. We therefore add a second kind of analysis that evaluates the model trained on an unseen test set. As the figure of merit, we use simple accuracy (percentage of correctly predicted clause combination types) and compare it against two different baselines (Section 4). 2.3 Corpus We run our training and testing on the release 4.0 of the OntoNotes corpus (Pradhan et al. (2007)). It contains several layers of annotation, including the PropBank annotation of predicate-argument structures (Palmer et al., 2005), Penn Treebank-style parses (Marcus et al., 1993), and a coreference annotation layer (BBN Technologies, 2007). The WSJ sections 00, 02-04, 09-12, 14, and 17 are used for training and section 20 is used for testing. There are in total 732 documents in the training part of the corpus and 76 in the testing subcorpus. Documents include an average of 46.4 sentences and 109.3 predicates, respectively. 3 Training corpus Subordinate pairs 7691 Coordinate pairs 2187 Other pairs 625 Total number of pairs 10530 Testing corpus Subordinate pairs Coordinate pairs Other pairs Total number of pairs 736 182 61 979 Table 1: Training and testing corpora Our t"
W13-0116,P98-2143,0,0.0577564,"osition in the sentence (Matthiessen and Thompson, 1988): key events that are necessary for the understanding of the story cannot be expressed as subordinate clauses. If this holds, it could be expected that such events have more salient participants of the discourse as arguments, and that their discourse status is at least partially determined by the salience of their participants. We assess the salience of participants with a total of 20 features, using some of the features used in anaphora resolution tasks: participant frequency and distance to the previous mention (see Chiarcos (2011) and Mitkov (1998), among others). Participant frequency should show how salient the participant is for the overall document. The distance to the previous mention helps to trace down smaller topics and characterize the participant’s role in the local discourse. Adjunct features cover the expression of adjuncts of the predicates. This group is designed to test whether presence of non-clausal modifiers of predicates influence their syntactic combination. The idea behind including these features is two-fold: on the one hand, they might account for the size of the clauses that should be combined. On the other hand,"
W13-0116,J05-1004,0,0.0297231,"d with the chi-square test. Since this analysis considers only the training set, it is amenable to overfitting. We therefore add a second kind of analysis that evaluates the model trained on an unseen test set. As the figure of merit, we use simple accuracy (percentage of correctly predicted clause combination types) and compare it against two different baselines (Section 4). 2.3 Corpus We run our training and testing on the release 4.0 of the OntoNotes corpus (Pradhan et al. (2007)). It contains several layers of annotation, including the PropBank annotation of predicate-argument structures (Palmer et al., 2005), Penn Treebank-style parses (Marcus et al., 1993), and a coreference annotation layer (BBN Technologies, 2007). The WSJ sections 00, 02-04, 09-12, 14, and 17 are used for training and section 20 is used for testing. There are in total 732 documents in the training part of the corpus and 76 in the testing subcorpus. Documents include an average of 46.4 sentences and 109.3 predicates, respectively. 3 Training corpus Subordinate pairs 7691 Coordinate pairs 2187 Other pairs 625 Total number of pairs 10530 Testing corpus Subordinate pairs Coordinate pairs Other pairs Total number of pairs 736 182"
W13-0116,P09-1077,0,0.0184257,"fferences between syntactic status of clauses to be combined. However, (Taboada, 2006) shows that some rhetorical relations are often expressed without any discourse cue, and such parameters of sentence structure as the order of phrases and their syntactic mode of combination become significant for the expression of rhetoric relation. There are several studies that consider syntactic means of expression of particular rhetorical relations. In particular, Grote et al. (1997) describe how syntactic structure and ordering of clauses correspond to the pragmatic subtypes of the Concession relation. Pitler et al. (2009) show that pairs of words taken from sentences linked by discourse relations, as well as Levin classes of verbs of the sentences and sentiment polarity information is useful for the prediction of implicit relations. The same authors also look into various entity-based features and show that again lexical information about mentioned entities correlates with the choice of discourse relation. In contrast, we focus on the correlation between the syntactic structure and the properties of events directly, since both types of clause combination may be used to encode the same rhetoric relation. We thi"
W13-0116,W09-3941,0,0.0135848,"studies. However, our work shows that coordination and subordination form distinct patterns of entity mentions which can be used to predict local text structure. Generation and Summarization. We believe that our results may be useful to the natural language generation and summarization communities. In generation, many systems assume overgenerate-and-rank 9 approach to sentence planning (for example, see Stent et al. (2004)). The description of features given in our work may help to create better ranking systems or even direct the generation of complex and compound sentences, in the spirit of Stent and Molina (2009). In summarization, Barzilay and McKeown (2005) present a sentence fusion technique for multidocument summarization which needs to restructure sentences to improve text coherence. Restructuring is currently done without regard to the underlying discourse structure. We believe that the features that we have identified can introduce a bias towards more appropriate structures during sentence fusion. 6 Conclusions In this paper, we have reported on an examination of various semantic and discourse structure-based factors and their effect on the choice of clause combination (subordination vs. coordi"
W13-0116,P04-1011,0,0.0222543,"ons of the same lexical item mirror the global structure of texts and can be used for discourse segmentation. The “shared participant” features that we use are similar to the approach of these studies. However, our work shows that coordination and subordination form distinct patterns of entity mentions which can be used to predict local text structure. Generation and Summarization. We believe that our results may be useful to the natural language generation and summarization communities. In generation, many systems assume overgenerate-and-rank 9 approach to sentence planning (for example, see Stent et al. (2004)). The description of features given in our work may help to create better ranking systems or even direct the generation of complex and compound sentences, in the spirit of Stent and Molina (2009). In summarization, Barzilay and McKeown (2005) present a sentence fusion technique for multidocument summarization which needs to restructure sentences to improve text coherence. Restructuring is currently done without regard to the underlying discourse structure. We believe that the features that we have identified can introduce a bias towards more appropriate structures during sentence fusion. 6 Co"
W13-0116,J08-1001,0,\N,Missing
W13-0116,C98-2138,0,\N,Missing
W13-0125,W10-0508,0,0.0396313,"Missing"
W13-0125,P05-1074,0,0.0180667,"(e.g. basic level terms such as table), the problem is alleviated in the software domain where abbreviations and English loanwords that can be substituted easily are frequent (examples 2/1/1, 2/1/2, 10/2/1 in Table 4). The most frequent change was the replacement of verbs by synonyms and nouns by synonyms or hypernyms, as in examples 3/3/1 and 9/5/2. Some turkers modified both syntax and lexemes to vary support verb constructions (5/4/2). While these phenomena are all “generic” paraphrasing devices that have been observed in previous studies on English and newswire text (Lin and Pantel, 2002; Bannard and Callison-Burch, 2005), we find two more classes of paraphrasing patterns that are specific to German and the social media domain, respectively. Prominent among German-specific changes are the large number of nominalisations (8/3/2) as well as active/passive switches (13/5/2). Next to the regular passive construction with the auxiliary werden, we often see “pseudo-passives” which use lassen combined with the reflexivised verb (4/4/2). As for domain-specific patterns, we frequently observe the alternation of interrogative and declarative sentences (17/3/2) noted before which is caused by the tendency of the original"
W13-0125,D08-1007,0,0.0326766,"Missing"
W13-0125,W08-0906,0,0.0323211,"It has rarely been used for Textual Entailment, though, since high-quality crowdsourcing relies on the ability to formulate the task in layman’s terms, which is challenging for entailment. We avoided this problem by asking turkers to provide summaries and paraphrases in two separate steps. Wang and Callison-Burch (2010) also use crowdsourcing to collect hypotheses for TE. In contrast to us, they do not ask turkers for full summaries and paraphrases, but have them extract facts from texts and create counter-facts from facts by inserting negations, using antonyms, or changing adverbs. Finally, Bernhard and Gurevych (2008) present a study on data that is similar to ours. Their goal is the automatic collection of paraphrases for English questions on social Q&A sites. Employing similar methods to us (e.g., word overlap and edit distance), they achieve very good results. Their task is simpler in that in concentrates on paraphrase relations among statements rather than summarisation relations between texts and statements. 6 Conclusions This paper makes two contributions. The first one is a freely available dataset5 for Textual Entailment tasks which covers (a) a new language, namely German; and (b), a new genre, na"
W13-0125,P06-1114,0,0.0124178,"that H is most likely true” (Dagan et al., 2005). Example 1 shows a positive entailment (T entails H1 ) and a negative entailment (T does not entail H2 ). (1) T: Yoko Ono unveiled a bronze statue of her late husband, John Lennon, to complete the official renaming of England’s Liverpool Airport as Liverpool John Lennon Airport. H1 : Yoko Ono is John Lennon’s widow. H2 : John Lennon renamed Liverpool Airport. The appeal of Textual Entailment is that it can arguably meet a substantial part of the semantic processing requirements of a range of language processing tasks such as Question Answering (Harabagiu and Hickl, 2006), Information Extraction (Romano et al., 2006), or Summarisation (Harabagiu et al., 2007). Consequently, there is now a research community that works on and improves Textual Entailment technology. In this spirit, the main TE forum, the yearly Recognising Textual Entailment (RTE) Challenge, has created a number of datasets that incorporate the properties of particular tasks, such as Semantic Search in RTE-5 (Bentivogli et al., 2009) or Novelty Detection in RTE-7 (Bentivogli et al., 2011). At the same time, work on RTE on has focused almost exclusively on English. There is at most a handful of s"
W13-0125,P10-4008,0,0.0269965,"problem statements) corresponding to the original posts. The search for relevant posts given a query can be phrased as a TE problem as follows: queries are hypotheses that are entailed by forum posts (texts) T iff the forum post is relevant for the query (Pe˜nas et al., 2008). Plan of the paper. Section 2 defines the task in more detail and describes the rationale behind our definition of the crowdsourcing tasks. Section 3 provides a detailed analysis of the queries that were produced by crowdsourcing. Section 4 assesses the difficulty of the dataset by modelling it with the RTE system EDITS (Kouylekov and Negri, 2010). Finally we relate our study to prior work and sum up. 2 2.1 Creating a German Social Media TE Dataset with Crowdsourcing Rationale As mentioned above, the promise of TE lies in its ability to model NLP tasks. One of the best-established of these tasks is search, which has been a part of the RTE challenges since RTE-5 (Bentivogli et al., 2009). In this setup, given a query statement and a set of documents, a document is relevant if it entails the query. That is, the documents serve as candidate texts T for a hypothesis H given by the query. We apply this setup to social media texts that discu"
W13-0125,N10-1045,0,0.0131151,"chnology. In this spirit, the main TE forum, the yearly Recognising Textual Entailment (RTE) Challenge, has created a number of datasets that incorporate the properties of particular tasks, such as Semantic Search in RTE-5 (Bentivogli et al., 2009) or Novelty Detection in RTE-7 (Bentivogli et al., 2011). At the same time, work on RTE on has focused almost exclusively on English. There is at most a handful of studies on Textual Entailment in other languages, notably German and Italian (Wang and Neumann, 2008; Negri et al., 2009; Bos et al., 2009) as well as a study on cross-lingual entailment (Mehdad et al., 2010).1 Consequently, virtually no TE technology is available for non-English languages. What is more, it is not clear how well existing algorithms for English RTE carry over to other languages, which might show very different types of surface variation from English. The same limitation exists in terms of genre/register. Virtually all existing datasets have been created from “clean” corpora – that is, properly tokenised, grammatical text, notably Wikipedia. Again, the question arises how well TE 1 There is also a translation of the RTE-3 dataset into German, but it is so far unpublished, although a"
W13-0125,E06-1052,0,0.0245011,"ample 1 shows a positive entailment (T entails H1 ) and a negative entailment (T does not entail H2 ). (1) T: Yoko Ono unveiled a bronze statue of her late husband, John Lennon, to complete the official renaming of England’s Liverpool Airport as Liverpool John Lennon Airport. H1 : Yoko Ono is John Lennon’s widow. H2 : John Lennon renamed Liverpool Airport. The appeal of Textual Entailment is that it can arguably meet a substantial part of the semantic processing requirements of a range of language processing tasks such as Question Answering (Harabagiu and Hickl, 2006), Information Extraction (Romano et al., 2006), or Summarisation (Harabagiu et al., 2007). Consequently, there is now a research community that works on and improves Textual Entailment technology. In this spirit, the main TE forum, the yearly Recognising Textual Entailment (RTE) Challenge, has created a number of datasets that incorporate the properties of particular tasks, such as Semantic Search in RTE-5 (Bentivogli et al., 2009) or Novelty Detection in RTE-7 (Bentivogli et al., 2011). At the same time, work on RTE on has focused almost exclusively on English. There is at most a handful of studies on Textual Entailment in other language"
W13-0125,D08-1027,0,0.0290574,"Missing"
W13-0125,W10-0725,0,0.0143089,"006). Datasets for other languages have been created in the context of the CLEF QA Answer Validation and Machine Reading tasks, but do not appear to be available to the general community. We have employed crowdsourcing, a technique whose practice has expanded greatly over the last years (Snow et al., 2008). It has rarely been used for Textual Entailment, though, since high-quality crowdsourcing relies on the ability to formulate the task in layman’s terms, which is challenging for entailment. We avoided this problem by asking turkers to provide summaries and paraphrases in two separate steps. Wang and Callison-Burch (2010) also use crowdsourcing to collect hypotheses for TE. In contrast to us, they do not ask turkers for full summaries and paraphrases, but have them extract facts from texts and create counter-facts from facts by inserting negations, using antonyms, or changing adverbs. Finally, Bernhard and Gurevych (2008) present a study on data that is similar to ours. Their goal is the automatic collection of paraphrases for English questions on social Q&A sites. Employing similar methods to us (e.g., word overlap and edit distance), they achieve very good results. Their task is simpler in that in concentrat"
W13-0216,J10-4006,1,0.761722,". Current distributional models are typically built by collecting contexts of word occurrences in large corpora, where “context” can be defined in many possible ways. Pairwise word similarity is then computed by comparing the similarity between the vectors which record the word co-occurrences in the data. Distributional models have been successful in modelling a range of cognitive tasks, including lexical development (Li, Farkas, and MacWhinney 2004), category-related deficits (Vigliocco, Vinson, Lewis, and Garrett 2004), and thematic fit (Erk, Padó, and Padó 2010). Distributional Memory (DM, Baroni and Lenci (2010)) is a general framework for building distributional semantic models from syntactically analysed corpora. It constructs a three-dimensional tensor of 1 Similarly, thematic-fit based accounts of selectional preferences encompass binary distinctions (e.g., eat requires a [+edible] object), while still including more fine-grained differences (e.g., crook is a more fitting object for arrest than cop). weighted word-relation-word tuples each tuple is mapped onto a score by a function σ : hw1 r w2 i → N, where w2 is a target word, r as a relation and w1 an argument or adjuct of the target word. For"
W13-0216,J10-4007,1,0.822284,"hooti has a higher weight than hteacher subj shooti. The set of relations can be defined in different ways, which gives rise to different flavors of DM. We use TypeDM, which uses generic syntactic relations as well as lexicalized relations (see Baroni and Lenci (2010) for details). For our experiments, we project the DM tensor onto a W1 × RW2 matrix, and we represent each target word W1 in terms of a vector with dimensions corresponding to pairs of context words and their relations (R × W2 ). On this matrix, we compute a verb’s expectations for its most typical object with a method similar to Erk et al. (2010) and Lenci (2011): For each verb v, we determine the 20 highest-scoring nouns in object relation and compute the centroid co (v) of their context vectors. The thematic fit of a new noun n for v’s object position is then defined as the cosine of the angle between n’s own context vector and co (v). Since all the vectors’ components are positive, the thematic fit values range between 0 and 1. 2.2 Datasets As stated above, we model three datasets from psycholinguistic experiments. The datasets fall into two categories: sentence triplets, and sentence quadruplets. Please refer to the corresponding"
W13-0216,J03-2004,0,0.689417,"tic fit has emerged as a pivotal concept to explain effects on expectations about upcoming input in language comprehension (McRae, Spivey-Knowlton, and Tanenhaus 1998; Ferretti, McRae, and Hatherell 2001; Matsuki, Chow, Hare, Elman, Scheepers, and McRae 2011). Concerning logical metonymy, there is considerable behavioral as well as modeling evidence that thematic fit plays an important role in metonymy interpretation, that is, the retrieval of covert events for metonymical constructions. Behavioral studies (Zarcone and Padó 2011; Zarcone, Padó, and Lenci 2012) as well as computational models (Lapata and Lascarides 2003; Zarcone, Utt, and Padó 2012) found that the retrieved event will be the event most compatible with our knowledge about typical events and their participants (as captured, e.g. by generalized event knowledge, (McRae and Matsuki 2009)), that is the interpretation with the highest thematic fit with the context. This is in contrast to traditional accounts of logical metonymy (Pustejovsky 1995) which ascribe covert event retrieval to complex lexical entries associating entities with events corresponding to their typical function or creation mode (qualia: book → read / write). The advantage of the"
W13-0216,W11-0607,1,0.886907,"ght than hteacher subj shooti. The set of relations can be defined in different ways, which gives rise to different flavors of DM. We use TypeDM, which uses generic syntactic relations as well as lexicalized relations (see Baroni and Lenci (2010) for details). For our experiments, we project the DM tensor onto a W1 × RW2 matrix, and we represent each target word W1 in terms of a vector with dimensions corresponding to pairs of context words and their relations (R × W2 ). On this matrix, we compute a verb’s expectations for its most typical object with a method similar to Erk et al. (2010) and Lenci (2011): For each verb v, we determine the 20 highest-scoring nouns in object relation and compute the centroid co (v) of their context vectors. The thematic fit of a new noun n for v’s object position is then defined as the cosine of the angle between n’s own context vector and co (v). Since all the vectors’ components are positive, the thematic fit values range between 0 and 1. 2.2 Datasets As stated above, we model three datasets from psycholinguistic experiments. The datasets fall into two categories: sentence triplets, and sentence quadruplets. Please refer to the corresponding psycholinguistic"
W13-0216,W12-1707,1,0.792069,"Missing"
W13-0604,J10-4006,1,0.9091,"s, using explicit or implicit generalizations of the fillers. These rely either primarily on a lexical hierarchy (Resnik, 1996), distributional information (Rooth et al., 1999; Erk et al., 2010) or both (Schulte im Walde et al., 2008). While such computationally-intensive approaches have proven effective in modeling selectional preferences in general, we are interested in learning about only one aspect of a verb’s argument, namely how ‘event-like’ it is. We use the WordNet (Fellbaum, 2010)4 lexical hierarchy to discover whether a noun has an event sense. We also use Distributional Memory (DM, Baroni and Lenci (2010)) as a source of distributional information that allows us to determine how strongly a noun is associated with a given verb as an object filler. DM is a general distributional semantic resource which allows the generation of vectorbased semantic models (Turney and Pantel, 2010) from the distribution of words in context. In general, distributional semantic models are two-dimensional, relating a word with other words in its context giving 3 4 We will subsequently simplify the terminology and speak of a “verb’s eventhood.” We use version 3 of WordNet, available at: http://wordnet.princeton.edu/wo"
W13-0604,J10-4007,1,0.905095,"Missing"
W13-0604,J03-2004,0,0.0344585,"ting nouns (The boy [started/saw]V [the puzzle/fight]N P ) and report significantly higher processing costs for the “coercion combination” (metonymic verb plus entity-denoting object: The boy started the puzzle). While there has been much debate in theoretical linguistics on individual verbs that may or may not give rise to logical metonymy (for example, on enjoy, see Pustejovsky (1995); Fodor and Lepore (1998); Lascarides and Copestake (1998)), work in psycholinguistics (McElree et al., 2001; Traxler et al., 2002; Pylkk¨anen and McElree, 2006) and computational modeling (Lapata et al., 2003; Lapata and Lascarides, 2003) seem to have agreed on a small set of “metonymic verbs” which is used when looking for empirical correlates of logical metonymy. However, this set of metonymic verbs is semantically rather heterogeneous, as it is selected based on intuition only. It includes not only aspectual verbs2 (begin, complete, continue, end, finish, start) but also psychological verbs (enjoy, hate, like, love, regret, savor, try), as well as others that elude straightforward categorization (attempt, endure, manage, master, prefer). 1 In this paper we follow the accepted broad linguistic-philosophical distinction betwe"
W13-0604,W11-0607,1,0.800359,"to determine to what extent ‘the typical object’ of a verb is event-like. First we take the k most strongly associated object fillers from DM, objk (v) for the verb v and then define the eventhood to be the percentage of these fillers that have an event sense. In other words, the eventhood k for a verb v is defined as: |EV ∩ objk (v)| . (2) k Selecting the top k scored fillers as prototypical arguments has proven a reliable method to characterize the expectations for the argument slot which allows, e.g., the modeling of selectional preferences (cf. Baroni and Lenci (2010); Erk et al. (2010); Lenci (2011)). For the present analysis, we fix k at 100 (i.e.  := 100 ), we thereby also eliminate the issue of using words from DM which are not covered in WordNet. The following section investigates the range of eventhood scores across the verbs in DM. k (v) = 2.3 Evaluation on Verbs in DM 400 200 0 Frequency 600 Figure 1 shows the distribution of eventhood across verbs in DM. Verbs with  ≈ 0, i.e. verbs with low eventhood, include unfrock, detain, marry, and behead, while verbs with high eventhood, i.e. those which rank the highest with respect to  (i.e.  ≈ 1), include expedite, undergo, halt, a"
W13-0604,J07-2002,1,0.75038,"Missing"
W13-0604,P99-1014,0,0.0345597,"a measure of “eventhood” of a verb’s object slot3 and to use it to distinguish between verb classes. Our hypotheses are that (a) aspectual verbs have a higher eventhood score than entity-selecting verbs and (b) aspectual verbs have a higher eventhood-score than non-aspectual metonymic verbs. 2.1 Selection of typical objects from corpus data There has been much work on modeling the various fillers of verbs, i.e. their selectional preferences, using explicit or implicit generalizations of the fillers. These rely either primarily on a lexical hierarchy (Resnik, 1996), distributional information (Rooth et al., 1999; Erk et al., 2010) or both (Schulte im Walde et al., 2008). While such computationally-intensive approaches have proven effective in modeling selectional preferences in general, we are interested in learning about only one aspect of a verb’s argument, namely how ‘event-like’ it is. We use the WordNet (Fellbaum, 2010)4 lexical hierarchy to discover whether a noun has an event sense. We also use Distributional Memory (DM, Baroni and Lenci (2010)) as a source of distributional information that allows us to determine how strongly a noun is associated with a given verb as an object filler. DM is a"
W13-0604,P08-1057,0,0.0139368,"se it to distinguish between verb classes. Our hypotheses are that (a) aspectual verbs have a higher eventhood score than entity-selecting verbs and (b) aspectual verbs have a higher eventhood-score than non-aspectual metonymic verbs. 2.1 Selection of typical objects from corpus data There has been much work on modeling the various fillers of verbs, i.e. their selectional preferences, using explicit or implicit generalizations of the fillers. These rely either primarily on a lexical hierarchy (Resnik, 1996), distributional information (Rooth et al., 1999; Erk et al., 2010) or both (Schulte im Walde et al., 2008). While such computationally-intensive approaches have proven effective in modeling selectional preferences in general, we are interested in learning about only one aspect of a verb’s argument, namely how ‘event-like’ it is. We use the WordNet (Fellbaum, 2010)4 lexical hierarchy to discover whether a noun has an event sense. We also use Distributional Memory (DM, Baroni and Lenci (2010)) as a source of distributional information that allows us to determine how strongly a noun is associated with a given verb as an object filler. DM is a general distributional semantic resource which allows the"
W15-0108,D10-1115,0,0.48573,"the result of a compositional process that combines the meanings of the base term read and the affix +er. This puts derivation into the purview of compositional distributional semantic models (CDSMs). CDSMs are normally used to compute the meaning of phrases and sentences by combining distributional representations of the individual words. A first generation of CDSMs represented all words as vectors and modeled composition as vector combination (Mitchell and Lapata, 2010). A second generation represents the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Coecke et al., 2010), which are combined using various composition operations. Lazaridou et al. predict vectors for derived terms and evaluate their approach on a set of English derivation patterns. Building on and extending their analysis, we turn to German derivation patterns and offer both qualitative and quantitative analyses of two composition models on a state-of-the-art vector space, with the aim of better understanding where these models work well and where they fail. Our contributions are as follows. First, we perform all analyses in parallel for six derivation patterns (two each fo"
W15-0108,C10-1011,0,0.0148778,"cal function model (L EX F UN) represents the pattern as a matrix P that encodes the linear transformation that maps base onto derived terms: P b ≈ d. The best matrix Pˆ is typically computed via least-squares regression between the predicted vectors dˆi and the actual vectors di . 3 Experimental Setup Distributional model. We build a vector space from the SdeWaC corpus (Faaß and Eckart, 2013), part-of-speech tagged and lemmatized using TreeTagger (Schmid, 1994). To alleviate sparsity arising from TreeTagger’s lexicon-driven lemmatization, we back off for unrecognized words to the MATE Tools (Bohnet, 2010), which have higher recall but lower precision than TreeTagger. We also reconstruct lemmas for separated prefix verbs based on the MATE dependency analysis. Finally, we get a word list with 289,946 types (content words only). From the corpus, we extract lemmatized sentences and train a state-of-the art predictive model, namely CBOW (Mikolov et al., 2013). This model builds distributed word vectors by learning to predict the current word based on a context. We use lemma-POS pairs as both target and context elements, 300 dimensions, negative sampling set to 15, and no hierarchical softmax. Selec"
W15-0108,P13-1149,0,0.341749,"ng base-derived term pairs of the same pattern. A regression analysis shows that semantic coherence of the base and derived terms within a pattern, as well as coherence of the semantic shifts from base to derived terms, all significantly impact prediction quality. 1 Introduction Derivation is a major morphological process of word formation (e.g., read → read+er), which is typically associated with a fairly specific semantic shift (+er: agentivization). It may therefore be surprising that the semantics of derivation is a relatively understudied phenomenon in distributional semantics. Recently, Lazaridou et al. (2013) proposed to consider the semantics of a derived term like read+er as the result of a compositional process that combines the meanings of the base term read and the affix +er. This puts derivation into the purview of compositional distributional semantic models (CDSMs). CDSMs are normally used to compute the meaning of phrases and sentences by combining distributional representations of the individual words. A first generation of CDSMs represented all words as vectors and modeled composition as vector combination (Mitchell and Lapata, 2010). A second generation represents the meaning of predic"
W15-0108,W13-3512,0,0.125158,"a large number of distinct patterns. Some cross part-of-speech boundaries (nominalization, verbalization, adjectivization), but many do not (gender indicators like actor / actress or (de-)intensifiers like red / reddish). In many languages, such as German ˇ or the Slavic languages, derivational morphology is extremely productive (Stekauer and Lieber, 2005). Particularly relevant from a semantic perspective is that the meanings of the base and derived terms are often, but not always, closely related to each other. Consequently, derivational knowledge can be used to improve semantic processing (Luong et al., 2013; Pad´o et al., 2013). However, relatively few databases of derivational relations exist. CELEX (Baayen et al., 1996) contains derivational information for several languages, but was largely hand-written. A recent large-coverage resource for German, DErivBase (Zeller et al., 2013), covers 280k lemmas and was created from a rule-based framework that is fairly portable across languages. It is unique in that each base-derived lemma pair is labeled with a sequence of derivation patterns from a set of 267 patterns, enabling easy access to instances of specific patterns (cf. Section 3). Compositiona"
W15-0108,N13-1090,0,0.267478,"work that is fairly portable across languages. It is unique in that each base-derived lemma pair is labeled with a sequence of derivation patterns from a set of 267 patterns, enabling easy access to instances of specific patterns (cf. Section 3). Compositional models for derivation. Base and derived terms are closely related in meaning. In addition, this relation is coherent to a substantial extent, due to the phenomenon of productivity. In English, for example, the suffix -er generally indicates an agentive nominalization (sleep / sleeper) and un- is a negation prefix (well / unwell). Though Mikolov et al. (2013) address some inflectional patterns, Lazaridou et al. (2013) were the first to use this observation to motivate modeling derivation with CDSMs. Conceptually, the meaning of the base term (represented as a distributional vector) is combined with some distributional representation of the affix to obtain a vector representing the meaning of the derived term. In their experiments, they found that the two best-motivated and best-performing composition models were the full additive model (Zanzotto et al., 2010) and the lexical function model (Baroni and Zamparelli, 2010). Botha and Blunsom (2014) us"
W15-0108,P13-2128,1,0.92115,"Missing"
W15-0108,C10-1142,0,0.0530275,"tive nominalization (sleep / sleeper) and un- is a negation prefix (well / unwell). Though Mikolov et al. (2013) address some inflectional patterns, Lazaridou et al. (2013) were the first to use this observation to motivate modeling derivation with CDSMs. Conceptually, the meaning of the base term (represented as a distributional vector) is combined with some distributional representation of the affix to obtain a vector representing the meaning of the derived term. In their experiments, they found that the two best-motivated and best-performing composition models were the full additive model (Zanzotto et al., 2010) and the lexical function model (Baroni and Zamparelli, 2010). Botha and Blunsom (2014) use a related approach to model morphology for language modeling. The additive model (A DD) (Mitchell and Lapata, 2010) generally represents a derivation pattern p as a vector computed as the shift from base term vector b to the derived term vector d, i.e., b + p ≈ d. Given a set of P base-derived term pairs (b, d) for p, the best pˆ is computed as the average of the vector difference, 1 pˆ = N i (di − bi ).1 The lexical function model (L EX F UN) represents the pattern as a matrix P that encodes the linear"
W15-0108,P13-1118,1,0.897303,"Missing"
W15-0108,P13-4006,0,\N,Missing
W16-2015,E14-4008,0,0.0371815,"t random. For example, tunnel (n.) occurs 38,967 times in the corpus and tunnel (v.) 2,949 times. Through downsampling, the vectors both for tunnel (n.) and for tunnel (v.) are constructed from 2,949 instances. 3.4 Frequency. We assess the frequency hypothesis by directly comparing the number of nominal and verbal corpus occurrences of a target lemma. Semantic Specificity. We operationalize the semantic specificity hypothesis by applying measures of information content to distributional representations. This follows the example of two recent studies. In the context of hyponymy identification, Santus et al. (2014) proposed entropy as a measure of the semantic specificity S(w) of a word w, via its distributional, L1-normalized vector w. ~ Entropy is supposed to be inversely correlated with semantic specificity, since higher specificity corresponds to more restrictions on context, which means lower entropy, defined as Vector Representations To measure semantic specificity, we perform a distributional analysis which represents each conversion case with two 10,000-dimensional bag-ofwords vectors: one for the verb and one for the noun, relying on automatic PoS tags (cf. Section 3.2). The dimensions correspo"
W16-2015,P13-2078,0,0.125647,"ith the number of conversion instances for each class. The two predictors agree more often than they disagree, but among the disagreements, the strong asymmetry between N-to-V (top) and V-to-N (below) is readily visible. Table 1: Accuracies for predicting the direction of derivation, presented by gold standard direction (all results on downsampled space) Predictor Intercept ∆ entropy ∆ KL divergence ∆ log frequency Estimate Std. Err. Sig. 0.15 -2.08 -2.22 1.74 0.06 0.18 0.18 0.09 ** *** *** *** Table 2: Logistic regression model (∆ always denotes noun value minus verb value) The second study (Herbelot and Ganesalingam, 2013) was directly interested in measuring specificity and proposed to equate it with the KullbackLeibler (KL) divergence D between a word vector w ~ and the “neutral” vector ~n: X w ~i S(w) = D(w||~ ~ n) = w ~ i · log (2) ~ni i where ~n is the prior distribution over all words. We compute ~n as the centroid of approximately 28,000 word vectors in our vector space; the vectors are computed according to the procedure in Section 3.3. In this approach, higher KL divergence corresponds to higher semantic specificity. We note that the entropy and KL divergence values are closely related mathematically a"
W17-5203,S16-1063,0,0.0150074,"ractically feasible approximation of emotions, there is no publicly available, manually vetted data set for Twitter emotions that would support accurate and comparable evaluations. In addition, it has been shown that distant annotation is conceptually different from manual annotation for sentiment and emotion (Purver and Battersby, 2012). With this paper, we contribute manual emotion annotation for a publicly available Twitter data set. We annotate the SemEval 2016 Stance Data set (Mohammad et al., 2016) which provides sentiment and stance information and is popular in the research community (Augenstein et al., 2016; Wei et al., 2016; Dias and Becker, 2016; Ebrahimi et al., 2016). It therefore enables further research on the relations between sentiment, emotions, and stances. For instance, if the distribution of subclasses of positive or negative emotions is different for against and in-favor, emotion-based features could contribute to stance detection. An additional feature of our resource is that we do not only provide a “majority annotation” as is usual. We do define a well-performing aggregated annotation, but additionally provide the individual labels of each of our six annotators. This enables furt"
W17-5203,P82-1020,0,0.698097,"Missing"
W17-5203,S16-1061,0,0.0231627,"ns, there is no publicly available, manually vetted data set for Twitter emotions that would support accurate and comparable evaluations. In addition, it has been shown that distant annotation is conceptually different from manual annotation for sentiment and emotion (Purver and Battersby, 2012). With this paper, we contribute manual emotion annotation for a publicly available Twitter data set. We annotate the SemEval 2016 Stance Data set (Mohammad et al., 2016) which provides sentiment and stance information and is popular in the research community (Augenstein et al., 2016; Wei et al., 2016; Dias and Becker, 2016; Ebrahimi et al., 2016). It therefore enables further research on the relations between sentiment, emotions, and stances. For instance, if the distribution of subclasses of positive or negative emotions is different for against and in-favor, emotion-based features could contribute to stance detection. An additional feature of our resource is that we do not only provide a “majority annotation” as is usual. We do define a well-performing aggregated annotation, but additionally provide the individual labels of each of our six annotators. This enables further research on differences in the percep"
W17-5203,D14-1181,0,0.001523,"in the t=0.5 annotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n x 300 dimensional matrix. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a fully connected dense layer with ReLu activations and finally to the 8 output neurons, which are gated with the sigmoid function. We again use dropout (0.5"
W17-5203,Q16-1023,0,0.00424888,"vely with negative sentiment. For the majority annotation (Table 5), the number of annotations is smaller. However, the average size of the odds ratios increase (from 1.96 for t=0.0 to 5.39 for t=0.5). A drastic example is disgust in combination with negative sentiment, the predominant combination. Disgust is only labeled once with positive sentiment in the t=0.5 annotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when need"
W17-5203,D16-1105,0,0.0427528,"Missing"
W17-5203,P16-1191,0,0.00670011,"he same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n x 300 dimensional matrix. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a fully connected dense layer with ReLu activations and finally to the 8 output neurons, which are gated with the sigmoid function. We again use dropout (0.5), this time before and after the convolutional layers. S"
W17-5203,W17-5205,0,0.0906458,"Missing"
W17-5203,S16-1003,0,0.0547157,"Missing"
W17-5203,C14-1008,0,0.0277193,"nnotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n x 300 dimensional matrix. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a fully connected dense layer with ReLu activations and finally to the 8 output neurons, which are gated with the sigmoid function. We again use dropout (0.5), this time before and afte"
W17-5203,W11-2207,0,0.0147641,"xists. These include review texts from different domains, for instance from Amazon and other shopping sites (Hu and Liu, 2004; Ding et al., 2008; Toprak et al., 2010; Lakkaraju et al., 2011), restaurants (Ganu et al., 2009), news articles (Wiebe et al., 2005), blogs (Kessler et al., 2010), as well as microposts on Twitter. For the latter, shown in the upper half of Table 1, there are general corpora (Nakov et al., 2013; Spina et al., 2012; Thelwall et al., 2012) as well as ones focused on very specific subdomains, for instance on ObamaMcCain Debates (Shamma et al., 2009), Health Care Reforms (Speriosu et al., 2011). A popular example for a manually annotated corpus for sentiment, which includes stance annotation for a set of topics is the SemEval 2016 data set (Mohammad et al., 2016). For emotion analysis, the set of annotated resources is smaller (compare the lower half of Table 1). A very early resource is the ISEAR data set (Scherer and Wallbott, 1997) which contains descriptions of emotional events. While motivated by psychological research, it was later repurposed for computational research. The first data set developed specifically for computational research was the tales corpus by Alm et al. (200"
W17-5203,S16-1001,0,0.0379907,"3 78 489 177 156 33 984 520 487 213 Table 2: Corpus Statistics. The threshold t measures that a fraction of more than t annotators labeled the respective emotion (e. g., t=0.0: at least one annotator t=0.99: all annotators). Overall number of tweets: 4,868. Max Anger Anticipation Disgust Fear Joy Sadness Surprise Trust 0.28 0.11 0.06 0.08 0.30 0.04 0.09 0.29 0.49 0.39 0.30 0.25 0.52 0.30 0.33 0.57 ers and have college-level proficiency in English. To train the annotators on the task, we performed two training iterations based on 50 randomly selected tweets from the SemEval 2016 Task 4 corpus (Nakov et al., 2016). After each iteration, we discussed annotation differences (informally) in face-to-face meetings. For the final annotation, tweets were presented to the annotators in a web interface which paired a tweet with a set of binary check boxes, one for each emotion. Taggers could annotate any set of emotions. Each annotator was assigned with 5/7 of the corpus with equally-sized overlap of instances based on an offset shift. Not all annotators finished their task.2 Corpus Annotation and Analysis 3.1 Min Table 3: Kappa Statistics for all pairs of annotators. Mohammad et al. (2015) annotated electoral"
W17-5203,S13-2052,0,0.0171164,"fear, surprise, disgust, anger, anticipation, joy, roles, style, purpose (number denotes subset in corpus with emotion annotations) For sentiment analysis, a large number of annotated data sets exists. These include review texts from different domains, for instance from Amazon and other shopping sites (Hu and Liu, 2004; Ding et al., 2008; Toprak et al., 2010; Lakkaraju et al., 2011), restaurants (Ganu et al., 2009), news articles (Wiebe et al., 2005), blogs (Kessler et al., 2010), as well as microposts on Twitter. For the latter, shown in the upper half of Table 1, there are general corpora (Nakov et al., 2013; Spina et al., 2012; Thelwall et al., 2012) as well as ones focused on very specific subdomains, for instance on ObamaMcCain Debates (Shamma et al., 2009), Health Care Reforms (Speriosu et al., 2011). A popular example for a manually annotated corpus for sentiment, which includes stance annotation for a set of topics is the SemEval 2016 data set (Mohammad et al., 2016). For emotion analysis, the set of annotated resources is smaller (compare the lower half of Table 1). A very early resource is the ISEAR data set (Scherer and Wallbott, 1997) which contains descriptions of emotional events. Whi"
W17-5203,S07-1013,0,0.300528,"Electoral Tweets 8 9 10 11 12 13 descriptions sentences blogs headlines tweets tweets Size 498 15,196 2,516 3,238 4,490 8,850 12,770 2,205 4,870 4,242 Topic General General Politics Politics Weather Weather Gas prices General 5 topics General Source Go et al. (2009) Nakov et al. (2013) Speriosu et al. (2011) Shamma et al. (2009) Cavender-Bares (2011) Busch (2011) Busch (2012) Hassan Saif and Alani (2013) Mohammad et al. (2016) Thelwall et al. (2012) 7,666 Emotional Events Scherer and Wallbott (1997) 1,580 Grim’s Fairytales Alm et al. (2005) 173 General Aman and Szpakowicz (2007) 1,250 General Strapparava and Mihalcea (2007) 7,102 General Mohammad and Bravo-Marquez (2017) 965 Elections Mohammad et al. (2015) Table 1: A selection of resources for sentiment analysis (on Twitter, 1–7) and emotion analysis (in general, 8–12). Annotation refers to the following annotation schemes: [1] positive-negative, [2] positivenegative-neutral, [3] positive-negative-mixed-other, [4] positive-negative-netural-unrelated-can’t tell, [5] positive-negative-neutral-mixed-other, [6] for-against, [7] positive and negative strength (range), [8] joy, fear, anger, sadness, disgust, shame, guilt, [9] angry, disgusted, fearful, happy, sad, po"
W17-5203,P16-2067,0,0.018869,"an Victory. Disgust occurs almost exclusively with negative sentiment. For the majority annotation (Table 5), the number of annotations is smaller. However, the average size of the odds ratios increase (from 1.96 for t=0.0 to 5.39 for t=0.5). A drastic example is disgust in combination with negative sentiment, the predominant combination. Disgust is only labeled once with positive sentiment in the t=0.5 annotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings la"
W17-5203,E12-1049,0,0.00955703,"ormation (hashtags, emoticons, emojis) that can be used as weak supervision for training classifiers (Suttles and Ide, 2013). The classifier then learns the association of all other words in the message with the “self-labeled” emotion (Wang et al., 2012). While this approach provides a practically feasible approximation of emotions, there is no publicly available, manually vetted data set for Twitter emotions that would support accurate and comparable evaluations. In addition, it has been shown that distant annotation is conceptually different from manual annotation for sentiment and emotion (Purver and Battersby, 2012). With this paper, we contribute manual emotion annotation for a publicly available Twitter data set. We annotate the SemEval 2016 Stance Data set (Mohammad et al., 2016) which provides sentiment and stance information and is popular in the research community (Augenstein et al., 2016; Wei et al., 2016; Dias and Becker, 2016; Ebrahimi et al., 2016). It therefore enables further research on the relations between sentiment, emotions, and stances. For instance, if the distribution of subclasses of positive or negative emotions is different for against and in-favor, emotion-based features could con"
W17-5203,P15-1150,0,0.0587705,"Missing"
W17-5203,roberts-etal-2012-empatweet,0,0.0198922,"ines. A notable gap is the unavailability of a publicly available set of microposts (e. g., tweets) with emotion labels. To the best of our knowledge, there are only three previous approaches to labeling tweets with discrete emotion labels. One is the recent data set on for emotion intensity estimation, a shared task aiming at the development of a regression model. The goal is not to predict the emotion class, but a distribution over their intensities, and the set of emotions is limited to fear, sadness, anger, and joy (Mohammad and Bravo-Marquez, 2017). Most similar to our work is a study by Roberts et al. (2012) which annotated 7,000 tweets manually for 7 emotions (anger, disgust, fear, joy, love, sadness and surprise). They chose 14 topics which they believe should elicit emotional tweets and collect hashtags to help identify tweets that are on these topics. After several iterations, the annotators reached κ = 0.67 inter-annotator agreement on 500 tweets. Unfortunately, the data appear not to be available any more. An additional limitation of that dataset was that 5,000 of the 7,000 tweets were annotated by one annotator only. In contrast, we provide several annotations for each tweet. 14 Label coun"
W17-5203,C16-1311,0,0.00555306,"Missing"
W17-5203,P10-1059,0,0.0103963,"rful, happy, sad, positively surprised, negatively surprised, [10] happiness, sadness, anger, disgust, surprise, fear, mixed, [11] anger, disgust, fear, joy, sadness, surprise, [12] anger, fear, joy, sadness, [13] positive, negative, mixed, intensity, trust, fear, surprise, disgust, anger, anticipation, joy, roles, style, purpose (number denotes subset in corpus with emotion annotations) For sentiment analysis, a large number of annotated data sets exists. These include review texts from different domains, for instance from Amazon and other shopping sites (Hu and Liu, 2004; Ding et al., 2008; Toprak et al., 2010; Lakkaraju et al., 2011), restaurants (Ganu et al., 2009), news articles (Wiebe et al., 2005), blogs (Kessler et al., 2010), as well as microposts on Twitter. For the latter, shown in the upper half of Table 1, there are general corpora (Nakov et al., 2013; Spina et al., 2012; Thelwall et al., 2012) as well as ones focused on very specific subdomains, for instance on ObamaMcCain Debates (Shamma et al., 2009), Health Care Reforms (Speriosu et al., 2011). A popular example for a manually annotated corpus for sentiment, which includes stance annotation for a set of topics is the SemEval 2016 dat"
W17-5203,S16-1062,0,0.00582856,"ximation of emotions, there is no publicly available, manually vetted data set for Twitter emotions that would support accurate and comparable evaluations. In addition, it has been shown that distant annotation is conceptually different from manual annotation for sentiment and emotion (Purver and Battersby, 2012). With this paper, we contribute manual emotion annotation for a publicly available Twitter data set. We annotate the SemEval 2016 Stance Data set (Mohammad et al., 2016) which provides sentiment and stance information and is popular in the research community (Augenstein et al., 2016; Wei et al., 2016; Dias and Becker, 2016; Ebrahimi et al., 2016). It therefore enables further research on the relations between sentiment, emotions, and stances. For instance, if the distribution of subclasses of positive or negative emotions is different for against and in-favor, emotion-based features could contribute to stance detection. An additional feature of our resource is that we do not only provide a “majority annotation” as is usual. We do define a well-performing aggregated annotation, but additionally provide the individual labels of each of our six annotators. This enables further research on di"
W17-5203,C16-1329,0,0.00375071,"(Table 5), the number of annotations is smaller. However, the average size of the odds ratios increase (from 1.96 for t=0.0 to 5.39 for t=0.5). A drastic example is disgust in combination with negative sentiment, the predominant combination. Disgust is only labeled once with positive sentiment in the t=0.5 annotation: Bi-LSTM has the same architecture as the normal LSTM, but includes an additional layer with a reverse direction. This approach has produced stateof-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer. #WeNeedFeminism because #NoMeansNo it doesnt mean yes, it doesnt mean try harder! CNN has proven remarkably effective for text classification (Kim, 2014; dos Santos and Gatti, 2014; Flekova and Gurevych, 2016) . We train a simple one-layer CNN with one convolutional layer on top of pre-trained word embeddings, following Kim (2014). The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n x 300 dimensional matrix. The em"
W17-5203,H05-1073,0,\N,Missing
W17-6904,P14-1023,1,0.760312,"Missing"
W17-6904,J10-4006,1,0.56304,"ion. Dataset. We have constructed a dataset for the task containing 40k sequences for training, 5k for validation and 10k for testing.1 It is assembled on the basis of 2k object categories with 50 ImageNet2 images each, sampled from a larger dataset (Lazaridou et al., 2015). These are natural images, which makes the task challenging. The object categories given in the queries are those specified in ImageNet. We build a set of linguistic attributes for each object by first extracting the 500 most associated, and thus plausible, syntactic neighbors for the category according to the DM resource (Baroni and Lenci, 2010). This excludes nonsensical combinations such as repair:dog. We further retain only (relatively) abstract verbs taking the target item as direct object.3 This is because (a) concrete verbs are likely to have strong visual correlates that could conflict with the image (cf. walk dog); and (b) referential expressions routinely successfully mix concrete and abstract cues (e.g., the dog I own). We remove all verbs with a score over 2.5 (on a 1–5 scale) in the concreteness norms of Brysbaert et al. (2014). We then construct each sequence as follows. First, we sample two random categories, and three"
W17-6904,J16-4002,1,0.840649,"that are already in the library. Our model shows promise: it beats traditional neural network architectures on the task. However, it is still outperformed by Memory Networks, another model with external memory. 1 Introduction Language combines discrete and continuous facets, as exemplified by the phenomenon of reference (Frege, 1892; Abbott, 2010): When we refer to an object in the world with the noun phrase the mug I bought, we use content words such as mug, which are notoriously fuzzy or vague in their meaning (Van Deemter, 2012; Murphy, 2002) and are best modeled through continuous means (Boleda and Herbelot, 2016). Once the referent for the mug has been established, however, it becomes a linguistic entity that we can manipulate in a largely discrete fashion, retrieving it and updating it with new information as needed (Remember the mug I bought? My brother stole it! Kamp and Reyle, 1993). Put differently, managing reference requires two distinct abilities: 1. The ability to categorize, that is, to recognize that different entities are equivalent with regard to some concept of interest (e.g. two mugs, two instances of the “things to take on a camping trip” category; Barsalou, 1983). This implies being a"
W17-6904,W08-2222,0,0.0549761,"wo referents), or to treat it as a new referent. Our second contribution is a neural network architecture with a module for referent representations: DIstributed model of REference, DIRE. DIRE uses the concept of external memory from deep learning (Joulin and Mikolov, 2015; Graves et al., 2016) to build an entity library for an exposure sequence that conceptually corresponds to the set of DRT discourse referents, using similarity-based reasoning on distributed representations to decide between aggregating and initializing entity representations. In contrast to symbolic implementations of DRT (Bos, 2008), which manipulate discourse referents on the basis of manually specified algorithms, DIRE learns to make these decisions directly from observing reference acts using end-to-end training. We see our paper as a first, modest step in the direction of data-driven learning of DRT-like behavior, and are of course still far from learning anything resembling a fully fledged DRT system. 2 Cross-modal Entity Tracking: Task and Data Task. Imagine an office, with a desk where there are three mugs and other objects. Adam tells Barbara that he just bought two of the mugs and he particularly likes the one o"
W17-6904,P12-1015,1,0.803766,"lities of computational models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 715154; AMORE); EU Horizon 2020 programme under the Marie Skłodowska-Curie grant agreement No 655577 (LOVe"
W17-6904,W15-0120,0,0.0134861,"ented a new task, cross-modal entity tracking, that tests the categorization and individuation capabilities of computational models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 7"
W17-6904,D15-1003,0,0.0188569,", cross-modal entity tracking, that tests the categorization and individuation capabilities of computational models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 715154; AMORE); EU Horizon 20"
W17-6904,J12-1006,0,0.0559812,"Missing"
W17-6904,N15-1016,1,0.845774,"ecent survey), but focuses on identifying language-external objects from images rather than mentions of a referent in text; to Visual Question Answering (Antol et al., 2015), but it cannot be solved with visual information alone; and to Referring Expression Generation (Krahmer and Van Deemter, 2012), but involves identification rather than generation. Dataset. We have constructed a dataset for the task containing 40k sequences for training, 5k for validation and 10k for testing.1 It is assembled on the basis of 2k object categories with 50 ImageNet2 images each, sampled from a larger dataset (Lazaridou et al., 2015). These are natural images, which makes the task challenging. The object categories given in the queries are those specified in ImageNet. We build a set of linguistic attributes for each object by first extracting the 500 most associated, and thus plausible, syntactic neighbors for the category according to the DM resource (Baroni and Lenci, 2010). This excludes nonsensical combinations such as repair:dog. We further retain only (relatively) abstract verbs taking the target item as direct object.3 This is because (a) concrete verbs are likely to have strong visual correlates that could conflic"
W17-6904,P13-1056,0,0.0187395,"nal models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 715154; AMORE); EU Horizon 2020 programme under the Marie Skłodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Ind"
W17-6922,P14-2118,0,0.0227292,"r and Schulte im Walde, 2016b). The questions we ask are (a) whether a derivative carries a significantly different valence from its base; and (b) whether there are interactions between valence and concreteness (i.e., whether valence shifts occur only in more concrete vs. abstract contexts). To the best of our knowledge, the effect of derivation on valence has not been explored in distributional semantics. Our work extends a couple of 1 studies that consider the interaction between valence and concreteness: Mohammad et al. (2016) present a collection of ratings targeting emotion and metaphor; Hill and Korhonen (2014) explore the interplay between subjectivity and concreteness. From a purely linguistic perspective, valence is situated between semantics and pragmatics; despite the interest for the interplay between semantics and pragmatics in derivational morphology (Dressler and Barbaresi, 1998; Plag, 2003), there has been no attempt yet to integrate theoretical considerations and computational modeling. Our contribution is twofold. First, computationally, we define a distributional procedure that quantifies the basis–derivative differences with respect to specific meaning components and aggregates these d"
W17-6922,W15-0108,1,0.834565,"e return to a question from §2: is there a difference between using valence scores from the Affective Norms and (re-)computing them distributionally? We repeated the analysis above using the Affective Norms valence scores, and found a much lower model fit (only .21 adjusted R2 , compared to .60 as in Table 1) as well as an absence of significant effects. In sum, the distributional valence scores do a substantially better job. 4 Study 2: Other Derivation Patterns Our second study extends the focus beyond über- to six other German within part-of-speech derivation patterns from a previous study (Kisselew et al., 2015): • N→N, FEMALE: -in (80 pairs). Ex: Bäcker, Bäckerin (&quot;baker&quot;, &quot;female baker&quot;) • N→N, DIMINUTIVE: -chen (80 pairs). Ex: Schiff, Schiffchen (&quot;ship&quot;, &quot;small ship&quot;) • A→A, OPPOSED: anti- (80 pairs). Ex: religiös, antireligiös (&quot;religious&quot;, &quot;antireligious&quot;) • A→A, NEGATIVE: un- (80 pairs). Ex: dankbar, undankbar (&quot;grateful&quot;, &quot;ungrateful&quot;) • V→V, DIRECTED: an- (68 pairs). Ex: sprechen, ansprechen (&quot;to speak&quot;, &quot;to address&quot;) • V→V, TRAVERSE: durch- (70 pairs). Ex: gehen, durchgehen (&quot;to go&quot;, &quot;to go through&quot;) Here, our hypotheses are that DIMINUTIVE comes with a positive valence shift and ADVERSE and"
W17-6922,L16-1413,0,0.0403416,"Missing"
W17-6922,N16-1039,0,0.0372027,"Missing"
W17-6922,P13-1149,0,0.0240982,"to establish the broad presence of valence effects in German derivation as well as strong interactions with concreteness. 1 Introduction Morphological derivation (Plag, 2003) is a word formation process which combines bases (e.g., Hund – “dog”) with affixes (e.g., the diminutive -chen) into new words (Hündchen “doggie”). The semantic properties of derivation have been extensively explored in theoretical linguistics, and a number of recent computational studies in compositional distributional semantics have modelled the mappings that hold between the vectors of bases, affixes, and derivatives (Lazaridou et al., 2013; Luong et al., 2013; Padó et al., 2016). What these studies crucially lack, though, is interpretability: typically, they model mappings in an embedding space, but have little to say about linguistic regularities such as systematic changes in meaning components. In this paper, our goal is to do exactly that, namely investigate the effects of derivation on a specific meaning component, emotional valence (henceforth, valence), which quantifies the speaker’s positive or negative affect towards the referent of a word. This choice is motivated by psycholinguistic considerations: Valence is very wel"
W17-6922,W13-3512,0,0.0200108,"resence of valence effects in German derivation as well as strong interactions with concreteness. 1 Introduction Morphological derivation (Plag, 2003) is a word formation process which combines bases (e.g., Hund – “dog”) with affixes (e.g., the diminutive -chen) into new words (Hündchen “doggie”). The semantic properties of derivation have been extensively explored in theoretical linguistics, and a number of recent computational studies in compositional distributional semantics have modelled the mappings that hold between the vectors of bases, affixes, and derivatives (Lazaridou et al., 2013; Luong et al., 2013; Padó et al., 2016). What these studies crucially lack, though, is interpretability: typically, they model mappings in an embedding space, but have little to say about linguistic regularities such as systematic changes in meaning components. In this paper, our goal is to do exactly that, namely investigate the effects of derivation on a specific meaning component, emotional valence (henceforth, valence), which quantifies the speaker’s positive or negative affect towards the referent of a word. This choice is motivated by psycholinguistic considerations: Valence is very well established in the"
W17-6922,S16-2003,0,0.021807,"s exploited, among other things, for metaphor identification (Turney et al., 2011; Köper and Schulte im Walde, 2016b). The questions we ask are (a) whether a derivative carries a significantly different valence from its base; and (b) whether there are interactions between valence and concreteness (i.e., whether valence shifts occur only in more concrete vs. abstract contexts). To the best of our knowledge, the effect of derivation on valence has not been explored in distributional semantics. Our work extends a couple of 1 studies that consider the interaction between valence and concreteness: Mohammad et al. (2016) present a collection of ratings targeting emotion and metaphor; Hill and Korhonen (2014) explore the interplay between subjectivity and concreteness. From a purely linguistic perspective, valence is situated between semantics and pragmatics; despite the interest for the interplay between semantics and pragmatics in derivational morphology (Dressler and Barbaresi, 1998; Plag, 2003), there has been no attempt yet to integrate theoretical considerations and computational modeling. Our contribution is twofold. First, computationally, we define a distributional procedure that quantifies the basis–"
W17-6922,C16-1122,1,0.844141,"ffects in German derivation as well as strong interactions with concreteness. 1 Introduction Morphological derivation (Plag, 2003) is a word formation process which combines bases (e.g., Hund – “dog”) with affixes (e.g., the diminutive -chen) into new words (Hündchen “doggie”). The semantic properties of derivation have been extensively explored in theoretical linguistics, and a number of recent computational studies in compositional distributional semantics have modelled the mappings that hold between the vectors of bases, affixes, and derivatives (Lazaridou et al., 2013; Luong et al., 2013; Padó et al., 2016). What these studies crucially lack, though, is interpretability: typically, they model mappings in an embedding space, but have little to say about linguistic regularities such as systematic changes in meaning components. In this paper, our goal is to do exactly that, namely investigate the effects of derivation on a specific meaning component, emotional valence (henceforth, valence), which quantifies the speaker’s positive or negative affect towards the referent of a word. This choice is motivated by psycholinguistic considerations: Valence is very well established in the literature as havin"
W17-6922,C16-1083,0,0.0285909,"Missing"
W17-6922,D11-1063,0,0.0159886,"14; Snefjella and Kuperman, 2016). Since it is not clear that the effects on valence take place independently of other variables, we extend our analysis to include a set of other meaning components, most notably concreteness, a second prominent meaning component in psycholinguistics (cf. the references above). Both meaning components are also highly relevant for NLP: (Variants of) valence occur under the names of sentiment and polarity and form the basic variable of interest in sentiment analysis (Pang and Lee, 2008). Concreteness is exploited, among other things, for metaphor identification (Turney et al., 2011; Köper and Schulte im Walde, 2016b). The questions we ask are (a) whether a derivative carries a significantly different valence from its base; and (b) whether there are interactions between valence and concreteness (i.e., whether valence shifts occur only in more concrete vs. abstract contexts). To the best of our knowledge, the effect of derivation on valence has not been explored in distributional semantics. Our work extends a couple of 1 studies that consider the interaction between valence and concreteness: Mohammad et al. (2016) present a collection of ratings targeting emotion and meta"
W17-6928,2014.lilt-9.5,0,0.0329859,"observed derived vector and can be interpreted as the extent to which the targeted derivational shift is predictable (Padó et al., 2016). In additional to (lack of) predictability, other contributing factors are: (a) lack of training data (absent here, since the data is balanced); (b) semantic ambiguity of patterns and base words (e.g., like Ukrainian pro-), which forces the CDSM to learn multiple transformations at the same time (we return to this point in Section 4.1); (c) limitations of the CDSM model (particularly true for the additive model, compare the discussion of adjective classes in Baroni et al. (2014)). The main observations are as follows: (a) SimpleAdd almost always outperforms LexFun independently on the underlying DSM representation (CBOW or NMF); (b) CBOW performs better than NMF (except for -nu-); (c) the best CDSMs outperform the baseline for the noun-noun patterns, for the adjectival patterns, and for the verbal perfectivizer -nu-; (d) performance is relatively constant across dimensionalities, at least for the best model, SimpleAdd on CBOW, while LexFun appears to be more affected. These results are strikingly similar to the findings of Kisselew et al. (2015) for German, to the ex"
W17-6928,D10-1115,0,0.0296065,"ft (e.g., fer ) whose output is its prediction for the vector of the derived word −−−→ −−−−→ (fer (work) = worker). We consider two types of CDSMs that performed well in previous evaluations. The first one is the simple additive model (SimpleAdd), which represents the shift induced by a derivational − pattern as the difference vector (e.g., → er) between the vector of the derived word and the vector of the −−→ − −−−→ base. It is added to new bases yielding a predicted vector for derived words (love + → er = lover). The second CDSM we test is the more expressive lexical function model (LexFun, Baroni and Zamparelli (2010)). It represents the shift as a matrix (i.e., Mer ) to be multiplied with new bases to predict derived −−→ −−−→ words (Mer · love = lover). This enables the model to account for interactions between dimensions in the shift. The downside is that LexFun uses O(d2 ) parameters instead of SimpleAdd’s O(d), where d is the dimensionality of the space. Thus, LexFun requires considerably more training data. The literature on CDSMs for derivation shows a contradictory picture: for English, Lazaridou et al. (2013) showed that LexFun outperforms additive (and multiplicative) models; for German, Kisselew"
W17-6928,P13-4006,0,0.0163504,"re braten (fry) → durchbraten (cook through), blättern (turn page) → durchblättern (skim)). In the adjectival domain, negation (ne-) also shows a high baseline, the high similarity of antonyms being a known problem in distributional semantics. Once again, this result is in line with the findings of Kisselew et al. (2015), who found the highest baseline for the adjectival negation (un-). In this context, the low baseline exhibited by nominal negation (ne-) seems to suggest that a higher degree of lexicalisation is at work for nouns 5 Compositional models were trained using the DISSECT toolkit (Dinu et al., 2013). For comparability with Kisselew et al. (2015) and Padó et al. (2016), vectors are normalized to unit length. 6 When calculating MRR, we restrict the neighbor lists to the words with the same part-of-speech of the derived word. 0.4 0.0 0.1 0.2 0.3 0.4 0.0 0.1 0.2 0.3 NMF−Baseline CBOW−Baseline NMF−LexFun CBOW−LexFun NMF−SimpleADD CBOW−SimpleADD 200 300 400 500 600 200 400 500 600 (b) -k- (female) 0.0 0.1 0.2 0.3 0.4 (a) ne300 200 300 400 500 600 0.4 0.3 0.2 0.1 0.0 0.0 0.1 0.2 0.3 0.4 (c) -k- (diminutive) 200 300 400 500 600 200 300 500 600 500 600 0.3 0.2 0.1 0.0 0.0 0.1 0.2 0.3 0.4 (e) pro0"
W17-6928,goldhahn-etal-2012-building,0,0.017463,"be vanished solodkyi, sweet → ne-solodkyi, not sweet shumnyi, noisy → bez-shumnyi, noiseless Table 1: Selection of experimental items For each pattern, we extracted pairs of base/derived words in which each word occurs at least 80 times in the corpus (see next paragraph) and randomly selected 70 pairs matching our frequency threshold. A native speaker manually checked the correctness of the base/derived pairs. Corpus. Our corpus is a concatenation of four Ukrainian corpora: three raw corpora (web-2012, news-2011, and wikipedia-2013) available through the University of Leipzig1 , described in Goldhahn et al. (2012), and the corpus described in Babych and Sharoff (2016).2 The corpus was part-ofspeech tagged and lemmatized using LanguageTool3 . Our concatenated corpus contains approximately 131 million tokens. Albeit relatively small by modern standards, this corpus size should enable the construction of reliable distributional representations (for comparison, the British National Corpus contains 100 million tokens). Distributional representations. The first distributional representation we consider is CBOW as implemented by word2vec (Mikolov et al., 2013) and used in previous studies (Kisselew et al., 20"
W17-6928,W15-0108,1,0.317949,"ce for a cross-lingual advantage of CBOW over NMF representations, as well as a simple additive over a lexical function model. In addition, we present two case studies in which we test the capabilities of CDSMs to deal with pattern-level ambiguity and apply the same CDSMs to inflectional patterns. 1 Introduction The potential and limitations of compositional distributional semantic models (CDSMs, Mitchell and Lapata (2010)) in modeling the semantic shifts associated with derivational processes have been explored in a number of evaluation studies on English (Lazaridou et al., 2013) and German (Kisselew et al., 2015; Köper et al., 2016; Padó et al., 2016). By considering high-resource languages, these studies can take advantage of large standard corpora, well-established pre-processing tools, and derivational lexicons to select experimental items, such as CELEX (Baayen et al., 1996). In this paper, we target Ukrainian, a language that is both morphologically rich and computationally low-resource. To the best of our knowledge, this is the first study employing CDSMs for modeling (aspects of) the derivational morphology of a Slavic language, for which most studies followed either knowledge-based (Pala and"
W17-6928,S16-2010,1,0.854509,"advantage of CBOW over NMF representations, as well as a simple additive over a lexical function model. In addition, we present two case studies in which we test the capabilities of CDSMs to deal with pattern-level ambiguity and apply the same CDSMs to inflectional patterns. 1 Introduction The potential and limitations of compositional distributional semantic models (CDSMs, Mitchell and Lapata (2010)) in modeling the semantic shifts associated with derivational processes have been explored in a number of evaluation studies on English (Lazaridou et al., 2013) and German (Kisselew et al., 2015; Köper et al., 2016; Padó et al., 2016). By considering high-resource languages, these studies can take advantage of large standard corpora, well-established pre-processing tools, and derivational lexicons to select experimental items, such as CELEX (Baayen et al., 1996). In this paper, we target Ukrainian, a language that is both morphologically rich and computationally low-resource. To the best of our knowledge, this is the first study employing CDSMs for modeling (aspects of) the derivational morphology of a Slavic language, for which most studies followed either knowledge-based (Pala and Hlaváˇcková, 2007; Š"
W17-6928,P13-1149,0,0.298031,"Ms for derivation; we provide evidence for a cross-lingual advantage of CBOW over NMF representations, as well as a simple additive over a lexical function model. In addition, we present two case studies in which we test the capabilities of CDSMs to deal with pattern-level ambiguity and apply the same CDSMs to inflectional patterns. 1 Introduction The potential and limitations of compositional distributional semantic models (CDSMs, Mitchell and Lapata (2010)) in modeling the semantic shifts associated with derivational processes have been explored in a number of evaluation studies on English (Lazaridou et al., 2013) and German (Kisselew et al., 2015; Köper et al., 2016; Padó et al., 2016). By considering high-resource languages, these studies can take advantage of large standard corpora, well-established pre-processing tools, and derivational lexicons to select experimental items, such as CELEX (Baayen et al., 1996). In this paper, we target Ukrainian, a language that is both morphologically rich and computationally low-resource. To the best of our knowledge, this is the first study employing CDSMs for modeling (aspects of) the derivational morphology of a Slavic language, for which most studies followed"
W17-6928,C16-1122,1,0.123332,"er NMF representations, as well as a simple additive over a lexical function model. In addition, we present two case studies in which we test the capabilities of CDSMs to deal with pattern-level ambiguity and apply the same CDSMs to inflectional patterns. 1 Introduction The potential and limitations of compositional distributional semantic models (CDSMs, Mitchell and Lapata (2010)) in modeling the semantic shifts associated with derivational processes have been explored in a number of evaluation studies on English (Lazaridou et al., 2013) and German (Kisselew et al., 2015; Köper et al., 2016; Padó et al., 2016). By considering high-resource languages, these studies can take advantage of large standard corpora, well-established pre-processing tools, and derivational lexicons to select experimental items, such as CELEX (Baayen et al., 1996). In this paper, we target Ukrainian, a language that is both morphologically rich and computationally low-resource. To the best of our knowledge, this is the first study employing CDSMs for modeling (aspects of) the derivational morphology of a Slavic language, for which most studies followed either knowledge-based (Pala and Hlaváˇcková, 2007; Šnajder, 2014) or cla"
W17-6928,W07-1710,0,0.0530392,"Missing"
W17-6928,piasecki-etal-2012-recognition,0,0.0145409,"source languages, these studies can take advantage of large standard corpora, well-established pre-processing tools, and derivational lexicons to select experimental items, such as CELEX (Baayen et al., 1996). In this paper, we target Ukrainian, a language that is both morphologically rich and computationally low-resource. To the best of our knowledge, this is the first study employing CDSMs for modeling (aspects of) the derivational morphology of a Slavic language, for which most studies followed either knowledge-based (Pala and Hlaváˇcková, 2007; Šnajder, 2014) or classification approaches (Piasecki et al., 2012). We believe that derivational morphology is a suitable starting point to gather evaluation data for CDSMs in new languages, given that the semantic relations between pairs are encoded straightforwardly at the surface level. Following the previous studies listed above, we operationalize derivation as a compositional operation: −−−→ the vector for a base word (e.g., work) is the input for the CDSM which implements a function modeling the derivational meaning shift (e.g., fer ) whose output is its prediction for the vector of the derived word −−−→ −−−−→ (fer (work) = worker). We consider two typ"
W17-6928,snajder-2014-derivbase,0,0.0186047,"6; Padó et al., 2016). By considering high-resource languages, these studies can take advantage of large standard corpora, well-established pre-processing tools, and derivational lexicons to select experimental items, such as CELEX (Baayen et al., 1996). In this paper, we target Ukrainian, a language that is both morphologically rich and computationally low-resource. To the best of our knowledge, this is the first study employing CDSMs for modeling (aspects of) the derivational morphology of a Slavic language, for which most studies followed either knowledge-based (Pala and Hlaváˇcková, 2007; Šnajder, 2014) or classification approaches (Piasecki et al., 2012). We believe that derivational morphology is a suitable starting point to gather evaluation data for CDSMs in new languages, given that the semantic relations between pairs are encoded straightforwardly at the surface level. Following the previous studies listed above, we operationalize derivation as a compositional operation: −−−→ the vector for a base word (e.g., work) is the input for the CDSM which implements a function modeling the derivational meaning shift (e.g., fer ) whose output is its prediction for the vector of the derived word"
W18-1204,W13-3520,0,0.0472256,"Missing"
W18-1204,W07-0607,0,0.0481769,"performance for the model based on individual character embeddings. Its forte is to deal with low-data situations, predicting meanings for unfamiliar words by utilizing familiar morphemes and other subword structures, in line with Landauer et al.’s (1997) claim of “vast numbers of weak interrelations that, if properly exploited, can greatly amplify learning by a process of inference”. In the future, we will evaluate our character-based model for other languages, and assess other aspects of its psycholinguistic plausibility, such as matching human behavior in performance and acquisition speed (Baroni et al., 2007). 10M corpora results. Finally, Table 6 shows the results for the 10M corpora. Here, we see a relatively heterogeneous picture regarding the individual models across benchmarks: WL does best on WS353-en; FT does best on RW and WS353-de; CL does best on GUR350. This behavior is consistent with the patterns we found for the 100M corpora, but more marked. Due to the inhomogeneity among models, the fusion model CAT does particularly well, outperforming FT on 3 of 4 benchmarks, often substantially so. As with the 100M corpora, the character aware models perform much better than WL for OOV pairs. Fo"
W18-1204,Q14-1020,1,0.803186,"Missing"
W18-1204,N07-2052,0,0.0676834,"Missing"
W18-1204,W13-3512,0,0.635245,"al ability of the human language faculty to generalize from little data. By seventh grade, students have only heard about 50 million spoken words, and read about 3.8 million tokens of text, acquiring a vocabulary of 40,000– 100,000 words (Landauer and Dumais, 1997). This also means that any new text likely contains outof-vocabulary words which students interpret by generalizing from existing knowledge – an ability that plain word embedding models lack. There are some studies that have focused on modeling infrequent and unseen words by capturing information at the subword and character levels. Luong et al. (2013) break words into morphemes, and use recursive neural networks to compose word meanings from morpheme meanings. Similarly, Bojanowski et al. (2017) represent words as bags of character n-grams, allowing morphology to inform word embeddings without requiring morphological analysis. However, both models are still typically applied to large corpora of training data, with the smallest English corpora used comprising about 1 billion tokens. Our study investigates how embedding models fare when applied to much smaller corpora, containing only millions of words. Few studies, except Sahlgren and Lenci"
W18-1204,D14-1162,0,0.097665,"two types of character-based embeddings on word relatedness prediction. On large corpora, performance of both model types is equal for frequent words, but character awareness already helps for infrequent words. Consistently, on small corpora, the characterbased models perform overall better than skipgrams. The concatenation of different embeddings performs best on small corpora and robustly on large corpora. 1 Introduction State-of-the-art word embedding models are routinely trained on very large corpora. For example, Mikolov et al. (2013a) train word2vec on a corpus of 6 billion tokens, and Pennington et al. (2014) report the best GloVe results on 42 billion tokens. From a language technology perspective, it is perfectly reasonable to use large corpora where available. However, even with large corpora, embeddings struggle to accurately model the meaning of infrequent words (Luong et al., 2013). Moreover, for the vast majority of languages, substantially less data is available. For example, there are only 4 languages with Wikipedias larger than 1 billion words,1 and 25 languages with more than 100 mil1 https://en.wikipedia.org/wiki/List_ of_Wikipedias#Detailed_list (as of 9 Jan 2018) 32 Proceedings of th"
W18-1204,D16-1099,0,0.100995,"uong et al. (2013) break words into morphemes, and use recursive neural networks to compose word meanings from morpheme meanings. Similarly, Bojanowski et al. (2017) represent words as bags of character n-grams, allowing morphology to inform word embeddings without requiring morphological analysis. However, both models are still typically applied to large corpora of training data, with the smallest English corpora used comprising about 1 billion tokens. Our study investigates how embedding models fare when applied to much smaller corpora, containing only millions of words. Few studies, except Sahlgren and Lenci (2016), have considered this setup in detail. We evaluate one word-based and two character-based embedding models on word relatedness tasks for English and German. We find that that the character-based models mimics human learning more closely, with both better results on small datasets and better performance on rare words. At the same time, a fused representation that takes both word and character level into account yields the best results for small corpora. Most modern approaches to computing word embeddings assume the availability of text corpora with billions of words. In this paper, we explore"
W18-3813,P98-1013,0,0.598171,"sing a vector space model. 1 Introduction Frame Semantics (Fillmore et al., 2003) is a theory of predicate-argument structure that describes meaning not at the level of individual words, but is instead based on the concept of a scenario or scene called a frame. Frames are defined by the group of words that evoke the scene (frame-evoking elements or FEEs), as well as their expected semantic arguments (frame elements). A J UDGMENT frame, for instance, would have FEEs praise, criticize, and boo and frame elements such as C OGNIZER, E VALUEE, E XPRESSOR and R EASON. The Berkeley FrameNet project (Baker et al., 1998) is the most well-known lexical resource based on Frame Semantics, and provides definitions for over 1200 frames. A very attractive feature of Frame Semantics is that it can abstract away from individual words, which makes it a promising descriptive framework for events that generalize across languages (Boas, 2005; Lönneker-Rodman, 2007). However, there is a fundamental tension within Frame Semantics between the characterization of the scene, which is arguably relatively language-independent, and the characterization of the FEEs and frame elements which can vary among even closely related lang"
W18-3813,J10-4006,0,0.0545479,"long process that must be repeated for each new frame and language. In this paper, we investigate to what extent this problem can be alleviated by the use of computational methods. More concretely, we build distributional representations, also known as embeddings, of frames This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 91 Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pages 91–101 Santa Fe, New Mexico, USA, August 20, 2018. (Turney and Pantel, 2010; Baroni and Lenci, 2010; Hermann et al., 2014) which build a compact representation of a frame’s meaning from the entire corpus. These embeddings are constructed for each language separately on the basis of expert-labeled corpora but are represented in a joint space, which enables us to automatically measure the cross-lingual similarity of language-specific frames (e.g., AWARENESS in English and AWARENESS in German). Our hypothesis is that frames which generalize well across languages will show a high similarity, while frames which do not generalize well will yield a low similarity. In this sense, we can see the set"
W18-3813,P14-1023,0,0.0191306,"community. The distributional hypothesis (Harris, 1954) forms the basis for DSM models, which claims that similar words should be expected to appear in similar contexts. Building on this assumption, current DSMs represent words as vectors in a low-dimensional space in which similarity is supposed to indicate semantic relatedness. DSMs can either be computed using count-based methods, which directly count word context co-occurrences and optionally apply dimensionality reduction, or prediction-based methods, where the representations are the parameter vectors of an underlying optimization task (Baroni et al., 2014). A widely-used prediction-based model is the word2vec model (Mikolov et al., 2013c). Word2vec produces word vectors by predicting the target for a window of context, a setup known as the continuous bag-of-words (CBOW) model. In the CBOW architecture, an input for the target verb “cook&quot; with a context window of 2 might appear as [‘the’,‘chef’,‘a’,‘meal’], and embeddings would accordingly be generated for each token in the vocabulary. The model is trained over a neural network with negative sampling, where noisy words are selected from a noise distribution and the model learns to approximate wo"
W18-3813,bauer-etal-2012-dependency,0,0.0246889,"t al., 2009). We exclude these from our analysis. Finally, another small difference regards the annotation of multi-word expressions, which we address further in Section 4. 4 Methods To build frame embeddings for English and German, we pre-process the two corpora introduced in the previous section. Below, we describe the steps we took to format the input for the model, and then we elaborate on how specifically we build the frame embeddings. 1 http://www.natcorp.ox.ac.uk/ 93 4.1 Preprocessing the FrameNet corpus We use the FrameNet 1.5 corpus, which provides tokenized and lemmatized sentences (Bauer et al., 2012). We perform three pre-processing steps. First, multi-word frame-evoking elements (FEEs) are concatenated into a single token. This is motivated by the presence of multi-word FEEs that evoke different frames from the single-word FEEs that they contain. One such case is the lexical unit ride, which evokes the R IDE _ VEHICLE frame, and the multi-word FEE ride out, which evokes S URVIVING. We keep instances of MWE FEEs such as ride out as single units in the vector space. The second step is a detection of named entities using the pretrained English model from spaCy’s named entity recognition (NE"
W18-3813,P03-1068,1,0.529634,"ides over 173k annotated datasets with manually-curated sentences, drawn from the 100 million word British National Corpus (BNC)1 , which is balanced across multiple genres, including novels and news reports. FrameNet’s FEEs can range from verbal, nominal, adjectival, and even adverbial and prepositional in some cases. In FrameNet’s v1.5 complete annotation set, there are over 11k FEEs, where each FEE attestation is labeled. This produced sentences in which multiple FEEs are present, and lexical units are annotated on average 20.7 times in the current annotated corpus. 3.2 SALSA Corpus SALSA (Erk et al., 2003; Burchardt et al., 2009) is a German corpus with frame semantic annotations. It was constructed over the syntactic annotations in the German TIGER (version 2.1), and is composed of 1.5 million words from German newswire text. SALSA (release 2.0) provides 24,184 sentences with frame annotations, and the corpus has labels for 998 unique FEEs. Each unique FEE is annotated on average 36.9 times. 3.3 Parallelism For the purposes of our study, it is important that the corpora are as comparable as possible. Comparability must be considered on two levels: corpus composition and annotation. With regar"
W18-3813,P14-1136,0,0.229178,"e repeated for each new frame and language. In this paper, we investigate to what extent this problem can be alleviated by the use of computational methods. More concretely, we build distributional representations, also known as embeddings, of frames This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 91 Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pages 91–101 Santa Fe, New Mexico, USA, August 20, 2018. (Turney and Pantel, 2010; Baroni and Lenci, 2010; Hermann et al., 2014) which build a compact representation of a frame’s meaning from the entire corpus. These embeddings are constructed for each language separately on the basis of expert-labeled corpora but are represented in a joint space, which enables us to automatically measure the cross-lingual similarity of language-specific frames (e.g., AWARENESS in English and AWARENESS in German). Our hypothesis is that frames which generalize well across languages will show a high similarity, while frames which do not generalize well will yield a low similarity. In this sense, we can see the set of frames that show th"
W18-3813,tonelli-pianta-2008-frame,0,0.0292878,"languages. For example, FrameNet distinguishes between the frames O PERATE _V EHICLE (drive) and U SE _V EHICLE (ride) which are often indistinguishable in German, at least for the very frequent verb fahren (drive/ride) (Burchardt et al., 2009). Another example is the well-known difference between Germanic and Romance languages regarding the conceptualization of motion events, where manner can be incorporated into the semantics of the FEE - run into a room, or externalized - enter a room running (Subirats, 2009). In general, some frames are evidently more parallel than others cross-lingually (Tonelli and Pianta, 2008; Torrent et al., 2018; Vossen et al., 2018). The question of whether a particular frame transfers across languages is often determined by linguists on the basis of large corpora, comparing either frames assigned to direct translations (Padó, 2007) or extracting “typical” uses of frames in the languages under consideration. Identifying the degree of applicability can, however, be a long process that must be repeated for each new frame and language. In this paper, we investigate to what extent this problem can be alleviated by the use of computational methods. More concretely, we build distribu"
W18-3813,P16-1157,0,0.0368504,"he most widespread application, they can in fact be applied to linguistic units of almost any granularity, from morphemes to word senses to sentences and even paragraphs and documents. In this study, we apply them to modeling the meaning of frames, i.e., semantically motivated predicate classes, following Hermann et al. (2014). 2.2 Cross-lingual Embeddings The goal of a cross-lingual DSM is to make meaning representations from different languages comparable, either by learning mutilingual embeddings in the same space or by mapping monolingual embeddings into a joint space (Ruder et al., 2017; Upadhyay et al., 2016). Mikolov et al. (2013b) introduced a particularly simple version that takes advantage of a vocabulary of shared bilingual seed words to map embeddings from a source language onto the vector space of a target language. They learn a transformation matrix W by minimizing the mean squared error (MSE) between the source seed words and their translations from the bilingual dictionary: 92 MSE = n X k Wxsi − xti k2 i=1 xs where represents the seed words from the source language, and xt are seed words from the target. The transformation matrix can then be applied to an embedding from the source space"
W19-0425,P98-1013,0,0.746993,"ngs for a specific task, arguably represent “top-down” information. Notably, such transformations can be understood equivalently as learning task-specific similarity metrics (Bellet et al., 2013). By learning general representations in a bottom-up pre-training phase and then comparing performance with additional top-down fine-tuning, we can discriminate how much general semantic knowledge is necessary to perform a categorization task and how much task-specific learning is required. In this paper, we investigate a lexical semantic task, specifically the identification of frame-semantic frames (Baker et al., 1998) in running text, from this categorization perspective. Frames can be understood as semantic classes that are sensitive both to the topic of the context and to specific properties of the predicate-argument structure. We present four categorization models for the task, all of which are based on the state-of-the-art BERT model (Devlin et al., 2018) but which differ in how they use its embeddings. Two models are prototype-based (i.e., compute a representation for each frame), and two are exemplar-based (i.e., represent a frame solely in terms of its instances). Within each group, we compare the u"
W19-0425,J14-1002,0,0.261016,"rce (Baker et al., 1998). The latest FrameNet lexicon release provides definitions for over 1.2k frames, and 13,640 lexical units (i.e., predicate–frame pairs), where there are approximately 12 lexical units per frame. FrameNet also provides sentence annotations that mark, for each lexical unit, the frame that is evoked as well as its frame elements in running text. This annotated corpus has sparked a lot of interest in computational linguistics, and the prediction of frame-semantic structures (frames and frame elements) has become known as (frame-)semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014). 2.1.1 Frame Identification In this paper, we focus on the first step of frame-semantic parsing called frame identification or frame assignment, where an occurrence of a predicate in context is labeled with its FrameNet frame. This is a categorization task that presents two main challenges: Ambiguity Most predicates in FrameNet are ambiguous, that is, they can evoke different frames depending on the context that they occur in. For example, treat has a medical sense (treat a disease) that evokes the M EDICAL INTERVENTION frame and a social sense (treat a person in some manner) that evokes the"
W19-0425,N10-1138,0,0.01979,"TERVENTION frame and a social sense (treat a person in some manner) that evokes the T REATING AND MISTREATING frame. These distinctions can be relatively subtle: say can evoke (among others) the frames S TATEMENT and T EXT CREATION which differ mainly in the modality of the communicative act (said to his friend vs. said in his book). Generalization As conceptual categories, frames are clearly open classes. No resource can exhaustively list all frames or the predicates that can evoke them. Frame identification was first modeled as a supervised classification task, based on linguistic features (Das et al., 2010). While such systems address the ambiguity problem to some degree, they tend to struggle with generalization. An alternative approach investigated the use of other machine-readable dictionaries (Green et al., 2004), but was not able to fundamentally overcome the generalization problem. Recent progress in supervised frame identification has come out of neural networks and distributed word representations (Peng et al., 2018; Hartmann et al., 2017). In these studies, frame-labeled corpora are used to learn embeddings for the frames as a side product of representation learning with different objec"
W19-0425,W09-1109,0,0.0361281,"s is very much on the use of novel machine learning techniques for applications. We nevertheless believe that categorization theory is still relevant for computational linguistics, and lexical semantics in particular. In fact, the emergence of distributed representations (embeddings) as a dominant representational paradigm has had a unifying effect on work in lexical semantics. The properties of high-dimensional embeddings provide a good match with the assumption behind much of categorization theory – namely, that categories arise naturally from the similarity structure of individual objects (Erk, 2009). Given this context, the exemplar–prototype dichotomy is a useful dimension on which models can be situated in terms of how much they generalize over objects: low for exemplar-inspired, but high for prototype-inspired models. Regarding the representation of word meaning in context, for example, the additive models first considered by Mitchell and Lapata (2008) fall into the prototype camp, while Erk and Pad´o (2010) propose exemplar-based models, and Reisinger and Mooney (2010) explore dynamic generalization in what they called ‘multi-prototype’ categorization models. However, for many tasks,"
W19-0425,P10-2017,1,0.832703,"Missing"
W19-0425,W18-2501,0,0.0158716,"t vectors. Neural network-based “predict” vectors are learned by treating contextual features as parameters of an objective function that is optimized on a corpus. One of the first, and still popular, “predict” models is the word2vec Skipgram model (Mikolov et al., 2013). It optimizes a word embedding using a context bag of words. This model, however, learns representations only at the lexical level, so that occurrences of a word in different contexts (cf. treat in Section 2.1.1) are represented equally. This has changed with the latest generation of embedding models, such as AllenNLP’s ELMo (Gardner et al., 2018) and Google’s BERT (Devlin et al., 2018), which build contextualized embeddings for occurrences of words based on the context as well as their relative positions. A second important recent development concerns the objective(s) used to learn the embedding. While traditional count vectors and early embedding models like Skipgram assume that embeddings are general, and trained in an task-agnostic fashion, there is an alternative thread of research that sees the training of embeddings as a side product of training task-specific neural network models on tasks like sentiment analysis or machine tran"
W19-0425,J02-3001,0,0.0405743,"the Berkeley FrameNet resource (Baker et al., 1998). The latest FrameNet lexicon release provides definitions for over 1.2k frames, and 13,640 lexical units (i.e., predicate–frame pairs), where there are approximately 12 lexical units per frame. FrameNet also provides sentence annotations that mark, for each lexical unit, the frame that is evoked as well as its frame elements in running text. This annotated corpus has sparked a lot of interest in computational linguistics, and the prediction of frame-semantic structures (frames and frame elements) has become known as (frame-)semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014). 2.1.1 Frame Identification In this paper, we focus on the first step of frame-semantic parsing called frame identification or frame assignment, where an occurrence of a predicate in context is labeled with its FrameNet frame. This is a categorization task that presents two main challenges: Ambiguity Most predicates in FrameNet are ambiguous, that is, they can evoke different frames depending on the context that they occur in. For example, treat has a medical sense (treat a disease) that evokes the M EDICAL INTERVENTION frame and a social sense (treat a person in some manne"
W19-0425,P04-1048,0,0.0212639,"and T EXT CREATION which differ mainly in the modality of the communicative act (said to his friend vs. said in his book). Generalization As conceptual categories, frames are clearly open classes. No resource can exhaustively list all frames or the predicates that can evoke them. Frame identification was first modeled as a supervised classification task, based on linguistic features (Das et al., 2010). While such systems address the ambiguity problem to some degree, they tend to struggle with generalization. An alternative approach investigated the use of other machine-readable dictionaries (Green et al., 2004), but was not able to fundamentally overcome the generalization problem. Recent progress in supervised frame identification has come out of neural networks and distributed word representations (Peng et al., 2018; Hartmann et al., 2017). In these studies, frame-labeled corpora are used to learn embeddings for the frames as a side product of representation learning with different objective functions. Hermann et al. (2014) learned embeddings jointly for frames and the sentential contexts in which they were evoked. The current state-of-the-art in frame identification performs full-fledged semantic"
W19-0425,E17-1045,0,0.0847942,"all frames or the predicates that can evoke them. Frame identification was first modeled as a supervised classification task, based on linguistic features (Das et al., 2010). While such systems address the ambiguity problem to some degree, they tend to struggle with generalization. An alternative approach investigated the use of other machine-readable dictionaries (Green et al., 2004), but was not able to fundamentally overcome the generalization problem. Recent progress in supervised frame identification has come out of neural networks and distributed word representations (Peng et al., 2018; Hartmann et al., 2017). In these studies, frame-labeled corpora are used to learn embeddings for the frames as a side product of representation learning with different objective functions. Hermann et al. (2014) learned embeddings jointly for frames and the sentential contexts in which they were evoked. The current state-of-the-art in frame identification performs full-fledged semantic role labeling, i.e., it jointly assigns frames as well as frame elements, using a bi-directional LSTM architecture (Peng et al., 2018). 2.2 Distributed Representations of Word Meaning Distributed representations of word meanings (embe"
W19-0425,P14-1136,0,0.352942,"tems address the ambiguity problem to some degree, they tend to struggle with generalization. An alternative approach investigated the use of other machine-readable dictionaries (Green et al., 2004), but was not able to fundamentally overcome the generalization problem. Recent progress in supervised frame identification has come out of neural networks and distributed word representations (Peng et al., 2018; Hartmann et al., 2017). In these studies, frame-labeled corpora are used to learn embeddings for the frames as a side product of representation learning with different objective functions. Hermann et al. (2014) learned embeddings jointly for frames and the sentential contexts in which they were evoked. The current state-of-the-art in frame identification performs full-fledged semantic role labeling, i.e., it jointly assigns frames as well as frame elements, using a bi-directional LSTM architecture (Peng et al., 2018). 2.2 Distributed Representations of Word Meaning Distributed representations of word meanings (embeddings) have become a standard representation format in semantics. These models are grounded in the distributional hypothesis (Harris, 1954), according to which similar words are expected"
W19-0425,P18-1031,0,0.020575,"text, for example, the additive models first considered by Mitchell and Lapata (2008) fall into the prototype camp, while Erk and Pad´o (2010) propose exemplar-based models, and Reisinger and Mooney (2010) explore dynamic generalization in what they called ‘multi-prototype’ categorization models. However, for many tasks, such comparisons – on a level playing field – are missing. An interesting recent development in the embedding literature is the emergence of the distinction between pre-training and fine-tuning (e.g., in BERT (Devlin et al., 2018), OpenAI’s GPT (Radford et al., 2018), or ULM (Howard and Ruder, 2018)): pre-training constructs embeddings that are supposedly general and are robust across many tasks. Fine-tuning can then further optimize embeddings for one particular task, at the cost of robustness. Importantly, pre-training takes advantage of massive amounts of unlabeled data, while fine-tuning can leverage small amounts of task-specific labeled data. This distinction ties in nicely with open questions in the categorization literature concerning the respective roles of “bottom-up” similarity information and “top-down” theory information (Smith and Sloman, 1996): task-independent pre-trainin"
W19-0425,P08-1028,0,0.083221,"ect on work in lexical semantics. The properties of high-dimensional embeddings provide a good match with the assumption behind much of categorization theory – namely, that categories arise naturally from the similarity structure of individual objects (Erk, 2009). Given this context, the exemplar–prototype dichotomy is a useful dimension on which models can be situated in terms of how much they generalize over objects: low for exemplar-inspired, but high for prototype-inspired models. Regarding the representation of word meaning in context, for example, the additive models first considered by Mitchell and Lapata (2008) fall into the prototype camp, while Erk and Pad´o (2010) propose exemplar-based models, and Reisinger and Mooney (2010) explore dynamic generalization in what they called ‘multi-prototype’ categorization models. However, for many tasks, such comparisons – on a level playing field – are missing. An interesting recent development in the embedding literature is the emergence of the distinction between pre-training and fine-tuning (e.g., in BERT (Devlin et al., 2018), OpenAI’s GPT (Radford et al., 2018), or ULM (Howard and Ruder, 2018)): pre-training constructs embeddings that are supposedly gene"
W19-0425,N18-1135,0,0.232839,"exhaustively list all frames or the predicates that can evoke them. Frame identification was first modeled as a supervised classification task, based on linguistic features (Das et al., 2010). While such systems address the ambiguity problem to some degree, they tend to struggle with generalization. An alternative approach investigated the use of other machine-readable dictionaries (Green et al., 2004), but was not able to fundamentally overcome the generalization problem. Recent progress in supervised frame identification has come out of neural networks and distributed word representations (Peng et al., 2018; Hartmann et al., 2017). In these studies, frame-labeled corpora are used to learn embeddings for the frames as a side product of representation learning with different objective functions. Hermann et al. (2014) learned embeddings jointly for frames and the sentential contexts in which they were evoked. The current state-of-the-art in frame identification performs full-fledged semantic role labeling, i.e., it jointly assigns frames as well as frame elements, using a bi-directional LSTM architecture (Peng et al., 2018). 2.2 Distributed Representations of Word Meaning Distributed representation"
W19-0425,N10-1013,0,0.0223245,"n behind much of categorization theory – namely, that categories arise naturally from the similarity structure of individual objects (Erk, 2009). Given this context, the exemplar–prototype dichotomy is a useful dimension on which models can be situated in terms of how much they generalize over objects: low for exemplar-inspired, but high for prototype-inspired models. Regarding the representation of word meaning in context, for example, the additive models first considered by Mitchell and Lapata (2008) fall into the prototype camp, while Erk and Pad´o (2010) propose exemplar-based models, and Reisinger and Mooney (2010) explore dynamic generalization in what they called ‘multi-prototype’ categorization models. However, for many tasks, such comparisons – on a level playing field – are missing. An interesting recent development in the embedding literature is the emergence of the distinction between pre-training and fine-tuning (e.g., in BERT (Devlin et al., 2018), OpenAI’s GPT (Radford et al., 2018), or ULM (Howard and Ruder, 2018)): pre-training constructs embeddings that are supposedly general and are robust across many tasks. Fine-tuning can then further optimize embeddings for one particular task, at the c"
W19-0425,D13-1170,0,0.00372231,"le’s BERT (Devlin et al., 2018), which build contextualized embeddings for occurrences of words based on the context as well as their relative positions. A second important recent development concerns the objective(s) used to learn the embedding. While traditional count vectors and early embedding models like Skipgram assume that embeddings are general, and trained in an task-agnostic fashion, there is an alternative thread of research that sees the training of embeddings as a side product of training task-specific neural network models on tasks like sentiment analysis or machine translation (Socher et al., 2013; Hill et al., 2017). The most recent models reconcile these two directions with a two-phase transfer learning setup. The first phase is pre-training, where taskagnostic embeddings are learned from large, unlabeled corpora. The second phase is fine-tuning, which adapts the pre-trained embeddings to a specific task using comparatively small amounts of task-specific labeled corpora. 2.2.1 Bidirectional Encoder Representations from Transformers (BERT) BERT (Devlin et al., 2018) is a state-of-the-art embedding model that provides contextualized embeddings in a pre-training/fine-tuning setup. BERT"
W19-0425,D17-1128,0,0.024002,"Missing"
W19-2502,1993.eamt-1.1,0,0.61827,"l of newspaper pages, without manual OCR post-correction (see Figure 1). Due to the multi-column format of almost all newspapers, each text file contain multiple articles. In addition, many articles span several pages: they are split across text files. This is an obvious obstacle to any analysis requiring complete articles. It becomes particularly pressing for articles that span multiple issues (typically days or weeks). Notable among them are serial stories or serial novels, serialization being among the most important publication strategies for literary works in the 19th and 20th centuries (Lund, 1993). In this paper, we investigate the task of article identification across newspaper pages, corresponding to step (c) above. We use only textual information from OCR as input, modelling the task as a sequence of a segmentation and a clustering step. Whereas most previous work solely uses image data for similar tasks, here, we examine the performance of an approach that uses textual information only. We introduce and provide a new annotated dataset sampled from the 1912 New York Tribune magazine. We find that clustering segments works relatively well for individual issues and becomes substantial"
W19-2502,P98-1012,0,0.32733,"our knowledge, there is no standard dataset for article identification in historical newspapers.3 Thus, we created such a dataset. We selected the five March 1912 issues of the New York tribune Sunday magazine4 for annotation since this dataset contains long articles, some but not all of which are serializations that extend over multiple issues. We annotated a total of 82 pages. Evaluation. In the first experiment, only the clustering needs to be evaluated. For the evaluation, we rely on the B-cubed measure, an adaptation of the familiar IR precision/recall/F1 measure to the clustering setup (Bagga and Baldwin, 1998). In the second experiment, we additionally evaluate automatic segmentation, for which we report precision and recall. Using this measure is motivated as when using automatic text segmentation as a preprocessing step, we prefer high recall, resulting in fine-grained 3 The National Library of the Netherlands (https:// www.kb.nl/en) gives access to Dutch newspaper and also provides a classifier to detect different genres. However, they do not detect articles crossing pages and avoid advertisements. 4 This data is made available as PDF and text by the Library of Congress via Chronicling America:"
W19-2502,J06-1002,0,0.0571788,"4; Meier et al., 2017). This strategy avoids having to deal with spelling errors arising from OCR. However, these methods are not applicable when only textual output is available. A different line of research addresses the detection of segments in texts. Often, contemporary newspaper texts, Wikipedia articles or novels are artificially merged (e.g. Choi, 2000; Galley et al., 2003). Most of these methods are based on similarities between adjacent sentences or segments. The similarities are mostly computed using words (Hearst, 1997; Choi, 2000) or dense vector representations like topic models (Bestgen, 2006; Riedl and Biemann, 2012) or embeddings (Alemi and Ginsparg, 2015). Another related task is genre classification, in particular for newspaper texts. Lorang et al. (2015) present a classifier for detecting poetic content, which is however based again on images and incorporates image preprocessing techniques. Lonij and Harbers (2016) build a general genre classifier for text spans, but only for historical Dutch newspapers. A general limitation of this approach is that the articles which we want to separate may not differ in gender: this is often true (e.g., editorial content in the middle with"
W19-2502,Q17-1010,0,0.00887064,"hen perform the spectral clustering. Two measures of pairwise segment similarity appear particularly appropriate for OCRed, and thus noisy, texts. The traditional one is the similarity of words or character n-gram distributions, using the Jaccard coefficient. We hypothesize, that due to OCR errors, character n-grams might work better than using complete words. Thus, we compute the Jaccard coefficient on words as well as on character n-grams (n=2– 8). A more recent approach is using the cosine similarity between 200 dimensional embeddings defined as centroids of their fastText word embeddings (Bojanowski et al., 2017). Using fastText we benefit from the functionality that embeddings can be generated from out-of-vocabulary words. 4 5 Experimental Setup Preprocessing. We remove all non-alphanumeric characters and transform similarities exponentially for clustering. The fastText embeddings are trained on all 1912 English-language newspapers available from Library of Congress. Design. We conduct two experiments. In the first experiment, we use our gold standard (manually annotated) segment boundaries and perform only clustering. This setup reveals the performance of the clustering method. The second experiment"
W19-2502,A00-2004,0,0.243847,"belong to the same article). Then, we cluster segments within and across pages to assign all segments of the same article in one cluster. Most previous work performs the segmentation of newspaper pages directly at the image level (Hebert et al., 2014; Meier et al., 2017). This strategy avoids having to deal with spelling errors arising from OCR. However, these methods are not applicable when only textual output is available. A different line of research addresses the detection of segments in texts. Often, contemporary newspaper texts, Wikipedia articles or novels are artificially merged (e.g. Choi, 2000; Galley et al., 2003). Most of these methods are based on similarities between adjacent sentences or segments. The similarities are mostly computed using words (Hearst, 1997; Choi, 2000) or dense vector representations like topic models (Bestgen, 2006; Riedl and Biemann, 2012) or embeddings (Alemi and Ginsparg, 2015). Another related task is genre classification, in particular for newspaper texts. Lorang et al. (2015) present a classifier for detecting poetic content, which is however based again on images and incorporates image preprocessing techniques. Lonij and Harbers (2016) build a gener"
W19-2502,P03-1071,0,0.0346651,"e same article). Then, we cluster segments within and across pages to assign all segments of the same article in one cluster. Most previous work performs the segmentation of newspaper pages directly at the image level (Hebert et al., 2014; Meier et al., 2017). This strategy avoids having to deal with spelling errors arising from OCR. However, these methods are not applicable when only textual output is available. A different line of research addresses the detection of segments in texts. Often, contemporary newspaper texts, Wikipedia articles or novels are artificially merged (e.g. Choi, 2000; Galley et al., 2003). Most of these methods are based on similarities between adjacent sentences or segments. The similarities are mostly computed using words (Hearst, 1997; Choi, 2000) or dense vector representations like topic models (Bestgen, 2006; Riedl and Biemann, 2012) or embeddings (Alemi and Ginsparg, 2015). Another related task is genre classification, in particular for newspaper texts. Lorang et al. (2015) present a classifier for detecting poetic content, which is however based again on images and incorporates image preprocessing techniques. Lonij and Harbers (2016) build a general genre classifier fo"
W19-2502,J97-1003,0,0.765509,"gmentation of newspaper pages directly at the image level (Hebert et al., 2014; Meier et al., 2017). This strategy avoids having to deal with spelling errors arising from OCR. However, these methods are not applicable when only textual output is available. A different line of research addresses the detection of segments in texts. Often, contemporary newspaper texts, Wikipedia articles or novels are artificially merged (e.g. Choi, 2000; Galley et al., 2003). Most of these methods are based on similarities between adjacent sentences or segments. The similarities are mostly computed using words (Hearst, 1997; Choi, 2000) or dense vector representations like topic models (Bestgen, 2006; Riedl and Biemann, 2012) or embeddings (Alemi and Ginsparg, 2015). Another related task is genre classification, in particular for newspaper texts. Lorang et al. (2015) present a classifier for detecting poetic content, which is however based again on images and incorporates image preprocessing techniques. Lonij and Harbers (2016) build a general genre classifier for text spans, but only for historical Dutch newspapers. A general limitation of this approach is that the articles which we want to separate may not dif"
W19-4816,D13-1160,0,0.149515,"Missing"
W19-4816,D14-1044,0,0.152178,"for FB15K are available at https://github.com/ JosuaStadelmaier/CPM ple embedding lookups by Relational Graph Convolutional Networks which are used as an encoder to learn globally optimized knowledge graph representations. Shen et al. (2017) propose a dynamic memory architecture that learns to perform inference and represents the current state of the art. 2.3 Modeling Paths for KBC Several previous studies considered paths as information sources. Lao and Cohen (2010) use random walk probabilities for paths that connect e1 and e2 as features for scoring the correctness of facts (e1 , r, e2 ). Gardner et al. (2014) generalize the random walk approach with a relevance-based component. Unlike the “full” KBC models discussed above, however, these models do not represent entities as vectors, which prevents them from capturing entity specific information and from letting entities directly interact with relations. Guu et al. (2015) show how vector space models like TransE (Bordes et al., 2013), Bilinear (Nickel et al., 2011) and Bilinear-diag (Yang et al., 2015) can be generalized to not only scoring the correctness of edges t = (e1 , r, e2 ) but also the correctness of paths p = (e1 , r1 , ..., rk , e2 ). Th"
W19-4816,D15-1038,0,0.368843,"resents the current state of the art. 2.3 Modeling Paths for KBC Several previous studies considered paths as information sources. Lao and Cohen (2010) use random walk probabilities for paths that connect e1 and e2 as features for scoring the correctness of facts (e1 , r, e2 ). Gardner et al. (2014) generalize the random walk approach with a relevance-based component. Unlike the “full” KBC models discussed above, however, these models do not represent entities as vectors, which prevents them from capturing entity specific information and from letting entities directly interact with relations. Guu et al. (2015) show how vector space models like TransE (Bordes et al., 2013), Bilinear (Nickel et al., 2011) and Bilinear-diag (Yang et al., 2015) can be generalized to not only scoring the correctness of edges t = (e1 , r, e2 ) but also the correctness of paths p = (e1 , r1 , ..., rk , e2 ). They propose a training objective that incorporates paths and demonstrate that it improves the performance of KBC models on predicting paths and on predicting single edges as well. In the case of TransE, the relations r1 , ..., rk of a path p can be represented by their composition rp = r1 + ... + rk . The distance co"
W19-4816,W17-2608,0,0.0172361,"stands in relation r to e1 as e1 + r. The representations are learned using a max-margin objective which minimizes the distance between e2 ’s predicted and actual positions, ke1 + r − e2 k, for correct facts, and maximizes it otherwise. Research on novel neural architectures for KBC is ongoing. Schlichtkrull et al. (2018) replace sim1 The model and its annotated predictions for FB15K are available at https://github.com/ JosuaStadelmaier/CPM ple embedding lookups by Relational Graph Convolutional Networks which are used as an encoder to learn globally optimized knowledge graph representations. Shen et al. (2017) propose a dynamic memory architecture that learns to perform inference and represents the current state of the art. 2.3 Modeling Paths for KBC Several previous studies considered paths as information sources. Lao and Cohen (2010) use random walk probabilities for paths that connect e1 and e2 as features for scoring the correctness of facts (e1 , r, e2 ). Gardner et al. (2014) generalize the random walk approach with a relevance-based component. Unlike the “full” KBC models discussed above, however, these models do not represent entities as vectors, which prevents them from capturing entity sp"
W19-4816,D15-1082,0,0.0197777,"ning objective that incorporates paths and demonstrate that it improves the performance of KBC models on predicting paths and on predicting single edges as well. In the case of TransE, the relations r1 , ..., rk of a path p can be represented by their composition rp = r1 + ... + rk . The distance computed by TransE can then be generalized to paths as ke1 + rp − e2 k. The objective proposed by Guu et al. encourages that e1 + rp is learned to be close to the set of entities that are reached when traversing the knowledge graph over the edges r1 , ..., rk , starting from e1 . PTransE, proposed by Lin et al. (2015), assesses the correctness of t by considering paths that connect e1 and e2 and assigns them scores that aim to indicate how reliable these paths are for estimating the correctness of t. They compute the reliability scores by using a heuristic called path-constraint resource allocation, which is based on the sizes of entity sets that can be reached by following the relations in a path step by step. They report improvements in the KBC task over the standard TransE model. This supports the idea of modeling paths explicitly to capture the context of a triple. A similar approach by Toutanova et al"
W19-4816,N13-1095,0,0.0875115,"Missing"
W19-4816,P16-1136,0,0.0250122,"n et al. (2015), assesses the correctness of t by considering paths that connect e1 and e2 and assigns them scores that aim to indicate how reliable these paths are for estimating the correctness of t. They compute the reliability scores by using a heuristic called path-constraint resource allocation, which is based on the sizes of entity sets that can be reached by following the relations in a path step by step. They report improvements in the KBC task over the standard TransE model. This supports the idea of modeling paths explicitly to capture the context of a triple. A similar approach by Toutanova et al. (2016) is based on Bilinear-diag instead of TransE and comprises an 148 source for estimating the correctness of t as well as providing explanations for the estimate. efficient algorithm to incorporate paths. 2.4 Providing Explanations for KBC One possibility to provide explanations for KBC predictions is to generate logical rules. In the literature, these rules are often formalized as Horn rules (Gusm˜ao et al., 2018) such as (e1 , r1 , e2 ) ∧ (e2 , r2 , e3 ) → (e1 , r3 , e3 ). This rule claims that the path with the relation sequence r1 , r2 between e1 and e2 implies the presence of the relation r"
W19-4816,P17-1088,0,0.013249,"can take arbitrary triples as input, provided that the involved entities and relations occur in the training set. There are several studies that analyze learned representations of neural KBC models like TransE or Bilinear-diag to find Horn rules (Yang et al., 2015) or paths in the knowledge graph (Zhang et al., 2019). While similar in motivation to our model, these approaches share the disadvantage of using a pipeline approach: The rules or paths are extracted post hoc and cannot be used by the representation learning step to improve the consistency of its predictions, as would be desirable. Xie et al. (2017) propose a neural KBC model that provides an alternative kind of explainability: it learns sparse attention vectors which capture abstract concepts shared by multiple relations. Due to the sparsity of attention vectors, the connections can be visualized and interpreted. 3 3.1 Motivation Formally, we define a path of length k as a sequence of the form (e1 , r1 , ..., rk , e2 ). Our fundamental intuition is that the correctness of triples and paths in their context can show different degrees of correlation, as the following examples illustrate. Example 1: The triple t1 = (e1 , country of birth,"
