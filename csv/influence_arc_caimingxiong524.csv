2020.acl-demos.24,P19-1444,0,0.163026,"Missing"
2020.acl-demos.24,D19-1378,0,0.0980991,"Missing"
2020.acl-demos.24,D18-1547,0,0.0138849,"by (Rajpurkar et al., 2018), we create a synthetic dataset which consists of untranslatable questions generated by applying rule-based transformations and adversarial filtering (Zellers et al., 2018) to examples in existing text-to-SQL datasets. We then train a stagewise model that first classifies if the input is translatable or not, and then predicts confusing spans in an untranslatable input. Dataset Construction. In order to construct the untranslatable questions, we firstly exam the types of untranslatable questions seen on the manually constructed CoSQL (Yu et al., 2019a) and MultiWOZ (Budzianowski et al., 2018) datasets (Table 4 of A.1). We then design our modification strategies to generate the untranslatable questions from the original text-to-SQL dataset automatically. Specifically, for a text-to-SQL example that contains a natural language question, a DB schema and a SQL query, we first identify all non-overlapping question spans that possibly refer to a table field occurred in the SELECT and WHERE clauses of the SQL query using string-matching heuristics. Then we apply Swap and Drop operations on the question and DB schema respectively to generate different types of untranslatable questions. Th"
2020.acl-demos.24,H94-1010,0,0.812291,"Missing"
2020.acl-demos.24,N19-1423,0,0.524929,"plemented the neural semantic parser. † Work done during internship at Salesforce Research. Jagadish, 2014; Setlur et al., 2016, 2019). While they have been shown effective in pilot study and production, rule-based approaches are limited in terms of coverage, scalability and naturalness – they are not robust against the diversity of human language expressions and are difficult to scale across domains. Recent advances in neural natural language processing (Sutskever et al., 2014; Dong and Lapata, 2016; See et al., 2017a; Liang et al., 2017; Lin et al., 2019; Bogin et al., 2019a), pre-training (Devlin et al., 2019; Hwang et al., 2019), and the availability of large-scale supervised datasets (Zhong 204 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 204–214 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018, 2019b,a) enabled deep learning based approaches to significantly improve the state-of-theart in nearly all subtasks of building an NLIDB. These include semantic parsing (Dong and Lapata, 2018; Zhang et al., 2019), ambiguity detection and confidence estimation (Dong et al.,"
2020.acl-demos.24,P18-1033,0,0.0408187,"not robust against the diversity of human language expressions and are difficult to scale across domains. Recent advances in neural natural language processing (Sutskever et al., 2014; Dong and Lapata, 2016; See et al., 2017a; Liang et al., 2017; Lin et al., 2019; Bogin et al., 2019a), pre-training (Devlin et al., 2019; Hwang et al., 2019), and the availability of large-scale supervised datasets (Zhong 204 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 204–214 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018, 2019b,a) enabled deep learning based approaches to significantly improve the state-of-theart in nearly all subtasks of building an NLIDB. These include semantic parsing (Dong and Lapata, 2018; Zhang et al., 2019), ambiguity detection and confidence estimation (Dong et al., 2018; Yao et al., 2019), natural language response generation (Liu et al., 2019) and so on. Moreover, by jointly modeling the natural language question and database schema in the neural space, latest text-to-SQL semantic parsers can work cross domains (Yu et al., 2018; Zhang et al., 2019). In this work, we"
2020.acl-demos.24,H90-1021,0,0.541793,"Missing"
2020.acl-demos.24,P17-1003,0,0.168557,"teraction flow and the neural question corrector. Victoria designed and implemented the neural semantic parser. † Work done during internship at Salesforce Research. Jagadish, 2014; Setlur et al., 2016, 2019). While they have been shown effective in pilot study and production, rule-based approaches are limited in terms of coverage, scalability and naturalness – they are not robust against the diversity of human language expressions and are difficult to scale across domains. Recent advances in neural natural language processing (Sutskever et al., 2014; Dong and Lapata, 2016; See et al., 2017a; Liang et al., 2017; Lin et al., 2019; Bogin et al., 2019a), pre-training (Devlin et al., 2019; Hwang et al., 2019), and the availability of large-scale supervised datasets (Zhong 204 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 204–214 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018, 2019b,a) enabled deep learning based approaches to significantly improve the state-of-theart in nearly all subtasks of building an NLIDB. These include semantic parsing (Dong and Lapata, 2018; Zhang"
2020.acl-demos.24,P19-1600,0,0.0431758,"Missing"
2020.acl-demos.24,D15-1166,0,0.0190622,"ng Untranslatable Questions and Confusing Spans. We utilize the BERT contextualized representations of [CLS] token, followed by a single-layer classifier to tell whether a given user question and table schema can be translated into SQL or not. To identify the questionable token spans of untranslatable question, following Zhang et al. (2019), we employ a hierarchical bi-LSTM structure to encode each column header and use the hidden states as the column header embedding. We then use a bi-LSTM to encode the question’s BERT embedding, and the hidden states are fed into a dot-product co-attention (Luong et al., 2015) layer over the column header embedding. The output of co-attention augmented question embedding is fed into a linear layer follow by softmax operator to predict the start and end tokens indices of the confusing spans in the question. 3.2.2 Database-aware Token Correction Figure 5 illustrates the proposed tokens correction module in P HOTON. We use the masked language model (MLM) of BERT (Devlin et al., 2019) to auto-correct the confusing tokens. Specifically, we replace the confusing tokens with the [MASK] special token. The output distribution of MLM head on the mask token is employed to sco"
2020.acl-demos.24,P17-1099,0,0.0610987,"emented the demo interaction flow and the neural question corrector. Victoria designed and implemented the neural semantic parser. † Work done during internship at Salesforce Research. Jagadish, 2014; Setlur et al., 2016, 2019). While they have been shown effective in pilot study and production, rule-based approaches are limited in terms of coverage, scalability and naturalness – they are not robust against the diversity of human language expressions and are difficult to scale across domains. Recent advances in neural natural language processing (Sutskever et al., 2014; Dong and Lapata, 2016; See et al., 2017a; Liang et al., 2017; Lin et al., 2019; Bogin et al., 2019a), pre-training (Devlin et al., 2019; Hwang et al., 2019), and the availability of large-scale supervised datasets (Zhong 204 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 204–214 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018, 2019b,a) enabled deep learning based approaches to significantly improve the state-of-theart in nearly all subtasks of building an NLIDB. These include semantic parsing (Dong and"
2020.acl-demos.24,D19-1284,0,0.0131539,"on the Spider dev set, which validates the effectiveness of our neural semantic parser for translating an input question into a valid SQL query. 5 Related Work Natural Language Interfaces to Databases. NLIDBs has been studied extensively in the past decades. Thanks to the availability of large-scale datasets (Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018), data-driven approaches have dominated the field, in which deep learning based models achieve the best performance in both strongly (Hwang et al., 2019; Zhang et al., 2019; Guo et al., 2019) and weakly (Liang et al., 2017; Min et al., 2019) supervised settings. However, most of existing text-to-SQL datasets include only questions that can be translated into a valid SQL query. Spider (Finegan-Dollak et al., 2018) specifically controlled question clarify during data collection to exclude poorly phrased and ambiguous questions. WikiSQL (Zhong et al., 2017) was constructed on top of manually written synchronous grammars, and the mapping between its questions and SQL queries can be effectively resolved via lexical matching in vector space (Hwang et al., 2019). CoSQL (Yu et al., 2019a) is by far the only existing corpus to our knowled"
2020.acl-demos.24,J84-3009,0,0.225912,"Missing"
2020.acl-demos.24,P18-2124,0,0.07226,"Missing"
2020.acl-demos.24,D19-1547,0,0.0418812,"estion “How many are there?”. Once we have the synthetic untranslatable questions, adversarial filtering is employed to iteratively refine the set of untranslatable examples to be more indiscernible by trivial stylistic classifiers (Zellers et al., 2018). Predicting Untranslatable Questions and Confusing Spans. We utilize the BERT contextualized representations of [CLS] token, followed by a single-layer classifier to tell whether a given user question and table schema can be translated into SQL or not. To identify the questionable token spans of untranslatable question, following Zhang et al. (2019), we employ a hierarchical bi-LSTM structure to encode each column header and use the hidden states as the column header embedding. We then use a bi-LSTM to encode the question’s BERT embedding, and the hidden states are fed into a dot-product co-attention (Luong et al., 2015) layer over the column header embedding. The output of co-attention augmented question embedding is fed into a linear layer follow by softmax operator to predict the start and end tokens indices of the confusing spans in the question. 3.2.2 Database-aware Token Correction Figure 5 illustrates the proposed tokens correctio"
2020.acl-demos.24,D19-1204,1,0.830008,"Missing"
2020.acl-demos.24,D18-1425,0,0.230392,"Missing"
2020.acl-demos.24,P19-1443,1,0.936631,"able Question Detection Inspired by (Rajpurkar et al., 2018), we create a synthetic dataset which consists of untranslatable questions generated by applying rule-based transformations and adversarial filtering (Zellers et al., 2018) to examples in existing text-to-SQL datasets. We then train a stagewise model that first classifies if the input is translatable or not, and then predicts confusing spans in an untranslatable input. Dataset Construction. In order to construct the untranslatable questions, we firstly exam the types of untranslatable questions seen on the manually constructed CoSQL (Yu et al., 2019a) and MultiWOZ (Budzianowski et al., 2018) datasets (Table 4 of A.1). We then design our modification strategies to generate the untranslatable questions from the original text-to-SQL dataset automatically. Specifically, for a text-to-SQL example that contains a natural language question, a DB schema and a SQL query, we first identify all non-overlapping question spans that possibly refer to a table field occurred in the SELECT and WHERE clauses of the SQL query using string-matching heuristics. Then we apply Swap and Drop operations on the question and DB schema respectively to generate diff"
2020.acl-demos.24,D18-1009,0,0.109076,"sifier to detect user input to which a SQL mapping cannot be immediately determined. This covers questions that are incomplete (e.g. What is the total?), ambiguous or vague (e.g. Show me homes with good schools), beyond the representation scope of SQL (e.g. How many tourists visited all of the 10 attractions?), or simply noisy (e.g. Cyrus teaches physics in department). 3.2.1 Untranslatable Question Detection Inspired by (Rajpurkar et al., 2018), we create a synthetic dataset which consists of untranslatable questions generated by applying rule-based transformations and adversarial filtering (Zellers et al., 2018) to examples in existing text-to-SQL datasets. We then train a stagewise model that first classifies if the input is translatable or not, and then predicts confusing spans in an untranslatable input. Dataset Construction. In order to construct the untranslatable questions, we firstly exam the types of untranslatable questions seen on the manually constructed CoSQL (Yu et al., 2019a) and MultiWOZ (Budzianowski et al., 2018) datasets (Table 4 of A.1). We then design our modification strategies to generate the untranslatable questions from the original text-to-SQL dataset automatically. Specifica"
2020.acl-main.408,2020.acl-main.386,0,0.013867,"make a prediction. We refer to rationales that correspond to the inputs most relied upon to come to a disposition as faithful. Most automatic evaluations of faithfulness measure the impact of perturbing or erasing words or tokens identified as important on model output (Arras et al., 2017; Montavon et al., 2017; Serrano and Smith, 2019; Samek et al., 2016; Jain and Wallace, 2019). We build upon these methods in Section 4. Finally, we note that a recent article urges the community to evaluate faithfulness on a continuous scale of acceptability, rather than viewing this as a binary proposition (Jacovi and Goldberg, 2020). 3 Datasets in ERASER For all datasets in ERASER we distribute both reference labels and rationales marked by humans as supporting these in a standardized format. We 4445 delineate train, validation, and test splits for all corpora (see Appendix A for processing details). We ensure that these splits comprise disjoint sets of source documents to avoid contamination.3 We have made the decision to distribute the test sets publicly,4 in part because we do not view the ‘correct’ metrics to use as settled. We plan to acquire additional human annotations on held-out portions of some of the included"
2020.acl-main.408,N19-1357,1,0.918768,"uring training. However, such direct supervision will not always be available, motivating work on methods that can explain (or “rationalize”) model predictions using only instance-level supervision. In the context of modern neural models for text classification, one might use variants of attention (Bahdanau et al., 2015) to extract rationales. Attention mechanisms learn to assign soft weights to (usually contextualized) token representations, and so one can extract highly weighted tokens as rationales. However, attention weights do not in general provide faithful explanations for predictions (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020; Brunner et al., 2020; Moradi et al., 2019; Vashishth et al., 2019). This likely owes to encoders entangling inputs, complicating the interpretation of attention weights on inputs over contextualized representations of the same.2 By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the classifier, by construction providing faithful explanations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al. (2016) proposed i"
2020.acl-main.408,2020.acl-main.409,1,0.78141,"g the interpretation of attention weights on inputs over contextualized representations of the same.2 By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the classifier, by construction providing faithful explanations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al. (2016) proposed instantiating two models with their own parameters; one to extract rationales, and one that consumes these to make a prediction. They trained these models jointly via REINFORCE (Williams, 1992) style optimization. Recently, Jain et al. (2020) proposed a variant of this two-model setup that uses heuristic feature scores to derive pseudo-labels on tokens comprising rationales; one model can then be used to perform hard extraction in this way, while a second (independent) model can make predictions on the basis of these. Elsewhere, Chang et al. (2019) introduced the notion of classwise rationales that explains support for different output classes using a game theoretic framework. Finally, other recent work has proposed using a differentiable binary mask over inputs, which also avoids recourse to REINFORCE (Bastings et al., 2019). Pos"
2020.acl-main.408,N18-1023,0,0.0239798,"for BoolQ, wherein source documents in the original train and validation set were not disjoint and we preserve this structure in our dataset. Questions, of course, are disjoint. 4 Consequently, for datasets that have been part of previous benchmarks with other aims (namely, GLUE/superGLUE) but which we have re-purposed for work on rationales in ERASER, e.g., BoolQ (Clark et al., 2019), we have carved out for release test sets from the original validation sets. 5 Annotation details are in Appendix B. texts. We take a subset of this dataset, including only supported and refuted claims. MultiRC (Khashabi et al., 2018). A reading comprehension dataset composed of questions with multiple correct answers that by construction depend on information from multiple sentences. Here each rationale is associated with a question, while answers are independent of one another. We convert each rationale/question/answer triplet into an instance within our dataset. Each answer candidate then has a label of True or False. Commonsense Explanations (CoS-E) (Rajani et al., 2019). This corpus comprises multiplechoice questions and answers from (Talmor et al., 2019) along with supporting rationales. The rationales in this case c"
2020.acl-main.408,D19-1002,0,0.0200855,"will not always be available, motivating work on methods that can explain (or “rationalize”) model predictions using only instance-level supervision. In the context of modern neural models for text classification, one might use variants of attention (Bahdanau et al., 2015) to extract rationales. Attention mechanisms learn to assign soft weights to (usually contextualized) token representations, and so one can extract highly weighted tokens as rationales. However, attention weights do not in general provide faithful explanations for predictions (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020; Brunner et al., 2020; Moradi et al., 2019; Vashishth et al., 2019). This likely owes to encoders entangling inputs, complicating the interpretation of attention weights on inputs over contextualized representations of the same.2 By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the classifier, by construction providing faithful explanations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al. (2016) proposed instantiating two models with their own parameters; on"
2020.acl-main.408,D19-1420,0,0.0131407,"in the test set in Table 3. 4.2 Measuring faithfulness As discussed above, a model may provide rationales that are plausible (agreeable to humans) but that it did not rely on for its output. In many settings one may want rationales that actually explain model predictions, i.e., rationales extracted for an instance in this case ought to have meaningfully influenced its prediction for the same. We call these faithful rationales. How best to measure rationale faithfulness is an open question. In this first version of ERASER we propose simple metrics motivated by prior work (Zaidan et al., 2007; Yu et al., 2019). In particular, following Yu et al. (2019) we define metrics intended to measure the comprehensiveness (were all features needed to make a prediction selected?) and sufficiency (do the extracted rationales contain enough signal to come to a disposition?) of rationales, respectively. Comprehensiveness. To calculate rationale comprehensiveness we create contrast examples (Zaidan et al., 2007): We construct a contrast example for xi , x ˜i , which is xi with the predicted rationales ri removed. Assuming a classification setting, let m(xi )j be the original prediction provided by a model m for th"
2020.acl-main.408,N07-1033,0,0.340523,"cy in terms of model performance realized given a fixed amount of annotator effort (Zaidan and Eisner, 2008). In particular, recent work by McDonnell et al. (2017, 2016) has observed that at least for some tasks, asking annotators to provide rationales justifying their categorizations does not impose much additional effort. Combining rationale annotation with active learning (Settles, 2012) is another promising direction (Wallace et al., 2010; Sharma et al., 2015). Learning from rationales. Work on learning from rationales marked by annotators for text classification dates back over a decade (Zaidan et al., 2007). Earlier efforts proposed extending standard discriminative models like Support Vector Machines (SVMs) with regularization terms that penalized parameter estimates which disagreed with provided rationales (Zaidan et al., 2007; Small et al., 2011). Other efforts have attempted to specify generative models of rationales (Zaidan and Eisner, 2008). More recent work has aimed to exploit rationales in training neural text classifiers. Zhang et al. (2016) proposed a rationale-augmented Convolutional Neural Network (CNN) for text classification, explicitly trained to identify sentences supporting cat"
2020.acl-main.408,D08-1004,0,0.182552,"aviors (Feng et al., 2018). Gradients of course assume model differentiability. Other methods do not require any model properties. Examples include LIME (Ribeiro et al., 2016) and Alvarez-Melis and Jaakkola (2017); these methods approximate model behavior locally by having it repeatedly make predictions over perturbed inputs and fitting a simple, explainable model over the outputs. Acquiring rationales. Aside from interpretability considerations, collecting rationales from annotators may afford greater efficiency in terms of model performance realized given a fixed amount of annotator effort (Zaidan and Eisner, 2008). In particular, recent work by McDonnell et al. (2017, 2016) has observed that at least for some tasks, asking annotators to provide rationales justifying their categorizations does not impose much additional effort. Combining rationale annotation with active learning (Settles, 2012) is another promising direction (Wallace et al., 2010; Sharma et al., 2015). Learning from rationales. Work on learning from rationales marked by annotators for text classification dates back over a decade (Zaidan et al., 2007). Earlier efforts proposed extending standard discriminative models like Support Vector"
2020.acl-main.408,D16-1076,1,0.90058,"Missing"
2020.acl-main.484,P19-1166,0,0.0911995,"d Debias algorithm first identifies a subspace that captures gender bias. Let µi := X → − w /|Di |. (3) w∈Di The bias subspace B is the first k (≥ 1) rows of SVD(C), where C := m X X − − (→ w − µi )T (→ w − µi )/|Di | (4) i=1 w∈Di Following the original implementation of Bolukbasi et al. (2016), we set k = 1. As a result the subspace B is simply a gender direction.3 Hard Debias then neutralizes the word embed− dings by transforming each → w such that every word 3 Bolukbasi et al. (2016) normalize all embeddings. However, we found it is unnecessary in our experiments. This is also mentioned in Ethayarajh et al. (2019) 5445 Figure 2: Clustering accuracy after projecting out D-th dominating direction and applying Hard Debias. Lower accuracy indicates less bias. w ∈ N has zero projection in the gender subspace. − For each word w ∈ N , we re-embed → w: → − − − w := → w −→ wB (5) Neighborhood Metric. The Neighborhood Metric proposed by (Gonen and Goldberg, 2019) is a bias measurement that does not rely on any specific gender direction. To do so it looks into similarities between words. The bias of a word is the proportion of words with the same gender bias polarity among its nearest neighboring words. We select"
2020.acl-main.484,W19-3621,0,0.590765,"tionally expensive; and 2) pre-trained biased word embeddings have already been extensively adopted in downstream NLP products and post-hoc bias mitigation presumably leads to less changes in the model pipeline since it keeps the core components of the original embeddings. Existing post-processing algorithms, including the seminal Hard Debias (Bolukbasi et al., 2016), debias embeddings by removing the component that corresponds to a gender direction as defined by a list of gendered words. While Bolukbasi et al. (2016) demonstrates that such methods alleviate gender bias in word analogy tasks, Gonen and Goldberg (2019) argue that the effectiveness of these efforts is limited, as the gender bias can still be recovered from the geomrtry of the debiased embeddings. We hypothesize that it is difficult to isolate the gender component of word embeddings in the manner employed by existing post-processing methods. For example, Gong et al. (2018); Mu and Viswanath (2018) show that word frequency significantly impact the geometry of word embeddings. Consequently, popular words and rare words cluster in different subregions of the embedding space, despite the fact that words in these clusters are not semantically simi"
2020.acl-main.484,P19-1160,0,0.0643372,"Missing"
2020.acl-main.484,W11-2501,0,0.028387,"among words. Concept Categorization. The goal of concept categorization is to cluster a set of words into different categorical subsets. For example, “sandwich” and “hotdog” are both food and “dog” and “cat” are animals. The clustering performance is evaluated in terms of purity (Manning et al., 2008) - the fraction of the total number of the words that are correctly classified. Experiments are conducted on four benchmark datasets: the Almuhareb-Poesio (AP) dataset (Almuhareb, 2006); the ESSLLI 2008 (Baroni et al., 2008); the Battig 1969 set (Battig and Montague, 1969) and the BLESS dataset (Baroni and Lenci, 2011). We run classical Kmeans algorithm with fixed k. Across four datasets, the performance of Double-Hard GloVe is on a par with GloVe embeddings, showing that the proposed debiasing method preserves useful semantic information in word embeddings. Full results can be found in Table4. 5 Related Work Gender Bias in Word Embeddings. Word embeddings have been criticized for carrying gender bias. Bolukbasi et al. (2016) show that word2vec (Mikolov et al., 2013b) embeddings trained on the Google News dataset exhibit occupational stereotypes, e.g. “programmer” is closer to “man” and “homemaker” is close"
2020.acl-main.484,W19-3805,0,0.077574,"Missing"
2020.acl-main.484,D17-1018,0,0.0587694,"Missing"
2020.acl-main.484,N13-1090,0,0.36459,"ddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches. 1 Introduction Despite widespread use in natural language processing (NLP) tasks, word embeddings have been criticized for inheriting unintended gender bias from training corpora. Bolukbasi et al. (2016) highlights that in word2vec embeddings trained on the Google News dataset (Mikolov et al., 2013a), “programmer” is more closely associated with “man” and “homemaker” is more closely associated with “woman”. Such gender bias also propagates to downstream tasks. Studies have shown that coreference resolution systems exhibit gender bias in predictions due to the use of biased word embeddings (Zhao et al., 2018a; Rudinger et al., 2018). Given the fact that pre-trained word embeddings ∗ This research was conducted during the author’s internship at Salesforce Research. have been integrated into a vast number of NLP models, it is important to debias word embeddings to prevent discrimination in"
2020.acl-main.484,D14-1162,0,0.0983395,"“her & his”, “herself & himself”, and “Mary & John” (Bolukbasi et al., 2016). 5444 corpus can degrade the quality of word embeddings. By carefully removing such frequency features, existing word embeddings can achieve higher performance on several benchmarks after fine-tuning. We hypothesize that such word frequency statistics also interferes with the components of the word embeddings associated with gender. In other words, frequency-based features learned by word embedding algorithms act as harmful noise in the previously proposed debiasing techniques. To verify this, we first retrain GloVe (Pennington et al., 2014) embeddings on the one billion English word benchmark (Chelba et al., 2013) following previous work (Zhao et al., 2018b; Kaneko and Bollegala, 2019). We obtain ten difference vectors for the gendered pairs in P and compute pairwise cosine similarity. This gives a similarity matrix S in which Spi ,pj denotes the cosine similarity between − − difference vectors → v pairi and → v pairj . We then select a specific word pair, e.g. “boy” & “girl”, and augment the corpus by sampling sentences containing the word “boy” twice. In this way, we produce a new training corpus with altered word frequency st"
2020.acl-main.484,P19-1164,0,0.0304662,"embeddings on word analogy and concept categorization benchmark datasets. Performance (x100) is measured in accuracy and purity, respectively. On both tasks, there is no significant degradation of performance due to applying the proposed method. et al., 2019) demonstrate that contextualized word embeddings also inherit gender bias. Gender bias in word embeddings also propagate to downstream tasks, which substantially affects predictions. Zhao et al. (2018a) show that coreference systems tend to link occupations to their stereotypical gender, e.g. linking “doctor” to “he” and “nurse” to “she”. Stanovsky et al. (2019) observe that popular industrial and academic machine translation systems are prone to gender biased translation errors. Recently, Vig et al. (2020) proposed causal mediation analysis as a way to interpret and analyze gender bias in neural models. Debiasing Word Embeddings. For contextualized embeddings, existing works propose taskspecific debiasing methods, while in this paper we focus on more generic ones. To mitigate gender bias, Zhao et al. (2018a) propose a new training approach which explicitly restricts gender information in certain dimensions during training. While this method separate"
2020.acl-main.484,N19-1064,1,0.869635,"hm with fixed k. Across four datasets, the performance of Double-Hard GloVe is on a par with GloVe embeddings, showing that the proposed debiasing method preserves useful semantic information in word embeddings. Full results can be found in Table4. 5 Related Work Gender Bias in Word Embeddings. Word embeddings have been criticized for carrying gender bias. Bolukbasi et al. (2016) show that word2vec (Mikolov et al., 2013b) embeddings trained on the Google News dataset exhibit occupational stereotypes, e.g. “programmer” is closer to “man” and “homemaker” is closer to “woman”. More recent works (Zhao et al., 2019; Kurita et al., 2019; Basta 5449 (a) GloVe (b) GN-GloVe (c) GN-GloVe(wa ) (d) GP-GloVe (e) GP-GN-GloVe (f) Hard-GloVe (g) Strong Hard-GloVe (h) Double-Hard GloVe Figure 3: tSNE visualization of top 500 most male and female embeddings. Double-Hard GloVe mixes up two groups to the maximum extent, showing less gender information is encoded. Embeddings Sem Analogy Syn Total MSR AP Concept Categorization ESSLI Battig BLESS GloVe 80.5 62.8 70.8 54.2 55.6 72.7 51.2 81.0 GN-GloVe GN-GloVe(wa ) 77.7 77.7 61.6 61.6 68.9 68.9 51.9 51.9 56.9 56.9 70.5 75.0 49.5 51.3 85.0 82.5 GP-GloVe GP-GN-GloVe 80.6 77"
2020.acl-main.484,N18-2003,1,0.485823,"s. 1 Introduction Despite widespread use in natural language processing (NLP) tasks, word embeddings have been criticized for inheriting unintended gender bias from training corpora. Bolukbasi et al. (2016) highlights that in word2vec embeddings trained on the Google News dataset (Mikolov et al., 2013a), “programmer” is more closely associated with “man” and “homemaker” is more closely associated with “woman”. Such gender bias also propagates to downstream tasks. Studies have shown that coreference resolution systems exhibit gender bias in predictions due to the use of biased word embeddings (Zhao et al., 2018a; Rudinger et al., 2018). Given the fact that pre-trained word embeddings ∗ This research was conducted during the author’s internship at Salesforce Research. have been integrated into a vast number of NLP models, it is important to debias word embeddings to prevent discrimination in NLP systems. To mitigate gender bias, prior work have proposed to remove the gender component from pre-trained word embeddings through postprocessing (Bolukbasi et al., 2016), or to compress the gender information into a few dimensions of the embedding space using a modified training scheme (Zhao et al., 2018b; K"
2020.acl-main.484,D18-1521,0,0.504466,"s. 1 Introduction Despite widespread use in natural language processing (NLP) tasks, word embeddings have been criticized for inheriting unintended gender bias from training corpora. Bolukbasi et al. (2016) highlights that in word2vec embeddings trained on the Google News dataset (Mikolov et al., 2013a), “programmer” is more closely associated with “man” and “homemaker” is more closely associated with “woman”. Such gender bias also propagates to downstream tasks. Studies have shown that coreference resolution systems exhibit gender bias in predictions due to the use of biased word embeddings (Zhao et al., 2018a; Rudinger et al., 2018). Given the fact that pre-trained word embeddings ∗ This research was conducted during the author’s internship at Salesforce Research. have been integrated into a vast number of NLP models, it is important to debias word embeddings to prevent discrimination in NLP systems. To mitigate gender bias, prior work have proposed to remove the gender component from pre-trained word embeddings through postprocessing (Bolukbasi et al., 2016), or to compress the gender information into a few dimensions of the embedding space using a modified training scheme (Zhao et al., 2018b; K"
2020.acl-main.706,D19-1052,0,0.0451164,"Missing"
2020.acl-main.706,2020.acl-main.708,0,0.20003,"e bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions and Future Directi"
2020.acl-main.706,2020.acl-main.18,0,0.0282211,"e bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions and Future Directi"
2020.acl-main.706,P17-1025,0,0.0299591,"for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a classifier and significantly improve Commonse"
2020.acl-main.706,W18-6505,0,0.0214463,"Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained"
2020.acl-main.706,P16-1154,0,0.0267129,"sts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be men"
2020.acl-main.706,P16-1014,0,0.0262209,", 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generate"
2020.acl-main.706,P16-1195,0,0.0197135,"generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale"
2020.acl-main.706,P04-1050,0,0.0480877,"Missing"
2020.acl-main.706,N12-1093,0,0.0316404,"e to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have show"
2020.acl-main.706,D16-1128,0,0.0238773,"ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 20"
2020.acl-main.706,D16-1011,0,0.0343869,"which are combined with natural language for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a"
2020.acl-main.706,P19-1195,0,0.115119,"f variable diameters) in the simulator to achieve a given goal. Figure 1 shows an example of a task with a specified goal. The input to ESPRIT is a sequence of frames from a physics simulation and the output is a natural language narrative that reflects the locations of the objects in the initial scene and a description of the sequence of physical events that would lead to the desired goal state, as shown in Figure 2. The first phase of the framework uses a neural network classifier to identify salient frames from the simulation. For the second phase we experimented with table-to-text models (Puduppully et al., 2019a,b) as well as pre-trained language models (Radford et al., 2018). We evaluated our framework for natural language generated reasoning using several automated and human evaluations with a focus on the understanding of qualitative physics and the ordering of a natural sequence of physical events. We found that our model achieves very high performance for phase one (identifying frames with salient physical events) and that, for phase two, the table-to-text models outperform pre-trained language models on qualitative physics reasoning. 2 2.1 Dataset PHYRE Benchmark We build our dataset by extend"
2020.acl-main.706,P09-1011,0,0.0603867,"et al. (2020) propose to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulceh"
2020.acl-main.706,P19-1487,1,0.734658,"(Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a classifier and significantly improve CommonsenseQA performance. Bisk et al. (2020) propose to use a questi"
2020.acl-main.706,D15-1166,0,0.0153733,"; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that"
2020.acl-main.706,N16-1086,0,0.015485,"ing task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results"
2020.acl-main.706,W00-1418,0,0.219091,"explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While tra"
2020.acl-main.706,N18-1137,0,0.0145506,"entifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al.,"
2020.acl-main.706,P17-2034,0,0.0279544,"hang et al., 2016). Recent papers use neural networks over visual inputs to predict future pixels (Finn et al., 2016; Lerer et al., 2016; Mirza et al., 2016; Du and Narasimhan, 2019) or make qualitative predictions (Groth et al., 2018; Li et al., 2016, 2017; Janner et al., 2019; Wu et al., 2015; Mao et al., 2019). Furthermore, several frameworks and benchmarks have been introduced to test visual reasoning such as PHYRE (Bakhtin et al., 2019), Mujoco (Todorov et al., 2012), and Intphys (Riochet et al., 2018), some of which are combined with natural language for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al.,"
2020.acl-main.706,P98-2209,0,0.135936,"ining and inference in a classifier and significantly improve CommonsenseQA performance. Bisk et al. (2020) propose to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention ("
2020.acl-main.706,P18-1151,1,0.861331,"We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use di"
2020.acl-main.706,D15-1199,0,0.0419796,"Missing"
2020.acl-main.706,D17-1239,0,0.0932855,"alls fall. The red ball lands on the ground and the green ball lands on the red ball and rolls to the right over the black vertical bar. Generation (AVG) The red ball lands in the cubby and the green ball lands on top and a little to the right, sending the green ball right. It rolls over the short black wall of the cage and onto the floor, where it keeps rolling right towards the purple goal... The red ball falls and knocks the green ball off of its curved black platform and to the left. It rolls leftwards and continues falling until it lands on the purple floor... Generation (BiLSTM) maries (Wiseman et al., 2017). Second, the output generated by the BiLSTM model predicts the incorrect direction of motion for the green ball, an error that is occasionally seen across generation descriptions of both models. This indicates that a table-to-text paradigm for generating such solution explanations is not adequate for learning the direction of motion for the physical reasoning required for these explanations. Table 6: Example input records, gold annotation, and generated simulation description from the AVG and BiLSTM models, taken from example 00014:394. We show only a short segment of the actual input records"
2020.acl-main.706,2020.acl-main.224,0,0.0218279,"iptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions"
2020.acl-main.706,D14-1179,0,\N,Missing
2020.acl-main.706,P19-1202,0,\N,Missing
2020.acl-main.706,D19-1204,1,\N,Missing
2020.acl-main.706,C98-2204,0,\N,Missing
2020.acl-main.88,D18-1241,0,0.0381685,"t locate the span within the ground truth rule sentence, while in 9 cases, it finds the correct rule sentence but extracts a different span. Another challenge comes from the one-to-many problem in sequence generation. When there are multiple underspecified rule sentences, the model asks about one of these underspecified rule sentences which is different from the ground truth one. This suggests that new evaluation metrics could be proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC C"
2020.acl-main.88,N19-1423,0,0.47998,"licitly tracking rules with external memories boosts both the decision accuracy and the quality of generated follow-up questions. In particular, EMT outperforms the previous best model E3 by 1.3 in macro-averaged decision accuracy and 10.8 in BLEU4 for follow-up question generation. In addition to the performance improvement, EMT yields interpretability by explicitly tracking rules, which is visualized to show the entailment-oriented reasoning process of our model. 2 As illustrated in Figure 2, our proposed method consists of the following four main modules. (1) The Encoding module uses BERT (Devlin et al., 2019) to encode the concatenation of the rule text, initial question, scenario and dialog history into contextualized representations. However, there are two main drawbacks to the existing methods. First, with respect to the reasoning of the rule text, existing methods do not explicitly track whether a condition listed in the rule has already been satisfied as the conversation flows so that it can make a better decision. Second, with respect to the extraction of question-related rules, it is difficult in the current approach to extract the most relevant text span to generate the next question. For"
2020.acl-main.88,P18-1177,0,0.0218596,"Missing"
2020.acl-main.88,P19-1480,1,0.83023,"inds the correct rule sentence but extracts a different span. Another challenge comes from the one-to-many problem in sequence generation. When there are multiple underspecified rule sentences, the model asks about one of these underspecified rule sentences which is different from the ground truth one. This suggests that new evaluation metrics could be proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend"
2020.acl-main.88,D19-1001,0,0.306002,"onal question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend bidirectional sequence generation approach with mixed decision making and question generation stages. Saeidi et al. (2018) split it into sub-tasks and combines hand-designed sub-models for decision classification, entailment and question generation. Zhong and Zettlemoyer (2019) propose to extract all possible rule text spans, assign each of them an entailment score, and edit the span with the highest score into a follow-up question. However, they do not use these entailment scores for decision making. Sharma et al. (2019) study patterns of the dataset and include addition"
2020.acl-main.88,P18-1136,1,0.84912,"tion) separately, EMT is a unified approach that exploits its memory states for both decision making and question generation. Memory-Augmented Neural Networks. Our work is also related to memory-augmented neural networks (Graves et al., 2014, 2016), which have been applied in some NLP tasks such as question answering (Henaff et al., 2017) and machine translation (Wang et al., 2016). For dialog applications, Zhang et al. (2019) propose a dialogue management model that employs a memory controller and a slot-value memory, Bordes et al. (2016) learn a restaurant bot by end-to-end memory networks, Madotto et al. (2018) incorporate external memory modules into dialog generation. 5 Conclusions In this paper, we have proposed a new framework for conversational machine reading (CMR) that comprises a novel explicit memory tracker (EMT) to track entailment states of the rule sentences explicitly within its memory module. The updated states are utilized for decision making and coarseto-fine follow-up question generation in a unified manner. EMT achieved a new state-of-the-art result on the ShARC CMR challenge. EMT also gives interpretability by showing the entailment-oriented reasoning process as the conversation"
2020.acl-main.88,P02-1040,0,0.10833,"Missing"
2020.acl-main.88,Q19-1016,0,0.061325,"ithin the ground truth rule sentence, while in 9 cases, it finds the correct rule sentence but extracts a different span. Another challenge comes from the one-to-many problem in sequence generation. When there are multiple underspecified rule sentences, the model asks about one of these underspecified rule sentences which is different from the ground truth one. This suggests that new evaluation metrics could be proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi"
2020.acl-main.88,D18-1233,0,0.180418,"Missing"
2020.acl-main.88,D16-1027,0,0.0143228,"as two key differences: (1) EMT makes decision via explicitly entailmentoriented reasoning, which, to our knowledge, is the first such approach; (2) Instead of treating decision making and follow-up question generation (or span extraction) separately, EMT is a unified approach that exploits its memory states for both decision making and question generation. Memory-Augmented Neural Networks. Our work is also related to memory-augmented neural networks (Graves et al., 2014, 2016), which have been applied in some NLP tasks such as question answering (Henaff et al., 2017) and machine translation (Wang et al., 2016). For dialog applications, Zhang et al. (2019) propose a dialogue management model that employs a memory controller and a slot-value memory, Bordes et al. (2016) learn a restaurant bot by end-to-end memory networks, Madotto et al. (2018) incorporate external memory modules into dialog generation. 5 Conclusions In this paper, we have proposed a new framework for conversational machine reading (CMR) that comprises a novel explicit memory tracker (EMT) to track entailment states of the rule sentences explicitly within its memory module. The updated states are utilized for decision making and coar"
2020.acl-main.88,E17-1042,0,0.073334,"Missing"
2020.acl-main.88,P19-1078,1,0.832483,"this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend bidirectional sequence generation approach with mixed decision making and question generation stages. Saeidi et al. (2018) split it into sub-tasks and combines hand-designed sub-models for decision classification, entailment and question generation. Zhong and Zettlemoyer (2019) propose to extract all possible rule text spans, assign each of them an entailment score, and"
2020.acl-main.88,P18-1135,1,0.842643,"proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend bidirectional sequence generation approach with mixed decision making and question generation stages. Saeidi et al. (2018) split it into sub-tasks and combines hand-designed sub-models for decision classification, entailment and question generation. Zhong and Zettlemoyer (2019) propose to extract all possible rule text spans, assign each of them an ent"
2020.acl-main.88,P19-1223,0,0.218973,"Missing"
2020.emnlp-main.191,N18-1016,0,0.050044,"Missing"
2020.emnlp-main.191,N18-2097,0,0.0233528,"n: Is this loan suitable for me? Entailment State Gold: Entailment; Predict: Unknown 27 Relevant Rule: The Montgomery GI Bill (MGIB) is an educational assistance program enacted by Congress to attract high quality men and women into the Armed Forces. Scenario: I applied and found out I can get a loan. My dad wants me to join the army, but I don’t. I’d rather go to school. Question: Does this program meet my needs? Entailment State Gold: Contradiction; Predict: Unknown Figure 3: Types of scenario interpretation errors in the development data based on 100 samples. marization (Joty et al., 2019; Cohan et al., 2018; Xu et al., 2020). Recently, discourse information has also been introduced in neural reading comprehension. Mihaylov and Frank (2019) design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser to segment surface-level in-line conditions for entailment reasoning. 5 Conclusion In this paper, we present D ISCERN, a system that does discourse-aware entailment reasoning for conversational machine reading. D ISCERN explicitly buil"
2020.emnlp-main.191,N19-1423,0,0.0191646,", 2019) or coarse-to-fine reasoning (Gao et al., 2020). However, we find that such sophisticated modelings may not be necessary, and we propose a simple but effective approach here. We split the rule text into sentences and concatenate the rule sentences and user-provided information into a sequence. Then we use RoBERTa to encode them into vectors grounded to tokens, as here we want to predict the position of a span within the rule text. Let [t1,1 , ..., t1,s1 ; t2,1 , ..., t2,s2 ; ...; tN,1 , ..., tN,sN ] be the encoded vectors for tokens from N rule sentences, we follow the BERTQA approach (Devlin et al., 2019) to learn a start vector ws ∈ Rd and an end vector we ∈ Rd to locate the start and end positions, under the restriction that the start and end positions must belong to the same rule sentence: Span = arg max(ws&gt; tk,i + we&gt; tk,j ) (10) i,j,k where i, j denote the start and end positions of the selected span, and k is the sentence which the span belongs to. The training objective is the sum of the log-likelihoods of the correct start and end positions. To supervise the span extraction process, the noisy supervision of spans are generated by selecting the span which has the minimum edit distance w"
2020.emnlp-main.191,2020.acl-main.88,1,0.759919,"(Saeidi et al., 2018). In this case, the machine needs to read the rule text, interpret the user scenario, clarify the unknown user’s background by asking questions, and derive the final answer. Taking Figure 1 as an example, to answer the user whether he is suitable for the loan program, the machine needs to interpret the rule text to know what are the requirements, understand he meets “American small business” from the user scenario, ask follow-up clarification questions about “for-profit business” and “not get financing Existing approaches (Zhong and Zettlemoyer, 2019; Sharma et al., 2019; Gao et al., 2020) decompose this problem into two sub-tasks. Given the rule text, user question, user scenario, and dialog history (if any), the first sub-task is to make a decision among “Yes”, “No”, “Inquire” and “Irrelevant”. The “Yes/No” directly answers the user question and “Irrelevant” means the user question is unanswerable by the rule text. If the user-provided information (user scenario, previous dialogs) are not enough to determine his fulfillment or eligibility, an “Inquire” decision is made and the second sub-task is activated. The second sub-task is to capture the underspecified condition from th"
2020.emnlp-main.191,P19-4003,1,0.816408,"was flooded Question: Is this loan suitable for me? Entailment State Gold: Entailment; Predict: Unknown 27 Relevant Rule: The Montgomery GI Bill (MGIB) is an educational assistance program enacted by Congress to attract high quality men and women into the Armed Forces. Scenario: I applied and found out I can get a loan. My dad wants me to join the army, but I don’t. I’d rather go to school. Question: Does this program meet my needs? Entailment State Gold: Contradiction; Predict: Unknown Figure 3: Types of scenario interpretation errors in the development data based on 100 samples. marization (Joty et al., 2019; Cohan et al., 2018; Xu et al., 2020). Recently, discourse information has also been introduced in neural reading comprehension. Mihaylov and Frank (2019) design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser to segment surface-level in-line conditions for entailment reasoning. 5 Conclusion In this paper, we present D ISCERN, a system that does discourse-aware entailment reasoning for conversational machine reading. D IS"
2020.emnlp-main.191,J15-3002,1,0.901989,"Missing"
2020.emnlp-main.191,D19-1001,0,0.365184,"ard deviations. It takes two hours for training on a 4-core server with an Nvidia GeForce GTX Titan X GPU. 3.2 Results Decision Making Sub-task. The decision making results in macro- and micro- accuracy on the blind, held out test set of ShARC are shown in Table 1. D ISCERN outperforms the previous best 2443 End-to-End Task (Leaderboard Performance) Micro Acc. Macro Acc. BLEU1 BLEU4 Seq2Seq (Saeidi et al., 2018) 44.8 42.8 34.0 7.8 Pipeline (Saeidi et al., 2018) 61.9 68.9 54.4 34.4 BERTQA (Zhong and Zettlemoyer, 2019) 63.6 70.8 46.2 36.3 UrcaNet (Sharma et al., 2019) 65.1 71.2 60.5 46.1 BiSon (Lawrence et al., 2019) 66.9 71.6 58.8 44.3 E3 (Zhong and Zettlemoyer, 2019) 67.6 73.3 54.1 38.7 EMT (Gao et al., 2020) 69.4 74.8 60.9 46.0 EMT+entailment (Gao et al., 2020) 69.1 74.6 63.9 49.5 D ISCERN (our single model) 73.2 78.3 64.0 49.1 Models Table 1: Performance on the blind, held-out test set of ShARC end-to-end task. Models BERTQA E3 UrcaNet EMT D ISCERN Yes 61.2 65.9 63.3 70.5 71.9 No 61.0 70.6 68.4 73.2 75.8 Inq. 62.6 60.5 58.9 70.8 73.3 Models E3 E3 +UniLM EMT D ISCERN (BERT) D ISCERN Irr. 96.4 96.4 95.7 98.6 99.3 Table 2: Class-wise decision prediction accuracy among “Yes”, “No”, “Inquire” and “Irreleva"
2020.emnlp-main.191,D19-5808,0,0.0270142,"Missing"
2020.emnlp-main.191,2021.ccl-1.108,0,0.109338,"Missing"
2020.emnlp-main.191,D19-1257,0,0.0448277,"GIB) is an educational assistance program enacted by Congress to attract high quality men and women into the Armed Forces. Scenario: I applied and found out I can get a loan. My dad wants me to join the army, but I don’t. I’d rather go to school. Question: Does this program meet my needs? Entailment State Gold: Contradiction; Predict: Unknown Figure 3: Types of scenario interpretation errors in the development data based on 100 samples. marization (Joty et al., 2019; Cohan et al., 2018; Xu et al., 2020). Recently, discourse information has also been introduced in neural reading comprehension. Mihaylov and Frank (2019) design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser to segment surface-level in-line conditions for entailment reasoning. 5 Conclusion In this paper, we present D ISCERN, a system that does discourse-aware entailment reasoning for conversational machine reading. D ISCERN explicitly builds the connection between entailment states of conditions and the final decisions. Results on the ShARC benchmark shows that D ISCERN o"
2020.emnlp-main.191,P02-1040,0,0.107488,"text, user question, user scenario, and dialog history (if any). The output is the answer among Yes, No, Irrelevant, or a follow-up question. The train, development, and test dataset sizes are 21890, 2270, and 8276, respectively. Evaluation Metrics. The decision making subtask uses macro- and micro- accuracy of four classes “Yes”, “No”, “Irrelevant”, “Inquire” as metrics. For the question generation sub-task, we evaluate models under both the official end-to-end setting (Saeidi et al., 2018) and the recently proposed oracle setting (Gao et al., 2020). In the official setting, the BLEU score (Papineni et al., 2002) is calculated only when both the ground truth decision and the predicted decision are “Inquire”, which makes the score dependent on the model’s “Inquire” predictions. For the oracle question generation setting, models are asked to generate a question when the ground truth decision is “Inquire”. Implementation Details. For the decision making sub-task, we finetune RoBERTa-base model (Wolf et al., 2019) with Adam (Kingma and Ba, 2015) optimizer for 5 epochs with a learning rate of 5e-5, a warm-up rate of 0.1, a batch size of 16, and a dropout rate of 0.35. The number of inter-sentence transform"
2020.emnlp-main.191,D18-1233,0,0.181667,"Missing"
2020.emnlp-main.191,2020.acl-main.451,0,0.107211,"able for me? Entailment State Gold: Entailment; Predict: Unknown 27 Relevant Rule: The Montgomery GI Bill (MGIB) is an educational assistance program enacted by Congress to attract high quality men and women into the Armed Forces. Scenario: I applied and found out I can get a loan. My dad wants me to join the army, but I don’t. I’d rather go to school. Question: Does this program meet my needs? Entailment State Gold: Contradiction; Predict: Unknown Figure 3: Types of scenario interpretation errors in the development data based on 100 samples. marization (Joty et al., 2019; Cohan et al., 2018; Xu et al., 2020). Recently, discourse information has also been introduced in neural reading comprehension. Mihaylov and Frank (2019) design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser to segment surface-level in-line conditions for entailment reasoning. 5 Conclusion In this paper, we present D ISCERN, a system that does discourse-aware entailment reasoning for conversational machine reading. D ISCERN explicitly builds the connection"
2020.emnlp-main.191,P19-1223,0,0.148231,"Missing"
2020.emnlp-main.269,D19-1209,0,0.175531,"estions through multiple rounds of interactions together with visual content understanding. The primary research direction in VisDial has been mostly focusing on developing various attention mechanisms (Bahdanau et al., 2015) for a bet* This work was mainly done when Yue Wang was an intern at Salesforce Research Asia, Singapore. ter fusion of vision and dialog contents. Compared to VQA that predicts an answer based only on the question about the image (Figure 1(a)), VisDial needs to additionally consider the dialog history. Typically, most of previous work (Niu et al., 2019; Gan et al., 2019; Kang et al., 2019) uses the question as a query to attend to relevant image regions and dialog history, where their interactions are usually exploited to obtain better visual-historical cues for predicting the answer. In other words, the attention flow in these methods is unidirectional – from question to the other components (Figure 1(b)). By contrast, in this work, we allow for bidirectional attention flow between all the entities using a unified Transformer (Vaswani et al., 2017) encoder, as shown in Figure 1(c). In this way, all the entities simultaneously play the role of an “information seeker” (query) an"
2020.emnlp-main.269,Q14-1006,0,\N,Missing
2020.emnlp-main.269,D14-1086,0,\N,Missing
2020.emnlp-main.269,P18-1238,0,\N,Missing
2020.emnlp-main.269,N19-1423,0,\N,Missing
2020.emnlp-main.269,P19-1644,0,\N,Missing
2020.emnlp-main.409,D19-1131,0,0.0144262,"010). 4 4.1 Experiments Datasets The multi-domain Wizard-of-Oz (MWOZ) dataset (Budzianowski et al., 2018) is one of the most common benchmark datasets for task-oriented dialogue systems. We use MWOZ to evaluate domain identification, dialogue slot tagging, and dialogue act prediction tasks. It contains 8420/1000/1000 dialogues for training, validation, and testing sets, respectively. There are seven domains in the training set and five domains in the others. There are 13 unique system dialogue acts and 18 unique slots as shown in Table 2. Besides, we use the out-of-scope intent (OOS) dataset (Larson et al., 2019) for our intent detection experiment. The OOS dataset is one of the largest annotated intent datasets, including 15,100/3,100/5,500 samples for the train, validation, and test sets, respectively. It has 150 intent classes over ten domains and an additional out-ofscope intent class, a user utterance that does not fall into any of the predefined intents. The whole intent list is shown in the Appendix. 4.2 Training Details We first process user utterance and system response using the tokenizer corresponding to each per-trained model. To obtain each representation, we run most of the pre-trained m"
2020.emnlp-main.409,2020.acl-main.9,0,0.0274127,"Missing"
2020.emnlp-main.409,2020.emnlp-main.273,0,0.123919,"Introduction Task-oriented dialogue systems achieve specific user goals within a limited number of dialogue turns via natural language. They have been used in a wide range of applications, such as booking restaurants (Wen et al., 2017), providing tourist information (Budzianowski et al., 2018), ordering tickets (Schulz et al., 2017), and healthcare consultation (Wei et al., 2018). They are also crucial components of intelligent virtual assistants like Siri, Alexa, and Google Assistant. Most of the task-oriented dialogue systems nowadays, are benefited from transfer learning (Wu et al., 2019; Lin et al., 2020), especially pre-trained language models trained on general text, such as BERT (Devlin et al., 2018) and GPT2 (Radford et al., 2019). However, previous work claims that linguistic patterns could differ between writing text and human conversation, resulting in a large gap of data distributions (Bao et al., 2019; Wolf et al., 2019b). Recently, several approaches are leveraging open-domain data (Henderson et al., 2019; Zhang et al., 2019), or aggregating task-oriented data (Wu et al., 2020) to pre-train language models. In this paper, we are interested in answering these questions: which language"
2020.emnlp-main.409,P17-1080,0,0.0309036,"ny difference? We investigate how good these pre-trained representations are for a task-oriented dialogue system, ignoring the model architectures and training strategies by only probing their final representations with fine-tuning models. A good representation implies better knowledge transferring and domain generalization ability, making downstream applications easier and cheaper to be improved. We tackle this problem with two probing solutions: supervised classifier probe and unsupervised mutual information probe. Classifier probe is commonly used in different NLP tasks such as morphology (Belinkov et al., 2017), sentence length (Adi et al., 2016), or linguistic structure (Hewitt and 5036 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5036–5051, c November 16–20, 2020. 2020 Association for Computational Linguistics Manning, 2019). In this setting, we fine-tune a simple classifier for a specific task (e.g., intent identification) on a fixed pre-trained language model. The probe uses supervision to find the best transformation for each sub-task. In addition, we present mutual information probe to investigate these language models by directly clustering the"
2020.emnlp-main.409,2021.ccl-1.108,0,0.1772,"Missing"
2020.emnlp-main.409,D19-5602,0,0.0360578,"Missing"
2020.emnlp-main.409,D18-1547,0,0.105834,"Missing"
2020.emnlp-main.409,2020.acl-main.420,0,0.0217257,"guistic structure (Hewitt and 5036 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5036–5051, c November 16–20, 2020. 2020 Association for Computational Linguistics Manning, 2019). In this setting, we fine-tune a simple classifier for a specific task (e.g., intent identification) on a fixed pre-trained language model. The probe uses supervision to find the best transformation for each sub-task. In addition, we present mutual information probe to investigate these language models by directly clustering their output representations, as recent study (Pimentel et al., 2020) suggests that a simple classifier may not be able to achieve the best estimate of mutual information between features and the downstream task. We apply two clustering techniques, K-means (Lloyd, 1982) and Gaussian mixture model (Reynolds, 2009), to calculate its adjusted normalized mutual information (ANMI) (Vinh et al., 2010) between the predicted clustering and the true task-specific clustering. We investigate 12 language models, as shown in Table 1, where five of them have been pre-trained with dialogue data. We evaluate four core taskoriented dialogue tasks, domain identification, intent"
2020.emnlp-main.409,W17-2626,0,0.0373116,"Missing"
2020.emnlp-main.409,E17-1042,0,0.0605753,"community, 3) find insights of pre-training factors for dialogue application that may be the key to success. 1 Dial. Data X X X X X X X V V V V V Parameters 109.5M 11.7M 66.4M 124.6M 124.4M 33.5M 108.9M 29M 124.4M 119.5M 119.5M 124.4M Output Dim. 768 768 768 768 768 256 768 1024 768 768 768 768 Table 1: An overview of selected pre-trained language models (Details in Section 2). Introduction Task-oriented dialogue systems achieve specific user goals within a limited number of dialogue turns via natural language. They have been used in a wide range of applications, such as booking restaurants (Wen et al., 2017), providing tourist information (Budzianowski et al., 2018), ordering tickets (Schulz et al., 2017), and healthcare consultation (Wei et al., 2018). They are also crucial components of intelligent virtual assistants like Siri, Alexa, and Google Assistant. Most of the task-oriented dialogue systems nowadays, are benefited from transfer learning (Wu et al., 2019; Lin et al., 2020), especially pre-trained language models trained on general text, such as BERT (Devlin et al., 2018) and GPT2 (Radford et al., 2019). However, previous work claims that linguistic patterns could differ between writing t"
2020.emnlp-main.409,2020.emnlp-main.66,1,0.761369,"ost of the task-oriented dialogue systems nowadays, are benefited from transfer learning (Wu et al., 2019; Lin et al., 2020), especially pre-trained language models trained on general text, such as BERT (Devlin et al., 2018) and GPT2 (Radford et al., 2019). However, previous work claims that linguistic patterns could differ between writing text and human conversation, resulting in a large gap of data distributions (Bao et al., 2019; Wolf et al., 2019b). Recently, several approaches are leveraging open-domain data (Henderson et al., 2019; Zhang et al., 2019), or aggregating task-oriented data (Wu et al., 2020) to pre-train language models. In this paper, we are interested in answering these questions: which language model has the most informative representations that is better for what task-oriented dialogue task? Does pretraining with dialogue-specific data or different objectives make any difference? We investigate how good these pre-trained representations are for a task-oriented dialogue system, ignoring the model architectures and training strategies by only probing their final representations with fine-tuning models. A good representation implies better knowledge transferring and domain gener"
2020.emnlp-main.409,P19-1078,1,0.815859,"ls in Section 2). Introduction Task-oriented dialogue systems achieve specific user goals within a limited number of dialogue turns via natural language. They have been used in a wide range of applications, such as booking restaurants (Wen et al., 2017), providing tourist information (Budzianowski et al., 2018), ordering tickets (Schulz et al., 2017), and healthcare consultation (Wei et al., 2018). They are also crucial components of intelligent virtual assistants like Siri, Alexa, and Google Assistant. Most of the task-oriented dialogue systems nowadays, are benefited from transfer learning (Wu et al., 2019; Lin et al., 2020), especially pre-trained language models trained on general text, such as BERT (Devlin et al., 2018) and GPT2 (Radford et al., 2019). However, previous work claims that linguistic patterns could differ between writing text and human conversation, resulting in a large gap of data distributions (Bao et al., 2019; Wolf et al., 2019b). Recently, several approaches are leveraging open-domain data (Henderson et al., 2019; Zhang et al., 2019), or aggregating task-oriented data (Wu et al., 2020) to pre-train language models. In this paper, we are interested in answering these questi"
2020.emnlp-main.411,N19-5002,0,0.0187219,"xamples. Table 1 (b) shows a negative example, where the input utterance comes from the intent, “bill due”, and the paired sentence from another intent, “bill balance”. 5066 3.3 Seamless Transfer from NLI A key characteristic of our method is that we seek to model the relations between the utterance pairs, instead of explicitly modeling the intent classes. To mitigate the data scarcity setting in few-shot learning, we consider transferring another intersentence-relation task. This work focuses on NLI; the task is to identify whether a hypothesis sentence can be entailed by a premise sentence (Bowman and Zhu, 2019). We treat the NLI task as a binary classification task: entailment (positive) or non-entailment (negative).2 We first pre-train our model with the NLI task, where the premise sentence corresponds to the u-position, and the hypothesis sentence corresponds to the ej,i -position in Equation (5). Note that it is not necessary to modify the model architecture since the task format is consistent, and we can train the NLI model solely based on existing NLI datasets. Once the NLI model pre-training is completed, we fine-tune the NLI model with the intent classification training examples described in"
2020.emnlp-main.411,D15-1075,0,0.428534,"u-position, and the hypothesis sentence corresponds to the ej,i -position in Equation (5). Note that it is not necessary to modify the model architecture since the task format is consistent, and we can train the NLI model solely based on existing NLI datasets. Once the NLI model pre-training is completed, we fine-tune the NLI model with the intent classification training examples described in Section 3.2. This allows us to transfer the NLI model to any intent detection datasets seamlessly. Why NLI? The NLI task has been actively studied, especially since the emergence of large scale datasets (Bowman et al., 2015; Williams et al., 2018), and we can directly leverage the progress. Moreover, recent work is investigating cross-lingual NLI (Eriguchi et al., 2018; Conneau et al., 2018), and this is encouraging to consider multilinguality in future work. On the other hand, while we can find examples relevant to the intent detection task, as shown in Table 1 ((c), (d), and (e)), we still need the few-shot fine-tuning. This is because a domain mismatch still exists in general, and perhaps more importantly, our intent detection approach is not exactly modeling NLI. Why not other tasks? There are other tasks mo"
2020.emnlp-main.411,2020.nlp4convai-1.5,0,0.056721,"Missing"
2020.emnlp-main.411,P10-1046,0,0.0432628,"Missing"
2020.emnlp-main.411,P17-1171,0,0.0222836,"same pre-trained models. However, our examplebased method has an inference-time bottleneck in Equation (5), where we need to compute the BERT encoding for all N × K (u, ej,i ) pairs. We follow common practice in document retrieval to reduce the inference-time bottleneck (Nie et al., 2019; Asai et al., 2020), by introducing a fast text retrieval model to select a set of top-k examples Ek from the training set E, based on its retrieval scores. We then replace E in Equation (4) with the shrunk set Ek . The cost of the paired BERT encoding is now constant, regardless the size of E. Either TF-IDF (Chen et al., 2017) or embedding-based retrieval (Johnson et al., 2017; Seo et al., 2019; Lee et al., 2019) can be used for the first step. We use the following fast kNN. Faster kNN As a baseline and a way to instantiate our joint approach, we use Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) to separately encode u and ej,i (x ∈ {u, ej,i }) as follows: v(x) = SBERT(x) ∈ Rd , (7) where the input text format is identical to that of BERT in Equation (2). SBERT is a BERT-based text embedding model, fine-tuned by siamese networks with NLI datasets. Thus both our method and SBERT transfer the NLI task in differen"
2020.emnlp-main.411,D18-1269,0,0.0235176,"format is consistent, and we can train the NLI model solely based on existing NLI datasets. Once the NLI model pre-training is completed, we fine-tune the NLI model with the intent classification training examples described in Section 3.2. This allows us to transfer the NLI model to any intent detection datasets seamlessly. Why NLI? The NLI task has been actively studied, especially since the emergence of large scale datasets (Bowman et al., 2015; Williams et al., 2018), and we can directly leverage the progress. Moreover, recent work is investigating cross-lingual NLI (Eriguchi et al., 2018; Conneau et al., 2018), and this is encouraging to consider multilinguality in future work. On the other hand, while we can find examples relevant to the intent detection task, as shown in Table 1 ((c), (d), and (e)), we still need the few-shot fine-tuning. This is because a domain mismatch still exists in general, and perhaps more importantly, our intent detection approach is not exactly modeling NLI. Why not other tasks? There are other tasks modeling relationships between sentences. Paraphrase (Wieting and Gimpel, 2018) and semantic relatedness (Marelli et al., 2014) tasks are such examples. It is possible to au"
2020.emnlp-main.411,L18-1004,0,0.0464104,"Missing"
2020.emnlp-main.411,D19-1258,0,0.109979,"he same model in Equation (2), except that we follow a different input format to accommodate pairs of utterances: [[CLS], u, [SEP], ej,i , [SEP]]. σ is the sigmoid function, and W ∈ R1×d and b ∈ R are the model parameters. We can interpret our method as wrapping both the embedding and matching functions into the paired encoding with the deep self-attention in BERT (Equation (5)) along with the discriminative model (Equation (6)). It has been shown that the paired text encoding is crucial in capturing complex relations between queries and documents in document retrieval (Watanabe et al., 2017; Nie et al., 2019; Asai et al., 2020). 3 3.2 I(u) = class arg max S(u, ej,i ) , (4) ej,i ∈E Proposed Method This section first describes how to directly model inter-utterance relations in our nearest neighbor classification scenario. We then introduce a binary classification strategy by synthesizing pairwise examples, and propose a seamless transfer of NLI. Finally, we describe how to speedup our method’s inference process. 3.1 Deep Pairwise Matching Function The objective of S(u, ej,i ) in Equation (4) is to find the best matched utterance from the training set E, given the input utterance u. The typical meth"
2020.emnlp-main.411,D19-1410,0,0.0228225,"(Nie et al., 2019; Asai et al., 2020), by introducing a fast text retrieval model to select a set of top-k examples Ek from the training set E, based on its retrieval scores. We then replace E in Equation (4) with the shrunk set Ek . The cost of the paired BERT encoding is now constant, regardless the size of E. Either TF-IDF (Chen et al., 2017) or embedding-based retrieval (Johnson et al., 2017; Seo et al., 2019; Lee et al., 2019) can be used for the first step. We use the following fast kNN. Faster kNN As a baseline and a way to instantiate our joint approach, we use Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) to separately encode u and ej,i (x ∈ {u, ej,i }) as follows: v(x) = SBERT(x) ∈ Rd , (7) where the input text format is identical to that of BERT in Equation (2). SBERT is a BERT-based text embedding model, fine-tuned by siamese networks with NLI datasets. Thus both our method and SBERT transfer the NLI task in different ways. Cosine similarity between v(u) and v(ej,i ) then replaces S(u, ej,i ) in Equation (6). To get a fair comparison, instead of using the encoding vectors produced by the original SBERT, we fine-tune SBERT with our intent training examples described in Section 3.2. The cosin"
2020.emnlp-main.411,P19-1436,0,0.0141365,"nce-time bottleneck in Equation (5), where we need to compute the BERT encoding for all N × K (u, ej,i ) pairs. We follow common practice in document retrieval to reduce the inference-time bottleneck (Nie et al., 2019; Asai et al., 2020), by introducing a fast text retrieval model to select a set of top-k examples Ek from the training set E, based on its retrieval scores. We then replace E in Equation (4) with the shrunk set Ek . The cost of the paired BERT encoding is now constant, regardless the size of E. Either TF-IDF (Chen et al., 2017) or embedding-based retrieval (Johnson et al., 2017; Seo et al., 2019; Lee et al., 2019) can be used for the first step. We use the following fast kNN. Faster kNN As a baseline and a way to instantiate our joint approach, we use Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) to separately encode u and ej,i (x ∈ {u, ej,i }) as follows: v(x) = SBERT(x) ∈ Rd , (7) where the input text format is identical to that of BERT in Equation (2). SBERT is a BERT-based text embedding model, fine-tuned by siamese networks with NLI datasets. Thus both our method and SBERT transfer the NLI task in different ways. Cosine similarity between v(u) and v(ej,i ) then replaces S(u"
2020.emnlp-main.411,D19-1045,0,0.0215046,"udied in the first scenario. In our paper, we have focused on the second scenario, assuming that there are only a limited number of training examples for each class. Our work is related to metric-based approaches such as matching networks (Vinyals et al., 2016a), prototypical networks (Snell et al., 2017) and relation networks (Sung et al., 2018), as they model nearest neighbours in an example-embedding or a classembedding space. We showed that a relation network with the RoBERTa embeddings does not perform comparably to our method. We also considered several ideas from prototypical networks (Sun et al., 2019), but those did not outperform our EmbkNN baseline. These results indicate that deep self-attention is the key to the nearest neighbor approach with OOS detection. 7 Conclusion In this paper, we have presented a simple yet efficient nearest-neighbor classification model to detect user intents and OOS intents. It includes paired encoding and discriminative training to model relations between the input and example utterances. Moreover, a seamless transfer from NLI and a joint approach with fast retrieval are designed to improve the performance in terms of the accuracy and inference speed. Experi"
2020.emnlp-main.411,P19-1488,0,0.0169508,"er input. This is illustrated in Figure 4, where the in-domain intent and OOS accuracy metrics (on the development set of the banking domain in the 5shot setting) improve with the increase of k, while the latency increases at the same time. Empirically, k = 20 appears to strike the balance between latency and accuracy, with the accuracy metrics similar to those of the DNNC method, while being much faster than DNNC (dashed lines are the corresponding DNNC references). 6 Discussions and Related Work Interpretability Interpretability is an important line of research recently (Jiang et al., 2019; Sydorova et al., 2019; Asai et al., 2020). The nearest neighbor approach (Simard et al., 1993) is appealing in that we can explicitly know which training example triggers each prediction. Table 11 in Appendix C shows some examples. Call for better embeddings Emb-kNN and RNkNN are not as competitive as DNNC. This encourages future work on the task-oriented evaluation of text embeddings in kNN. Training time Our DNNC method needs longer training time than that of the classifier (e.g., 90 vs. 40 seconds to train a single-domain model), because we synthesize the pairwise examples. As a first step, we used all the trai"
2020.emnlp-main.411,W18-5446,0,0.0863705,"Missing"
2020.emnlp-main.411,D19-1670,0,0.0380088,"ion for each result. Using the development set We would not always have access to a large enough development set in the few-shot learning scenario. However, we still use the development set provided by the dataset to investigate the models’ behaviors when changing hyper-parameters like the threshold. Models compared our experiments: We list the models used in • Classifier baselines: “Classifier” is the RoBERTa-based classification model described in Section 2.2. We further seek solid baselines by data augmentation. “ClassifierEDA” is the classifier trained with data augmentation techniques in Wei and Zou (2019). “Classifier-BT” is the classifier trained with back-translation data augmentation (Yu et al., 2018; Shleifer, 2019) by using a transformer-based English↔German translation system (Vaswani et al., 2017). • Non-BERT classifier: We also test a stateof-the-art fast embedding-based classifier, “USE+ConveRT” (Henderson et al., 2019; Casanueva et al., 2020), in the “all domains” setting. Casanueva et al. (2020) showed that the “USE+ConveRT” outperformed a BERT classifier on the CLINC150 dataset, while it was not evaluated along with the OOS detection task. We modified their original code6 to apply"
2020.emnlp-main.411,P18-1042,0,0.0266018,"ge the progress. Moreover, recent work is investigating cross-lingual NLI (Eriguchi et al., 2018; Conneau et al., 2018), and this is encouraging to consider multilinguality in future work. On the other hand, while we can find examples relevant to the intent detection task, as shown in Table 1 ((c), (d), and (e)), we still need the few-shot fine-tuning. This is because a domain mismatch still exists in general, and perhaps more importantly, our intent detection approach is not exactly modeling NLI. Why not other tasks? There are other tasks modeling relationships between sentences. Paraphrase (Wieting and Gimpel, 2018) and semantic relatedness (Marelli et al., 2014) tasks are such examples. It is possible to automatically create large-scale paraphrase datasets by machine translation (Ganitkevitch et al., 2013). However, our task is not a paraphrasing task, and creating negative examples is crucial and non-trivial (Cham2 A widely-used format is a three-way classification task with entailment, neutral, and contradiction, but we merge the latter two classes into a single non-entailment class. bers and Jurafsky, 2010). In contrast, as described above, the NLI setting comes with negative examples by nature. The"
2020.emnlp-main.411,N18-1101,0,0.5181,"tell me any of the names and addresses? Annie considered. It’s Sunday, what channel is this? I want to go back to Marguerite. Utterance to be compared help me move my money please Label pos. i need to know the amounts due for my utilities and cable bills Are you able to inform me of any name or address? neg. pos. It’s Sunday, can you change the channel? I never want to return to Marguerite. neg. neg. Table 1: Training examples for our model. The first two examples ((a)–(b)) come from the CLINC150 dataset (Larson et al., 2019), and the other three examples ((c)–(e)) come from the MNLI dataset (Williams et al., 2018). with k = 1), a simple and well-established concept for classification (Simard et al., 1993; Cunningham and Delany, 2007). The basic idea is to classify an input into the same class of the most relevant training example based on a certain metric. In our task, we formulate a nearest neighbor classification model as the following: ! we propose to formulate S(u, ej,i ) as follows: h = BERT(u, ej,i ) ∈ Rd , S(u, ej,i ) = σ(W · h + b) ∈ R, (5) (6) where class(ej,i ) is a function returning the intent label (class) of the training example ej,i , and S is a function that estimates some relevance sco"
2020.emnlp-main.411,2020.emnlp-main.66,1,0.85037,"We follow Larson et al. (2019) to report in-domain accuracy, Accin , and OOS recall, Roos . These two metrics are defined as follows: https://github.com/clinc/oos-eval. (9) and select a threshold value to maximize the score on the development set. There is a trade-off to be noted; the larger the value of T is, the higher Roos (and the lower Accin ) we expect, because the models predict OOS more frequently. Notes on Jin oos Our joint score Jin oos in Equation (9) gives the same weight to the two metrics, Accin and Roos , compared to other combined metrics. For example, Larson et al. (2019) and Wu et al. (2020) used a joint accuracy score: Cin + Coos Accin + rRoos , = Nin + Noos 1+r (10) where r = Noos /Nin depends on the balance between Nin and Noos , and thus this combined metric can put much more weight on the in-domain accuracy when Nin  Noos . Table 2 shows r = 100/3000 (= 0.0333) in the development set of the “all domains” setting, which underestimates the importance of Roos . Actually, the OOS recall scores in Larson et al. (2019) and Wu et al. (2020) are much lower than those with our RoBERTa classifier, and the trade-off with respect to the tuning process was not discussed.4 We also report"
2020.emnlp-main.412,2020.acl-main.9,0,0.0425933,"Missing"
2020.emnlp-main.412,D18-1547,0,0.0456783,"Missing"
2020.emnlp-main.412,D19-1459,1,0.843017,"across domains and possibly datasets. To address this issue, Paul et al. (2019) propose a universal schema for DAs by aligning annotations for multiple existing corpora. In this regard, another useful corpora employed as a testbed in this work is Schema-guided dialogues (SGD) (Rastogi et al., 2020), which covers 20 domains under the same DA annotation schema. It is often challenging and costly to obtain a large amount of in-domain dialogues with annotations. However, unlabeled dialogue corpora in target domain can easily be curated from past conversation logs or collected via crowd-sourcing (Byrne et al., 2019; Budzianowski et al., 2018) at a more reasonable cost. The goal of this work is to investigate how to leverage pre-trained masked language models (e.g., BERT) to better adapt DA taggers to unseen domains with available unlabeled dialogues. Pre-trained language models (Devlin et al., 2019; Liu et al., 2019) have been successful for several NLP tasks including dialogue systems (Wolf et al., 5083 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5083–5089, c November 16–20, 2020. 2020 Association for Computational Linguistics Figure 2: Given a dialogue"
2020.emnlp-main.412,N19-1423,0,0.485933,"t al., 2020), which covers 20 domains under the same DA annotation schema. It is often challenging and costly to obtain a large amount of in-domain dialogues with annotations. However, unlabeled dialogue corpora in target domain can easily be curated from past conversation logs or collected via crowd-sourcing (Byrne et al., 2019; Budzianowski et al., 2018) at a more reasonable cost. The goal of this work is to investigate how to leverage pre-trained masked language models (e.g., BERT) to better adapt DA taggers to unseen domains with available unlabeled dialogues. Pre-trained language models (Devlin et al., 2019; Liu et al., 2019) have been successful for several NLP tasks including dialogue systems (Wolf et al., 5083 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5083–5089, c November 16–20, 2020. 2020 Association for Computational Linguistics Figure 2: Given a dialogue turn in target domain, we obtain teacher and student representations by applying two different maskings on its flattened original representation. We use the output binary probability distributions (per dialog act) of the teacher as soft targets to train the student. Orange and green colo"
2020.emnlp-main.412,2020.acl-main.740,0,0.0499982,"Missing"
2020.emnlp-main.412,W14-4337,0,0.0624687,"Missing"
2020.emnlp-main.412,2021.ccl-1.108,0,0.0917789,"Missing"
2020.emnlp-main.412,C18-1300,0,0.0346237,"Missing"
2020.emnlp-main.412,N19-1373,0,0.0215908,"ces of the same dialog act (DA) are distinct due to the domain difference, making the crossdomain generalization challenging. Introduction Dialog act (DA) tagging, one of the important NLU components of modern task-oriented dialog systems, aims to capture the speaker’s intention behind the utterances at each dialog turn. Several different schema and taxonomies have been introduced by several different researchers (Core and Allen, 1997; Stolcke et al., 2000; Bunt et al., 2010; Mezza et al., 2018) over the years. However, the main focus of the recent work (Kumar et al., 2018; Chen et al., 2018; Raheja and Tetreault, 2019) on DA tagging was on human-human social conversations (Godfrey et al., 1992; Jurafsky et al., 1997), which is less applicable for task-oriented setting. Recently, several task-oriented dialogue datasets (Shah et al., 2018; Henderson et al., 2014; Budzianowski et al., 2018) have been released. However, the discrepancy in their annotation schema hinders the progress on building DA taggers that can generalize across domains and possibly datasets. To address this issue, Paul et al. (2019) propose a universal schema for DAs by aligning annotations for multiple existing corpora. In this regard, ano"
2020.emnlp-main.412,J00-3003,0,0.7045,"Missing"
2020.emnlp-main.412,D19-1670,0,0.0288883,"This objective is used to update the DA tagger via the supervision coming from labeled source data S. We use binarycross entropy loss JSTL (θ; x, y) defined as: − [y · log pθ (·|x) + (1 − y) · log(1 − pθ (·|x))] (2) 2.3 Learning with M ASK AUGMENT Semi-supervised learning (SSL) (Berthelot et al., 2019, 2020; Sohn et al., 2020; Li et al., 2020) is 5084 an effective approach for improving deep learning models by leveraging in-domain unlabeled data. Unlike traditional SSL setting, our objective is to primarily address the underlying source-to-target domain shift. In prior work (Xie et al., 2019; Wei and Zou, 2019), unsupervised data augmentation methods including word replacement and backtranslation have been shown useful for short written text classification. However, such augmentation methods are shown to be less effective (Shleifer, 2019) when used with pre-trained models. Besides, back-translation is less applicable in our scenario as translation of multi-turn dialogue itself is a rather challenging task compared to short text. Instead, we propose a simple and controllable data augmentation–M ASK AUGMENT–to explore a new unsupervised teacher-student learning scheme for domain adaptation of DA tagge"
2020.emnlp-main.412,2020.emnlp-main.66,1,0.86437,"Missing"
2020.emnlp-main.501,P19-4007,0,0.0319994,"Missing"
2020.emnlp-main.501,D18-1269,0,0.028776,"tudy how multilingual pre-training affects the extractor and show that it allows transfer of task-specific knowledge from the victim model to other pre-trained languages. We consider two instances in our experiments: (i) one where the extractor has access to no real data and only queries gibberish, and (ii) extractor has access to some real data in English. Here, we refer to real data as data which was also used to train the victim model. In our experiments, the victim model is trained on the MNLI dataset (Williams et al., 2018). We perform all cross-lingual experiments on the XNLI benchmark (Conneau et al., 2018). This benchmark contains NLI instances in several languages whose test sets were translated by humans using the MNLI dataset. In order to generate gibberish input data, we follow the approach of Krishna et al. (2020). For the hypothesis, we generate sentences of random length by sampling words uniformly from the wordlevel vocabulary of WikiText-103. The length of the sentence is sampled based on the distribution of lengths in WikiText-103. For the premise, we randomly swap three words of the hypothesis for random words leaving the rest identical. This is to mimic common NLI inputs which have"
2020.emnlp-main.501,N19-1423,0,0.119058,"leaning, and model training and tuning. Recent work by Krishna et al. (2020) has demonstrated that deployed NLP models can be stolen by adversaries by querying victim models with gibberish input data that consists of random sequences of words. In particular, they showed that the following approach is sufficient for stealing text classification and question answering models. First, unlabeled data is created by randomly sampling words from a vocabulary. Second, a deployed API is queried with each random input sequence to obtain a label for each. Third, a pre-trained language model such as BERT (Devlin et al., 2019) is fine-tuned on the victim-labeled gibberish data. The resulting model retains a significant fraction of the victim model’s performance without ever seeing a single well-formed input sentence. This Extracted Multilingual NLI Model Figure 1: Extraction of multilingual models from monolingual APIs. (Extraction phase:) A pre-trained multilingual model is fine-tuned on gibberish data whose labels are queried from a monolingual API. (Inference phase): This model is then used for zero-shot cross-lingual transfer on different languages. process of “stealing” from an API, or “extracting” a local cop"
2020.emnlp-main.501,D18-1407,0,0.0199121,"Missing"
2020.emnlp-main.501,N18-1101,0,0.0128824,"that is trained by extracting task-specific knowledge from the victim model. We aim to study how multilingual pre-training affects the extractor and show that it allows transfer of task-specific knowledge from the victim model to other pre-trained languages. We consider two instances in our experiments: (i) one where the extractor has access to no real data and only queries gibberish, and (ii) extractor has access to some real data in English. Here, we refer to real data as data which was also used to train the victim model. In our experiments, the victim model is trained on the MNLI dataset (Williams et al., 2018). We perform all cross-lingual experiments on the XNLI benchmark (Conneau et al., 2018). This benchmark contains NLI instances in several languages whose test sets were translated by humans using the MNLI dataset. In order to generate gibberish input data, we follow the approach of Krishna et al. (2020). For the hypothesis, we generate sentences of random length by sampling words uniformly from the wordlevel vocabulary of WikiText-103. The length of the sentence is sampled based on the distribution of lengths in WikiText-103. For the premise, we randomly swap three words of the hypothesis for"
2020.emnlp-main.66,W17-5526,0,0.0710966,"Missing"
2020.emnlp-main.66,2020.acl-main.9,0,0.242502,"Missing"
2020.emnlp-main.66,D19-5602,0,0.0703563,"Missing"
2020.emnlp-main.66,D18-1547,0,0.27744,"Missing"
2020.emnlp-main.66,D19-1459,0,0.205233,". Henderson et al. (2019b) pre-trained a response selection model for task-oriented dialogues. They first pre-train on Reddit corpora and then fine-tune on target dialogue domains, but their training and fine-tuning code is not released. Peng et al. (2020) focus on the natural language generation (NLG) task, which assumes dialogue acts and slot-tagging results are given to generate a natural language response. Pre-training on a set of annotated NLG corpora can improve conditional generation quality using a GPT-2 model. Name MetaLWOZ (Lee et al., 2019) Schema (Rastogi et al., 2019) Taskmaster (Byrne et al., 2019) MWOZ (Budzianowski et al., 2018) MSR-E2E (Li et al., 2018) SMD (Eric and Manning, 2017) Frames (Asri et al., 2017) WOZ (Mrkˇsi´c et al., 2016) CamRest676 (Wen et al., 2016) # Dialogue 37,884 22,825 13,215 10,420 10,087 3,031 1,369 1,200 676 # Utterance 432,036 463,284 303,066 71,410 74,686 15,928 19,986 5,012 2,744 Avg. Turn 11.4 20.3 22.9 6.9 7.4 5.3 14.6 4.2 4.1 # Domain 47 17 6 7 3 3 3 1 1 Table 1: Data statistics for task-oriented dialogue datasets. 3 Method This section discusses each dataset used in our taskoriented pre-training and how we process the data. Then we introduce the selecte"
2020.emnlp-main.66,W14-4337,0,0.303403,"Missing"
2020.emnlp-main.66,P19-1536,0,0.0530865,"Missing"
2020.emnlp-main.66,D19-1131,0,0.101159,"espectively. Across seven different domains, in total, it has 30 (domain, slot) pairs that need to be tracked in the test set. We use its revised version MWOZ 2.1, which has the same dialogue transcripts but with cleaner state label annotation. Evaluation Datasets We pick up several datasets, OOS, DSTC2, GSIM, and MWOZ, for downstream evaluation. The first three corpora are not included in the pre-trained task-oriented datasets. For MWOZ, to be fair, we do not include its test set dialogues during the pretraining stage. Details of each evaluation dataset are discussed in the following: • OOS (Larson et al., 2019): The out-of-scope intent dataset is one of the largest annotated intent datasets, including 15,100/3,100/5,500 samples for the train, validation, and test sets, respectively. It covers 151 intent classes over ten domains, including 150 in-scope intent and one outof-scope intent. The out-of-scope intent means that a user utterance that does not fall into any of the predefined intents. Each of the intents has 100 training samples. • DSTC2 (Henderson et al., 2014): DSTC2 is a human-machine task-oriented dataset that may include a certain system response noise. It has 1,612/506/1117 dialogues for"
2020.emnlp-main.66,2021.ccl-1.108,0,0.0909468,"Missing"
2020.emnlp-main.66,2020.findings-emnlp.17,0,0.0434299,"gues, on the other hand, has few related works. Budzianowski and Vuli´c (2019) first apply the GPT-2 model to train on response generation task, which takes system belief, database result, and last dialogue turn as input to predict next system responses. It only uses one dataset to train its model because few public datasets have database information available. Henderson et al. (2019b) pre-trained a response selection model for task-oriented dialogues. They first pre-train on Reddit corpora and then fine-tune on target dialogue domains, but their training and fine-tuning code is not released. Peng et al. (2020) focus on the natural language generation (NLG) task, which assumes dialogue acts and slot-tagging results are given to generate a natural language response. Pre-training on a set of annotated NLG corpora can improve conditional generation quality using a GPT-2 model. Name MetaLWOZ (Lee et al., 2019) Schema (Rastogi et al., 2019) Taskmaster (Byrne et al., 2019) MWOZ (Budzianowski et al., 2018) MSR-E2E (Li et al., 2018) SMD (Eric and Manning, 2017) Frames (Asri et al., 2017) WOZ (Mrkˇsi´c et al., 2016) CamRest676 (Wen et al., 2016) # Dialogue 37,884 22,825 13,215 10,420 10,087 3,031 1,369 1,200"
2020.emnlp-main.66,W18-5446,0,0.0933208,"Missing"
2020.emnlp-main.66,D16-1264,0,0.16871,"Missing"
2020.emnlp-main.66,W17-7301,0,0.0134412,"icted distributions Pint and the true intent labels. Dialogue state tracking can be treated as a multi-class classification problem using a predefined ontology. Unlike intent, we use dialogue history X (a sequence of utterances) as input and a model predicts slot values for each (domain, slot) pair at each dialogue turn. Each corresponding value vij , the i-th value for the j-th (domain, slot) pair, is passed into a pre-trained model and fixed its representation during training. limited by hardware. We also try different negative sampling strategies during pre-training such as local sampling (Saeidi et al., 2017), but do not observe significant change compared to random sampling. Overall pre-training loss function is the weighted-sum of Lmlm and Lrcl , and in our experiments, we simply sum them up. We gradually reduce the learning rate without a warm-up period. We train TOD-BERT with AdamW (Loshchilov and Hutter, 2017) optimizer with a dropout ratio of 0.1 on all layers and attention weights. GELU activation functions (Hendrycks and Gimpel, 2016) is used. Models are early-stopped using perplexity scores of a held-out development set, with mini-batches containing 32 sequences of maximum length 512 toke"
2020.emnlp-main.66,N18-3006,0,0.0531715,"over ten domains, including 150 in-scope intent and one outof-scope intent. The out-of-scope intent means that a user utterance that does not fall into any of the predefined intents. Each of the intents has 100 training samples. • DSTC2 (Henderson et al., 2014): DSTC2 is a human-machine task-oriented dataset that may include a certain system response noise. It has 1,612/506/1117 dialogues for train, validation, and test sets, respectively. We follow Paul et al. (2019) to map the original dialogue act labels to universal dialogue acts, which results in 9 different system dialogue acts. • GSIM (Shah et al., 2018a): GSIM is a humanrewrote machine-machine task-oriented corpus, including 1500/469/1039 dialogues for the train, validation, and test sets, respectively. We combine its two domains, movie and restaurant domains, into one single corpus. It is collected by Machines Talking To Machines (M2M) (Shah et al., 2018b) approach, a functionality-driven process combining a dialogue self-play step and a crowdsourcing step. We map its dialogue act labels to universal dialogue acts (Paul et al., 2019), resulting in 6 different system dialogue acts. • MWOZ (Budzianowski et al., 2018): MWOZ is the most common"
2020.emnlp-main.66,P19-1078,1,0.877039,"Missing"
2020.emnlp-main.66,W18-3022,0,0.0856921,"Missing"
2020.emnlp-main.66,W17-5506,0,\N,Missing
2020.emnlp-main.66,W19-5932,0,\N,Missing
2020.emnlp-main.660,D15-1075,0,0.334807,"way. By a forward-looking perspective, instead, a single machine that can handle diverse (seen and unseen) tasks is desired. The reason is that we cannot always rely on expensive human resources to annotate large-scale task-specific labeled data, especially considering the inestimable number of tasks to be explored. Therefore, a reasonable attempt is to map diverse NLP tasks into a common learning problem—solving this common problem equals to solving any downstream NLP tasks, even some tasks that are new or have insufficient annotations. Textual entailment (aka. natural language inference in Bowman et al. (2015)) is the task of studying the relation of two assertive sentences, Premise (P) and Hypothesis (H): whether H is true given P. Textual entailment (TE) was originally brought up as a unified framework for modeling diverse NLP tasks (Dagan et al., 2005; Poliak et al., 2018). The research on TE dates back more than two decades and has made significant progress. Particularly, with the advances of deep neural networks and the availability of large-scale human annotated datasets, fine-tuned systems often claim surpassing human performance on certain benchmarks. Nevertheless, two open problems remain."
2020.emnlp-main.660,N19-1300,0,0.0152841,"AIL is to not only learn the matching function, but also map the instances in S and T to the same space. 8233 UFO-E NTAIL vs. STILTS. Given the source data S and a couple of labeled examples from the target T , STILTS (Phang et al., 2018) first trains RoBERTa on S, then fine-tune on the labeled examples of T . Both the pretraining and fine-tuning use the same RoBERTa system in Figure 1. It has been widely used as the state of the art technique for making use of related tasks to improve target tasks, especially when the target tasks have limited annotations (Liu et al., 2019; Sap et al., 2019; Clark et al., 2019). By the architecture, STILTS relies on the standard RoBERTa classifier which consists of a RoBERTa encoder and a logistic regression on the top; UFO-E NTAIL instead has a cross-task nearest neighbor block on the top of the RoBERTa encoder. STILTS tries to learn the target-specific parameters by tuning on the k labeled examples. However, this is very challenging if k is over small, like values {1, 3, 5, 10} we will use in our problems. We can also think STILTS learns class prototypical representations implicitly (i.e., the weights in the logistic regression layer), however, the bias term in th"
2020.emnlp-main.660,P07-1033,0,0.367983,"Missing"
2020.emnlp-main.660,N19-1423,0,0.0427111,"w years, the research on textual entailment has been driven by the creation of large-scale datasets, such as SNLI (Bowman et al., 2015), science domain SciTail (Khot et al., 2018), and multi-genre MNLI (Williams et al., 2018). Representative work includes the first attentive recurrent neural network (Rockt¨aschel et al., 2016) and its followers (Wang and Jiang, 2016; Wang et al., 2017), as well as the attentive convolutional networks such as attentive pooling (dos Santos et al., 2016) and attentive convolution (Yin and Sch¨utze, 2018), and self-attentive large-scale language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). All these studies result in systems that are overly tailored to the datasets. Our work differs in that we care more about fewshot applications of textual entailment, assuming that a new domain or an NLP task is not provided with rich annotated data. Generalization via domain adaptation. Two main types of domain adaptation (DA) problems have been studied in literature: supervised DA and semi-supervised DA. In the supervised case, we have access to a large annotated data in the source domain and a small-scale annotated data in the target domain (Daum´e III, 2007;"
2020.emnlp-main.660,N19-1039,0,0.0214007,"r work differs in that we care more about fewshot applications of textual entailment, assuming that a new domain or an NLP task is not provided with rich annotated data. Generalization via domain adaptation. Two main types of domain adaptation (DA) problems have been studied in literature: supervised DA and semi-supervised DA. In the supervised case, we have access to a large annotated data in the source domain and a small-scale annotated data in the target domain (Daum´e III, 2007; Kang and Feng, 2018). In the semi-supervised case, we have a large but unannotated corpus in the target domain (Miller, 2019). In contrast to semi-supervised DA, our work does not assume the availability of a large unlabeled data from the target domain or task. We also build more ambitious missions than the supervised DA since our work aims to adapt the model to new domains as well as new NLP tasks. Generalization via few-shot learning. Fewshot problems are studied typically in the image domain (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Ren et al., 2018; Sung et al., 2018). The core idea in metric-based few-shot 8230 learning is similar to nearest neighbors. The predicted probability of a test ins"
2020.emnlp-main.660,W18-5441,0,0.0444137,"Missing"
2020.emnlp-main.660,N18-2017,0,0.0191469,"so think STILTS learns class prototypical representations implicitly (i.e., the weights in the logistic regression layer), however, the bias term in the logistic regression layer reflect mainly the distribution in the source S, which is less optimal for predicting in the target T . 4 Experiments We apply UFO-E NTAIL to entailment tasks of open domain and open NLP tasks. Experimental setup. Our system is implemented with Pytorch on the transformers package released by Huggingface2 . We use “RoBERTalarge” initialized by the pretrained language model. To mitigate the potential bias or artifacts (Gururangan et al., 2018) in sampling, all numbers of k-shot are average of five runs in seeds {42, 16, 32, 64, 128}. Due to GPU memory constraints, we only update the nearest neighbor block, the hidden layer and top-5 layers in RoBERTa. For other training configurations, please refer to our released code. Baselines. The following baselines are shared by experiments on open entailment tasks and open NLP tasks. • 0-shot. We assume zero examples from target domains. We train a RoBERTa classifier3 on 2 https://github.com/huggingface/ transformers 3 Specifically, the “RobertaForSequenceClassification” classifier in the Hu"
2020.emnlp-main.660,D13-1020,0,0.03641,"onverted to be textual entailment. Our work provides a new perspective to tackle these NLP issues, especially given only a couple of labeled examples. Question Answering. We attempt to handle the QA setting in which only a couple of labeled examples are provided. A QA problem can be formulated as a textual entailment problem—the document acts as the premise, and the (question, answer candidate), after converting into a natural sentence, acts as the hypothesis. Then a true (resp. false) hypothesis can be translated into a correct (resp. incorrect) answer. We choose the QA benchmark MCTest-500 (Richardson et al., 2013) which releases an entailment-formatted corpus. MCTest500 is a set of 500 items (split into 300 train, 50 dev and 150 test). Each item consists of a document, four questions followed by one correct answer, and three incorrect answers. Deep learning has not achieved significant success on it because of the limited training data (Trischler et al., 2016)—this is exactly our motivation that applying few-shot textual entailment to handle annotation-scarce NLP problems. For MCTest benchmark, we treat one question as one example. K-shot means we randomly sample k annotated questions (each corresponds"
2020.emnlp-main.660,D18-1514,0,0.0520493,"a method named Matching Networks. Snell et al. (2017) propose Prototypical Networks which first build prototypical representations for each class by summing up representations of supporting examples, then compare classes with test instances by squared Euclidean distances. Unlike fixed metric measures, the Relation Network (Sung et al., 2018) implements the comparison through learning a matching metric in a multi-layer architecture. In the language domain, Yu et al. (2018) combine multiple metrics learned from diverse clusters of training tasks for an unseen few-shot text classification task. Han et al. (2018) release a few-shot relation classification dataset “FewRel” and compare a couple of representative methods on it. These few-shot studies assume that, in the same domain, a part of the classes have limited samples, while other classes have adequate examples. In this work, we make a more challenging assumption that all classes in the target domain have only a couple of examples, and the training classes and testing classes are from different domains. Unified natural language processing. McCann et al. (2018) cast a group of NLP tasks as question answering over context, such as machine translatio"
2020.emnlp-main.660,D19-1454,0,0.0395338,"Missing"
2020.emnlp-main.660,2021.ccl-1.108,0,0.306573,"Missing"
2020.emnlp-main.660,P16-1041,0,0.012281,"emise, and the (question, answer candidate), after converting into a natural sentence, acts as the hypothesis. Then a true (resp. false) hypothesis can be translated into a correct (resp. incorrect) answer. We choose the QA benchmark MCTest-500 (Richardson et al., 2013) which releases an entailment-formatted corpus. MCTest500 is a set of 500 items (split into 300 train, 50 dev and 150 test). Each item consists of a document, four questions followed by one correct answer, and three incorrect answers. Deep learning has not achieved significant success on it because of the limited training data (Trischler et al., 2016)—this is exactly our motivation that applying few-shot textual entailment to handle annotation-scarce NLP problems. For MCTest benchmark, we treat one question as one example. K-shot means we randomly sample k annotated questions (each corresponds to a short article and has four answer candidates). We obtain k entailment pairs for the class “entailment” and 3k pairs for the class “non-entailment”. The official evaluation metrics in MCTest include accuracy and NDCG4 . Here, we report accuracy. Coreference Resolution. Coreference resolution aims to cluster the entities and pronouns that 8235 ref"
2020.emnlp-main.660,N16-1170,0,0.0186353,"not guarantee the accessibility of rich annotations. 2 Related Work Textual Entailment. Textual entailment was first studied in Dagan et al. (2005) and the main focus in the early stages was to study lexical and some syntactic features. In the past few years, the research on textual entailment has been driven by the creation of large-scale datasets, such as SNLI (Bowman et al., 2015), science domain SciTail (Khot et al., 2018), and multi-genre MNLI (Williams et al., 2018). Representative work includes the first attentive recurrent neural network (Rockt¨aschel et al., 2016) and its followers (Wang and Jiang, 2016; Wang et al., 2017), as well as the attentive convolutional networks such as attentive pooling (dos Santos et al., 2016) and attentive convolution (Yin and Sch¨utze, 2018), and self-attentive large-scale language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). All these studies result in systems that are overly tailored to the datasets. Our work differs in that we care more about fewshot applications of textual entailment, assuming that a new domain or an NLP task is not provided with rich annotated data. Generalization via domain adaptation. Two main types of domain ad"
2020.emnlp-main.660,Q18-1042,0,0.0190498,"we randomly sample k annotated questions (each corresponds to a short article and has four answer candidates). We obtain k entailment pairs for the class “entailment” and 3k pairs for the class “non-entailment”. The official evaluation metrics in MCTest include accuracy and NDCG4 . Here, we report accuracy. Coreference Resolution. Coreference resolution aims to cluster the entities and pronouns that 8235 refer to the same object. This is a challenging task in NLP, and greatly influences the capability of machines in understanding the text. We test on the coreference resolution benchmark GAP (Webster et al., 2018), a human-labeled corpus from Wikipedia for recognizing ambiguous pronoun-name coreference. An example from the GAP dataset is shown here: “McFerran’s horse farm was named Glen View. After his death in 1885, John E. Green acquired the farm.” For a specific pronoun in the sentence, GAP provides two entity candidates for it to link. To correctly understand the meaning of this sentence, a machine must know which person (“McFerran” or “John E. Green”) the pronoun “his” refers to. GAP has such kind of annotated examples of sizes split as 2k/454/2k in train/dev/test. Please note that some examples h"
2020.emnlp-main.660,N18-1101,0,0.652655,", c November 16–20, 2020. 2020 Association for Computational Linguistics entailment. We argue that textual entailment particularly matters when the target NLP task has insufficient annotations; in this way, some NLP tasks that share the same inference pattern and annotations are insufficient to build a task-specific model can be handled by a unified entailment system. Motivated by the two issues, we build UFOE NTAIL—the first ever generalized few-shot textual entailment system with the following setting. We first assume that we can access a largescale generic purpose TE dataset, such as MNLI (Williams et al., 2018); this dataset enables us to build a base entailment system with acceptable performance. To get even better performance in any new domain or new task, we combine the generic purpose TE dataset with a couple of domain/taskspecific examples to learn a better-performing entailment for that new domain/task. This is a reasonable assumption because in the real-world, any new domain or new task does not typically have large annotated data, but obtaining a couple of examples is usually feasible. Technically, our UFO-E NTAIL is inspired by the Prototypical Network (Snell et al., 2017), a popular metric"
2020.emnlp-main.660,D19-1404,1,0.883773,"Missing"
2020.emnlp-main.660,Q18-1047,1,0.890372,"Missing"
2020.emnlp-main.660,N18-1109,0,0.0760185,"ses for those supporting samples. Vinyals et al. (2016) compare each test instance with those supporting examples by the cosine distance in a method named Matching Networks. Snell et al. (2017) propose Prototypical Networks which first build prototypical representations for each class by summing up representations of supporting examples, then compare classes with test instances by squared Euclidean distances. Unlike fixed metric measures, the Relation Network (Sung et al., 2018) implements the comparison through learning a matching metric in a multi-layer architecture. In the language domain, Yu et al. (2018) combine multiple metrics learned from diverse clusters of training tasks for an unseen few-shot text classification task. Han et al. (2018) release a few-shot relation classification dataset “FewRel” and compare a couple of representative methods on it. These few-shot studies assume that, in the same domain, a part of the classes have limited samples, while other classes have adequate examples. In this work, we make a more challenging assumption that all classes in the target domain have only a couple of examples, and the training classes and testing classes are from different domains. Unifie"
2020.emnlp-main.750,P18-1013,0,0.0265814,"Missing"
2020.emnlp-main.750,K16-1028,0,0.17715,"Missing"
2020.emnlp-main.750,D19-1051,1,0.869105,"Missing"
2020.emnlp-main.750,D18-1207,1,0.894773,"Missing"
2020.emnlp-main.750,D18-1441,0,0.0178223,"on Figure 1: Procedure to generate synthetic training data. S is a set of source documents, T + is a set of semantically invariant text transformations, T − is a set of semantically variant text transformations, + is a positive label, − is a negative label. caused by poor generation were not labeled. The validation set consists of 931 examples, the test set contains 503 examples. The model outputs used for annotation were provided by the authors of papers: Hsu et al. (2018); Gehrmann et al. (2018); Jiang and Bansal (2018); Chen and Bansal (2018); See et al. (2017); Kry´sci´nski et al. (2018); Li et al. (2018); Pasunuru and Bansal (2018); Zhang et al. (2018); Guo et al. (2018). Effort was made to collect a larger set of annotations through crowdsourcing platforms, however the inter-annotator agreement and general quality of annotations was too low to be considered reliable. This aligns with the conclusions of (Falke et al., 2019), where the authors showed that for the task of factual consistency the interannotator agreement coefficient κ reached 0.75 only when 12 annotations were collected for each example. This in turn yields high annotations costs that our approach aims to circumvent. 3.3 Models"
2020.emnlp-main.750,D19-1387,0,0.0785939,"Missing"
2020.emnlp-main.750,D15-1044,0,0.0989058,"ul assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC. 1 Introduction The goal of text summarization is to transduce long documents into a shorter form that retains the most important aspects from the source document. Common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017) where models directly copy salient parts of the source document into the summary, abstractive (Rush et al., 2015; Paulus et al., 2017) where the important parts are paraphrased to form novel sentences, and hybrid (Gehrmann et al., 2018), combining the two methods by employing specialized extractive and abstractive components. Advancements in neural architectures (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), transfer learning (McCann et al., 2017; Devlin et al., 2018), and availability of large-scale supervised datasets (Nallapati et al., 2016; Grusky et al., 2018) allowed deep learning-based approaches to dominate the field. State-of-the-art solutions utilize self-attentive Transforme"
2020.emnlp-main.750,P17-1099,0,0.142467,"w sent, −)} end for end for return D end function Figure 1: Procedure to generate synthetic training data. S is a set of source documents, T + is a set of semantically invariant text transformations, T − is a set of semantically variant text transformations, + is a positive label, − is a negative label. caused by poor generation were not labeled. The validation set consists of 931 examples, the test set contains 503 examples. The model outputs used for annotation were provided by the authors of papers: Hsu et al. (2018); Gehrmann et al. (2018); Jiang and Bansal (2018); Chen and Bansal (2018); See et al. (2017); Kry´sci´nski et al. (2018); Li et al. (2018); Pasunuru and Bansal (2018); Zhang et al. (2018); Guo et al. (2018). Effort was made to collect a larger set of annotations through crowdsourcing platforms, however the inter-annotator agreement and general quality of annotations was too low to be considered reliable. This aligns with the conclusions of (Falke et al., 2019), where the authors showed that for the task of factual consistency the interannotator agreement coefficient κ reached 0.75 only when 12 annotations were collected for each example. This in turn yields high annotations costs tha"
2020.emnlp-main.750,N18-1074,0,0.0784731,"Missing"
2020.emnlp-main.750,D19-1670,0,0.125476,"thors proposed a novel, dual-encoder architecture that in parallel encodes the source documents and all the facts contained in them. During generation, the decoder attends to both the encoded source and facts which, according to the authors, forces the output to be conditioned on the both inputs. Human evaluation showed that the proposed technique substantially lowered the number of errors in generated single-sentence summaries. The synthetic data generation process proposed as part of our approach is based on prior work done in the domains of data augmentation and weakly-supervised learning. Wei and Zou (2019) proposed an augmentation framework aimed at boosting performance of text classification models. The authors used 4 text transformations to synthesize data: synonym replacement, random insertion, random swap, random deletion, 9333 and showed increased performance of classifiers on 5 downstream tasks, both for convolutional and recurrent neural models. In (Sennrich et al., 2015; Edunov et al., 2018) the authors introduced and analyzed the effects of using backtranslation based data augmentation on the performance of machine translation models, while Iyyer et al. (2018) used the mentioned transf"
2020.emnlp-main.750,N18-1101,0,0.290348,"n’s corridors of power. Luckily, Japanese can sleep soundly in their beds tonight as the government’s top military official earnestly revealed that (...) Model generated claims Quadriplegic man Nyia Parler, 41, left in woods for days can not be extradited. Video game ”Space Invaders” was developed in Japan back in 1970. Table 1: Examples of factually incorrect claims output by summarization models. Green text highlights the support in the source documents for the generated claims, red text highlights the errors made by summarization models. checking. Current NLI datasets (Bowman et al., 2015; Williams et al., 2018) focus on classifying logical entailment between short, single sentence pairs, but verifying factual consistency requires the entire source document. Fact checking focuses on verifying facts against the whole of available knowledge, whereas factual consistency checking focuses on adherence of facts to information provided by a source document without guarantee that the information is true. We propose a novel, weakly-supervised BERTbased (Devlin et al., 2018) model for verifying factual consistency, and we add specialized modules that explain which portions of both the source document and gener"
2020.emnlp-main.750,D18-1089,0,0.16615,"ntence of the summary is verified against the entire body of the source document. 3.1 Training data Currently, there are no training datasets for factual consistency checking. Creating a large-scale, highquality dataset with supervision collected from human annotators is expensive and time consuming. We consider an alternative approach to acquiring training data that is highly scalable. Considering the state of summarization, in which the level of abstraction of generated summaries is low and models mostly paraphrase single sentences and short spans from the source (Kry´sci´nski et al., 2018; Zhang et al., 2018), we propose using a synthetic, weaklysupervised dataset for the task at hand. Our data creation method requires an unannotated collection of source documents in the same domain as the summarization models that are to be checked. Examples are created by first sampling single sentences, later referred to as claims, from the source documents. Claims then pass through a set of textual transformations that output novel sentences with both positive and negative labels. Though transformations are applied to single sentences, we found that, in keeping with our aforementioned observations of model-gen"
2020.emnlp-main.750,N19-1131,0,0.35857,"model outputs. Through human evaluation we show that the explanatory modules that augment our factual consistency model provide useful assistance to humans as they verify the factual consistency between a source document and generated summaries. Together with this manuscript we release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC. 2 Related Work This work builds on prior research in factual consistency in text summarization and natural language generation. Goodrich et al. (2019) proposed an automatic, model-dependent metric for evaluating the factual accuracy of generated text. Facts are represented as subject-relation-object triplets and factual accuracy is defined as the precision between facts extracted from the summary and source document. Despite positive results, the authors highlighted remaining challenges, such as its inability to adapt to negated relations or relation names expressed by synonyms. A parallel line of research focused on improving factual consistency of summarization models by exploring different architectural choices and strategies for both tr"
2020.findings-emnlp.303,P17-1061,0,0.0308137,"next token. To simulate the left-toright generation process, another attention mask is utilized for the decoder. In the attention mask for the decoder, tokens in the intent can only attend to intent tokens, while tokens in the utterance can attend to both the intent and all the left tokens in the utterance. For the first token z which holds composed latent information, it is only allowed to attend to itself due to the vanishing latent variable problem. The latent information can be overwhelmed by the information of other tokens when adapting VAE to natural language generators either for LSTM (Zhao et al., 2017) or transformers (Xia et al., 2020). To further increase the impact of the composed latent information z and alleviate the vanishing latent variable problem, we concatenate the token representations of z to all the other token embeddings output from the last transformer layer in the decoder. The hidden dimension increases to 2 × dh after the concatenation. To reduce the hidden dimension 2.4 Learning with contrastive loss Although the model can generate utterances for a given intent, such as “are there any alarms set for seven am” for “Alarm Query”, there are some negative utterances generated."
2020.findings-emnlp.303,D19-1670,0,0.0855476,"Missing"
2020.findings-emnlp.303,D18-1348,1,0.710184,"ts show that our proposed model achieves state-of-the-art performances on two real-world intent detection datasets. 1 Introduction Intelligent assistants have gained great popularity in recent years since they provide a new way for people to interact with the Internet conversationally (Hoy, 2018). However, it is still challenging to answer people’s diverse questions effectively. Among all the challenges, identifying user intentions from their spoken language is important and essential for all the downstream tasks. Most existing works (Hu et al., 2009; Xu and Sarikaya, 2013; Chen et al., 2016; Xia et al., 2018) formulate intent detection as a classification task and achieve high performance on pre-defined intents with sufficient labeled examples. With this ∗ Work was done when Congying was a research intern at Salesforce Research. ever-changing world, a realistic scenario is that we have imbalanced training data with existing manyshot intents and insufficient few-shot intents. Previous intent detection models (Yin, 2020; Yin et al., 2019) deteriorate drastically in discriminating the few-shot intents. To alleviate this scarce annotation problem, several methods (Wei and Zou, 2019; Malandrakis et al."
2020.findings-emnlp.303,D19-1404,0,0.0325446,"from their spoken language is important and essential for all the downstream tasks. Most existing works (Hu et al., 2009; Xu and Sarikaya, 2013; Chen et al., 2016; Xia et al., 2018) formulate intent detection as a classification task and achieve high performance on pre-defined intents with sufficient labeled examples. With this ∗ Work was done when Congying was a research intern at Salesforce Research. ever-changing world, a realistic scenario is that we have imbalanced training data with existing manyshot intents and insufficient few-shot intents. Previous intent detection models (Yin, 2020; Yin et al., 2019) deteriorate drastically in discriminating the few-shot intents. To alleviate this scarce annotation problem, several methods (Wei and Zou, 2019; Malandrakis et al., 2019; Yoo et al., 2019) have been proposed to augment the training data for low-resource spoken language understanding (SLU). Wei and Zou (2019) introduce simple data augmentation rules for language transformation like insert, delete and swap. Malandrakis et al. (2019) and Yoo et al. (2019) utilize variational autoencoders (Kingma and Welling, 2013) with simple LSTMs (Hochreiter and Schmidhuber, 1997) that have limited model capac"
2020.findings-emnlp.400,D18-1547,0,0.0349221,"Missing"
2020.findings-emnlp.400,N19-1423,0,0.0132154,"n words for Ndrop times. Then we use Ndrop dialogue history together with the one without dropping any word as input to the base model and obtain Ndrop + 1 model predictions. Masking words into unknown words can also strengthen the representation learning because when important words are masked, a model needs to rely on its contextual information to obtain a meaningful representation for the masked word. For example, “I want a cheap restaurant that does not spend much.” becomes “I want a [UNK] restaurant that [UNK] not spend much.” This idea is motivated by the masked language model learning (Devlin et al., 2019). We randomly mask words instead of only hiding slot values because it is not easy to recognize the slot values without ontology. Afterward, we produce a “gues” for its latent variables: the attention distribution and the slot gate distribution in our setting. Using the Ndrop + 1 model’s predictions, we follow the label guessing process in MixMatch algorithm (Berthelot et al., 2019) to obtain a smooth latent distribution. We compute the average of the model’s predicted distributions by ˆ ⇤ij = Aˆ⇤ij , G Ndrop P+1 d=1 d , ✓) P (Aij , Gij |X1:t Ndrop + 1 , (3) where ✓ is the model parameters. Ai"
2020.findings-emnlp.400,W14-4340,0,0.0326239,"DST performance can facilitate downstream applications such as dialogue management. However, collecting dialogue state labels is very expensive and timeconsuming (Budzianowski et al., 2018), requiring dialogue experts or trained turkers to indicate all (domain, slot, value) information for each turn in dialogues. This problem becomes important from single-domain to multi-domain scenarios. It will be more severe for a massive-multi-domain setting, making DST models less scalable to a new domain. Existing DST models require plenty of state labels, especially those ontology-based DST approaches (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018). They assume a predefined ontology that lists all possible values is available, but an ontology requires complete state annotation and is hard to get in real scenario (Xu and Hu, 2018). They also cannot track unseen slot values that are not predefined. Ontology-free approaches (Xu and Hu, 2018; Chao and Lane, 2019), on the other hand, are proposed to generate slot values from dialogue history directly. They achieve good performance on multi-domain DST by copyattention mechanism but still observe a significant performance drop under limited labeled d"
2020.findings-emnlp.400,P19-1546,0,0.0218393,"Missing"
2020.findings-emnlp.400,P17-1163,0,0.0329934,"Missing"
2020.findings-emnlp.400,D14-1162,0,0.0820533,"Missing"
2020.findings-emnlp.400,P18-2069,0,0.0229157,"Missing"
2020.findings-emnlp.400,D19-1196,0,0.0212759,"Missing"
2020.findings-emnlp.400,D18-1299,0,0.0259931,"Missing"
2020.findings-emnlp.400,P19-1004,0,0.0521308,"Missing"
2020.findings-emnlp.400,P17-1099,0,0.0267264,"X1:t , and a state generator decodes slot values Vij for each (domain, slot) pair {(Di , Sj )}, where i denotes the domain index and j is the slot index. The context encoder and the state generator can be either a pre-trained language model or a simple recurrent neural network. During the decoding stage for each Vij , a copy-attention mechanism such as text span extraction (Vinyals et al., 2015) or pointer Figure 1: The block diagram of copy-attention ontology-free framework for dialogue state tracking. The self-supervised modules (dotted parts) are discarded during inference time. generator (See et al., 2017) approach is added to the state generator and strengthen its value generation process. Moreover, many ontology-free DST models are also equipped with a slot gate mechanism (Xu and Hu, 2018; Rastogi et al., 2019; Zhang et al., 2019), which is a classifier that predicts whether a (domain, slot) pair is mentioned, not mentioned, or a user does not care about it. In this pipeline setting, they can add additional supervision to their models and ignore the not mentioned pairs’ prediction. More specifically, the (domain, slot) pair {(Di , Sj )} obtains its context vector Cij to predict a slot gate di"
2020.findings-emnlp.400,N19-1178,0,0.0590554,"Missing"
2020.findings-emnlp.400,2020.acl-main.63,0,0.0506634,"Missing"
2020.findings-emnlp.400,D18-1412,0,0.0592434,"Missing"
2020.findings-emnlp.400,D19-1125,0,0.0334505,"Missing"
2020.findings-emnlp.400,W13-4067,0,0.08314,"Missing"
2020.findings-emnlp.400,E17-1042,0,0.0610076,"Missing"
2020.findings-emnlp.400,W14-4339,0,0.0397234,"Missing"
2020.findings-emnlp.400,2020.emnlp-main.66,1,0.852109,"Missing"
2020.findings-emnlp.400,P19-1078,1,0.92616,"al., 2017; Zhong et al., 2018). They assume a predefined ontology that lists all possible values is available, but an ontology requires complete state annotation and is hard to get in real scenario (Xu and Hu, 2018). They also cannot track unseen slot values that are not predefined. Ontology-free approaches (Xu and Hu, 2018; Chao and Lane, 2019), on the other hand, are proposed to generate slot values from dialogue history directly. They achieve good performance on multi-domain DST by copyattention mechanism but still observe a significant performance drop under limited labeled data scenario (Wu et al., 2019a). In this paper, we approach the DST problem using copy-augmented ontology-free models from a rarely discussed perspective, assuming that only a few dialogues in a dataset have annotated state labels. We present two self-supervised learning (SSL) solutions: 1) Preserving latent consistency: We encourage a DST model to have similar latent distributions (e.g., attention weights and hidden states) for a set of slightly perturbed inputs. This assumption is known as consistency assumption (Zhou et al., 2004; Chapelle et al., 2009; Berthelot et al., 2019) in semi-supervised learning, making distri"
2020.findings-emnlp.400,P19-1375,0,0.185007,"al., 2017; Zhong et al., 2018). They assume a predefined ontology that lists all possible values is available, but an ontology requires complete state annotation and is hard to get in real scenario (Xu and Hu, 2018). They also cannot track unseen slot values that are not predefined. Ontology-free approaches (Xu and Hu, 2018; Chao and Lane, 2019), on the other hand, are proposed to generate slot values from dialogue history directly. They achieve good performance on multi-domain DST by copyattention mechanism but still observe a significant performance drop under limited labeled data scenario (Wu et al., 2019a). In this paper, we approach the DST problem using copy-augmented ontology-free models from a rarely discussed perspective, assuming that only a few dialogues in a dataset have annotated state labels. We present two self-supervised learning (SSL) solutions: 1) Preserving latent consistency: We encourage a DST model to have similar latent distributions (e.g., attention weights and hidden states) for a set of slightly perturbed inputs. This assumption is known as consistency assumption (Zhou et al., 2004; Chapelle et al., 2009; Berthelot et al., 2019) in semi-supervised learning, making distri"
2020.findings-emnlp.400,P18-1134,0,0.0759477,"s to indicate all (domain, slot, value) information for each turn in dialogues. This problem becomes important from single-domain to multi-domain scenarios. It will be more severe for a massive-multi-domain setting, making DST models less scalable to a new domain. Existing DST models require plenty of state labels, especially those ontology-based DST approaches (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018). They assume a predefined ontology that lists all possible values is available, but an ontology requires complete state annotation and is hard to get in real scenario (Xu and Hu, 2018). They also cannot track unseen slot values that are not predefined. Ontology-free approaches (Xu and Hu, 2018; Chao and Lane, 2019), on the other hand, are proposed to generate slot values from dialogue history directly. They achieve good performance on multi-domain DST by copyattention mechanism but still observe a significant performance drop under limited labeled data scenario (Wu et al., 2019a). In this paper, we approach the DST problem using copy-augmented ontology-free models from a rarely discussed perspective, assuming that only a few dialogues in a dataset have annotated state label"
2020.findings-emnlp.400,P18-1135,1,0.846224,"cations such as dialogue management. However, collecting dialogue state labels is very expensive and timeconsuming (Budzianowski et al., 2018), requiring dialogue experts or trained turkers to indicate all (domain, slot, value) information for each turn in dialogues. This problem becomes important from single-domain to multi-domain scenarios. It will be more severe for a massive-multi-domain setting, making DST models less scalable to a new domain. Existing DST models require plenty of state labels, especially those ontology-based DST approaches (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018). They assume a predefined ontology that lists all possible values is available, but an ontology requires complete state annotation and is hard to get in real scenario (Xu and Hu, 2018). They also cannot track unseen slot values that are not predefined. Ontology-free approaches (Xu and Hu, 2018; Chao and Lane, 2019), on the other hand, are proposed to generate slot values from dialogue history directly. They achieve good performance on multi-domain DST by copyattention mechanism but still observe a significant performance drop under limited labeled data scenario (Wu et al., 2019a). In this pap"
2020.findings-emnlp.438,H94-1010,0,0.608493,"Missing"
2020.findings-emnlp.438,N19-1423,0,0.182714,"estion, the target DB structure, and the contextualization of both. State-of-the-art cross-DB text-to-SQL semantic parsers adopt the following design principles to 4870 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4870–4888 c November 16 - 20, 2020. 2020 Association for Computational Linguistics address the aforementioned challenges. First, the question and schema representation should be contextualized with each other (Hwang et al., 2019; Guo et al., 2019; Wang et al., 2019; Yin et al., 2020). Second, large-scale pre-trained language models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019c) can significantly boost parsing accuracy by providing better representations of text and capturing long-term dependencies. Third, under data privacy constraints, leveraging available DB content can resolve ambiguities in the DB schema (Bogin et al., 2019b; Wang et al., 2019; Yin et al., 2020). Consider the second example in Figure 1, knowing “PLVDB” is a value of the field Journal.Name helps the model to generate the WHERE condition. We present BRIDGE, a powerful sequential textDB encoding framework assembling the three design principles mentioned above. BRIDGE"
2020.findings-emnlp.438,P16-1154,0,0.0204889,"” with it. To resolve this problem, we make use of anchor text to link value mentions in the question with the corresponding DB fields. We perform fuzzy string match between Q and the picklist of each field in the DB. The matched field values (anchor texts) where PV (yt ) is the softmax LSTM output distribution and X˜ is the length-(|Q |+ |S|) sequence that consists of only the question words and special tokens [T] and [C] from X. We use the attention weights of the last head to compute the pointing distribution2 . We extend the input state to the LSTM decoder using selective read proposed by Gu et al. (2016). 1 This approach may over-match anchor texts from fields other than those in the correct SQL query. Yet keeping the additional matches in X may provide useful information rather than noise. We plan to verify this in future work. 2 In practice we find this approach better than using just one head or using the average of multiple head weights. 4872 hS hQ … g … … fprimary … fforegin … ftype hX Bi-LSTM Bi-LSTM BERT CLS Show names… SEP T Properties … C Property Type Code V House V Apartment C … T Reference Property Types … Show names of properties that are either houses or apartments C Property Ty"
2020.findings-emnlp.438,P19-1444,0,0.279783,"area focus on training and testing the semantic parser on a single DB (Hemphill et al., 1990; Dahl et al., 1994; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016). However, DBs are widely used in many domains and developing a semantic parser for each individual DB is unlikely to scale in practice. More recently, large-scale datasets consisting of hundreds of DBs and the corresponding questionSQL pairs have been released (Yu et al., 2018; Zhong et al., 2017; Yu et al., 2019b,a) to encourage the development of semantic parsers that can work well across different DBs (Guo et al., 2019; Bogin et al., 2019b; Zhang et al., 2019; Wang et al., 2019; Suhr et al., 2020; Choi et al., 2020). The setup is challenging as it requires the model to interpret a question conditioned on a relational DB unseen during training and accurately express the question intent via SQL logic. Consider the two examples shown in Figure 1, both questions have the intent to count, but the corresponding SQL queries are drastically different due to differences in the target DB schema. As a result, cross-DB text-to-SQL semantic parsers cannot trivially memorize seen SQL patterns, but instead has to accurate"
2020.findings-emnlp.438,H90-1021,0,0.170618,"Missing"
2020.findings-emnlp.438,2020.acl-main.398,0,0.0570484,"lier version of this model is implemented within the Photon NLIDB model (Zeng et al., 2020), with up to one anchor text per field and an inferior anchor text matching algorithm. Joint Text-Table Representation and Pretraining BRIDGE is a general framework for jointly representing question, relational DB schema and DB values, and has the potential to be applied to a wide range of problems that requires joint textual-tabular data understanding. Recently, Yin et al. (2020) proposes TaBERT, an LM for jointly representing textual and tabular data pre-trained over millions of web tables. Similarly, Herzig et al. (2020) proposes TaPas, a pretrained text-table LM that supports arithmetic operations for weakly supervised table QA. Both TaBERT and TaPaSand supports arit focus on representing text with a single table. TaBERT was applied to Spider by encoding each table individually and modeling crosstable correlation through hierarchical attention. In comparison, BRIDGE serialized the relational DB schema and uses BERT to model cross-table dependencies. TaBERT adopts the “content snapshot” Train Dev Test #Q # SQL #DB 8,695 1,034 2,147 4,730 564 – 140 20 40 Table 2: Spider Dataset Statistics mechanism which retri"
2020.findings-emnlp.438,P82-1020,0,0.707721,"Missing"
2020.findings-emnlp.438,2021.ccl-1.108,0,0.161608,"Missing"
2020.findings-emnlp.438,N18-2074,0,0.053925,"e condition in Lemma 1 with little overhead in decoding speed. 3 Written: SELECT FROM WHERE GROUPBY HAVING ORDERBY LIMIT Exec: FROM WHERE GROUPBY HAVING SELECT ORDERBY LIMIT Table 1: The written order vs. execution order of all SQL clauses appeared in Spider. 3 Related Work Text-to-SQL Semantic Parsing Recently the field has witnessed a re-surge of interest for textto-SQL semantic parsing (Androutsopoulos et al., 1995), by virtue of the newly released large-scale datasets (Zhong et al., 2017; Yu et al., 2018; Zhang et al., 2019) and matured neural network modeling tools (Vaswani et al., 2017; Shaw et al., 2018; Devlin et al., 2019). While existing models have surpassed human performance on benchmarks consisting of single-table and simple SQL queries (Hwang et al., 2019; Lyu et al., 2020; He et al., 2019a), ample space of improvement still remains for the Spider benchmark which consists of relational DBs and complex SQL queries4 . Recent architectures proposed for this problem show increasing complexity in both the encoder and the decoder (Guo et al., 2019; Wang et al., 2019; Choi et al., 2020). Bogin et al. (2019a,b) proposed to encode relational DB schema as a graph and also use the graph structur"
2020.findings-emnlp.438,D18-1548,0,0.064112,"Missing"
2020.findings-emnlp.438,2020.acl-main.742,0,0.458229,"l et al., 1990; Dahl et al., 1994; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016). However, DBs are widely used in many domains and developing a semantic parser for each individual DB is unlikely to scale in practice. More recently, large-scale datasets consisting of hundreds of DBs and the corresponding questionSQL pairs have been released (Yu et al., 2018; Zhong et al., 2017; Yu et al., 2019b,a) to encourage the development of semantic parsers that can work well across different DBs (Guo et al., 2019; Bogin et al., 2019b; Zhang et al., 2019; Wang et al., 2019; Suhr et al., 2020; Choi et al., 2020). The setup is challenging as it requires the model to interpret a question conditioned on a relational DB unseen during training and accurately express the question intent via SQL logic. Consider the two examples shown in Figure 1, both questions have the intent to count, but the corresponding SQL queries are drastically different due to differences in the target DB schema. As a result, cross-DB text-to-SQL semantic parsers cannot trivially memorize seen SQL patterns, but instead has to accurately model the natural language question, the target DB structure, and the contex"
2020.findings-emnlp.438,P19-3007,0,0.0410546,"Missing"
2020.findings-emnlp.438,J84-3009,0,0.268833,"Missing"
2020.findings-emnlp.438,P17-1099,0,0.015613,"N | hS = [h , . . . , h , h , . . . , h are inserted into the question-schema representation X, succeeding the corresponding field names and separated by the special token [V]. If multiple values were matched for one field, we concatenate all of them in matching order (Figure 2). If a question mention is matched with values in multiple fields. We add all matches and let the model learn to resolve ambiguity1 . The anchor texts provide additional lexical clues for BERT to identify the corresponding mention in Q. And we name this mechanism “bridging”. 2.4 We use an LSTM-based pointer-generator (See et al., 2017) with multi-head attention (Vaswani et al., 2017) as the decoder. The decoder starts from the final state of the question encoder. At each step, the decoder performs one of the following actions: generating a token from the vocabulary V, copying a token from the question Q or copying a schema component from S. Mathematically, at each step t, given the decoder state st and the encoder representation [hQ ; hS ] ∈ R(|Q|+|S|)×n , we compute the multi-head attention as defined in Vaswani et al. (2017): + bg ) ]∈R |S|×n Decoder n o s t WU(h) (h j WV(h) )> ; αt(h)j = softmax e(h) √ tj j n/H |Q|+|S| X"
2020.findings-emnlp.438,P19-1010,0,0.0178259,"tecture of RAT-SQL is deep, consisting of 8 relational self-attention layers on top of BERT-large. In comparison, BRIDGE uses BERT combined with minimal subsequent layers. It uses a simple sequence decoder with search space-pruning heuristics and applies little abstraction to the SQL surface form. Its encoding architecture took inspiration from the table-aware BERT encoder proposed by Hwang et al. (2019), which is very effective for WikiSQL but has not been successful adapted to Spider. Yavuz et al. (2018) uses question-value matches to achieve high-precision condition predictions on WikiSQL. Shaw et al. (2019) also shows that value information is critical to the cross-DB semantic parsing tasks, yet the paper reported negative results augmenting an GNN encoder with BERT and the overall model performance is much below state-of-the-art. While previous work such as (Guo et al., 2019; Wang et al., 2019; Yin et al., 2020) use feature embeddings or relational attention layers to explicitly model schema linking, BRIDGE models the linking implicitly with BERT and lexical anchors. An earlier version of this model is implemented within the Photon NLIDB model (Zeng et al., 2020), with up to one anchor text per"
2020.findings-emnlp.438,D18-1197,0,0.0222403,"which effectively covers relations in the schema graph and its linking with the question. The overall architecture of RAT-SQL is deep, consisting of 8 relational self-attention layers on top of BERT-large. In comparison, BRIDGE uses BERT combined with minimal subsequent layers. It uses a simple sequence decoder with search space-pruning heuristics and applies little abstraction to the SQL surface form. Its encoding architecture took inspiration from the table-aware BERT encoder proposed by Hwang et al. (2019), which is very effective for WikiSQL but has not been successful adapted to Spider. Yavuz et al. (2018) uses question-value matches to achieve high-precision condition predictions on WikiSQL. Shaw et al. (2019) also shows that value information is critical to the cross-DB semantic parsing tasks, yet the paper reported negative results augmenting an GNN encoder with BERT and the overall model performance is much below state-of-the-art. While previous work such as (Guo et al., 2019; Wang et al., 2019; Yin et al., 2020) use feature embeddings or relational attention layers to explicitly model schema linking, BRIDGE models the linking implicitly with BERT and lexical anchors. An earlier version of"
2020.findings-emnlp.438,2020.acl-main.745,0,0.432253,"memorize seen SQL patterns, but instead has to accurately model the natural language question, the target DB structure, and the contextualization of both. State-of-the-art cross-DB text-to-SQL semantic parsers adopt the following design principles to 4870 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4870–4888 c November 16 - 20, 2020. 2020 Association for Computational Linguistics address the aforementioned challenges. First, the question and schema representation should be contextualized with each other (Hwang et al., 2019; Guo et al., 2019; Wang et al., 2019; Yin et al., 2020). Second, large-scale pre-trained language models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019c) can significantly boost parsing accuracy by providing better representations of text and capturing long-term dependencies. Third, under data privacy constraints, leveraging available DB content can resolve ambiguities in the DB schema (Bogin et al., 2019b; Wang et al., 2019; Yin et al., 2020). Consider the second example in Figure 1, knowing “PLVDB” is a value of the field Journal.Name helps the model to generate the WHERE condition. We present BRIDGE, a powerful sequentia"
2020.findings-emnlp.438,D18-1193,0,0.423657,"Text-to-SQL semantic parsing addresses the problem of mapping natural language utterances to executable relational DB queries. Early work in this area focus on training and testing the semantic parser on a single DB (Hemphill et al., 1990; Dahl et al., 1994; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016). However, DBs are widely used in many domains and developing a semantic parser for each individual DB is unlikely to scale in practice. More recently, large-scale datasets consisting of hundreds of DBs and the corresponding questionSQL pairs have been released (Yu et al., 2018; Zhong et al., 2017; Yu et al., 2019b,a) to encourage the development of semantic parsers that can work well across different DBs (Guo et al., 2019; Bogin et al., 2019b; Zhang et al., 2019; Wang et al., 2019; Suhr et al., 2020; Choi et al., 2020). The setup is challenging as it requires the model to interpret a question conditioned on a relational DB unseen during training and accurately express the question intent via SQL logic. Consider the two examples shown in Figure 1, both questions have the intent to count, but the corresponding SQL queries are drastically different due to differences"
2020.findings-emnlp.438,D19-1204,1,0.840283,"Missing"
2020.findings-emnlp.438,P19-1443,1,0.8214,"es the problem of mapping natural language utterances to executable relational DB queries. Early work in this area focus on training and testing the semantic parser on a single DB (Hemphill et al., 1990; Dahl et al., 1994; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016). However, DBs are widely used in many domains and developing a semantic parser for each individual DB is unlikely to scale in practice. More recently, large-scale datasets consisting of hundreds of DBs and the corresponding questionSQL pairs have been released (Yu et al., 2018; Zhong et al., 2017; Yu et al., 2019b,a) to encourage the development of semantic parsers that can work well across different DBs (Guo et al., 2019; Bogin et al., 2019b; Zhang et al., 2019; Wang et al., 2019; Suhr et al., 2020; Choi et al., 2020). The setup is challenging as it requires the model to interpret a question conditioned on a relational DB unseen during training and accurately express the question intent via SQL logic. Consider the two examples shown in Figure 1, both questions have the intent to count, but the corresponding SQL queries are drastically different due to differences in the target DB schema. As a result,"
2020.findings-emnlp.438,2020.acl-demos.24,1,0.850485,"ndition predictions on WikiSQL. Shaw et al. (2019) also shows that value information is critical to the cross-DB semantic parsing tasks, yet the paper reported negative results augmenting an GNN encoder with BERT and the overall model performance is much below state-of-the-art. While previous work such as (Guo et al., 2019; Wang et al., 2019; Yin et al., 2020) use feature embeddings or relational attention layers to explicitly model schema linking, BRIDGE models the linking implicitly with BERT and lexical anchors. An earlier version of this model is implemented within the Photon NLIDB model (Zeng et al., 2020), with up to one anchor text per field and an inferior anchor text matching algorithm. Joint Text-Table Representation and Pretraining BRIDGE is a general framework for jointly representing question, relational DB schema and DB values, and has the potential to be applied to a wide range of problems that requires joint textual-tabular data understanding. Recently, Yin et al. (2020) proposes TaBERT, an LM for jointly representing textual and tabular data pre-trained over millions of web tables. Similarly, Herzig et al. (2020) proposes TaPas, a pretrained text-table LM that supports arithmetic op"
2020.nlp4convai-1.14,D14-1162,1,0.113795,"Missing"
2020.nlp4convai-1.14,N19-1170,0,0.0144113,"conversation increased. Figure 4 visualizes this metric both for our model and KVMemNet. In the case of both models, the consistency decreases as the chat history get longer, indicating that models have problems keeping track of their previous statements. When analyzing the linear trend we noticed that the decrease in performance is slower for the Sketch-Fill-A-R model. We hypothesize that this effect can be partially caused by the high diversity of sequences generated by the KVMemNet, which in turn affects the models ability to generate consistent conversation. Effect of question responses (See et al., 2019) note that for a conversation to be engaging, responses in chit-chat dialogue should be a mix of statements and questions, where the model inquires about certain traits and information of the other agent. We expand on this by evaluating the effect of a question’s presence in the response has on the ratings coming from the judges. The results are presented in Figure 4c. The study showed that there is a strong correlation between the model asking a question and the users rating the response as (a) KVMemNet (b) Sketch-Fill-A-R (c) Sketch-Fill-A-R: Human ratings vs question/no-question responses F"
2020.starsem-1.17,D18-1547,0,0.0637825,"Missing"
2020.starsem-1.17,D19-5817,0,0.0477496,"Missing"
2020.starsem-1.17,P19-1546,0,0.489185,"rs. To mitigate the above issues, recently, (Zhou and Small, 2019) introduced a question asking model to generate questions asking for values of eachdomain slot pair and a dynamic knowledge graph to learn relationships between the (domain, slot) pairs. (Rastogi et al., 2020) introduced a BERT-based model (Devlin et al., 2019) to strike a balance between the two methods by pre-defining categorical and non-categorical slots. However, more studies are needed to know which slots are better handled by either of the two slot types, and the way to use the pre-trained models is not well investigated (Lee et al., 2019; Gao et al., 2019b; Rastogi et al., 2020). Inspired by the task-oriented dialog schema design in (Rastogi et al., 2020) and the recent successful experience in locating text spans in machine reading comprehensions (Gao et al., 2019b; Asai et al., 2019). we design a simple yet effective DualStrategy Dialog State Tracking model (DS-DST), which adapts a single BERT question answering model to jointly handle both the categorical and non-categorical slots, and different with previous approaches on multi-domain DST, we enable the model with direct interactions between dialog context and the slot. W"
2020.starsem-1.17,N19-1423,0,0.580859,"ctly from the input source using a copy mechanism without requiring an ontology, e.g., learning span matching with start and end positions in the dialog context. However, it is nontrivial to handle situations where values do not appear in the dialog context or have various descriptions by users. To mitigate the above issues, recently, (Zhou and Small, 2019) introduced a question asking model to generate questions asking for values of eachdomain slot pair and a dynamic knowledge graph to learn relationships between the (domain, slot) pairs. (Rastogi et al., 2020) introduced a BERT-based model (Devlin et al., 2019) to strike a balance between the two methods by pre-defining categorical and non-categorical slots. However, more studies are needed to know which slots are better handled by either of the two slot types, and the way to use the pre-trained models is not well investigated (Lee et al., 2019; Gao et al., 2019b; Rastogi et al., 2020). Inspired by the task-oriented dialog schema design in (Rastogi et al., 2020) and the recent successful experience in locating text spans in machine reading comprehensions (Gao et al., 2019b; Asai et al., 2019). we design a simple yet effective DualStrategy Dialog Sta"
2020.starsem-1.17,P18-1133,0,0.0548056,"dialog contexts, SUMBT (Lee et al., 2019) employs BERT to extract representations of candidate values, and BERT-DST (Rastogi et al., 2020) adopts BERT to encode the inputs of the user turn as well as the previous system turn. Different from these approaches where the dialog context and domain-slot pairs are usually separately encoded, we employ strong interactions to encode them. 1 . Moreover, We investigate and provide insights to decide slot types and conduct a comprehensive analysis of the popular MultiWOZ datasets. Another direction for multi-domain DST is based on generative approaches (Lei et al., 2018; Wu et al., 2019; Le et al., 2020) which generate slot values without relying on fixed vocabularies and spans. However, such generative methods suffer from generating ill-formatted strings (e.g., repeated words) upon long strings, which is common in DST. For example, the hotel address may be long and a small difference makes the whole dialog state tracking incorrect. By contrast, both the categorical (picklist-based) and non-categorical (span-based) methods can rely on existing strings rather than generating them. 3 pose a dual strategy model with direct interactions between dialog context an"
2020.starsem-1.17,P19-1617,0,0.0221163,"turn and Xt has {x1 , . . . , xm } tokens. Our goal is to predict the values for all the domain-slot pairs in S. Here we assume that M domain-slot pairs in S are treated as non-categorical slots, and the remaining N − M pairs as categorical slots. Each categorical slot has L possible candidate values (picklist), i.e., {V1 , . . . , VL }, where L is the size of the picklist, and each value has {v1 , . . . , vc } tokens. Bearing these notations in mind, we then pro1 Recent work on question answering has shown that the joint encoding of query-context pairs is crucial to achieving high accuracy (Qiu et al., 2019; Asai et al., 2019) Slot-Context Encoder 3.2 Slot-Gate Classification As there are many domain-slot pairs in multidomain dialogues, it is nontrivial to correctly predict whether a domain-slot pair appears at each turn of the dialogue. Here we follow (Wu et al., 2019; Xu and Hu, 2018) and design a slot gate classification module for our neural network. Specifically, at the tth turn, the classifier makes a decision among {none, dontcare, prediction}, where none denotes that a domain-slot pair is not mentioned or the value is ‘none’ at this turn, dontcare implies that the user can accept any val"
2020.starsem-1.17,W19-5932,0,0.141048,"Missing"
2020.starsem-1.17,W14-4337,0,0.149343,"ultiWOZ 2.0 (Budzianowski et al., 2018) and competitive performance on MultiWOZ 2.1 (Eric et al., 2019). Our model also performs robustly across the two different settings. • We conducted a comprehensive error analysis on the dataset, including the effects of the dual strategy for each slot, to facilitate future research. 2 Related Work Multi-domain DST, which tracks dialog states in complicated conversations across multiple domains with many slots, has been a hot research topic during the past few years, along with the development of Dialogue State Tracking Challenges (Williams et al., 2013; Henderson et al., 2014a,b; Kim et al., 2016, 2017, 2019). Traditional approaches usually rely on hand-crafted features or domain-specific lexicon (Henderson et al., 2014c; Wen et al., 2016), making them difficult to be adapted to new domains. In addition, these approaches require a pre-defined full ontology, in which the values of a slot are constrained by a set of candidate values (Ramadan et al., 2018; Liu and Lane, 2017; Zhong et al., 2018; Lee et al., 2019; Chen et al., 2020). To tackle these issues, several methods have been proposed to extract slot values through span matching with start and end positions in"
2020.starsem-1.17,P18-2069,0,0.104889,"Missing"
2020.starsem-1.17,W14-4340,0,0.0770855,"ultiWOZ 2.0 (Budzianowski et al., 2018) and competitive performance on MultiWOZ 2.1 (Eric et al., 2019). Our model also performs robustly across the two different settings. • We conducted a comprehensive error analysis on the dataset, including the effects of the dual strategy for each slot, to facilitate future research. 2 Related Work Multi-domain DST, which tracks dialog states in complicated conversations across multiple domains with many slots, has been a hot research topic during the past few years, along with the development of Dialogue State Tracking Challenges (Williams et al., 2013; Henderson et al., 2014a,b; Kim et al., 2016, 2017, 2019). Traditional approaches usually rely on hand-crafted features or domain-specific lexicon (Henderson et al., 2014c; Wen et al., 2016), making them difficult to be adapted to new domains. In addition, these approaches require a pre-defined full ontology, in which the values of a slot are constrained by a set of candidate values (Ramadan et al., 2018; Liu and Lane, 2017; Zhong et al., 2018; Lee et al., 2019; Chen et al., 2020). To tackle these issues, several methods have been proposed to extract slot values through span matching with start and end positions in"
2020.starsem-1.17,D19-1196,0,0.160931,"Missing"
2020.starsem-1.17,2020.acl-main.563,0,0.573832,"Missing"
2020.starsem-1.17,W13-4065,0,0.0277001,"s state of the art on MultiWOZ 2.0 (Budzianowski et al., 2018) and competitive performance on MultiWOZ 2.1 (Eric et al., 2019). Our model also performs robustly across the two different settings. • We conducted a comprehensive error analysis on the dataset, including the effects of the dual strategy for each slot, to facilitate future research. 2 Related Work Multi-domain DST, which tracks dialog states in complicated conversations across multiple domains with many slots, has been a hot research topic during the past few years, along with the development of Dialogue State Tracking Challenges (Williams et al., 2013; Henderson et al., 2014a,b; Kim et al., 2016, 2017, 2019). Traditional approaches usually rely on hand-crafted features or domain-specific lexicon (Henderson et al., 2014c; Wen et al., 2016), making them difficult to be adapted to new domains. In addition, these approaches require a pre-defined full ontology, in which the values of a slot are constrained by a set of candidate values (Ramadan et al., 2018; Liu and Lane, 2017; Zhong et al., 2018; Lee et al., 2019; Chen et al., 2020). To tackle these issues, several methods have been proposed to extract slot values through span matching with sta"
2020.starsem-1.17,2020.emnlp-main.66,1,0.894296,"Missing"
2020.starsem-1.17,P19-1078,1,0.940578,"sed approaches (Ramadan et al., 2018; Zhong et al., 2018; Chen et al., 2020) require full access to the pre-defined ontol154 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 154–167 Barcelona, Spain (Online), December 12–13, 2020 ogy to perform classification over the candidatevalue list. However, in practice, we may not have access to an ontology or only have partial ontology in the industry. Even if a full ontology exists, it is computationally expensive to enumerate all the values when the full ontology for some slots is very large and diverse (Wu et al., 2019; Xu and Hu, 2018). The ontology-free approaches (Gao et al., 2019b; Xu and Hu, 2018) find slot values directly from the input source using a copy mechanism without requiring an ontology, e.g., learning span matching with start and end positions in the dialog context. However, it is nontrivial to handle situations where values do not appear in the dialog context or have various descriptions by users. To mitigate the above issues, recently, (Zhou and Small, 2019) introduced a question asking model to generate questions asking for values of eachdomain slot pair and a dynamic knowledge graph to l"
2020.starsem-1.17,P18-1134,0,0.241203,"onsists of a set of < domain, slot, value &gt; triplets, and DST aims to track all the states accumulated across the conversational turns. Fig. 1 shows a dialogue with corresponding annotated turn states. Introduction ∗ Work done while the first author was an intern at Salesforce Research. † Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. Traditional approaches for DST usually rely on hand-crafted features and domain-specific lexicon, and can be categorized into two classes (Xu and Hu, 2018; Gao et al., 2019b; Ramadan et al., 2018; Zhong et al., 2018): i.e., ontology-based and ontology-free. The ontology-based approaches (Ramadan et al., 2018; Zhong et al., 2018; Chen et al., 2020) require full access to the pre-defined ontol154 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 154–167 Barcelona, Spain (Online), December 12–13, 2020 ogy to perform classification over the candidatevalue list. However, in practice, we may not have access to an ontology or only have partial ontology in the industry. Even if a full ontology exists, it is"
2020.starsem-1.17,2020.nlp4convai-1.13,1,0.866872,"Missing"
2020.starsem-1.17,P18-1135,1,0.945962,"DST aims to track all the states accumulated across the conversational turns. Fig. 1 shows a dialogue with corresponding annotated turn states. Introduction ∗ Work done while the first author was an intern at Salesforce Research. † Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. Traditional approaches for DST usually rely on hand-crafted features and domain-specific lexicon, and can be categorized into two classes (Xu and Hu, 2018; Gao et al., 2019b; Ramadan et al., 2018; Zhong et al., 2018): i.e., ontology-based and ontology-free. The ontology-based approaches (Ramadan et al., 2018; Zhong et al., 2018; Chen et al., 2020) require full access to the pre-defined ontol154 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 154–167 Barcelona, Spain (Online), December 12–13, 2020 ogy to perform classification over the candidatevalue list. However, in practice, we may not have access to an ontology or only have partial ontology in the industry. Even if a full ontology exists, it is computationally expensive to enumerate all the values when the"
2021.acl-long.85,D15-1075,0,0.0244735,"ne for comparison in our experiments. 4 Experiments Datasets We consider two distinct datasets for experiments, where one is to regard text from unseen corpora as OOD, and the other one is to detect class-level OOD samples within the same corpus. • Cross-corpus dataset (SST) We follow the experimental setting in Hendrycks et al. (2020), by providing in-domain Dn with the original training set of SST dataset (Socher et al., 2013) and considering samples from four other datasets (i.e., 20 Newsgroups (Lang, 1995), English-German Multi30K (Elliott et al., 2016), RTE (Dagan et al., 2005) and SNLI (Bowman et al., 2015)) as OOD data. For evaluation, we use the original test data of SST as in-domain positives and randomly pick 500 samples from each of the four datasets as OOD negatives. We do not include any sentiment labels from SST to Dn for training. • Cross-intent dataset (CLINIC150) This is a crowdsourced dialog dataset (Larson et al., 2019), including in-domain queries covering 150 intents and out-of-domain queries that do not fall within any of the 150 intents. We use all 15,000 queries that are originally in its training data as in-domain samples but discard their intent labels. For evaluation, we mix"
2021.acl-long.85,N19-1423,0,0.629157,"n that generates the in-domain samples, which requires a weaker assumption than prior work (e.g., Lee et al., 2018; Hendrycks et al., 2020). We suppose that there are only in-domain samples, which allows us to understand the properties of data itself regardless of tasks. Therefore, methods developed for this problem are more applicable than task-specific ones and can be further adapted to tasks where no classification labels are present, such as active learning or transfer learning. To solve the problem, we utilize the latent embeddings of pre-trained transformers (e.g., Vaswani et al., 2017; Devlin et al., 2019; Liu et al., 2019) to represent the input data, which allow us to apply classical OOD detection methods such as one-class support vector machines (Schölkopf et al., 2001) or support vector data description (Tax and Duin, 2004) on them. However, the best practice on how to extract features from BERT is usually task-specific. For supervised classification, we can represent the text sequence using the hidden state of [CLS] token from the top layer. Meanwhile BERT’s intermediate layers also capture rich linguistic information that may outperform the top layer for specific NLP tasks. By performing"
2021.acl-long.85,W16-3210,0,0.052609,"Missing"
2021.acl-long.85,2020.acl-main.740,0,0.0547788,"Missing"
2021.acl-long.85,2020.acl-main.244,0,0.162557,"problem that only scales linearly with the number of layer L. We further define: b −1 (f` (xi ) − cb` ) , M` (xi ) = (f` (xi ) − cb` )&gt; Σ ` 3 We can further assume kΣk = 1, where the norm can be the operator norm or Frobenius norm, which can give the definition of the hyper-ellipsoid with unique Σ and R. 3.2 Feature fine-tuning We can also fine-tune the pre-trained transformer f on the unsupervised in-domain dataset Dn so x) can better represent the distribution of that f (x Dn . We explore two domain-specific fine-tuning approaches. In-domain masked language modeling (IMLM) Gururangan et al. (2020) find that domain-adaptive masked language modeling (Devlin et al., 2019) would improve supervised classification capability of BERT when it is transferred to that domain. 1055 Type In-Domain In-Domain Out-of-Domain Out-of-Domain Out-of-Domain Source SST SST RTE SNLI Multi30K Type In-Domain In-Domain In-Domain In-Domain Out-of-Domain Out-of-Domain Out-of-Domain Intent Transfer PTO Request Food Last Tell Joke — — — Cross-corpus Examples (SST) Text if you love reading and or poetry , then by all means check it out there ’s no disguising this as one of the worst films of the summer capital punish"
2021.acl-long.85,P84-1044,0,0.458339,"Missing"
2021.acl-long.85,P19-1356,0,0.0467944,"nt the input data, which allow us to apply classical OOD detection methods such as one-class support vector machines (Schölkopf et al., 2001) or support vector data description (Tax and Duin, 2004) on them. However, the best practice on how to extract features from BERT is usually task-specific. For supervised classification, we can represent the text sequence using the hidden state of [CLS] token from the top layer. Meanwhile BERT’s intermediate layers also capture rich linguistic information that may outperform the top layer for specific NLP tasks. By performing probing tasks on each layer, Jawahar et al. (2019) suggest bottom layers 1052 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1052–1061 August 1–6, 2021. ©2021 Association for Computational Linguistics of BERT capture more surface features, middle layers focus more on syntax and semantic features are well represented by top ones. As no prior knowledge about OOD samples is usually provided in practice, deciding which layer of features is the most effective for OOD detection is itself non-trivial. Some OOD samples may just c"
2021.acl-long.85,D19-1131,0,0.0503345,"Missing"
2021.acl-long.85,2021.naacl-main.106,1,0.839928,"Missing"
2021.acl-long.85,2020.coling-main.125,0,0.115231,"p neural networks, despite achieving good performance on many challenging tasks, can make overconfident predictions for completely irrelevant and out-of-domain (OOD) inputs, leading to significant AI safety issues (Hendrycks and Gimpel, 2017). Detecting out-of-domain inputs is a fundamental task for trustworthy AI applications in realworld use cases, because those applications are often subject to ill-defined queries or even potentially malicious inputs. Prior work on out-of-domain detection (e.g., Hendrycks and Gimpel, 2017; Lee et al., 2018; Liang et al., 2018; Hendrycks et al., 2019, 2020; Xu et al., 2020) mostly requires indomain task labels, limiting its usage to supervised classification. However, deployed applica1 Code is available at https://github.com/rivercold/ BERT-unsupervised-OOD. tions rarely receive controlled inputs and are susceptible to an ever-evolving set of user inputs that are scarcely labeled. For example, for many nonclassification tasks, such as summarization or topic modeling, there are no available classifiers or task labels, which limits the practical usage of recently proposed out-of-domain detection methods. Therefore, it is natural to ask the following question: Can"
2021.acl-long.85,2021.ccl-1.108,0,0.0770724,"Missing"
2021.acl-long.85,2020.emnlp-main.411,1,0.718562,"Missing"
2021.acl-long.85,D13-1170,0,0.00287279,"esides, the added classification layer can actually be applied for OOD detection using the MSP method, and this is exactly the setting of zero-shot classification, which we use as a baseline for comparison in our experiments. 4 Experiments Datasets We consider two distinct datasets for experiments, where one is to regard text from unseen corpora as OOD, and the other one is to detect class-level OOD samples within the same corpus. • Cross-corpus dataset (SST) We follow the experimental setting in Hendrycks et al. (2020), by providing in-domain Dn with the original training set of SST dataset (Socher et al., 2013) and considering samples from four other datasets (i.e., 20 Newsgroups (Lang, 1995), English-German Multi30K (Elliott et al., 2016), RTE (Dagan et al., 2005) and SNLI (Bowman et al., 2015)) as OOD data. For evaluation, we use the original test data of SST as in-domain positives and randomly pick 500 samples from each of the four datasets as OOD negatives. We do not include any sentiment labels from SST to Dn for training. • Cross-intent dataset (CLINIC150) This is a crowdsourced dialog dataset (Larson et al., 2019), including in-domain queries covering 150 intents and out-of-domain queries tha"
2021.eacl-main.151,C18-1261,0,0.0233163,"s. We suspect this is due to the strong coupling between the energy function and the classification logits. We provide concrete examples in Table 3. However, we need to mention that we do not observe this interesting trend (Figure 4) in all datasets (e.g., QNLI). 4 7.5 Figure 4: The entropy of the posterior (Pθ p¨|xq) versus ˆθ pxq for SST-2 test-set samples. energy value E Related Works Finally, we review applications of NCE or energybased models in the NLP literature. Due to its selfnormalizing property, NCE training has been used for faster inference (Mnih and Teh, 2012; Chen et al., 2015; Labeau and Allauzen, 2018) of auto1757 regressive language models. It has also been used in an attempt to train a sentence-level bi-directional LM (He et al., 2016). More closely related to our work, Deng et al. (2020) adopts NCE to train an EBM defined on top of a text encoder (the scalar variant), and uses it to improve language generation. EBM has also been recently used in non-autoregressive machine translation (Tu et al., 2020). 5 Conclusion In this work, we explore joint EBM training during the finetuning of pretrained text encoders with noise contrastive estimation. We find that joint EBM training can greatly im"
2021.emnlp-main.417,K16-1002,0,0.0273888,"n several previous works (Iyyer et al., 2018b; Chen et al., 2019; Li et al., 2019; Kumar et al., 2019; Goyal and Durrett, 2020) that make use of syntactic structures to produce more diverse paraphrases. More recently, Qian et al. (2019) employ distinct generators to produce diverse paraphrases. Retrieval-augmented generation methods have also been investigated (Kazemnejad et al., 2020; Lewis et al., 2020). However, most of these approaches require parallel data. Unsupervised Approaches Unsupervised paraphrasing, on the other hand, is a rather less explored and more challenging problem in NLP. Bowman et al. (2016) train a variational autoencoder (VAE) to maximize the lower bounds for the reconstruction log-likelihood of the input sentence without requiring any parallel corpora. Sampling from the 6 Related Work trained VAE’s decoder leads to sentences that can Paraphrase generation has been a long-standing practically be considered as paraphrases as the detask that has several applications on downstream coder aims to reconstruct the input sentence by its NLP tasks including text summarization (Cao et al., training objective. Miao et al. (2018) introduce a 2016), semantic parsing (Berant and Liang, 2014)"
2021.emnlp-main.417,D18-2029,0,0.0689323,"Missing"
2021.emnlp-main.417,N19-1254,0,0.0205961,"ta et al., 2018); sometimes such models are also used to evaluate paraphrasing quality (Thompson and Post, 2020). Round-trip translation between two languages (i.e., back-translation) with strong neural machine translation (NMT) models has also become a widely used approach for paraphrase generation (Yu et al., 2018). Consequently, supervised models using datasets like ParaNMT obtain their performance mainly from sequence-level distillation (Kim and Rush, 2016), where the data comes from the underlying supervised translation models. There have been several previous works (Iyyer et al., 2018b; Chen et al., 2019; Li et al., 2019; Kumar et al., 2019; Goyal and Durrett, 2020) that make use of syntactic structures to produce more diverse paraphrases. More recently, Qian et al. (2019) employ distinct generators to produce diverse paraphrases. Retrieval-augmented generation methods have also been investigated (Kazemnejad et al., 2020; Lewis et al., 2020). However, most of these approaches require parallel data. Unsupervised Approaches Unsupervised paraphrasing, on the other hand, is a rather less explored and more challenging problem in NLP. Bowman et al. (2016) train a variational autoencoder (VAE) to ma"
2021.emnlp-main.417,2020.emnlp-main.634,0,0.0222596,"ri-gram blocking entry ab → c in the block dictionary. If this entry is triggered, then the bi-gram blocking entry b → c will also have been triggered. Hence we found it unnecessary to include higher-order n-grams. 2.3 Self-Supervision To help the model internalize patterns learned from task-adaption, we pseudo-label the training set (Siddhant et al., 2020) by decoding the taskadapted model with Dynamic Blocking. Having obtained the self-supervision data, we discard the task-adapted model and start from the pretrained language model to avoid catastrophic forgetting (Chronopoulou et al., 2019; Chen et al., 2020). We also include reversed data (i.e., swapping source and target) because during task-adaptation the target is always longer than the input, and including reversed data helps to offset this bias of sequence length. 3 Experimental Setup 3.1 BERT-iBLEU To evaluate paraphrasing quality, we propose a new metric named BERT-iBLEU which encourages semantic closeness while penalizing surface-form similarity. For semantic closeness we use the unsupervised metric BERT-score (Zhang et al., 2019), which leverages a pretrained language model to compute the cosine similarity between each token in the candi"
2021.emnlp-main.417,N18-1170,0,0.125414,"ed with extensive knowledge in paraphrasing. This knowledge may be attributed to the fact that text spans sharParaphrase generation restates text input in a different surface form while preserving its semantics. It has various applications on downstream NLP tasks including text summarization (Cao et al., 2016), semantic parsing (Berant and Liang, 2014), as well as diversifying text generation for user-facing systems such as chatbots. To evaluate model robustness, a paraphraser can be used to generate adversarial examples, which also serve as augmented data to train the target neural networks (Iyyer et al., 2018a). Besides, paraphrasing queries makes Question Answering systems more likely to match with key1 words in a knowledge base (Fader et al., 2014; Yin https://www.kaggle.com/c/ et al., 2015). quora-question-pairs 5136 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5136–5150 c November 7–11, 2021. 2021 Association for Computational Linguistics ing similar context usually stay semantically close together – word embedding (Mikolov et al., 2013) being a classic example. In other words, the paraphrasing capability of language models stems from the strong"
2021.emnlp-main.417,P13-1158,0,0.034655,"to train the self-supervised model. However, it is expensive to annotate paraphrases, resulting in only a few human-labeled datasets. The existing ones are either small-scale like MRPC (Dolan and Brockett, 2005), or of closed domains like QQP1 which consists entirely of questions. Consequently, previous work explored automatically (hence noisily) annotated datasets such as PIT-2015 (Xu et al., 2013), Twitter URL Paraphrase Corpus (Lan et al., 2017), ParaNMT (Wieting and Gimpel, 2018), and ParaBank (Hu et al., 2019), or re-purposed datasets including MSCOCO (Lin et al., 2014) and WikiAnswers (Fader et al., 2013). The scarcity of highquality datasets motivates us to consider unsupervised alternatives. In this work, we explore a transfer learning approach, which leverages unsupervised large-scale pretrained models like T5 (Raffel et al., 2019) and BART (Lewis et al., 2019). The effectiveness of BERT-score (Zhang et al., 2019) in identifying text similarity hints that pretrained language models are equipped with extensive knowledge in paraphrasing. This knowledge may be attributed to the fact that text spans sharParaphrase generation restates text input in a different surface form while preserving its s"
2021.emnlp-main.417,K19-1005,0,0.0449173,"Missing"
2021.emnlp-main.417,P19-1332,0,0.0995213,"-adaptive phase (Section 2.1), evaluating semantic similarity compared to these metrics. 6 except that they corrupt the inputs by removing all We use the BookCorpus dataset (Zhu et al., 2015) to compute the IDF weights. stop words rather than a fixed percentage of arbi5139 4 trary words.7 Instead of GPT-2 as used by their work, we use BART which shows stronger results on downstream tasks. The rest of the settings remain the same.8 For the experiments on ParaNMT we use the SOW-REAP model released by Goyal and Durrett (2020).9 3.4 Automatic Evaluation To evaluate paraphrasing quality, we follow Li et al. (2019) to report iBLEU (Sun and Zhou, 2012), BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) on QQP, and report BLEU and ROUGE on ParaNMT. Follwing Goyal and Durrett (2020), for ParaNMT both BLEU and ROUGE are calculated by first selecting the candidate that achieves the best sentence-level score with the ground-truth, and then compute the corpus-level score of all these candidates. We use py-rouge10 to compute ROUGE and the Datasets library from HuggingFace11 to compute BLEU. We also report BERT-iBLEU for the models we reproduced. 3.5 Human Evaluation We conduct human evaluations on MTurk.12 For"
2021.emnlp-main.417,C16-1275,0,0.026359,"cross-lingual transfer on a variety of downstream classification tasks, such as Named Entity Recognition (Moon et al., 2019), Natural Language Inference, and Document Classification (Artetxe and Schwenk, 2019). Our work hence demonstrates that it is possible to perform such a transfer even for generative tasks like paraphrasing. We also hypothesize that the paraphrasing quality should improve if we apply our training pipeline to mBART or mT5 (Xue et al., 2020). We leave this as future work. Supervised Approaches Neural sequence-tosequence (Seq2Seq) models have been used to address this task (Prakash et al., 2016; Li et al., 2017; See et al., 2017; Vaswani et al., 2017; Gupta et al., 2018); sometimes such models are also used to evaluate paraphrasing quality (Thompson and Post, 2020). Round-trip translation between two languages (i.e., back-translation) with strong neural machine translation (NMT) models has also become a widely used approach for paraphrase generation (Yu et al., 2018). Consequently, supervised models using datasets like ParaNMT obtain their performance mainly from sequence-level distillation (Kim and Rush, 2016), where the data comes from the underlying supervised translation models."
2021.emnlp-main.417,D19-1313,0,0.0187291,"translation) with strong neural machine translation (NMT) models has also become a widely used approach for paraphrase generation (Yu et al., 2018). Consequently, supervised models using datasets like ParaNMT obtain their performance mainly from sequence-level distillation (Kim and Rush, 2016), where the data comes from the underlying supervised translation models. There have been several previous works (Iyyer et al., 2018b; Chen et al., 2019; Li et al., 2019; Kumar et al., 2019; Goyal and Durrett, 2020) that make use of syntactic structures to produce more diverse paraphrases. More recently, Qian et al. (2019) employ distinct generators to produce diverse paraphrases. Retrieval-augmented generation methods have also been investigated (Kazemnejad et al., 2020; Lewis et al., 2020). However, most of these approaches require parallel data. Unsupervised Approaches Unsupervised paraphrasing, on the other hand, is a rather less explored and more challenging problem in NLP. Bowman et al. (2016) train a variational autoencoder (VAE) to maximize the lower bounds for the reconstruction log-likelihood of the input sentence without requiring any parallel corpora. Sampling from the 6 Related Work trained VAE’s d"
2021.emnlp-main.417,2020.acl-main.28,0,0.0321273,"014), constrained sentence generation approach by usand question answering (Yu et al., 2018). Early ing Metropolis-Hastings sampling, which allows works on paraphrase generation mostly rely on for decoding with complicated discrete constraints rule-based or statistical machine translation sys- such as the occurrence of multiple keywords, hence tems (McKeown, 1980; Meteer and Shaked, 1988; not requiring any parallel corpora. Roy and GrangBannard and Callison-Burch, 2005). ier (2019) introduce a model that allows interpo5143 lation from continuous auto-encoders to vectorquantized auto-encoders. Liu et al. (2020) cast the paraphrasing as an optimization problem, where it searches the sentence space to find the optimal point for an objective function that takes semantic similarity, expression diversity, and language fluency into account. Siddique et al. (2020) optimize a similar objective with deep reinforcement learning. Transfer Learning There have been few works leveraging pre-trained language models for paraphrasing, either in a supervised (Witteveen and Andrews, 2019) or an unsupervised (Hegde and Patil, 2020) setting. Both works employ GPT-2 as their backbone generation model. Similarly, we opt f"
2021.emnlp-main.417,2020.acl-main.704,0,0.0488811,"Missing"
2021.emnlp-main.417,W18-6456,0,0.0182911,"ame corpus as inputs. We filter out any sentences in SOW-REAP’s test set to avoid training on test examples. 3.3 Reproduction of Previous Models For the experiments on QQP we reproduce the supervised Transformer with the pre-trained T5-base For more details on Dynamic Blocking, please refer to Appendix D. model, which is stronger than the usual setting 5 In early experiments we tried another unsupervised metric where the paraphraser trains from scratch. We also Universal Sentence Encoder (Cer et al., 2018) and supervised reproduce the model from Hegde and Patil (2020), metrics including RUSE (Shimanaka et al., 2018), SentenceBERT (Reimers and Gurevych, 2019), and BLEURT (Sellam which we refer to as CorruptLM. This model is et al., 2020). We observed that BERT-score worked better at similar to our task-adaptive phase (Section 2.1), evaluating semantic similarity compared to these metrics. 6 except that they corrupt the inputs by removing all We use the BookCorpus dataset (Zhu et al., 2015) to compute the IDF weights. stop words rather than a fixed percentage of arbi5139 4 trary words.7 Instead of GPT-2 as used by their work, we use BART which shows stronger results on downstream tasks. The rest of the set"
2021.emnlp-main.417,2020.acl-main.252,0,0.0281655,"It is 5138 beneficial to combine the two decoding methods because beam search helps to weed out ungrammatical or semantically invalid candidates.4 Note that we only adopt bi-gram blocking because it is a superset of all higher-gram blockings. Consider, e.g., a tri-gram blocking entry ab → c in the block dictionary. If this entry is triggered, then the bi-gram blocking entry b → c will also have been triggered. Hence we found it unnecessary to include higher-order n-grams. 2.3 Self-Supervision To help the model internalize patterns learned from task-adaption, we pseudo-label the training set (Siddhant et al., 2020) by decoding the taskadapted model with Dynamic Blocking. Having obtained the self-supervision data, we discard the task-adapted model and start from the pretrained language model to avoid catastrophic forgetting (Chronopoulou et al., 2019; Chen et al., 2020). We also include reversed data (i.e., swapping source and target) because during task-adaptation the target is always longer than the input, and including reversed data helps to offset this bias of sequence length. 3 Experimental Setup 3.1 BERT-iBLEU To evaluate paraphrasing quality, we propose a new metric named BERT-iBLEU which encourag"
2021.emnlp-main.417,D19-5623,0,0.0152593,"ison-Burch, 2005). ier (2019) introduce a model that allows interpo5143 lation from continuous auto-encoders to vectorquantized auto-encoders. Liu et al. (2020) cast the paraphrasing as an optimization problem, where it searches the sentence space to find the optimal point for an objective function that takes semantic similarity, expression diversity, and language fluency into account. Siddique et al. (2020) optimize a similar objective with deep reinforcement learning. Transfer Learning There have been few works leveraging pre-trained language models for paraphrasing, either in a supervised (Witteveen and Andrews, 2019) or an unsupervised (Hegde and Patil, 2020) setting. Both works employ GPT-2 as their backbone generation model. Similarly, we opt for more recent large-scale pre-trained models like BART and T5. 7 Conclusion We design an effective training pipeline that enables large-scale pre-trained models to generate high-quality paraphrases in an unsupervised setting through task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking. We demonstrate with automatic and human evaluations that our model achieves state-of-theart results on benchmark datasets. We also show that our"
2021.emnlp-main.417,W13-2515,0,0.0296591,"ain a task-adapted model with a denoising objective so that it is able to reconstruct input text. We then use Dynamic Blocking (DB) to generate pseudopairs of paraphrasing data. Finally, the generated data is used to train the self-supervised model. However, it is expensive to annotate paraphrases, resulting in only a few human-labeled datasets. The existing ones are either small-scale like MRPC (Dolan and Brockett, 2005), or of closed domains like QQP1 which consists entirely of questions. Consequently, previous work explored automatically (hence noisily) annotated datasets such as PIT-2015 (Xu et al., 2013), Twitter URL Paraphrase Corpus (Lan et al., 2017), ParaNMT (Wieting and Gimpel, 2018), and ParaBank (Hu et al., 2019), or re-purposed datasets including MSCOCO (Lin et al., 2014) and WikiAnswers (Fader et al., 2013). The scarcity of highquality datasets motivates us to consider unsupervised alternatives. In this work, we explore a transfer learning approach, which leverages unsupervised large-scale pretrained models like T5 (Raffel et al., 2019) and BART (Lewis et al., 2019). The effectiveness of BERT-score (Zhang et al., 2019) in identifying text similarity hints that pretrained language mod"
2021.emnlp-main.808,I11-1038,0,0.02931,"2020), natural language generation (Camburu et al., 2018; Rajani et al., 2019; Hase et al., 2020; Wiegreffe et al., 2020), and recently, kNN-based methods (Papernot and McDaniel, 2018; Rajani et al., 2020). Influence Functions. The use of influence-based diagnostics can be traced back to the seminal papers such as Cook (1977); Cook and Weisberg (1980, 1982); Cook (1986). Recently, Koh and Liang (2017) brought influence functions to largescale deep learning and have been followed up by numerous publications. For example, Koh et al. (2018) used them for data poisoning attacks, Schulam and Saria (2019) for calibrating trust in individual model predictions, Brunet et al. (2019) for tracing biases in word embeddings, Koh et al. (2019) and Basu et al. (2020) for identifying important groups of training data, and Feldman and Zhang (2020) for studying neural networks memorization. In the context of influence functions for NLP, Han et al. (2020) used them to explain model predictions and unveil data artifacts. Yang et al. (2020) used them to estimate the quality of synthetic training samples in the context of dataaugmentation. Meng et al. (2020) explored the combination of gradient-based methods"
2021.findings-acl.434,2020.nlp4convai-1.5,0,0.0383808,"Missing"
2021.findings-acl.434,N19-1423,0,0.0246829,"ditional network and objective function designed to avoid manifold intrusion. Other work tried to explain the work mechanisms of M IXUP from different threads, such as “M IXUP as directional adversarial training” (Archambault et al., 2019), “M IXUP training as the complexity reduction” (Kimura, 2020) To date, only a couple of previous studies explored the effectiveness of the standard M IXUP in NLP. Guo et al. (2019a) tried two strategies: interpolating word embeddings or sentence embeddings generated by convolutional/recurrent neural networks. Sun et al. (2020) incorporated M IXUP into BERT (Devlin et al., 2019), the state of the art architecture in NLP. To improve the standard M IXUP, Guo (2020) added non-linearity to the M IXUP for text classification tasks. However, that non-linear M IXUP works on word embedding level, which is less applicable to Transformer-style (Vaswani et al., 2017) systems. All the work above are pair-wise mixup, this work is the first work that interpolates all the examples in the same mini-batch to cover the representation space better. 3 The Base Model: M IXUP Given a pair of samples (xi , yi ) and (xj , yj ) from the original mini-batch (x: input, y: the one-hot label), t"
2021.findings-acl.434,N18-1101,0,0.0189867,"w-shot setting that train with limited training data. Unfortunately, prior work about mixup never evaluated on few-shot scenarios. Tasks. We evaluate on the following three tasks. • Textual Entailment. Textual entailment is a task that figures out the truth value of a hypothesis sentence given a premise sentence (Dagan et al., 2005). This is a binary classification (“entailment” or “non-entailment”) problem where the input is a sentence pair. We use the GLUE RTE (Wang et al., 2019) benchmark which has 2,490/277/2,999 examples in train/dev/test. The smaller size of this dataset (compared MNLI (Williams et al., 2018) for example) makes it a good testbed for data augmentation techniques. • Relation Classification. FewRel (Han et al., 2018) is a large-scale relation classification dataset. It has 100 relation types, each with 700 labeled examples. The original FewRel relation set was split by 64/16/20 for developing meta-learning techniques which only allow a test instance to search for its relation type within the 20 candidates. This is not a practical setting because (i) in relation detection, an input should search for a label in the entire space of defined relations, (ii) we should always define a ”None"
2021.findings-acl.434,P17-2090,0,0.0299217,"mini-batch. In this work, we propose BATCH M IXUP—improving the model learning by interpolating hidden states of the entire mini-batch. BATCH M IXUP can generate new points scattered throughout the space corresponding to the mini-batch. In experiments, BATCH M IXUP shows superior performance than competitive baselines in improving the performance of NLP tasks while using different ratios of training data. 1 Introduction The study of data augmentation techniques has a long history in the NLP community. Typical data augmentations include synonym replacement (Kobayashi, 2018), back-translation (Fadaee et al., 2017), adding data noise (Xie et al., 2017), etc. Mostly, these techniques are combined with the augmentation-free models in pipeline. M IXUP (Zhang et al., 2018) is able to augment the data by linearly combining each two examples by their hidden representations, keeping the whole system trained in end-to-end. M IXUP has shown effectiveness in a range of NLP tasks (Sun et al., 2020). Nevertheless, it has two drawbacks. First, M IXUP generates new points merely and exactly on the connecting edges of random point pairs; these new points cover pretty limited region in the representation space of the m"
2021.findings-acl.434,2020.emnlp-main.660,1,0.799154,"Missing"
2021.findings-acl.434,D18-1514,0,0.0260426,"Missing"
2021.findings-acl.434,N18-2072,0,0.0222383,"d regions in the entire space of the mini-batch. In this work, we propose BATCH M IXUP—improving the model learning by interpolating hidden states of the entire mini-batch. BATCH M IXUP can generate new points scattered throughout the space corresponding to the mini-batch. In experiments, BATCH M IXUP shows superior performance than competitive baselines in improving the performance of NLP tasks while using different ratios of training data. 1 Introduction The study of data augmentation techniques has a long history in the NLP community. Typical data augmentations include synonym replacement (Kobayashi, 2018), back-translation (Fadaee et al., 2017), adding data noise (Xie et al., 2017), etc. Mostly, these techniques are combined with the augmentation-free models in pipeline. M IXUP (Zhang et al., 2018) is able to augment the data by linearly combining each two examples by their hidden representations, keeping the whole system trained in end-to-end. M IXUP has shown effectiveness in a range of NLP tasks (Sun et al., 2020). Nevertheless, it has two drawbacks. First, M IXUP generates new points merely and exactly on the connecting edges of random point pairs; these new points cover pretty limited reg"
2021.findings-acl.434,2021.ccl-1.108,0,0.0612197,"Missing"
2021.findings-acl.434,2020.coling-main.305,1,0.92628,"t ratios of training data. 1 Introduction The study of data augmentation techniques has a long history in the NLP community. Typical data augmentations include synonym replacement (Kobayashi, 2018), back-translation (Fadaee et al., 2017), adding data noise (Xie et al., 2017), etc. Mostly, these techniques are combined with the augmentation-free models in pipeline. M IXUP (Zhang et al., 2018) is able to augment the data by linearly combining each two examples by their hidden representations, keeping the whole system trained in end-to-end. M IXUP has shown effectiveness in a range of NLP tasks (Sun et al., 2020). Nevertheless, it has two drawbacks. First, M IXUP generates new points merely and exactly on the connecting edges of random point pairs; these new points cover pretty limited region in the representation space of the mini-batch. Second, the training of a system equipped with M IXUP is considerably inefficient— generally, M IXUP slows down the training by n times if it generates n new points for each original point pair. In this work, we propose BATCH M IXUP, an improved mixup paradigm that generates new points scattered uniformly throughout the whole representation region of the mini-batch."
2021.findings-acl.435,N19-1423,0,0.0379345,"52 words) doc. Curation summarization news (229∼842 words) hypothesis length single sentence (4∼18 words) single sentence (6∼22 words) multi-sent (80 ∼100 words) 3∼4 sent. (40∼50 words) multi-sent (64∼279 words) Table 1: Data resources that are reformatted into D OC NLI. even a document, and the hypotheses cover a large range of granularity: from a single sentence to a longer paragraph (e.g., 250 words); (ii) Diverse domains; (iii) No severe artifacts; for example, we do not include the hypotheses that can be easily found “grammatically incorrect” by well-trained language models such as BERT (Devlin et al., 2019). 3.1 Data Preprocessing Table 1 lists all the resources that we use to create D OC NLI. Briefly, D OC NLI combines and reformats five existing NLP benchmarks: adversarial NLI (ANLI) (Nie et al., 2020), the question answering benchmark SQuAD (Rajpurkar et al., 2016) and three summarization benchmarks (DUC20013 , CNN/DailyMail (Nallapati et al., 2016), and Curation4 (Curation, 2020)). Next, we describe how each data resource is integrated into D OC NLI. ANLI to D OC NLI. ANLI is a large-scale NLI dataset collected via an iterative, adversarial human-and-model-in-the-loop procedure. In each roun"
2021.findings-acl.435,N18-2017,0,0.0191109,"skar et al., 2019), a state-ofthe-art controllable text generator, to generate a new sentence which is used to replace the selected sentence. This operation generates a new “fake” summary. Table 2 illustrates a (document, real summary) pair in the Curation dataset, and the three types of “fake” summaries we generated. 3.2 Mitigating Artifacts in D OC NLI In Section 3.1, we transformed these NLI, QA and summarization datasets to satisfy the format of D OC NLI. We refer this resulting dataset as rawD OC NLI. In consideration of the common artifacts in some popular sentence-level NLI benchmarks (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018), we tried a “hypothesis-only” baseline based on RoBERTa on this raw-D OC NLI. Sur4916 D OC NLI raw D OC NLI added pairs entail ANLI SQuAD {(D, R)} {(Fi+ , Fi )} not entail ANLI SQuAD {(D, Fi )} {(Fi , R)} train 466,653 475,661 942,314 entail. not entail sum dev 28,890 205,368 234,258 test 33,128 233,958 267,086 Table 4: Data sizes of D OC NLI. Table 3: D: a document in summarization benchmarks; R: a real summary; Fi : a fake summary derived from R (i=1· · · n); Fi+ : Using CTRL to insert a generated sentence between a random pair of consecutive sentences"
2021.findings-acl.435,2021.ccl-1.108,0,0.0736248,"Missing"
2021.findings-acl.435,D15-1075,0,0.0303937,"amework to study those NLP problems by casting the background text as a premise and the text of target meaning as a hypothesis. Then, a good NLI recognizer can be considerably translated to a well-performing system regarding respective NLP tasks. NLI was first studied in (Dagan et al., 2005). Research in the early stages was mostly driven by the PASCAL Recognizing Textual Entailment (RTE) challenges which are annual competitions with benchmark datasets released. In the past few years, the study of NLI has moved forward rapidly along with the construction of large-scale datasets, such as SNLI (Bowman et al., 2015), the science domain SciTail (Khot et al., 2018) and multi-genre MNLI (Williams et al., 2018), etc. However, some NLI datasets may not be suitable any more for solving downstream NLP problems since they were commonly crowdsourced in isolation from any end task 2 (Khot et al., 2018). In addition, most NLI datasets and studies paid attention merely to sentence-level inference — both the premises and hypotheses are single (and usually short) sentences. This makes them unsuitable for other open-ended NLP problems. For example, to verify the factual correctness of a document summary, sentence-level"
2021.findings-acl.435,P17-1152,0,0.0474086,"Missing"
2021.findings-acl.435,2020.acl-main.441,0,0.404372,"and answer span as inputs. Kry´sci´nski et al. (2019) created a (document, sentence) pair data “FactCC” to train a classifier for checking the factual correctness of single sentences in automatically generated summaries. FactCC is specific to the target summarization benchmark dataset, so it is unclear how well FactCC can generalize to other summarization benchmarks and other NLP problems. In addition, only single sentences act as hypotheses. Nevertheless, that literature exactly showed that document-level NLI, especially the inference of document-level hypotheses, is highly desirable. ANLI (Nie et al., 2020) also gather multi-sentence as premises. However, the sentence sizes in ANLI premises are pretty limited and the hypotheses in ANLI are single sentences consistently. To our knowledge, our D OC NLI is the first dataset that uses hypotheses longer than single sentences, and stays closely with end NLP tasks. 3 Data Creation What kind of document-level NLI dataset is preferred? (i) We want the premise is a paragraph or 4914 original task domain premise length various multi-sentence ANLI NLI (wiki, news, etc.) (20∼94 words) paragraph SQuAD QA wiki (27∼237 words) DUC doc. summarization news (2001)"
2021.findings-acl.435,S18-2023,0,0.0401762,"Missing"
2021.findings-acl.435,D13-1020,0,0.308729,"cessitate document-level inference. Task-specific finetuning can further improve the performance and achieve new state of the art for some end tasks. 2 Related Work To our knowledge, document-level NLI has attracted very little ink in the community, possibly because of the lack of labeled datasets. In this section, we mainly describe some prior NLI datasets that share some spirits with our D OC NLI. End-task driven. As mentioned in Section 1, the RTE series were driven by downstream NLP tasks such as information retrieval, information extraction, question answering, and summarization. MCTest (Richardson et al., 2013) is a question answering task in which a paragraph is given as background knowledge, then each question is paired with a positive answer and some negative answers. The MCTest benchmark released an NLI version of this corpus by treating the whole paragraph as a premise and combining the question and answer candidates as hypotheses. SciTail (Khot et al., 2018) is also derived from the end QA task of answering multiple-choice school-level science questions. Unlike MCTest, the premises in SciTail are single sentences selected by an information retrieval approach. By casting an end NLP task as NLI,"
2021.findings-acl.435,N18-1074,0,0.236391,", some NLI datasets may not be suitable any more for solving downstream NLP problems since they were commonly crowdsourced in isolation from any end task 2 (Khot et al., 2018). In addition, most NLI datasets and studies paid attention merely to sentence-level inference — both the premises and hypotheses are single (and usually short) sentences. This makes them unsuitable for other open-ended NLP problems. For example, to verify the factual correctness of a document summary, sentence-level NLI systems cannot be of much help (Kry´sci´nski et al., 2019). Considering the fact-checking task FEVER (Thorne et al., 2018) as another example, in order to figure out the truth value of a claim against a Wikipedia article, NLI 2 Except for RTE and SciTail 4913 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4913–4922 August 1–6, 2021. ©2021 Association for Computational Linguistics has to be done on individual sentences instead of using the whole article as the premise. In short, some NLP tasks require the reasoning of NLI to go beyond the sentence granularity, regarding both the premise and the hypothesis. In this work, we introduce D OC NLI, a largescale dataset for document-lev"
2021.findings-acl.435,L18-1239,0,0.0242261,"able text generator, to generate a new sentence which is used to replace the selected sentence. This operation generates a new “fake” summary. Table 2 illustrates a (document, real summary) pair in the Curation dataset, and the three types of “fake” summaries we generated. 3.2 Mitigating Artifacts in D OC NLI In Section 3.1, we transformed these NLI, QA and summarization datasets to satisfy the format of D OC NLI. We refer this resulting dataset as rawD OC NLI. In consideration of the common artifacts in some popular sentence-level NLI benchmarks (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018), we tried a “hypothesis-only” baseline based on RoBERTa on this raw-D OC NLI. Sur4916 D OC NLI raw D OC NLI added pairs entail ANLI SQuAD {(D, R)} {(Fi+ , Fi )} not entail ANLI SQuAD {(D, Fi )} {(Fi , R)} train 466,653 475,661 942,314 entail. not entail sum dev 28,890 205,368 234,258 test 33,128 233,958 267,086 Table 4: Data sizes of D OC NLI. Table 3: D: a document in summarization benchmarks; R: a real summary; Fi : a fake summary derived from R (i=1· · · n); Fi+ : Using CTRL to insert a generated sentence between a random pair of consecutive sentences in the Fi , in a way similar to what w"
2021.findings-acl.435,N18-1101,0,0.0271823,"t of target meaning as a hypothesis. Then, a good NLI recognizer can be considerably translated to a well-performing system regarding respective NLP tasks. NLI was first studied in (Dagan et al., 2005). Research in the early stages was mostly driven by the PASCAL Recognizing Textual Entailment (RTE) challenges which are annual competitions with benchmark datasets released. In the past few years, the study of NLI has moved forward rapidly along with the construction of large-scale datasets, such as SNLI (Bowman et al., 2015), the science domain SciTail (Khot et al., 2018) and multi-genre MNLI (Williams et al., 2018), etc. However, some NLI datasets may not be suitable any more for solving downstream NLP problems since they were commonly crowdsourced in isolation from any end task 2 (Khot et al., 2018). In addition, most NLI datasets and studies paid attention merely to sentence-level inference — both the premises and hypotheses are single (and usually short) sentences. This makes them unsuitable for other open-ended NLP problems. For example, to verify the factual correctness of a document summary, sentence-level NLI systems cannot be of much help (Kry´sci´nski et al., 2019). Considering the fact-checkin"
2021.findings-acl.435,P19-1217,0,0.0221376,"NLI +finetune Prior state-of-the-art FEVER binary 50.00 86.64 87.51 88.84 89.44 – SciTail b-MNLI majority 60.33 66.66 ESIM (Chen et al., 2017) 70.60 – De-Att (Parikh et al., 2016) 72.30 – DGEM (Khot et al., 2018) 77.30 – BERT-large 89.71 90.55 Longformer-base 92.23 92.03 RoBERTa-large 95.13 93.95 D OC NLI (pretrain) 78.17 91.13 +finetune 96.04 94.07 Prior state-of-the-art 97.70 – MCTest v160 v500 25.00 25.00 75.41 70.66 82.50 78.66 90.00 85.83 90.83 90.66 80.00 75.50 Table 6: Train on D OC NLI, test on NLP tasks that are out-of-domain and require document-level NLI. SOTA of MCTest comes from (Yu et al., 2019). ous SOTA fact-checking system. We combine “refute” and “not-enough-info” as a single class “not entail”, and rename this data as “FEVERbinary”. We randomly split FEVER-binary by 203,152/8,209/10,000 for train/dev/test respectively.7 MCTest (Richardson et al., 2013). In Related Work, we have introduced MCTest. Briefly, it is a multi-choice QA benchmark in the domain of fictional story. The authors of MCTest released an NLI-version MCTest by combining the question and the positive (resp. negative) answer candidate as a positive (resp. negative) hypothesis. MCTest consists of two subsets. MCTes"
2021.findings-acl.454,D18-1444,0,0.0927742,"ory into a string, ending with a special token, “TL;DR”. Take Figure 1 as an example, the summary sketch is “1 what 2 abstain ’s just one of ... square garden 8 why 9 abstain TL;DR”. We train our model first to generate this summary sketch and then generate the final summary in an autoregressive way. We use TL;DR token to distinguish sketch and final summary during inference time. 2.3 Controllability Due to the success of controllable language modeling (Keskar et al., 2019), the ability to control text summarization in the News domain has gradually been attracting attention (Fan et al., 2018; Liu et al., 2018) The high-level intuition for our solution is that if we can control a generative model only to generate one sentence as output for a partiallyhighlighted input, we can control the number of output sentences by choosing how to highlight the input. We highlight each dialogue split using the special token &lt; hl >. For example, in Figure 1, we generate the first summary sentence for the first segment from turn one to four, and the second and third from turn five to seven and turn eight to nine, respectively (separated by the dashed lines). This way, we can not only gain the summary controllability"
2021.findings-acl.454,P14-1115,0,0.031826,"ffectiveness with reinforcement learning. Recently, Liu and Lapata, 2019 apply BERT on text summarization and propose a general framework for both extractive and abstractive models. Zhang et al., 2019c pre-train hierarchical document encoder for extractive summarization. Lewis et al., 2019 introduces BART, a denoising autoencoder for pretraining sequence-tosequence models. BART significantly outperforms the best previous work in terms of ROUGE metrics. Dialogue Summarization Regarding to the datasets in dialogue summarization, initial abstractive dialogue summarization work (Oya et al., 2014; Mehdad et al., 2014; Banerjee et al., 2015) are conducted on the AMI meeting corpus (McCowan et al., 2005), with only 141 summaries. Goo and Chen, 2018 propose to use the topic descriptions (high-level goals of meetings) in AMI as reference summaries and use dialogue acts as training signals. Pan et al., 2018 build the Dial2Desc dataset by reversing a visual dialogue task, aligning image dialogues with the image caption as a summary. Liu et al., 2019 collect their dataset from the logs in the DiDi customer service center. It is restricted to taskoriented scenario, where one speaker is the user and the other is t"
2021.findings-acl.454,K16-1028,0,0.0543081,"to produce an abridged version of the input text by distilling its most critical information. In particular, abstractive – as opposed to extractive – summarization requires generative models with a high level of semantic understanding, as the output words do not necessarily appear in the source text. While it is more challenging, it gives more flexibility to a summary compared to extractive summarization models (Zhang et al., 2018). Significant research efforts have been focused on summarization of singlespeaker documents such as text documents (Liao et al., 2018), News (Hermann et al., 2015; Nallapati et al., 2016; See et al., 2017) or scientific ∗ Equal contribution. Work mainly done when Linqing Liu was an intern at Salesforce Research. publications (Qazvinian and Radev, 2008; Nikolov et al., 2018). However, dialogue summarization has not received much attention despite the prevalence of dialogues (text messages, email, social media, etc.) and the vast application potential of dialogue summarization systems. Since dialogue language is inherently different from written text, it poses a unique set of challenges (Zechner, 2001): 1) Distributed information across multiple speakers. The most important inf"
2021.findings-acl.454,J81-4005,0,0.601068,"Missing"
2021.findings-acl.454,2020.emnlp-main.66,1,0.806206,"language models have been employed as encoders and decoders since they (Radford et al., 2019; Yang et al., 2019; Dong et al., 2019) have achieved remarkable success across many NLP tasks. For general text summarization, this has also been the case with models such as BART (Lewis et al., 2019) and PEGASUS (Zhang et al., 2019a). However, there are no results reported for self-supervised pretrained language models applied to dialogue summarisation, and people have argued that there is an intrinsic difference of linguistic patterns between human conversations and written text (Wolf et al., 2019b; Wu et al., 2020a; Wu and Xiong, 2020). We would like to answer the question which generative language model is the best base model for dialogue summarization tasks. 2.2 Sketch Construction Conversational data, unlike news or scientific publications, includes lots of non-factual sentences such as chit-chats and greetings. Removing these least critical information in the dialogues could potentially help the model better focus on the main content. Based on this hypothesis, we combine a syntax-driven sentence compression method (Xu and Durrett, 2019) with neural content selection. Another potentially useful attr"
2021.findings-acl.454,2020.findings-emnlp.400,1,0.737408,"language models have been employed as encoders and decoders since they (Radford et al., 2019; Yang et al., 2019; Dong et al., 2019) have achieved remarkable success across many NLP tasks. For general text summarization, this has also been the case with models such as BART (Lewis et al., 2019) and PEGASUS (Zhang et al., 2019a). However, there are no results reported for self-supervised pretrained language models applied to dialogue summarisation, and people have argued that there is an intrinsic difference of linguistic patterns between human conversations and written text (Wolf et al., 2019b; Wu et al., 2020a; Wu and Xiong, 2020). We would like to answer the question which generative language model is the best base model for dialogue summarization tasks. 2.2 Sketch Construction Conversational data, unlike news or scientific publications, includes lots of non-factual sentences such as chit-chats and greetings. Removing these least critical information in the dialogues could potentially help the model better focus on the main content. Based on this hypothesis, we combine a syntax-driven sentence compression method (Xu and Durrett, 2019) with neural content selection. Another potentially useful attr"
2021.findings-acl.454,P19-1078,1,0.850855,"ell-deserved break” and the first summary sentence “Suzanne is at work and is having a break now.” The entire model is trained using cross-entropy loss for the generated tokens. During inference, we first use the trained binary classifier to predict cutting points. Then, we use the predicted segmentation to add highlighting tokens into a dialogue. Finally, after generating multiple summary sentences separately, we concatenate them to be the final summary. 5111 Longest-3* Pointer Generator (See et al., 2017)* Fast Abs RL (Chen and Bansal, 2018)* Transformer (Vaswani et al., 2017)* DynamicConv (Wu et al., 2019b)* DynamicConv + GPT-2 emb* D-HGN (Feng et al., 2020) TGDGA (Zhao et al., 2020) DialoGPT (Zhang et al., 2019d) UniLM (Dong et al., 2019) PEGASUS (Zhang et al., 2019a) BART-xsum (Lewis et al., 2019) BART-xsum + Sketch (Ours) BART-xsum + Ctrl (Ours) CODS (Ours) ROUGE-1 32.46 37.27 41.03 42.37 41.07 45.41 42.03 43.11 39.77 47.85 50.50 51.74 51.79 52.84 52.65 ROUGE-2 10.27 14.42 16.93 18.44 17.11 20.65 18.07 19.15 16.58 24.23 27.23 26.46 26.85 27.35 27.84 ROUGE-L 29.92 34.36 39.05 39.27 37.27 41.45 39.56 40.49 38.42 46.67 49.32 48.72 49.15 50.29 50.79 Table 1: Dialogue summarization ROUGE evaluat"
2021.findings-acl.454,2020.emnlp-main.409,1,0.720469,"ve been employed as encoders and decoders since they (Radford et al., 2019; Yang et al., 2019; Dong et al., 2019) have achieved remarkable success across many NLP tasks. For general text summarization, this has also been the case with models such as BART (Lewis et al., 2019) and PEGASUS (Zhang et al., 2019a). However, there are no results reported for self-supervised pretrained language models applied to dialogue summarisation, and people have argued that there is an intrinsic difference of linguistic patterns between human conversations and written text (Wolf et al., 2019b; Wu et al., 2020a; Wu and Xiong, 2020). We would like to answer the question which generative language model is the best base model for dialogue summarization tasks. 2.2 Sketch Construction Conversational data, unlike news or scientific publications, includes lots of non-factual sentences such as chit-chats and greetings. Removing these least critical information in the dialogues could potentially help the model better focus on the main content. Based on this hypothesis, we combine a syntax-driven sentence compression method (Xu and Durrett, 2019) with neural content selection. Another potentially useful attribute for the conversa"
2021.findings-acl.454,D18-1088,0,0.0137233,"UGE-L score. In addition, we conduct a case study and show competitive human evaluation results and controllability to humanannotated summaries. 1 Introduction Text summarization aims to produce an abridged version of the input text by distilling its most critical information. In particular, abstractive – as opposed to extractive – summarization requires generative models with a high level of semantic understanding, as the output words do not necessarily appear in the source text. While it is more challenging, it gives more flexibility to a summary compared to extractive summarization models (Zhang et al., 2018). Significant research efforts have been focused on summarization of singlespeaker documents such as text documents (Liao et al., 2018), News (Hermann et al., 2015; Nallapati et al., 2016; See et al., 2017) or scientific ∗ Equal contribution. Work mainly done when Linqing Liu was an intern at Salesforce Research. publications (Qazvinian and Radev, 2008; Nikolov et al., 2018). However, dialogue summarization has not received much attention despite the prevalence of dialogues (text messages, email, social media, etc.) and the vast application potential of dialogue summarization systems. Since di"
2021.findings-acl.454,P19-1499,0,0.171025,"sentence dialogue summary Y = {Y1 , . . . , YM } that is suppose to be briefer than the overall dialogue history. 2.1 Generative Pre-trained Language Models As a first, our model needs transform a conversational history input into a dialogue summary. Re5109 cently, self-supervised pretrained language models have been employed as encoders and decoders since they (Radford et al., 2019; Yang et al., 2019; Dong et al., 2019) have achieved remarkable success across many NLP tasks. For general text summarization, this has also been the case with models such as BART (Lewis et al., 2019) and PEGASUS (Zhang et al., 2019a). However, there are no results reported for self-supervised pretrained language models applied to dialogue summarisation, and people have argued that there is an intrinsic difference of linguistic patterns between human conversations and written text (Wolf et al., 2019b; Wu et al., 2020a; Wu and Xiong, 2020). We would like to answer the question which generative language model is the best base model for dialogue summarization tasks. 2.2 Sketch Construction Conversational data, unlike news or scientific publications, includes lots of non-factual sentences such as chit-chats and greetings. Re"
2021.findings-acl.454,2020.coling-main.39,0,0.0671581,"Missing"
2021.findings-acl.454,D19-1053,0,0.038759,"Missing"
2021.findings-emnlp.19,D13-1160,0,0.0563956,"n 4.3 for more details. 4.2 Question Answering Datasets In this section, we describe the dataset and basic setup for experiments. We use four QA datasets that have been most commonly used benchmarks for open-domain QA evaluation (Lee et al., 2019; Karpukhin et al., 2020): Natural Questions (NQ) (Kwiatkowski et al., 2019) consists of questions mined from real Google search queries, for which the answers are spans in Wikipedia documents identified by annotators. TriviaQA (Joshi et al., 2017) contains a set of trivia questions with answers that were originally scraped from the Web. WebQuestions (Berant et al., 2013) consists of questions selected using Google Suggest API, where the answers are entities in Freebase. CuratedTREC (TREC) (Baudiš and Šediv`y, 2015) is a collection of questions from TREC QA tracks as well as various Web sources, intended for open-domain QA from unstructured text. 4.1 4.3 f (q, pi ) = fψ (q, pi ) + λ · fφ (q, dpi ) (3) where dpi is the document pi belongs to and λ is the coefficient controlling the balance. We noticed that the relevance scores of DHR-D and DHR-P are in the close scale for all our experiments. λ ∈ [0.5, 1] is a quite robust choice for desired performance. 4 Expe"
2021.findings-emnlp.19,P17-1171,0,0.018525,"f-the-art approaches on multiple open-domain QA datasets. Our empirical results demonstrate that we achieve comparable or better results in the open-retrieval setting. Extensive ablation studies on various components and strategies are conducted. 2 2.1 Notations and Preliminaries Text Retrieval for Open-Domain QA In open-domain QA, we are given a large corpus (e.g., Wikipedia) C = {d1 , d2 , . . . , dN }, where each document di is formed by a sequence of pas(i) (i) (i) sages, di = {p1 , p2 , . . . , pl }. The task of endto-end open-domain QA can be formulated with a retriever-reader approach (Chen et al., 2017); we first find a passage (or a set of passages) relevant to a given question, and then use a reading comprehension model to actually derive its answer. It is common that we retrieve top-k passages to be examined by the reading step. The retrieval step is crucial, affecting the reading comprehension step. 2.2 Dual Encoder Retrieval Model In the retrieval process, a commonly used approach referred as a dual encoder model (Bromley et al., 1993) consists of a question encoder EQ and a context encoder EP , which encodes the question and the passage to l dimensional vectors, respectively. Unlike sp"
2021.findings-emnlp.19,N19-1423,0,0.167098,"e gold passage. The document title of the second DPR retrieved passage matches most question tokens. Both of the retrieved passages tend to result in a wrong answer. of the retrieval stage is to identify a set of relevant contexts within a diversified large corpus (e.g., Wikipedia). The reader component then consumes the retrieved evidence as input and predicts an answer. In this paper, we focus on improving the efficiency and the effectiveness of the retrieval component, which in turn leads to improved overall answer generation for open-domain QA. Pretrained transformer models, such as BERT (Devlin et al., 2019), are widely used in recent studies on the retriever-reader framework (Asai et al., 2019; Lewis et al., 2020; Guu et al., 2020). To serve as input to the retriever, documents are split into short passages, and in the Dense Passage Retrieval, DPR (Karpukhin et al., 1 Introduction 2020), a dual encoder framework is applied to The goal of open-domain Question Answering encode questions and the split passages separately. (QA) is to answer a question without pre-specified State-of-the-art dense retrievers outperform sparse source domain (Kwiatkowski et al., 2019). One of term-based retrievers, like"
2021.findings-emnlp.19,2021.eacl-main.74,0,0.0446233,"Missing"
2021.findings-emnlp.19,P17-1147,0,0.0257725,"taset. The two columns of Train and Dev denote the original examples in the dataset and the actual questions used. See Section 4.3 for more details. 4.2 Question Answering Datasets In this section, we describe the dataset and basic setup for experiments. We use four QA datasets that have been most commonly used benchmarks for open-domain QA evaluation (Lee et al., 2019; Karpukhin et al., 2020): Natural Questions (NQ) (Kwiatkowski et al., 2019) consists of questions mined from real Google search queries, for which the answers are spans in Wikipedia documents identified by annotators. TriviaQA (Joshi et al., 2017) contains a set of trivia questions with answers that were originally scraped from the Web. WebQuestions (Berant et al., 2013) consists of questions selected using Google Suggest API, where the answers are entities in Freebase. CuratedTREC (TREC) (Baudiš and Šediv`y, 2015) is a collection of questions from TREC QA tracks as well as various Web sources, intended for open-domain QA from unstructured text. 4.1 4.3 f (q, pi ) = fψ (q, pi ) + λ · fφ (q, dpi ) (3) where dpi is the document pi belongs to and λ is the coefficient controlling the balance. We noticed that the relevance scores of DHR-D a"
2021.findings-emnlp.19,Q19-1026,0,0.0798511,"man rights document.… DPR Retrieved Passage, John Peters Humphrey: John Peters Humphrey, OC (April 30, … He is most famous as the author of the first draft of the Universal Declaration of Human Rights.… DPR Retrieved Passage, Drafting of the Universal Declaration of Human Rights: The Universal Declaration of Human Rights … by Drafting Committee … Members of the Commission who contributed significantly to the creation of the Declaration included Canadian John Peters Humphrey of the United Nations Secretariat, Eleanor Roosevelt … Figure 1: An example of distracting passages in Natural Question (Kwiatkowski et al., 2019). The first DPR retrieved passage shares similar semantics with the gold passage. The document title of the second DPR retrieved passage matches most question tokens. Both of the retrieved passages tend to result in a wrong answer. of the retrieval stage is to identify a set of relevant contexts within a diversified large corpus (e.g., Wikipedia). The reader component then consumes the retrieved evidence as input and predicts an answer. In this paper, we focus on improving the efficiency and the effectiveness of the retrieval component, which in turn leads to improved overall answer generation"
2021.findings-emnlp.19,P19-1612,0,0.309669,"nse Passage Retrieval, DPR (Karpukhin et al., 1 Introduction 2020), a dual encoder framework is applied to The goal of open-domain Question Answering encode questions and the split passages separately. (QA) is to answer a question without pre-specified State-of-the-art dense retrievers outperform sparse source domain (Kwiatkowski et al., 2019). One of term-based retrievers, like BM25 (Robertson and the most prevalent architectures in open-domain Zaragoza, 2009), but they suffer from several QA is the retriever-reader approach (Chen et al., weaknesses. First, due to the lack of effective 2017; Lee et al., 2019). Given a question, the task pruning strategy, extracting relevant passages ∗ from a large corpus undergoes an efficiency issue Work was done when the first author was a research intern at Salesforce Research especially in the inference time. Second, given a 188 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 188–200 November 7–11, 2021. ©2021 Association for Computational Linguistics question, many passages may comprehend similar topics with subtle semantic difference. This fact requires the retriever and the reader to encode passages to their accurate semantic re"
2021.findings-emnlp.19,2021.eacl-main.92,0,0.0323377,"nd Sachan et al. (2021) presented the end-to-end supervised training of the reader and retriever. Furthermore, Mao et al. (2020) generated various contexts of a question to enrich the semantics of the questions is beneficial to improve DPR retrieval accuracy. Xiong et al. (2020c) used a pretrained sequence-to-sequence model to generate questionpassage pairs for pretraining and proposed a simple progressive pretraining algorithm to ensure the effective negative samples in each batch. A pretrained sequence-to-sequence model is exploited to create question-passage pairs in the zero-shot setting (Ma et al., 2021). Hierarchical Retrieval. Hierarchical sparse retriever got attention in early 2000s. Levinson and Ellis (1992) proposed a multi-level hierarchical retrieval method in database search of conceptual graphs. In Web search, Cui et al. (2003) developed a structured document retriever which exploits both content and hierarchical structure of documents, and returns document elements with appropriate granularity. Bonab et al. (2019) incorporated hierarchical domain information into information re7 Conclusion trieval models such that the domain specification resolves the ambiguity of questions. Recent"
2021.findings-emnlp.19,2020.acl-tutorials.8,0,0.0237436,"ves and construct a different set of examples in each training iteration. Lu et al. (2020) explored different types of negatives and uses them in both the pre-training and fine-tuning stages. The other direction of recent research works on improving the training strategy in dense retrieval. Rather than using the gold document as distant supervised training of retrieval, Izacard and Grave (2020) leveraged attention score of a reader model to obtain synthetic labels for the retriever. And Sachan et al. (2021) presented the end-to-end supervised training of the reader and retriever. Furthermore, Mao et al. (2020) generated various contexts of a question to enrich the semantics of the questions is beneficial to improve DPR retrieval accuracy. Xiong et al. (2020c) used a pretrained sequence-to-sequence model to generate questionpassage pairs for pretraining and proposed a simple progressive pretraining algorithm to ensure the effective negative samples in each batch. A pretrained sequence-to-sequence model is exploited to create question-passage pairs in the zero-shot setting (Ma et al., 2021). Hierarchical Retrieval. Hierarchical sparse retriever got attention in early 2000s. Levinson and Ellis (1992)"
2021.findings-emnlp.19,D19-1258,0,0.0286352,"erarchical Retrieval. Hierarchical sparse retriever got attention in early 2000s. Levinson and Ellis (1992) proposed a multi-level hierarchical retrieval method in database search of conceptual graphs. In Web search, Cui et al. (2003) developed a structured document retriever which exploits both content and hierarchical structure of documents, and returns document elements with appropriate granularity. Bonab et al. (2019) incorporated hierarchical domain information into information re7 Conclusion trieval models such that the domain specification resolves the ambiguity of questions. Recently, Nie et al. (2019); Asai et al. (2019) proposed a hierar- In this work, we propose Dense Hierarchical Retrieval (DHR) for open-domain QA and demonchical retrieval approach with both paragraph and strate that the hierarchical model provides evident sentence level retrievers to extract supporting facts benefits in terms of accuracy and efficiency. The for the large-scale machine reading task. Dense Retrieval with Pre-trained Encoders. hierarchical information is crucial to associate pasWith the strong embedding-based ability of the pre- sages with documents such that the passage-level trained model, Lee et al. (2"
2021.findings-emnlp.19,2021.acl-long.519,0,0.0399218,"ANCE (Xiong et al., 2020a) used the retrieval model trained in the previous iteration to discover new negatives and construct a different set of examples in each training iteration. Lu et al. (2020) explored different types of negatives and uses them in both the pre-training and fine-tuning stages. The other direction of recent research works on improving the training strategy in dense retrieval. Rather than using the gold document as distant supervised training of retrieval, Izacard and Grave (2020) leveraged attention score of a reader model to obtain synthetic labels for the retriever. And Sachan et al. (2021) presented the end-to-end supervised training of the reader and retriever. Furthermore, Mao et al. (2020) generated various contexts of a question to enrich the semantics of the questions is beneficial to improve DPR retrieval accuracy. Xiong et al. (2020c) used a pretrained sequence-to-sequence model to generate questionpassage pairs for pretraining and proposed a simple progressive pretraining algorithm to ensure the effective negative samples in each batch. A pretrained sequence-to-sequence model is exploited to create question-passage pairs in the zero-shot setting (Ma et al., 2021). Hiera"
2021.naacl-main.220,P18-1033,0,0.0683384,"sterior distribution of q(y|x, d) via massive samples from q(x, y|d). 4 Experiments We show that our generative model can be used to synthesize data in two settings of semantic parsing. We also present an ablation study for our approach. In-Domain Setting We first evaluate our method in the conventional in-domain setting where training and test data are from the same database. Specifically, we synthesize new data for the G EO Q UERY dataset (Zelle and Mooney, 1996) which contains 880 utterance-SQL pairs on the database of U.S. geography. We evaluate in both question and query split, following Finegan-Dollak et al. (2018). The traditional question split ensures that no utterance is repeated between the train and test sets. This only tests limited generalization as many utterances correspond to the same SQL query; query split is introduced to ensure that neither utterances nor SQL queries repeat. The query split tests compositional generalization of a semantic parser as only fragments of test SQL queries occur in the training set. With generated SQL queries at hand, we then show how we map SQLs to utterances to obtain more paired data. We notice that SQL-to-utterance translation, which belongs to the general ta"
2021.naacl-main.220,P19-1444,0,0.0332005,"Missing"
2021.naacl-main.220,D19-1394,0,0.037277,"Missing"
2021.naacl-main.220,P17-1089,0,0.100358,"Missing"
2021.naacl-main.220,P16-1002,0,0.0474668,"ogical forms). Moreover, their grammars are designed specifically for linguistically faithful languages, e.g., logical forms, thus not suitable for programming languages such as SQL. In contrast, our generative model is more flexible and efficient to train due to the two-stage decomposition. 3 In this section, we explain how our method can be applied to text-to-SQL parsing. 3.1 2 Related Work Data Augmentation Data augmentation for semantic parsing has gained increasing attention in recent years. Dong et al. (2017) use backtranslation (Sennrich et al., 2016) to obtain paraphrase of questions. Jia and Liang (2016) induce a high-precision SCFG from training data to generate more new “recombinant” examples. Yu et al. (2018a, 2020) follow the same spirit and use a handcrafted SCFG rule to generate new parallel data. However, the production rules of these approaches usually have low coverage of meaning representations. In this work, instead of using SCFG that accounts for rigid alignments between utterance and programs, we use a two-stage approach that implicitly models the alignments by taking advantage of powerful conditional text generators such 1 Method Problem Definition Formally, the labeled data for"
2021.naacl-main.220,D17-1160,0,0.154627,"Missing"
2021.naacl-main.220,2020.acl-main.703,0,0.0399469,"contrast, a single translation model is trained and shared across source databases. We use RATSQL (Wang et al., 2020b) as our base parser. The size of the synthesized data is always proportional to the size of the original data. We tune the ratio in {1, 3, 6, 12}, and find that 3, 6 works best for G EO Q UERY and S PIDER respectively. We use the RAT-SQL implementation from Wang et al. (2020a) which supports value prediction and evaluation by execution. We train it with the default hyper-parameters. For the SQL-to-utterance translation model, we reuse all the default hyperparameters from BART (Lewis et al., 2020). Both models are trained using NVIDIA V100. Model RAT-SQL♠ (Wang et al., 2020b) RYANSQL♠ (Choi et al., 2020) IRNet♦ (Guo et al., 2019) GAZP (Zhong et al., 2020) BRIDGE♠ (Lin et al., 2020) Base Parser♣ Base Parser♣ + Syn Pre-Train w.o. trained PCFG w.o. pre-trained BART S ET M ATCH E XECUTION 69.7 70.6 61.9 59.1 70.0 70.4 71.8 71.4 70.6 59.2 68.0 69.4 72.5 72.3 70.8 Table 2: Set match and execution accuracies on S PI DER . ♠ stands for models with BERT-large, ♦ for BERT-base, ♣ for Electra-base. DER , we report exact set match (Yu et al., 2018b) along with execution accuracy on the dev set. Th"
2021.naacl-main.220,2020.findings-emnlp.438,1,0.830816,"Missing"
2021.naacl-main.220,D08-1082,0,0.0424914,"n test databases in the context of cross-database semantic parsing. Our work complements GAZP and shows that synthesizing data indirectly in training databases can also be beneficial for cross-database semantic parsing. Crucially, we learn the distribution of SQL programs instead of relying on handcrafted templates as in GAZP. The induced distribution helps a model explore unseen programs, leading to better compositional generalization of a parser. Generative Models In the history of semantic parsing, grammar-based generative models (Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005; Lu et al., 2008) have played an important role. However, learning and inference of such models are usually expensive as they typically require grammar induction (from text to logical forms). Moreover, their grammars are designed specifically for linguistically faithful languages, e.g., logical forms, thus not suitable for programming languages such as SQL. In contrast, our generative model is more flexible and efficient to train due to the two-stage decomposition. 3 In this section, we explain how our method can be applied to text-to-SQL parsing. 3.1 2 Related Work Data Augmentation Data augmentation for sema"
2021.naacl-main.220,P16-1009,0,0.0375218,"s they typically require grammar induction (from text to logical forms). Moreover, their grammars are designed specifically for linguistically faithful languages, e.g., logical forms, thus not suitable for programming languages such as SQL. In contrast, our generative model is more flexible and efficient to train due to the two-stage decomposition. 3 In this section, we explain how our method can be applied to text-to-SQL parsing. 3.1 2 Related Work Data Augmentation Data augmentation for semantic parsing has gained increasing attention in recent years. Dong et al. (2017) use backtranslation (Sennrich et al., 2016) to obtain paraphrase of questions. Jia and Liang (2016) induce a high-precision SCFG from training data to generate more new “recombinant” examples. Yu et al. (2018a, 2020) follow the same spirit and use a handcrafted SCFG rule to generate new parallel data. However, the production rules of these approaches usually have low coverage of meaning representations. In this work, instead of using SCFG that accounts for rigid alignments between utterance and programs, we use a two-stage approach that implicitly models the alignments by taking advantage of powerful conditional text generators such 1"
2021.naacl-main.220,2021.naacl-main.33,1,0.839317,"Missing"
2021.naacl-main.220,2020.acl-main.677,1,0.87316,"= (agg_type agg_id, column col_id) agg_type = NoneAggOp |Max |Min cond = And(cond left, cond right) |Or(cond left, cond right) |Not(cond c) Figure 2: A simplified ASDL grammar for SQL, where “sql, select, cond, agg"" stands for variable types, “where, agg_id"" for variable names, and “And, Or, Not"" for constructor names. where q(y|d) models the distribution of SQLs given a database, and q(x|y, d) models the translation process from SQL to utterances. 3.2 Database-Specific PCFG: q(y|d) We use abstract syntax trees (ASTs) to model the underlying grammar of SQL, following Yin and Neubig (2018) and Wang et al. (2020b). Specifically, we use ASDL (Wang et al., 1997) formalism to define ASTs. To illustrate, Figure 2 shows a simplified ASDL grammar for SQL. The ASDL grammar of SQL can be represented by a set of contextfree grammar (CFG) rules, as elaborated in the Appendix. By assuming the strong independence of each production rule, we model the probability of generating a SQL as the product ofQthe probability of each production rule q(y) = N i= q(Ti ). It is well known that estimating the probability of a production rule via maximum-likelihood training is equivalent to simple counting, which is defined as"
2021.naacl-main.220,N06-1056,0,0.137412,"ong et al., 2020) which synthesizes parallel data directly on test databases in the context of cross-database semantic parsing. Our work complements GAZP and shows that synthesizing data indirectly in training databases can also be beneficial for cross-database semantic parsing. Crucially, we learn the distribution of SQL programs instead of relying on handcrafted templates as in GAZP. The induced distribution helps a model explore unseen programs, leading to better compositional generalization of a parser. Generative Models In the history of semantic parsing, grammar-based generative models (Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2005; Lu et al., 2008) have played an important role. However, learning and inference of such models are usually expensive as they typically require grammar induction (from text to logical forms). Moreover, their grammars are designed specifically for linguistically faithful languages, e.g., logical forms, thus not suitable for programming languages such as SQL. In contrast, our generative model is more flexible and efficient to train due to the two-stage decomposition. 3 In this section, we explain how our method can be applied to text-to-SQL parsing. 3.1 2 Re"
2021.naacl-main.220,P07-1121,0,0.137248,"Missing"
2021.naacl-main.220,D18-2002,0,0.0286691,"select = (agg∗ aggs) agg = (agg_type agg_id, column col_id) agg_type = NoneAggOp |Max |Min cond = And(cond left, cond right) |Or(cond left, cond right) |Not(cond c) Figure 2: A simplified ASDL grammar for SQL, where “sql, select, cond, agg"" stands for variable types, “where, agg_id"" for variable names, and “And, Or, Not"" for constructor names. where q(y|d) models the distribution of SQLs given a database, and q(x|y, d) models the translation process from SQL to utterances. 3.2 Database-Specific PCFG: q(y|d) We use abstract syntax trees (ASTs) to model the underlying grammar of SQL, following Yin and Neubig (2018) and Wang et al. (2020b). Specifically, we use ASDL (Wang et al., 1997) formalism to define ASTs. To illustrate, Figure 2 shows a simplified ASDL grammar for SQL. The ASDL grammar of SQL can be represented by a set of contextfree grammar (CFG) rules, as elaborated in the Appendix. By assuming the strong independence of each production rule, we model the probability of generating a SQL as the product ofQthe probability of each production rule q(y) = N i= q(Ti ). It is well known that estimating the probability of a production rule via maximum-likelihood training is equivalent to simple counting"
2021.naacl-main.220,D18-1193,0,0.542072,"ding of neural parsers (Yin and Neubig, 2018; Krishnamurthy et al., 2017). In this work, we utilize grammars to generate (unseen) programs, which are then used to synthesize more parallel data for semantic parsing. Concretely, we use text-to-SQL as an example task, and propose a generative model to synthesize utterance-SQL pairs. As illustrated in Fig1 Introduction ure 1, we first employ a probabilistic context-free grammar (PCFG) to model the distribution of SQL Recently, synthesizing data for semantic parsing queries. Then with the help of a SQL-to-text transhas gained increasing attention (Yu et al., 2018a, lation model, the corresponding utterances of SQL 2020; Zhong et al., 2020). However, these mod- queries are generated subsequently. Our approach els require handcrafted rules (or templates) to syn- is in the same spirit as back-translation (Sennrich thesize new programs or utterance-program pairs. et al., 2016). The major difference is that the ‘target This can be sub-optimal as fixed rules cannot cap- language’, in our case, is a formal language with ture the underlying distribution of programs which known underlying grammar. Just like the training usually vary across different domains (H"
2021.naacl-main.220,D18-1425,0,0.043536,"Missing"
2021.naacl-main.220,2020.emnlp-main.558,0,0.541366,"In this work, we utilize grammars to generate (unseen) programs, which are then used to synthesize more parallel data for semantic parsing. Concretely, we use text-to-SQL as an example task, and propose a generative model to synthesize utterance-SQL pairs. As illustrated in Fig1 Introduction ure 1, we first employ a probabilistic context-free grammar (PCFG) to model the distribution of SQL Recently, synthesizing data for semantic parsing queries. Then with the help of a SQL-to-text transhas gained increasing attention (Yu et al., 2018a, lation model, the corresponding utterances of SQL 2020; Zhong et al., 2020). However, these mod- queries are generated subsequently. Our approach els require handcrafted rules (or templates) to syn- is in the same spirit as back-translation (Sennrich thesize new programs or utterance-program pairs. et al., 2016). The major difference is that the ‘target This can be sub-optimal as fixed rules cannot cap- language’, in our case, is a formal language with ture the underlying distribution of programs which known underlying grammar. Just like the training usually vary across different domains (Herzig of a semantic parser, the training of the data syntheand Berant, 2019)."
2021.naacl-main.37,D19-1052,0,0.0377301,"Missing"
2021.naacl-main.37,2020.acl-main.708,0,0.0149657,"construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and s"
2021.naacl-main.37,2020.findings-emnlp.190,0,0.0356274,"Missing"
2021.naacl-main.37,2020.acl-main.18,0,0.0285505,"construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and s"
2021.naacl-main.37,P19-1483,0,0.0424379,"Missing"
2021.naacl-main.37,W19-8652,0,0.0305433,"Missing"
2021.naacl-main.37,L18-1544,0,0.0274201,"e structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia tables, which is significantly richer than those defined in the domain specific ontologies E2E and WebNLG were built on (Table 2). We also intro1 Introduction duce a novel tree ontology annotation approach on Automatically generating textual descriptions from tables, which converts a flat table schema into a"
2021.naacl-main.37,D19-1428,0,0.0202842,"generating textual descriptions from tables, which converts a flat table schema into a structured data improves the accessibility of knowl- tree structured semantic frame. The tree ontology edge bases to lay users. Such applications include reflects the core and auxiliary relations in the table explaining data records to non-experts (Cawsey schema, and naturally occurs across many domains. et al., 1997), writing sports news (Chen and As a result, DART provides high-quality sentence Mooney, 2008), summarizing information in mul- annotations to tree structured semantic frames extiple documents (Fan et al., 2019), and generating tracted from various data sources, including Wikdialogue responses (Wen et al., 2015). iSQL (Zhong et al., 2017) and WikiTableQuestions While significant progress has been made in this (Pasupat and Liang, 2015), two open-domain quesfield, there are still several issues with existing tion answering datasets, as well as E2E (Novikova Data-to-Text datasets. First, they adopt a flat ontol- et al., 2017b) and WebNLG (Gardent et al., 2017) ogy structure of the data, such as slot-value pairs (Figure 1). We evaluated several state-of-the-art for data records (Lebret et al., 2016; Novi"
2021.naacl-main.37,W17-3518,0,0.177339,"itates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia tables, which is significantly richer than those defi"
2021.naacl-main.37,2020.acl-main.703,0,0.0345274,"Missing"
2021.naacl-main.37,P09-1011,0,0.106929,"Missing"
2021.naacl-main.37,2020.findings-emnlp.165,0,0.0746934,"Missing"
2021.naacl-main.37,N19-1236,0,0.0269971,"Missing"
2021.naacl-main.37,D17-1238,0,0.0117771,"ges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia ta"
2021.naacl-main.37,P19-1195,0,0.0491104,"Missing"
2021.naacl-main.37,W07-2315,0,0.135635,"Missing"
2021.naacl-main.37,D19-1314,0,0.0450819,"Missing"
2021.naacl-main.37,2020.acl-main.704,0,0.020086,"Missing"
2021.naacl-main.37,W17-5525,0,0.0286406,"ges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia ta"
2021.naacl-main.37,P17-2002,0,0.0678008,"Missing"
2021.naacl-main.37,2020.emnlp-main.89,0,0.0169662,"ork effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the natu"
2021.naacl-main.37,T87-1019,0,0.795136,"Missing"
2021.naacl-main.409,2020.tacl-1.5,0,0.0355149,"Missing"
2021.naacl-main.409,2020.acl-main.703,0,0.0488575,"Missing"
2021.naacl-main.409,2021.ccl-1.108,0,0.0951963,"Missing"
2021.naacl-main.409,D16-1264,0,0.268093,"of-the-art baselines? Hence, we train and evaluate two SCRIPT models “small” and “base” with an encoder of the 14M and 110M parameters, respectively. For a direct comparison, the models are trained on the OpenWebText corpus (Gokaslan and Cohen, 2019) with identical pre-processing and optimization procedures as in (Devlin et al., 2018) and (Clark et al., 2020). We refer to the Appendix for details. 3.1 Transfer to Downstream Tasks We evaluate the efficacy of our method on the GLUE natural language understanding benchmark (Wang et al., 2018) and the SQuAD 1.1 and 2.0 question answering dataset (Rajpurkar et al., 2016a). We report mean scores of GLUE tasks over 8 fine-tuning runs with varying random seed. For the evaluation on SQuAD, we re-trained the “small” models with a sequence length of 512 tokens. Table 1 depicts improved scores across the benchmarks. The task specific GLUE scores are shown in Table 2. GLUE SQuAD 1.1 SQuAD 2.0 Model Mean EM ELECTRA-small SCRIPT-small 80.38 81.32 74.13 81.65 65.91 68.59 74.84 82.43 67.03 69.81 ELECTRA-base SCRIPT-base 85.06 85.76 84.57 90.72 80.86 83.52 85.43 91.56 81.74 84.25 F1 EM F1 Table 1: GLUE and SQuAD dev-set scores for models pret=1 1(t ) log p(t |¯x )]. (6)"
2021.naacl-main.409,D13-1170,0,0.00861209,"Missing"
2021.naacl-main.409,W18-5446,0,0.0309494,"Missing"
2021.naacl-main.409,N18-1101,0,0.040346,"Missing"
2021.nlp4convai-1.2,D15-1075,0,0.121031,"Missing"
2021.nlp4convai-1.2,2020.acl-main.747,0,0.0485477,"Missing"
2021.nlp4convai-1.2,2021.eacl-main.20,0,0.0627791,"Missing"
2021.nlp4convai-1.2,N19-1423,0,0.0225991,"ame as an additional intent class like the other intent labels, or set up a threshold T , if the maximum entailment probability score is over T , we assign the corresponding label as the prediction, otherwise we assign OOS as the prediction for the query utterance. Method Natural Language Inference 3 Natural Language Inference, or NLI, is a fundamental NLP task that aims to identify the relationship between a premise and a hypothesis. The relationship can be binary, (entailment and nonentailment) or ternary (entailment, contradiction, and neutral). Pre-trained transformer models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have achieved promising results on 3.1 Experiments Datasets 3.1.1 General NLI corpus for pre-training To unleash the full potential of transformer model on NLI task, we follow the data processing and training pipeline provided by Zhang et al. (2020) to combine three NLI corpus (SNLI (Bowman 9 Figure 1: An illustration of training data in USLP and DNNC 3.2 et al., 2015), MNLI (Williams et al., 2018), and WNLI (Levesque, 2011)) from the GLUE benchmark (Wang et al., 2018) and use them for NLI pre-training. 3.1.2 We use the nlpaug library (Ma, 2019) for tokenlevel d"
2021.nlp4convai-1.2,D19-1403,0,0.026686,"ging. To our knowledge, however, there is no known work that has explored to leverage semantic labels for NLI-style intent classification. Neither has any work been done to study how model perIntroduction Many methods have been considered for few-shot intent classification. A simple but often effective approach is to simply generate more data through data augmentation. Wei and Zou, 2019 and Kumar et al., 2019 explored data augmentation at token- and feature-level to boost model performance. Meta-learning has also been studied extensively for few-shot learning. For instance, Induction Network (Geng et al., 2019) tried to learn general class representations via episode-based meta training and predict utterance labels based on the relation score between the query utterance and ∗ Work was done when the author was a full time research scientist at Salesforce Research, now works at Google 8 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 8–15 November 10, 2021. ©2021 Association for Computational Linguistics formance changes with regard to the interplay of data augmentation, different labeling, and number of training examples. Based upon DNNC, our proposed met"
2021.nlp4convai-1.2,W18-5446,0,0.0605124,"Missing"
2021.nlp4convai-1.2,2020.acl-main.128,0,0.0365739,"l classification approach by >20 points (in-domain accuracy). We also find that longer and semantically meaningful labels tend to benefit model performance, however, the benefit shrinks as more training data is available. 1 However, the DNNC work ignores one valuable and readily available supervision in the training data, the semantics in the labels. Our work is largely motivated by the hypothesis that semantic labels may carry valuable information about the intents and could benefit few-shot learning. There are prior works exploring how to leverage semantic labels in NLP tasks. For examples, Hou et al. (2020b) has proposed to improve the Prototypical Network (Snell et al., 2017) by directly embedding semantic labels; Hou et al. (2020a) has tried to use semantic information in labels for few-shot slot tagging. To our knowledge, however, there is no known work that has explored to leverage semantic labels for NLI-style intent classification. Neither has any work been done to study how model perIntroduction Many methods have been considered for few-shot intent classification. A simple but often effective approach is to simply generate more data through data augmentation. Wei and Zou, 2019 and Kumar"
2021.nlp4convai-1.2,D19-1670,0,0.0254539,"r examples, Hou et al. (2020b) has proposed to improve the Prototypical Network (Snell et al., 2017) by directly embedding semantic labels; Hou et al. (2020a) has tried to use semantic information in labels for few-shot slot tagging. To our knowledge, however, there is no known work that has explored to leverage semantic labels for NLI-style intent classification. Neither has any work been done to study how model perIntroduction Many methods have been considered for few-shot intent classification. A simple but often effective approach is to simply generate more data through data augmentation. Wei and Zou, 2019 and Kumar et al., 2019 explored data augmentation at token- and feature-level to boost model performance. Meta-learning has also been studied extensively for few-shot learning. For instance, Induction Network (Geng et al., 2019) tried to learn general class representations via episode-based meta training and predict utterance labels based on the relation score between the query utterance and ∗ Work was done when the author was a full time research scientist at Salesforce Research, now works at Google 8 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pag"
2021.nlp4convai-1.2,N18-1101,0,0.0348587,"thesis. The relationship can be binary, (entailment and nonentailment) or ternary (entailment, contradiction, and neutral). Pre-trained transformer models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have achieved promising results on 3.1 Experiments Datasets 3.1.1 General NLI corpus for pre-training To unleash the full potential of transformer model on NLI task, we follow the data processing and training pipeline provided by Zhang et al. (2020) to combine three NLI corpus (SNLI (Bowman 9 Figure 1: An illustration of training data in USLP and DNNC 3.2 et al., 2015), MNLI (Williams et al., 2018), and WNLI (Levesque, 2011)) from the GLUE benchmark (Wang et al., 2018) and use them for NLI pre-training. 3.1.2 We use the nlpaug library (Ma, 2019) for tokenlevel data augmentation. In-domain utterances are augmented 4 times using random insertion, cBERTbased substitution, random swapping, and synonym replacement API. More details about the configurations can be found in Appendix A. CLINC150 CLINC150, introduced by Larson et al. (2019), is a multi-domain dataset for intent classification task. It has three dataset variants for in-domain and outof-scope (OOS). We use the small dataset, which"
2021.nlp4convai-1.2,2020.emnlp-demos.6,0,0.0534707,"Missing"
2021.nlp4convai-1.2,D19-1404,0,0.0178705,"unlabeled datasets and use them to train classifier. Although this approach has been proven to be effective, it requires extra unlabeled data and additional human supervision on description generation task. DNNC (Zhang et al., 2020) reformulates few-shot text classification as NLI-style pairwise comparison between training example and query utterance. However, DNNC requires at least two examples per intent for training and has to make M ×N (M: number of intents; N: number of training examples per intent) pairwise comparisons for each classification. Along the line of NLI-based classification, Yin et al. (2019) explored to leverage short semantic labels. However, this work is limited to zero-short setting and doesn’t provide extensive analysis on how semantic information in labels affects model performance. Zhang et al. (2020) proposed to formulate fewshot intent classification as natural language inference (NLI) between query utterances and examples in the training set. The method is known as discriminative nearest neighbor classification or DNNC. Inspired by this work, we propose to simplify the NLI-style classification pipeline to be the entailment prediction on the utterance-semantic-label-pair"
2021.nlp4convai-1.2,2020.emnlp-main.411,1,0.86532,"rge-scale pre-trained language models are often employed to mitigate the lack of annotated data for the target task. Schick and Schütze, 2021 leveraged pre-trained RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020) to learn to generate task descriptions on small labeled datasets. They then use the trained models to produce descriptions and soft labels on large, task-specific unlabeled datasets and use them to train classifier. Although this approach has been proven to be effective, it requires extra unlabeled data and additional human supervision on description generation task. DNNC (Zhang et al., 2020) reformulates few-shot text classification as NLI-style pairwise comparison between training example and query utterance. However, DNNC requires at least two examples per intent for training and has to make M ×N (M: number of intents; N: number of training examples per intent) pairwise comparisons for each classification. Along the line of NLI-based classification, Yin et al. (2019) explored to leverage short semantic labels. However, this work is limited to zero-short setting and doesn’t provide extensive analysis on how semantic information in labels affects model performance. Zhang et al. ("
D17-1206,D15-1159,0,0.00674333,"= [ht−1 ; ht ; xt ; yt ], where ht is the hidden state of the first (POS) layer. We define (pos) the weighted label embedding yt as follows: (pos) yt = C X j=1 (1) p(yt (1) = j|ht )`(j), (2) (1) where C is the number of the POS tags, p(yt = (1) j|ht ) is the probability value that the j-th POS tag is assigned to wt , and `(j) is the corresponding label embedding. The probability values are predicted by the POS layer, and thus no gold POS tags are needed. This output embedding is similar to the K-best POS tag feature which has been shown to be effective in syntactic tasks (Andor et al., 2016; Alberti et al., 2015). For predicting the chunking tags, we employ the same strategy as POS tagging by using the concatenated bi→ − (2) ← −(2) (2) directional hidden states ht = [ h t ; h t ] in the chunking layer. We also use a single ReLU hidden layer before the softmax classifier. 1924 2.4 Syntactic Task: Dependency Parsing Dependency parsing identifies syntactic relations (such as an adjective modifying a noun) between word pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs of words. The input vector for the LSTM includes hidden states, word representations, and the labe"
D17-1206,P16-1231,0,0.284093,"(2) (1) (pos) (1) gt = [ht−1 ; ht ; xt ; yt ], where ht is the hidden state of the first (POS) layer. We define (pos) the weighted label embedding yt as follows: (pos) yt = C X j=1 (1) p(yt (1) = j|ht )`(j), (2) (1) where C is the number of the POS tags, p(yt = (1) j|ht ) is the probability value that the j-th POS tag is assigned to wt , and `(j) is the corresponding label embedding. The probability values are predicted by the POS layer, and thus no gold POS tags are needed. This output embedding is similar to the K-best POS tag feature which has been shown to be effective in syntactic tasks (Andor et al., 2016; Alberti et al., 2015). For predicting the chunking tags, we employ the same strategy as POS tagging by using the concatenated bi→ − (2) ← −(2) (2) directional hidden states ht = [ h t ; h t ] in the chunking layer. We also use a single ReLU hidden layer before the softmax classifier. 1924 2.4 Syntactic Task: Dependency Parsing Dependency parsing identifies syntactic relations (such as an adjective modifying a noun) between word pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs of words. The input vector for the LSTM includes hidden states, word repres"
D17-1206,C10-1011,0,0.0644419,"Missing"
D17-1206,Q17-1010,0,0.0304414,"encies improve the relatedness task. The relatedness and entailment tasks are closely related to each other. If the semantic relatedness between two sentences is very low, they are unlikely to entail each other. Based on this observation, we make use of the information from the relatedness task for improving the entailment task. 2.1 it = σ (Wi gt + bi ) , ft = σ (Wf gt + bf ) , ut = tanh (Wu gt + bu ) , ct = it ut + ft ct−1 , Word-Level Task: POS Tagging The first layer of the model is a bi-directional LSTM (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) whose hidden states 1 Bojanowski et al. (2017) previously proposed to train the character n-gram embeddings by the Skip-gram objective. (1) ot = σ (Wo gt + bo ) , ht = ot tanh (ct ) , → − where we define the input gt as gt = [ h t−1 ; xt ], i.e. the concatenation of the previous hidden state and the word representation of wt . The backward pass is expanded in the same way, but a different set of weights are used. For predicting the POS tag of wt , we use the concatenation of the forward and backward states in a one-layer bi-LSTM layer corresponding to the → − ← − t-th word: ht = [ h t ; h t ]. Then each ht (1 ≤ t ≤ L) is fed into a standa"
D17-1206,D16-1257,0,0.0160407,"histicated attention mechanism called biaffine attention. It should be promising to incorporate their attention mechanism into our parsing component. Semantic relatedness Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of “JMTDE ” is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al. (2015) explicitly used syntactic trees, and Zhou et al. (2016) relied on attention mechanisms. However, our method uses the simple maxpooling strategy, which suggests that it is worth 4 Choe and Charniak (2016) employed a tri-training method to expand the training data with 400,000 trees in addition to the WSJ data, and they reported 95.9 UAS and 94.1 LAS by converting their constituency trees into dependency trees. Kuncoro et al. (2017) also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter. 1928 A↑ B↑ Single 97.45 95.02 93.35 91.42 0.247 81.8 POS Chunking Dependency UAS Dependency LAS Relatedness Entailment C↑ D↓ E↑ JMTall 97.55 n/a 94.67 92.90 0.233 86.2 JMTAB 97.52 95.77 n/a n/a n/a n/a JMTABC 97.54 n/a 94.71 92.92 n/a n/a JMTDE n/a n/a n/a n/a 0.238 86.8 JMTCD n/a n/a 93.53 91"
D17-1206,P15-1033,0,0.0112343,"Missing"
D17-1206,P96-1011,0,0.0606322,"didates of the par(3) (3) ent node as m (t, j) = ht · (Wd hj ), where Wd is a parameter matrix. For the root, we define (3) hL+1 = r as a parameterized vector. To compute the probability that wj (or the root node) is the parent of wt , the scores are normalized: exp (m (t, j)) (3) p(j|ht ) = PL+1 . k=1,k6=t exp (m (t, k)) (3) The dependency labels are predicted using as input to a softmax classifier with a single ReLU layer. We greedily select the parent node and the dependency label for each word. When the parsing result is not a well-formed tree, we apply the first-order Eisner’s algorithm (Eisner, 1996) to obtain a well-formed tree from it. (3) (3) [ht ; hj ] 2.5 Semantic Task: Semantic relatedness The next two tasks model the semantic relationships between two input sentences. The first task measures the semantic relatedness between two sentences. The output is a real-valued relatedness score for the input sentence pair. The second task is textual entailment, which requires one to determine whether a premise sentence entails a hypothesis sentence. There are typically three classes: entailment, contradiction, and neutral. We use the fourth and fifth bi-LSTM layer for the relatedness and enta"
D17-1206,P16-1078,1,0.760695,"on Sentence1 Sentence2 Figure 1: Overview of the joint many-task model predicting different linguistic outputs at successively deeper layers. different tasks either entirely separately or at the same depth (Collobert et al., 2011). Introduction The potential for leveraging multiple levels of representation has been demonstrated in various ways in the field of Natural Language Processing (NLP). For example, Part-Of-Speech (POS) tags are used for syntactic parsers. The parsers are used to improve higher-level tasks, such as natural language inference (Chen et al., 2016) and machine translation (Eriguchi et al., 2016). These systems are often pipelines and not trained end-to-end. Deep NLP models have yet shown benefits from predicting many increasingly complex tasks each at a successively deeper layer. Existing models often ignore linguistic hierarchies by predicting ∗ Entailment encoder word level semantic level Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model togethe"
D17-1206,D17-1012,1,0.826872,"Missing"
D17-1206,P82-1020,0,0.819591,"Missing"
D17-1206,N01-1025,0,0.0296584,"Missing"
D17-1206,J81-4005,0,0.687478,"Missing"
D17-1206,E17-1117,0,0.0281293,"Missing"
D17-1206,S14-2055,0,0.0131081,"Missing"
D17-1206,D16-1076,0,0.0204543,"Missing"
D17-1206,D15-1176,0,0.0300327,"heir constituency trees into dependency trees. Kuncoro et al. (2017) also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter. 1928 A↑ B↑ Single 97.45 95.02 93.35 91.42 0.247 81.8 POS Chunking Dependency UAS Dependency LAS Relatedness Entailment C↑ D↓ E↑ JMTall 97.55 n/a 94.67 92.90 0.233 86.2 JMTAB 97.52 95.77 n/a n/a n/a n/a JMTABC 97.54 n/a 94.71 92.92 n/a n/a JMTDE n/a n/a n/a n/a 0.238 86.8 JMTCD n/a n/a 93.53 91.62 0.251 n/a JMTCE n/a n/a 93.57 91.69 n/a 82.4 Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better. Method JMTall Ling et al. (2015) Kumar et al. (2016) Ma and Hovy (2016) Søgaard (2011) Collobert et al. (2011) Tsuruoka et al. (2011) Toutanova et al. (2003) Acc. ↑ 97.55 97.78 97.56 97.55 97.50 97.29 97.28 97.27 Method JMTAB Single Søgaard and Goldberg (2016) Suzuki and Isozaki (2008) Collobert et al. (2011) Kudo and Matsumoto (2001) Tsuruoka et al. (2011) Table 3: Chunking results. Table 2: POS tagging results. Method JMTall JMTDE Zhou et al. (2016) Tai et al. (2015) MSE ↓ 0.233 0.238 0.243 0.253 JMTall 97.88 97.59 94.51 92.60 0.236 84.6 w/o SC 97.79 97.08 94.52 92.62 0.698 75.0 w/o LE 97.85 97.40 94.09 92.14 0.261 81.6 LA"
D17-1206,P16-1101,0,0.181132,"Missing"
D17-1206,P16-1105,0,0.122728,"closely-related tasks, such as POS tagging and chunking. However, the number of tasks was limited or they have very similar task settings like word-level tagging, and it was not clear how lower-level tasks could be also improved by combining higher-level tasks. More related to our work, Godwin et al. (2016) also followed Søgaard and Goldberg (2016) to jointly learn POS tagging, chunking, and language modeling, and Zhang and Weiss (2016) have shown that it is effective to jointly learn POS tagging and dependency parsing by sharing internal representations. In the field of relation extraction, Miwa and Bansal (2016) proposed a joint learning model for entity detection and relation extraction. All of them suggest the importance of multi-task learning, and we investigate the potential of handling different types of NLP tasks rather than closely-related ones in a single hierarchical deep model. In the field of computer vision, some transfer and multi-task learning approaches have also been proposed (Li and Hoiem, 2016; Misra et al., 2016). For example, Misra et al. (2016) proposed a multi-task learning model to handle different tasks. However, they assume that each data sample has annotations for the differ"
D17-1206,P11-2009,0,0.0126651,"Missing"
D17-1206,P16-2038,0,0.691838,"ce Research. † Corresponding author. We introduce a Joint Many-Task (JMT) model, outlined in Figure 1, which predicts increasingly complex NLP tasks at successively deeper layers. Unlike traditional pipeline systems, our single JMT model can be trained end-to-end for POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment, by considering linguistic hierarchies. We propose an adaptive training and regularization strategy to grow this model in its depth. With the help of this strategy we avoid catastrophic interference between the tasks. Our model is motivated by Søgaard and Goldberg (2016) who showed that predicting two different tasks is more accurate when performed in different layers than in the same layer (Collobert et al., 2011). Experimental results show that our single model achieves competitive results for all of the five different tasks, demonstrating that us1923 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1923–1933 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing linguistic hierarchies is more important than handling different tasks in the same layer. 2 The Joint Many-Task"
D17-1206,N03-1033,0,0.0461524,"Missing"
D17-1206,W11-0328,1,0.301353,"Missing"
D17-1206,P15-1032,0,0.0320417,"Missing"
D17-1206,Q16-1019,0,0.0649491,"04 92.03 0.765 71.2 POS Chunking Dependency UAS Dependency LAS Table 7: Effectiveness of the Shortcut Connections (SC) and the Label Embeddings (LE). investigating such simple methods before developing complex methods for simple tasks. Currently, our JMT model does not explicitly use the learned dependency structures, and thus the explicit use of the output from the dependency layer should be an interesting direction of future work. Textual entailment Table 6 shows the results of textual entailment, and our JMT model achieves the state-of-the-art result. The previous state-ofthe-art result in Yin et al. (2016) relied on attention mechanisms and dataset-specific data preprocessing and features. Again, our simple maxpooling strategy achieves the state-of-the-art result boosted by the joint training. These results show the importance of jointly handling related tasks. 6.2 UAS ↑ 94.67 93.35 95.74 94.61 94.23 94.10 93.99 93.10 92.88 Table 4: Dependency results. Method JMTall JMTDE Yin et al. (2016) Lai and Hockenmaier (2014) Table 5: Semantic relatedness results. POS Chunking Dependency UAS Dependency LAS Relatedness Entailment Method JMTall Single Dozat and Manning (2017) Andor et al. (2016) Alberti et"
D17-1206,E17-1063,0,0.0843233,"ch as an adjective modifying a noun) between word pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs of words. The input vector for the LSTM includes hidden states, word representations, and the label embeddings for the two previous tasks: (3) (3) (2) (pos) (chk) gt = [ht−1 ; ht ; xt ; (yt + yt )], where we computed the chunking vector in a similar fashion as the POS vector in Eq. (2). We predict the parent node (head) for each word. Then a dependency label is predicted for each child-parent pair. This approach is related to Dozat and Manning (2017) and Zhang et al. (2017), where the main difference is that our model works on a multi-task framework. To predict the parent node of wt , we define a matching function between wt and the candidates of the par(3) (3) ent node as m (t, j) = ht · (Wd hj ), where Wd is a parameter matrix. For the root, we define (3) hL+1 = r as a parameterized vector. To compute the probability that wj (or the root node) is the parent of wt , the scores are normalized: exp (m (t, j)) (3) p(j|ht ) = PL+1 . k=1,k6=t exp (m (t, k)) (3) The dependency labels are predicted using as input to a softmax classifier with a single ReLU layer. We gr"
D17-1206,P16-1147,0,0.107798,"Missing"
D17-1206,C16-1274,0,0.0956201,"uracy without the POS and chunking information. The best result to date has been achieved by the model propsoed in Dozat and Manning (2017), which uses higher dimensional representations than ours and proposes a more sophisticated attention mechanism called biaffine attention. It should be promising to incorporate their attention mechanism into our parsing component. Semantic relatedness Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of “JMTDE ” is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al. (2015) explicitly used syntactic trees, and Zhou et al. (2016) relied on attention mechanisms. However, our method uses the simple maxpooling strategy, which suggests that it is worth 4 Choe and Charniak (2016) employed a tri-training method to expand the training data with 400,000 trees in addition to the WSJ data, and they reported 95.9 UAS and 94.1 LAS by converting their constituency trees into dependency trees. Kuncoro et al. (2017) also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter. 1928 A↑ B↑ Single 97.45 95.02 93.35 91.42 0.247 81.8 POS Chunking De"
D17-1206,P08-1076,0,0.0292929,"Missing"
D17-1206,P15-1150,1,0.333124,"Missing"
D18-1207,N15-1014,0,0.0325782,"each evaluation criterion are reported in Table 4. These results show that our model matches the relevance score of See et al. (2017) and Liu et al. (2018), but is slightly inferior to them in terms of readability. 5 Related work Text summarization. Existing summarization approaches are usually either extractive or abstractive. In extractive summarization, the model selects passages from the input document and combines them to form a shorter summary, sometimes with a post-processing step to ensure final coherence of the output (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015; Nallapati et al., 2017). While extractive models are usually robust and produce coherent summaries, they cannot create concise summaries that paraphrase the source document using new phrases. Abstractive summarization allows the model to paraphrase the source document and create concise summaries with phrases not in the source document. The state-of-the-art abstractive summarization models are based on sequence-tosequence models with attention (Bahdanau et al., 2015). Extensions to this model include a selfattention mechanism (Paulus et al., 2017) and an article coverage vector (See et al.,"
D18-1207,W03-0501,0,0.172857,"porates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document. 1 Introduction Text summarization concerns the task of compressing a long sequence of text into a more concise form. The two most common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017), where the model extracts salient parts of the source document, and abstractive (Paulus et al., 2017; See et al., 2017), where the model not only extracts but also concisely paraphrases the important parts of the document via generation. We focus on developing a summarization model that produces an increased level of abstraction. That is, the model produces concise summaries without only copying long passages from the source document. ∗ Work performed while at Salesforce Research. A high quality summary is shorter than the original document, conveys only the most impo"
D18-1207,K16-1028,0,0.192942,"Missing"
D18-1207,P16-1188,0,0.0660358,"and Cieri, 2003) and some DUC datasets (Over et al., 2007) have been used for headline generation models (Rush et al., 2015; Nallapati et al., 2016), where the generated summary is shorter than 75 characters. However, generating longer summaries is a more challenging task, especially for abstractive models. Nallapati et al. (2016) have proposed using the CNN/Daily Mail dataset (Hermann et al., 2015) to train models for generating longer, multi-sentence summaries up to 100 words. The New York Times dataset (Sandhaus, 2008) has also been used as a benchmark for the generation of long summaries (Durrett et al., 2016; Paulus et al., 2017). Training strategies for sequential models. The common approach to training models for sequence generation is maximum likelihood estimation with teacher forcing. At each time step, the model is given the previous ground-truth output and predicts the current output. The sequence objective is the accumulation of cross entropy losses from each time step. Despite its popularity, this approach for sequence generation is suboptimal due to exposure bias (Huszar, 2015) and loss-evaluation mismatch (Wiseman and Rush, 2016). Goyal et al. (2016) propose one way to reduce exposure b"
D18-1207,D13-1155,0,0.104206,"Missing"
D18-1207,P82-1020,0,0.814979,"Missing"
D18-1207,W04-1013,0,0.116623,". We focus on developing a summarization model that produces an increased level of abstraction. That is, the model produces concise summaries without only copying long passages from the source document. ∗ Work performed while at Salesforce Research. A high quality summary is shorter than the original document, conveys only the most important and no extraneous information, and is semantically and syntactically correct. Because it is difficult to gauge the correctness of the summary, evaluation metrics for summarization models use word overlap with the ground-truth summary in the form of ROUGE (Lin, 2004) scores. However, word overlap metrics do not capture the abstractive nature of high quality human-written summaries: the use of paraphrases with words that do not necessarily appear in the source document. The state-of-the-art abstractive text summarization models have high word overlap performance, however they tend to copy long passages of the source document directly into the summary, thereby producing summaries that are not abstractive (See et al., 2017). We propose two general extensions to summarization models that improve the level of abstraction of the summary while preserving word ov"
D18-1207,N18-2102,0,0.0978431,"ts are omitted in cases where they have not been made available by previous authors. We also include the novel n-gram scores for the ground-truth summaries as a comparison to indicate the level of abstraction of human written summaries. 1812 Model R-1 R-2 R-L NN-1 NN-2 NN-3 NN-4 anonymized Ground-truth summaries ML+RL, intra-attn (Paulus et al., 2017) 39.87 15.82 36.9 14.40 1.04 52.07 10.86 71.63 21.53 80.84 29.27 ML+RL ROUGE+Novel, with LM (ours) 40.02 15.53 37.44 3.54 21.91 37.48 47.13 full-text Ground-truth summaries Pointer-gen + coverage (See et al., 2017) SumGAN (Liu et al., 2018) RSal (Pasunuru and Bansal, 2018) RSal+Ent RL (Pasunuru and Bansal, 2018) 39.53 39.92 40.36 40.43 17.28 17.65 17.97 18.00 36.38 36.71 37.00 37.10 13.55 0.07 0.22 - 49.97 2.24 3.15 2.37 - 70.32 6.03 7.68 6.00 - 80.02 9.72 11.84 9.50 - ML+RL ROUGE+Novel, with LM (ours) 40.19 17.38 37.52 3.25 17.21 30.46 39.47 Table 2: Comparison of ROUGE (R-) and novel n-gram (NN-) test results for our model and other abstractive summarization models on the CNN/Daily Mail dataset. Even though our model outputs significantly fewer novel n-grams than human written summaries, it has a much higher percentage of novel n-grams than all the previous a"
D18-1207,D15-1044,0,0.386242,"Missing"
D18-1207,P17-1099,0,0.815631,"urage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document. 1 Introduction Text summarization concerns the task of compressing a long sequence of text into a more concise form. The two most common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017), where the model extracts salient parts of the source document, and abstractive (Paulus et al., 2017; See et al., 2017), where the model not only extracts but also concisely paraphrases the important parts of the document via generation. We focus on developing a summarization model that produces an increased level of abstraction. That is, the model produces concise summaries without only copying long passages from the source document. ∗ Work performed while at Salesforce Research. A high quality summary is shorter than the original document, conveys only the most important and no extraneous information, and is semantically and syntactically correct. Because it is difficult to gauge the correctness of the summa"
D18-1207,D16-1137,0,0.0329756,"en used as a benchmark for the generation of long summaries (Durrett et al., 2016; Paulus et al., 2017). Training strategies for sequential models. The common approach to training models for sequence generation is maximum likelihood estimation with teacher forcing. At each time step, the model is given the previous ground-truth output and predicts the current output. The sequence objective is the accumulation of cross entropy losses from each time step. Despite its popularity, this approach for sequence generation is suboptimal due to exposure bias (Huszar, 2015) and loss-evaluation mismatch (Wiseman and Rush, 2016). Goyal et al. (2016) propose one way to reduce exposure bias by explicitly forcing the hidden representations of the model to be similar during training and inference. Bengio et al. (2015) and Wiseman and Rush (2016) propose an alternate method that exposes the network to the test dynamics during training. Reinforcement learning methods (Sutton and Barto, 1998), such as policy learning (Sutton et al., 1999), mitigate the mismatch between the optimization objective and the evaluation metrics by directly optimizing evaluation metrics. This approach has led to consistent improvements in domains"
D18-1362,D13-1160,0,0.0812083,"existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models. 1 belong_to? U.S. Government collaborate _with collaborate_with Barack_Obama John_McCain endorsed_by born_in Hawaii collaborate_with? locate_in U.S. live_in live_in collaborate _with Hillary_Clinton Figure 1: Example of an incomplete knowledge graph which contains missing links (dashed lines) that can possibly be inferred from existing facts (solid lines). Introduction Large-scale knowledge graphs (KGs) support a variety of downstream NLP applications such as semantic search (Berant et al., 2013) and dialogue generation (He et al., 2017). Whether curated automatically or manually, practical KGs often fail to include many relevant facts. A popular approach for modeling incomplete KGs is knowledge graph embeddings, which map both entities and relations in the KG to a vector space and learn a truth value function for any potential KG triple parameterized by the entity and relation vectors (Yang et al., 2014; Dettmers et al., 2018). Embedding based approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks. An alternative"
D18-1362,N18-1165,0,0.486888,"asoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths. 3243 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3243–3253 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Figure 2: Percentage of false negatives hit (where t"
D18-1362,D13-1080,0,0.103185,"d approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks. An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from"
D18-1362,D15-1038,0,0.0725629,"Missing"
D18-1362,P17-1097,0,0.405676,"use practical KGs are intrinsically incomplete, the agent may arrive at a correct answer whose link to the source entity is missing from the training graph without receiving any reward (false negative targets, Figure 2). Second, since no ground truth path is available for training, the agent may traverse spurious paths that lead to a correct answer only incidentally (false positive paths). Because REINFORCE (Williams, 1992) is an on-policy RL algorithm (Sutton and Barto, 1998) which encourages past actions with high reward, it can bias the policy toward spurious paths found early in training (Guu et al., 2017). We propose two modeling advances for RL approaches in the walk-based QA framework to address the aforementioned problems. First, instead of using a binary reward based on whether the agent has reached a correct answer or not, we adopt pre-trained state-of-the-art embeddingbased models (Dettmers et al., 2018; Trouillon et al., 2016) to estimate a soft reward for target entities whose correctness cannot be determined. As embedding-based models capture link semantics well, unobserved but correct answers would receive a higher reward score compared to a true negative entity using a well-trained"
D18-1362,P17-1162,0,0.0361183,"chmark datasets and is comparable or better than embedding-based models. 1 belong_to? U.S. Government collaborate _with collaborate_with Barack_Obama John_McCain endorsed_by born_in Hawaii collaborate_with? locate_in U.S. live_in live_in collaborate _with Hillary_Clinton Figure 1: Example of an incomplete knowledge graph which contains missing links (dashed lines) that can possibly be inferred from existing facts (solid lines). Introduction Large-scale knowledge graphs (KGs) support a variety of downstream NLP applications such as semantic search (Berant et al., 2013) and dialogue generation (He et al., 2017). Whether curated automatically or manually, practical KGs often fail to include many relevant facts. A popular approach for modeling incomplete KGs is knowledge graph embeddings, which map both entities and relations in the KG to a vector space and learn a truth value function for any potential KG triple parameterized by the entity and relation vectors (Yang et al., 2014; Dettmers et al., 2018). Embedding based approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks. An alternative solution for KG reasoning is to infer mis"
D18-1362,D11-1049,0,0.34106,"., 2018). Embedding based approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks. An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searc"
D18-1362,D12-1093,0,0.0602325,"Missing"
D18-1362,E17-1013,0,0.0633859,"asoning tasks. An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths. 3243 Proceedings of the 2"
D18-1362,D15-1174,0,0.530458,"Missing"
D18-1362,P16-1136,0,0.0519733,"ation in more complex reasoning tasks. An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths. 3"
D18-1362,D17-1060,0,\N,Missing
D18-1362,D17-1260,0,\N,Missing
D18-1362,W16-0106,0,\N,Missing
D19-1051,D13-1155,0,0.0408225,"cal and semantic matching by applying graph analysis algorithms to the WordNet semantic network. Despite being a step in the direction of a more comprehensive evaluation protocol, none of these metrics gained sufficient traction in the research community, leaving ROUGE as the default automatic evaluation toolkit for text summarization. 2.3 Models Existing summarization models fall into three categories: abstractive, extractive, and hybrid. Extractive models select spans of text from the input and copy them directly into the summary. Non-neural approaches (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015) utilized domain expertise to develop heuristics for summary content selection, whereas more recent, neural techniques allow for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et"
D19-1051,W07-0718,0,0.117626,"Missing"
D19-1051,D18-1443,0,0.235404,"Missing"
D19-1051,E06-1032,0,0.112419,"ROUGE variants for evaluating summarization outputs. The study reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinha"
D19-1051,P16-1223,0,0.0245637,"elations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and training strategies that improve the state of the art on benchmark problems."
D19-1051,P18-2103,0,0.0165628,"as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and training strategies that improve the state of the art on benchmark problems. However, it is also important to analyze and question the current methods and research settings. Zhang et al. (2018) conduc"
D19-1051,P18-1063,0,0.309801,"Richard Socher Salesforce Research {kryscinski,nkeskar,bmccann,cxiong,rsocher}@salesforce.com Abstract models. Current approaches to text summarization utilize advanced attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018), multi-task and multi-reward training techniques (Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), reinforcement learning strategies (Paulus et al., 2017; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018), and hybrid extractive-abstractive models (Liu et al., 2018; Hsu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018). Many of the introduced models are trained on the CNN/DailyMail (Nallapati et al., 2016a) news corpus, a popular benchmark for the field, and are evaluated based on n-gram overlap between the generated and target summaries with the ROUGE package (Lin, 2004). Despite substantial research effort, the progress on these benchmarks has stagnated. State-of-theart models only slightly outperform the Lead-3 baseline, which generates summaries by extracting the first three sentences of the source document. We argue that this stagnation can be partially attributed to the current research setup, which i"
D19-1051,N16-1012,0,0.035441,"ge-scale datasets have been proposed. The majority of 540 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 540–551, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong"
D19-1051,D15-1013,0,0.267066,"Bansal, 2018; Kry´sci´nski et al., 2018), and unsupervised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested readers to the original papers. 2.4 ever, summary-level rankings and automatic metric correlations benefit from improving annotator consistency. Graham (2015) compare the fitness of the BLEU metric (Papineni et al., 2002) and a number of different ROUGE variants for evaluating summarization outputs. The study reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the"
D19-1051,N18-2097,0,0.0279557,"source docEvaluation Metrics Manual and semi-automatic (Nenkova and Passonneau, 2004; Passonneau et al., 2013) evaluation of large-scale summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward training (Paulus et al., 2017; Jiang and Bansal, 2018; Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), and unsupervised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested"
D19-1051,N18-1065,0,0.395823,"udy reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Anal"
D19-1051,N15-1014,0,0.0496827,"y applying graph analysis algorithms to the WordNet semantic network. Despite being a step in the direction of a more comprehensive evaluation protocol, none of these metrics gained sufficient traction in the research community, leaving ROUGE as the default automatic evaluation toolkit for text summarization. 2.3 Models Existing summarization models fall into three categories: abstractive, extractive, and hybrid. Extractive models select spans of text from the input and copy them directly into the summary. Non-neural approaches (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015) utilized domain expertise to develop heuristics for summary content selection, whereas more recent, neural techniques allow for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 201"
D19-1051,P18-1064,0,0.0405743,"le summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward training (Paulus et al., 2017; Jiang and Bansal, 2018; Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), and unsupervised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested readers to the original papers. 2.4 ever, summary-level rankings and automatic metric correlations benefit from improving annotato"
D19-1051,D18-1409,0,0.060945,"Missing"
D19-1051,N18-2017,0,0.0195151,"(2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and training strategies that improve the state of the art on benchmark problems. However, it is also important to analyze and question the current methods and"
D19-1051,W03-0501,0,0.343353,"cally collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs. 1 Introduction Text summarization aims at compressing long textual documents into a short, human readable form that contains the most important information from the source. Two strategies of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where salient fragments of the source document are identified and directly copied into the summary, and abstractive (Rush et al., 2015; See et al., 2017), where the salient parts are detected and paraphrased to form the final output. The number of summarization models introduced every year has been increasing rapidly. Advancements in neural network architectures (Sutskever et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Vaswani et al., 2017) and the availability of large scale data (Sandhaus, 2008; Nallapati et al., 2016a; Grusky et al., 2018) enabled the"
D19-1051,W04-1013,0,0.736525,"k and multi-reward training techniques (Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), reinforcement learning strategies (Paulus et al., 2017; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018), and hybrid extractive-abstractive models (Liu et al., 2018; Hsu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018). Many of the introduced models are trained on the CNN/DailyMail (Nallapati et al., 2016a) news corpus, a popular benchmark for the field, and are evaluated based on n-gram overlap between the generated and target summaries with the ROUGE package (Lin, 2004). Despite substantial research effort, the progress on these benchmarks has stagnated. State-of-theart models only slightly outperform the Lead-3 baseline, which generates summaries by extracting the first three sentences of the source document. We argue that this stagnation can be partially attributed to the current research setup, which involves uncurated, automatically collected datasets and non-informative evaluations protocols. We critically evaluate our hypothesis, and support our claims by analyzing three key components of the experimental setting: datasets, evaluation metrics, and mode"
D19-1051,E14-1075,0,0.0395032,"Processing, pages 540–551, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collect"
D19-1051,P18-1013,0,0.235234,"rish Keskar, Bryan McCann, Caiming Xiong, Richard Socher Salesforce Research {kryscinski,nkeskar,bmccann,cxiong,rsocher}@salesforce.com Abstract models. Current approaches to text summarization utilize advanced attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018), multi-task and multi-reward training techniques (Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), reinforcement learning strategies (Paulus et al., 2017; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018), and hybrid extractive-abstractive models (Liu et al., 2018; Hsu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018). Many of the introduced models are trained on the CNN/DailyMail (Nallapati et al., 2016a) news corpus, a popular benchmark for the field, and are evaluated based on n-gram overlap between the generated and target summaries with the ROUGE package (Lin, 2004). Despite substantial research effort, the progress on these benchmarks has stagnated. State-of-theart models only slightly outperform the Lead-3 baseline, which generates summaries by extracting the first three sentences of the source document. We argue that this stagnation can be partially at"
D19-1051,D18-1208,0,0.135518,"Missing"
D19-1051,K16-1028,0,0.212514,"Missing"
D19-1051,D18-1206,0,0.480626,"mmarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles associated with one, singlesentence summary targeted at abstractive models. Newsroom (Grusky et al., 2018) is a diverse collection of articles sourced from 38 major online news outlets. This dataset was released together with a leaderboard and held-out testing split. Outside of the news domain, several datasets were collected from open discussion boards and other portals offering structure information. Reddit TIFU (Kim et al., 2018) is a collection of posts scraped from Reddit where users post their daily stories and each post is required to contain a Too Long; D"
D19-1051,D18-1207,1,0.910544,"Missing"
D19-1051,N18-1158,0,0.149252,"mmarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles associated with one, singlesentence summary targeted at abstractive models. Newsroom (Grusky et al., 2018) is a diverse collection of articles sourced from 38 major online news outlets. This dataset was released together with a leaderboard and held-out testing split. Outside of the news domain, several datasets were collected from open discussion boards and other portals offering structure information. Reddit TIFU (Kim et al., 2018) is a collection of posts scraped from Reddit where users post their daily stories and each post is required to contain a Too Long; D"
D19-1051,W16-3617,0,0.0129595,"51, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles as"
D19-1051,N04-1019,0,0.693315,"cent, neural techniques allow for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018) . Abstractive models paraphrase the source docEvaluation Metrics Manual and semi-automatic (Nenkova and Passonneau, 2004; Passonneau et al., 2013) evaluation of large-scale summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward train"
D19-1051,D18-1441,0,0.0736567,"Missing"
D19-1051,J18-3002,0,0.0166853,"commendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and traini"
D19-1051,D15-1222,0,0.0820131,"el, and health. 2.2 lap between candidate and reference summaries. Overlap can be computed between consecutive (n-grams) and non-consecutive (skip-grams) subsequences of tokens. ROUGE scores are based on exact token matches, meaning that computing overlap between synonymous phrases is not supported. Many approaches have extended ROUGE with support for synonyms and paraphrasing. ParaEval (Zhou et al., 2006) uses a three-step comparison strategy, where the first two steps perform optimal and greedy paraphrase matching based on paraphrase tables before reverting to exact token overlap. ROUGE-WE (Ng and Abrecht, 2015) replaces exact lexical matches with a soft semantic similarity measure approximated with the cosine distances between distributed representations of tokens. ROUGE 2.0 (Ganesan, 2018) leverages synonym dictionaries, such as WordNet, and considers all synonyms of matched words when computing token overlap. ROUGE-G (ShafieiBavani et al., 2018) combines lexical and semantic matching by applying graph analysis algorithms to the WordNet semantic network. Despite being a step in the direction of a more comprehensive evaluation protocol, none of these metrics gained sufficient traction in the researc"
D19-1051,J09-4008,0,0.0403482,"om the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architect"
D19-1051,N15-1166,0,0.0150113,"onal Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles associated with one, singlesentence summary targeted at abstractive models. Newsro"
D19-1051,D15-1044,0,0.461502,"orrelated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs. 1 Introduction Text summarization aims at compressing long textual documents into a short, human readable form that contains the most important information from the source. Two strategies of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where salient fragments of the source document are identified and directly copied into the summary, and abstractive (Rush et al., 2015; See et al., 2017), where the salient parts are detected and paraphrased to form the final output. The number of summarization models introduced every year has been increasing rapidly. Advancements in neural network architectures (Sutskever et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Vaswani et al., 2017) and the availability of large scale data (Sandhaus, 2008; Nallapati et al., 2016a; Grusky et al., 2018) enabled the transition from systems based on expert knowledge and heuristics to data-driven approaches powered by end-to-end deep neural 2 2.1 Related Work Datasets To accom"
D19-1051,P12-2070,0,0.0927104,"ed dimension. Kedzie et al. (2018) offered a thorough analysis of how neural models perform content selection across different data domains, and exposed data biases that dominate the learning signal in the news domain and architectural limitations of current approaches in learning robust sentence-level representations. Liu and Liu (2010) examine the correlation between ROUGE scores and human judgments when evaluating meeting summarization data and show that the correlation strength is low, but can be improved by leveraging unique meeting characteristics, such as available speaker information. Owczarzak et al. (2012) inspect how inconsistencies in human annotator judgments affect the ranking of summaries and correlations with automatic evaluation metrics. The results showed that systemlevel rankings, considering all summaries, were stable despite inconsistencies in judgments, how3 3.1 Datasets Underconstrained task The task of summarization is to compress long documents by identifying and extracting the most important information from the source documents. However, assessing the importance of information is a difficult task in itself, that highly depends on the expectations and prior knowledge of the targ"
D19-1051,P17-1099,0,0.897715,"n judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs. 1 Introduction Text summarization aims at compressing long textual documents into a short, human readable form that contains the most important information from the source. Two strategies of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where salient fragments of the source document are identified and directly copied into the summary, and abstractive (Rush et al., 2015; See et al., 2017), where the salient parts are detected and paraphrased to form the final output. The number of summarization models introduced every year has been increasing rapidly. Advancements in neural network architectures (Sutskever et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Vaswani et al., 2017) and the availability of large scale data (Sandhaus, 2008; Nallapati et al., 2016a; Grusky et al., 2018) enabled the transition from systems based on expert knowledge and heuristics to data-driven approaches powered by end-to-end deep neural 2 2.1 Related Work Datasets To accommodate the requirem"
D19-1051,P02-1040,0,0.106359,"ised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested readers to the original papers. 2.4 ever, summary-level rankings and automatic metric correlations benefit from improving annotator consistency. Graham (2015) compare the fitness of the BLEU metric (Papineni et al., 2002) and a number of different ROUGE variants for evaluating summarization outputs. The study reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or"
D19-1051,D18-1085,0,0.0891176,"with support for synonyms and paraphrasing. ParaEval (Zhou et al., 2006) uses a three-step comparison strategy, where the first two steps perform optimal and greedy paraphrase matching based on paraphrase tables before reverting to exact token overlap. ROUGE-WE (Ng and Abrecht, 2015) replaces exact lexical matches with a soft semantic similarity measure approximated with the cosine distances between distributed representations of tokens. ROUGE 2.0 (Ganesan, 2018) leverages synonym dictionaries, such as WordNet, and considers all synonyms of matched words when computing token overlap. ROUGE-G (ShafieiBavani et al., 2018) combines lexical and semantic matching by applying graph analysis algorithms to the WordNet semantic network. Despite being a step in the direction of a more comprehensive evaluation protocol, none of these metrics gained sufficient traction in the research community, leaving ROUGE as the default automatic evaluation toolkit for text summarization. 2.3 Models Existing summarization models fall into three categories: abstractive, extractive, and hybrid. Extractive models select spans of text from the input and copy them directly into the summary. Non-neural approaches (Neto et al., 2002; Dorr"
D19-1051,P13-2026,0,0.0314186,"for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018) . Abstractive models paraphrase the source docEvaluation Metrics Manual and semi-automatic (Nenkova and Passonneau, 2004; Passonneau et al., 2013) evaluation of large-scale summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward training (Paulus et al., 2017;"
D19-1051,D16-1112,0,0.0218343,"2.1 Related Work Datasets To accommodate the requirements of modern data-driven approaches, several large-scale datasets have been proposed. The majority of 540 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 540–551, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for"
D19-1051,N18-2102,0,0.0260122,"odels is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward training (Paulus et al., 2017; Jiang and Bansal, 2018; Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), and unsupervised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested readers to the original papers. 2.4 ever, summary-level rankings and automatic metric correlations benefit from improving annotator consistency. Graham (2015"
D19-1051,P17-1108,0,0.043788,"Missing"
D19-1051,S18-2023,0,0.0542373,"Missing"
D19-1051,W15-5009,0,0.0587322,"Missing"
D19-1051,W18-6319,0,0.0134174,"he study reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Anal"
D19-1051,D19-1324,0,0.0183643,"xtractive, and hybrid. Extractive models select spans of text from the input and copy them directly into the summary. Non-neural approaches (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015) utilized domain expertise to develop heuristics for summary content selection, whereas more recent, neural techniques allow for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018) . Abstractive models paraphrase the source docEvaluation Metrics Manual and semi-automatic (Nenkova and Passonneau, 2004; Passonneau et al., 2013) evaluation of large-scale summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic"
D19-1051,D18-1089,0,0.262614,"8); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and training strategies that improve the state of the art on benchmark problems. However, it is also important to analyze and question the current methods and research settings. Zhang et al. (2018) conducted a quantitative study of the level of abstraction in abstractive summarization models and showed that wordlevel, copy-only extractive models achieve comparable results to fully abstractive models in the measured dimension. Kedzie et al. (2018) offered a thorough analysis of how neural models perform content selection across different data domains, and exposed data biases that dominate the learning signal in the news domain and architectural limitations of current approaches in learning robust sentence-level representations. Liu and Liu (2010) examine the correlation between ROUGE sco"
D19-1051,N06-1057,0,0.060132,"g, 2018) is a collection of articles from the WikiHow knowledge base, where each article contains instructions for performing procedural, multi-step tasks covering various areas, including: arts, finance, travel, and health. 2.2 lap between candidate and reference summaries. Overlap can be computed between consecutive (n-grams) and non-consecutive (skip-grams) subsequences of tokens. ROUGE scores are based on exact token matches, meaning that computing overlap between synonymous phrases is not supported. Many approaches have extended ROUGE with support for synonyms and paraphrasing. ParaEval (Zhou et al., 2006) uses a three-step comparison strategy, where the first two steps perform optimal and greedy paraphrase matching based on paraphrase tables before reverting to exact token overlap. ROUGE-WE (Ng and Abrecht, 2015) replaces exact lexical matches with a soft semantic similarity measure approximated with the cosine distances between distributed representations of tokens. ROUGE 2.0 (Ganesan, 2018) leverages synonym dictionaries, such as WordNet, and considers all synonyms of matched words when computing token overlap. ROUGE-G (ShafieiBavani et al., 2018) combines lexical and semantic matching by ap"
D19-1051,P18-1061,0,0.0247346,"ection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles associated with one, singlesentence summary targeted at abstractive models. Newsroom (Grusky et al., 2018) is a diverse collection of articles sourced from 38 major online news outlets. This dataset was released together with a leaderboard and held-out testing split. Outside of the news domain, several datasets were collected from open discussion boards and other portals offering structure information. Reddit TIFU (Kim et"
D19-1051,W15-4708,0,\N,Missing
D19-1051,D18-1440,0,\N,Missing
D19-1051,N19-1260,0,\N,Missing
D19-1157,D18-1168,0,0.120669,"erest is segmented from long, untrimmed videos. These methods only identify actions from a pre-defined set of categories, which limits their application to situations where only unconstrained language descriptions are available. This more general problem is referred to as natural language localization (NLL) (Hendricks et al., 2017; Gao et al., 2017a). The goal is to retrieve a temporal segment from an untrimmed video based on an arbitrary text query. Recent work focuses on learning the mapping from visual segments to the input text (Hendricks et al., 2017; Gao et al., 2017a; Liu et al., 2018; Hendricks et al., 2018; Zhang et al., ∗ Work done when the author was at Salesforce Research. † Corresponding author. 2018) and retrieving segments based on the alignment scores. However, in order to successfully train a NLL model, a large number of diverse language descriptions are needed to describe different temporal segments of videos which incurs high human labeling cost. We propose Weakly Supervised Language Localization Networks (WSLLN) which requires only video-sentence pairs during training with no information of where the activities temporally occur. Intuitively, it is much easier to annotate videolevel d"
D19-1537,W10-2903,0,0.0605657,"lumn) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-drive"
D19-1537,H94-1010,0,0.712423,"xample. match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves. 2 Cross-Domain Context-Depencent Semantic Parsing 2.1 Datasets We use SParC 1 (Yu et al., 2019b), a large-scale cross-domain context-dependent semantic parsing dataset with SQL labels, as our main evaluation benchmark. A SParC example is shown in Table 3. We also report performance on ATIS (Hemphill et al., 1990; Dahl et al., 1994a) for direct comparison to Suhr et al. (2018). In addition, we evaluate the cross-domain context-independent text-toSQL ability of our model on Spider2 (Yu et al., 1 2 https://yale-lily.github.io/sparc https://yale-lily.github.io/spider 2018c), which SParC is built on. We summarize and compare the data statistics in Table 1 and Table 2. While the ATIS dataset has been extensively studied, it is limited to a particular domain. By contrast, SParC is both context-dependent and cross-domain. Each interaction in SParC is constructed using a question in Spider as the interaction goal, where the ann"
D19-1537,N19-1423,0,0.0255756,"of user utterance and column headers. dorms have a TV louge (b) Utterance Encoder. Bi LSTM Concatennation Attention over Utterance Tokens Self-Attention Among Table Columns Bi LSTM Bi LSTM Bi LSTM Bi LSTM Bi LSTM dorm . id dorm . name has . dorm id has . amenity id amenity Bi LSTM . id amenity . name (c) Table Encoder. Figure 2: Utterance-Table Encoder for the example in (a). Utterance-Table BERT Embedding. We consider two options as the input to the first layer biLSTM. The first choice is the pretrained word embedding. Second, we also consider the contextualized word embedding based on BERT (Devlin et al., 2019). To be specific, we follow Hwang et al. (2019) to concatenate the user utterance and all the column headers in a single sequence separated by the [SEP] token: [CLS], Xi , [SEP], c1 , [SEP], . . . , cm , [SEP] This sequence is fed into the pretrained BERT model whose hidden states at the last layer is used as the input embedding. 3.2 The hidden state of this interaction encoder hI encodes the history as the interaction proceeds. Turn Attention When issuing the current utterance, the user may omit or explicitly refer to the previously mentioned information. To this end, we adopt the turn attent"
D19-1537,P16-1004,0,0.0241913,"2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Frie"
D19-1537,P18-1068,0,0.1126,"meaning of user utterances, the structure of table schema, and the relationship between the two. To this end, we build an utterance-table encoder with co-attention between the two as illustrated in Figure 2. Figure 2b shows the utterance encoder. For the user utterance at each turn, we first use a bi-LSTM to encode utterance tokens. The bi-LSTM hidden state is fed into a dot-product attention layer (Luong et al., 2015) over the column header embeddings. For each utterance token embedding, we get an attention weighted average of the column header embeddings to obtain the most relevant columns (Dong and Lapata, 2018). We then concatenate the bi-LSTM hidden state and the column attention vector, and use a second layer biLSTM to generate the utterance token embedding hE . Figure 2c shows the table encoder. For each column header, we concatenate its table name and its column name separated by a special dot token (i.e., table name . column name). Each column header is processed by a bi-LSTM layer. To better capture the internal structure of the table schemas (e.g., foreign key), we then employ a selfattention (Vaswani et al., 2017) among all column headers. We then use an attention layer to capture the relati"
D19-1537,P18-1033,1,0.853357,"L queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et a"
D19-1537,N18-1177,0,0.0515028,"Missing"
D19-1537,P16-1154,0,0.0390467,", we predict a switch pcopy to decide if we need copy from the previous query or insert a new token. pcopy = σ(ck Wcopy + bcopy ) pinsert = 1 − pcopy (5) Then, we use a separate layer to score the query tokens at turn t − 1, and the output distribution is modified as the following to take into account the editing probability: Pprev SQL = softmax(ok Wprev SQL hQ t−1 ) mSQL = ok WSQL + bSQL mcolumn = ok Wcolumn hC PSQL S column = softmax([mSQL ; mcolumn ]) P (yk ) = pcopy · Pprev SQL (yk ∈ prev SQL) [ +pinsert · PSQL S column (yk ∈ SQL column) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; A"
D19-1537,D18-1188,0,0.0130342,"r et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to"
D19-1537,P19-1444,0,0.46871,"Missing"
D19-1537,P18-1124,0,0.0623869,"Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) m"
D19-1537,P17-1097,0,0.0224668,"(Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an i"
D19-1537,H90-1021,0,0.51482,"unge’) Table 3: SParC example. match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves. 2 Cross-Domain Context-Depencent Semantic Parsing 2.1 Datasets We use SParC 1 (Yu et al., 2019b), a large-scale cross-domain context-dependent semantic parsing dataset with SQL labels, as our main evaluation benchmark. A SParC example is shown in Table 3. We also report performance on ATIS (Hemphill et al., 1990; Dahl et al., 1994a) for direct comparison to Suhr et al. (2018). In addition, we evaluate the cross-domain context-independent text-toSQL ability of our model on Spider2 (Yu et al., 1 2 https://yale-lily.github.io/sparc https://yale-lily.github.io/spider 2018c), which SParC is built on. We summarize and compare the data statistics in Table 1 and Table 2. While the ATIS dataset has been extensively studied, it is limited to a particular domain. By contrast, SParC is both context-dependent and cross-domain. Each interaction in SParC is constructed using a question in Spider as the interaction"
D19-1537,N18-2115,0,0.0394036,"017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) an"
D19-1537,P17-1089,0,0.0436623,"sing executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018;"
D19-1537,D18-1192,0,0.0123607,"while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 201"
D19-1537,P17-1167,0,0.0294541,"al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE"
D19-1537,D19-1624,0,0.0439311,"r which takes bag-of-words representations of column headers as input. They also modify the decoder to select between a SQL keyword or a column header. (2) SyntaxSQL-con: This is adapted from the original context-agnostic SyntaxSQLNet (Yu et al., 2018b) by using bi-LSTMs to encode the interaction history including the utterance and the associated SQL query response. It also employs a column attention mechanism to compute representations of the previous question and SQL query. Spider. We compare with the results as reported in Yu et al. (2018b). Furthermore, we also include recent results from Lee (2019) who propose to use recursive decoding procedure, Bogin SQLNet (Xu et al., 2017) SyntaxSQLNet (Yu et al., 2018b) +data augmentation (Yu et al., 2018b) Lee (2019) GNN (Bogin et al., 2019) IRNet (Guo et al., 2019) IRNet (BERT) (Guo et al., 2019) Ours + utterance-table BERT Embedding Dev Set 10.9 18.9 24.8 28.5 40.7 53.2 61.9 36.4 57.6 Test Set 12.4 19.7 27.2 24.3 39.4 46.7 54.7 32.9 53.4 Table 4: Spider results on dev set and test set. et al. (2019) introducing graph neural networks for encoding schemas, and Guo et al. (2019) who achieve state-of-the-art performance by using an intermediate repr"
D19-1537,P16-1138,0,0.0348913,"promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating"
D19-1537,D15-1166,0,0.0400847,"), (X2 , Y2 ), . . . , (Xt−1 , Yt−1 )] Furthermore, in the cross-domain setting, each interaction is grounded to a different database. Therefore, the model is also given the schema of the current database as an input. We consider relational databases with multiple tables, and each table contains multiple column headers: T = [c1 , c2 , . . . , cl , . . . , cm ] where m is the number of column headers, and each cl consists of multiple words including its table name and column name (§ 3.1). 3 Methodology We employ an encoder-decoder architecture with attention mechanisms (Sutskever et al., 2014; Luong et al., 2015) as illustrated in Figure 1. The framework consists of (1) an utterance-table encoder to explicitly encode the user utterance and table schema at each turn, (2) A turn attention incorporating the recent history for decoding, (3) a table-aware decoder taking into account the context of the utterance, the table schema, and the previously generated query to make editing decisions. 3.1 Utterance-Table Encoder An effective encoder captures the meaning of user utterances, the structure of table schema, and the relationship between the two. To this end, we build an utterance-table encoder with co-att"
D19-1537,P96-1008,0,0.628325,"ations by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Y"
D19-1537,P15-1142,0,0.0267292,"ith context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE contains three domains using stack- or list-like elements and most queries include a single binary predicate. SequentialQA is created by decomposing some complicated questions in WikiTableQuestions (Pasupat and Liang, 2015). Since both SCONE and SequentialQA are annotated with only denotations but not query labels, they don’t include many questions with rich semantic and contextual types. For example, SequentialQA (Iyyer et al., 2017) requires that the answer to follow-up questions must be a subset of previous answers, and most of the questions can be answered by simple SQL queries with SELECT and WHERE clauses. Concurrent with our work, Yu et al. (2019a) introduced CoSQL, a large-scale cross-domain conversational text-to-SQL corpus collected under the Wizard-of-Oz setting. Each dialogue in CoSQL simulates a DB"
D19-1537,D17-1127,0,0.0360468,"oyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Su"
D19-1537,P18-1193,0,0.0113637,"17; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segme"
D19-1537,N18-1203,0,0.0305507,"Missing"
D19-1537,P18-1034,0,0.0408402,"languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Mill"
D19-1537,P15-1129,0,0.0350818,"tences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions int"
D19-1537,D14-1162,1,0.106791,"(Guo et al., 2019) IRNet (BERT) (Guo et al., 2019) Ours + utterance-table BERT Embedding Dev Set 10.9 18.9 24.8 28.5 40.7 53.2 61.9 36.4 57.6 Test Set 12.4 19.7 27.2 24.3 39.4 46.7 54.7 32.9 53.4 Table 4: Spider results on dev set and test set. et al. (2019) introducing graph neural networks for encoding schemas, and Guo et al. (2019) who achieve state-of-the-art performance by using an intermediate representation to bridge natural language questions and SQL queries. 5.3 Implementation Details Our model is implemented in PyTorch (Paszke et al., 2017). We use pretrained 300-dimensional GloVe (Pennington et al., 2014) word embedding. All LSTM layers have 300 hidden size, and we use 1 layer for encoder LSTMs, and 2 layers for decoder LSTMs. We use the ADAM optimizer (Kingma and Ba, 2015) to minimize the tokenlevel cross-entropy loss with a batch size of 16. Model parameters are randomly initialized from a uniform distribution U [−0.1, 0.1]. The main model has an initial learning rate of 0.001 and it will be multiplied by 0.8 if the validation loss increases compared with the previous epoch. When using BERT instead of GloVe, we use the pretrained small uncased BERT model with 768 hidden size5 , and we fine t"
D19-1537,P17-1099,0,0.0417505,"pcopy to decide if we need copy from the previous query or insert a new token. pcopy = σ(ck Wcopy + bcopy ) pinsert = 1 − pcopy (5) Then, we use a separate layer to score the query tokens at turn t − 1, and the output distribution is modified as the following to take into account the editing probability: Pprev SQL = softmax(ok Wprev SQL hQ t−1 ) mSQL = ok WSQL + bSQL mcolumn = ok Wcolumn hC PSQL S column = softmax([mSQL ; mcolumn ]) P (yk ) = pcopy · Pprev SQL (yk ∈ prev SQL) [ +pinsert · PSQL S column (yk ∈ SQL column) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer,"
D19-1537,D18-1197,0,0.0120536,"ost of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames whic"
D19-1537,P17-1041,0,0.060978,"iously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2"
D19-1537,N18-2093,1,0.932754,"representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets includin"
D19-1537,P09-1110,0,0.0185736,"still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE contains three domains using stack- or list-like elements and most queries include a single binary predicate. SequentialQA is created by decomposing some complicated questions in WikiTableQu"
D19-1537,D18-1193,1,0.950706,"representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets includin"
D19-1537,D18-1425,1,0.89856,"Missing"
D19-1537,P19-1443,1,0.875363,"Missing"
D19-6106,P14-1006,0,0.0249403,"relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: Agglomerative clustering of Languages based on the PWCCA similarity between their represenations, generated from layer 6 of a pretrained multilingual uncased BERT. data from ten distinct genres of English language for the the task of natural language inference (prediction of whether the relationship between two sentences represents entailment, con"
D19-6106,C12-1089,0,0.0312437,"ssing (NLP) in multilingual settings often relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: Agglomerative clustering of Languages based on the PWCCA similarity between their represenations, generated from layer 6 of a pretrained multilingual uncased BERT. data from ten distinct genres of English language for the the task of natural language inference (prediction of whether the relationship be"
D19-6106,D19-1167,0,0.373251,"t al., 2013; Conneau et al., 2018). A multilingual version of BERT trained on over 100 languages achieved state-of-the-art performance across a wide range of languages as well. Performance for lowresource languages has been further improved by additionally leveraging parallel data (Lample and Conneau, 2019) and leveraging machine translation systems for cross-lingual regularization (Singh et al., 2019). Prior work in zero-shot machine translation has investigated the extent to which multilingual neural machine translation systems trained with a shared subword vocabulary Johnson et al. (2017); Kudugunta et al. (2019) learn a form of interlingua, a common representational space for semantically similar text across languages. We aim to extend this study to language models pretrained with multilingual data in order to investigate the extent to which the resulting contextualized word embeddings represent an interlingua. Canonical correlation analysis (CCA) is a classical tool from multivariate statistics (Hotelling, 1992) that investigates the relationships between two sets of random variables. Singular value and projection weighted variants of CCA allow for analysis of representations of the same data points"
D19-6106,N19-1112,0,0.036883,"all language combinations tested, the summary representation (associated with the [CLS] token) for semantically similar inputs translated into multiple languages is most similar at the shallower layers of BERT, close to the initial embeddings. The representations steadily become more dissimilar in deeper layers until the final layer. The jump in similarity in the final layer can be explained by the common classification layer that contains only three classes. In order to finally choose an output class, the network must project towards one of the three embeddings associated with those classes (Liu et al., 2019). The trend towards dissimilarity in deeper layers suggests that contextualization in BERT is not a process of semantic abstraction as would be expected of an interlingua. Though semantic features common to the multiple translations of the input might also be extracted, the similarity between representations is dominated by features that differentiate them. BERT appears to preserve and refine features that separate the inputs, which we speculate are more closely related to syntactic and grammatical features of the input. Representations at the shallower layers, closer to the subword embeddings"
D19-6106,P18-1073,0,0.044593,"Missing"
D19-6106,W15-1521,0,0.0156455,"nto multilingual representations. 1 Introduction Natural language processing (NLP) in multilingual settings often relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: Agglomerative clustering of Languages based on the PWCCA similarity between their represenations, generated from layer 6 of a pretrained multilingual uncased BERT. data from ten distinct genres of English language for the the ta"
D19-6106,D18-1269,0,0.0823751,"g Xiong2 , Richard Socher2 Stanford University1 , Salesforce Research2 jasdeep@cs.stanford.edu {bmccann,cxiong,rsocher}@salesforce.com Abstract using dictionaries between languages (Mikolov et al., 2013a; Faruqui and Dyer, 2014), and crosslingual embeddings can be learned jointly through entirely unsupervised methods (Conneau et al., 2017; Artetxe et al., 2018). Contextualized word embeddings like CoVe, ElMo, and BERT (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2018) improve a wide variety of natural language tasks (Wang et al., 2018; Rajpurkar et al., 2016; Socher et al., 2013; Conneau et al., 2018). A multilingual version of BERT trained on over 100 languages achieved state-of-the-art performance across a wide range of languages as well. Performance for lowresource languages has been further improved by additionally leveraging parallel data (Lample and Conneau, 2019) and leveraging machine translation systems for cross-lingual regularization (Singh et al., 2019). Prior work in zero-shot machine translation has investigated the extent to which multilingual neural machine translation systems trained with a shared subword vocabulary Johnson et al. (2017); Kudugunta et al. (2019) learn a fo"
D19-6106,E14-1049,0,0.0607179,"Missing"
D19-6106,D13-1141,1,0.788308,"al settings often relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: Agglomerative clustering of Languages based on the PWCCA similarity between their represenations, generated from layer 6 of a pretrained multilingual uncased BERT. data from ten distinct genres of English language for the the task of natural language inference (prediction of whether the relationship between two sentence"
D19-6106,D14-1162,1,0.106837,"enetic trees hand-designed by linguists. The subword tokenization employed by BERT provides a stronger bias towards such structure than character- and wordlevel tokenizations. We release a subset of the XNLI dataset translated into an additional 14 languages at https://www.github. com/salesforce/xnli_extension to assist further research into multilingual representations. 1 Introduction Natural language processing (NLP) in multilingual settings often relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China,"
D19-6106,N18-1202,0,0.0530808,"Missing"
D19-6106,D16-1264,0,0.0117057,"ation Jasdeep Singh1 , Bryan McCann2 , Caiming Xiong2 , Richard Socher2 Stanford University1 , Salesforce Research2 jasdeep@cs.stanford.edu {bmccann,cxiong,rsocher}@salesforce.com Abstract using dictionaries between languages (Mikolov et al., 2013a; Faruqui and Dyer, 2014), and crosslingual embeddings can be learned jointly through entirely unsupervised methods (Conneau et al., 2017; Artetxe et al., 2018). Contextualized word embeddings like CoVe, ElMo, and BERT (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2018) improve a wide variety of natural language tasks (Wang et al., 2018; Rajpurkar et al., 2016; Socher et al., 2013; Conneau et al., 2018). A multilingual version of BERT trained on over 100 languages achieved state-of-the-art performance across a wide range of languages as well. Performance for lowresource languages has been further improved by additionally leveraging parallel data (Lample and Conneau, 2019) and leveraging machine translation systems for cross-lingual regularization (Singh et al., 2019). Prior work in zero-shot machine translation has investigated the extent to which multilingual neural machine translation systems trained with a shared subword vocabulary Johnson et al"
D19-6106,D13-1170,1,0.016616,"ryan McCann2 , Caiming Xiong2 , Richard Socher2 Stanford University1 , Salesforce Research2 jasdeep@cs.stanford.edu {bmccann,cxiong,rsocher}@salesforce.com Abstract using dictionaries between languages (Mikolov et al., 2013a; Faruqui and Dyer, 2014), and crosslingual embeddings can be learned jointly through entirely unsupervised methods (Conneau et al., 2017; Artetxe et al., 2018). Contextualized word embeddings like CoVe, ElMo, and BERT (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2018) improve a wide variety of natural language tasks (Wang et al., 2018; Rajpurkar et al., 2016; Socher et al., 2013; Conneau et al., 2018). A multilingual version of BERT trained on over 100 languages achieved state-of-the-art performance across a wide range of languages as well. Performance for lowresource languages has been further improved by additionally leveraging parallel data (Lample and Conneau, 2019) and leveraging machine translation systems for cross-lingual regularization (Singh et al., 2019). Prior work in zero-shot machine translation has investigated the extent to which multilingual neural machine translation systems trained with a shared subword vocabulary Johnson et al. (2017); Kudugunta e"
D19-6106,W18-5446,0,0.0484003,"Missing"
N16-1019,Q13-1005,0,0.0599451,"15), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Artzi and Zettlemoyer, 2013). Different approaches and emphases have been explored. For example, linear programming has been applied to mediate perceptual differences between humans and robots for referential grounding (Liu and Chai, 2015). Approaches to semantic parsing have been applied to ground language to internal world representations (Chen and Mooney, 2008; Artzi and Zettlemoyer, 2013). Logical Semantics with Perception (LSP) (Krishnamurthy and Kollar, 2013) was applied to ground natural language queries to visual referents through jointly parsing natural language (combinatory categorical grammar (CCG)) and visual"
N16-1019,P15-1006,0,0.0175925,"from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Artzi and Zettlemoyer, 2013). Different ap"
N16-1019,P15-2017,0,0.0193807,"to part of the TACoS dataset. This annotation captures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 201"
N16-1019,P15-1005,0,0.0339368,"Missing"
N16-1019,W13-3820,0,0.0195411,"entify linguistic entities from the text that serve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional SRL is not targeted to represent verb semantics that are grounded to the physical world so that artificial agents can truly understand the ongoing activities and (learn to) perform the specified actions. To address this issue, we propose a new task on grounded semantic role labeling. Figure 1 shows an example of grounded SRL. 149 Proceedings of NAACL-HLT 2016, pages 149–159, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics The sentence the woman takes out a cucumber from the refri"
N16-1019,J02-3001,0,0.00957276,"implicit role (destination) is grounded to a track id. Introduction Linguistic studies capture semantics of verbs by their frames of thematic roles (also referred to as semantic roles or verb arguments) (Levin, 1993). For example, a verb can be characterized by agent (i.e., the animator of the action) and patient (i.e., the object on which the action is acted upon), and other roles such as instrument, source, destination, etc. Given a verb frame, the goal of Semantic Role Labeling (SRL) is to identify linguistic entities from the text that serve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional SRL is not targeted to repre"
N16-1019,D14-1086,0,0.0532011,"rbs and in a much more complex domain where object recognition and tracking are notably more difficult. Third, our work results in additional layers of annotation to part of the TACoS dataset. This annotation captures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to"
N16-1019,Q13-1016,0,0.0770443,"description) is grounded to the cutting board (track 5). To tackle this problem, we have developed an approach to jointly process language and vision by incorporating semantic role information. In particular, we use a benchmark dataset (TACoS) which consists of parallel video and language descriptions in a complex cooking domain (Regneri et al., 2013) in our investigation. We have further annotated several layers of information for developing and evaluating grounded semantic role labeling algorithms. Compared to previous works on language grounding (Tellex et al., 2011; Yu and Siskind, 2013; Krishnamurthy and Kollar, 2013), our work presents several contributions. First, beyond arguments explicitly mentioned in language descriptions, our work simultaneously grounds explicit and implicit roles with an attempt to better connect verb semantics with actions from the underlying physical world. By incorporating semantic role information, our approach has led to better grounding performance. Second, most previous works only focused on a small number of verbs with limited activities. We base our investigation on a wider range of verbs and in a much more complex domain where object recognition and tracking are notably m"
N16-1019,P13-2138,0,0.00947237,"tracking are notably more difficult. Third, our work results in additional layers of annotation to part of the TACoS dataset. This annotation captures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al"
N16-1019,N15-1016,0,0.0104544,"re work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Artzi and Zettlemoyer, 2013). Different approaches and emphases have been explored. For example, linear programming has been applied to mediate per"
N16-1019,W12-1621,1,0.857349,"al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Artzi and Zettlemoyer, 2013). Different approaches and emphases have been explored. For example, linear programming has been applied to mediate perceptual differences between humans and robots for referential grounding (Liu and Chai, 2015). Approaches to semantic parsing have been applied to ground language to internal world representations (Chen and Mooney, 2008; Artzi and Zettlemoyer, 2013). Logical Semantics with Perception (LSP) (Krishnamurth"
N16-1019,N15-1015,0,0.0235219,"re of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Art"
N16-1019,P14-5010,0,0.00278094,"rred from the video. For the verb cut, the location and the tool are also rarely specified by linguistic expressions. Nevertheless, these implicit roles contribute to the overall understanding of actions and should also be grounded too. 4.2 Automated Processing To build the structure of the CRF as shown in Figure 2 and extract features for learning and inference, we have applied the following approaches to process language and vision. Language Processing. Language processing consists of three steps to build a structure containing syntactic and semantic information. First, the Stanford Parser (Manning et al., 2014) is applied to create a dependency parsing tree for each sentence. Second, Senna (Collobert et al., 2011) is applied to identify semantic role labels for the key verb in the sentence. The linguistic entities with semantic roles are matched against the dependency nodes in the tree and the corresponding semantic role labels are added to the tree. Third, for each verb, the Propbank (Palmer et al., 2005) entries are searched to extract all relevant semantic roles. The implicit roles (i.e., not specified linguistically) are added as direct children of verb nodes in the tree. Through these three ste"
N16-1019,N15-1017,0,0.0764911,"aptures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (T"
N16-1019,N15-1174,0,0.0339839,"Missing"
N16-1019,J05-1004,0,0.190181,"ch role including the implicit role (destination) is grounded to a track id. Introduction Linguistic studies capture semantics of verbs by their frames of thematic roles (also referred to as semantic roles or verb arguments) (Levin, 1993). For example, a verb can be characterized by agent (i.e., the animator of the action) and patient (i.e., the object on which the action is acted upon), and other roles such as instrument, source, destination, etc. Given a verb frame, the goal of Semantic Role Labeling (SRL) is to identify linguistic entities from the text that serve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional S"
N16-1019,Q13-1003,0,0.223327,"ctually does the take-out action in the visual scene (track 1) ; the patient is grounded to the cucumber taken out (track 3); and the source is grounded to the refrigerator (track 4). The implicit role of destination (which is not explicitly mentioned in the language description) is grounded to the cutting board (track 5). To tackle this problem, we have developed an approach to jointly process language and vision by incorporating semantic role information. In particular, we use a benchmark dataset (TACoS) which consists of parallel video and language descriptions in a complex cooking domain (Regneri et al., 2013) in our investigation. We have further annotated several layers of information for developing and evaluating grounded semantic role labeling algorithms. Compared to previous works on language grounding (Tellex et al., 2011; Yu and Siskind, 2013; Krishnamurthy and Kollar, 2013), our work presents several contributions. First, beyond arguments explicitly mentioned in language descriptions, our work simultaneously grounds explicit and implicit roles with an attempt to better connect verb semantics with actions from the underlying physical world. By incorporating semantic role information, our app"
N16-1019,D07-1002,0,0.0416949,"erve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional SRL is not targeted to represent verb semantics that are grounded to the physical world so that artificial agents can truly understand the ongoing activities and (learn to) perform the specified actions. To address this issue, we propose a new task on grounded semantic role labeling. Figure 1 shows an example of grounded SRL. 149 Proceedings of NAACL-HLT 2016, pages 149–159, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics The sentence the woman takes out a cucumber from the refrigerator describes an activity in a visual scene"
N16-1019,N15-1173,0,0.0370204,"difficult. Third, our work results in additional layers of annotation to part of the TACoS dataset. This annotation captures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 201"
N16-1019,P13-1006,0,0.123839,"tioned in the language description) is grounded to the cutting board (track 5). To tackle this problem, we have developed an approach to jointly process language and vision by incorporating semantic role information. In particular, we use a benchmark dataset (TACoS) which consists of parallel video and language descriptions in a complex cooking domain (Regneri et al., 2013) in our investigation. We have further annotated several layers of information for developing and evaluating grounded semantic role labeling algorithms. Compared to previous works on language grounding (Tellex et al., 2011; Yu and Siskind, 2013; Krishnamurthy and Kollar, 2013), our work presents several contributions. First, beyond arguments explicitly mentioned in language descriptions, our work simultaneously grounds explicit and implicit roles with an attempt to better connect verb semantics with actions from the underlying physical world. By incorporating semantic role information, our approach has led to better grounding performance. Second, most previous works only focused on a small number of verbs with limited activities. We base our investigation on a wider range of verbs and in a much more complex domain where object recog"
N16-1019,P15-1109,0,0.0121407,"id. Introduction Linguistic studies capture semantics of verbs by their frames of thematic roles (also referred to as semantic roles or verb arguments) (Levin, 1993). For example, a verb can be characterized by agent (i.e., the animator of the action) and patient (i.e., the object on which the action is acted upon), and other roles such as instrument, source, destination, etc. Given a verb frame, the goal of Semantic Role Labeling (SRL) is to identify linguistic entities from the text that serve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional SRL is not targeted to represent verb semantics that are grounded to the"
P18-1135,D16-1053,0,0.127889,"s in the sequence, demb the dimension of the embeddings, and X ∈ Rn×demb the word embeddings corresponding to words in the sequence. We produce a global encoding H g of X using a global bidirectional LSTM. H g = biLSTMg (X) ∈ Rn×drnn (2) H = β s H s + (1 − β s ) H g ∈ Rn×drnn (3) Here, the scalar β s is a learned parameter between 0 and 1 that is specific to the slot s. Next, we compute a global-local self-attention context c over H. Self-attention, or intra-attention, is a very effective method of computing summary context over variable-length sequences for natural language processing tasks (Cheng et al., 2016; Vaswani et al., 2017; He et al., 2017; Lee et al., 2017). In our case, we use a global self-attention module to compute attention context useful for general-purpose state tracking, as well as a local self-attention module to compute slot-specific attention context. For each ith element Hi , we compute a scalar global self-attention score agi which is subsequently normalized across all elements using a softmax function. (1) where drnn is the dimension of the LSTM state. We similarly produce a local encoding H s of X, 1460 agi = W g Hi + bg ∈ R p g = softmax (a ) ∈ R g (4) n (5) The global sel"
P18-1135,N13-1092,0,0.0163688,"Missing"
P18-1135,D17-1206,1,0.897826,", we simply accumulate turn goals. In the event that the current turn goal specifies a slot that has been specified before, the new specification takes precedence. For example, suppose the user specifies a food=french restaurant during the current turn. If the joint goal has no existing food specifications, then we simply add food=french to the joint goal. Alternatively, if food=thai had been specified in a previous turn, we simply replace it with food=french. 3.3 Implementation Details We use fixed, pretrained GloVe embeddings (Pennington et al., 2014) as well as character n-gram embeddings (Hashimoto et al., 2017). Each model is trained using ADAM (Kingma and Ba, 2015). For regularization, we apply dropout with 0.2 drop rate (Srivastava et al., 2014) to the output of each local module and each global module. We use the development split for hyperparameter tuning and apply early stopping using the joint goal accuracy. For the DSTC2 task, we train using transcripts of user utterances and evaluate using the noisy ASR transcriptions. During evaluation, we take the sum of the scores resulting from each ASR output as the output score of a particular slot-value. We then normalize this sum using a sigmoid func"
P18-1135,P17-1044,0,0.137171,"he embeddings, and X ∈ Rn×demb the word embeddings corresponding to words in the sequence. We produce a global encoding H g of X using a global bidirectional LSTM. H g = biLSTMg (X) ∈ Rn×drnn (2) H = β s H s + (1 − β s ) H g ∈ Rn×drnn (3) Here, the scalar β s is a learned parameter between 0 and 1 that is specific to the slot s. Next, we compute a global-local self-attention context c over H. Self-attention, or intra-attention, is a very effective method of computing summary context over variable-length sequences for natural language processing tasks (Cheng et al., 2016; Vaswani et al., 2017; He et al., 2017; Lee et al., 2017). In our case, we use a global self-attention module to compute attention context useful for general-purpose state tracking, as well as a local self-attention module to compute slot-specific attention context. For each ith element Hi , we compute a scalar global self-attention score agi which is subsequently normalized across all elements using a softmax function. (1) where drnn is the dimension of the LSTM state. We similarly produce a local encoding H s of X, 1460 agi = W g Hi + bg ∈ R p g = softmax (a ) ∈ R g (4) n (5) The global self-attention context cg is then the sum"
P18-1135,W14-4337,0,0.776336,"est. The joint goal is the set of accumulated turn goals up to the current turn. Figure 1 shows an example dialogue with annotated turn states, in which the user reserves a restaurant. Traditional dialogue state trackers rely on Spoken Language Understanding (SLU) systems (Henderson et al., 2012) in order to understand user utterances. These trackers accumulate errors from the SLU, which sometimes do not have the necessary dialogue context to interpret the user utterances. Subsequent DST research forgo the SLU and directly infer the state using the conversation history and the user utterance (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Mrkˇsi´c et al., 2015). These trackers rely on handcrafted semantic dictionaries and delexicalization — the anonymization of slots and values using generic tags — to achieve generalization. Recent work by Mrkˇsi´c et al. (2017) apply representation learning using convolutional neural networks to learn features relevant for each state as opposed to hand-crafting features. A key problem in DST that is not addressed by existing methods is the extraction of rare slotvalue pairs that compose the state during each turn. Because task oriented dialogues cover large 1458 Pr"
P18-1135,W14-4340,0,0.538945,"est. The joint goal is the set of accumulated turn goals up to the current turn. Figure 1 shows an example dialogue with annotated turn states, in which the user reserves a restaurant. Traditional dialogue state trackers rely on Spoken Language Understanding (SLU) systems (Henderson et al., 2012) in order to understand user utterances. These trackers accumulate errors from the SLU, which sometimes do not have the necessary dialogue context to interpret the user utterances. Subsequent DST research forgo the SLU and directly infer the state using the conversation history and the user utterance (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Mrkˇsi´c et al., 2015). These trackers rely on handcrafted semantic dictionaries and delexicalization — the anonymization of slots and values using generic tags — to achieve generalization. Recent work by Mrkˇsi´c et al. (2017) apply representation learning using convolutional neural networks to learn features relevant for each state as opposed to hand-crafting features. A key problem in DST that is not addressed by existing methods is the extraction of rare slotvalue pairs that compose the state during each turn. Because task oriented dialogues cover large 1458 Pr"
P18-1135,D17-1018,0,0.103158,"d X ∈ Rn×demb the word embeddings corresponding to words in the sequence. We produce a global encoding H g of X using a global bidirectional LSTM. H g = biLSTMg (X) ∈ Rn×drnn (2) H = β s H s + (1 − β s ) H g ∈ Rn×drnn (3) Here, the scalar β s is a learned parameter between 0 and 1 that is specific to the slot s. Next, we compute a global-local self-attention context c over H. Self-attention, or intra-attention, is a very effective method of computing summary context over variable-length sequences for natural language processing tasks (Cheng et al., 2016; Vaswani et al., 2017; He et al., 2017; Lee et al., 2017). In our case, we use a global self-attention module to compute attention context useful for general-purpose state tracking, as well as a local self-attention module to compute slot-specific attention context. For each ith element Hi , we compute a scalar global self-attention score agi which is subsequently normalized across all elements using a softmax function. (1) where drnn is the dimension of the LSTM state. We similarly produce a local encoding H s of X, 1460 agi = W g Hi + bg ∈ R p g = softmax (a ) ∈ R g (4) n (5) The global self-attention context cg is then the sum of each element Hi"
P18-1135,D15-1166,0,0.0256073,"ships by injecting semantic similarity constraints from the Paraphrase Database (Wieting et al., 2015; Ganitkevitch et al., 2013). On the one hand, these specialized embeddings are more difficult to obtain than word embeddings from language modeling. On the other hand, these embeddings are not specific to any dialogue domain and are directly usable for new domains. Neural attention models in NLP. Attention mechanisms have led to improvements on a variety of natural language processing tasks. Bahdanau et al. (2015) propose attentional sequence to sequence models for neural machine translation. Luong et al. (2015) analyze various attention techniques and highlight the effectiveness of the simple, parameterless dot product attention. Similar models have also proven successful in tasks such as summarization (See et al., 2017; Paulus et al., 2018). Self-attention, or intra-attention, has led improvements in language modeling, sentiment 1464 System actions in previous turn User utterance Predicted turn belief state N/A I would like Polynesian food in the South part of town. Please send me phone number and address. request(phone) request(address) inform(food=polynesian) inform(area=south) request(address) r"
P18-1135,P15-2130,0,0.226293,"Missing"
P18-1135,P17-1163,0,0.753228,"Missing"
P18-1135,D14-1162,1,0.112142,"how to obtain turn goals and requests. To compute the joint goal, we simply accumulate turn goals. In the event that the current turn goal specifies a slot that has been specified before, the new specification takes precedence. For example, suppose the user specifies a food=french restaurant during the current turn. If the joint goal has no existing food specifications, then we simply add food=french to the joint goal. Alternatively, if food=thai had been specified in a previous turn, we simply replace it with food=french. 3.3 Implementation Details We use fixed, pretrained GloVe embeddings (Pennington et al., 2014) as well as character n-gram embeddings (Hashimoto et al., 2017). Each model is trained using ADAM (Kingma and Ba, 2015). For regularization, we apply dropout with 0.2 drop rate (Srivastava et al., 2014) to the output of each local module and each global module. We use the development split for hyperparameter tuning and apply early stopping using the joint goal accuracy. For the DSTC2 task, we train using transcripts of user utterances and evaluate using the noisy ASR transcriptions. During evaluation, we take the sum of the scores resulting from each ASR output as the output score of a partic"
P18-1135,E17-1029,0,0.0509186,"er does not answer the system’s previous request for the choice of food and instead asks for what food is available. The model misinterprets the lack of response as the user not caring about the choice of food. 4 Related Work Dialogue State Tracking. Traditional dialogue state trackers rely on a separate SLU component that serves as the initial stage in the dialogue agent pipeline. The downstream tracker then combines the semantics extracted by the SLU with previous dialogue context in order to estimate the current dialogue state (Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014; Perez and Liu, 2017). Recent results in dialogue state tracking show that it is beneficial to jointly learn speech understanding and dialogue tracking (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). These approaches directly take as input the N-best list produced by the ASR system. By avoiding the accumulation of errors from the initial SLU component, these joint approaches typically achieved stronger performance on tasks such as DSTC2. One drawback to these approaches is that they rely on hand-crafted features and complex domain-specific lexicon (in addition to the ontology), and consequen"
P18-1135,P17-1099,0,0.029697,"embeddings from language modeling. On the other hand, these embeddings are not specific to any dialogue domain and are directly usable for new domains. Neural attention models in NLP. Attention mechanisms have led to improvements on a variety of natural language processing tasks. Bahdanau et al. (2015) propose attentional sequence to sequence models for neural machine translation. Luong et al. (2015) analyze various attention techniques and highlight the effectiveness of the simple, parameterless dot product attention. Similar models have also proven successful in tasks such as summarization (See et al., 2017; Paulus et al., 2018). Self-attention, or intra-attention, has led improvements in language modeling, sentiment 1464 System actions in previous turn User utterance Predicted turn belief state N/A I would like Polynesian food in the South part of town. Please send me phone number and address. request(phone) request(address) inform(food=polynesian) inform(area=south) request(address) request(phone) Yes please. request(phone) request(address) I just want to eat at a cheap restaurant in the south part of town. What food types are available, can you also provide some phone numbers? request(phone)"
P18-1135,W13-4067,0,0.346022,"ing instances. the model. Here, the user does not answer the system’s previous request for the choice of food and instead asks for what food is available. The model misinterprets the lack of response as the user not caring about the choice of food. 4 Related Work Dialogue State Tracking. Traditional dialogue state trackers rely on a separate SLU component that serves as the initial stage in the dialogue agent pipeline. The downstream tracker then combines the semantics extracted by the SLU with previous dialogue context in order to estimate the current dialogue state (Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014; Perez and Liu, 2017). Recent results in dialogue state tracking show that it is beneficial to jointly learn speech understanding and dialogue tracking (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). These approaches directly take as input the N-best list produced by the ASR system. By avoiding the accumulation of errors from the initial SLU component, these joint approaches typically achieved stronger performance on tasks such as DSTC2. One drawback to these approaches is that they rely on hand-crafted features and complex domain-specific lexicon (in ad"
P18-1135,E17-1042,0,0.236563,"Missing"
P18-1135,Q15-1025,0,0.0289056,"eatures and complex domain-specific lexicon (in addition to the ontology), and consequently are difficult to extend and scale to new domains. The recent Neural Belief Tracker (NBT) by Mrkˇsi´c et al. (2017) avoids reliance on hand-crafted features and lexicon by using representation learning. The NBT employs convolutional filters over word embeddings in lieu of previously-used hand-engineered features. Moreover, to outperform previous methods, the NBT uses pretrained embeddings tailored to retain semantic relationships by injecting semantic similarity constraints from the Paraphrase Database (Wieting et al., 2015; Ganitkevitch et al., 2013). On the one hand, these specialized embeddings are more difficult to obtain than word embeddings from language modeling. On the other hand, these embeddings are not specific to any dialogue domain and are directly usable for new domains. Neural attention models in NLP. Attention mechanisms have led to improvements on a variety of natural language processing tasks. Bahdanau et al. (2015) propose attentional sequence to sequence models for neural machine translation. Luong et al. (2015) analyze various attention techniques and highlight the effectiveness of the simpl"
P18-1135,W14-4339,0,0.19631,"el. Here, the user does not answer the system’s previous request for the choice of food and instead asks for what food is available. The model misinterprets the lack of response as the user not caring about the choice of food. 4 Related Work Dialogue State Tracking. Traditional dialogue state trackers rely on a separate SLU component that serves as the initial stage in the dialogue agent pipeline. The downstream tracker then combines the semantics extracted by the SLU with previous dialogue context in order to estimate the current dialogue state (Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014; Perez and Liu, 2017). Recent results in dialogue state tracking show that it is beneficial to jointly learn speech understanding and dialogue tracking (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). These approaches directly take as input the N-best list produced by the ASR system. By avoiding the accumulation of errors from the initial SLU component, these joint approaches typically achieved stronger performance on tasks such as DSTC2. One drawback to these approaches is that they rely on hand-crafted features and complex domain-specific lexicon (in addition to the on"
P18-1135,W13-4065,0,0.183891,"tions, we introduce a sentinel action to each turn which allows the attention mechanism to ignore previous system actions. The score y act indicates the degree to which the value was expressed by the previous actions. 1461 The final score y is then a weighted sum between the two scores y utt and y act , normalized by the sigmoid function σ. ( ) y = σ y utt + wy act ∈ R (23) Here, the weight w is a learned parameter. 3 Experiments 3.1 Dataset The Dialogue Systems Technology Challenges (DSTC) provides a common framework for developing and evaluating dialogue systems and dialogue state trackers (Williams et al., 2013; Henderson et al., 2014a). Under this framework, dialogue semantics such as states and actions are based on a task ontology such as restaurant reservation. During each turn, the user may inform the system of particular goals (e.g. inform(food=french)), or request for more information from the system (e.g. request(address)). For instance, food and area are examples of slots in the DSTC2 task, and french and chinese are example values within the food slot. We train and evaluate our model using DSTC2 as well as the Wizard of Oz (WoZ) restaurant reservation task (Wen et al., 2017), which also adh"
P18-1160,D13-1160,0,0.0870238,"Koˇcisk`y et al., 2017), and textbooks (Lai et al., 2017; Xie et al., 2017). Many neural QA models have successfully addressed these tasks by leveraging coattention or bidirectional attention mechanisms (Xiong et al., 2018; Seo et al., 2017) to model the codependent context over the document and the question. However, Jia and Liang (2017) find that many QA models are sensitive to adversarial inputs. Recently, researchers have developed largescale QA datasets, which requires answering the question over a large set of documents in a closed (Joshi et al., 2017) or open-domain (Dunn et al., 2017; Berant et al., 2013; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016;"
P18-1160,P17-1171,0,0.314814,"gram of the questions answered correctly (on exact match (EM)) by the model given a full document (F ULL) and the model given an oracle sentence (O RACLE) on SQuAD (left) and NewsQA (right). of-the-art QA models, achieving 83.1 F1 on the SQuAD development set. It features a deep residual coattention encoder, a dynamic pointing decoder, and a mixed objective that combines cross entropy loss with self-critical policy learning. SReader is another competitive QA model that is simpler and faster than DCN+, with 79.9 F1 on the SQuAD development set. It is a simplified version of the reader in DrQA (Chen et al., 2017), which obtains 78.8 F1 on the SQuAD development set. Model details and training procedures are shown in Appendix A. 3.2 Sentence Selector Our sentence selector scores each sentence with respect to the question in parallel. The score indicates whether the question is answerable with this sentence. The model architecture is divided into the encoder module and the decoder module. The encoder is a shared module with S-Reader, which computes sentence encodings and question encodings from the sentence and the question as inputs. First, the encoder computes sentence embeddings D ∈ Rhd ×Ld , question"
P18-1160,P17-1020,0,0.107273,"/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required to answer the question, (ii) we choose the minimal context by selecting variable number of sentences for each question, while they use a fixe"
P18-1160,D17-1206,1,0.801512,"Missing"
P18-1160,D17-1215,0,0.270305,"at also provides a paragraph for each question, but the paragraphs are longer than those in SQuAD. TriviaQA (Joshi et al., 2017) is a dataset on a large set of documents from the Wikipedia domain and Web domain. Here, we only use the Wikipedia domain. Each question is given a much longer context in the form of multiple documents. SQuAD-Open (Chen et al., 2017) is an opendomain question answering dataset based on SQuAD. In SQuAD-Open, only the question and the answer are given. The model is responsible for identifying the relevant context from all English Wikipedia articles. SQuAD-Adversarial (Jia and Liang, 2017) is a variant of SQuAD. It shares the same training set as SQuAD, but an adversarial sentence is added to each paragraph in a subset of the development set. We use accuracy (Acc) and mean average precision (MAP) to evaluate sentence selection. We also measure the average number of selected sentences (N sent) to compare the efficiency of our Dyn method and the Top k method. To evaluate the performance in the task of question answering, we measure F1 and EM (Exact Match), both being standard metrics for evaluating span-based QA. In addition, we measure training speed (Train Sp) and inference spe"
P18-1160,P17-1147,0,0.536648,"y comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs. 1 Introduction The task of textual question answering (QA), in which a machine reads a document and answers a question, is an important and challenging problem in natural language processing. Recent progress in performance of QA models has been largely due to the variety of available QA datasets (Richardson et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Joshi et al., 2017; Koˇcisk`y et al., 2017). ∗ All work was done while the author was an intern at Salesforce Research. Many neural QA models have been proposed for these datasets, the most successful of which tend to leverage coattention or bidirectional attention mechanisms that build codependent representations of the document and the question (Xiong et al., 2018; Seo et al., 2017). Yet, learning the full context over the document is challenging and inefficient. In particular, when the model is given a long document, or multiple documents, learning the full context is intractably slow and hence difficult to"
P18-1160,P14-5010,0,0.00471157,"Missing"
P18-1160,D16-1147,0,0.0329925,"; Berant et al., 2013; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required to answer the questio"
P18-1160,P17-2081,1,0.851718,"; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required to answer the question, (ii) we choose t"
P18-1160,D14-1162,1,0.109605,"Missing"
P18-1160,D17-1111,0,0.0307052,"l., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required to answer the question, (ii) we choose the minimal context by selecting variable number of sentences for each question, while they use a fixed size of number as a hyperparameter, (iii) our framework is flexible in that it does not require end-to-end training and can be combined with existing QA models, and (iv) they do not show robustness to adversarial inputs. 6 Conclus"
P18-1160,D16-1264,0,0.788517,"d inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs. 1 Introduction The task of textual question answering (QA), in which a machine reads a document and answers a question, is an important and challenging problem in natural language processing. Recent progress in performance of QA models has been largely due to the variety of available QA datasets (Richardson et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Joshi et al., 2017; Koˇcisk`y et al., 2017). ∗ All work was done while the author was an intern at Salesforce Research. Many neural QA models have been proposed for these datasets, the most successful of which tend to leverage coattention or bidirectional attention mechanisms that build codependent representations of the document and the question (Xiong et al., 2018; Seo et al., 2017). Yet, learning the full context over the document is challenging and inefficient. In particular, when the model is given a long document, or multiple documents, learning the full context"
P18-1160,D13-1020,0,0.302388,"cant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs. 1 Introduction The task of textual question answering (QA), in which a machine reads a document and answers a question, is an important and challenging problem in natural language processing. Recent progress in performance of QA models has been largely due to the variety of available QA datasets (Richardson et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Joshi et al., 2017; Koˇcisk`y et al., 2017). ∗ All work was done while the author was an intern at Salesforce Research. Many neural QA models have been proposed for these datasets, the most successful of which tend to leverage coattention or bidirectional attention mechanisms that build codependent representations of the document and the question (Xiong et al., 2018; Seo et al., 2017). Yet, learning the full context over the document is challenging and inefficient. In particular, when the model is given a long document, or"
P18-1160,K17-1028,0,0.0463791,"Missing"
P18-1160,D15-1237,0,0.0517367,"over a large set of documents in a closed (Joshi et al., 2017) or open-domain (Dunn et al., 2017; Berant et al., 2013; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are"
P18-1160,Q16-1019,0,0.0357082,"(Dunn et al., 2017; Berant et al., 2013; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required"
P18-1160,D17-1082,0,\N,Missing
P19-1078,D18-1398,0,0.0366563,"y. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limit"
P19-1078,P16-1014,0,0.0299372,"ded dialogue history is represented as Ht = enc |Xt |×dhdd , where d [henc hdd is the 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal"
P19-1078,K16-1002,0,0.113926,"Missing"
P19-1078,W14-4337,0,0.761555,"lid arrows on the left are the single-turn mapping, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST)"
P19-1078,D18-1547,0,0.333872,"For example, as shown in Fig. 1, (slot, value) pairs such as (price, cheap) and (area, centre) are extracted from the conversation. Accurate DST performance is crucial for ∗ Work partially done while the first author was an intern at Salesforce Research. 808 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808–819 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics of possible values. Therefore, many of the previous works that are based on neural classification models may not be applicable in real scenario. Budzianowski et al. (2018) recently introduced a multi-domain dialogue dataset (MultiWOZ), which adds new challenges in DST due to its mixed-domain conversations. As shown in Fig. 1, a user can start a conversation by asking to reserve a restaurant, then requests information regarding an attraction nearby, and finally asks to book a taxi. In this case, the DST model has to determine the corresponding domain, slot and value at each turn of dialogue, which contains a large number of combinations in the ontology, i.e., 30 (domain, slot) pairs and over 4,500 possible slot values in total. Another challenge in the multidoma"
P19-1078,W14-4340,0,0.678325,"lid arrows on the left are the single-turn mapping, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST)"
P19-1078,N18-2115,0,0.0286875,"th only two possible values in the ontology. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications"
P19-1078,P18-5002,0,0.0582567,"Missing"
P19-1078,D18-1299,0,0.504676,"Missing"
P19-1078,P18-1133,0,0.202571,"eatures and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share parameters between slots, and Zhong et al. (2018) uses slot-specific local modules to learn slot features, which has proved to successfully improve tracking of rare slot values. Lei et al. (2018) use a Seq2Seq model to generate belief spans and the delexicalized response at the same time. Ren et al. (2018) propose StateNet that generates a dialogue history representation and compares the distances between this representation and value vectors in the candidate set. Xu and Hu (2018) use the index-based pointer network for different slots, and show the ability to point to unknown values. However, many of them require a predefined domain ontology, and the models were only evaluated on single-domain setting (DSTC2). For multi-domain DST, Rastogi et al. (2017) propose a multi-domain approac"
P19-1078,P18-1136,1,0.873258,"represented as Ht = enc |Xt |×dhdd , where d [henc hdd is the 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal and the true words Y l"
P19-1078,P17-1099,0,0.030768,"he 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal and the true words Y label is used. We tween Pjk j define Lv as sum of two dis"
P19-1078,D16-1022,0,0.0865702,"Missing"
P19-1078,P17-1163,0,0.574186,"Missing"
P19-1078,D17-1314,0,0.0174056,"earning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are diffic"
P19-1078,P17-2023,0,0.0220702,"earning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are diffic"
P19-1078,D14-1162,1,0.106752,"five domains. The numbers in the last three rows indicate the number of dialogues for train, validation and test sets. Fi (Θi − ΘS,i )2 , (8) MinimizeΘ L(Θ) Hotel price, type, parking, stay, day, people, area, stars, internet, name 3381 416 394 4.2 Training Details Multi-domain Joint Training The model is trained end-to-end using the Adam optimizer (Kingma and Ba, 2015) with a batch size of 32. The learning rate annealing is in the range of [0.001, 0.0001] with a dropout ratio of 0.2. Both α and β in Eq (7) are set to one. All the embeddings are initialized by concatenating Glove embeddings (Pennington et al., 2014) and character embeddings (Hashimoto et al., 2016), where the dimension is 400 for each vocabulary word. A greedy search decoding strategy is used for our state generator since the generated slot values are usually short in length. In addition, to in(9) where L(Θ, K) is the loss value of the K stored samples. Lopez-Paz et al. (2017) show how to solve the optimization problem in Eq (9) with quadratic programming if the loss of the stored samples increases. 812 crease model generalization and simulate an outof-vocabulary setting, a word dropout is utilized with the utterance encoder by randomly"
P19-1078,P18-2069,0,0.230289,"tes a dialogue history representation and compares the distances between this representation and value vectors in the candidate set. Xu and Hu (2018) use the index-based pointer network for different slots, and show the ability to point to unknown values. However, many of them require a predefined domain ontology, and the models were only evaluated on single-domain setting (DSTC2). For multi-domain DST, Rastogi et al. (2017) propose a multi-domain approach using two-layer bi-GRU. Although it does not need an ad-hoc state update mechanism, it relies on delexicalization to extract the features. Ramadan et al. (2018) propose a model to jointly track domain and the dialogue states using multiple bi-LSTM. They utilize semantic similarity between utterances and the ontology terms and allow the information to be shared across domains. For a more general overview, readers may refer to the neural dialogue review paper from Gao et al. (2018). 7 Conclusion We introduce a transferable dialogue state generator for multi-domain dialogue state tracking, which learns to track states without any predefined domain ontology. TRADE shares all of its parameters across multiple domains and achieves stateof-the-art joint goa"
P19-1078,W13-4067,0,0.279858,"al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share"
P19-1078,E17-1042,0,0.323621,"Missing"
P19-1078,W14-4339,0,0.101165,", 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share parameters betwe"
P19-1078,P17-1062,0,0.0575854,"ple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representatio"
P19-1078,P18-1134,0,0.500512,"for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST) is a core component in task-oriented dialogue systems, such as restaurant reservation or ticket booking. The goal of DST is to extract user goals/intentions expressed d"
P19-1078,N18-1109,0,0.0241526,"type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opin"
P19-1078,W18-5001,0,0.0369032,"nd, number-related slots such as arrive by, people, and stay usually have the lowest error rates. We also find that the type slot of hotel domain has a high error rate, even if it is an easy task with only two possible values in the ontology. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al."
P19-1078,P18-1135,1,0.917453,"ng, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST) is a core component in task-oriented dialogu"
P19-1443,W06-3000,0,0.242624,"he development in dialogue (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018) uses predefined ontology and slot-value pairs with limited natural language meaning representation, whereas we focus on general SQL queries that enable more powerful semantic meaning representation. Recently, some conversational question answering datasets have been introduced, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018). They differ from SParC in that the answers are free-form text instead of SQL queries. On the other hand, Kato et al. (2004); Chai and Jin (2004); Bertomeu et al. (2006) conduct early studies of the contextual phenomena and thematic relations in database dialogue/QA systems, which we use as references when constructing SParC. 3 Data Collection We create the SParC dataset in four stages: selecting interaction goals, creating questions, annotating SQL representations, and reviewing. Interaction goal selection To ensure thematic relevance within each question sequence, we use questions in the original Spider dataset as the thematic guidance for constructing meaningful query interactions, i.e. the interaction goal. Each sequence is based on a question in Spider a"
P19-1443,W04-2504,0,0.426463,"atabase (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The users may explicitly refer to or omit previously mentioned entities and constraints, and may introduce refinements, additions or substitutions to what has already been said (Figure 1). This requires a practical text-to-SQL system to effectively process context information to synthesize the correct SQL logic. To enable modeling advances in contextdependent semantic parsing, we introduce SParC (cross-domain Semantic Parsing in Context), an expert-labeled dataset which contains 4,298 coherent question sequences (12k+ questions paired with SQL queries) querying 200 co"
P19-1443,H94-1010,0,0.882449,"ese work focus on precisely mapping stand-alone utterances to SQL queries, generating SQL queries in a context-dependent scenario (Miller et al., 1996; Zettlemoyer and 4511 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511–4523 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collins, 2009; Suhr et al., 2018) has been studied less often. The most prominent context-dependent text-to-SQL benchmark is ATIS1 , which is set in the flight-booking domain and contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The us"
P19-1443,P16-1004,0,0.0694107,"atabases have to address. In addition, it enables us to test the generalization of the trained systems to unseen databases and domains. We asked 15 college students with SQL experience to come up with question sequences over the Spider databases (§ 3). Questions in the original Spider dataset were used as guidance to the students for constructing meaningful interactions: each sequence is based on a question in Spider and the student has to ask inter-related questions to ob1 A subset of ATIS is also frequently used in contextindependent semantic parsing research (Zettlemoyer and Collins, 2007; Dong and Lapata, 2016). 2 The data is available at https://yale-lily. github.io/spider. tain information that answers the Spider question. At the same time, the students are encouraged to come up with related questions which do not directly contribute to the Spider question so as to increase data diversity. The questions were subsequently translated to complex SQL queries by the same student. Similar to Spider, the SQL Queries in SParC cover complex syntactic structures and most common SQL keywords. We split the dataset such that a database appears in only one of the train, development and test sets. We provide det"
P19-1443,P18-1068,0,0.0403454,"at there is plenty of room for advancement in modeling and learning on the SParC dataset. 2 Related Work Context-independent semantic parsing Early studies in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Li and Jagadish, 2014; Pasupat and Liang, 2015; Dong and Lapata, 2016; Iyer et al., 2017) were based on small and singledomain datasets such as ATIS (Hemphill et al., 1990; Dahl et al., 1994) and GeoQuery (Zelle and Mooney, 1996). Recently, an increasing number of neural approaches (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018a; Dong and Lapata, 2018; Yu et al., 2018b) have started to use large and crossdomain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c). Most of them focus on converting stand-alone natural language questions to executable queries. Table 1 compares SParC with other semantic parsing datasets. Context-dependent semantic parsing with SQL labels Only a few datasets have been constructed for the purpose of mapping contextdependent questions to structured queries. 3 Exact string match ignores ordering discrepancies of SQL components whose order does not matter. Exact set matching is ab"
P19-1443,H90-1021,0,0.896741,"Missing"
P19-1443,W14-4337,0,0.0264241,"Missing"
P19-1443,P17-1089,0,0.361004,"Missing"
P19-1443,P17-1167,0,0.14015,"Missing"
P19-1443,W04-2509,0,0.345013,"contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The users may explicitly refer to or omit previously mentioned entities and constraints, and may introduce refinements, additions or substitutions to what has already been said (Figure 1). This requires a practical text-to-SQL system to effectively process context information to synthesize the correct SQL logic. To enable modeling advances in contextdependent semantic parsing, we introduce SParC (cross-domain Semantic Parsing in Context), an expert-labeled dataset which contains 4,298 coherent question sequences (12k+ questions paired with SQL quer"
P19-1443,P16-1138,0,0.462801,"rsing datasets. Context-dependent semantic parsing with SQL labels Only a few datasets have been constructed for the purpose of mapping contextdependent questions to structured queries. 3 Exact string match ignores ordering discrepancies of SQL components whose order does not matter. Exact set matching is able to consider ordering issues in SQL evaluation. See more evaluation details in section 6.1. 4512 Dataset SParC ATIS (Hemphill et al., 1990; Dahl et al., 1994) Spider (Yu et al., 2018c) WikiSQL (Zhong et al., 2017) GeoQuery (Zelle and Mooney, 1996) SequentialQA (Iyyer et al., 2017) SCONE (Long et al., 2016) Context X X 7 7 7 X X Resource database database database table database table environment Annotation SQL SQL SQL SQL SQL denotation denotation Cross-domain X 7 X X 7 X 7 Table 1: Comparison of SParC with existing semantic parsing datasets. Hemphill et al. (1990); Dahl et al. (1994) collected the contextualized version of ATIS that includes series of questions from users interacting with a flight database. Adopted by several works later on (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018), ATIS has only a single domain for flight planning which limits the possible SQL lo"
P19-1443,P96-1008,0,0.654289,"Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work focus on precisely mapping stand-alone utterances to SQL queries, generating SQL queries in a context-dependent scenario (Miller et al., 1996; Zettlemoyer and 4511 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511–4523 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collins, 2009; Suhr et al., 2018) has been studied less often. The most prominent context-dependent text-to-SQL benchmark is ATIS1 , which is set in the flight-booking domain and contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to ac"
P19-1443,P17-1163,0,0.0488151,"Missing"
P19-1443,P15-1142,0,0.0459989,"asets used in recovering context-dependent meaning (including SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017)) contain no logical form annotations but only denotation (Berant and Liang, 2014) instead. SCONE (Long et al., 2016) contains some instructions in limited domains such as chemistry experiments. The formal representations in the dataset are world states representing state changes after each instruction instead of programs or logical forms. SequentialQA (Iyyer et al., 2017) was created by asking crowd workers to decompose some complicated questions in WikiTableQuestions (Pasupat and Liang, 2015) into sequences of inner-related simple questions. As shown in Table 1, neither of the two datasets were annotated with query labels. Thus, to make the tasks feasible, SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017) exclude many questions with rich semantic and contextual types. For example, (Iyyer et al., 2017) requires that the answers to the questions in SequentialQA must appear in the table, and most of them can be solved by simple SQL queries with SELECT and WHERE clauses. Such direct mapping without formal query labels becomes unfeasible for complex questions. Furthermore"
P19-1443,D14-1162,1,0.110555,"ts corresponding table name and column name separated by a special dot token (i.e., table name.column name), and use the average word embedding10 of tokens in this sequence as the column header embedding hC . Decoder The decoder is implemented with another LSTM (LSTMD ) with attention to the LSTME representations of the questions in η previous turns. At each decoding step, the decoder chooses to generate either a SQL keyword (e.g., select, where, group by) or a column header. To achieve this, we use separate layers to score SQL keywords and column headers, 10 We use the 300-dimensional GloVe (Pennington et al., 2014) pretrained word embeddings. 4517 Model and finally use the softmax operation to generate the output probability distribution over both categories. 5.2 SyntaxSQLNet with history input (SyntaxSQL-con) SyntaxSQLNet is a syntax tree based neural model for the complex and cross-domain contextindependent text-to-SQL task introduced by Yu et al. (2018b). The model consists of a table-aware column attention encoder and a SQL-specific syntax tree-based decoder. The decoder adopts a set of inter-connected neural modules to generate different SQL syntax components. We extend this model by providing the"
P19-1443,N18-1203,0,0.332817,"Missing"
P19-1443,D07-1071,0,0.146156,"atural language interfaces to databases have to address. In addition, it enables us to test the generalization of the trained systems to unseen databases and domains. We asked 15 college students with SQL experience to come up with question sequences over the Spider databases (§ 3). Questions in the original Spider dataset were used as guidance to the students for constructing meaningful interactions: each sequence is based on a question in Spider and the student has to ask inter-related questions to ob1 A subset of ATIS is also frequently used in contextindependent semantic parsing research (Zettlemoyer and Collins, 2007; Dong and Lapata, 2016). 2 The data is available at https://yale-lily. github.io/spider. tain information that answers the Spider question. At the same time, the students are encouraged to come up with related questions which do not directly contribute to the Spider question so as to increase data diversity. The questions were subsequently translated to complex SQL queries by the same student. Similar to Spider, the SQL Queries in SParC cover complex syntactic structures and most common SQL keywords. We split the dataset such that a database appears in only one of the train, development and t"
P19-1443,P09-1110,0,0.281655,"l., 2018c) WikiSQL (Zhong et al., 2017) GeoQuery (Zelle and Mooney, 1996) SequentialQA (Iyyer et al., 2017) SCONE (Long et al., 2016) Context X X 7 7 7 X X Resource database database database table database table environment Annotation SQL SQL SQL SQL SQL denotation denotation Cross-domain X 7 X X 7 X 7 Table 1: Comparison of SParC with existing semantic parsing datasets. Hemphill et al. (1990); Dahl et al. (1994) collected the contextualized version of ATIS that includes series of questions from users interacting with a flight database. Adopted by several works later on (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018), ATIS has only a single domain for flight planning which limits the possible SQL logic it contains. In contrast to ATIS, SParC consists of a large number of complex SQL queries (with most SQL syntax components) inquiring 200 databases in 138 different domains, which contributes to its diversity in query semantics and contextual dependencies. Similar to Spider, the databases in the train, development and test sets of SParC do not overlap. Context-dependent semantic parsing with denotations Some datasets used in recovering context-dependent meaning (including SCONE (Long et"
P19-1443,P18-1135,1,0.81917,"uery labels becomes unfeasible for complex questions. Furthermore, SequentialQA contains questions based only on a single Wikipedia tables at a time. In contrast, SParC contains 200 significantly larger databases, and complex query labels with all common SQL key components. This requires a system developed for SParC to handle information needed over larger databases in different domains. Conversational QA and dialogue system Language understanding in context is also studied for dialogue and question answering systems. The development in dialogue (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018) uses predefined ontology and slot-value pairs with limited natural language meaning representation, whereas we focus on general SQL queries that enable more powerful semantic meaning representation. Recently, some conversational question answering datasets have been introduced, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018). They differ from SParC in that the answers are free-form text instead of SQL queries. On the other hand, Kato et al. (2004); Chai and Jin (2004); Bertomeu et al. (2006) conduct early studies of the contextual phenomena and thematic relations in database di"
P19-1443,N18-2093,1,0.869956,"ORDER BY Introduction date_became_customer DESC LIMIT 1 Querying a relational database is often challenging and a natural language interface has long been regarded by many as the most powerful database interface (Popescu et al., 2003; Bertomeu et al., 2006; Li and Jagadish, 2014). The problem of mapping a natural language utterance into executable SQL queries (text-to-SQL) has attracted increasing attention from the semantic parsing community by virtue of a continuous effort of dataset creation (Zelle and Mooney, 1996; Iyyer et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a) and the modeling innovation that follows it (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018b; Shi et al., 2018). Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work f"
P19-1443,D18-1193,1,0.919435,"ORDER BY Introduction date_became_customer DESC LIMIT 1 Querying a relational database is often challenging and a natural language interface has long been regarded by many as the most powerful database interface (Popescu et al., 2003; Bertomeu et al., 2006; Li and Jagadish, 2014). The problem of mapping a natural language utterance into executable SQL queries (text-to-SQL) has attracted increasing attention from the semantic parsing community by virtue of a continuous effort of dataset creation (Zelle and Mooney, 1996; Iyyer et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a) and the modeling innovation that follows it (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018b; Shi et al., 2018). Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work f"
P19-1443,W06-3001,0,\N,Missing
P19-1443,Q13-1005,0,\N,Missing
P19-1443,P14-1133,0,\N,Missing
P19-1443,P18-1033,1,\N,Missing
P19-1443,D18-1241,0,\N,Missing
P19-1487,D15-1075,0,0.055723,"omplete phrases in the input text that by itself is sufficient to predict the desired output. Humangenerated natural language explanations for classification data have been used in the past to train a semantic parser that in turn generates more noisy 4933 labeled data which can used to train a classifier (Hancock et al., 2018). Camburu et al. (2018) generate explanations and predictions for the natural language inference problem (Camburu et al., 2018). However, the authors report that interpretability comes at the cost of loss in performance on the popular Stanford Natural Language Inference (Bowman et al., 2015) dataset. We find that, unlike for e-SNLI, explanations for CQA lead to improved performance in what Camburu et al. (2018) would call the explain-predict setting. In the multi-modal setting, Rajani and Mooney (2018) showed that visual explanations can be leveraged to improve performance of VQA (Antol et al., 2015) and that an ensemble explanation is significantly better than individual explanations using both automated and human evaluations (Rajani and Mooney, 2017). Knowledge Transfer in NLP Natural language processing has often relied on the transfer of world-knowledge through pretrained wor"
P19-1487,D17-1070,0,0.0529786,"Missing"
P19-1487,N19-1423,0,0.641334,"onsense reasoning Datasets that require models to learn to predict relations between situations or events in natural language have been introduced in the recent past. The Story Cloze (also referred to as ROC Stories) involves predicting the correct story ending from a set of plausible endings (Mostafazadeh et al., 2016) while the Situations with Adversarial Generations (SWAG) involves predicting the next scene based on an initial event (Zellers et al., 2018). Language Modeling based techniques such as the GPT and BERT models get human-level performance on these datasets (Radford et al., 2018; Devlin et al., 2019). They have been less successful on tasks that require clear understanding of how pronouns resolve between sentences and how that interacts with world knowledge. For example, the Winograd Schemas (Winograd, 1972) and challenges derived from that format (Levesque et al., 2012; McCann et al., 2018; Wang et al., 2018) have proven difficult for even the most modern machine learning methods (Trinh and Le, 2018) to achieve near-human performance, but the emphasis on pronoun resolution in those challenges leaves room for exploration of other kinds of commonsense reasoning. CQA is a new dataset that c"
P19-1487,P18-1175,0,0.132063,"Missing"
P19-1487,marelli-etal-2014-sick,0,0.0275802,"Missing"
P19-1487,N16-1098,0,0.0255083,". We demonstrate explanation transfer on two out-of-domain datasets. Note that before our final submission, the organizers released a more challenging v1.11 of CQA with 5 answer choices instead of 3 and so we also included the new version in our results and discussions. 2 Background and Related Work Commonsense reasoning Datasets that require models to learn to predict relations between situations or events in natural language have been introduced in the recent past. The Story Cloze (also referred to as ROC Stories) involves predicting the correct story ending from a set of plausible endings (Mostafazadeh et al., 2016) while the Situations with Adversarial Generations (SWAG) involves predicting the next scene based on an initial event (Zellers et al., 2018). Language Modeling based techniques such as the GPT and BERT models get human-level performance on these datasets (Radford et al., 2018; Devlin et al., 2019). They have been less successful on tasks that require clear understanding of how pronouns resolve between sentences and how that interacts with world knowledge. For example, the Winograd Schemas (Winograd, 1972) and challenges derived from that format (Levesque et al., 2012; McCann et al., 2018; Wan"
P19-1487,P02-1040,0,0.102975,"Missing"
P19-1487,D14-1162,1,0.106837,"r CQA lead to improved performance in what Camburu et al. (2018) would call the explain-predict setting. In the multi-modal setting, Rajani and Mooney (2018) showed that visual explanations can be leveraged to improve performance of VQA (Antol et al., 2015) and that an ensemble explanation is significantly better than individual explanations using both automated and human evaluations (Rajani and Mooney, 2017). Knowledge Transfer in NLP Natural language processing has often relied on the transfer of world-knowledge through pretrained word vectors like Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Contextualized word vectors (McCann et al., 2017; Peters et al., 2018) refined these representations for particular inputs by using different forms of general encoding. Language models trained from scratch on large amounts of data have made groundbreaking success in this direction by carefully finetuning for specific tasks (Dai and Le, 2015; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019). These models have the advantage that only a few parameters need to be learned from scratch and thus perform surprisingly well even on small amounts of supervised data. Fine-tuned languag"
P19-1487,P18-1031,0,0.020347,"ni and Mooney, 2017). Knowledge Transfer in NLP Natural language processing has often relied on the transfer of world-knowledge through pretrained word vectors like Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Contextualized word vectors (McCann et al., 2017; Peters et al., 2018) refined these representations for particular inputs by using different forms of general encoding. Language models trained from scratch on large amounts of data have made groundbreaking success in this direction by carefully finetuning for specific tasks (Dai and Le, 2015; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019). These models have the advantage that only a few parameters need to be learned from scratch and thus perform surprisingly well even on small amounts of supervised data. Fine-tuned language models do not however work as well for directly predicting answers for CQA (Talmor et al., 2019). In our work, we show how these finetuned language models are more effective when leveraged to generate explanations and empirically prove that they also linguistically capture common sense. 3 Common Sense Explanations (CoS-E) We used Amazon Mechanical Turk (MTurk) to collect explanations f"
P19-1487,N18-1202,0,0.362017,"oS-E)1 . CoS-E contains human explanations in 1 the form of both open-ended natural language explanations as well as highlighted span annotations that represent words selected by humans as important for predicting the right answer (see Table 1). Talmor et al. (2019) show that using Google search to extract context from top 100 result snippets for each of the question and answer choices does not help much in improving the accuracy on CQA trained using even the state-of-the-art reading comprehension model BiDAF++ (Seo et al., 2017) augmented with a self-attention layer and ELMo representations (Peters et al., 2018). In contrast, we leverage a pretrained language model to generate explanations that are useful for commonsense reasoning. We propose Commonsense Auto-Generated Explanations (CAGE) as a framework for generating explanations for CQA. We break down the task of commonsense reasoning into two phases. In the first phase, we provide a CQA example alongside the corresponding CoS-E explanation to a language model. The language model conditions on the question and answer choices from the example and is trained to generate the CoS-E explanation. In the second phase, we use the language model https://git"
P19-1487,D16-1011,0,0.183881,"Missing"
P19-1487,N18-1201,1,0.886933,"Missing"
P19-1487,N19-1421,0,0.589924,"dels for commonsense reasoning. 1 Question: After getting drunk people couldn’t understand him,it was because of his what? Choices: lower standards,slurred speech, or falling down CoS-E: People who are drunk have difficulty speaking. Question: People do what during their time off from work? Choices: take trips, brow shorter, or become hysterical CoS-E: People usually do something relaxing, such as taking trips,when they don’t need to work. Table 1: Examples from our CoS-E dataset. Introduction Commonsense reasoning is a challenging task for modern machine learning methods (Zhong et al., 2018; Talmor et al., 2019). Explanations are a way to verbalize the reasoning that the models learn during training. Common sense Question Answering (CQA) is a multiple-choice question answering dataset proposed for developing natural language processing (NLP) models with commonssense reasoning capabilities (Talmor et al., 2019). Although these efforts have led to progress, it is still unclear how these models perform reasoning and to what extent that reasoning is based on world knowledge. We collect human explanations for commonsense reasoning built on top of CQA and introduce them as Common Sense Explanations (CoS-E)"
P19-1487,W18-5446,0,0.0554676,"Missing"
P19-1487,H89-1033,0,0.788183,"involves predicting the correct story ending from a set of plausible endings (Mostafazadeh et al., 2016) while the Situations with Adversarial Generations (SWAG) involves predicting the next scene based on an initial event (Zellers et al., 2018). Language Modeling based techniques such as the GPT and BERT models get human-level performance on these datasets (Radford et al., 2018; Devlin et al., 2019). They have been less successful on tasks that require clear understanding of how pronouns resolve between sentences and how that interacts with world knowledge. For example, the Winograd Schemas (Winograd, 1972) and challenges derived from that format (Levesque et al., 2012; McCann et al., 2018; Wang et al., 2018) have proven difficult for even the most modern machine learning methods (Trinh and Le, 2018) to achieve near-human performance, but the emphasis on pronoun resolution in those challenges leaves room for exploration of other kinds of commonsense reasoning. CQA is a new dataset that consists of 9500 questions with one correct answer and two distractor answers (Talmor et al., 2019). The authors claim that because all the answer choices are drawn from the same source concept, the dataset requir"
P19-1487,D18-1009,0,0.0407981,"ging v1.11 of CQA with 5 answer choices instead of 3 and so we also included the new version in our results and discussions. 2 Background and Related Work Commonsense reasoning Datasets that require models to learn to predict relations between situations or events in natural language have been introduced in the recent past. The Story Cloze (also referred to as ROC Stories) involves predicting the correct story ending from a set of plausible endings (Mostafazadeh et al., 2016) while the Situations with Adversarial Generations (SWAG) involves predicting the next scene based on an initial event (Zellers et al., 2018). Language Modeling based techniques such as the GPT and BERT models get human-level performance on these datasets (Radford et al., 2018; Devlin et al., 2019). They have been less successful on tasks that require clear understanding of how pronouns resolve between sentences and how that interacts with world knowledge. For example, the Winograd Schemas (Winograd, 1972) and challenges derived from that format (Levesque et al., 2012; McCann et al., 2018; Wang et al., 2018) have proven difficult for even the most modern machine learning methods (Trinh and Le, 2018) to achieve near-human performanc"
W19-5212,P07-2045,0,0.00670993,"Missing"
W19-5212,P17-2021,0,0.0361063,"ext translation without XML. To compute the BLEU scores, we use language-specific tokenizers; for example, we use Kytea (Neubig et al., 2011) for Simplified Chinese and Japanese, and the Moses (Koehn et al., 2007) tokenizer for English, Dutch, Finnish, French, German, and Russian. 3.1 The task in our dataset is to translate text with structured information, and therefore we consider using syntax-based NMT models. A possible approach is incorporating parse trees or parsing algorithms into NMT models (Eriguchi et al., 2016, 2017), and another is using sequential models on linearized structures (Aharoni and Goldberg, 2017). We employ the latter approach to incorporate source-side and target-side XML structures, and note that this allows using standard sequenceto-sequence models without modification. We have a set of parallel text segments for a language pair (X , Y), and the task is translating a text segment x ∈ X to another y ∈ Y. Each x in the dataset is represented with a sequence of tokens including some XML tags: x = [x1 , x2 , . . . , xN ], where N is the length of the sequence. Its corresponding reference y is also represented with a sequence of tokens: y = [y1 , y2 , . . . , yM ], where M is the sequen"
W19-5212,D18-2012,0,0.0151095,"recover the NE&NUM scores, especially for the recall. We have also observed that using beam search, which improves BLEU scores, degrades the NE&NUM scores. A lesson learned from these results is that work to improve BLEU scores can sometimes lead to degradation of other important metrics. This section describes our experimental settings. We will release the preprocessing scripts and the training code (implemented with PyTorch) upon publication. More details are described in the supplementary material. 4.1 Model Configurations Tokenization and Detokenization We used the SentencePiece toolkit (Kudo and Richardson, 2018) for sub-word tokenization and detokenization for the NMT outputs. Without XML tags If we remove all the XML tags from our dataset, the task becomes a plain MT task. We carried out our baseline experiments for the plain text translation task, and for each language pair we trained a joint SentencePiece model to obtain its shared sub-word vocabulary. For training each NMT model, we used training examples whose maximum token length is 100. With XML tags For our XML-based experiments, we also trained a joint SentencePiece model for each language pair, where one important note is that all the XML t"
W19-5212,N18-1118,0,0.017515,"heir tail4 has text, to avoid missing phrases within sentences. Next, we remove the root tag from each translation pair, because the correspondence is obvious. We also remove fine-grained information such as attributes in the XML tags for the dataset; from the viewpoint of real-world usage, we can recover (or copy) the missing information as a post-processing step. As a result of this process, a translation pair can consist of multiple sentences as shown in Example (c) of Figure 1. We do not split them into single sentences, considering a recent trend of context-sensitive machine translation (Bawden et al., 2018; M¨uller et al., 2018; Zhang et al., 2018; Miculicich et al., 2018). One can use split sentences for training a model, but an important note is that there is no guarantee that all the internal sentences are perfectly aligned. We note that this structure-based alignment process means we do not rely on statistical alignment models to construct our parallel datasets. Text alignment Figure 3 shows how to extract parallel text segments based on the tag categorization. There are three aligned translatable tags, and they result in three separate translation pairs. The note tag is translatable, so th"
W19-5212,P18-2059,0,0.0357865,"Missing"
W19-5212,P14-5010,0,0.00428096,"Missing"
W19-5212,P16-1078,1,0.845706,"then evaluate BLEU. The metric is compatible with the case where we use the dataset for plain text translation without XML. To compute the BLEU scores, we use language-specific tokenizers; for example, we use Kytea (Neubig et al., 2011) for Simplified Chinese and Japanese, and the Moses (Koehn et al., 2007) tokenizer for English, Dutch, Finnish, French, German, and Russian. 3.1 The task in our dataset is to translate text with structured information, and therefore we consider using syntax-based NMT models. A possible approach is incorporating parse trees or parsing algorithms into NMT models (Eriguchi et al., 2016, 2017), and another is using sequential models on linearized structures (Aharoni and Goldberg, 2017). We employ the latter approach to incorporate source-side and target-side XML structures, and note that this allows using standard sequenceto-sequence models without modification. We have a set of parallel text segments for a language pair (X , Y), and the task is translating a text segment x ∈ X to another y ∈ Y. Each x in the dataset is represented with a sequence of tokens including some XML tags: x = [x1 , x2 , . . . , xN ], where N is the length of the sequence. Its corresponding referenc"
W19-5212,D18-1050,0,0.0617295,"r services for new markets, and human professionals typically perform the translation with the help of a translation memory (Silvestre Baquero and Mitkov, 2017) to increase efficiency and maintain consistent termiIntroduction Machine translation is a fundamental research area in the field of natural language processing (NLP). To build a machine learning-based translation system, we usually need a large amount of bilingually-aligned text segments. Examples of widely-used datasets are those included in WMT (Bojar et al., 2018) and LDC1 , while new evaluation datasets are being actively created (Michel and Neubig, 2018; Bawden et al., ∗ 1 Now at Google Brain. https://www.ldc.upenn.edu/ 116 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 116–127 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics use of structured formatting (using XML) to convey information to readers, so the translators have aimed to ensure consistency of formatting and markup structure, not just text content, between languages. nology. Explicitly handling such structured text can help bring the benefits of state-of-the-art machine translation models to add"
W19-5212,P17-2012,0,0.0273173,"Missing"
W19-5212,D18-1325,0,0.0286765,"Next, we remove the root tag from each translation pair, because the correspondence is obvious. We also remove fine-grained information such as attributes in the XML tags for the dataset; from the viewpoint of real-world usage, we can recover (or copy) the missing information as a post-processing step. As a result of this process, a translation pair can consist of multiple sentences as shown in Example (c) of Figure 1. We do not split them into single sentences, considering a recent trend of context-sensitive machine translation (Bawden et al., 2018; M¨uller et al., 2018; Zhang et al., 2018; Miculicich et al., 2018). One can use split sentences for training a model, but an important note is that there is no guarantee that all the internal sentences are perfectly aligned. We note that this structure-based alignment process means we do not rely on statistical alignment models to construct our parallel datasets. Text alignment Figure 3 shows how to extract parallel text segments based on the tag categorization. There are three aligned translatable tags, and they result in three separate translation pairs. The note tag is translatable, so the entire element is 4 For example, the tail of the xref tag in the E"
W19-5212,W18-6307,0,0.0431427,"Missing"
W19-5212,P11-2093,0,0.0190521,"nclude the most widely-used metric, BLEU, without XML tags. 119 how to handle our dataset with a sequential NMT model. We then propose a simple constrained beam search for accurately generating XML structures conditioned by source information. We further incorporate multiple copy mechanisms to strengthen the baselines. That is, we remove all the XML tags covered by our dataset and then evaluate BLEU. The metric is compatible with the case where we use the dataset for plain text translation without XML. To compute the BLEU scores, we use language-specific tokenizers; for example, we use Kytea (Neubig et al., 2011) for Simplified Chinese and Japanese, and the Moses (Koehn et al., 2007) tokenizer for English, Dutch, Finnish, French, German, and Russian. 3.1 The task in our dataset is to translate text with structured information, and therefore we consider using syntax-based NMT models. A possible approach is incorporating parse trees or parsing algorithms into NMT models (Eriguchi et al., 2016, 2017), and another is using sequential models on linearized structures (Aharoni and Goldberg, 2017). We employ the latter approach to incorporate source-side and target-side XML structures, and note that this allo"
W19-5212,Q17-1024,0,0.0919077,"Missing"
W19-5212,D16-1244,0,0.0477508,"Missing"
W19-5212,E17-2025,0,0.0197621,"y adding a “null” token in natural language inference (Parikh et al., 2016). We then define attention scores between y hK (yj ) and the expanded c: a(j, i) = score(hyK (yj ), ci , c), where the normalized scoring function score is implemented as a singlehead attention model proposed in Vaswani et al. (2017). If the next reference token yj+1 is not included in the copy target sequence, the loss function is defined as follows: (3) j=1 where we assume that y1 is a special token BOS to indicate the beginning of the sequence, and yM is an end-of-sequence token EOS. Following Inan et al. (2017) and Press and Wolf (2017), we use W as an embedding matrix, and we share the single vocabulary V for both X and Y. That is, each of v(xi ) or v(yj ) is equivalent to a row vector in W . 3.2 XML-Constrained Beam Search At test time, standard sequence-to-sequence generation methods do not always output valid XML structures, and even if an output is a valid XML structure, it does not always match the tag set of its source-side XML structure. To generate source-conditioned XML structures as accurately as possible, we propose a simple constrained beam search method. We add three constrains to a standard beam search method."
W19-5212,P17-1099,0,0.397148,"d as hy0 (yj ) = We use NMT models to provide competitive baselines for our dataset. This section first describes 120 √ d · v(yj ) + e(j). For more details about the parameterized functions f and g, and the positional embeddings, please refer to Vaswani et al. (2017). Then hyK (yj ) is used to predict the next token w by a softmax layer: pg (w|x, y≤j ) = softmax(W hyK (yj ) + b), where W ∈ R|V|×d is a weight matrix, b ∈ R|V |is a bias vector, and V is the vocabulary. The loss function is defined as follows: L(x, y) = − M −1 X log pg (w = yj+1 |x, y≤j ), the general idea of the pointer used in See et al. (2017). For the sake of discrete decisions, we reformulate the pointer method. Following the previous work, we have a sequence of tokens which are targets of our pointer method: c = [c(z1 ), c(z2 ), . . . , c(zU )], where c(zi ) ∈ Rd is a vector representation of the i-th token zi , and U is the sequence length. As in Section 3.1, we have hyK (yj ) to predict the (j + 1)-th token. Before defining an attention mechanism between hyK (yj ) and c, we append a parameterized vector c(z0 ) = c0 to c. We expect c0 to be responsible for decisions of “not copying” tokens, and the idea is inspired by adding a"
W19-5212,silvestre-baquero-mitkov-2017-translation,0,0.140085,"n the Web, is not always stored as plain text, but often wrapped with markup languages to incorporate document structure and metadata such as formatting information. Many companies and software platforms provide online help as Web documents, often translated into different languages to deliver useful information to people in different countries. Translating such Web-structured text is a major component of the process by which companies localize their software or services for new markets, and human professionals typically perform the translation with the help of a translation memory (Silvestre Baquero and Mitkov, 2017) to increase efficiency and maintain consistent termiIntroduction Machine translation is a fundamental research area in the field of natural language processing (NLP). To build a machine learning-based translation system, we usually need a large amount of bilingually-aligned text segments. Examples of widely-used datasets are those included in WMT (Bojar et al., 2018) and LDC1 , while new evaluation datasets are being actively created (Michel and Neubig, 2018; Bawden et al., ∗ 1 Now at Google Brain. https://www.ldc.upenn.edu/ 116 Proceedings of the Fourth Conference on Machine Translation (WMT"
W19-5212,I17-1038,0,0.0142075,"nolingual corpora. Can MT help the localization process? In general, it is encouraging to observe many “4” scores in Figure 8. However, one important note is that it takes significant amount of time for the translators to verify the NMT outputs are good enough. That is, having better scored NMT outputs does not necessarily lead to improving the productivity of the translators; in other words, we need to take into account the time for the quality verification when we consider using our NMT system for that purpose. Previous work has investigated the effectiveness of NMT models for post-editing (Skadina and Pinnis, 2017), but it has not yet been investigated whether using NMT models can improve the translators’ productivity alongside the use of a well-constructed translation memory (Silvestre Baquero and Mitkov, 2017). Therefore, our future work is investigating the effectiveness of using the NMT models in the real-world localization process where a translation memory is available. 6 7 Conclusion We have presented our new dataset for XMLstructured text translation. Our dataset covers 17 languages each of which can be either source or target of machine translation. The dataset is of high quality because it con"
W19-5212,D18-1049,0,0.0488915,"Missing"
