2021.findings-acl.332,Using surprisal and f{MRI} to map the neural bases of broad and local contextual prediction during natural language comprehension,2021,-1,-1,2,0,8278,shohini bhattasali,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.clpsych-1.7,Community-level Research on Suicidality Prediction in a Secure Environment: Overview of the {CLP}sych 2021 Shared Task,2021,-1,-1,5,0,398,sean macavaney,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,0,"Progress on NLP for mental health {---} indeed, for healthcare in general {---} is hampered by obstacles to shared, community-level access to relevant data. We report on what is, to our knowledge, the first attempt to address this problem in mental health by conducting a shared task using sensitive data in a secure data enclave. Participating teams received access to Twitter posts donated for research, including data from users with and without suicide attempts, and did all work with the dataset entirely within a secure computational environment. We discuss the task, team results, and lessons learned to set the stage for future tasks on sensitive or confidential data."
2021.acl-long.126,Syntopical Graphs for Computational Argumentation Tasks,2021,-1,-1,8,1,12878,joe barrow,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Approaches to computational argumentation tasks such as stance detection and aspect detection have largely focused on the text of independent claims, losing out on potentially valuable context provided by the rest of the collection. We introduce a general approach to these tasks motivated by syntopical reading, a reading process that emphasizes comparing and contrasting viewpoints in order to improve topic understanding. To capture collection-level context, we introduce the syntopical graph, a data structure for linking claims within a collection. A syntopical graph is a typed multi-graph where nodes represent claims and edges represent different possible pairwise relationships, such as entailment, paraphrase, or support. Experiments applying syntopical graphs to the problems of detecting stance and aspects demonstrate state-of-the-art performance in each domain, significantly outperforming approaches that do not utilize collection-level information."
2020.nlpcovid19-2.30,Developing a Curated Topic Model for {COVID}-19 Medical Research Literature,2020,-1,-1,1,1,8279,philip resnik,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"Topic models can facilitate search, navigation, and knowledge discovery in large document collections. However, automatic generation of topic models can produce results that fail to meet the needs of users. We advocate for a set of user-focused desiderata in topic modeling for the COVID-19 literature, and describe an effort in progress to develop a curated topic model for COVID-19 articles informed by subject matter expertise and the way medical researchers engage with medical literature."
2020.emnlp-main.137,{I}mproving {N}eural {T}opic {M}odels using {K}nowledge {D}istillation,2020,-1,-1,3,0,7701,alexander hoyle,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics."
2020.acl-main.29,A Joint Model for Document Segmentation and Segment Labeling,2020,-1,-1,6,1,12878,joe barrow,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30{\%} on average, while also improving segment labeling."
2020.acl-main.723,A Prioritization Model for Suicidality Risk Assessment,2020,-1,-1,2,0,23096,hanchin shing,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We reframe suicide risk assessment from social media as a ranking problem whose goal is maximizing detection of severely at-risk individuals given the time available. Building on measures developed for resource-bounded document retrieval, we introduce a well founded evaluation paradigm, and demonstrate using an expert-annotated test collection that meaningful improvements over plausible cascade model baselines can be achieved using an approach that jointly ranks individuals and their social media posts."
W19-3003,{CLP}sych 2019 Shared Task: Predicting the Degree of Suicide Risk in {R}eddit Posts,2019,-1,-1,2,0.952381,16583,ayah zirikly,Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology,0,"The shared task for the 2019 Workshop on Computational Linguistics and Clinical Psychology (CLPsych{'}19) introduced an assessment of suicide risk based on social media postings, using data from Reddit to identify users at no, low, moderate, or severe risk. Two variations of the task focused on users whose posts to the r/SuicideWatch subreddit indicated they might be at risk; a third task looked at screening users based only on their more everyday (non-SuicideWatch) posts. We received submissions from 15 different teams, and the results provide progress and insight into the value of language signal in helping to predict risk level."
D19-1120,A Multilingual Topic Model for Learning Weighted Topic Links Across Corpora with Low Comparability,2019,0,0,3,1,26832,weiwei yang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Multilingual topic models (MTMs) learn topics on documents in multiple languages. Past models align topics across languages by implicitly assuming the documents in different languages are highly comparable, often a false assumption. We introduce a new model that does not rely on this assumption, particularly useful in important low-resource language scenarios. Our MTM learns weighted topic links and connects cross-lingual topics only when the dominant words defining them are similar, outperforming LDA and previous MTMs in classification tasks using documents{'} topic posteriors as features. It also learns coherent topics on documents with low comparability."
W18-0603,"Expert, Crowdsourced, and Machine Assessment of Suicide Risk via Online Postings",2018,-1,-1,6,0,23096,hanchin shing,Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic,0,"We report on the creation of a dataset for studying assessment of suicide risk via online postings in Reddit. Evaluation of risk-level annotations by experts yields what is, to our knowledge, the first demonstration of reliability in risk assessment by clinicians based on social media postings. We also introduce and demonstrate the value of a new, detailed rubric for assessing suicide risk, compare crowdsourced with expert performance, and present baseline predictive modeling experiments using the new dataset, which will be made available to researchers through the American Association of Suicidology."
W18-0604,{CLP}sych 2018 Shared Task: Predicting Current and Future Psychological Health from Childhood Essays,2018,-1,-1,5,0,22950,veronica lynn,Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic,0,"We describe the shared task for the CLPsych 2018 workshop, which focused on predicting current and future psychological health from an essay authored in childhood. Language-based predictions of a person{'}s current health have the potential to supplement traditional psychological assessment such as questionnaires, improving intake risk measurement and monitoring. Predictions of future psychological health can aid with both early detection and the development of preventative care. Research into the mental health trajectory of people, beginning from their childhood, has thus far been an area of little work within the NLP community. This shared task represents one of the first attempts to evaluate the use of early language to predict future health; this has the potential to support a wide variety of clinical health care tasks, from early assessment of lifetime risk for mental health problems, to optimal timing for targeted interventions aimed at both prevention and treatment."
C18-1152,Assessing Composition in Sentence Vector Representations,2018,35,2,4,1,8001,allyson ettinger,Proceedings of the 27th International Conference on Computational Linguistics,0,"An important component of achieving language understanding is mastering the composition of sentence meaning, but an immediate challenge to solving this problem is the opacity of sentence vector representations produced by current neural sentence composition models. We present a method to address this challenge, developing tasks that directly target compositional meaning information in sentence vector representations with a high degree of precision and control. To enable the creation of these controlled tasks, we introduce a specialized sentence generation system that produces large, annotated sentence sets meeting specified syntactic, semantic and lexical constraints. We describe the details of the method and generation system, and then present results of experiments applying our method to probe for compositional information in embeddings from a number of existing sentence composition models. We find that the method is able to extract useful information about the differing capacities of these models, and we discuss the implications of our results with respect to these systems{'} capturing of sentence information. We make available for public use the datasets used for these experiments, as well as the generation system."
D17-1203,Adapting Topic Models using Lexical Associations with Tree Priors,2017,17,1,3,1,26832,weiwei yang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Models work best when they are optimized taking into account the evaluation criteria that people care about. For topic models, people often care about interpretability, which can be approximated using measures of lexical association. We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured by word embeddings. Tree priors improve topic interpretability without hurting extrinsic performance."
W16-2524,Probing for semantic evidence of composition by means of simple classification tasks,2016,21,21,3,1,8001,allyson ettinger,Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP},0,"We propose a diagnostic method for probing specific information captured in vector representations of sentence meaning, via simple classification tasks with strategically constructed sentence sets. We identify some key types of semantic information that we might expect to be captured in sentence composition, and illustrate example classification tasks for targeting this information."
W16-0319,The {UMD} {CLP}sych 2016 Shared Task System: Text Representation for Predicting Triage of Forum Posts about Mental Health,2016,6,0,4,0,28601,meir friedenberg,Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology,0,None
W16-0321,The {GW}/{UMD} {CLP}sych 2016 Shared Task System,2016,5,2,3,0.804598,16583,ayah zirikly,Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology,0,None
S16-1184,{CLIP}@{UMD} at {S}em{E}val-2016 Task 8: Parser for {A}bstract {M}eaning {R}epresentation using Learning to Search,2016,13,3,4,0.740741,4267,sudha rao,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"In this paper we describe our approach to the Abstract Meaning Representation (AMR) parsing shared task as part of SemEval 2016. We develop a novel technique to parse English sentences into AMR using Learning to Search. We decompose the AMR parsing task into three subtasks that of predicting the concepts, the relations, and the root. Each of these subtasks are treated as a sequence of predictions. Using Learning to Search, we add past predictions as features for future predictions, and define a combined loss over the entire AMR structure."
P16-1065,A Discriminative Topic Model using Document Network Structure,2016,39,9,3,1,26832,weiwei yang,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Document collections often have links between documentsxe2x80x94citations, hyperlinks, or revisionsxe2x80x94and which links are added is often based on topical similarity. To model these intuitions, we introduce a new topic model for documents situated within a network structure, integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features. Experiments on a scientific paper dataset and collection of webpages show that, by more robustly exploiting the rich link structure within a document network, our model improves link prediction, topic quality, and block distributions."
P16-1177,Learning Text Pair Similarity with Context-sensitive Autoencoders,2016,27,24,2,0,12582,hadi amiri,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1163,Retrofitting Sense-Specific Word Vectors Using Parallel Text,2016,16,4,2,1,8001,allyson ettinger,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
W15-3003,Data Selection With Fewer Words,2015,19,3,2,0,18812,amittai axelrod,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"We present a method that improves data selection by combining a hybrid word/part-of-speech representation for corpora, with the idea of distinguishing between rare and frequent events. We validate our approach using data selection for machine translation, and show that it maintains or improves BLEU and TER translation scores while substantially improving vocabulary coverage and reducing data selection model size. Paradoxically, the coverage improvement is achieved by abstracting away over 97% of the total training corpus vocabulary using simple part-of-speech tags during the data selection process."
W15-1207,The {U}niversity of {M}aryland {CLP}sych 2015 Shared Task System,2015,10,16,1,1,8279,philip resnik,Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,0,"The 2015 ACL Workshop on Computational Linguistics and Clinical Psychology included a shared task focusing on classification of a sample of Twitter users according to three mental health categories: users who have self-reported a diagnosis of depression, users who have self-reported a diagnosis of post-traumatic stress disorder (PTSD), and control users who have done neither (Coppersmith et al., 2015; Coppersmith et al., 2014). Like other shared tasks, the goal here was to assess the state of the art with regard to a challenging problem, to advance that state of the art, and to bring together and hopefully expand the community of researchers interested in solving it."
W15-1212,Beyond {LDA}: Exploring Supervised Topic Modeling for Depression-Related Language in {T}witter,2015,23,41,1,1,8279,philip resnik,Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,0,"Topic models can yield insight into how depressed and non-depressed individuals use language differently. In this paper, we explore the use of supervised topic models in the analysis of linguistic signal for detecting depression, providing promising results using several models."
P15-2072,The Media Frames Corpus: Annotations of Frames Across Issues,2015,30,43,4,0,4218,dallas card,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We describe the first version of the Media Frames Corpus: several thousand news articles on three policy issues, annotated in terms of media framing. We motivate framing as a phenomenon of study for computational linguistics and describe our annotation process."
P15-1139,Tea Party in the House: A Hierarchical Ideal Point Topic Model and Its Application to Republican Legislators in the 112th Congress,2015,33,26,3,1,37071,vietan nguyen,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We introduce the Hierarchical Ideal Point Topic Model, which provides a rich picture of policy issues, framing, and voting behavior using a joint model of votes, bill text, and the language that legislators use when debating bills. We use this model to look at the relationship between Tea Party Republicans and xe2x80x9cestablishmentxe2x80x9d Republicans in the U.S. House of Representatives during the 112th Congress. 1 Capturing Political Polarization Ideal-point models are one of the most widely used tools in contemporary political science research (Poole and Rosenthal, 2007). These models estimate political preferences for legislators, known as their ideal points, from binary data such as legislative votes. Popular formulations analyze legislatorsxe2x80x99 votes and place them on a one-dimensional scale, most often interpreted as an ideological spectrum from liberal to conservative. Moving beyond a single dimension is attractive, however, since people may lean differently based on policy issues; for example, the conservative movement in the U.S. includes fiscal conservatives who are relatively liberal on social issues, and vice versa. In multi-dimensional ideal point models, therefore, the ideal point of each legislator is no longer characterized by a single number, but by a multi-dimensional vector. With that move comes a new challenge, though: the additional dimensions are often difficult to interpret. To mitigate this problem, recent research has introduced methods that estimate multi-dimensional ideal points using both voting data and the texts of the bills being voted on, e.g., using topic models and associating each dimension of the ideal point space with a topic. The words most strongly associated with the topic can sometimes provide a readable description of its corresponding dimension. In this paper, we develop this idea further by introducing HIPTM, the Hierarchical Ideal Point Topic Model, to estimate multi-dimensional ideal points for legislators in the U.S. Congress. HIPTM differs from previous models in three ways. First, HIPTM uses not only votes and associated bill text, but also the language of the legislators themselves; this allows predictions of ideal points from politiciansxe2x80x99 writing alone. Second, HIPTM improves the interpretability of ideal-point dimensions by incorporating data from the Congressional Bills Project (Adler and Wilkerson, 2015), in which bills are labeled with major topics from the Policy Agendas Project Topic Codebook.1 And third, HIPTM discovers a hierarchy of topics, allowing us to analyze both agenda issues and issue-specific frames that legislators use on the congressional floor, following Nguyen et al. (2013) in modeling framing as second-level agenda setting (McCombs, 2005). Using this new model, we focus on Republican legislators during the 112th U.S. Congress, from January 2011 until January 2013. This is a particularly interesting session of Congress for political scientists, because of the rise of the Tea Party, a decentralized political movement with populist, libertarian, and conservative elements. Although united with xe2x80x9cestablishmentxe2x80x9d Republicans against Democrats in the 2010 midterm elections, leading to massive Democratic defeats, the Tea Party wasxe2x80x94and still isxe2x80x94wrestling with establishment Republicans for control of the Republican party. The Tea Party is a new and complex phenomenon for political scientists; as Carmines and Dxe2x80x99Amico (2015) observe: xe2x80x9cConventional views of ideology as a single-dimensional, left-right spectrum experience great difficulty in understanding or explaining the Tea Party.xe2x80x9d Our model identifies legislators who have low (or high) levels of xe2x80x9cTea Partinessxe2x80x9d but are (or are not) members of the Tea Party Caucus, and providing insights into the nahttp://www.policyagendas.org/"
N15-1052,Dialogue focus tracking for zero pronoun resolution,2015,18,6,4,0.740741,4267,sudha rao,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We take a novel approach to zero pronoun resolution in Chinese: our model explicitly tracks the flow of focus in a discourse. Our approach, which generalizes to deictic references, is not reliant on the presence of overt noun phrase antecedents to resolve to, and allows us to address the large percentage of xe2x80x9cnon-anaphoricxe2x80x9d pronouns filtered out in other approaches. We furthermore train our model using readily available parallel Chinese/English corpora, allowing for training without hand-annotated data. Our results demonstrate improvements on two test sets, as well as the usefulness of linguistically motivated features."
D15-1030,Birds of a Feather Linked Together: A Discriminative Topic Model using Link-based Priors,2015,20,3,3,1,26832,weiwei yang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"A wide range of applications, from social media to scientific literature analysis, involve graphs in which documents are connected by links. We introduce a topic model for link prediction based on the intuition that linked documents will tend to have similar topic distributions, integrating a max-margin learning criterion and lexical term weights in the loss function. We validate our approach on the tweets from 2,000 Sina Weibo users and evaluate our modelxe2x80x99s reconstruction of the social network."
W14-2520,"{``}{I} Want to Talk About, Again, My Record On Energy ...{''}: Modeling Agendas and Framing in Political Debates and Other Conversations",2014,-1,-1,1,1,8279,philip resnik,Proceedings of the {ACL} 2014 Workshop on Language Technologies and Computational Social Science,0,None
P14-1105,Political Ideology Detection Using Recursive Neural Networks,2014,31,105,4,0,4054,mohit iyyer,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"An individualxe2x80x99s words often reveal their political ideology. Existing automated techniques to identify ideology from text focus on bags of words or wordlists, ignoring syntax. Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language, we apply a recursive neural network (RNN) framework to the task of identifying the political position evinced by a sentence. To show the importance of modeling subsentential elements, we crowdsource political annotations at a phrase and sentence level. Our model outperforms existing models on our newly annotated dataset and an existing dataset."
P14-1106,A Unified Model for Soft Linguistic Reordering Constraints in Statistical Machine Translation,2014,45,6,3,1,9182,junhui li,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system: 1) a syntactic reordering model that explores reorderings for context free grammar rules; and 2) a semantic reordering model that focuses on the reordering of predicate-argument structures. We develop novel features based on both models and use them as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. However, the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model, and we therefore provide a detailed analysis of the behavior differences between the two."
D14-1182,Sometimes Average is Best: The Importance of Averaging for Prediction using {MCMC} Inference in Topic Modeling,2014,29,8,3,1,37071,vietan nguyen,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Markov chain Monte Carlo (MCMC) approximates the posterior distribution of latent variable models by generating many samples and averaging over them. In practice, however, it is often more convenient to cut corners, using only a single sample or following a suboptimal averaging strategy. We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction."
W13-2214,Towards Efficient Large-Scale Feature-Rich Statistical Machine Translation,2013,25,0,4,1,26610,vladimir eidelman,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present the system we developed to provide efficient large-scale feature-rich discriminative training for machine translation. We describe how we integrate with MapReduce using Hadoop streaming to allow arbitrarily scaling the tuning set and utilizing a sparse feature set. We report our findings on German-English and RussianEnglish translation, and discuss benefits, as well as obstacles, to tuning on larger development sets drawn from the parallel training data."
P13-4034,Mr. {MIRA}: Open-Source Large-Margin Structured Learning on {M}ap{R}educe,2013,19,4,4,1,26610,vladimir eidelman,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present an open-source framework for large-scale online structured learning. Developed with the flexibility to handle cost-augmented inference problems such as statistical machine translation (SMT), our large-margin learner can be used with any decoder. Integration with MapReduce using Hadoop streaming allows efficient scaling with increasing size of training data. Although designed with a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing."
P13-1110,Online Relative Margin Maximization for Statistical Machine Translation,2013,39,4,3,1,26610,vladimir eidelman,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data. However, these solutions are impractical in complex structured prediction problems such as statistical machine translation. We present an online gradient-based algorithm for relative margin maximization, which bounds the spread of the projected data while maximizing the margin. We evaluate our optimizer on Chinese-English and ArabicEnglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set."
N13-3009,{A}rgviz: Interactive Visualization of Topic Dynamics in Multi-party Conversations,2013,15,2,4,1,37071,vietan nguyen,Proceedings of the 2013 {NAACL} {HLT} Demonstration Session,0,"We introduce an efficient, interactive frameworkxe2x80x94Argvizxe2x80x94for experts to analyze the dynamic topical structure of multi-party conversations. Users inject their needs, expertise, and insights into models via iterative topic refinement. The refined topics feed into a segmentation model, whose outputs are shown to users via multiple coordinated views."
N13-1060,Modeling Syntactic and Semantic Structures in Hierarchical Phrase-based Translation,2013,25,17,2,1,9182,junhui li,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Incorporating semantic structure into a linguistics-free translation model is challenging, since semantic structures are closely tied to syntax. In this paper, we propose a two-level approach to exploiting predicate-argument structure reordering in a hierarchical phrase-based translation model. First, we introduce linguistically motivated constraints into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. Second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. Experiments on Chinese-to-English translation demonstrate that both advances significantly improve translation accuracy."
D13-1133,Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students,2013,24,37,1,1,8279,philip resnik,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We investigate the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. Using Pennebakerxe2x80x99s Linguistic Inquiry and Word Count (LIWC) lexicon to provide baseline features, we show that straightforward topic modeling using Latent Dirichlet Allocation (LDA) yields interpretable, psychologically relevant xe2x80x9cthemesxe2x80x9d that add value in prediction of clinical assessments."
P12-2023,Topic Models for Dynamic Translation Model Adaptation,2012,17,65,3,1,26610,vladimir eidelman,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts, where topics are induced in an unsupervised way using topic models; this can be thought of as inducing subcorpora for adaptation without any human annotation. We use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features. Conditioning lexical probabilities on the topic biases translations toward topic-relevant output, resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline."
P12-1009,{SITS}: A Hierarchical Nonparametric Model using Speaker Identity for Topic Segmentation in Multiparty Conversations,2012,46,29,3,1,37071,vietan nguyen,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"One of the key tasks for analyzing conversational data is segmenting it into coherent topic segments. However, most models of topic segmentation ignore the social aspect of conversations, focusing only on the words used. We introduce a hierarchical Bayesian nonparametric model, Speaker Identity for Topic Segmentation (SITS), that discovers (1) the topics used in a conversation, (2) how these topics are shared across conversations, (3) when these topics shift, and (4) a person-specific tendency to introduce new topics. We evaluate against current unsupervised segmentation models to show that including person-specific information improves segmentation performance on meeting corpora and on political debates. Moreover, we provide evidence that SITS captures an individual's tendency to introduce new topics in political contexts, via analysis of the 2008 US presidential debates and the television program Crossfire."
N12-1046,Encouraging Consistent Translation Choices,2012,29,32,3,0.757576,30398,ferhan ture,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"It has long been observed that monolingual text exhibits a tendency toward one sense per discourse, and it has been argued that a related one translation per discourse constraint is operative in bilingual contexts as well. In this paper, we introduce a novel method using forced decoding to confirm the validity of this constraint, and we demonstrate that it can be exploited in order to improve machine translation quality. Three ways of incorporating such a preference into a hierarchical phrase-based MT model are proposed, and the approach where all three are combined yields the greatest improvements for both Arabic-English and Chinese-English translation experiments."
W11-2140,Noisy {SMS} Machine Translation in Low-Density Languages,2011,13,5,3,1,26610,vladimir eidelman,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper presents the system we developed for the 2011 WMT Haitian Creole--English SMS featured translation task. Applying standard statistical machine translation methods to noisy real-world SMS data in a low-density language setting such as Haitian Creole poses a unique set of challenges, which we attempt to address in this work. Along with techniques to better exploit the limited available training data, we explore the benefits of several methods for alleviating the additional noise inherent in the SMS and transforming it to better suite the assumptions of our hierarchical phrase-based model system. We show that these methods lead to significant improvements in BLEU score over the baseline."
W11-2148,The Value of Monolingual Crowdsourcing in a Real-World Translation Scenario: Simulation using {H}aitian {C}reole Emergency {SMS} Messages,2011,11,14,2,0,44216,chang hu,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"MonoTrans2 is a translation system that combines machine translation (MT) with human computation using two crowds of monolingual source (Haitian Creole) and target (English) speakers. We report on its use in the WMT 2011 Haitian Creole to English translation task, showing that MonoTrans2 translated 38% of the sentences well compared to Google Translate's 25%."
W10-1707,The {U}niversity of {M}aryland Statistical Machine Translation System for the Fifth Workshop on Machine Translation,2010,-1,-1,3,1,26610,vladimir eidelman,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,None
W10-0723,Shedding (a Thousand Points of) Light on Biased Language,2010,12,38,2,0,42837,tae yano,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,"This paper considers the linguistic indicators of bias in political text. We used Amazon Mechanical Turk judgments about sentences from American political blogs, asking annotators to indicate whether a sentence showed bias, and if so, in which political direction and through which word tokens. We also asked annotators questions about their own political views. We conducted a preliminary analysis of the data, exploring how different groups perceive bias in different blogs, and showing some lexical indicators strongly associated with perceived bias."
W10-0730,Measuring Transitivity Using Untrained Annotators,2010,7,10,3,0.757576,16057,nitin madnani,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,Hopper and Thompson (1980) defined a multi-axis theory of transitivity that goes beyond simple syntactic transitivity and captures how much action takes place in a sentence. Detecting these features requires a deep understanding of lexical semantics and real-world pragmatics. We propose two general approaches for creating a corpus of sentences labeled with respect to the Hopper-Thompson transitivity schema using Amazon Mechanical Turk. Both approaches assume no existing resources and incorporate all necessary annotation into a single system; this is done to allow for future generalization to other languages. The first task attempts to use language-neutral videos to elicit human-composed sentences with specified transitivity attributes. The second task uses an iterative process to first label the actors and objects in sentences and then annotate the sentences' transitivity. We examine the success of these techniques and perform a preliminary classification of the transitivity of held-out data.
W10-0735,Error Driven Paraphrase Annotation using {M}echanical {T}urk,2010,10,35,2,0,41349,olivia buzek,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,"The source text provided to a machine translation system is typically only one of many ways the input sentence could have been expressed, and alternative forms of expression can often produce a better translation. We introduce here error driven paraphrasing of source sentences: instead of paraphrasing a source sentence exhaustively, we obtain paraphrases for only the parts that are predicted to be problematic for the translation system. We report on an Amazon Mechanical Turk study that explores this idea, and establishes via an oracle evaluation that it holds the potential to substantially improve translation quality."
P10-4002,"cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models",2010,29,188,9,0.727273,3925,chris dyer,Proceedings of the {ACL} 2010 System Demonstrations,0,"We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C implementation means that memory use and runtime performance are significantly better than comparable decoders."
N10-1052,Generalizing Hierarchical Phrase-based Translation using Rules with Adjacent Nonterminals,2010,9,2,2,1,14459,hendra setiawan,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Hierarchical phrase-based translation (Hiero, (Chiang, 2005)) provides an attractive framework within which both short- and long-distance reorderings can be addressed consistently and efficiently. However, Hiero is generally implemented with a constraint preventing the creation of rules with adjacent nonterminals, because such rules introduce computational and modeling challenges. We introduce methods to address these challenges, and demonstrate that rules with adjacent nonterminals can improve Hiero's generalization power and lead to significant performance gains in Chinese-English translation."
N10-1128,"Context-free reordering, finite-state translation",2010,33,33,2,0.727273,3925,chris dyer,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We describe a class of translation model in which a set of input variants encoded as a context-free forest is translated using a finite-state translation model. The forest structure of the input is well-suited to representing word order alternatives, making it straightforward to model translation as a two step process: (1) tree-based source reordering and (2) phrase transduction. By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model."
D10-1005,Holistic Sentiment Analysis Across Languages: Multilingual Supervised {L}atent {D}irichlet {A}llocation,2010,46,82,2,0.952381,3312,jordan boydgraber,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we develop multilingual supervised latent Dirichlet allocation (MlSLDA), a probabilistic generative model that allows insights gleaned from one language's data to inform how the model captures properties of other languages. MlSLDA accomplishes this by jointly modeling two aspects of text: how multilingual concepts are clustered into thematically coherent topics and how topics associated with text connect to an observed regression variable (such as ratings on a sentiment scale). Concepts are represented in a general hierarchical framework that is flexible enough to express semantic ontologies, dictionaries, clustering constraints, and, as a special, degenerate case, conventional topic models. Both the topics and the regression are discovered via posterior inference from corpora. We show MlSLDA can build topics that are consistent across languages, discover sensible bilingual lexical correspondences, and leverage multilingual corpora to better predict sentiment."
D10-1013,Improving Translation via Targeted Paraphrasing,2010,26,33,1,1,8279,philip resnik,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation that makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e. paraphrases) with only monolingual knowledge of the source language. Evaluations demonstrate that this approach can yield substantial improvements in translation quality."
D10-1028,Modeling Perspective Using {A}daptor {G}rammars,2010,22,27,3,0,46393,eric hardisty,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Strong indications of perspective can often come from collocations of arbitrary length; for example, someone writing get the government out of my X is typically expressing a conservative rather than progressive viewpoint. However, going beyond unigram or bigram features in perspective classification gives rise to problems of data sparsity. We address this problem using nonparametric Bayesian modeling, specifically adaptor grammars (Johnson et al., 2006). We demonstrate that an adaptive naive Bayes model captures multiword lexical usages associated with perspective, and establishes a new state-of-the-art for perspective classification results using the Bitter Lemons corpus, a collection of essays about mid-east issues from Israeli and Palestinian points of view."
D10-1052,Discriminative Word Alignment with a Function Word Reordering Model,2010,26,10,3,1,14459,hendra setiawan,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We address the modeling, parameter estimation and search challenges that arise from the introduction of reordering models that capture non-local reordering in alignment modeling. In particular, we introduce several reordering models that utilize (pairs of) function words as contexts for alignment reordering. To address the parameter estimation challenge, we propose to estimate these reordering models from a relatively small amount of manually-aligned corpora. To address the search challenge, we devise an iterative local search algorithm that stochastically explores reordering possibilities. By capturing non-local reordering phenomena, our proposed alignment model bears a closer resemblance to state-of-the-art translation model. Empirical results show significant improvements in alignment quality as well as in translation performance over baselines in a large-scale Chinese-English translation task."
2010.amta-workshop.3,Position Paper: Improving Translation via Targeted Paraphrasing,2010,26,33,2,0,31901,yakov kronrod,"Proceedings of the Workshop on Collaborative Translation: technology, crowdsourcing, and the translator perspective",0,"Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation that makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e. paraphrases) with only monolingual knowledge of the source language. Evaluations demonstrate that this approach can yield substantial improvements in translation quality."
W09-0426,The {U}niversity of {M}aryland Statistical Machine Translation System for the {F}ourth {W}orkshop on {M}achine {T}ranslation,2009,30,14,4,0.727273,3925,chris dyer,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"This paper describes the techniques we explored to improve the translation of news text in the German-English and Hungarian-English tracks of the WMT09 shared translation task. Beginning with a convention hierarchical phrase-based system, we found benefits for using word segmentation lattices as input, explicit generation of beginning and end of sentence markers, minimum Bayes risk decoding, and incorporation of a feature scoring the alignment of function words in the hypothesized translation. We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements."
P09-1037,Topological Ordering of Function Words in Hierarchical Phrase-based Translation,2009,18,18,4,1,14459,hendra setiawan,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance."
N09-1057,More than Words: Syntactic Packaging and Implicit Sentiment,2009,29,101,2,0,47353,stephan greene,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Work on sentiment analysis often focuses on the words and phrases that people use in overtly opinionated text. In this paper, we introduce a new approach to the problem that focuses not on lexical indicators, but on the syntactic packaging of ideas, which is well suited to investigating the identification of implicit sentiment, or perspective. We establish a strong predictive connection between linguistically well motivated features and implicit sentiment, and then show how computational approximations of these features can be used to improve on existing state-of-the-art sentiment classification results."
D09-1040,Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases,2009,37,128,3,1,34833,yuval marton,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called low density languages. But pivoting requires additional parallel texts. We address this problem by deriving paraphrases monolingually, using distributional semantic similarity measures, thus providing access to larger training resources, such as comparable and unrelated monolingual corpora. We present what is to our knowledge the first successful integration of a collocational approach to untranslated words with an end-to-end, state of the art SMT system demonstrating significant translation improvements in a low-resource setting."
D09-1081,Estimating Semantic Distance Using Soft Semantic Constraints in Knowledge-Source {--} Corpus Hybrid Models,2009,288,13,3,1,34833,yuval marton,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Strictly corpus-based measures of semantic distance conflate co-occurrence information pertaining to the many possible senses of target words. We propose a corpus-thesaurus hybrid method that uses soft constraints to generate word-senseaware distributional profiles (DPs) from coarser concept DPs (derived from a Roget-like thesaurus) and sense-unaware traditional word DPs (derived from raw text). Although it uses a knowledge source, the method is not vocabulary-limited: if the target word is not in the thesaurus, the method falls back gracefully on the word's co-occurrence information. This allows the method to access valuable information encoded in a lexical resource, such as a thesaurus, while still being able to effectively handle domain-specific terms and named entities. Experiments on word-pair ranking by semantic distance show the new hybrid method to be superior to others."
P08-1114,Soft Syntactic Constraints for Hierarchical Phrased-Based Translation,2008,28,111,2,1,34833,yuval marton,Proceedings of ACL-08: HLT,1,"In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then nding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English."
P08-1115,Generalizing Word Lattice Translation,2008,37,141,3,0,47926,christopher dyer,Proceedings of ACL-08: HLT,1,"Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for ChineseEnglish and Arabic-English translation."
I08-3008,Cross-Language Parser Adaptation between Related Languages,2008,17,86,2,0,5828,daniel zeman,Proceedings of the {IJCNLP}-08 Workshop on {NLP} for Less Privileged Languages,0,"The present paper describes an approach to adapting a parser to a new language. Presumably the target language is much poorer in linguistic resources than the source language. The technique has been tested on two European languages due to test data availability; however, it is easily applicable to any pair of sufficiently related languages, including some of the Indic language group. Our adaptation technique using existing annotations in the source language achieves performance equivalent to that obtained by training on 1546 trees in the target language."
D08-1024,Online Large-Margin Training of Syntactic and Structural Translation Features,2008,36,234,3,0.234375,3180,david chiang,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT. We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train a large number of Marton and Resnik's soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 Bleu on a subset of the NIST 2006 Arabic-English evaluation data."
2008.amta-papers.13,Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization,2008,-1,-1,2,0.897436,16057,nitin madnani,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases."
W07-0716,Using Paraphrases for Parameter Tuning in Statistical Machine Translation,2007,25,70,3,0.952381,16057,nitin madnani,Proceedings of the Second Workshop on Statistical Machine Translation,0,"Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality."
S07-1071,"Tor, {T}or{M}d: Distributional Profiles of Concepts for Unsupervised Word Sense Disambiguation",2007,12,6,3,0.454545,13005,saif mohammad,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"Words in the context of a target word have long been used as features by supervised word-sense classifiers. Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words---the distributional profile of a concept (DPC)---without the use of manually annotated data. We implemented an unsupervised naive Bayes word sense classifier using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese-English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17). We also created a simple PMI-based classifier to attempt the English Lexical Substitution Task (task #10); however, its performance was poor."
2006.amta-papers.11,"Word-Based Alignment, Phrase-Based Translation: What{'}s the Link?",2006,30,48,2,1,1313,adam lopez,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"State-of-the-art statistical machine translation is based on alignments between phrases {--} sequences of words in the source and target sentences. The learning step in these systems often relies on alignments between words. It is often assumed that the quality of this word alignment is critical for translation. However, recent results suggest that the relationship between alignment quality and translation quality is weaker than previously thought. We investigate this question directly, comparing the impact of high-quality alignments with a carefully constructed set of degraded alignments. In order to tease apart various interactions, we report experiments investigating the impact of alignments on different aspects of the system. Our results confirm a weak correlation, but they also illustrate that more data and better feature engineering may be more beneficial than better alignment."
W05-0812,Improved {HMM} Alignment Models for Languages with Scarce Resources,2005,14,25,2,1,1313,adam lopez,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"We introduce improvements to statistical word alignment based on the Hidden Markov Model. One improvement incorporates syntactic knowledge. Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more complex models, resulting in over a 5.5% absolute reduction in error on Romanian-English."
P05-3009,The {L}inguist{'}s {S}earch {E}ngine: An Overview,2005,14,37,1,1,8279,philip resnik,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"The Linguist's Search Engine (LSE) was designed to provide an intuitive, easy-to-use interface that enables language researchers to seek linguistically interesting examples on the Web, based on syntactic and lexical criteria. We briefly describe its user interface and architecture, as well as recent developments that include LSE search capabilities for Chinese."
H05-2007,Pattern Visualization for Machine Translation Output,2005,7,6,2,1,1313,adam lopez,Proceedings of {HLT}/{EMNLP} 2005 Interactive Demonstrations,0,"We describe a method for identifying systematic patterns in translation data using part-of-speech tag sequences. We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems, and demonstrate how our application can be used by developers to explore patterns in machine translation output."
H05-1098,"The {H}iero Machine Translation System: Extensions, Evaluation, and Analysis",2005,27,54,5,0.234375,3180,david chiang,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems."
H05-1109,{OCR} Post-Processing for Low Density Languages,2005,19,18,2,1,51121,okan kolak,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We present a lexicon-free post-processing method for optical character recognition (OCR), implemented using weighted finite state machines. We evaluate the technique in a number of scenarios relevant for natural language processing, including creation of new OCR capabilities for low density languages, improvement of OCR performance for a native commercial system, acquisition of knowledge from a foreign-language dictionary, creation of a parallel text, and machine translation from OCR output."
W04-0821,The {U}niversity of {M}aryland Senseval-3 system descriptions,2004,8,2,3,0,51634,clara cabezas,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
P04-1048,Inducing Frame Semantic Verb Classes from {W}ord{N}et and {LDOCE},2004,15,24,3,1,50483,rebecca green,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"This paper presents SemFrame, a system that induces frame semantic verb classes from WordNet and LDOCE. Semantic frames are thought to have significant potential in resolving the paraphrase problem challenging many language-based applications.When compared to the handcrafted FrameNet, SemFrame achieves its best recall-precision balance with 83.2% recall (based on SemFrame's coverage of FrameNet frames) and 73.8% precision (based on SemFrame verbs' semantic relatedness to frame-evoking verbs). The next best performing semantic verb classes achieve 56.9% recall and 55.0% precision."
N03-2026,Desparately Seeking {C}ebuano,2003,2,7,5,0.625,12879,douglas oard,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,This paper describes an effort to rapidly develop language resources and component technology to support searching Cebuano news stories using English queries. Results from the first 60 hours of the exercise are presented.
N03-1018,A Generative Probabilistic {OCR} Model for {NLP} Applications,2003,22,55,3,1,51121,okan kolak,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In this paper, we introduce a generative probabilistic optical character recognition (OCR) model that describes an end-to-end process in the noisy channel framework, progressing from generation of true text through its transformation into the noisy output of an OCR system. The model is designed for use in error correction, with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks. We present an implementation of the model based on finite-state models, demonstrate the model's ability to significantly reduce character and word error rate, and provide evaluation results involving automatic extraction of translation lexicons from printed text."
J03-3002,The Web as a Parallel Corpus,2003,39,492,1,1,8279,philip resnik,Computational Linguistics,0,"Parallel corpora have become an essential resource for work in multilingual natural language processing. In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web,first reviewing the original algorithm and results and then presenting a set of significant enhancements. These enhancements include the use of supervised learning based on structural features of documents to improve classification performance, a new content-based measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair."
P02-1033,An Unsupervised Method for Word Sense Tagging using Parallel Corpora,2002,26,209,2,0,7377,mona diab,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"We present an unsupervised method for word sense disambiguation that exploits translation correspondences in parallel corpora. The technique takes advantage of the fact that cross-language lexicalizations of the same concept tend to be consistent, preserving some core element of its semantics, and yet also variable, reflecting differing translator preferences and the influence of context. Working with parallel corpora introduces an extra complication for evaluation, since it is difficult to find a corpus that is both sense tagged and parallel with another language; therefore we use pseudo-translations, created by machine translation systems, in order to make possible the evaluation of the approach against a standard test set. The results demonstrate that word-level translation correspondences are a valuable source of information for sense disambiguation."
P02-1050,Evaluating Translational Correspondence using Annotation Projection,2002,25,105,2,0,238,rebecca hwa,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Recently, statistical machine translation models have begun to take advantage of higher level linguistic structures such as syntactic dependencies. Underlying these models is an assumption about the directness of translational correspondence between sentences in the two languages; however, the extent to which this assumption is valid and useful is not well understood. In this paper, we present an empirical study that quantifies the degree to which syntactic dependencies are preserved when parses are projected directly from English to Chinese. Our results show that although the direct correspondence assumption is often too restrictive, a small set of principled, elementary linguistic transformations can boost the quality of the projected Chinese parses by 76% relative to the unimproved baseline."
S01-1014,Supervised Sense Tagging using Support Vector Machines,2001,10,23,2,0,51634,clara cabezas,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"We describe the University of Maryland's supervised sense tagger, which participated in the SENSEVAL-2 lexical sample evaluations for English, Spanish, and Swedish; we also present unofficial results for Basque. We designed a highly modular combination of language-independent feature extraction and supervised learning using support vector machines in order to permit rapid ramp-up, language independence, and capability for future expansion."
P01-1032,Mapping Lexical Entries in a Verbs Database to {W}ord{N}et Senses,2001,19,12,4,1,50483,rebecca green,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes automatic techniques for mapping 9611 entries in a database of English verbs to WordNet senses. The verbs were initially grouped into 491 classes based on syntactic features. Mapping these verbs into WordNet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and cross-language information retrieval. Our techniques make use of (1) a training set of 1791 disambiguated entries, representing 1442 verb entries from 167 classes; (2) word sense probabilities, from frequency counts in a tagged corpus; (3) semantic similarity of WordNet senses for verbs within the same class; (4) probabilistic correlations between WordNet data and attributes of the verb classes. The best results achieved 72% precision and 58% recall, versus a lower bound of 62% precision and 38% recall for assigning the most frequently occurring WordNet sense, and an upper bound of 87% precision and 75% recall for human judgment."
J01-4009,Book Reviews: Parallel Text Processing: Alignment and Use of Translation Corpora,2001,2,0,1,1,8279,philip resnik,Computational Linguistics,0,None
H01-1033,Improved Cross-Language Retrieval using Backoff Translation,2001,7,36,1,1,8279,philip resnik,Proceedings of the First International Conference on Human Language Technology Research,0,"The limited coverage of available translation lexicons can pose a serious challenge in some cross-language information retrieval applications. We present two techniques for combining evidence from dictionary-based and corpus-based translation lexicons, and show that backoff translation outperforms a technique based on merging lexicons."
H01-1060,Rapidly Retargetable Interactive Translingual Retrieval,2001,2,4,3,0,11446,ginaanne levow,Proceedings of the First International Conference on Human Language Technology Research,0,"This paper describes a system for rapidly retargetable interactive translingual retrieval. Basic functionality can be achieved for a new document language in a single day, and further improvements require only a relatively modest additional investment. We applied the techniques first to search Chinese collections using English queries, and have successfully added French, German, and Italian document collections. We achieve this capability through separation of language-dependent and language-independent components and through the application of asymmetric techniques that leverage an extensive English retrieval infrastructure."
P99-1068,Mining the Web for Bilingual Text,1999,5,233,1,1,8279,philip resnik,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"STRAND (Resnik, 1998) is a language-independent system for automatic discovery of text in parallel translation on the World Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language."
resnik-1998-parallel,Parallel strands: a preliminary investigation into mining the Web for bilingual text,1998,10,90,1,1,8279,philip resnik,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Parallel corpora are a valuable resource for machine translation, but at present their availability and utility is limited by genre- and domain-specificity, licensing restrictions, and the basic dificulty of locating parallel texts in all but the most dominant of the world{'}s languages. A parallel corpus resource not yet explored is the World Wide Web, which hosts an abundance of pages in parallel translation, offering a potential solution to some of these problems and unique opportunities of its own. This paper presents the necessary first step in that exploration: a method for automatically finding parallel translated documents on the Web. The technique is conceptually simple, fully language independent, and scalable, and preliminary evaluation results indicate that the method may be accurate enough to apply without human intervention."
W97-0907,A Language Identification Application Built on the {J}ava Client / Server Platform,1997,-1,-1,2,0,55536,gary adams,From Research to Commercial Applications: Making {NLP} Work in Practice,0,None
W97-0209,Selectional Preference and Sense Disambiguation,1997,-1,-1,1,1,8279,philip resnik,"Tagging Text with Lexical Semantics: Why, What, and How?",0,None
W97-0213,A Perspective on Word Sense Disambiguation Methods and Their Evaluation,1997,-1,-1,1,1,8279,philip resnik,"Tagging Text with Lexical Semantics: Why, What, and How?",0,None
W97-0217,Evaluating Automatic Semantic Taggers,1997,-1,-1,1,1,8279,philip resnik,"Tagging Text with Lexical Semantics: Why, What, and How?",0,None
A97-1050,Semi-Automatic Acquisition of Domain-Specific Translation Lexicons,1997,22,36,1,1,8279,philip resnik,Fifth Conference on Applied Natural Language Processing,0,"We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons."
W95-0105,Disambiguating Noun Groupings with Respect to {W}ordnet Senses,1995,17,27,1,1,8279,philip resnik,Third Workshop on Very Large Corpora,0,"Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve. However, for many tasks, one is interested in relationships among word {em senses}, not words. This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns --- the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms. Disambiguation is performed with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels. The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented."
C94-2195,A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation,1994,18,163,2,0,51339,eric brill,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, we describe a new corpus-based approach to prepositional phrase attachment disambiguation, and present results comparing performance of this algorithm with other corpus-based approaches to this problem."
W93-0307,Structural Ambiguity and Conceptual Relations,1993,16,32,1,1,8279,philip resnik,{V}ery {L}arge {C}orpora: Academic and Industrial Perspectives,0,"Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text. However, analyses based solely on lexical relationships suffer from sparseness of data: it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters. For example, the lexical association strategy for resolving ambiguous prepositional phrase attachments [Hindle and Rooth. 1991] takes into account only the attachment site (a verb or its direct object) and the preposition, ignoring the object of the preposition. We investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account. Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone. a qualitative analysis of the results suggests that the problem lies not in the noun class information, but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation. This suggests several possible revisions of our proposal. 1. P r e f e r e n c e S t r a t e g i e s Prepositional phrase attachment is a paradigmatic case of the structural ambiguity problems faced by natural language parsing systems. Most models of grammar will not constrain the analysis of such attachments in examples like (1): the grammar simply specifies that a prepositional phrase such as on computer theft can be attached in several ways, and leaves the problem of selecting the correct choice to some other process. (1) a. Eventually, Mr. Stoll was invited to both the CIA and NSA to brief high-ranking officers on computer theft. b. Eventually, Mr. Stoll was invited to both the ClA and NSA [to brief [high-ranking officers on computer theft]]. c. Eventually, Mr. Stoll was invited to both the CIA and NSA [to brief [high-ranking ollicers] [on computer theft]]. As [Church and Patil, 1982] point out, the number of analyses given combinations of such all ways ambiguous constructions grows rapidly even for sentences of quite Marti A. Hearst Computer Science Division 465 Evans Hall University of California, Berkeley Berkeley, CA 94720 USA mar t i @ c s . b e r k e l e y . e d u reasonable length, so this other process has an important role to play. Discussions of sentence processing have focused primarily on structurally-based preference strategies such as right association and minimal attachment [Kimball, 1973; Frazier, 1979; Ford et al., 1982]; [Hobbs and Bear, 1990], while acknowledging the importance of semantics and pragmatics in attachment decisions, propose two syntactically-based attachment rules that are meant to be generalizations of those structural strategies. Others, however, have argued that syntactic considerations alone are insumcient for determining prepositional phrase attachments, suggesting instead that preference relationships among lexical items are the crucial factor. For example: [Wilks et aL, 1985] argue that the right attachment rules posited by [Frazier, 1979] are incorrect for phrases in general, and supply counterexarnples. They further argue that lexical preferences alone as suggested by [Ford et al., 1982] are too simplistic, and suggest instad the use of preference semantics. In the preference semantics framework, attachment relations of phrases are determined by comparing the preferences emanating from all the entities involved in the attachment, until the best mutual fit is found. Their CASSEX system represents the various meanings of the preposition in terms of (a) the preferred semantic class of the noun or verb that proceeds the preposition (e.g., move, be, strike), (b) the case of the preposition (e.g., instrument, time, loc.static), and (c) the preferred semantic class of the head noun of the prepositional phrase (e.g., physob, event). The difficult part of this method is the identification of preference relationships and particularly determining the strengths of the preferences and how they should interact. (See also discussion in [Schubert, 19841.) lDahlgren and McDowell, 1986] also suggests using preferences based on hand-built knowledge about the prepositions and their objects, specifying a simpler set of rules than those of [Wilks et al., 1985]."
H93-1054,Semantic Classes and Syntactic Ambiguity,1993,13,70,1,1,8279,philip resnik,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"In this paper we propose to define selectional preference and semantic similarity as information-theoretic relationships involving conceptual classes, and we demonstrate the applicability of these definitions to the resolution of syntactic ambiguity. The space of classes is defined using WordNet [8], and conceptual relationships are determined by means of statistical analysis using parsed text in the Penn Treebank."
P92-1053,A Class-Based Approach to Lexical Discovery,1992,4,63,1,1,8279,philip resnik,30th Annual Meeting of the Association for Computational Linguistics,1,None
C92-2065,Probabilistic {T}ree-{A}djoining {G}rammar as a Framework for Statistical Natural Language Processing,1992,17,89,1,1,8279,philip resnik,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, I argue for the use of a probabilistic form of tree-adjoining grammar (TAG) in statistical natural language processing. I first discuss two previous statistical approaches --- one that concentrates on the probabilities of structural operations, and another that emphasizes co-occurrence relationships between words. I argue that a purely structural apprach, exemplified by probabilistic context-free grammar, lacks sufficient sensitivity to lexical context, and, conversely, that lexical co-occurence analyses require a richer notion of locality that is best provided by importing some notion of structure.I then propose probabilistic TAG as a framework for statistical language modelling, arguing that it provides an advantageous combination of structure, locality, and lexical sensitivity. Issues in the acquisition of probabilistic TAG and parameter estimation are briefly considered."
C92-1032,Left-Corner Parsing and Psychological Plausibility,1992,15,57,1,1,8279,philip resnik,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"It is well known that even extremely limited centerembedding causes people to have difficulty in comprehension, but that left- and right-branching constructions produce no such effect. If the difficulty in comprehension is taken to be a result of processing load, as is widely assumed, then measuring the processing load induced by a parsing strategy on these constructions may help determine its plausibility as a psychological model. On this basis, it has been argued [AJ91, JL83] that by identifying processing load with space utilization, we can rule out both top-down and bottom-up parsing as viable candidates for the human sentence processing mechanism, and that left-corner parsing represents a plausible alternative.Examining their arguments in detail, we find difficulties with each presentation. In this paper we revise the argument and validate its central claim. In so doing, we discover that the key distinction between the parsing methods is not the form of prediction (top-down vs. bottom-up vs. left-corner), but rather the ability to instantiate the operation of composition."
P90-1029,Multiple Underlying Systems: Translating User Requests into Programs to Produce Answers,1990,11,15,2,0,55866,robert bobrow,28th Annual Meeting of the Association for Computational Linguistics,1,"A user may typically need to combine the strengths of more than one system in order to perform a task. In this paper, we describe a component of the Janus natural language interface that translates intensional logic expressions representing the meaning of a request into executable code for each application program, chooses which combination of application systems to use, and designs the transfer of data among them in order to provide an answer. The complete Janus natural language system has been ported to two large command and control decision support aids."
