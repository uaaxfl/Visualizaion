2006.amta-papers.9,W03-1004,0,0.035486,"fferent assumptions they make about the available resources. Most approaches are not applicable to the problem of inducing lexical mappings between diglossic dialects because their assumptions do not hold in this domain. One approach is to try to build small parallel corpora out of large comparable corpora. Fung and Cheung (2004) proposed a bootstrapping method that extracts parallel sentences from texts that may be unrelated on the document level. This approach requires a seed lexicon and computes lexical similarity scores. It also requires large corpora that contain some parallel sentences. Barzilay and Elhadad (2003) applied a similar method monolingually to find paraphrases. Another method proposed by Munteanu et al. (2004) requires a set of seed parallel corpora of 5000 sentences for each language. While in the world of parallel corpora 5000 sentence pairs are considered minuscule, they may not exist at all for dialect pairs such as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal webs"
2006.amta-papers.9,J90-2002,0,0.16046,"istical analyses on the corpora to find correlations between the symbols. The quality of the mapping depends on the degree of relatedness between the corpora. Parallel corpora, in which every pair of sentences is a translation of each other, facilitate the induction of a mapping between word tokens (situated occurrences); in contrast, one might only be able to glean a mapping between word types (as in a wide coverage dictionary) from non-parallel corpora. 1 Introduction A translation lexicon is an important component of multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instan"
2006.amta-papers.9,P04-1023,0,0.0150155,"multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instance, consider the problem of finding a mapping between two dialects of a diglossic language (i.e., the language exists in two forms: a “prestigious” variety for formal communications and a colloquial variety for everyday use). Because the dialects serve different social functions, parallel corpora between dialects do not naturally occur. In these cases, inducing lexical mappings from non-parallel corpora is the more challenging alternative. Even when parallel sentence pairs are not available, one might still be able to bootstrap a"
2006.amta-papers.9,W04-3208,0,0.0125752,"d statistics of the seed dictionary, the algorithm achieves a modest improvement. 2 Mapping between Comparable Corpora Several methods have been proposed to induce lexical mappings from non-parallel corpora. In this section, we provide an overview of several common approaches and discuss different assumptions they make about the available resources. Most approaches are not applicable to the problem of inducing lexical mappings between diglossic dialects because their assumptions do not hold in this domain. One approach is to try to build small parallel corpora out of large comparable corpora. Fung and Cheung (2004) proposed a bootstrapping method that extracts parallel sentences from texts that may be unrelated on the document level. This approach requires a seed lexicon and computes lexical similarity scores. It also requires large corpora that contain some parallel sentences. Barzilay and Elhadad (2003) applied a similar method monolingually to find paraphrases. Another method proposed by Munteanu et al. (2004) requires a set of seed parallel corpora of 5000 sentences for each language. While in the world of parallel corpora 5000 sentence pairs are considered minuscule, they may not exist at all for d"
2006.amta-papers.9,P04-1067,0,0.0247933,"her language. Different similarity metrics (e.g., Cosine, Jacquard, Euclidean) can be used for the comparison; Rapp used the city-block distance. A number of related algorithms have been suggested by other researchers. Diab and Finch (2000) proposed a method that does not explicitly require a seed dictionary, though they do assume that punctuations behave similarly between the two languages. This method first builds a set of similarity vectors between pairs of the 1000 most frequent words within one language; then it compares these vectors to all possible vectors pairs for the other language. Gaussier et al. (2004) proposed an extension that focused on explicitly modeling synonyms within each monolingual corpus. 76 Although this last class of methods is more flexible than filtering for parallel sentences from comparable corpora or using bridge dictionaries, its assumptions are still too stringent for the diglossic dialect domain. First, the methods assume the availability of large quantities of corpus samples in both languages. While it is not difficult to obtain a large MSA corpus, only a small Levantine corpus is available. The dependence on seed dictionary is also problematic. It is not feasible to r"
2006.amta-papers.9,N01-1020,0,0.146897,"ll for dialect pairs such as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal websites written in colloquial dialects, the methods that search the web for parallel texts typically search for pages that link to their own translation by looking for certain structures that indicate as such. It has also been proposed that one might use a bridge language to find lexical mappings (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002). The key requirement is that the language pair of interest can be related to each other via a third language with which lexical mappings have already been established. This is an unlikely situation for the diglossic language domain because it is rare to find an established dictionary purely between a colloquial dialect and some third language. An other alternative is to take aggregate word statistics over large samples of the languages in comparable corpora. An instance of this class of algorithms is a method proposed by Rapp (1999). This method requires a seed di"
2006.amta-papers.9,J00-2004,0,0.0405958,"icon is an important component of multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instance, consider the problem of finding a mapping between two dialects of a diglossic language (i.e., the language exists in two forms: a “prestigious” variety for formal communications and a colloquial variety for everyday use). Because the dialects serve different social functions, parallel corpora between dialects do not naturally occur. In these cases, inducing lexical mappings from non-parallel corpora is the more challenging alternative. Even when parallel sentence pairs are not a"
2006.amta-papers.9,N04-1034,0,0.0135162,"nducing lexical mappings between diglossic dialects because their assumptions do not hold in this domain. One approach is to try to build small parallel corpora out of large comparable corpora. Fung and Cheung (2004) proposed a bootstrapping method that extracts parallel sentences from texts that may be unrelated on the document level. This approach requires a seed lexicon and computes lexical similarity scores. It also requires large corpora that contain some parallel sentences. Barzilay and Elhadad (2003) applied a similar method monolingually to find paraphrases. Another method proposed by Munteanu et al. (2004) requires a set of seed parallel corpora of 5000 sentences for each language. While in the world of parallel corpora 5000 sentence pairs are considered minuscule, they may not exist at all for dialect pairs such as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal websites written in colloquial dialects, the methods that search the web for parallel texts typically search for p"
2006.amta-papers.9,J03-1002,0,0.00298554,"rtant component of multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instance, consider the problem of finding a mapping between two dialects of a diglossic language (i.e., the language exists in two forms: a “prestigious” variety for formal communications and a colloquial variety for everyday use). Because the dialects serve different social functions, parallel corpora between dialects do not naturally occur. In these cases, inducing lexical mappings from non-parallel corpora is the more challenging alternative. Even when parallel sentence pairs are not available, one might"
2006.amta-papers.9,P99-1067,0,0.851889,"f this work is to investigate one such scenario: finding lexical mappings between dialects of a diglossic language, in which people conduct their written communications in a prestigious formal dialect, but they communicate verbally in a colloquial dialect. Because the two dialects serve different socio-linguistic functions, parallel corpora do not naturally exist between them. An example of a diglossic dialect pair is Modern Standard Arabic (MSA) and Levantine Arabic. In this paper, we evaluate the applicability of a standard algorithm for inducing lexical mappings between comparable corpora (Rapp, 1999) to such diglossic corpora pairs. The focus of the paper is an in-depth error analysis, exploring the notion of relatedness in diglossic corpora and scrutinizing the effects of various dimensions of relatedness (such as mode, topic, style, and statistics) on the quality of the resulting translation lexicon. Abstractly speaking, a translation lexicon is a mapping between two disjoint sets of symbols. Given some corpus sample over each set of symbols, one might induce the mapping by performing statistical analyses on the corpora to find correlations between the symbols. The quality of the mappin"
2006.amta-papers.9,J03-3002,0,0.0180126,"evel. This approach requires a seed lexicon and computes lexical similarity scores. It also requires large corpora that contain some parallel sentences. Barzilay and Elhadad (2003) applied a similar method monolingually to find paraphrases. Another method proposed by Munteanu et al. (2004) requires a set of seed parallel corpora of 5000 sentences for each language. While in the world of parallel corpora 5000 sentence pairs are considered minuscule, they may not exist at all for dialect pairs such as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal websites written in colloquial dialects, the methods that search the web for parallel texts typically search for pages that link to their own translation by looking for certain structures that indicate as such. It has also been proposed that one might use a bridge language to find lexical mappings (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002). The key requirement is that the language pair of interest can be related to each other via a"
2006.amta-papers.9,W02-2026,0,0.142198,"as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal websites written in colloquial dialects, the methods that search the web for parallel texts typically search for pages that link to their own translation by looking for certain structures that indicate as such. It has also been proposed that one might use a bridge language to find lexical mappings (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002). The key requirement is that the language pair of interest can be related to each other via a third language with which lexical mappings have already been established. This is an unlikely situation for the diglossic language domain because it is rare to find an established dictionary purely between a colloquial dialect and some third language. An other alternative is to take aggregate word statistics over large samples of the languages in comparable corpora. An instance of this class of algorithms is a method proposed by Rapp (1999). This method requires a seed dictionary (i.e., a collection"
2006.amta-papers.9,N01-1026,0,0.0118241,"ion of each other, facilitate the induction of a mapping between word tokens (situated occurrences); in contrast, one might only be able to glean a mapping between word types (as in a wide coverage dictionary) from non-parallel corpora. 1 Introduction A translation lexicon is an important component of multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instance, consider the problem of finding a mapping between two dialects of a diglossic language (i.e., the language exists in two forms: a “prestigious” variety for formal communications and a colloquial variety for everyday use). Bec"
2006.amta-papers.9,E06-1047,0,\N,Missing
2009.eamt-1.22,D07-1090,0,0.0384085,"Missing"
2009.eamt-1.22,P03-1040,0,0.0600563,"Missing"
2009.eamt-1.22,koen-2004-pharaoh,0,0.0638779,"part, however we use a phrase difficulty classifier to find the most difficult phrase of each sentence. As shown in Figure 1, after the classifier finds the difficult phrase, the adapted language model is constructed for it. After the creation of the adapted language model, the phrase is translated in the context of the full sentence, similar to the previous section. 4 Experimental Setup Figure 1: Translation Pipeline for Adapted LMs translation engine. The SMT engine is the open source Phramer decoder (Olteanu, 2006) that uses the same training and decoding framework of the Pharaoh decoder (Koehn, 2004). We modify the decoder to use alternative language models for the translation of a special phrase within each sentence. We train both the baseline and the adapted language models using the SRI language modeling package (Stolcke, 2002). 4.1 We construct two translation systems by varying the size of the training corpora. This data variation enable us to asses our approach in different translation scenarios. We use Arabic-English parallel corpora released by the Linguistic Data Consortium (LDC). The first (small) system is trained with one million words of parallel corpus2 . The second (medium)"
2009.eamt-1.22,2006.amta-papers.13,0,0.0470295,"Missing"
2009.eamt-1.22,W07-0737,1,0.889418,"sity problem, but it creates more irrelevant model parameters. Moreover, large volumes of training data may not always be available. In this paper, we consider ways of filtering the irrelevant and noisy parameters in order to improve translation quality. We propose a language model adaptation method for the translation of phrases. We construct one adapted language model per source language phrase by using c 2009 European Association for Machine Translation. Motivation Previously we explored the estimation of the translation difficulty of phrases, using an automatic MT evaluation (BLEU) score (Mohit and Hwa, 2007). We used our estimation method to label a set of gold standard phrases with the difficulty information. Moreover, we showed that it is possible to automatically learn the translation difficulty by constructing a phrase difficulty classifier. In this paper, our aim is to improve MT quality by focusing on what we call Difficult To Translate Phrases (DTPs) within source language sentences. We compiled a group of DTPs for a baseline SMT system. We then manually examined a group of difficult phrases to learn about the reasons that make these phrases difficult. Among various, often overlapping reas"
2009.eamt-1.22,W06-3121,0,0.14683,"ion method as part of a pre-translation pipeline. We still apply the adaptation to the DTP part, however we use a phrase difficulty classifier to find the most difficult phrase of each sentence. As shown in Figure 1, after the classifier finds the difficult phrase, the adapted language model is constructed for it. After the creation of the adapted language model, the phrase is translated in the context of the full sentence, similar to the previous section. 4 Experimental Setup Figure 1: Translation Pipeline for Adapted LMs translation engine. The SMT engine is the open source Phramer decoder (Olteanu, 2006) that uses the same training and decoding framework of the Pharaoh decoder (Koehn, 2004). We modify the decoder to use alternative language models for the translation of a special phrase within each sentence. We train both the baseline and the adapted language models using the SRI language modeling package (Stolcke, 2002). 4.1 We construct two translation systems by varying the size of the training corpora. This data variation enable us to asses our approach in different translation scenarios. We use Arabic-English parallel corpora released by the Linguistic Data Consortium (LDC). The first (s"
2009.eamt-1.22,P02-1040,0,0.0768312,"Missing"
2009.eamt-1.22,D08-1090,0,0.0641723,"Missing"
2009.eamt-1.22,D07-1103,0,\N,Missing
2009.eamt-1.22,P07-1004,0,\N,Missing
2009.eamt-1.22,P07-1066,0,\N,Missing
2010.amta-papers.17,P07-1038,1,0.878948,"and their relative rankings. The ranking is determined according to their actual translation quality (measured by BLEU-3 against a human reference). Duplicate translations are removed so that each ranked list specifies a total order. The learning task is to predict the gold standard ranking based on the features that can be extracted from the translation candidates. In other words, the ranker’s objective can be seen as performing a kind of automatic MT evaluation metric that does not require references on the translation candidates. Based on related work in MT evaluation (Specia et al., 2009; Albrecht and Hwa, 2007), we use a similar set of features such as: • N-gram matching with the underlying LM corpus • N-gram probabilities from the LM • Target to source-language lexical ambiguity • Average word movement from source to targetlanguage • The ratio of punctuation and digit in the source and target phrases • The average BLEU score: A BLEU evaluation of the hypothesis, using the other competing hypothesis as the pseudo references model’s top choice resulted in translation improvements over the baseline. LM weight used Baseline adapt. by group adapt. of indiv. wt. • Bigram and trigram POS tags: In order to"
2010.amta-papers.17,W08-0331,0,0.0129242,"expected performance. To perform this kind of judgment, we make use of techniques similar to confidence estimation and automatic MT evaluation. Previous work on MT evaluation without references focuses on sentential or corpus level evaluation and uses regression to predict a translation quality score (Albrecht and Hwa, 2007; Specia et al., 2009). In contrast, we apply the evaluation metric to segments at a sub-sentential level. We are less interested in the absolute scores than in the quality of the candidates relative to each other. In this way, our approach is more similar to the metric of (Duh, 2008), in which the evaluation is conducted by ranking. Also relevant is the series of work on system modification, such as post-decoding discriminative re-ranking. The discriminative re-ranking benefits from a set of richer but more computationally expensive features. These features range from deeper linguistic knowledge (Och et al., 2004) to system related knowledge like word and phrase level confidence score (Zens and Ney, 2006). Similarly we benefit from additional features in our weight ranking; however, the set of candidate hypotheses generated for weight ranking is different from a decoder’s"
2010.amta-papers.17,koen-2004-pharaoh,0,0.0473095,"eling errors, but the LM features are influential enough for finding segments with LM-related problems. In most experiments in this work (except section 6), the most difficult segment is gold-standard. That means that for each sentence, the reference translations are used to find the segment with the lowest translation quality. 2.2 Implementation We modify the standard PB-SMT pipeline to facilitate LM weight adaptation. Our basic PB-SMT system uses the SRI package (Stolcke, 2002) as the target language model and Phramer (Olteanu et al., 2006), an open source implementation similar to Pharaoh (Koehn, 2004), as the decoder. We modify the decoder to allow the usage of different LM weights for different parts of a sentence; it accepts two decoding weights: One is used for the difficult segment, and the other one is used for the translation of the rest of the sentence. In order to apply this separation of the decoding weights, we constrain the choice of phrases in hypothesis expansion at the boundaries of the difficult segment. We allow the decoder to shift the difficult segment’s boundaries with one word to use more of the phrase table entries. We assume that every sentence holds at least one segm"
2010.amta-papers.17,W07-0737,1,0.912592,"We argue that the right scope for a language model weight to excise its influence is at the subsentential level. Even a long and complex sentence may contain parts that are relatively straightforward. Our approach is to first identify the portion of a source sentence whose translation may be problematic for the target language model (we refer to this special portion as a segment); then use an adapted LM weight for the translation of the special segment but use the default LM weight for the rest of the sentence. We previously defined such special segment as Difficult to Translate Phrase (DTP) (Mohit and Hwa, 2007). We also described an SVM classifier that can predict whether a segment of source text will be problematic for the MT system with good accuracy. In an extended work, we adapted the language model of a SMT system for the translation of the DTPs (Mohit et al., 2009). For each DTP we automatically selected the relevant subset of the training data and constructed an adapted language model. Moreover, we translated the difficult phrase with the adapted model. In this paper, we continue our segment-specific system customization framework for testing our idea of modifying the language model weights."
2010.amta-papers.17,2009.eamt-1.22,1,0.786776,"e whose translation may be problematic for the target language model (we refer to this special portion as a segment); then use an adapted LM weight for the translation of the special segment but use the default LM weight for the rest of the sentence. We previously defined such special segment as Difficult to Translate Phrase (DTP) (Mohit and Hwa, 2007). We also described an SVM classifier that can predict whether a segment of source text will be problematic for the MT system with good accuracy. In an extended work, we adapted the language model of a SMT system for the translation of the DTPs (Mohit et al., 2009). For each DTP we automatically selected the relevant subset of the training data and constructed an adapted language model. Moreover, we translated the difficult phrase with the adapted model. In this paper, we continue our segment-specific system customization framework for testing our idea of modifying the language model weights. To find a better LM weight for the special segments, we consider two options. One is to treat all the special segments as a group and learn an appropriate weight for the group; another is to predict a weight value for each segment. The first option can be performed"
2010.amta-papers.17,P03-1021,0,0.00966444,"ic properties of the source-language text, etc. The influence of each feature is decided by its associated weight value; they are combined to determine the decoding score. For example, in the Table 1 which presents a standard Phrase-Based SMT (PB-SMT) formulation, the (λ)s are the decoding weights for the Language Model (LM), Translation Model (TM), Word Penalty (WP) and Distortion (d) features. The translation model feature function(φ) holds four model parameters, and each parameter gets an entry in the λφ weight vector. The weights are estimated using the Minimum Error Rate Training (MERT) (Och, 2003). MERT tunes the SMT system based on an iterative translation task performed on a development set. In each iteration, the MERT estimates a new set of decoding weights. It then check the effects of the new weights on the translation quality of the development data, using automatic evaluation metrics such as BLEU (Papineni et al., 2002). This evaluation is usually performed at the corpus level. After the training converges, the decoder computes scores for all translation hypotheses using the tuned weights, regardless of their characteristics. The goal of our work is to improve translation by usi"
2010.amta-papers.17,W06-3121,0,0.0139166,"uses of translation difficulties includes factors other than language modeling errors, but the LM features are influential enough for finding segments with LM-related problems. In most experiments in this work (except section 6), the most difficult segment is gold-standard. That means that for each sentence, the reference translations are used to find the segment with the lowest translation quality. 2.2 Implementation We modify the standard PB-SMT pipeline to facilitate LM weight adaptation. Our basic PB-SMT system uses the SRI package (Stolcke, 2002) as the target language model and Phramer (Olteanu et al., 2006), an open source implementation similar to Pharaoh (Koehn, 2004), as the decoder. We modify the decoder to allow the usage of different LM weights for different parts of a sentence; it accepts two decoding weights: One is used for the difficult segment, and the other one is used for the translation of the rest of the sentence. In order to apply this separation of the decoding weights, we constrain the choice of phrases in hypothesis expansion at the boundaries of the difficult segment. We allow the decoder to shift the difficult segment’s boundaries with one word to use more of the phrase tabl"
2010.amta-papers.17,P02-1040,0,0.0780057,"M), Translation Model (TM), Word Penalty (WP) and Distortion (d) features. The translation model feature function(φ) holds four model parameters, and each parameter gets an entry in the λφ weight vector. The weights are estimated using the Minimum Error Rate Training (MERT) (Och, 2003). MERT tunes the SMT system based on an iterative translation task performed on a development set. In each iteration, the MERT estimates a new set of decoding weights. It then check the effects of the new weights on the translation quality of the development data, using automatic evaluation metrics such as BLEU (Papineni et al., 2002). This evaluation is usually performed at the corpus level. After the training converges, the decoder computes scores for all translation hypotheses using the tuned weights, regardless of their characteristics. The goal of our work is to improve translation by using different decoder weights for different parts of a source sentence. Specifically, we propose to adapt the language model weight for decoding the problematic segment of a source sentence. This is a sequence of five to fifteen words whose characteristics are significantly different from the average case so as to cause problems for th"
2010.amta-papers.17,D08-1090,0,0.025947,"ned on a larger parallel corpus, there is still a 0.85 BLEU score improvement. This suggests that individual weight adaptation may still be helpful for larger MT systems. 7 Related Work The major concepts used in our work are adaptation, re-scoring, automatic MT evaluation, and ranking; they have been widely studied in the MT literature. In this section, we highlight some of the most relevant previous work. In previous work on MT adaptation, many proposed to modify the baseline system to incorporate features from the source language, lexical translations and from the underlying system itself (Snover et al., 2008; Tam et al., 2007; Kim, 2004). In these studies, the components of the baseline SMT system are modified. Furthermore, new models are constructed for the translation of special test sets or individual phrases and sentences. In contrast, our approach does not directly change the baseline components. Instead, we allow the decoder to adjust the influence from a component according to its expected performance. To perform this kind of judgment, we make use of techniques similar to confidence estimation and automatic MT evaluation. Previous work on MT evaluation without references focuses on sentent"
2010.amta-papers.17,2009.eamt-1.5,0,0.101502,"anslation candidates and their relative rankings. The ranking is determined according to their actual translation quality (measured by BLEU-3 against a human reference). Duplicate translations are removed so that each ranked list specifies a total order. The learning task is to predict the gold standard ranking based on the features that can be extracted from the translation candidates. In other words, the ranker’s objective can be seen as performing a kind of automatic MT evaluation metric that does not require references on the translation candidates. Based on related work in MT evaluation (Specia et al., 2009; Albrecht and Hwa, 2007), we use a similar set of features such as: • N-gram matching with the underlying LM corpus • N-gram probabilities from the LM • Target to source-language lexical ambiguity • Average word movement from source to targetlanguage • The ratio of punctuation and digit in the source and target phrases • The average BLEU score: A BLEU evaluation of the hypothesis, using the other competing hypothesis as the pseudo references model’s top choice resulted in translation improvements over the baseline. LM weight used Baseline adapt. by group adapt. of indiv. wt. • Bigram and trig"
2010.amta-papers.17,P07-1066,0,0.0196056,"lel corpus, there is still a 0.85 BLEU score improvement. This suggests that individual weight adaptation may still be helpful for larger MT systems. 7 Related Work The major concepts used in our work are adaptation, re-scoring, automatic MT evaluation, and ranking; they have been widely studied in the MT literature. In this section, we highlight some of the most relevant previous work. In previous work on MT adaptation, many proposed to modify the baseline system to incorporate features from the source language, lexical translations and from the underlying system itself (Snover et al., 2008; Tam et al., 2007; Kim, 2004). In these studies, the components of the baseline SMT system are modified. Furthermore, new models are constructed for the translation of special test sets or individual phrases and sentences. In contrast, our approach does not directly change the baseline components. Instead, we allow the decoder to adjust the influence from a component according to its expected performance. To perform this kind of judgment, we make use of techniques similar to confidence estimation and automatic MT evaluation. Previous work on MT evaluation without references focuses on sentential or corpus leve"
2010.amta-papers.17,W06-3110,0,0.0141766,"ial level. We are less interested in the absolute scores than in the quality of the candidates relative to each other. In this way, our approach is more similar to the metric of (Duh, 2008), in which the evaluation is conducted by ranking. Also relevant is the series of work on system modification, such as post-decoding discriminative re-ranking. The discriminative re-ranking benefits from a set of richer but more computationally expensive features. These features range from deeper linguistic knowledge (Och et al., 2004) to system related knowledge like word and phrase level confidence score (Zens and Ney, 2006). Similarly we benefit from additional features in our weight ranking; however, the set of candidate hypotheses generated for weight ranking is different from a decoder’s n-best list. 8 Conclusion In contrast with the traditional method of using static decoding weights, here we introduced a framework of using dynamic decoding weights. We explored varying the language model’s decoder weights based on the characteristics of the source text. Following the insight that the weight adaptation should be performed on the part of a sentence with which the baseline MT system is having problems, we turn"
2010.amta-papers.17,N04-1021,0,\N,Missing
2020.coling-main.428,2020.acl-main.265,0,0.0337959,"ental results show that the proposed approach mitigates the unintended bias at no or little cost of retrieval accuracy as compared to the original models; in fact, the retrieval accuracy for the modified BERT is slightly boosted. The code and data for this project is available.1 2 Political Ideology Bias on Social Topic Detection We investigate the impact of political ideology biases on extracting tweets relevant to specific social topics. Unlike gender or racial bias, which has been widely studied in language representation, machine translation or relation extraction (Stanovsky et al., 2019; Gaut et al., 2020; Blodgett and O’Connor, 2017), there exists fewer work on political ideology biases. Political ideology biases on social topic detection may arise from the difference in language usages between political ideological groups. Prior studies have compared different language usages between political ideological groups such as conservatives and liberals in the US. Such differences are reflected in general linguistics patterns such as language complexity (Schoonvelde et al., 2019) and emotions associated with language (Wojcik et al., 2015). Moreover, while talking about the same topic, language devi"
2020.wnut-1.26,N19-1423,0,0.072505,"as DKL (Ah ||Bh ), where Ah and Bh indicate the samples of class A and B augmented using heuristic h, and Equation 2 could be used to identify which heuristic is generating “harder to distinguish” samples and so more suitable for the classification task. arg min DKL (Ah ||Bh ) h (2) Finally, to transform sentences from their discrete word representation into a continuous distribution representation, we utilize a few of the numerous pre-trained embeddings that nowadays are the de facto approach for encoding sentences into vector space (Cho et al., 2014; Le and Mikolov, 2014; Cer et al., 2018; Devlin et al., 2019). We examine the applicability of our proposed approach by studying two classification tasks: sentiment analysis, as a resource-rich problem domain that allows experimenting with both labelpreserving and non-label-preserving heuristics, and verbosity analysis, as a resource-limited problem domain that the absence of sizable labeled data limits the options to non-label-preserving heuristics. 4 Augmented Datasets In this section, we go over some heuristic options for augmenting training corpora for sentiment analysis and verbosity detection domains. 202 4.1 Augmented Sentiment Corpus Our sentime"
2020.wnut-1.26,W13-1703,0,0.0790286,"uristic. We removed all of the original YPD sentences so that these datasets contain only augmented samples. We refer to each dataset with the same name as the heuristic function it is augmented with. Table 1 shows examples of sentences augmented using the label-preserving and non-label-preserving sentiment heuristics. 4.2 Augmented Verbosity Corpus The verbosity detection task is to predict whether a sentence is verbose or concise. Unlike the sentiment analysis domain, the set of existing resources for the verbosity detection problem is much more limited: NUCLE covers grammatical redundancy (Dahlmeier et al., 2013), and Kashefi et al. (2018) has a small corpus called Semantic Pleonasm Corpus (SPC) that contains semantic redundancy (i.e., 203 • Duplicate (DUP). This heuristic is an obvious case for word redundancy by duplicating an adjective word of the sentence right next to itself. • Synonym (SYN). This heuristic inserts a synonym next to an adjective word of the sentence. The conventional way to get synonyms of a word is to use WordNet, however, since these synonyms may express a different quality of the noun clause compared to the original adjective, augmented construction might not be semantically r"
2020.wnut-1.26,P17-2090,0,0.0225894,"t approaches for text data augmentation; for example, (Zhang et al., 2015; Wei and Zou, 2019) used thesaurus-based and (Wang and Yang, 2015; Kobayashi, 2018; Jiao et al., 2019) used embedding-based lexical substitution approach, (Wei and Zou, 2019; Xie et al., 2019) used random noise injection, including random word insertion, deletion, or sentence shuffling, (Luque, 2019) used instance crossover by combining halves of tweets, (Guo et al., 2019) adapt the mixup approach (Zhang et al., 2018) to text by interpolating the distributed representation of different sentences, (Sennrich et al., 2016; Fadaee et al., 2017; Xie et al., 2019) used back-translation, and (Hu et al., 2017; Iyyer et al., 2018; Anaby-Tavor et al., 2020; Kumar et al., 2020) used (deep) generative models to augment more training examples. However, with all these textual augmentation options, trying all of them for a (classifier) training task might be impractical, and to our best knowledge, there is not a guideline for how to choose between them for a task. 3 Quantification of Heuristics Suitability A straightforward approach to assess which heuristic and data augmentation approach is more appropriate for the task is to try every heuri"
2020.wnut-1.26,N18-1170,0,0.0193061,"ou, 2019) used thesaurus-based and (Wang and Yang, 2015; Kobayashi, 2018; Jiao et al., 2019) used embedding-based lexical substitution approach, (Wei and Zou, 2019; Xie et al., 2019) used random noise injection, including random word insertion, deletion, or sentence shuffling, (Luque, 2019) used instance crossover by combining halves of tweets, (Guo et al., 2019) adapt the mixup approach (Zhang et al., 2018) to text by interpolating the distributed representation of different sentences, (Sennrich et al., 2016; Fadaee et al., 2017; Xie et al., 2019) used back-translation, and (Hu et al., 2017; Iyyer et al., 2018; Anaby-Tavor et al., 2020; Kumar et al., 2020) used (deep) generative models to augment more training examples. However, with all these textual augmentation options, trying all of them for a (classifier) training task might be impractical, and to our best knowledge, there is not a guideline for how to choose between them for a task. 3 Quantification of Heuristics Suitability A straightforward approach to assess which heuristic and data augmentation approach is more appropriate for the task is to try every heuristic to generate an augmented dataset, then train a classifier on each and check th"
2020.wnut-1.26,N18-2036,1,0.297537,"e original YPD sentences so that these datasets contain only augmented samples. We refer to each dataset with the same name as the heuristic function it is augmented with. Table 1 shows examples of sentences augmented using the label-preserving and non-label-preserving sentiment heuristics. 4.2 Augmented Verbosity Corpus The verbosity detection task is to predict whether a sentence is verbose or concise. Unlike the sentiment analysis domain, the set of existing resources for the verbosity detection problem is much more limited: NUCLE covers grammatical redundancy (Dahlmeier et al., 2013), and Kashefi et al. (2018) has a small corpus called Semantic Pleonasm Corpus (SPC) that contains semantic redundancy (i.e., 203 • Duplicate (DUP). This heuristic is an obvious case for word redundancy by duplicating an adjective word of the sentence right next to itself. • Synonym (SYN). This heuristic inserts a synonym next to an adjective word of the sentence. The conventional way to get synonyms of a word is to use WordNet, however, since these synonyms may express a different quality of the noun clause compared to the original adjective, augmented construction might not be semantically redundant. For this reason,"
2020.wnut-1.26,D14-1181,0,0.00492962,"tains 50K nonparallel samples of each class. We refer to each dataset with the same name as the heuristic function that was used to augment it. Table 2 shows examples of sentences augmented using the non-label-preserving verbosity heuristics. While duplicating the word “delicious“ or adding “tasty” next to it makes the sentence verbose, adding “redolent” does not make it verbose because “redolent” and “delicious“ are describing different quality of the “bread.” 5 To measure the accuracy of sentiment and verbosity classification in answering Q1, we trained an LSTM (Liu et al., 2016) and a CNN (Kim, 2014) classifier on each the augmented dataset. The classification result for each task and augmented dataset is reported in Section 5.2. The LSTM and CNN models are trained on augmented corpora separately for each task; the sentiment classifiers are evaluated on a held-out portion of the YPD, and the verbosity classifiers are evaluated on SPC. None of the sentences of the held-out YPD and SPC are used during the creation of the augmented datasets. To answer Q2, we use two pre-trained encoder models: Universal Sentence Encoder (USE) (Cer et al., 2018) and Bidirectional Encoder Representations from"
2020.wnut-1.26,N18-2072,0,0.0150228,"ion: “which heuristic and data augmentation approach is more appropriate for a classification task?” In Section 3, we propose a low-cost approach to quantify the evaluation of different heuristics and the resulting augmented datasets for classification tasks. We believe our proposed approach could be a contribution to the NLP community because data augmentation has been shown to be useful for many NLP applications, with researchers proposing many different approaches for text data augmentation; for example, (Zhang et al., 2015; Wei and Zou, 2019) used thesaurus-based and (Wang and Yang, 2015; Kobayashi, 2018; Jiao et al., 2019) used embedding-based lexical substitution approach, (Wei and Zou, 2019; Xie et al., 2019) used random noise injection, including random word insertion, deletion, or sentence shuffling, (Luque, 2019) used instance crossover by combining halves of tweets, (Guo et al., 2019) adapt the mixup approach (Zhang et al., 2018) to text by interpolating the distributed representation of different sentences, (Sennrich et al., 2016; Fadaee et al., 2017; Xie et al., 2019) used back-translation, and (Hu et al., 2017; Iyyer et al., 2018; Anaby-Tavor et al., 2020; Kumar et al., 2020) used ("
2020.wnut-1.26,2020.lifelongnlp-1.3,0,0.01164,"ang, 2015; Kobayashi, 2018; Jiao et al., 2019) used embedding-based lexical substitution approach, (Wei and Zou, 2019; Xie et al., 2019) used random noise injection, including random word insertion, deletion, or sentence shuffling, (Luque, 2019) used instance crossover by combining halves of tweets, (Guo et al., 2019) adapt the mixup approach (Zhang et al., 2018) to text by interpolating the distributed representation of different sentences, (Sennrich et al., 2016; Fadaee et al., 2017; Xie et al., 2019) used back-translation, and (Hu et al., 2017; Iyyer et al., 2018; Anaby-Tavor et al., 2020; Kumar et al., 2020) used (deep) generative models to augment more training examples. However, with all these textual augmentation options, trying all of them for a (classifier) training task might be impractical, and to our best knowledge, there is not a guideline for how to choose between them for a task. 3 Quantification of Heuristics Suitability A straightforward approach to assess which heuristic and data augmentation approach is more appropriate for the task is to try every heuristic to generate an augmented dataset, then train a classifier on each and check the final classification performance (Qiu et al.,"
2020.wnut-1.26,P16-1009,0,0.0225407,"proposing many different approaches for text data augmentation; for example, (Zhang et al., 2015; Wei and Zou, 2019) used thesaurus-based and (Wang and Yang, 2015; Kobayashi, 2018; Jiao et al., 2019) used embedding-based lexical substitution approach, (Wei and Zou, 2019; Xie et al., 2019) used random noise injection, including random word insertion, deletion, or sentence shuffling, (Luque, 2019) used instance crossover by combining halves of tweets, (Guo et al., 2019) adapt the mixup approach (Zhang et al., 2018) to text by interpolating the distributed representation of different sentences, (Sennrich et al., 2016; Fadaee et al., 2017; Xie et al., 2019) used back-translation, and (Hu et al., 2017; Iyyer et al., 2018; Anaby-Tavor et al., 2020; Kumar et al., 2020) used (deep) generative models to augment more training examples. However, with all these textual augmentation options, trying all of them for a (classifier) training task might be impractical, and to our best knowledge, there is not a guideline for how to choose between them for a task. 3 Quantification of Heuristics Suitability A straightforward approach to assess which heuristic and data augmentation approach is more appropriate for the task"
2020.wnut-1.26,D13-1170,0,0.00860578,"Missing"
2020.wnut-1.26,D15-1306,0,0.0268789,"answer the key question: “which heuristic and data augmentation approach is more appropriate for a classification task?” In Section 3, we propose a low-cost approach to quantify the evaluation of different heuristics and the resulting augmented datasets for classification tasks. We believe our proposed approach could be a contribution to the NLP community because data augmentation has been shown to be useful for many NLP applications, with researchers proposing many different approaches for text data augmentation; for example, (Zhang et al., 2015; Wei and Zou, 2019) used thesaurus-based and (Wang and Yang, 2015; Kobayashi, 2018; Jiao et al., 2019) used embedding-based lexical substitution approach, (Wei and Zou, 2019; Xie et al., 2019) used random noise injection, including random word insertion, deletion, or sentence shuffling, (Luque, 2019) used instance crossover by combining halves of tweets, (Guo et al., 2019) adapt the mixup approach (Zhang et al., 2018) to text by interpolating the distributed representation of different sentences, (Sennrich et al., 2016; Fadaee et al., 2017; Xie et al., 2019) used back-translation, and (Hu et al., 2017; Iyyer et al., 2018; Anaby-Tavor et al., 2020; Kumar et"
2020.wnut-1.26,D19-1670,0,0.117555,"success of the task. In this paper, we aim to answer the key question: “which heuristic and data augmentation approach is more appropriate for a classification task?” In Section 3, we propose a low-cost approach to quantify the evaluation of different heuristics and the resulting augmented datasets for classification tasks. We believe our proposed approach could be a contribution to the NLP community because data augmentation has been shown to be useful for many NLP applications, with researchers proposing many different approaches for text data augmentation; for example, (Zhang et al., 2015; Wei and Zou, 2019) used thesaurus-based and (Wang and Yang, 2015; Kobayashi, 2018; Jiao et al., 2019) used embedding-based lexical substitution approach, (Wei and Zou, 2019; Xie et al., 2019) used random noise injection, including random word insertion, deletion, or sentence shuffling, (Luque, 2019) used instance crossover by combining halves of tweets, (Guo et al., 2019) adapt the mixup approach (Zhang et al., 2018) to text by interpolating the distributed representation of different sentences, (Sennrich et al., 2016; Fadaee et al., 2017; Xie et al., 2019) used back-translation, and (Hu et al., 2017; Iyyer et"
2021.wnut-1.41,2020.wnut-1.26,1,0.725687,"that allow us to (weakly) label some of its examples with at least one of the desired classes. For example, the Yelp dataset2 has a data category called “tips.” Since “tips” are very short sentences, they are likely to be concise; or Yelp reviews with 4 star and above are likely to carry a positive sentiment. Next, we apply some domain-inspired nonlabel-preserving augmentation heuristics to the instances of one class (e.g., “positive”) to generate instances of the opposite class (e.g., “negative”). The details of the heuristics and data augmentation for these domains are discussed elsewhere (Kashefi and Hwa, 2020). Table 2 summarizes the heuristic strategy we used to augment a training dataset for each prob2 lem domain. When evaluating the models for sentiment analysis and pleonasm detection tasks, we use the YPD-Test and SPC, respectively, which are both collected from Yelp. For the specificity detection task, we use the news headline part of the iSTS as the benchmark to evaluate the model. Thus, for data augmentation we use “all the news3 ” corpus, as a data source in a related domain. The resulting augmented training corpora for sentiment analysis, pleonasm detection, and specificity detection tasks"
2021.wnut-1.41,N18-2036,1,0.853437,"that are not contributing to the overall meaning of a sentence (Quinn, 1993; Lehmann, 2005). For example, the word “free” may deem semantically redundant at the presence of the word “gift” in the sentence: “I received a free gift.” Like sentiment analysis, this domain has a clear corresponding global classification task: whether the sentence is “concise” or “verbose”? However, this domain has a much more limited set of existing resources: NUCLE covers grammatical redundancy (Dahlmeier et al., 2013), and the Semantic Pleonasm Corpus (SPC), a small corpus with pleonasm annotation at wordlevel (Kashefi et al., 2018), which we use as the benchmark dataset in this experiment. Specificity Detection. This task at local level aims to pinpoint the phrases that uniquely relate the sentence to a particular subject (Li and Nenkova, 2015; Lugini and Litman, 2018). For example, the phrase “bus accident” in the sentence “10 people killed in bus accident in Pakistan”, makes it more specific than the phrase “road accident” in the sentence “10 people killed in road accident in 4.2 Local Prediction Inference from Pakistan.” It also has a corresponding global predicWeakly-Labeled User-Generated Data tion task: whether th"
2021.wnut-1.41,D16-1011,0,0.117896,"trapositive frameout which word(s) in the comment are contributed work outperforms the alternative approaches the most to the user’s decision to report the comon resource-constrained problem domains.1 ment, we already found the inappropriate words (local classification) and confirms whether the sen1 Introduction tence is correctly reported or not. Many NLP applications analyze a piece of text In prior work, researchers observed that local at multiple levels. For example, a product re- predictions might serve as rationales (Zaidan et al., view might be analyzed for whether it is informa- 2007; Lei et al., 2016; Bao et al., 2018) for the tive overall; its paragraphs might be analyzed for global problem. For example, if a review is classiwhether they are relevant to certain aspect of the fied as positive, there must be some words within product; or its words might be analyzed for whether that review that are also positive such that they they express intense emotions. These classification serve as the rationale for the overall prediction. tasks of varying scopes of context are often re- Thus, if rationales can be identified for the global lated. For instance, one might posit that a review classificati"
2021.wnut-1.41,louis-nenkova-2012-corpus,0,0.0771895,"Missing"
2021.wnut-1.41,D18-1216,0,0.0131707,"ut which word(s) in the comment are contributed work outperforms the alternative approaches the most to the user’s decision to report the comon resource-constrained problem domains.1 ment, we already found the inappropriate words (local classification) and confirms whether the sen1 Introduction tence is correctly reported or not. Many NLP applications analyze a piece of text In prior work, researchers observed that local at multiple levels. For example, a product re- predictions might serve as rationales (Zaidan et al., view might be analyzed for whether it is informa- 2007; Lei et al., 2016; Bao et al., 2018) for the tive overall; its paragraphs might be analyzed for global problem. For example, if a review is classiwhether they are relevant to certain aspect of the fied as positive, there must be some words within product; or its words might be analyzed for whether that review that are also positive such that they they express intense emotions. These classification serve as the rationale for the overall prediction. tasks of varying scopes of context are often re- Thus, if rationales can be identified for the global lated. For instance, one might posit that a review classification task, their loca"
2021.wnut-1.41,K16-1002,0,0.0293161,". Contrapositive Inference ∃ lij , ∃lij ∗ |F(Ci j∗ ) = ¬yi (4) Implementing contrapositive inference, however, is not as straightforward as direct inference. A bottom-up approach requires calculating an adversarial semantic alternative for each local feature to assess their contribution, which might be very i j complicated and resource-intensive. Instead of that, (1) we adapt a simpler top-down approach that can be Now, if we can find the local feature lij that is seen as a generalization of machine translation, or mainly responsible for making the greater-context as a form of style transfer (Bowman et al., 2016; Ci belong to the class yi , we may infer the same Shen et al., 2017; Yang et al., 2018; Prabhumoye class label for that local feature as well. The direct et al., 2018; Zhang et al., 2019). inference involves finding the most contributing We aim to develop a transfer function (see Seclocal features directly from the global prediction tion 3) that rewrites an arbitrary greater-context function. A straightforward way to implement this (e.g., a sentence) known to be in one class (e.g., scheme is to take the local feature with the highest Class A) into a corresponding text in another class weight"
2021.wnut-1.41,W13-1703,0,0.0187858,"llowing resource-constrained problem domains: Pleonasm Detection. This task at local level aims to find redundant words that are not contributing to the overall meaning of a sentence (Quinn, 1993; Lehmann, 2005). For example, the word “free” may deem semantically redundant at the presence of the word “gift” in the sentence: “I received a free gift.” Like sentiment analysis, this domain has a clear corresponding global classification task: whether the sentence is “concise” or “verbose”? However, this domain has a much more limited set of existing resources: NUCLE covers grammatical redundancy (Dahlmeier et al., 2013), and the Semantic Pleonasm Corpus (SPC), a small corpus with pleonasm annotation at wordlevel (Kashefi et al., 2018), which we use as the benchmark dataset in this experiment. Specificity Detection. This task at local level aims to pinpoint the phrases that uniquely relate the sentence to a particular subject (Li and Nenkova, 2015; Lugini and Litman, 2018). For example, the phrase “bus accident” in the sentence “10 people killed in bus accident in Pakistan”, makes it more specific than the phrase “road accident” in the sentence “10 people killed in road accident in 4.2 Local Prediction Infere"
2021.wnut-1.41,N19-1357,0,0.01393,"uting local features. We define the direct inference of local prediction from the global prediction as follows: Global → Local: if the global prediction for a greater-context be some Class A, there exists a smaller local portion that significantly influenced the global prediction in the first place so the local prediction for it could infer the same class label as the global prediction. However, it is reported that the classifiers might sometimes fail to learn the most semantically related features and make predictions based on just salient ones (Ribeiro et al., 2016; Mudrakarta et al., 2018; Jain and Wallace, 2019). We believe adding an extra constraint to the inference, while keeping the relationship between the local and corresponding global prediction intact, can improve the identification of the semantically related local features, therefore, we introduce the contrapositive inference of local prediction from the global prediction as follows: The proposed prediction framework is evaluated on several problem domains: sentiment analysis (as a resource-rich problem domain), and semantic pleonasm detection and specificity detection (as two resource-constrained domains). Results validate our insights abou"
2021.wnut-1.41,D15-1166,0,0.0224567,"riments to validate this aspect of the contrapositive local inference, we conducted a few experiments for local prediction inference from augmented training corpora of the global prediction in the sentiment analysis, pleonasm detection, and specificity detection problem domains (Section 4.2). In these set of experiments, we compare two implementation of each inference schemes (a simpler and a more complex models), as follows: Direct:ATN. For the global prediction, we use a BiLSTM classifier and wrap the weights at each time-step into an attention weight using a multiplicative attention layer (Luong et al., 2015). The local feature with the highest attention weight would receive the same class prediction as the global class label. Direct:RTNL. We use the model proposed by Lei et al. (2016) as the state-of-the-art implementation for the direct inference of the local prediction as the rationales for the global prediction (Equation 3), tuned for each experimental settings (see Appendix A). Their model consist a RCNN generator and encoder to produce rationales and predictions, and uses REINFORCE for generation learning (Sutton et al., 2000). Contrapositive:LOO. This baseline serves as a light version of t"
2021.wnut-1.41,P18-1080,0,0.0415564,"Missing"
2021.wnut-1.41,N16-3020,0,0.380684,"fi/contrapositive-inference the most semantically relevant rationales. For ex371 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 371–380 November 11, 2021. ©2021 Association for Computational Linguistics ample, irrelevant words or punctuation marks are more influential to the decision of neural text classifiers than verbs or other semantically related textual units (Mudrakarta et al., 2018); or the presence of snow in an image is the main feature to distinguish huskies from wolves rather than the features related to the animal themselves (Ribeiro et al., 2016). In this work, we argue that this semantic relationship can be made more robust by enforcing the contrapositive constraint between the local prediction and its corresponding global prediction. That is, suppose we know that an instance is globally predicted to belong to some Class A and that some local portion l contributed the most to that prediction (the rationale); if l is replaced so as to negate its semantic contribution, then the global label should also change (e.g. now belongs to some Class B). We propose that, if, and only if, the contrapositive constraint is satisfied should the loca"
2021.wnut-1.41,D13-1170,0,0.015723,"Missing"
2021.wnut-1.41,W19-4807,0,0.0176827,"le, consider the sentiinference is to consider the contributing local features as the rationales for the global prediction. Ra- ment classification problem. The transfer function might rewrite a positive sentence: “the food tionales are defined as the reason behind the label annotation for the global prediction (Zaidan et al., was great” into a negative sentence “The food was awful.” While the overall sentence length is the 2007) and mostly used to improve the classification same, the learned transfer function chose to replace of the greater-context (Marshall et al., 2016; Zhang et al., 2016; Strout et al., 2019; Du et al., 2019). “great” with “awful,” therefore “great” is likely to However, some studies have tried to develop sys- be a sentiment expressing word. By casting the problem as a style transfer task, we avoid the thorny tems for automatic extraction of the rationales (Lei tasks of quantifying fluency and meaning retention et al., 2016; Ehsan et al., 2018) as the smaller text span that could replace the whole greater-context, and even if the transfer function does not provide a correct semantic negation, for example, rewrites while keeping the global-prediction intact: the “great” as “cold”"
2021.wnut-1.41,P14-2066,0,0.0187509,"ed Near-Miss Concise Heuristic: insert a non-synonym → delicious +redolent bread word next to an adjective, base on language model prediction • Specific Label: contains more than 2 named entities News Headline → Rouhani wants nuclear deal • Augmented General Heuristic: substitute a noun with an hyponym → President wants nuclear deal • General Label: contains no named entities → 10 killed in camp • Augmented Specific Heuristic: substitute a noun with an hypernym → 10 killed in death camp Table 2: Augmentation heuristic strategies for different problem domains Nenkova, 2012; Louis et al., 2013; Tan and Lee, 2014), and the Interpretable Semantic Textual Similarity (iSTS) dataset, which comprises phrase-level specificity annotation (Agirre et al., 2016), which we use as the benchmark in our experiment. In addition, we also make a low-resource case for the sentiment analysis domain in order to compare the prediction result of our approach with previous experiment, where a sizable training corpus was available for global prediction. 4.2.1 Data Augmentation In order to augment a sizable dataset to train the inference models, we start by identifying an existing real-world data source that possesses some cha"
2021.wnut-1.41,N07-1033,0,0.165859,"Missing"
2021.wnut-1.41,D16-1076,0,0.019988,"n illustrative example, consider the sentiinference is to consider the contributing local features as the rationales for the global prediction. Ra- ment classification problem. The transfer function might rewrite a positive sentence: “the food tionales are defined as the reason behind the label annotation for the global prediction (Zaidan et al., was great” into a negative sentence “The food was awful.” While the overall sentence length is the 2007) and mostly used to improve the classification same, the learned transfer function chose to replace of the greater-context (Marshall et al., 2016; Zhang et al., 2016; Strout et al., 2019; Du et al., 2019). “great” with “awful,” therefore “great” is likely to However, some studies have tried to develop sys- be a sentiment expressing word. By casting the problem as a style transfer task, we avoid the thorny tems for automatic extraction of the rationales (Lei tasks of quantifying fluency and meaning retention et al., 2016; Ehsan et al., 2018) as the smaller text span that could replace the whole greater-context, and even if the transfer function does not provide a correct semantic negation, for example, rewrites while keeping the global-prediction intact: t"
C10-2157,P06-1032,0,0.35069,"t al., 2008), and mass versus count nouns (Nagata et al., 2006). However, previous work suggests that grammar error correction is considerably more challenging than detection (Han et al., 2010). Furthermore, an ESL learner’s writing may contain multiple interacting errors that are difficult to detect and correct in isolation. A promising research direction is to tackle automatic grammar error correction as a machine translation (MT) problem. The disfluent sentences produced by an ESL learner can be seen as the input source language, and the corrected revision is the result of the translation. Brockett et al. (2006) showed that phrase-based statistical MT can help to correct mistakes made on mass nouns. To our knowledge, phrase-based MT techniques have not been applied for rewriting entire sentences. One major challenge is the lack of appropriate training data such as a sizable parallel corpus. Another concern is that phrasebased MT may not be similar enough to the problem of correcting ESL learner mistakes. While MT rewrites an entire source sentence into the target language, not every word written by an ESL learner needs to be modified. Another alternative that may afford a more general model of ESL er"
C10-2157,2003.mtsummit-papers.6,0,0.0514579,"inal draft and R represents the revised draft. Note that while R is assumed to be an improvement upon O, its quality may fall short of the gold standard revision, G. To train the syntax-driven MT models, we optimize the joint probability of observing the sentence pair, Pr(O, R), through some form of mapping between their parse trees, τO and τR . An added wrinkle to our problem is that it might not always be possible to assign a sensible syntactic structure to an ungrammatical sentence. It is well-known that an English parser trained on the Penn Treebank is bad at handling disfluent sentences (Charniak et al., 2003; Foster et al., 2008). In our domain, since O (and perhaps also R) might be disfluent, an important question that a translation model must address is: how should the mapping between the trees τO and τR be handled? 3 Syntax-Driven Models for Essay Revisions There is extensive literature on syntax-driven approaches to MT (cf. a recent survey by 1374 Lopez (2008)); we focus on two particular formalisms that reflects different perspectives on the role of syntax. Our goal is to assess which formalism is a better fit with the domain of essay revision modeling, in which the data largely consist of i"
C10-2157,W07-1604,0,0.175679,"offers a more plausible model of ESL learners’ revision writing process. 1 Introduction When learning a second language, students make mistakes along the way. While some mistakes are idiosyncratic and individual, many are systematic and common to people who share the same primary language. There has been extensive research on grammar error detection. Most previous efforts focus on identifying specific types of problems commonly encountered by English as a Second Language (ESL) learners. Some examples include the proper usage of determiners (Yi et al., 2008; Gamon et al., 2008), prepositions (Chodorow et al., 2007; Gamon et al., 2008; Hermet et al., 2008), and mass versus count nouns (Nagata et al., 2006). However, previous work suggests that grammar error correction is considerably more challenging than detection (Han et al., 2010). Furthermore, an ESL learner’s writing may contain multiple interacting errors that are difficult to detect and correct in isolation. A promising research direction is to tackle automatic grammar error correction as a machine translation (MT) problem. The disfluent sentences produced by an ESL learner can be seen as the input source language, and the corrected revision is t"
C10-2157,P09-1053,0,0.0227631,"Missing"
C10-2157,P08-2056,0,0.067927,"Missing"
C10-2157,J03-1002,0,0.00292943,"el, the initial parameters are determined as follows: For the monolingual target parsing model parameters, we first parse the target side of the corpus (i.e., the revised sentences) with the Stanford parser; we then use the maximum likelihood estimates based on these parse trees to initialize the parameters of the target parser, Dependency Model with Valence (DMV). We uniformly initialized the configuration parameters; the parent-child configuration and other configuration each has 0.5 probability. For the alignment parameters, we ran the GIZA++ implementation of the IBM word alignment model (Och and Ney, 2003) on the sentence pairs, and used the resulting translation table as our initial estimation. There may be better initialization setups, but the difference between those setups will become small after a few rounds of EM. Once trained, the two models compute the joint probability of every sentence pair in the test corpus as described in Section 3.3. 4.3 Experiment I To evaluate how well the models describe the ESL revision domain, we want to see which model is less “surprised” by the test data. We expected that the better model should be able to transform more sentence pair in the test corpus; we"
C10-2157,I08-1059,0,0.108591,"verage, the tree-to-string approach offers a more plausible model of ESL learners’ revision writing process. 1 Introduction When learning a second language, students make mistakes along the way. While some mistakes are idiosyncratic and individual, many are systematic and common to people who share the same primary language. There has been extensive research on grammar error detection. Most previous efforts focus on identifying specific types of problems commonly encountered by English as a Second Language (ESL) learners. Some examples include the proper usage of determiners (Yi et al., 2008; Gamon et al., 2008), prepositions (Chodorow et al., 2007; Gamon et al., 2008; Hermet et al., 2008), and mass versus count nouns (Nagata et al., 2006). However, previous work suggests that grammar error correction is considerably more challenging than detection (Han et al., 2010). Furthermore, an ESL learner’s writing may contain multiple interacting errors that are difficult to detect and correct in isolation. A promising research direction is to tackle automatic grammar error correction as a machine translation (MT) problem. The disfluent sentences produced by an ESL learner can be seen as the input source lang"
C10-2157,W06-3104,0,0.3188,"ners revise their writings: they are transforming structures in their primary language to those in the new language. In this paper, we conduct a first inquiry into the applicability of syntax-driven MT methods to automatic grammar error correction. In particular, we investigate whether a syntaxdriven model can capture ESL students’ process of writing revisions. We compare two approaches: a tree-to-string mapping proposed by Yamada & Knight (2001) and a tree-totree mapping using the Quasi-Synchronous 1373 Coling 2010: Poster Volume, pages 1373–1381, Beijing, August 2010 Grammar (QG) formalism (Smith and Eisner, 2006). We train both models on a parallel corpus consisting of multiple drafts of essays by ESL students. The approaches are evaluated on how well they model the revision pairs in an unseen test corpus. Experimental results suggest that 1) the QG model has more flexibility and is able to describe more types of transformations; but 2) the YK model is better at capturing the incremental improvements in the ESL learners’ revision writing process. 2 are typically too many students for the teachers to provide detailed manual inspection and correction at a large scale. More commonly, students are asked t"
C10-2157,han-etal-2010-using,0,0.013124,"ematic and common to people who share the same primary language. There has been extensive research on grammar error detection. Most previous efforts focus on identifying specific types of problems commonly encountered by English as a Second Language (ESL) learners. Some examples include the proper usage of determiners (Yi et al., 2008; Gamon et al., 2008), prepositions (Chodorow et al., 2007; Gamon et al., 2008; Hermet et al., 2008), and mass versus count nouns (Nagata et al., 2006). However, previous work suggests that grammar error correction is considerably more challenging than detection (Han et al., 2010). Furthermore, an ESL learner’s writing may contain multiple interacting errors that are difficult to detect and correct in isolation. A promising research direction is to tackle automatic grammar error correction as a machine translation (MT) problem. The disfluent sentences produced by an ESL learner can be seen as the input source language, and the corrected revision is the result of the translation. Brockett et al. (2006) showed that phrase-based statistical MT can help to correct mistakes made on mass nouns. To our knowledge, phrase-based MT techniques have not been applied for rewriting"
C10-2157,hermet-etal-2008-using,0,0.0613289,"Missing"
C10-2157,P04-1061,0,0.0173901,"Missing"
C10-2157,D09-1086,0,0.0289696,"Missing"
C10-2157,D07-1003,0,0.030383,"ginal draft). 3.2 Tree-to-Tree Model The Quasi-Synchronous Grammar formalism (Smith and Eisner, 2006) is a generative model that aims to produce the most likely target tree for a given source tree. It differs from the more strict synchronous grammar formalisms (Wu, 1995; Melamed et al., 2004) because it does not try to perform simultaneous parsing on parallel grammars; instead, the model learns an augmented target-language grammar whose rules make “soft alignments” with a given source tree. QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al., 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). In this work we use an instantiation of QG that largely follows the model described by Smith and Eisner (2006). The model is trained on a parallel corpus in which both the first-draft and revised sentences have been parsed. Using EM to estimate its parameters, it learns an augmented target PCFG grammar1 whose production rules form associations with the given source trees. Consider the scenario in Figure 1. Given a source tree τO , the trained model generates a target tree by expand"
C10-2157,P01-1067,0,0.0586178,"syntax-driven MT model may not need to train on a very large parallel corpus. Second, syntactic transformations provide an intuitive description of how second language learners revise their writings: they are transforming structures in their primary language to those in the new language. In this paper, we conduct a first inquiry into the applicability of syntax-driven MT methods to automatic grammar error correction. In particular, we investigate whether a syntaxdriven model can capture ESL students’ process of writing revisions. We compare two approaches: a tree-to-string mapping proposed by Yamada & Knight (2001) and a tree-totree mapping using the Quasi-Synchronous 1373 Coling 2010: Poster Volume, pages 1373–1381, Beijing, August 2010 Grammar (QG) formalism (Smith and Eisner, 2006). We train both models on a parallel corpus consisting of multiple drafts of essays by ESL students. The approaches are evaluated on how well they model the revision pairs in an unseen test corpus. Experimental results suggest that 1) the QG model has more flexibility and is able to describe more types of transformations; but 2) the YK model is better at capturing the incremental improvements in the ESL learners’ revision w"
C10-2157,I08-2082,0,0.019397,"ides a greater coverage, the tree-to-string approach offers a more plausible model of ESL learners’ revision writing process. 1 Introduction When learning a second language, students make mistakes along the way. While some mistakes are idiosyncratic and individual, many are systematic and common to people who share the same primary language. There has been extensive research on grammar error detection. Most previous efforts focus on identifying specific types of problems commonly encountered by English as a Second Language (ESL) learners. Some examples include the proper usage of determiners (Yi et al., 2008; Gamon et al., 2008), prepositions (Chodorow et al., 2007; Gamon et al., 2008; Hermet et al., 2008), and mass versus count nouns (Nagata et al., 2006). However, previous work suggests that grammar error correction is considerably more challenging than detection (Han et al., 2010). Furthermore, an ESL learner’s writing may contain multiple interacting errors that are difficult to detect and correct in isolation. A promising research direction is to tackle automatic grammar error correction as a machine translation (MT) problem. The disfluent sentences produced by an ESL learner can be seen as"
C10-2157,P08-1021,0,0.0187273,"lier and later drafts are reversed. If a model is trained to prefer more fluent English sentences are the revision, it should be perplexed on this corpus. information retrieval techniques. Chodorow et al. (2007) instead treat it as a classification problem and employed a maximum entropy classifier. Similar to our approach, Brockett et al. (2006) view error correction as a Machine Translation problem. But their translation system is built on phrase level, with the purpose of correcting local errors such as mass noun errors. The problem of error correction at a syntactic level is less explored. Lee and Seneff (2008) examined the task of correcting verb form misuse by applying tree template matching rules. The parse tree transformation rules are learned from synthesized training data. 6 Conclusion This paper investigates the suitability of syntax-driven MT approaches for modeling the revision writing process of ESL learners. We have considered both the Yamada & Knight tree-to-string model, which only considers syntactic information from the typically more fluent revised text, as well as QuasiSynchronous Grammar, a tree-to-tree model that attempts to learn syntactic transformation patterns between the stud"
C10-2157,P00-1067,0,0.0428661,"features such as word identity and POS tagging information are combined to deal with some specific kind of error. Among them, (Burstein et al., 2004) developed a tool called Critique that detects collocation errors and word choice errors. Nagata et al. (2006) uses a rule-based approach in distinguishing mass and count nouns. Knight and Chander (1994) and Han et al. (2006) both addressed the misuse of articles. Chodorow et al. (2007), Gamon et al. (2008), Hermet et al. (2008) proposed several techniques in detecting and correcting proposition errors. In detecting errors and giving suggestions, Liu et al. (2000), Gamon et al. (2008) and Hermet et al. (2008) make use of 1379 number of instances average edit distance percentage of identical pairs average O length average R length QG cross entropy YK cross entropy DQG ∩ DY K 19 0.05 0.95 9.00 9.05 81.85 51.2 Neither 50 2.88 0.40 14.18 14.98 N/A N/A DQG − DY K 30 2.17 0.5 12.53 12.47 139.36 N/A DY K − DQG 1 1 0 17 16 N/A 103.75 Table 4: This table compares the two models on a “trick” test corpus in which the earlier and later drafts are reversed. If a model is trained to prefer more fluent English sentences are the revision, it should be perplexed on thi"
C10-2157,P04-1084,0,0.0275973,"stituency orderings, can be modeled by a combination of the insert, replace, and reorder operators. The YK model allows us to perform transformations on a higher syntactic level. Another potential benefit is that the model does not attempt to assign syntactic interpretations over the source sentences (i.e., the less fluent original draft). 3.2 Tree-to-Tree Model The Quasi-Synchronous Grammar formalism (Smith and Eisner, 2006) is a generative model that aims to produce the most likely target tree for a given source tree. It differs from the more strict synchronous grammar formalisms (Wu, 1995; Melamed et al., 2004) because it does not try to perform simultaneous parsing on parallel grammars; instead, the model learns an augmented target-language grammar whose rules make “soft alignments” with a given source tree. QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al., 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). In this work we use an instantiation of QG that largely follows the model described by Smith and Eisner (2006). The model is trained on a parallel corpus in whic"
C10-2157,P06-1031,0,0.180396,"learning a second language, students make mistakes along the way. While some mistakes are idiosyncratic and individual, many are systematic and common to people who share the same primary language. There has been extensive research on grammar error detection. Most previous efforts focus on identifying specific types of problems commonly encountered by English as a Second Language (ESL) learners. Some examples include the proper usage of determiners (Yi et al., 2008; Gamon et al., 2008), prepositions (Chodorow et al., 2007; Gamon et al., 2008; Hermet et al., 2008), and mass versus count nouns (Nagata et al., 2006). However, previous work suggests that grammar error correction is considerably more challenging than detection (Han et al., 2010). Furthermore, an ESL learner’s writing may contain multiple interacting errors that are difficult to detect and correct in isolation. A promising research direction is to tackle automatic grammar error correction as a machine translation (MT) problem. The disfluent sentences produced by an ESL learner can be seen as the input source language, and the corrected revision is the result of the translation. Brockett et al. (2006) showed that phrase-based statistical MT"
C12-1178,P06-1032,0,0.0305404,"ade by ESL writers are not random. In their studies, Rozovskaya & Roth (2011) find that those who share the same native language tend to make similar types of mistakes. The natural question that arises is: what are the underlying causes for the mistakes? In the frame of computational linguistics research, the question might be rephrased as: Can we build a mathematical model that simulates ESL writing mistakes? A model that builds a table of confusion sets whose distributions correlate well with the mistakes made by ESL writers is an important component in simulating ESL writing. For instance, Brockett et al. (2006) simulates an ESL corpus according to a set of manually constructed rules, which would not be available until confusion sets are established. In addition to aiding our understanding of the underlying causes of ESL writing mistakes, confusion sets also have useful practical applications. Generally speaking, reducing the confusion set helps lead the classifiers in the GEC system to a better performance by prohibiting them from considering the outcomes that are both unlikely and misleading. For example, although ESL learners normally would not confuse within with in, classifiers may have difficul"
C12-1178,J93-2003,0,0.0239061,"Missing"
C12-1178,W07-1604,0,0.0351711,"15–2930, COLING 2012, Mumbai, December 2012. 2915 1 Introduction A large portion of the English text (e.g., on the web) is written by people whose native language is not English. Many English as a Second Language (ESL) writers, even those with a high level of proficiency, make common grammatical mistakes. Researchers working on Grammar Error Correction (GEC) try to analyze the patterns of these mistakes in order to understand the underlying reasons for their occurrence and to build tools that help ESL writers to correct their errors (Leacock et al., 2010). Many recently developed GEC systems (Chodorow et al., 2007; J. R. Tetreault & Chodorow, 2008; Gamon et al., 2009; Liu et al., 2010; Rozovskaya & Roth, 2011; Dahlmeier & Ng, 2011a) share a similar infrastructure: first, they isolate some specific types of errors (e.g., preposition errors, article errors, or word choice errors); then, they propose a correction for each instance by treating it as a classification problem. To cast the correction problem as a classification problem, the system has to know, a priori, what are the set of possible corrections for an error. That is, the system needs to pre-define a confusion set for each error type. Previous"
C12-1178,D11-1010,0,0.159205,"is written by people whose native language is not English. Many English as a Second Language (ESL) writers, even those with a high level of proficiency, make common grammatical mistakes. Researchers working on Grammar Error Correction (GEC) try to analyze the patterns of these mistakes in order to understand the underlying reasons for their occurrence and to build tools that help ESL writers to correct their errors (Leacock et al., 2010). Many recently developed GEC systems (Chodorow et al., 2007; J. R. Tetreault & Chodorow, 2008; Gamon et al., 2009; Liu et al., 2010; Rozovskaya & Roth, 2011; Dahlmeier & Ng, 2011a) share a similar infrastructure: first, they isolate some specific types of errors (e.g., preposition errors, article errors, or word choice errors); then, they propose a correction for each instance by treating it as a classification problem. To cast the correction problem as a classification problem, the system has to know, a priori, what are the set of possible corrections for an error. That is, the system needs to pre-define a confusion set for each error type. Previous work has shown the importance of the role of confusion sets. However, the construction of confusion sets requires a gre"
C12-1178,P11-1092,0,0.130482,"is written by people whose native language is not English. Many English as a Second Language (ESL) writers, even those with a high level of proficiency, make common grammatical mistakes. Researchers working on Grammar Error Correction (GEC) try to analyze the patterns of these mistakes in order to understand the underlying reasons for their occurrence and to build tools that help ESL writers to correct their errors (Leacock et al., 2010). Many recently developed GEC systems (Chodorow et al., 2007; J. R. Tetreault & Chodorow, 2008; Gamon et al., 2009; Liu et al., 2010; Rozovskaya & Roth, 2011; Dahlmeier & Ng, 2011a) share a similar infrastructure: first, they isolate some specific types of errors (e.g., preposition errors, article errors, or word choice errors); then, they propose a correction for each instance by treating it as a classification problem. To cast the correction problem as a classification problem, the system has to know, a priori, what are the set of possible corrections for an error. That is, the system needs to pre-define a confusion set for each error type. Previous work has shown the importance of the role of confusion sets. However, the construction of confusion sets requires a gre"
C12-1178,P99-1004,0,0.078442,"ad, they infer words’meaning/function from the context, and then connecting the new words to the words they are already feel familiar(Fischer, 1990). Under this perspective, learning the extensions of words is not explicit, it is a means to achieve the primary goal of understanding word meanings. To simulates an intension based learner, we build a model of word similarity metrics from processing standard English text. Specifically, we build distributional models in which the similarities of words are calculated from a comparison of the contexts they appear in (Pereira et al., 1993; Lin, 1998; Lee, 1999). Then, to fill in a word’s confusion set, we pick the words that are most similar according to the metric. Pantel & Lin (2002) showed this method is able to yield similarities that correlate well with the similarities of words’ intensions. In our work, we calculate the words’ intension similarity by using a distributional model (Pereira et al., 1993; Lee, 1999), in which each preposition is represented as a distributional vector of its 2918 context features. Examples of usage contexts that have been shown to be relevant for the task of preposition selection in previous work (De Felice, 2008;"
C12-1178,D10-1104,0,0.204008,"ion of the English text (e.g., on the web) is written by people whose native language is not English. Many English as a Second Language (ESL) writers, even those with a high level of proficiency, make common grammatical mistakes. Researchers working on Grammar Error Correction (GEC) try to analyze the patterns of these mistakes in order to understand the underlying reasons for their occurrence and to build tools that help ESL writers to correct their errors (Leacock et al., 2010). Many recently developed GEC systems (Chodorow et al., 2007; J. R. Tetreault & Chodorow, 2008; Gamon et al., 2009; Liu et al., 2010; Rozovskaya & Roth, 2011; Dahlmeier & Ng, 2011a) share a similar infrastructure: first, they isolate some specific types of errors (e.g., preposition errors, article errors, or word choice errors); then, they propose a correction for each instance by treating it as a classification problem. To cast the correction problem as a classification problem, the system has to know, a priori, what are the set of possible corrections for an error. That is, the system needs to pre-define a confusion set for each error type. Previous work has shown the importance of the role of confusion sets. However, th"
C12-1178,J04-4002,0,0.0898419,"Missing"
C12-1178,P93-1024,0,0.536052,"orizing dictionary entries; instead, they infer words’meaning/function from the context, and then connecting the new words to the words they are already feel familiar(Fischer, 1990). Under this perspective, learning the extensions of words is not explicit, it is a means to achieve the primary goal of understanding word meanings. To simulates an intension based learner, we build a model of word similarity metrics from processing standard English text. Specifically, we build distributional models in which the similarities of words are calculated from a comparison of the contexts they appear in (Pereira et al., 1993; Lin, 1998; Lee, 1999). Then, to fill in a word’s confusion set, we pick the words that are most similar according to the metric. Pantel & Lin (2002) showed this method is able to yield similarities that correlate well with the similarities of words’ intensions. In our work, we calculate the words’ intension similarity by using a distributional model (Pereira et al., 1993; Lee, 1999), in which each preposition is represented as a distributional vector of its 2918 context features. Examples of usage contexts that have been shown to be relevant for the task of preposition selection in previous"
C12-1178,D10-1094,0,0.310549,"ice errors); then, they propose a correction for each instance by treating it as a classification problem. To cast the correction problem as a classification problem, the system has to know, a priori, what are the set of possible corrections for an error. That is, the system needs to pre-define a confusion set for each error type. Previous work has shown the importance of the role of confusion sets. However, the construction of confusion sets requires a great deal of human involvement. English teachers are involved in Liu et al. (2010) to manually filter the initial large verb confusion sets; Rozovskaya & Roth (2010a) used annotated ESL corpus to limit their confusion sets for prepositions. They have shown that even for closed word classes such as prepositions, limiting the confusion sets help simplify the classifiers’ tasks and finally lead to both a better precision and recall. In this paper, we propose a method to automatically construct confusion sets without manual intervention or an annotated ESL corpus. Our approach is to model and simulate how ESL learners might learn words from reading English text. In the process of mastering the language, the learners are often confused about how to choose bet"
C12-1178,P11-1093,0,0.0501481,"text (e.g., on the web) is written by people whose native language is not English. Many English as a Second Language (ESL) writers, even those with a high level of proficiency, make common grammatical mistakes. Researchers working on Grammar Error Correction (GEC) try to analyze the patterns of these mistakes in order to understand the underlying reasons for their occurrence and to build tools that help ESL writers to correct their errors (Leacock et al., 2010). Many recently developed GEC systems (Chodorow et al., 2007; J. R. Tetreault & Chodorow, 2008; Gamon et al., 2009; Liu et al., 2010; Rozovskaya & Roth, 2011; Dahlmeier & Ng, 2011a) share a similar infrastructure: first, they isolate some specific types of errors (e.g., preposition errors, article errors, or word choice errors); then, they propose a correction for each instance by treating it as a classification problem. To cast the correction problem as a classification problem, the system has to know, a priori, what are the set of possible corrections for an error. That is, the system needs to pre-define a confusion set for each error type. Previous work has shown the importance of the role of confusion sets. However, the construction of confusi"
C12-1178,P10-2065,0,0.0891113,"hen, to fill in a word’s confusion set, we pick the words that are most similar according to the metric. Pantel & Lin (2002) showed this method is able to yield similarities that correlate well with the similarities of words’ intensions. In our work, we calculate the words’ intension similarity by using a distributional model (Pereira et al., 1993; Lee, 1999), in which each preposition is represented as a distributional vector of its 2918 context features. Examples of usage contexts that have been shown to be relevant for the task of preposition selection in previous work (De Felice, 2008; J. Tetreault et al., 2010; Dahlmeier & Ng, 2011a) include: Gov: the syntactic dependency governors of the preposition Obj: the dependency objects of the preposition GovTag, ObjTag: the part-of-speech tags of the dependency governors and objects L1-Trans: L1 translations of the preposition We employ Gov, Obj, GovTag, ObjTag features to capture the grammatical context of the preposition selection. We also employ L1-Trans to capture both the intended semantic meaning of the preposition and the L1 background information which was shown to be relevant to confusions(Rozovskaya & Roth, 2010a; Dahlmeier & Ng, 2011a). The dist"
C12-1178,C08-1109,0,0.127461,"December 2012. 2915 1 Introduction A large portion of the English text (e.g., on the web) is written by people whose native language is not English. Many English as a Second Language (ESL) writers, even those with a high level of proficiency, make common grammatical mistakes. Researchers working on Grammar Error Correction (GEC) try to analyze the patterns of these mistakes in order to understand the underlying reasons for their occurrence and to build tools that help ESL writers to correct their errors (Leacock et al., 2010). Many recently developed GEC systems (Chodorow et al., 2007; J. R. Tetreault & Chodorow, 2008; Gamon et al., 2009; Liu et al., 2010; Rozovskaya & Roth, 2011; Dahlmeier & Ng, 2011a) share a similar infrastructure: first, they isolate some specific types of errors (e.g., preposition errors, article errors, or word choice errors); then, they propose a correction for each instance by treating it as a classification problem. To cast the correction problem as a classification problem, the system has to know, a priori, what are the set of possible corrections for an error. That is, the system needs to pre-define a confusion set for each error type. Previous work has shown the importance of t"
C12-1178,N10-1018,0,\N,Missing
C12-1178,I08-1059,0,\N,Missing
C98-1088,P98-1091,1,0.0584837,"tains all possible rules in Chomsky Normal Form constructed by the nonterminal and terminal symbols. The initial parameters associated with each rule are randomly generated subject to an admissibility constraint. As long as all the rules have a non-zero probability, any string has a non-zero chance of being generated. To train the grammar, we follow the InsideOutside re-estimation algorithm described by Lari and Young (1990). The Inside-Outside reestimation algorithm can also be extended to train PLTIGs. The equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998). As with PCFGs, the initial grammar must be able to generate any string. A simple PLTIG that fits the requirement is one that simulates a bigram model. It is represented by a tree set that contains a right auxiliary tree for each lexical item as depicted in Figure 1. Each tree has one adjunction site into which other right auxiliary trees can adjoin. The tree set has only one initial tree, which is anchored by an empty lexical item. The initial tree represents the start of the sentence. Any string can be constructed by right adjoining the words together in order. Training the parameters of th"
C98-1088,P92-1017,0,0.0725836,"and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its nonlexicalized counterpart. Furthermore, training of PLTIGs displays faster convergence than PCFGs. and Waters, 1994). We now apply a probabilistic variant of this formalism, Probabilistic Tree Insertion Grammars (PLTIGs), to natural language processing problems of stochastic parsing and language modeling. This paper presents two sets of experiments, comparing PLTIGs with non-lexicalized Probabilistic Context-Free Grammars (PCFGs) (Pereira and Schabes, 1992) and non-hierarchical N-gram models that use the right branching bracketing heuristics (period attaches high) as their parsing strategy. We show that PLTIGs can be induced from partially bracketed data, and that the resulting trained grammars can parse unseen sentences and estimate the likelihood of their occurrences in the language. The experiments are run on two corpora: the Air Travel Information System (ATIS) corpus and a subset of the Wall Street Journal TreeBank corpus. The results show that the lexicalized nature of the formalism helps our induced PLTIGs to converge faster and provide a"
C98-1088,P95-1023,0,0.0128498,"parsable, as shown by Schabes and Waters (1994). Furthermore, LTIGs can be parameterized to form probabilistic models (Schabes and Waters, 1993). Informally speaking, a parameter is associated with each possible adjunction or substitution operation between a tree and a node. For instance, suppose there are V left auxiliary trees that might adjoin into node ~. Then there are V + 1 parameters associated with node 1The best theoretical upper bound on time complexity for the recognition of Tree Adjoining Languages is O(M(n2)), where M(k) is the time needed to multiply two k x k boolean matrices.(Rajasekaran and Yooseph, 1995) 558 E|ementttT Tt.¢¢Sets: ttnlt tw°rd l tw°rd 2 Example sentence: The cat tw°rd n chases the mouse Corresponding derivation tree: E X * word I X * word 2 X , word ~init n .~t. adj. tthe .~.~t. .dj. t e a t . . ~ t . adj. Figure 1: A set of elementary LTIG trees that represent a bigram grammar. The arrows indicate adjunction sites. tchas¢ s .~.~t. adj. tth e ~,f.~t. adj. ~rr~ouse that describe the distribution of the likelihood of any left auxiliary tree adjoining into node r/. (We need one extra parameter for the case of no left adjunction.) A similar set of parameters is constructed for the"
C98-1088,1993.iwpt-1.20,0,0.720679,"y restricting auxiliary tree structures to be in one of two forms: the left auxiliary tree, whose non-empty frontier nodes are all to the left of the foot node; or the right auxiliary tree, whose non-empty frontier nodes are all to the right of the foot node. Auxiliary trees of different types cannot adjoin into each other if the adjunction would result in a wrapping auxiliary tree. The resulting system is strongly equivalent to CFGs, yet is fully lexicalized and still O(n 3) parsable, as shown by Schabes and Waters (1994). Furthermore, LTIGs can be parameterized to form probabilistic models (Schabes and Waters, 1993). Informally speaking, a parameter is associated with each possible adjunction or substitution operation between a tree and a node. For instance, suppose there are V left auxiliary trees that might adjoin into node ~. Then there are V + 1 parameters associated with node 1The best theoretical upper bound on time complexity for the recognition of Tree Adjoining Languages is O(M(n2)), where M(k) is the time needed to multiply two k x k boolean matrices.(Rajasekaran and Yooseph, 1995) 558 E|ementttT Tt.¢¢Sets: ttnlt tw°rd l tw°rd 2 Example sentence: The cat tw°rd n chases the mouse Corresponding d"
C98-1088,C88-2121,0,0.028829,"xicalized context-free formalism. This method is not practical because altering the structures of the grammar damages the linguistic information stored in the original grammar (Schabes and Waters, 1994). Second, one might propagate lexical information upward through the productions. Examples of formalisms using this approach include the work of Magerman (1995), Charniak (1997), Collins (1997), and Goodman (1997). A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. The Lexicalized Tree-Adjoining Grammar (LTAG) formalism (Schabes et al., 1988), (Schabes, 1990) , although not context-free, is the most well-known instance in this category. PLTIGs belong to this third category and generate only context-free languages. LTAGs (and LTIGs) are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations. We distinguish two types of trees in the set of elementary trees: the initial trees and the auxiliary trees. Unlike full parse trees but reminiscent of the productions of a context-free grammar, both types of trees may have nonterminal leaf nodes. Auxiliary trees have, in addition, a distinguished nontermina"
C98-1088,P97-1003,0,0.0452642,"7 mar. There are three ways in which one might do so. First, one can modify the tree structures so that all context-free productions contain lexical items. Greibach normal form provides a well-known example of such a lexicalized context-free formalism. This method is not practical because altering the structures of the grammar damages the linguistic information stored in the original grammar (Schabes and Waters, 1994). Second, one might propagate lexical information upward through the productions. Examples of formalisms using this approach include the work of Magerman (1995), Charniak (1997), Collins (1997), and Goodman (1997). A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. The Lexicalized Tree-Adjoining Grammar (LTAG) formalism (Schabes et al., 1988), (Schabes, 1990) , although not context-free, is the most well-known instance in this category. PLTIGs belong to this third category and generate only context-free languages. LTAGs (and LTIGs) are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations. We distinguish two types of trees in the set of elementary trees: the initial trees"
C98-1088,1997.iwpt-1.13,0,0.0286823,"ee ways in which one might do so. First, one can modify the tree structures so that all context-free productions contain lexical items. Greibach normal form provides a well-known example of such a lexicalized context-free formalism. This method is not practical because altering the structures of the grammar damages the linguistic information stored in the original grammar (Schabes and Waters, 1994). Second, one might propagate lexical information upward through the productions. Examples of formalisms using this approach include the work of Magerman (1995), Charniak (1997), Collins (1997), and Goodman (1997). A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. The Lexicalized Tree-Adjoining Grammar (LTAG) formalism (Schabes et al., 1988), (Schabes, 1990) , although not context-free, is the most well-known instance in this category. PLTIGs belong to this third category and generate only context-free languages. LTAGs (and LTIGs) are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations. We distinguish two types of trees in the set of elementary trees: the initial trees and the auxiliary t"
C98-1088,J95-4002,0,\N,Missing
C98-1088,P95-1037,0,\N,Missing
D16-1182,P16-1231,0,0.0537012,"Missing"
D16-1182,H91-1060,0,0.312524,"ence against it. Even if the “gold-standard” is not perfectly correct in absolute terms, it represents the norm from which parse trees of problematic sentences diverge: if a parser were robust against ungrammatical sentences, its output for these sentences should be similar to its output for the well-formed ones. Determining the evaluation metric for comparing these trees, however, presents another challenge. Since the words of the ungrammatical sentence and its grammatical counterpart do not necessarily match (an example is given in Figure 1), we cannot use standard metrics such as Parseval (Black et al., 1991). We also cannot use adapted metrics for comparing parse trees of unmatched sentences (e.g., Sparseval (Roark et al., 2006)), because these metrics consider all the words regardless of the mismatches (extra or missing words) between two sentences. This is a problem for comparing ungrammatical sentences to grammatical ones because a parser is unfairly penalized when it assigns relations to extra words and when it does not assign relations to missing words. Since a parser cannot modify the sentence, we do not want to penalize these extraneous or missing relations; on the other hand, we do want t"
D16-1182,W15-1616,0,0.0189262,"mpirical Methods in Natural Language Processing, pages 1765–1774, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics puts against manually annotated gold standards. Although there are a few small semi-manually constructed treebanks on learner texts (Geertzen et al., 2013; Ott and Ziai, 2010) or tweets (Daiber and van der Goot, 2016), their sizes make them unsuitable for the evaluation of parser robustness. Moreover, some researchers have raised valid questions about the merit of creating a treebank for ungrammatical sentences or adapting the annotation schema (Cahill, 2015; Ragheb and Dickinson, 2012). A “gold-standard free” alternative is to compare the parser output for each problematic sentence with the parse tree of the corresponding correct sentence. Foster (2004) used this approach over a small set of ungrammatical sentences and showed that parser’s accuracy is different for different types of errors. A limitation of this approach is that the comparison works best when the differences between the problematic sentence and the correct sentence are small. This is not the case for some ungrammatical sentences (especially from MT systems). Another closely-rela"
D16-1182,D14-1082,0,0.0545094,"Missing"
D16-1182,P15-1038,0,0.0384915,"Missing"
D16-1182,L16-1102,0,0.151125,"Missing"
D16-1182,de-marneffe-etal-2006-generating,0,0.278725,"Missing"
D16-1182,foster-2004-parsing,0,0.0110321,"lthough there are a few small semi-manually constructed treebanks on learner texts (Geertzen et al., 2013; Ott and Ziai, 2010) or tweets (Daiber and van der Goot, 2016), their sizes make them unsuitable for the evaluation of parser robustness. Moreover, some researchers have raised valid questions about the merit of creating a treebank for ungrammatical sentences or adapting the annotation schema (Cahill, 2015; Ragheb and Dickinson, 2012). A “gold-standard free” alternative is to compare the parser output for each problematic sentence with the parse tree of the corresponding correct sentence. Foster (2004) used this approach over a small set of ungrammatical sentences and showed that parser’s accuracy is different for different types of errors. A limitation of this approach is that the comparison works best when the differences between the problematic sentence and the correct sentence are small. This is not the case for some ungrammatical sentences (especially from MT systems). Another closely-related approach is to semi-automatically create treebanks from artificial errors. For example, Foster generated artificial errors to the sentences from the Penn Treebank for evaluating the effect of erro"
D16-1182,N10-1060,0,0.0271672,"ins of ungrammatical sentences: learner English and machine translation outputs. We have developed an evaluation metric and conducted a suite of experiments. Our analyses may help practitioners to choose an appropriate parser for their tasks, and help developers to improve parser robustness against ungrammatical sentences. 1 • a metric and methodology for evaluating ungrammatical sentences without referring to a gold standard corpus; Introduction Previous works have shown that, in general, parser performances degrade when applied to out-ofdomain sentences (Gildea, 2001; McClosky et al., 2010; Foster, 2010; Petrov et al., 2010; Foster et al., 2011). If a parser performs reasonably well for a wide range of out-of-domain sentences, it is said to be robust (Bigert et al., 2005; Kakkonen, 2007; Foster, 2007). Sentences that are ungrammatical, awkward, or too casual/colloquial can all be seen as special kinds of out-of-domain sentences. These types of sentences are commonplace for NLP applications, from product reviews and social media analysis to intelligent language tutors and multilingual processing. Since parsing is an essential component for many applications, it is natural to ask: Are some par"
D16-1182,W01-0521,0,0.050154,"he-art dependency parsers on two domains of ungrammatical sentences: learner English and machine translation outputs. We have developed an evaluation metric and conducted a suite of experiments. Our analyses may help practitioners to choose an appropriate parser for their tasks, and help developers to improve parser robustness against ungrammatical sentences. 1 • a metric and methodology for evaluating ungrammatical sentences without referring to a gold standard corpus; Introduction Previous works have shown that, in general, parser performances degrade when applied to out-ofdomain sentences (Gildea, 2001; McClosky et al., 2010; Foster, 2010; Petrov et al., 2010; Foster et al., 2011). If a parser performs reasonably well for a wide range of out-of-domain sentences, it is said to be robust (Bigert et al., 2005; Kakkonen, 2007; Foster, 2007). Sentences that are ungrammatical, awkward, or too casual/colloquial can all be seen as special kinds of out-of-domain sentences. These types of sentences are commonplace for NLP applications, from product reviews and social media analysis to intelligent language tutors and multilingual processing. Since parsing is an essential component for many application"
D16-1182,P11-2008,0,0.0808866,"Missing"
D16-1182,D14-1108,0,0.0266982,"focused on accuracy and speed (Choi et al., 2015; Kummerfeld et al., 2012; McDonald and Nivre, 2011; Kong and Smith, 2014) have not taken ungrammatical sentences into consideration. In this paper, we report a set of empirical analyses of eight leading dependency parsers on two domains of ungrammatical text: English-as-a-Second Language (ESL) learner text and machine translation (MT) outputs. We also vary the types of training sources; the parsers are trained with the Penn Treebank (to be comparable with other studies) and Tweebank, a treebank on tweets (to be a bit more like the test domain) (Kong et al., 2014). The main contributions of the paper are: For many NLP applications that require a parser, the sentences of interest may not be well-formed. If the parser can overlook problems such as grammar mistakes and produce a parse tree that closely resembles the correct analysis for the intended sentence, we say that the parser is robust. This paper compares the performances of eight state-of-the-art dependency parsers on two domains of ungrammatical sentences: learner English and machine translation outputs. We have developed an evaluation metric and conducted a suite of experiments. Our analyses may"
D16-1182,D12-1096,0,0.165831,"Missing"
D16-1182,P13-2109,0,0.0439635,"Missing"
D16-1182,N10-1004,0,0.0189867,"ncy parsers on two domains of ungrammatical sentences: learner English and machine translation outputs. We have developed an evaluation metric and conducted a suite of experiments. Our analyses may help practitioners to choose an appropriate parser for their tasks, and help developers to improve parser robustness against ungrammatical sentences. 1 • a metric and methodology for evaluating ungrammatical sentences without referring to a gold standard corpus; Introduction Previous works have shown that, in general, parser performances degrade when applied to out-ofdomain sentences (Gildea, 2001; McClosky et al., 2010; Foster, 2010; Petrov et al., 2010; Foster et al., 2011). If a parser performs reasonably well for a wide range of out-of-domain sentences, it is said to be robust (Bigert et al., 2005; Kakkonen, 2007; Foster, 2007). Sentences that are ungrammatical, awkward, or too casual/colloquial can all be seen as special kinds of out-of-domain sentences. These types of sentences are commonplace for NLP applications, from product reviews and social media analysis to intelligent language tutors and multilingual processing. Since parsing is an essential component for many applications, it is natural to ask"
D16-1182,J11-1007,0,0.0615825,"Missing"
D16-1182,E06-1011,0,0.0403092,"eatures. Yara Parser (Rasooli and Tetreault, 2015)9 A transition-based parser that uses beam search training and dynamic oracle. 4.2 Experimental Setup Our experiments are conducted over a wide range of dependency parsers that are trained on two different treebanks: Penn Treebank (PTB) and Tweebank. We evaluate the robustness of parsers over two datasets that contain ungrammatical sentences: writings of English-as-a-Second language learners and machine translation outputs. We choose datasets for which the corresponding correct sentences are available (or easily reconstructed). 4.1 MST Parser (McDonald and Pereira, 2006)4 A firstorder graph-based parser that searches for maximum spanning trees. Parsers Our evaluation is over eight state-of-the-art dependency parsers representing a wide range of approaches. We use the publicly available versions of each parser with the standard parameter settings. 1767 Data We train all the parsers using two treebanks and test their robustness over two ungrammatical datasets. 4.2.1 Parser Training Data Penn Treebank (PTB) We follow the standard splits of Penn Treebank, using section 2-21 for training, section 22 for development, and section 23 for 2 www.maltparser.org code.goo"
D16-1182,D10-1069,0,0.022705,"atical sentences: learner English and machine translation outputs. We have developed an evaluation metric and conducted a suite of experiments. Our analyses may help practitioners to choose an appropriate parser for their tasks, and help developers to improve parser robustness against ungrammatical sentences. 1 • a metric and methodology for evaluating ungrammatical sentences without referring to a gold standard corpus; Introduction Previous works have shown that, in general, parser performances degrade when applied to out-ofdomain sentences (Gildea, 2001; McClosky et al., 2010; Foster, 2010; Petrov et al., 2010; Foster et al., 2011). If a parser performs reasonably well for a wide range of out-of-domain sentences, it is said to be robust (Bigert et al., 2005; Kakkonen, 2007; Foster, 2007). Sentences that are ungrammatical, awkward, or too casual/colloquial can all be seen as special kinds of out-of-domain sentences. These types of sentences are commonplace for NLP applications, from product reviews and social media analysis to intelligent language tutors and multilingual processing. Since parsing is an essential component for many applications, it is natural to ask: Are some parsers • a quantitative"
D16-1182,potet-etal-2012-collection,0,0.0684802,"Missing"
D16-1182,J08-2005,0,0.0327696,"Table 4 shows results. As expected, closed-class errors are generally more difficult for parsers. But when parsers are trained on PTB and tested on MT, there are some exceptions: Turbo, Mate, MST and Yara parsers tend to be more robust on closed-class errors. This result corroborates the importance of building grammar error correction systems to handle closed-class errors such as preposition errors. 5.4.3 Impact of error semantic role An error can be either in a verb role, an argument role, or no semantic role. We extract semantic role of the error by running Illinoise semantic role labeler (Punyakanok et al., 2008) on corrected version of the Train on PTB §1-21 ESL MT Open class Closed class Open class Closed class Parser min Malt Mate MST SNN SyntaxNet Turbo Tweebo Yara max Train on Tweebanktrain ESL MT Open class Closed class Open class Closed class 95.1 (SNN) 94.5 (Yara) 89.6 (SyntaxNet) 91.5 (SNN) 96.8 (Malt) 96.1 (SNN) 97.6 (Malt) 97.0 (Malt) Table 4: Parser robustness on sentences with one error, where the error either occurs on an open-class (lexical) word or a closed-class (functional) word. Parser Verb min Malt Mate MST SNN SyntaxNet Turbo Tweebo Yara max ESL Argument Train on PTB §1-21 No role"
D16-1182,C12-2094,0,0.0171105,"ds in Natural Language Processing, pages 1765–1774, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics puts against manually annotated gold standards. Although there are a few small semi-manually constructed treebanks on learner texts (Geertzen et al., 2013; Ott and Ziai, 2010) or tweets (Daiber and van der Goot, 2016), their sizes make them unsuitable for the evaluation of parser robustness. Moreover, some researchers have raised valid questions about the merit of creating a treebank for ungrammatical sentences or adapting the annotation schema (Cahill, 2015; Ragheb and Dickinson, 2012). A “gold-standard free” alternative is to compare the parser output for each problematic sentence with the parse tree of the corresponding correct sentence. Foster (2004) used this approach over a small set of ungrammatical sentences and showed that parser’s accuracy is different for different types of errors. A limitation of this approach is that the comparison works best when the differences between the problematic sentence and the correct sentence are small. This is not the case for some ungrammatical sentences (especially from MT systems). Another closely-related approach is to semi-autom"
D16-1182,roark-etal-2006-sparseval,0,0.0249875,"parse trees of problematic sentences diverge: if a parser were robust against ungrammatical sentences, its output for these sentences should be similar to its output for the well-formed ones. Determining the evaluation metric for comparing these trees, however, presents another challenge. Since the words of the ungrammatical sentence and its grammatical counterpart do not necessarily match (an example is given in Figure 1), we cannot use standard metrics such as Parseval (Black et al., 1991). We also cannot use adapted metrics for comparing parse trees of unmatched sentences (e.g., Sparseval (Roark et al., 2006)), because these metrics consider all the words regardless of the mismatches (extra or missing words) between two sentences. This is a problem for comparing ungrammatical sentences to grammatical ones because a parser is unfairly penalized when it assigns relations to extra words and when it does not assign relations to missing words. Since a parser cannot modify the sentence, we do not want to penalize these extraneous or missing relations; on the other hand, we do want to identify cascading effects on the parse tree due to a grammar error. For the purpose of evaluating parser robustness agai"
D16-1182,N03-1033,0,0.0133995,"his assumption will generate more arcs from the root, but since we use the same evaluation setting for all the parsers, the results are comparable. We evaluate the accuracy of the trained parser on Tweebank with the unlabeled attachment F1 score (same procedure as Kong et al. (2014)). 4.2.2 Robustness Test Data To test the robustness of parsers, we choose two datasets of ungrammatical sentences for which their corresponding correct sentences are available. For a fair comparison, we automatically assign POS tags to the test data. When parsers are trained on PTB, we use the Stanford POS tagger (Toutanova et al., 2003). When parsers are trained on Tweebank, we coarsen POS tags to be compatible with the Twitter POS tags using the mappings specified by Gimpel et al. (2011). English-as-a-Second Language corpus (ESL) For the ungrammatical sentences, we use the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011) that contains the writings of English as a second language learners and their corresponding error corrections. Given the errors and their corrections, we can easily reconstruct the corrected version 10 github.com/ikekonglp/TweeboParser/tree/ master/Tweebank 1768 of each ungrammatical"
D16-1182,E14-1072,1,0.898207,"Missing"
D16-1182,P11-1019,0,0.024564,"ta To test the robustness of parsers, we choose two datasets of ungrammatical sentences for which their corresponding correct sentences are available. For a fair comparison, we automatically assign POS tags to the test data. When parsers are trained on PTB, we use the Stanford POS tagger (Toutanova et al., 2003). When parsers are trained on Tweebank, we coarsen POS tags to be compatible with the Twitter POS tags using the mappings specified by Gimpel et al. (2011). English-as-a-Second Language corpus (ESL) For the ungrammatical sentences, we use the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011) that contains the writings of English as a second language learners and their corresponding error corrections. Given the errors and their corrections, we can easily reconstruct the corrected version 10 github.com/ikekonglp/TweeboParser/tree/ master/Tweebank 1768 of each ungrammatical ESL sentence. From this corpus, we randomly select 10,000 sentences with at least one error; there are 4954 with one error; 2709 with two errors; 1290 with three; 577 with four; 259 with five; 111 with six; and 100 with 7+ errors. Machine Translation corpus (MT) Machine translation outputs are another domain of u"
D18-1199,E06-1042,0,0.0479243,"e, supervised classifiers are effective. Rajani et al. (2014) extracted all non-stop-words in the context and used them as ”bag of words” features to train a L2 regularized Logistic Regression (L2LR) classifier (Fan et al., 2008). As local context of an idiom holds clues for discriminating between its literal and figurative usages, Liu and Hwa (2017) find that context representation also plays a significant role in idiom usage recognition. They took an adaptive approach, applying supervised ensemble learning over three classifiers based on different context representations (Peng et al., 2014; Birke and Sarkar, 2006; Rajani et al., 2014). 3 Our Approach Given a target idiomatic expression and a collection of instances in which the idiom occurs, our proposed system (Figure 1) determines whether the idiom in each instance is meant figuratively or literally. We first build a Literal Usage Representation for each idiom by leveraging the distributional semantics of its constituents (Sec 3.1). Given an instance of idiom, we can determine its usage by the semantic similarity between the context of the instance and the Literal Usage Representation. We define a Literal Usage Metric to transform the semantic simil"
D18-1199,S13-2018,0,0.0186129,"1 https://twitter.com/BTeboe/status/ 958792419302100993 2 https://twitter.com/DukeRaccoon/ status/477530732173471744 60 idioms have a clear literal meaning as well as a figurative one (Fazly et al., 2009). Being able to distinguish the intended usage of an idiom in context has been shown to benefit many natural language processing (NLP) applications, e.g., machine translation and sentiment analysis (Salton et al., 2014; Williams et al., 2015). While supervised models for idiom usage recognition have had some successes, they require appropriately annotated training examples (Peng et al., 2014; Byrne et al., 2013; Liu and Hwa, 2017). A more challenging problem is to recognize idiom usages without a dictionary or some annotated examples (Korkontzelos et al., 2013). Some previous unsupervised models tried to exploit linguistic differences in usages. For example, Fazly et al.(2009) observed that an idiom appearing in its canonical form is usually used figuratively; Sporleder and Li(2009) relied on the break in lexical coherence between the idioms and the context to signal a figurative usage. These heuristics, however, are not always applicable because the distinctions they depend upon may not be present"
D18-1199,D14-1082,0,0.0169688,"s, we consider two representative probabilistic latent variable models: Latent Dirichlet Allocation (LDA) (Blei et al., 2003)4 and unsupervised Naive Bayes (NB). For both models, the latent variable is the idiom usage (figurative vs. literal); the observables 4 Although originally conceived for modeling document content, LDA can be applied to any kind of discrete input are linguistic features that can be extracted from the instances, described below: Subordinate Clause We encode a binary feature indicating whether the target expression is followed by a subordinate clause (the Stanford Parser (Chen and Manning, 2014) is used). This feature is useful for some idioms such as in the dark. It usually suggests a figurative usage as in You’ve kept us totally in the dark about what happened that night. Selectional Preference Violation of selectional preference is normally a signal of figurative usage (e.g., having an abstract entity as the subject of play with fire). We encode this feature if the head word of the idiom is a verb and focus on the subject of the verb. We apply Stanford Name Entity tagger (Finkel et al., 2005) with 3 classes (”Location”, ”Person”, ”Organization”) on the sentence containing the idio"
D18-1199,W06-1203,0,0.166055,"applicable because the distinctions they depend upon may not be present or obvious. To improve generalization across different idioms and usage contexts, we need a more reliable heuristic, and appropriately incorporate it into an unsupervised learning framework. We propose a heuristic that differentiates usages based on distributional semantics (Harris, 1954; Turney and Pantel, 2010). Our key insight is that when an idiom is used literally, its relationship with its context is more predictable than when it is used figuratively. This is because the literal meaning of an idiom is compositional (Katz and Giesbrecht, 2006), and the constituent words that make up the idiom are also meant literally. For example, in instance (2), spill is meant literally and can take on objects other than beans; moreover, one of the context words, mess, can often be seen to co1723 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1723–1731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics occur with spill in other text, even without beans. Our strategy is to represent an idiom’s literal usage in terms of the word embeddings of the idiom’s"
D18-1199,D09-1033,0,0.25379,"t decision boundary between literal and figurative usages. For example, Fazly et al. (2009) proposed a method that relies on the concept of canonical form. Based on the observation that while literal usages are less syntactically restricted, figurative usages tend to occur in a small number of canonical form(s). As shown in the examples above, however, this rule of thumb does not always hold. Sporleder and Li (2009) proposed a method by building a cohesion graph to include all content words in the context; if removing the idiom improves cohesion, they assume the instance is figurative. Later, Li and Sporleder (2009) used their cohesion graph method to label a subset of the test data with high confidence. This subset is then passed on as training data to the supervised classifier, which then labels the remainder of the dataset. When manually annotated examples are available, supervised classifiers are effective. Rajani et al. (2014) extracted all non-stop-words in the context and used them as ”bag of words” features to train a L2 regularized Logistic Regression (L2LR) classifier (Fan et al., 2008). As local context of an idiom holds clues for discriminating between its literal and figurative usages, Liu a"
D18-1199,D14-1216,0,0.0704912,"out of a sample of 1 https://twitter.com/BTeboe/status/ 958792419302100993 2 https://twitter.com/DukeRaccoon/ status/477530732173471744 60 idioms have a clear literal meaning as well as a figurative one (Fazly et al., 2009). Being able to distinguish the intended usage of an idiom in context has been shown to benefit many natural language processing (NLP) applications, e.g., machine translation and sentiment analysis (Salton et al., 2014; Williams et al., 2015). While supervised models for idiom usage recognition have had some successes, they require appropriately annotated training examples (Peng et al., 2014; Byrne et al., 2013; Liu and Hwa, 2017). A more challenging problem is to recognize idiom usages without a dictionary or some annotated examples (Korkontzelos et al., 2013). Some previous unsupervised models tried to exploit linguistic differences in usages. For example, Fazly et al.(2009) observed that an idiom appearing in its canonical form is usually used figuratively; Sporleder and Li(2009) relied on the break in lexical coherence between the idioms and the context to signal a figurative usage. These heuristics, however, are not always applicable because the distinctions they depend upon"
D18-1199,J09-1005,0,0.382342,"and literally in the second: (1) [fig.] The beans have been spilled. From what I’ve read on Twitter I could probably fill out the forms and submit it to the FISA court. I don’t know what the big secret is.1 (2) [lit.] Spill the beans, flip the fruit, bust open a box of hot pockets. Make a general mess of the kitchen.2 This type of ambiguity is commonplace – prior work suggests that about half out of a sample of 1 https://twitter.com/BTeboe/status/ 958792419302100993 2 https://twitter.com/DukeRaccoon/ status/477530732173471744 60 idioms have a clear literal meaning as well as a figurative one (Fazly et al., 2009). Being able to distinguish the intended usage of an idiom in context has been shown to benefit many natural language processing (NLP) applications, e.g., machine translation and sentiment analysis (Salton et al., 2014; Williams et al., 2015). While supervised models for idiom usage recognition have had some successes, they require appropriately annotated training examples (Peng et al., 2014; Byrne et al., 2013; Liu and Hwa, 2017). A more challenging problem is to recognize idiom usages without a dictionary or some annotated examples (Korkontzelos et al., 2013). Some previous unsupervised mode"
D18-1199,P05-1045,0,0.0365466,"hether the target expression is followed by a subordinate clause (the Stanford Parser (Chen and Manning, 2014) is used). This feature is useful for some idioms such as in the dark. It usually suggests a figurative usage as in You’ve kept us totally in the dark about what happened that night. Selectional Preference Violation of selectional preference is normally a signal of figurative usage (e.g., having an abstract entity as the subject of play with fire). We encode this feature if the head word of the idiom is a verb and focus on the subject of the verb. We apply Stanford Name Entity tagger (Finkel et al., 2005) with 3 classes (”Location”, ”Person”, ”Organization”) on the sentence containing the idiom. If the subject is labeled as an Entity, its class will be encoded in the feature vector. Pronouns such as ”I” and ”he” also indicate the subject is a ”Person”. However, they are normally not tagged by Stanford Name Entity tagger. To overcome this issue, we add Part-of-Speech of the subject into the feature vector. Abstractness Abstract words refer to things which are hard to perceive directly with our senses. Abstractness has been shown to be useful in the detection of metaphor, another type of figurat"
D18-1199,W14-1007,0,0.0228717,"Spill the beans, flip the fruit, bust open a box of hot pockets. Make a general mess of the kitchen.2 This type of ambiguity is commonplace – prior work suggests that about half out of a sample of 1 https://twitter.com/BTeboe/status/ 958792419302100993 2 https://twitter.com/DukeRaccoon/ status/477530732173471744 60 idioms have a clear literal meaning as well as a figurative one (Fazly et al., 2009). Being able to distinguish the intended usage of an idiom in context has been shown to benefit many natural language processing (NLP) applications, e.g., machine translation and sentiment analysis (Salton et al., 2014; Williams et al., 2015). While supervised models for idiom usage recognition have had some successes, they require appropriately annotated training examples (Peng et al., 2014; Byrne et al., 2013; Liu and Hwa, 2017). A more challenging problem is to recognize idiom usages without a dictionary or some annotated examples (Korkontzelos et al., 2013). Some previous unsupervised models tried to exploit linguistic differences in usages. For example, Fazly et al.(2009) observed that an idiom appearing in its canonical form is usually used figuratively; Sporleder and Li(2009) relied on the break in l"
D18-1199,E09-1086,0,0.38465,"ny can also be meant literally. A number of models have been proposed in the literature to recognize an idiom’s usages under different context. Many rely on specific linguistic property to draw a clear-cut decision boundary between literal and figurative usages. For example, Fazly et al. (2009) proposed a method that relies on the concept of canonical form. Based on the observation that while literal usages are less syntactically restricted, figurative usages tend to occur in a small number of canonical form(s). As shown in the examples above, however, this rule of thumb does not always hold. Sporleder and Li (2009) proposed a method by building a cohesion graph to include all content words in the context; if removing the idiom improves cohesion, they assume the instance is figurative. Later, Li and Sporleder (2009) used their cohesion graph method to label a subset of the test data with high confidence. This subset is then passed on as training data to the supervised classifier, which then labels the remainder of the dataset. When manually annotated examples are available, supervised classifiers are effective. Rajani et al. (2014) extracted all non-stop-words in the context and used them as ”bag of word"
D18-1199,D11-1063,0,0.030263,"s (”Location”, ”Person”, ”Organization”) on the sentence containing the idiom. If the subject is labeled as an Entity, its class will be encoded in the feature vector. Pronouns such as ”I” and ”he” also indicate the subject is a ”Person”. However, they are normally not tagged by Stanford Name Entity tagger. To overcome this issue, we add Part-of-Speech of the subject into the feature vector. Abstractness Abstract words refer to things which are hard to perceive directly with our senses. Abstractness has been shown to be useful in the detection of metaphor, another type of figurative language (Turney et al., 2011). A figurative usage of an idiomatic phrase may have relatively more abstract contextual words. For example, in the sentence She has lived life in the fast lane, the word life is considered as an abstract word. This is a useful indicator that in the fast lane is used figuratively. We use the MRC Psycholinguistic Database Machine Usable Dictionary (Coltheart, 1981) which contains a list of 4295 words with their abstractness measure between 100 and 700. We calculate the average abstractness score for all the contextual words (with stop words being removed) in the sentence containing the idiom. T"
dorr-etal-2002-duster,han-etal-2000-handling,0,\N,Missing
dorr-etal-2002-duster,melamed-1998-empirical,0,\N,Missing
dorr-etal-2002-duster,A00-1009,0,\N,Missing
dorr-etal-2002-duster,J93-2003,0,\N,Missing
dorr-etal-2002-duster,W01-1406,0,\N,Missing
dorr-etal-2002-duster,2001.mtsummit-ebmt.4,0,\N,Missing
dorr-etal-2002-duster,W01-1403,0,\N,Missing
dorr-etal-2002-duster,C00-1078,0,\N,Missing
dorr-etal-2002-duster,C00-2131,0,\N,Missing
dorr-etal-2002-duster,W00-1306,1,\N,Missing
dorr-etal-2002-duster,N01-1026,0,\N,Missing
dorr-etal-2002-duster,J90-2002,0,\N,Missing
dorr-etal-2002-duster,P97-1062,0,\N,Missing
dorr-etal-2002-duster,P01-1067,0,\N,Missing
dorr-etal-2002-duster,P02-1050,1,\N,Missing
dorr-etal-2002-duster,J97-3002,0,\N,Missing
dorr-etal-2002-duster,P00-1056,0,\N,Missing
E03-1008,H91-1060,0,0.0320313,"Missing"
E03-1008,W99-0613,0,0.0609689,"Missing"
E03-1008,W01-0521,0,0.161063,"Missing"
E03-1008,J93-2004,0,0.0278308,"Missing"
E03-1008,W01-0501,0,0.185139,"Missing"
E03-1008,N01-1023,1,0.616825,"Missing"
E03-1008,P95-1026,0,0.323358,"Missing"
E03-1008,J03-4003,0,\N,Missing
E03-1008,P02-1046,0,\N,Missing
E09-1008,H01-1033,0,0.0186586,"ing women of all ages that ’the flavor of life’.” SIN-West, a spokeswoman for the U.S. Bandai Group declared: “We want to bring to women of all ages that ’flavor of life’.” West, a spokeswoman for the U.S. Toy Manufacturing Group, and soon to be Vice President-said: “We want to bring women of all ages that ’flavor of life’.” “We wanted to let women of all ages taste the ’flavor of life’,” said Bandai’s spokeswoman Kasumi Nakanishi. text they attempted to understand, we may alter the design of our translation model. Our objective is also related to that of cross-language information retrieval (Resnik et al., 2001). This work can be seen as providing the next step in helping users to gain some understanding of the information in the documents once they are retrieved. By facilitating better collaborations between MT and target-language readers, we can naturally increase human annotated data for exploring alternative MT models. This form of symbiosis is akin to the paradigm proposed by von Ahn and Dabbish (2004). They designed interactive games in which the player generated data could be used to improve image tagging and other classification tasks (von Ahn, 2006). While our interface does not have the ent"
E09-1008,P05-3025,0,0.0279979,"overall of 0.83). This suggests The Chinese Room affords a potential for humanhuman collaboration as well. The experiment also made clear some limitations of the current resources. One is domain dependency. Because NLP technologies are typically trained on news corpora, their bias toward the news domain may mislead our users. For ex7 Related Work While there have been many successful computeraided translation systems both for research and as commercial products (Bowker, 2002; Langlais et al., 2000), collaborative translation has not been as widely explored. Previous efforts such as DerivTool (DeNeefe et al., 2005) and Linear B (Callison-Burch, 2005) placed stronger emphasis on improving MT. They elicited more in-depth interactions between the users and the MT system’s phrase tables. These approaches may be more appropriate for users who are MT researchers themselves. In contrast, our approach focuses on providing intuitive visualization of a variety of information sources for users who may not be MTsavvy. By tracking the types of information they consulted, the portions of translations they selected to modify, and the portions of the source 66 Table 7: Some examples of translations corrected by the par"
E09-1008,2004.tmi-1.8,0,0.017578,"serve as a useful diagnostic tool to help MT researchers verify ideas about what types of models and data are useful in translation. It may also provide a means of data collection for MT training. To be sure, there are important challenges to be addressed, such as participation incentive and quality assurance, but similar types of collaborative efforts have been shown fruitful in other domains (Cosley et al., 2007). Finally, the statistics of user actions may be useful for translation evaluation. They may be informative features for developing automatic metrics for sentence-level evaluations (Kulesza and Shieber, 2004). Figure 3 compares the average access counts (per sentence) of different resources (aggregated over all participants and documents). The option of inspect retrieved examples in detail (i.e., bring them up on the sentence workspace) was rarely used. The inspiration for this feature was from work on translation memory (Macklovitch et al., 2000); however, it was not as informative for our participants because they experienced a greater degree of uncertainty than professional translators. 6 2nd Lang. 0.43 0.51 Discussion The results suggest that collaborative translation is a promising approach."
E09-1008,W00-0507,0,0.38365,"st, an oracle of users who did not use the system is 0.54 (cf. the MT’s overall of 0.35 and the bilingual translator’s overall of 0.83). This suggests The Chinese Room affords a potential for humanhuman collaboration as well. The experiment also made clear some limitations of the current resources. One is domain dependency. Because NLP technologies are typically trained on news corpora, their bias toward the news domain may mislead our users. For ex7 Related Work While there have been many successful computeraided translation systems both for research and as commercial products (Bowker, 2002; Langlais et al., 2000), collaborative translation has not been as widely explored. Previous efforts such as DerivTool (DeNeefe et al., 2005) and Linear B (Callison-Burch, 2005) placed stronger emphasis on improving MT. They elicited more in-depth interactions between the users and the MT system’s phrase tables. These approaches may be more appropriate for users who are MT researchers themselves. In contrast, our approach focuses on providing intuitive visualization of a variety of information sources for users who may not be MTsavvy. By tracking the types of information they consulted, the portions of translations"
E09-1008,macklovitch-etal-2000-transsearch,0,0.0243373,"efforts have been shown fruitful in other domains (Cosley et al., 2007). Finally, the statistics of user actions may be useful for translation evaluation. They may be informative features for developing automatic metrics for sentence-level evaluations (Kulesza and Shieber, 2004). Figure 3 compares the average access counts (per sentence) of different resources (aggregated over all participants and documents). The option of inspect retrieved examples in detail (i.e., bring them up on the sentence workspace) was rarely used. The inspiration for this feature was from work on translation memory (Macklovitch et al., 2000); however, it was not as informative for our participants because they experienced a greater degree of uncertainty than professional translators. 6 2nd Lang. 0.43 0.51 Discussion The results suggest that collaborative translation is a promising approach. Participant experiences were generally positive. Because they felt like they understood the translations better, they did not mind putting in the time to collaborate with the system. Table 7 shows some of the participants’ outputs. Although there are some translation errors that cannot be overcome with our current system (e.g., transliterated"
E09-1008,2005.eamt-1.26,0,0.0600718,"Missing"
E09-1008,C04-1046,0,\N,Missing
E14-1072,A00-1043,0,0.0488833,"se findings are often unpredictable and uncertain. Ex4 : . . . the cost incurred is not only just large sum of money . . . … and the cost incurred is not only just large sum of money … Figure 1: Among the three circled words, “just” is more redundant because deleting it hurts nether fluency nor meaning. These words/phrases are considered redundant because they are unnecessary (e.g. Ex1 , Ex2 ) or repetitive (e.g. Ex3 , Ex4 ). However, in NUCLE’s annotation scheme, some words that were marked redundant are really words that carry undesirable meanings. For example: Sentence compression systems (Jing, 2000; Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2007) aim at shortening a sentence while retaining the most important information and keeping it grammatically correct. This goal distinguishes these systems from ours in two major aspects. First, sentence compression systems assume that the original sentence is well-written; therefore retaining words specific to the sentence (e.g. “uncertain” in Ex3 ) can be a good strategy (Clarke and Lapata, 2007). In the ESL context, however, even specific words could still be redundant. For example, although “uncertain” is specific to Ex3 , it i"
E14-1072,P06-1032,0,0.0204893,"redundant. For example, although “uncertain” is specific to Ex3 , it is redundant, because its meaning is already implied by “unpredictable”. Second, sentence compression systems try to shorten a sentence as much as possible, but an ESL redundancy detector should leave as much of the input sentences unchanged, if possible. One challenge involved in redundancy detection is that it often involves open class words (Ex3 ), as well as multi-word expressions (Ex1 , Ex4 ). Current GEC systems dealing with such error types are mostly MT based. MT systems tend to either require large training corpora (Brockett et al., 2006; Liu et al., 2010), or provide whole sentence rewritings (Madnani et al., 2012). Hermet and D´esilets (2009) attempted to extract single preposition corrections from whole sentence rewritings. Our work incorporates alignments information to handle complex changes on both word and phrase levels. In our approximation, we consider MT output as an approximation of word/phrase meanings. Using words in other languages to represent meanings has been explored in Carpuat and Wu (2007), where the focus is the aligned words’ identities. Our work instead focuses more on how many words each word is aligne"
E14-1072,J93-2003,0,0.04189,"Missing"
E14-1072,D07-1007,0,0.0276104,"C systems dealing with such error types are mostly MT based. MT systems tend to either require large training corpora (Brockett et al., 2006; Liu et al., 2010), or provide whole sentence rewritings (Madnani et al., 2012). Hermet and D´esilets (2009) attempted to extract single preposition corrections from whole sentence rewritings. Our work incorporates alignments information to handle complex changes on both word and phrase levels. In our approximation, we consider MT output as an approximation of word/phrase meanings. Using words in other languages to represent meanings has been explored in Carpuat and Wu (2007), where the focus is the aligned words’ identities. Our work instead focuses more on how many words each word is aligned to. Ex5 : . . . through which they can insert a special ... Ex6 : . . . the analysis and therefore selection of a single solution for adaptation. . . Note that unlike redundancies, these undesirable words/phrases change the sentences’ meanings. Despite the difference in definitions, our experimental work uses the NUCLE corpus because it provides many real world examples of redundancy. While redundancy detection has not yet been widely studied, it is related to several areas"
E14-1072,D10-1104,0,0.135406,"although “uncertain” is specific to Ex3 , it is redundant, because its meaning is already implied by “unpredictable”. Second, sentence compression systems try to shorten a sentence as much as possible, but an ESL redundancy detector should leave as much of the input sentences unchanged, if possible. One challenge involved in redundancy detection is that it often involves open class words (Ex3 ), as well as multi-word expressions (Ex1 , Ex4 ). Current GEC systems dealing with such error types are mostly MT based. MT systems tend to either require large training corpora (Brockett et al., 2006; Liu et al., 2010), or provide whole sentence rewritings (Madnani et al., 2012). Hermet and D´esilets (2009) attempted to extract single preposition corrections from whole sentence rewritings. Our work incorporates alignments information to handle complex changes on both word and phrase levels. In our approximation, we consider MT output as an approximation of word/phrase meanings. Using words in other languages to represent meanings has been explored in Carpuat and Wu (2007), where the focus is the aligned words’ identities. Our work instead focuses more on how many words each word is aligned to. Ex5 : . . . t"
E14-1072,W08-0336,0,0.0917045,"Missing"
E14-1072,N12-1019,0,0.183587,"t, because its meaning is already implied by “unpredictable”. Second, sentence compression systems try to shorten a sentence as much as possible, but an ESL redundancy detector should leave as much of the input sentences unchanged, if possible. One challenge involved in redundancy detection is that it often involves open class words (Ex3 ), as well as multi-word expressions (Ex1 , Ex4 ). Current GEC systems dealing with such error types are mostly MT based. MT systems tend to either require large training corpora (Brockett et al., 2006; Liu et al., 2010), or provide whole sentence rewritings (Madnani et al., 2012). Hermet and D´esilets (2009) attempted to extract single preposition corrections from whole sentence rewritings. Our work incorporates alignments information to handle complex changes on both word and phrase levels. In our approximation, we consider MT output as an approximation of word/phrase meanings. Using words in other languages to represent meanings has been explored in Carpuat and Wu (2007), where the focus is the aligned words’ identities. Our work instead focuses more on how many words each word is aligned to. Ex5 : . . . through which they can insert a special ... Ex6 : . . . the an"
E14-1072,D07-1001,0,0.200213,"x4 : . . . the cost incurred is not only just large sum of money . . . … and the cost incurred is not only just large sum of money … Figure 1: Among the three circled words, “just” is more redundant because deleting it hurts nether fluency nor meaning. These words/phrases are considered redundant because they are unnecessary (e.g. Ex1 , Ex2 ) or repetitive (e.g. Ex3 , Ex4 ). However, in NUCLE’s annotation scheme, some words that were marked redundant are really words that carry undesirable meanings. For example: Sentence compression systems (Jing, 2000; Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2007) aim at shortening a sentence while retaining the most important information and keeping it grammatically correct. This goal distinguishes these systems from ours in two major aspects. First, sentence compression systems assume that the original sentence is well-written; therefore retaining words specific to the sentence (e.g. “uncertain” in Ex3 ) can be a good strategy (Clarke and Lapata, 2007). In the ESL context, however, even specific words could still be redundant. For example, although “uncertain” is specific to Ex3 , it is redundant, because its meaning is already implied by “unpredicta"
E14-1072,E06-1038,0,0.207492,"and uncertain. Ex4 : . . . the cost incurred is not only just large sum of money . . . … and the cost incurred is not only just large sum of money … Figure 1: Among the three circled words, “just” is more redundant because deleting it hurts nether fluency nor meaning. These words/phrases are considered redundant because they are unnecessary (e.g. Ex1 , Ex2 ) or repetitive (e.g. Ex3 , Ex4 ). However, in NUCLE’s annotation scheme, some words that were marked redundant are really words that carry undesirable meanings. For example: Sentence compression systems (Jing, 2000; Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2007) aim at shortening a sentence while retaining the most important information and keeping it grammatically correct. This goal distinguishes these systems from ours in two major aspects. First, sentence compression systems assume that the original sentence is well-written; therefore retaining words specific to the sentence (e.g. “uncertain” in Ex3 ) can be a good strategy (Clarke and Lapata, 2007). In the ESL context, however, even specific words could still be redundant. For example, although “uncertain” is specific to Ex3 , it is redundant, because its meaning is alre"
E14-1072,W11-1601,0,0.0172782,"ification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the phrase “critical reception.” A sentence simplification system might rewrite it into “reviews”; but a system that removes redundancy should leave it unchanged because neither “critical” nor “reception” is extraneous. Moreover, consider the redundant phrase “had once before” in Ex4 . A simplification system does not need to change it because these words do not add complexity to the sentence. 3 A Probabilistic Model of Redundancy We consider a word or a phrase to be redundant if de"
E14-1072,D10-1094,0,0.0568468,"redundancies, these undesirable words/phrases change the sentences’ meanings. Despite the difference in definitions, our experimental work uses the NUCLE corpus because it provides many real world examples of redundancy. While redundancy detection has not yet been widely studied, it is related to several areas of active research, such as grammatical error correction (GEC), sentence simplification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the phrase “critical reception.” A sentence simplification system might rewrite it into “reviews”;"
E14-1072,P11-1092,0,0.0484362,"n. . . Note that unlike redundancies, these undesirable words/phrases change the sentences’ meanings. Despite the difference in definitions, our experimental work uses the NUCLE corpus because it provides many real world examples of redundancy. While redundancy detection has not yet been widely studied, it is related to several areas of active research, such as grammatical error correction (GEC), sentence simplification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the phrase “critical reception.” A sentence simplification system migh"
E14-1072,P07-1003,0,0.056153,"Missing"
E14-1072,P10-2065,0,0.083303,"e solution for adaptation. . . Note that unlike redundancies, these undesirable words/phrases change the sentences’ meanings. Despite the difference in definitions, our experimental work uses the NUCLE corpus because it provides many real world examples of redundancy. While redundancy detection has not yet been widely studied, it is related to several areas of active research, such as grammatical error correction (GEC), sentence simplification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the phrase “critical reception.” A sentence si"
E14-1072,W09-2110,0,0.0268808,"Missing"
H05-1107,W03-0403,0,0.0155585,"e an alternative to “pure” unannotated data; our data have been automatically annotated with projected labels from English. Although the projected labels are error-prone, they provide us with more information than automatically predicted labels used in bootstrapping methods. With a somewhat different goal in mind, active learning addresses the problem of choosing the most informative data for annotators to label so that the model would achieve the greatest improvement. Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). The drawback of an active learning approach is that it assumes that a staff of annotators is waiting on call, ready to label the examples chosen by the system at every iteration. In practice, it is more likely that one could only afford to staff annotators for a limited period of time. Although active learning is not a focus in this paper, we owe some ideas to active learning in choosing a small initial set of training examples; we discuss these ideas in section 3.2. More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser tr"
H05-1107,W03-0407,0,0.0174128,"y and coverage. Also related is the co-training algorithm (Blum and Mitchell, 1998) in which the bootstrapping process requires multiple learners that have different views of the problem. The key to co-training is that the views should be conditionally independent given the label. The strong independence requirement on the views is difficult to satisfy. For practical applications, different features sets or models (that are not conditionally independent) have been used as an approximation for different views. Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al., 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). Due to the relaxation of the view independence assumption, most empirical studies suggest a marginal improvement. The common thread between EM, self-training, and co-training is that they all bootstrap off of unannotated data. In this work, we explore an alternative to “pure” unannotated data; our data have been automatically annotated with projected labels from English. Although the projected labels are error-prone, they provide us with more information than automati"
H05-1107,P02-1033,0,0.113226,"Missing"
H05-1107,J94-4004,0,0.0136098,"show that our proposed approach achieves a significant improvement over EM and self-training and systems that are only trained on manual annotations. 1 Introduction Natural language applications that use supervised learning methods require annotated training data, but annotated data is scarce for many ∗ We thank Stephen Clark, Roger Levy, Carol Nichols, and the three anonymous reviewers for their helpful comments. The projection approach faces both a theoretical and a practical challenge. Theoretically, it is well-known that two languages often do not express the same meaning in the same way (Dorr, 1994). Practically, the projection framework is sensitive to component errors. In particular, poor word alignments significantly degrade the accuracy of the projected annotations. Previous research on resource projection attempts to address these problems by redistributing the parameter values (Yarowsky and Ngai, 2001) or by applying transformation rules (Hwa et al., 851 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 851–858, Vancouver, October 2005. 2005 Association for Computational Linguistics 2002). The"
H05-1107,P96-1042,0,0.0101905,"annotated data. In this work, we explore an alternative to “pure” unannotated data; our data have been automatically annotated with projected labels from English. Although the projected labels are error-prone, they provide us with more information than automatically predicted labels used in bootstrapping methods. With a somewhat different goal in mind, active learning addresses the problem of choosing the most informative data for annotators to label so that the model would achieve the greatest improvement. Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). The drawback of an active learning approach is that it assumes that a staff of annotators is waiting on call, ready to label the examples chosen by the system at every iteration. In practice, it is more likely that one could only afford to staff annotators for a limited period of time. Although active learning is not a focus in this paper, we owe some ideas to active learning in choosing a small initial set of training examples; we discuss these ideas in section 3.2. More recently, Smith and Smith (2004) proposed to merge an English parser, a word al"
H05-1107,P02-1050,1,0.89837,"Missing"
H05-1107,W04-2405,0,0.0131591,"hell, 1998) in which the bootstrapping process requires multiple learners that have different views of the problem. The key to co-training is that the views should be conditionally independent given the label. The strong independence requirement on the views is difficult to satisfy. For practical applications, different features sets or models (that are not conditionally independent) have been used as an approximation for different views. Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al., 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). Due to the relaxation of the view independence assumption, most empirical studies suggest a marginal improvement. The common thread between EM, self-training, and co-training is that they all bootstrap off of unannotated data. In this work, we explore an alternative to “pure” unannotated data; our data have been automatically annotated with projected labels from English. Although the projected labels are error-prone, they provide us with more information than automatically predicted labels used in bootstrapping methods. With a somewha"
H05-1107,W01-0501,0,0.0111451,"quires multiple learners that have different views of the problem. The key to co-training is that the views should be conditionally independent given the label. The strong independence requirement on the views is difficult to satisfy. For practical applications, different features sets or models (that are not conditionally independent) have been used as an approximation for different views. Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al., 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). Due to the relaxation of the view independence assumption, most empirical studies suggest a marginal improvement. The common thread between EM, self-training, and co-training is that they all bootstrap off of unannotated data. In this work, we explore an alternative to “pure” unannotated data; our data have been automatically annotated with projected labels from English. Although the projected labels are error-prone, they provide us with more information than automatically predicted labels used in bootstrapping methods. With a somewhat different goal in mind, active learning addresses the pr"
H05-1107,W96-0213,0,0.157848,"lly tagged data (projected from English) and a small corpus of manually tagged data; the two models are then combined into one via the Whitten-Bell backoff language model. 3.1 Projected Data One method of acquiring a large corpus of automatically POS tagged Chinese data is by projection (Yarowsky and Ngai, 2001). This 853 approach requires a sentence-aligned EnglishChinese corpus, a high-quality English tagger, and a method of aligning English and Chinese words that share the same meaning. Given the parallel corpus, we tagged the English words with a publicly available maximum entropy tagger (Ratnaparkhi, 1996), and we used an implementation of the IBM translation model (AlOnaizan et al., 1999) to align the words. The Chinese words in the parallel corpus would then receive the same POS tags as the English words to which they are aligned. Next, the basic projection algorithm is modified to accommodate two complicating factors. First, word alignments are not always one-to-one. To compensate, we assign a default tag to unaligned Chinese words; in the case of one-Chinese-to-manyEnglish, the Chinese word would receive the tag of the final English word. Second, English and Chinese do not share the same ta"
H05-1107,N01-1023,0,0.0130497,"s the co-training algorithm (Blum and Mitchell, 1998) in which the bootstrapping process requires multiple learners that have different views of the problem. The key to co-training is that the views should be conditionally independent given the label. The strong independence requirement on the views is difficult to satisfy. For practical applications, different features sets or models (that are not conditionally independent) have been used as an approximation for different views. Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al., 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). Due to the relaxation of the view independence assumption, most empirical studies suggest a marginal improvement. The common thread between EM, self-training, and co-training is that they all bootstrap off of unannotated data. In this work, we explore an alternative to “pure” unannotated data; our data have been automatically annotated with projected labels from English. Although the projected labels are error-prone, they provide us with more information than automatically predicted labels u"
H05-1107,W04-3207,0,0.12745,"ng examples has been explored in a number of ways in the past. Most popular is the family of bootstrapping algorithms, in which a model is seeded with a small amount of labeled data and iteratively improved as more unlabeled data are folded into the training set, typically, through unsupervised learning. Another approach is active learning (Cohn et al., 1996), in which the model is also iteratively improved but the training examples are chosen by the learning model, and the learning process is supervised. Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). Of the bootstrapping methods, perhaps the most well-known is the Expectation Maximization (EM) algorithm. This approach has been explored in the context of many NLP applications; one example is text classification (Nigam 852 et al., 1999). Another bootstrapping approach reminiscent of EM is self-training. Yarowsky (1995) used this method for word sense disambiguation. In self-training, annotated examples are used as seeds to train an initial classifier with any supervised learning method. This initial classifier is then used to automatically annotate data from a large pool of unlabeled examp"
H05-1107,J93-2006,0,0.0126761,"way of merging these two taggers into a single HMM (denoted as Tinterp ) is to use interpolation: swap score(si , rj ) { Ssel new = (Ssel − si ) ∪ rj ; if ( |Ssel new |> M ) return -1; else return T Y P E(Ssel new ) − T Y P E(Ssel ); } pinterp (w|t) = λ × panno (w|t) +(1 − λ) × pproj (w|t) pinterp (ti |ti−1 , ti−2 ) = panno (ti |ti−1 , ti−2 ) Figure 1: The pseudo-code for MWC algorithm. The input is M and S and the output is Ssel coverage of unknown words (MWC). This algorithm is described in Figure 1, 3.3 Basic POS Tagging Model It is well known that a POS tagger can be trained with an HMM (Weischedel et al., 1993). Given a trained model, the most likely tag sequence Tˆ = {t1 , t2 , . . . tn } is computed for the ˆ = {w1 , w2 , . . . wn }: input word sentence: W Tˆ = arg max P (T |W ) = arg max P (T |W )P (T ) T where λ is a tunable weighting parameter1 of the merged tagger. This approach may be problematic because it forces the model to always include some fraction of poor parameter values. Therefore, we propose to estimate the observation probabilities using backoff. The parameters of Tback are estimated as follows: ( pback (w|t) = α(t) × panno (w|t) if panno (w|t) > 0 β(t) × pproj (w|t) if panno (w|t"
H05-1107,N01-1026,0,0.80693,"∗ We thank Stephen Clark, Roger Levy, Carol Nichols, and the three anonymous reviewers for their helpful comments. The projection approach faces both a theoretical and a practical challenge. Theoretically, it is well-known that two languages often do not express the same meaning in the same way (Dorr, 1994). Practically, the projection framework is sensitive to component errors. In particular, poor word alignments significantly degrade the accuracy of the projected annotations. Previous research on resource projection attempts to address these problems by redistributing the parameter values (Yarowsky and Ngai, 2001) or by applying transformation rules (Hwa et al., 851 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 851–858, Vancouver, October 2005. 2005 Association for Computational Linguistics 2002). Their experimental results suggest that while these techniques can overcome some errors, they are not sufficient for projected data that are very noisy. In this work, we tackle the same problems by relaxing the zero manual annotation constraint. The main question we address is: how can we make the most out of a small"
H05-1107,P95-1026,0,0.0622142,"e learning (Cohn et al., 1996), in which the model is also iteratively improved but the training examples are chosen by the learning model, and the learning process is supervised. Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). Of the bootstrapping methods, perhaps the most well-known is the Expectation Maximization (EM) algorithm. This approach has been explored in the context of many NLP applications; one example is text classification (Nigam 852 et al., 1999). Another bootstrapping approach reminiscent of EM is self-training. Yarowsky (1995) used this method for word sense disambiguation. In self-training, annotated examples are used as seeds to train an initial classifier with any supervised learning method. This initial classifier is then used to automatically annotate data from a large pool of unlabeled examples. Of these newly labeled data, the ones labeled with the highest confidence are used as examples to train a new classifier. Yarowsky showed that repeated application of this process resulted in a series of word sense classifiers with improved accuracy and coverage. Also related is the co-training algorithm (Blum and Mit"
hashemi-hwa-2014-comparison,W10-1004,0,\N,Missing
hashemi-hwa-2014-comparison,W08-1205,0,\N,Missing
hashemi-hwa-2014-comparison,W12-3102,0,\N,Missing
hashemi-hwa-2014-comparison,P02-1040,0,\N,Missing
hashemi-hwa-2014-comparison,P11-1093,0,\N,Missing
hashemi-hwa-2014-comparison,D11-1148,0,\N,Missing
hashemi-hwa-2014-comparison,D11-1010,0,\N,Missing
hashemi-hwa-2014-comparison,U09-1008,0,\N,Missing
hashemi-hwa-2014-comparison,W11-2107,0,\N,Missing
J04-3001,P01-1005,0,0.029703,"benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification 271 Computational Linguistics Volume 30, Number 3 applications. Some examples include text categorization (Lewis and Catlett 1994), base noun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (Engelson Dagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and word sense disambiguation (Fujii et al. 1998). More challenging are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson, Califf, and Mooney 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. A recent effort that focuses on statistical syntactic parsing is the work by Tang, Lou, and Roukos (2002). Their results suggest that the number of training examples can be further reduced by using a hybrid"
J04-3001,C94-2195,0,0.0124192,"s far. The loop continues until one of three stopping conditions is met: The hypothesis is considered to perform well enough, all candidates are labeled, or an absolute cutoff point is reached (e.g., no more resources). 3. Sample Selection for Prepositional-Phrase Attachment One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models (Brill and Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximumentropy model (Ratnaparkhi 1998). Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier. Our experiments show that the best evaluation function can reduce the number of labeled examples by nearly half without"
J04-3001,A00-2018,0,0.279803,"Missing"
J04-3001,P97-1003,0,0.0947563,"Missing"
J04-3001,W95-0103,0,0.036203,"e stopping conditions is met: The hypothesis is considered to perform well enough, all candidates are labeled, or an absolute cutoff point is reached (e.g., no more resources). 3. Sample Selection for Prepositional-Phrase Attachment One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models (Brill and Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximumentropy model (Ratnaparkhi 1998). Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier. Our experiments show that the best evaluation function can reduce the number of labeled examples by nearly half without loss of accuracy. 3.1 A Summary of the Colli"
J04-3001,P96-1042,0,0.13216,"Missing"
J04-3001,A00-2005,0,0.111245,"Missing"
J04-3001,P98-1091,1,0.869702,"Missing"
J04-3001,W00-1306,1,0.839403,"Missing"
J04-3001,J04-3001,1,0.106556,"Missing"
J04-3001,W01-0710,1,0.892339,"Missing"
J04-3001,J93-2004,0,0.0286652,"Missing"
J04-3001,P00-1016,0,0.369486,"inty of the parser may result in better performance for lexicalized parsers. 5. Related Work Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification 271 Computational Linguistics Volume 30, Number 3 applications. Some examples include text categorization (Lewis and Catlett 1994), base noun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (Engelson Dagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and word sense disambiguation (Fujii et al. 1998). More challenging are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson, Califf, and Mooney 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. A recent effort that focuses on statistical syntactic parsing is the work by Tang, Lou, and Roukos"
J04-3001,P92-1017,0,0.190806,"riteria. We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models. 1. Introduction Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data. For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992). Current state-of-the-art statistical parsers (Collins 1999; Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However, supervised training data are difficult to obtain; existing corpora might not contain the relevant type of annotation, and the data might not be in the domain of interest. For example, one might need lexical-semantic analyses in addition to the syntactic analyses in the treebank, or one might be interested in processing languages, domains, or genres for which there are no annotated corpora. Because"
J04-3001,W01-0501,0,0.0414582,"Missing"
J04-3001,P98-2177,0,0.0121543,"ered to perform well enough, all candidates are labeled, or an absolute cutoff point is reached (e.g., no more resources). 3. Sample Selection for Prepositional-Phrase Attachment One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it. Researchers have proposed many computational models for resolving PPattachment ambiguities. Some well-known approaches include rule-based models (Brill and Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximumentropy model (Ratnaparkhi 1998). Following the tradition of using learning PPattachment as a way to gain insight into the parsing problem, we first apply sample selection to reduce the amount of annotation used in training a PP-attachment model. We use the Collins-Brooks model as the basic learning algorithm and experiment with several evaluation functions based on the types of predictive criteria described earlier. Our experiments show that the best evaluation function can reduce the number of labeled examples by nearly half without loss of accuracy. 3.1 A Summary of the Collins-Brooks Model The Collins-Brooks model takes"
J04-3001,N01-1023,0,0.0330763,"bottleneck problem in parsing. For example, Henderson and Brill (2002) consider the case in which acquiring additional human-annotated training data is not possible. They show that parser performance can be improved by using boosting and bagging techniques with multiple parsers. This approach assumes that there are enough existing labeled data to train the individual parsers. Another technique for making better use of unlabeled data is cotraining (Blum and Mitchell 1998), in which two sufficiently different learners help each other learn by labeling training data for one another. The work of Sarkar (2001) and Steedman, Osborne, et al. (2003) suggests that co-training can be helpful for statistical parsing. Pierce and Cardie (2001) have shown, in the context of base noun identification, that combining sample selection and cotraining can be an effective learning framework for large-scale training. Similar approaches are being explored for parsing (Steedman, Hwa, et al. 2003; Hwa et al. 2003). 6. Conclusion In this article, we have argued that sample selection is a powerful learning technique for reducing the amount of human-labeled training data. Our empirical studies suggest that sample selecti"
J04-3001,1993.iwpt-1.20,0,0.382431,"Missing"
J04-3001,N03-1031,1,0.788975,"Missing"
J04-3001,E03-1008,1,0.618941,"Missing"
J04-3001,P02-1016,0,0.615761,"Missing"
J04-3001,J98-4002,0,\N,Missing
J04-3001,J03-4003,0,\N,Missing
J04-3001,C98-1088,1,\N,Missing
J04-3001,C98-2172,0,\N,Missing
N03-1031,A00-2018,0,0.0290607,"s are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human"
N03-1031,P96-1042,0,0.012772,"Missing"
N03-1031,N01-1023,1,0.558581,"e. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a subset of their output, chosen by some selection mechanism, is used in order to minimize errors. The choice of selection method significantly affects the quality of the resulting parsers. We investigate a novel approach of selecting training examples for co-training parsers by incorporating the idea of maximizing training uti"
N03-1031,E03-1008,1,0.765705,"Missing"
N03-1031,W00-1306,1,0.314843,"ility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples inc"
N03-1031,P02-1016,0,0.00757357,"rion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a sub"
N03-1031,J93-2004,0,0.0255646,"h, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the"
N03-1031,P00-1016,0,0.0133509,"Missing"
N03-1031,W01-0501,0,0.0173865,"have different goals, their selection methods focus on different criteria: co-training typically favors selecting accurately labeled examples, while sample selection typically favors selecting examples with high training utility, which often are not sentences that the parsers already label accurately. In this work, we investigate selection methods for co-training that explore the trade-off between maximizing training utility and minimizing errors. Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training (Pierce and Cardie, 2001), in which the selected examples are manually checked and corrected before being added to the 1 In the context of training parsers, a labeled example is a sentence with its parse tree. Throughout this paper, we use the term “label” and “parse” interchangeably. training data. For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain. For corrected co-training, we show that selecting examples with high training utility reduces the number of sentences the human annotator has to check. For both frameworks, we show that"
N03-1031,J03-4003,0,\N,Missing
N03-1031,P02-1046,0,\N,Missing
N10-1040,W07-0702,0,0.0204284,"model that employs more linguistically rich features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations. 1 2 Introduction Recently, there have been a number of successful attempts at improving phrase-based statistical machine translation by exploiting linguistic knowledge such as morphology, part-of-speech tags, and syntax. Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-toright on the target sentence. Traditionally, n-best rerankers (Shen et al., 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al., 2007). We argue that it can be desirable to pre-translate parts of the source text before sentence-level decoding begins, using a richer model that would typ"
N10-1040,2003.mtsummit-papers.6,0,0.0247554,"g sentence-level decoding. In Approach Re-ranking tends to use expensive features of the entire source and target sentences, s and t, and alignments, a, to produce a score for the translation. We will call this scoring function φ(s, t, a). While φ(·) might capture quite a bit of linguistic information, it can be problematic to use this function for decoding directly. This is due to both the expense of computing it, and the difficulty in using it to guide the decoder’s search. For example, a choice of φ(·) that relies on a top-down parser is difficult to integrate into a left-to-right decoder (Charniak et al., 2003). Our idea is to use an expensive scoring function to guide the search for potential translations for part of a source sentence, S, even if translating all of it isn’t feasible. We can then provide these translations to the decoder, along with their scores, to incorporate them as it builds the complete translation of S. This differs from approaches such as (Och and Ney, 2004) because we generate new phrase pairs in isolation, rather than incorporating everything into the sentence-level decoder. The baseline system is the Moses phrase-based translation system (Koehn 301 Human Language Technolog"
N10-1040,N09-1025,0,0.0223355,"ional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations. 1 2 Introduction Recently, there have been a number of successful attempts at improving phrase-based statistical machine translation by exploiting linguistic knowledge such as morphology, part-of-speech tags, and syntax. Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-toright on the target sentence. Traditionally, n-best rerankers (Shen et al., 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al., 2007). We argue that it can be desirable to pre-translate parts of the source text before sentence-level decoding begins, using a richer model that would typically be out of reach during sentence-level decoding. In Approach Re"
N10-1040,D09-1023,0,0.0183639,"more linguistically rich features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations. 1 2 Introduction Recently, there have been a number of successful attempts at improving phrase-based statistical machine translation by exploiting linguistic knowledge such as morphology, part-of-speech tags, and syntax. Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-toright on the target sentence. Traditionally, n-best rerankers (Shen et al., 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al., 2007). We argue that it can be desirable to pre-translate parts of the source text before sentence-level decoding begins, using a richer model that would typically be out of reach d"
N10-1040,N07-2015,0,0.0214012,"h as morphology, part-of-speech tags, and syntax. Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-toright on the target sentence. Traditionally, n-best rerankers (Shen et al., 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al., 2007). We argue that it can be desirable to pre-translate parts of the source text before sentence-level decoding begins, using a richer model that would typically be out of reach during sentence-level decoding. In Approach Re-ranking tends to use expensive features of the entire source and target sentences, s and t, and alignments, a, to produce a score for the translation. We will call this scoring function φ(s, t, a). While φ(·) might capture quite a bit of linguistic information, it can be problematic to use this function for decoding directly. This is due to both the expense of computing it, a"
N10-1040,D07-1091,0,0.0132569,"features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations. 1 2 Introduction Recently, there have been a number of successful attempts at improving phrase-based statistical machine translation by exploiting linguistic knowledge such as morphology, part-of-speech tags, and syntax. Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-toright on the target sentence. Traditionally, n-best rerankers (Shen et al., 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al., 2007). We argue that it can be desirable to pre-translate parts of the source text before sentence-level decoding begins, using a richer model that would typically be out of reach during sentence-level de"
N10-1040,N03-1017,0,0.0925877,"igned words are always kept as their surface form. φP OS (s, t, a) assign a score based on the probability of the resulting prototypes; more likely prototypes should yield higher scores. We choose: φP OS (s, t, a) = p(SP, AP |T P ) · p(T P, AP |SP ) where SP is the source prototype constructed from s, t, a. Similarly, T P and AP are the target and alignment prototypes, respectively. To compute φP OS (·), we must build a model for each of p(SP, AP |T P ) and p(T P, AP |SP ). To do this, we start with a corpus of aligned, POS-tagged bilingual text. We then find phrases that are consistent with (Koehn et al., 2003). As we extract these phrase pairs, we convert each into a phrase proto302 type by replacing surface forms with POS tags for all aligned words in the prototype. After we have processed the bilingual training text, we have collected a set of phrase prototypes and a count of how often each was observed. 2.2 Generating New Phrases To generate phrases, we scan through the source text to be translated, finding any span of source words that matches the source prototype of at least one phrase prototype. For each such phrase, and for each phrase prototype which it matches, we generate all target phras"
N10-1040,P07-2045,0,0.0145799,"l Setup 0.37 The Linguistic Data Consortium Arabic-English corpus23 is used to train the baseline MT system (34K sentences, about one million words), and to learn phrase prototypes. The LDC multi-translation Arabic-English corpus (NIST2003)4 is used for tuning and testing; the tuning set consists of the first 500 sentences, and the test set consists of the next 500 sentences. The language model is a 4-gram model built from the English side of the parallel corpus, plus the English side of the wmt07 GermanEnglish and French-English news commentary data. The baseline translation system is Moses (Koehn et al., 2007), with the msd-bidirectional-fe reordering model. Evaluation is done using the BLEU (Papineni et al., 2001) metric with four references. All text is lowercased before evaluation; recasing is not used. We use the Stanford Arabic POS Tagging system, based on (Toutanova et al., 2003)5 . The word-to-word dictionary that is used in the phrase generation step of our method is extracted from the highest-scoring translations for each source word in the baseline phrase table. For some closedclass words, we use a small, manually constructed dictionary to reduce the noise in the phrase table that exists"
N10-1040,J04-4002,0,0.0415676,"to both the expense of computing it, and the difficulty in using it to guide the decoder’s search. For example, a choice of φ(·) that relies on a top-down parser is difficult to integrate into a left-to-right decoder (Charniak et al., 2003). Our idea is to use an expensive scoring function to guide the search for potential translations for part of a source sentence, S, even if translating all of it isn’t feasible. We can then provide these translations to the decoder, along with their scores, to incorporate them as it builds the complete translation of S. This differs from approaches such as (Och and Ney, 2004) because we generate new phrase pairs in isolation, rather than incorporating everything into the sentence-level decoder. The baseline system is the Moses phrase-based translation system (Koehn 301 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 301–304, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics et al., 2007). 2.1 Description of Our Scoring Function For this work, we consider a scoring function based on part-of-speech (POS) tags, φP OS (·). It operates in two steps: it converts the source and tar"
N10-1040,P03-1021,0,0.0181278,"Missing"
N10-1040,2001.mtsummit-papers.68,0,0.0316023,"tem (34K sentences, about one million words), and to learn phrase prototypes. The LDC multi-translation Arabic-English corpus (NIST2003)4 is used for tuning and testing; the tuning set consists of the first 500 sentences, and the test set consists of the next 500 sentences. The language model is a 4-gram model built from the English side of the parallel corpus, plus the English side of the wmt07 GermanEnglish and French-English news commentary data. The baseline translation system is Moses (Koehn et al., 2007), with the msd-bidirectional-fe reordering model. Evaluation is done using the BLEU (Papineni et al., 2001) metric with four references. All text is lowercased before evaluation; recasing is not used. We use the Stanford Arabic POS Tagging system, based on (Toutanova et al., 2003)5 . The word-to-word dictionary that is used in the phrase generation step of our method is extracted from the highest-scoring translations for each source word in the baseline phrase table. For some closedclass words, we use a small, manually constructed dictionary to reduce the noise in the phrase table that exists for very common words. We use this in place of a stand-alone dictionary to reduce the need for additional r"
N10-1040,N04-1023,0,0.0261824,"tence translations. 1 2 Introduction Recently, there have been a number of successful attempts at improving phrase-based statistical machine translation by exploiting linguistic knowledge such as morphology, part-of-speech tags, and syntax. Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-toright on the target sentence. Traditionally, n-best rerankers (Shen et al., 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al., 2007). We argue that it can be desirable to pre-translate parts of the source text before sentence-level decoding begins, using a richer model that would typically be out of reach during sentence-level decoding. In Approach Re-ranking tends to use expensive features of the entire source and target sentences, s and t, and alignments, a, to produce a score for the translation. We will call this scoring function"
N10-1040,N03-1033,0,0.0101694,"tuning set consists of the first 500 sentences, and the test set consists of the next 500 sentences. The language model is a 4-gram model built from the English side of the parallel corpus, plus the English side of the wmt07 GermanEnglish and French-English news commentary data. The baseline translation system is Moses (Koehn et al., 2007), with the msd-bidirectional-fe reordering model. Evaluation is done using the BLEU (Papineni et al., 2001) metric with four references. All text is lowercased before evaluation; recasing is not used. We use the Stanford Arabic POS Tagging system, based on (Toutanova et al., 2003)5 . The word-to-word dictionary that is used in the phrase generation step of our method is extracted from the highest-scoring translations for each source word in the baseline phrase table. For some closedclass words, we use a small, manually constructed dictionary to reduce the noise in the phrase table that exists for very common words. We use this in place of a stand-alone dictionary to reduce the need for additional resources. 4 Experiments To see the effect on the BLEU score of the resulting sentence-level translation, we vary the amount of bilingual data used to build the phrase prototy"
N10-1040,C04-1073,0,0.0316744,"tion task is more constrained, we can use a model that employs more linguistically rich features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations. 1 2 Introduction Recently, there have been a number of successful attempts at improving phrase-based statistical machine translation by exploiting linguistic knowledge such as morphology, part-of-speech tags, and syntax. Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-toright on the target sentence. Traditionally, n-best rerankers (Shen et al., 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al., 2007). We argue that it can be desirable to pre-translate parts of the source text before sentence-level decoding beg"
N10-1040,P02-1040,0,\N,Missing
N16-1040,W11-1601,0,0.0242686,"e sentence. We consider both grammatical fluency as well as references resolution in our automatic Post-Editing technique. These phrasal challenges set our goals apart from related work on lexical simplification and substitution (Specia et al., 2012; Jauhar and Specia, 2012; McCarthy, 2002; McCarthy and Navigli, 2007) and 1 This work uses TheFreeDictionary.com. 363 Proceedings of NAACL-HLT 2016, pages 363–373, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics from general sentence simplification (Wubben et al., 2012; Siddharthan, 2014; Zhu et al., 2010; Coster and Kauchak, 2011) methods. We validate the plausibility of the proposed methods with empirical experiments on a manually annotated corpus.2 Results from both automatic evaluations and user studies show that the proposed approach can generate high-quality paraphrases of sentences containing idiomatic expressions. A successful idiom paraphrase generator may not only benefit non-native speakers, but may also facilitate other NLP applications. 2 Background The main idea of this work is to produce a fluent and meaningful paraphrase of the original sentence similar to how a human non-native reader might approach the"
N16-1040,de-marneffe-etal-2006-generating,0,0.0604187,"Missing"
N16-1040,J09-1005,0,0.241915,"Missing"
N16-1040,W08-1105,0,0.0231593,"ets my blood up.3 If they do not understand the expression gets my blood up, they may look it up in a dictionary: Definition: Fig. to get someone or oneself angry. (Fixed order.)4 Then they might try to reconcile the definition with the context of the sentence and arrive at: Paraphrased : This kind of language really barfs me out and gets me angry. In the example above, only a portion of the full definition is needed. One possible way to identify this relevant nugget is to apply sentence compression techniques (McDonald, 2006; Siddharthan, ˇ 2011; Stajner et al., 2013; Filippova et al., 2015; Filippova and Strube, 2008; Narayan and Gardent, 2014; Cohn and Lapata, 2009). However, all these methods have been developed for standard texts with complete sentences, and it is not clear whether they are suited to dictionary definitions. Consider Table 1, in which a corpus of 1000 randomly selected 2 https://github.com/liucs1986/idiom_ corpus 3 https://twitter.com/ezzwanaezwnd/ status/231992426548559872 4 http://idioms.thefreedictionary.com/ get+blood+up Corpus Average length Punctuation density CLspoken CLwritten Definition 17 18 12 2.16 2.07 2.86 Table 1: Some statistics over normal text corpora and an idiom defin"
N16-1040,D15-1042,0,0.0634971,"Missing"
N16-1040,S12-1066,0,0.0498072,"Missing"
N16-1040,S13-2007,0,0.257481,"Missing"
N16-1040,J94-4002,0,0.54456,"t replacements (e.g., the idiom lather something up with the definition to apply thick soapsuds to something), further analyses on the idiom’s sentential context are necessary for many idioms (e.g., see eye to eye has no obvious slot). Typical reference expressions in a definition include something, someone, somebody, you, they, which often refer to noun phrases (NPs) in and around the idiom in the sentence. When the sentence context contains multiple NPs, we need to choose the right one to resolve the reference. To do so, we rely on two commonly used factors: recency and syntactic agreement (Lappin and Leass, 1994). Similar to the work of Siddharthan (2006), we extract all NPs in the original sentence with their agreement types and grammatical functions; for each NP, we assign it a score with equal weights of recency and syntactic factors. We choose the NP that satisfy the agreements and grammatical functions with the highest score, breaking ties by selecting the closest NP. When no contextual NP is suitable, we replace the reference expression with generics such as ”it,” ”people,” or ”person” instead. There is one subtle difference between reference resolution in our work and typical cases. In addition"
N16-1040,S07-1009,0,0.0754886,"Missing"
N16-1040,W02-0816,0,0.160989,"Missing"
N16-1040,E06-1038,0,0.412991,"s the following sentence: Sentence: This kind of language really barfs me out and gets my blood up.3 If they do not understand the expression gets my blood up, they may look it up in a dictionary: Definition: Fig. to get someone or oneself angry. (Fixed order.)4 Then they might try to reconcile the definition with the context of the sentence and arrive at: Paraphrased : This kind of language really barfs me out and gets me angry. In the example above, only a portion of the full definition is needed. One possible way to identify this relevant nugget is to apply sentence compression techniques (McDonald, 2006; Siddharthan, ˇ 2011; Stajner et al., 2013; Filippova et al., 2015; Filippova and Strube, 2008; Narayan and Gardent, 2014; Cohn and Lapata, 2009). However, all these methods have been developed for standard texts with complete sentences, and it is not clear whether they are suited to dictionary definitions. Consider Table 1, in which a corpus of 1000 randomly selected 2 https://github.com/liucs1986/idiom_ corpus 3 https://twitter.com/ezzwanaezwnd/ status/231992426548559872 4 http://idioms.thefreedictionary.com/ get+blood+up Corpus Average length Punctuation density CLspoken CLwritten Definiti"
N16-1040,P98-2143,0,0.279992,"ement phrase agrees with how it is used in the sentence. Similarly, when replacing a verb phrase idiom, we need to perform verb tense, person and number agreement checks, such as converting get someone angry to gets someone angry in the example mentioned in Section 2. 3.2.2 Reference Resolution Reference expression is common in definition of idiom. For example, the idiom see eye to eye has a shortened definition of they agree with each other. The referent they has to be resolved when we substitute the idiom with it. The general reference resolution problem is a long-standing challenge in NLP (Mitkov, 1998; Hobbs, 1978; Hobbs, 1979); even in the limited context of our idiom substitution problem, it is not trivial. While regular expression matching may work for idioms that contain simple slot replacements (e.g., the idiom lather something up with the definition to apply thick soapsuds to something), further analyses on the idiom’s sentential context are necessary for many idioms (e.g., see eye to eye has no obvious slot). Typical reference expressions in a definition include something, someone, somebody, you, they, which often refer to noun phrases (NPs) in and around the idiom in the sentence."
N16-1040,P14-1041,0,0.230198,"not understand the expression gets my blood up, they may look it up in a dictionary: Definition: Fig. to get someone or oneself angry. (Fixed order.)4 Then they might try to reconcile the definition with the context of the sentence and arrive at: Paraphrased : This kind of language really barfs me out and gets me angry. In the example above, only a portion of the full definition is needed. One possible way to identify this relevant nugget is to apply sentence compression techniques (McDonald, 2006; Siddharthan, ˇ 2011; Stajner et al., 2013; Filippova et al., 2015; Filippova and Strube, 2008; Narayan and Gardent, 2014; Cohn and Lapata, 2009). However, all these methods have been developed for standard texts with complete sentences, and it is not clear whether they are suited to dictionary definitions. Consider Table 1, in which a corpus of 1000 randomly selected 2 https://github.com/liucs1986/idiom_ corpus 3 https://twitter.com/ezzwanaezwnd/ status/231992426548559872 4 http://idioms.thefreedictionary.com/ get+blood+up Corpus Average length Punctuation density CLspoken CLwritten Definition 17 18 12 2.16 2.07 2.86 Table 1: Some statistics over normal text corpora and an idiom definition corpus. idiom definit"
N16-1040,P02-1040,0,0.111986,"Missing"
N16-1040,W14-1007,0,0.182857,"Missing"
N16-1040,W11-2802,0,0.0632287,"Missing"
N16-1040,S12-1046,0,0.0546827,"Missing"
N16-1040,E09-1086,0,0.268638,"Missing"
N16-1040,P03-1039,0,0.0586145,"egrade the sequential fluency of the shortened definition. A better alternative is to segment the definition into syntactic chunks such as non-embedded NP, VP, ADJP, ADVP and PP phrases using off-the-shelf namely, viz. , specifically, so that, when, (, ). 6 These are obtained by using the shallow parser in Natural Language Tool Kit (NLTK) (Bird, 2006) and the parts-ofspeech tagger in Stanford Parser (De Marneffe et al., 2006). shallow parsers (e.g., NLTK). Chunks have been shown to minimize the generation of discontinuous sentences in previous works in machine translation (Zhang et al., 2007; Watanabe et al., 2003). We apply a trained binary Support Vector Machine (SVM) classifier to each chunk to predict whether it should be kept or discarded. The shortened definition consists of only chunks that are kept. Lexical and syntactical features are extracted from definition chunks as well as the sentence containing the original idiom. We have also incorporated features that related previous works have found ˇ to be beneficial (Stajner et al., 2013; Narayan and Gardent, 2014). The following is a brief description of our feature set. Features from the Sentence: These features encode the syntactic context of th"
N16-1040,P12-1107,0,0.0359854,"Missing"
N16-1040,C10-1142,0,0.0763236,"Missing"
N16-1040,W07-0401,0,0.0521845,"Missing"
N16-1040,C10-1152,0,0.0603323,"ion phrase into the sentence. We consider both grammatical fluency as well as references resolution in our automatic Post-Editing technique. These phrasal challenges set our goals apart from related work on lexical simplification and substitution (Specia et al., 2012; Jauhar and Specia, 2012; McCarthy, 2002; McCarthy and Navigli, 2007) and 1 This work uses TheFreeDictionary.com. 363 Proceedings of NAACL-HLT 2016, pages 363–373, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics from general sentence simplification (Wubben et al., 2012; Siddharthan, 2014; Zhu et al., 2010; Coster and Kauchak, 2011) methods. We validate the plausibility of the proposed methods with empirical experiments on a manually annotated corpus.2 Results from both automatic evaluations and user studies show that the proposed approach can generate high-quality paraphrases of sentences containing idiomatic expressions. A successful idiom paraphrase generator may not only benefit non-native speakers, but may also facilitate other NLP applications. 2 Background The main idea of this work is to produce a fluent and meaningful paraphrase of the original sentence similar to how a human non-nativ"
N16-1040,P06-4018,0,\N,Missing
N16-1040,C98-2138,0,\N,Missing
N16-3008,P03-1054,0,0.0237134,"by aligning sentences across drafts. An added sentence or a deleted sentence is treated as aligned to null. The 1 rewriting assistant interface: www.cs.pitt.edu/ ˜zhangfan/argrewrite now supported on chrome and firefox browser only 2 revision correction component: www.cs.pitt.edu/ Tutorial ˜zhangfan/revisionCorrection.jar. to the web and java interface: www.cs.pitt.edu/ ˜zhangfan/argrewrite/tutorial.pdf 38 Figure 1: System structure of our rewriting assistance system. aligned pairs where the sentences in the pair are not identical are extracted as revisions. We first use the Stanford Parser (Klein and Manning, 2003) to break the original text into sentences and then align the sentences using the algorithm in our prior work (Zhang and Litman, 2014) which considers both sentence similarity (calculated using TF*IDF score) and the global context of sentences. Revision classification. Following the argumentative revision definition in our prior work (Zhang and Litman, 2015), revisions are first categorized to Content (Text-based) and Surface3 according to whether the revision changed the meaning of the essay or not. The Text-based revisions include Thesis/Ideas (Claim), Rebuttal, Reasoning (Warrant), Evidence"
N16-3008,W14-1818,1,0.63572,"of these systems, instructors typically can neither correct the errors made by 37 the automatic analysis nor observe/assess the students’ revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; Zhang and Litman, 2015), we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings. The purpose of each change between revisions is demonstrated to the writer as a kind of feedback. If the author’s revision purpose is not correctly recognized, it indicates that the effect of the writer’s change might have not met the writer’s expectation, which suggests that the writer should revise their revisions."
N16-3008,W15-0616,1,0.84836,"ctors typically can neither correct the errors made by 37 the automatic analysis nor observe/assess the students’ revision efforts. We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; Zhang and Litman, 2015), we focused on 1) and 2), the automatic extraction and classification of revisions for argumentative writings. In this work, we extend our framework to integrate the automatic analyzer with a web-based interface to support student argumentative writings. The purpose of each change between revisions is demonstrated to the writer as a kind of feedback. If the author’s revision purpose is not correctly recognized, it indicates that the effect of the writer’s change might have not met the writer’s expectation, which suggests that the writer should revise their revisions. The framework also connec"
N18-2036,P11-1092,0,0.014176,"the meaning of the expression (Merriam-Webster, 1983; Quinn, 1993; Lehmann, 2005). Although pleonastic phrases may serve literary functions (e.g., to add emphasis) (Miller, 1951; Chernov, 1979), most modern writing style guides caution against them in favor of concise writing (Hart et al., 1905; Williams, 2003; Turabian, 2013; Gowers, 2014; Strunk, 1920). An automatic pleonasm detector would be beneficial for natural language processing (NLP) applications that support student writing, such as grammar error correction (GEC) (Han et al., 2006; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011), automatic essay grading (Larkey, 1998; Landauer, 2003; Ong et al., 2014), and intelligent writing tutors (Merrill et al., 1992; Aleven et al., 2009; Atkinson, 2016). Pleonastic phrases may also negatively impact NLP applications in general because they introduce an unnecessary complexity to the language. Their removal might facilitate NLP tasks such as parsing, summarization, and machine translation. However, automated pleonasm detection is a challenging problem, in parts because there is no appropriate resources to support the development of such systems. While 2 Semantic Pleonasm Although"
N18-2036,W13-1703,0,0.0899549,"e. Given our design choices, the current SPC is not a large corpus; we posit that it can nonetheless serve as a valuable resource for developing systems to detect semantic pleonasm. For example, the earlier work of Xue and Hwa (2014) might have benefited from this resource. They wanted to detect the word in a sentence that contributes the least to the meaning of the sentence; however, their experiments were hampered by a mismatch between their intended domain and the corpus they evaluated on – while their model estimated a word’s semantic redundancy, their experiments were performed on NUCLE (Dahlmeier et al., 2013), a learner corpus that focused more on grammatical errors. Moreover, since their detector always returned the word with the lowest meaning contribution score, they only evaluated their model on sentences known to contain an unnecessary word; without appropriate negative examples, it is not clear how to apply their system to sentences with no redundancy. These are two use-case scenarios that the SPC may address. To verify our claim, we will first compare the performances of several word redundancy metrics, including a replication of the metric of Xue and Hwa, on our corpus with their performan"
N18-2036,W14-2104,0,0.0144631,"5). Although pleonastic phrases may serve literary functions (e.g., to add emphasis) (Miller, 1951; Chernov, 1979), most modern writing style guides caution against them in favor of concise writing (Hart et al., 1905; Williams, 2003; Turabian, 2013; Gowers, 2014; Strunk, 1920). An automatic pleonasm detector would be beneficial for natural language processing (NLP) applications that support student writing, such as grammar error correction (GEC) (Han et al., 2006; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011), automatic essay grading (Larkey, 1998; Landauer, 2003; Ong et al., 2014), and intelligent writing tutors (Merrill et al., 1992; Aleven et al., 2009; Atkinson, 2016). Pleonastic phrases may also negatively impact NLP applications in general because they introduce an unnecessary complexity to the language. Their removal might facilitate NLP tasks such as parsing, summarization, and machine translation. However, automated pleonasm detection is a challenging problem, in parts because there is no appropriate resources to support the development of such systems. While 2 Semantic Pleonasm Although pleonasm is generally a semantic and rhetorical concept, it could have dif"
N18-2036,D10-1094,0,0.0198656,"ch that removing them would not significantly alter the meaning of the expression (Merriam-Webster, 1983; Quinn, 1993; Lehmann, 2005). Although pleonastic phrases may serve literary functions (e.g., to add emphasis) (Miller, 1951; Chernov, 1979), most modern writing style guides caution against them in favor of concise writing (Hart et al., 1905; Williams, 2003; Turabian, 2013; Gowers, 2014; Strunk, 1920). An automatic pleonasm detector would be beneficial for natural language processing (NLP) applications that support student writing, such as grammar error correction (GEC) (Han et al., 2006; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011), automatic essay grading (Larkey, 1998; Landauer, 2003; Ong et al., 2014), and intelligent writing tutors (Merrill et al., 1992; Aleven et al., 2009; Atkinson, 2016). Pleonastic phrases may also negatively impact NLP applications in general because they introduce an unnecessary complexity to the language. Their removal might facilitate NLP tasks such as parsing, summarization, and machine translation. However, automated pleonasm detection is a challenging problem, in parts because there is no appropriate resources to support the development of"
N18-2036,P10-2065,0,0.0377471,"not significantly alter the meaning of the expression (Merriam-Webster, 1983; Quinn, 1993; Lehmann, 2005). Although pleonastic phrases may serve literary functions (e.g., to add emphasis) (Miller, 1951; Chernov, 1979), most modern writing style guides caution against them in favor of concise writing (Hart et al., 1905; Williams, 2003; Turabian, 2013; Gowers, 2014; Strunk, 1920). An automatic pleonasm detector would be beneficial for natural language processing (NLP) applications that support student writing, such as grammar error correction (GEC) (Han et al., 2006; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011), automatic essay grading (Larkey, 1998; Landauer, 2003; Ong et al., 2014), and intelligent writing tutors (Merrill et al., 1992; Aleven et al., 2009; Atkinson, 2016). Pleonastic phrases may also negatively impact NLP applications in general because they introduce an unnecessary complexity to the language. Their removal might facilitate NLP tasks such as parsing, summarization, and machine translation. However, automated pleonasm detection is a challenging problem, in parts because there is no appropriate resources to support the development of such systems. While 2 Se"
N18-2036,W11-2123,0,0.00833606,"indeed contains a pleonasm. A corpus of naturally occurring text is unsuitable for training the classifier because the distribution is heavily skewed toward the no redundancy case. Random down-sampling is also not ideal because some might be too obvious (e.g., very short sentences). SPC addresses this problem by filtering for challenging negative cases: sentences that contain a pair of words that are heuristically deemed to be semantically related, but are not judged to 3 In our re-implementation, the language model is trained on a portion of English Gigaword (Graff et al., 2003) using KenLM (Heafield, 2011); the word alignments are derived from Bing’s English-French Translator 4 using sense2vec word-embeddings (Trask et al., 2015) 5 https://languagetool.org/ 228 Feature Description UG the one-hot representation (Harris and Harris, 2010) of unigrams of the sentence TG the one-hot representation of trigrams of the sentence TFIDF the one-hot representation of smoothed TFIDF tuples of trigrams of the sentence WSTAT [max(ALL), avg(ALL), min(ALL), Len(s), LM(s)] the three features that directly encode the words of the sentence are more relevant (U G, T G, T F IDF ) than the group of statistics over th"
N18-2036,E14-1072,1,0.850759,"onastic construction. Table 1 shows that annotators are more likely to agree whether a sentence contains a pleonasm than exactly which words should be considered redundant. In many cases, a majority consensus is achieved with one annotator disagreeing with the others. The result suggests that when there is a single word redundancy, removing either of the synonyms could be appropriate. Given our design choices, the current SPC is not a large corpus; we posit that it can nonetheless serve as a valuable resource for developing systems to detect semantic pleonasm. For example, the earlier work of Xue and Hwa (2014) might have benefited from this resource. They wanted to detect the word in a sentence that contributes the least to the meaning of the sentence; however, their experiments were hampered by a mismatch between their intended domain and the corpus they evaluated on – while their model estimated a word’s semantic redundancy, their experiments were performed on NUCLE (Dahlmeier et al., 2013), a learner corpus that focused more on grammatical errors. Moreover, since their detector always returned the word with the lowest meaning contribution score, they only evaluated their model on sentences known"
P02-1050,han-etal-2000-handling,0,\N,Missing
P02-1050,P97-1003,0,\N,Missing
P02-1050,J93-2003,0,\N,Missing
P02-1050,W01-1406,0,\N,Missing
P02-1050,2001.mtsummit-ebmt.4,0,\N,Missing
P02-1050,W01-1403,0,\N,Missing
P02-1050,H01-1014,0,\N,Missing
P02-1050,W00-1306,1,\N,Missing
P02-1050,N01-1026,0,\N,Missing
P02-1050,J90-2002,0,\N,Missing
P02-1050,P98-1035,0,\N,Missing
P02-1050,C98-1035,0,\N,Missing
P02-1050,P01-1067,0,\N,Missing
P02-1050,J94-4004,0,\N,Missing
P02-1050,N01-1023,0,\N,Missing
P02-1050,P01-1017,0,\N,Missing
P04-3028,N04-1026,1,0.759015,"only 6 labeled examples in the training set. The Cotraining system added examples from the 140 “pseudo-labeled” examples1 in the Prediction Set. The size of the training set increased in each iteration by adding the 2 best examples (those with the highest confidence scores) labeled by the two learners. The Emotional learner and the NonEmotional learner were set to work with the set of features selected by the wrapper approach to optimize the precision (PPV and NPV) as described in section 4.1. We have applied Weka’s (Witten and Frank, 2000) AdaBoost’s version of j48 decision trees (as used in Forbes-Riley and Litman, 2004) to the 140 unseen examples of the test set for generating the learning curve shown in figure 4. Figure 4 illustrates the learning curve of the accuracy on the test set, taking the union of the set of features selected to label the examples. We used the 3 best features for PPV for the Emotional Learner and the best feature for NPV for the NonEmotional Learner (see Section 4.1). The x-axis shows the number of training examples added; the y-axis shows the accuracy of the classifier on test instances. We compare the learning curve from Co-training with a baseline of majority class and an upper-bo"
P04-3028,W04-2326,1,0.690256,"Missing"
P04-3028,N04-3002,1,0.873332,"Missing"
P05-3015,P97-1003,0,0.0821117,"Missing"
P05-3015,W03-0419,0,0.013552,"Missing"
P05-3018,J93-2004,0,0.0248294,"Missing"
P05-3018,P00-1056,0,0.0603248,"Missing"
P05-3018,W04-3207,0,0.0153229,"for the annotators and a visualization tool for the researchers to examine the data. The data-collection interface asks the users to make lexical and phrasal mappings (word alignments) between the two languages. Some studies suggest that supervised word aligned data may improve machine translation performance (Callison-Burch et al., 2004). The interface can also be configured to ask the annotators to correct projected annotated resources. The idea of projecting English annotation resources across word alignments has been explored in several studies (Yarowsky and Ngai, 2001; Hwa et al., 2005; Smith and Smith, 2004). Currently, our annotation interface is configured for correcting projected POS tagging for Chinese. The visualization tool aggregates the annotators’ work, takes various statistics, and visually displays the aggregate information. Our goal is to aid the researchers conducting the experiment to identify noise in the annotations as well as problematic constructs for which the guidelines should provide further clarifications. ∗ This work has been supported, in part, by CRAW Distributed Mentor Program. We thank Karina Ivanetich, David Chiang, and the NLP group at Pitt for helpful feedbacks on th"
P05-3018,xia-etal-2000-developing,0,0.10877,"Missing"
P05-3018,N01-1026,0,0.0134995,"of two pieces of software: a user interface for the annotators and a visualization tool for the researchers to examine the data. The data-collection interface asks the users to make lexical and phrasal mappings (word alignments) between the two languages. Some studies suggest that supervised word aligned data may improve machine translation performance (Callison-Burch et al., 2004). The interface can also be configured to ask the annotators to correct projected annotated resources. The idea of projecting English annotation resources across word alignments has been explored in several studies (Yarowsky and Ngai, 2001; Hwa et al., 2005; Smith and Smith, 2004). Currently, our annotation interface is configured for correcting projected POS tagging for Chinese. The visualization tool aggregates the annotators’ work, takes various statistics, and visually displays the aggregate information. Our goal is to aid the researchers conducting the experiment to identify noise in the annotations as well as problematic constructs for which the guidelines should provide further clarifications. ∗ This work has been supported, in part, by CRAW Distributed Mentor Program. We thank Karina Ivanetich, David Chiang, and the NLP"
P05-3018,E03-1086,0,\N,Missing
P05-3018,P04-1023,0,\N,Missing
P07-1038,P07-1111,1,0.596987,"3; Quirk, 2004). The main distinction is that confidence estimation is typically performed with a particular system in mind, and may rely on systeminternal information in estimation. In this study, we draw on only system-independent indicators so that the resulting metric may be more generally applied. This allows us to have a clearer picture of the contributing factors as they interact with different types of MT systems. Also relevant is previous work that applied machine learning approaches to MT evaluation, both with human references (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004; Albrecht and Hwa, 2007; Liu and Gildea, 2007) and without (Gamon et al., 2005). One motivation for the learning approach is the ease of combining multiple criteria. Literature in translation evaluation reports a myriad of criteria that people use in their judgments, but it is not clear how these factors should be combined mathematically. Machine learning offers a principled and unified framework to induce a computational model of human’s decision process. Disparate indicators can be encoded as one or more input features, and the learning algorithm tries to find a mapping from input features to a score that quantifi"
P07-1038,W05-0909,0,0.0324625,"ce-based metrics. The well-known BLEU (Papineni et al., 2002) is based on the number of common n-grams between the translation hypothesis and human reference translations of the same sentence. Metrics such as ROUGE, Head Word Chain (HWC), METEOR, and other recently proposed methods all offer different ways of comparing machine and human translations. ROUGE utilizes ’skip n-grams’, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately (Banerjee and Lavie, 2005). The HWC metrics compare dependency and constituency trees for both reference and machine translations (Liu and Gildea, 2005). 3 MT Evaluation with Pseudo References using Regression Reference-based metrics are typically thought of as measurements of “similarity to good translations” because human translations are used as references, but in more general terms, they are distance measurements between two sentences. The distance between a translation hypothesis and an imperfect reference is still somewhat informative. As a toy example, consider a one-dimensional line segment. A distance from the"
P07-1038,P01-1020,0,0.237362,"n on MT outputs (Blatz et al., 2003; Ueffing et al., 2003; Quirk, 2004). The main distinction is that confidence estimation is typically performed with a particular system in mind, and may rely on systeminternal information in estimation. In this study, we draw on only system-independent indicators so that the resulting metric may be more generally applied. This allows us to have a clearer picture of the contributing factors as they interact with different types of MT systems. Also relevant is previous work that applied machine learning approaches to MT evaluation, both with human references (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004; Albrecht and Hwa, 2007; Liu and Gildea, 2007) and without (Gamon et al., 2005). One motivation for the learning approach is the ease of combining multiple criteria. Literature in translation evaluation reports a myriad of criteria that people use in their judgments, but it is not clear how these factors should be combined mathematically. Machine learning offers a principled and unified framework to induce a computational model of human’s decision process. Disparate indicators can be encoded as one or more input features, and the learning algorithm tries to find a m"
P07-1038,2005.eamt-1.15,0,0.506737,"an judgments. Reference-based metrics such as BLEU (Papineni et al., 2002) have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source? This approach requires the participation of human translators, who provide the “gold standard” reference sentences. However, keeping humans in the evaluation loop represents a significant expenditure both in terms of time and resources; therefore it is worthwhile to explore ways of reducing the degree of human involvement. To this end, Gamon et al. (2005) proposed a learning-based evaluation metric that does not com296 pare against reference translations. Under a learning framework, the input (i.e., the sentence to be evaluated) is represented as a set of features. These are measurements that can be extracted from the input sentence (and may be individual metrics themselves). The learning algorithm combines the features to form a model (a composite evaluation metric) that produces the final score for the input. Without human references, the features in the model proposed by Gamon et al. were primarily language model features and linguistic ind"
P07-1038,W04-3250,0,0.0886101,"Missing"
P07-1038,2004.tmi-1.8,0,0.398528,", 2003; Ueffing et al., 2003; Quirk, 2004). The main distinction is that confidence estimation is typically performed with a particular system in mind, and may rely on systeminternal information in estimation. In this study, we draw on only system-independent indicators so that the resulting metric may be more generally applied. This allows us to have a clearer picture of the contributing factors as they interact with different types of MT systems. Also relevant is previous work that applied machine learning approaches to MT evaluation, both with human references (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004; Albrecht and Hwa, 2007; Liu and Gildea, 2007) and without (Gamon et al., 2005). One motivation for the learning approach is the ease of combining multiple criteria. Literature in translation evaluation reports a myriad of criteria that people use in their judgments, but it is not clear how these factors should be combined mathematically. Machine learning offers a principled and unified framework to induce a computational model of human’s decision process. Disparate indicators can be encoded as one or more input features, and the learning algorithm tries to find a mapping from input features"
P07-1038,P04-1077,0,0.0141764,"ach pseudo reference system. While our model does not use human references directly, its features are adapted from the following distance-based metrics. The well-known BLEU (Papineni et al., 2002) is based on the number of common n-grams between the translation hypothesis and human reference translations of the same sentence. Metrics such as ROUGE, Head Word Chain (HWC), METEOR, and other recently proposed methods all offer different ways of comparing machine and human translations. ROUGE utilizes ’skip n-grams’, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately (Banerjee and Lavie, 2005). The HWC metrics compare dependency and constituency trees for both reference and machine translations (Liu and Gildea, 2005). 3 MT Evaluation with Pseudo References using Regression Reference-based metrics are typically thought of as measurements of “similarity to good translations” because human translations are used as references, but in more general terms, they are distance measurements between two sentences. The distance between a translation hypo"
P07-1038,C04-1072,0,0.0594924,"ach pseudo reference system. While our model does not use human references directly, its features are adapted from the following distance-based metrics. The well-known BLEU (Papineni et al., 2002) is based on the number of common n-grams between the translation hypothesis and human reference translations of the same sentence. Metrics such as ROUGE, Head Word Chain (HWC), METEOR, and other recently proposed methods all offer different ways of comparing machine and human translations. ROUGE utilizes ’skip n-grams’, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately (Banerjee and Lavie, 2005). The HWC metrics compare dependency and constituency trees for both reference and machine translations (Liu and Gildea, 2005). 3 MT Evaluation with Pseudo References using Regression Reference-based metrics are typically thought of as measurements of “similarity to good translations” because human translations are used as references, but in more general terms, they are distance measurements between two sentences. The distance between a translation hypo"
P07-1038,W05-0904,0,0.563534,"othesis and human reference translations of the same sentence. Metrics such as ROUGE, Head Word Chain (HWC), METEOR, and other recently proposed methods all offer different ways of comparing machine and human translations. ROUGE utilizes ’skip n-grams’, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately (Banerjee and Lavie, 2005). The HWC metrics compare dependency and constituency trees for both reference and machine translations (Liu and Gildea, 2005). 3 MT Evaluation with Pseudo References using Regression Reference-based metrics are typically thought of as measurements of “similarity to good translations” because human translations are used as references, but in more general terms, they are distance measurements between two sentences. The distance between a translation hypothesis and an imperfect reference is still somewhat informative. As a toy example, consider a one-dimensional line segment. A distance from the end-point uniquely determines the position of a point. When the reference location is anywhere else on the line segment, a re"
P07-1038,N07-1006,0,0.15123,"n distinction is that confidence estimation is typically performed with a particular system in mind, and may rely on systeminternal information in estimation. In this study, we draw on only system-independent indicators so that the resulting metric may be more generally applied. This allows us to have a clearer picture of the contributing factors as they interact with different types of MT systems. Also relevant is previous work that applied machine learning approaches to MT evaluation, both with human references (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004; Albrecht and Hwa, 2007; Liu and Gildea, 2007) and without (Gamon et al., 2005). One motivation for the learning approach is the ease of combining multiple criteria. Literature in translation evaluation reports a myriad of criteria that people use in their judgments, but it is not clear how these factors should be combined mathematically. Machine learning offers a principled and unified framework to induce a computational model of human’s decision process. Disparate indicators can be encoded as one or more input features, and the learning algorithm tries to find a mapping from input features to a score that quantifies the input’s quality"
P07-1038,P02-1040,0,0.106164,"developing sentence-level MT evaluation metrics that do not directly rely on human reference translations. Our metrics are developed using regression learning and are based on a set of weaker indicators of fluency and adequacy (pseudo references). Experimental results suggest that they rival standard reference-based metrics in terms of correlations with human judgments on new test instances. 1 Introduction Automatic assessment of translation quality is a challenging problem because the evaluation task, at its core, is based on subjective human judgments. Reference-based metrics such as BLEU (Papineni et al., 2002) have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source? This approach requires the participation of human translators, who provide the “gold standard” reference sentences. However, keeping humans in the evaluation loop represents a significant expenditure both in terms of time and resources; therefore it is worthwhile to explore ways of reducing the degree of human involvement. To this end, Gamon et al. (2005) proposed a learning-based evaluation metric that does"
P07-1038,quirk-2004-training,0,0.0342482,"MT research and development, where new test set with multiple references are also hard to come by. One could work with previous datasets from events such as the NIST MT Evals, but there is a danger of over-fitting. One also could extract a single reference from parallel corpora, although it is known that automatic metrics are more reliable when comparing against multiple references. The aim of this work is to develop a trainable automatic metric for evaluation without human references. This can be seen as a form of confidence estimation on MT outputs (Blatz et al., 2003; Ueffing et al., 2003; Quirk, 2004). The main distinction is that confidence estimation is typically performed with a particular system in mind, and may rely on systeminternal information in estimation. In this study, we draw on only system-independent indicators so that the resulting metric may be more generally applied. This allows us to have a clearer picture of the contributing factors as they interact with different types of MT systems. Also relevant is previous work that applied machine learning approaches to MT evaluation, both with human references (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004; Albrecht and Hw"
P07-1038,2003.mtsummit-papers.52,0,0.00969065,"ance is in day-to-day MT research and development, where new test set with multiple references are also hard to come by. One could work with previous datasets from events such as the NIST MT Evals, but there is a danger of over-fitting. One also could extract a single reference from parallel corpora, although it is known that automatic metrics are more reliable when comparing against multiple references. The aim of this work is to develop a trainable automatic metric for evaluation without human references. This can be seen as a form of confidence estimation on MT outputs (Blatz et al., 2003; Ueffing et al., 2003; Quirk, 2004). The main distinction is that confidence estimation is typically performed with a particular system in mind, and may rely on systeminternal information in estimation. In this study, we draw on only system-independent indicators so that the resulting metric may be more generally applied. This allows us to have a clearer picture of the contributing factors as they interact with different types of MT systems. Also relevant is previous work that applied machine learning approaches to MT evaluation, both with human references (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004; A"
P07-1038,H05-1093,0,\N,Missing
P07-1038,C04-1046,0,\N,Missing
P07-1111,P06-2003,0,0.251055,"Missing"
P07-1111,W05-0909,0,0.0451672,"d. First, similarity can be expressed in terms of string edit distances. In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed (Tillmann et al., 1997; Snover et al., 2006; Leusch et al., 2006). Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to look for similarities at a deeper structural level. For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. With this wide array of metrics to choose from, 881 MT developers need a way to evaluate them. One possibility is to examine whether the automatic metric ranks the human reference tran"
P07-1111,E06-1032,0,0.0263158,"en doubly approximated (class labels instead of ordinals, humanlikeness instead of human-acceptability) does negatively impact the performance of the derived metrics. In particular, we showed that they do not generalize as well to new data as metrics trained from direct regression. We see two lingering potential objections toward developing metrics with regression-learning. One is the concern that a system under evaluation might try to explicitly “game the metric5 .” This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al., 2006). In a learning framework, potential pitfalls for individual metrics are ameliorated through a combination of evidences. That said, it is still prudent to defend against the potential of a system gaming a subset of the features. For example, our fluency-predictor features are not strong indicators of translation qualities by themselves. We want to avoid training a metric that as5 Or, in a less adversarial setting, a system may be performing minimum error-rate training (Och, 2003) 886 signs a higher than deserving score to a sentence that just happens to have many n-gram matches against the tar"
P07-1111,P01-1020,0,0.80778,"aluation metrics have been proposed to approximate human judgments of MT output quality. Although studies have shown them to correlate with human judgments at the document level, they are not sensitive enough to provide reliable evaluations at the sentence level (Blatz et al., 2003). This suggests that current metrics do not fully reflect the set of criteria that people use in judging sentential translation quality. 880 A recent direction in the development of metrics for sentence-level evaluation is to apply machine learning to create an improved composite metric out of less indicative ones (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004). Under the assumption that good machine translation will produce “human-like” sentences, classifiers are trained to predict whether a sentence is authored by a human or by a machine based on features of that sentence, which may be the sentence’s scores from individual automatic evaluation metrics. The confidence of the classifier’s prediction can then be interpreted as a judgment on the translation quality of the sentence. Thus, the composite metric is encoded in the confidence scores of the classification labels. While the learning approach to metric design offers"
P07-1111,N06-1058,0,0.0128603,"y is itself underspecified, several different families of metrics have been developed. First, similarity can be expressed in terms of string edit distances. In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed (Tillmann et al., 1997; Snover et al., 2006; Leusch et al., 2006). Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to look for similarities at a deeper structural level. For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. With this wide array of metrics to choose from, 881 MT developers need a way to evaluate them. One pos"
P07-1111,W04-3250,0,0.192376,"Missing"
P07-1111,2004.tmi-1.8,0,0.726518,"oposed to approximate human judgments of MT output quality. Although studies have shown them to correlate with human judgments at the document level, they are not sensitive enough to provide reliable evaluations at the sentence level (Blatz et al., 2003). This suggests that current metrics do not fully reflect the set of criteria that people use in judging sentential translation quality. 880 A recent direction in the development of metrics for sentence-level evaluation is to apply machine learning to create an improved composite metric out of less indicative ones (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004). Under the assumption that good machine translation will produce “human-like” sentences, classifiers are trained to predict whether a sentence is authored by a human or by a machine based on features of that sentence, which may be the sentence’s scores from individual automatic evaluation metrics. The confidence of the classifier’s prediction can then be interpreted as a judgment on the translation quality of the sentence. Thus, the composite metric is encoded in the confidence scores of the classification labels. While the learning approach to metric design offers the promise of ease of comb"
P07-1111,E06-1031,0,0.0156222,"matic evaluation metrics. 2 MT Evaluation Recent automatic evaluation metrics typically frame the evaluation problem as a comparison task: how similar is the machine-produced output to a set of human-produced reference translations for the same source text? However, as the notion of similarity is itself underspecified, several different families of metrics have been developed. First, similarity can be expressed in terms of string edit distances. In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed (Tillmann et al., 1997; Snover et al., 2006; Leusch et al., 2006). Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to look for similarities"
P07-1111,P04-1077,0,0.0261391,"e notion of similarity is itself underspecified, several different families of metrics have been developed. First, similarity can be expressed in terms of string edit distances. In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed (Tillmann et al., 1997; Snover et al., 2006; Leusch et al., 2006). Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to look for similarities at a deeper structural level. For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. With this wide array of metrics to choose from, 881 MT developers need"
P07-1111,C04-1072,0,0.0319151,"e notion of similarity is itself underspecified, several different families of metrics have been developed. First, similarity can be expressed in terms of string edit distances. In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed (Tillmann et al., 1997; Snover et al., 2006; Leusch et al., 2006). Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to look for similarities at a deeper structural level. For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. With this wide array of metrics to choose from, 881 MT developers need"
P07-1111,W05-0904,0,0.387307,"ms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to look for similarities at a deeper structural level. For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. With this wide array of metrics to choose from, 881 MT developers need a way to evaluate them. One possibility is to examine whether the automatic metric ranks the human reference translations highly with respect to machine translations (Lin and Och, 2004b; Amig´o et al., 2006). The reliability of a metric can also be more directly assessed by determining how well it correlates with human judgments of the same data. For instance, as a part of the recent NIST spon"
P07-1111,P06-2070,0,0.0427737,"more sophisticated modifications have been proposed (Tillmann et al., 1997; Snover et al., 2006; Leusch et al., 2006). Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to look for similarities at a deeper structural level. For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. With this wide array of metrics to choose from, 881 MT developers need a way to evaluate them. One possibility is to examine whether the automatic metric ranks the human reference translations highly with respect to machine translations (Lin and Och, 2004b; Amig´o et al., 2006). The reliability of a metr"
P07-1111,N03-2021,0,0.0260967,"ed in terms of string edit distances. In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed (Tillmann et al., 1997; Snover et al., 2006; Leusch et al., 2006). Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to look for similarities at a deeper structural level. For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. With this wide array of metrics to choose from, 881 MT developers need a way to evaluate them. One possibility is to examine whether the automatic metric ranks the human reference translations highly with respect to"
P07-1111,P03-1021,0,0.0156455,"l automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al., 2006). In a learning framework, potential pitfalls for individual metrics are ameliorated through a combination of evidences. That said, it is still prudent to defend against the potential of a system gaming a subset of the features. For example, our fluency-predictor features are not strong indicators of translation qualities by themselves. We want to avoid training a metric that as5 Or, in a less adversarial setting, a system may be performing minimum error-rate training (Och, 2003) 886 signs a higher than deserving score to a sentence that just happens to have many n-gram matches against the target-language reference corpus. This can be achieved by supplementing the current set of human assessed training examples with automatically assessed training examples, similar to the labeling process used in the Human-Likeness classification framework. For instance, as negative training examples, we can incorporate fluent sentences that are not adequate translations and assign them low overall assessment scores. A second, related concern is that because the metric is trained on e"
P07-1111,P02-1040,0,0.100219,"parison task: how similar is the machine-produced output to a set of human-produced reference translations for the same source text? However, as the notion of similarity is itself underspecified, several different families of metrics have been developed. First, similarity can be expressed in terms of string edit distances. In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed (Tillmann et al., 1997; Snover et al., 2006; Leusch et al., 2006). Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to look for similarities at a deeper structural level. For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse tr"
P07-1111,quirk-2004-training,0,0.0333276,"roach to metric design offers the promise of ease of combining multiple metrics and the potential for improved performance, several salient questions should be addressed more fully. First, is learning a “Human Likeness” classifier the most suitable approach for framing the MTevaluation question? An alternative is regression, in which the composite metric is explicitly learned as a function that approximates humans’ quantitative judgments, based on a set of human evaluated training sentences. Although regression has been considered on a small scale for a single system as confidence estimation (Quirk, 2004), this approach has not been studied as extensively due to scalability and generalization concerns. Second, how does the diversity of the model features impact the learned metric? Third, how well do learning-based metrics generalize beyond their training examples? In particular, how well can a metric that was developed based Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 880–887, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics on one group of MT systems evaluate the translation qualities of new systems? In this"
P07-1111,2006.amta-papers.25,0,0.0558777,"er than standard automatic evaluation metrics. 2 MT Evaluation Recent automatic evaluation metrics typically frame the evaluation problem as a comparison task: how similar is the machine-produced output to a set of human-produced reference translations for the same source text? However, as the notion of similarity is itself underspecified, several different families of metrics have been developed. First, similarity can be expressed in terms of string edit distances. In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed (Tillmann et al., 1997; Snover et al., 2006; Leusch et al., 2006). Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU (Papineni et al., 2002) the basic n-gram precision idea has been augmented in a number of ways. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. Finally, researchers have begun to"
P07-1111,C04-1046,0,\N,Missing
P07-1111,D08-1076,0,\N,Missing
P14-2098,W10-1004,0,0.10158,"revisions (Tajiri et al., 2012). However, existing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard. We instead use error annotated data, in which the corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). These corpora all provide experts’ corrections along with error 2 UIUC corpus contains annotations of essays collected from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003). 601 Type A name gap-between-edits tense-change word-order-error same-word-set revised-to editdistance=1 B not-in-dict word-choice preposition-error description Gap between the two edits. In particular, we use the number of words between the two edits’ original words, as well as the revised words. Note that Swanson and Yamangil’s approach is a special case that only conside"
P14-2098,J08-4005,0,0.025424,"Missing"
P14-2098,N12-1067,0,0.131382,"Missing"
P14-2098,N12-1037,0,0.151404,"writing mistakes within sentences, and point out (1) why each case is considered a mistake, and (2) how each mistake should be corrected. Because this is time consuming, tutors often just rewrite the sentences without giving any explanations (Fregeau, 1999). Due to the effort involved in comparing revisions with the original texts, students often fail to learn from these revisions (Williams, 2003). Computer aided language learning tools offer a solution for providing more detailed feedback. Programs can be developed to compare the student’s original sentences with the tutor-revised sentences. Swanson and Yamangil (2012) have proposed a promising framework for this purpose. Their approach has two components: one to detect individual corrections within a revision, which they termed correction detection; another to determine what the correction fixes, which they termed error type selection. Although they reported a high accuracy for the error type selection classifier alone, the bottleneck of their system is the other 2 Correction Detection Comparing a student-written sentence with its revision, we observe that each correction can be decomposed into a set of more basic edits such as word insertions, word deleti"
P14-2098,W13-1703,0,0.0601551,"collection of student essays and their revisions (Tajiri et al., 2012). However, existing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard. We instead use error annotated data, in which the corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). These corpora all provide experts’ corrections along with error 2 UIUC corpus contains annotations of essays collected from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003). 601 Type A name gap-between-edits tense-change word-order-error same-word-set revised-to editdistance=1 B not-in-dict word-choice preposition-error description Gap between the two edits. In particular, we use the number of words between the two edits’ original words, as well as the revised words. Note that Swanson and Yamangil’s ap"
P14-2098,W11-2838,0,0.0423047,"ing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard. We instead use error annotated data, in which the corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). These corpora all provide experts’ corrections along with error 2 UIUC corpus contains annotations of essays collected from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003). 601 Type A name gap-between-edits tense-change word-order-error same-word-set revised-to editdistance=1 B not-in-dict word-choice preposition-error description Gap between the two edits. In particular, we use the number of words between the two edits’ original words, as well as the revised words. Note that Swanson and Yamangil’s approach is a special case that only considers if the basic-edits have zero gap in both sen"
P14-2098,P12-2039,0,0.0228728,"ts. We use features in Table 1 in the proposed classifier. We design the features to indicate: (A) whether merging the two basic-edits matches the pattern for a common correction. (B) whether one basic-edit addresses one single error. We train the classifier using samples extracted from revisions where individual corrections are explicitly annotated. We first extract the basic4 Experimental Setup We combine Levenshtein algorithm with different merging algorithms for correction detection. 4.1 Dataset An ideal data resource would be a real-world collection of student essays and their revisions (Tajiri et al., 2012). However, existing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard. We instead use error annotated data, in which the corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and"
P14-2098,P11-1019,0,0.0740844,"ideal data resource would be a real-world collection of student essays and their revisions (Tajiri et al., 2012). However, existing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard. We instead use error annotated data, in which the corrections were provided by human experts. We simulate the revisions by applying corrections onto the original sentence. The teachers’ annotations are treated as gold standard for the detailed corrections. We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), UIUC corpus2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). These corpora all provide experts’ corrections along with error 2 UIUC corpus contains annotations of essays collected from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003). 601 Type A name gap-between-edits tense-change word-order-error same-word-set revised-to editdistance=1 B not-in-dict word-choice preposition-error description Gap between the two edits. In particular, we use the number of words between the two edits’ original words, as well as the revised wor"
P14-2098,N10-1145,0,0.0501374,"hrase extraction (Koehn et al., 2003) and paraphrase extraction (Cohn et al., 2008). They are fundamentally different in that the granularity of the extracted phrase pairs is a major concern in our work – we need to guarantee each detected phrase pair to address exactly one writing problem. In comparison, phrase extraction systems aim to improve the end-to-end MT or paraphrasing systems. A bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases. Recent work therefore focuses on identifying the alignment/edits between two sentences (Snover et al., 2009; Heilman and Smith, 2010). 3 (a) The pattern indicates that (b) The pattern indicates that the two edits address the the two edits do not address same problem the same problem Figure 5: Patterns indicating whether two edits address the same writing mistake. Figure 6: Extracting training instances for the merger. Our goal is to train classifiers to tell if two basic edits should be merged (True or False). We break each correction (outer polygons, also colored in red) in the training corpus into a set of basic edits (black polygons). We construct an instance for each consecutive pair of basic edits. If two basic edits w"
P14-2098,N03-1017,0,0.0119653,"Missing"
P17-1144,E12-1036,0,0.668116,"involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the purpose of each revision is manually coded using a revision schema closely related to argument mining/discourse analysis. Within the domain of argumentative essays, the corp"
P17-1144,W99-0411,0,0.224684,"ve essays. Drafts are manually aligned at the sentence level, and the writer’s purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction. 1 Introduction Most writing-related natural language processing (NLP) research focuses on the analysis of single drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writi"
P17-1144,P01-1014,0,0.115176,"d works differently for first revision attempts and second revision attempts. H6. The revision classification model trained on L2 essays has a different preference from the model trained on native essays. 4.2.2 Methodology We followed the work of (Zhang and Litman, 2015), where unigram features (words) were used as the baseline and the SVM classifier was used. Besides unigrams, three groups of features used in revision analysis, argument mining and discourse analysis research were extracted (Location, Textual and Language) as in Table 6 (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Burstein et al., 2001; Falakmasir et al., 2014). For H4, 10-fold (participant) cross-validation is conducted on all the essays in the corpus. Unweighted average F-score for each revision category is reported, using unigram features versus using all features. Zhang and Litman (2015) observed a significant improvement over the unigram baseline using all the features. If H4 is true, we should expect a similar improvement over the unigram baseline using our corpus. For H5, 10-fold cross-validation was conducted for the revisions from Draft1 to Draft2 and revisions from Draft2 to Draft3 separately. We compared the impr"
P17-1144,P16-2051,0,0.0671933,"Missing"
P17-1144,D11-1009,0,0.016758,"guage processing (NLP) research focuses on the analysis of single drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipe"
P17-1144,W13-1703,0,0.029161,"Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the pur"
P17-1144,C12-1044,0,0.118232,"the entire sentence pair will be annotated as one sentence revision. Second, we need to define the quality we want to observe about the revision sentence pair. For this first corpus, we focus on recognizing the purpose of the revision, as in the example above. It is a useful property, and it has previously been studied by others in the literature. People have considered both binary purpose categories such as Content vs. Surface (Faigley and Witte, 1981) or Factual vs. Fluency (Bronner and Monz, 2012) as well as more fine-grained categories (Pfeil et al., 2006; Jones, 2008; Liu and Ram, 2009; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015). Our corpus follows the two-tiered schema used by (Zhang and Litman, 2015) (see Section 3.2). Third, we not only have to decide on the annotation format, we also need to decide how to obtain Draft1 Draft2 Draft3 Write Draft1 @home Revise Draft1 @home Revise Draft2 @lab Annotated Revisions I (Rev12) Annotated Revisions II (Rev23) Figure 1: Our collected corpus contains five components: three drafts of an essay and two annotated revisions between drafts. the raw text: argumentative essays with multiple drafts. We decided to sample from a population of predominantly coll"
P17-1144,N16-1164,0,0.027203,"ed with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction. 1 Introduction Most writing-related natural language processing (NLP) research focuses on the analysis of single drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison"
P17-1144,D13-1055,0,0.522909,"s topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the purpose of each revision is manually coded using a revision schema closely related to argument mining/discourse analysis. Within the domain of argum"
P17-1144,I05-5002,0,0.0748553,"and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employ"
P17-1144,C08-2023,0,0.0254765,"The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the purpose of each revision is manually coded using a revision schema closely related to argument mining/discourse analysis. Within the domain of argumentative essays, the corpus will be useful for supporting research in argumentative revision analysis and the application of argument mining techniques. The corpus may also be useful for research on paraphrase comparisons, grammar error correction, and computational stylistics (Popescu and Dinu, 2008; Flekova et al., 2016). In this paper, we present two example uses of our corpus: 1) rewriting behavior data analysis, and 2) automatic revision purpose classification. 2 Corpus Design Decisions Consider this scenario: Alice begins her social science argumentative essay with the sentence “Electronic communication allows people to make connections beyond physical limits.” An analytical system might (rightly) identify the sentence as the thesis of her essay, and an evaluative system might give the essay a low score due to this sentence’s vagueness and a later lack of evidence (though Alice may"
P17-1144,N12-1037,0,0.0202213,"drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds"
P17-1144,P11-1019,0,0.0516712,"ysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of Wikipedia revisions do not correspond well with other kinds of texts. This work presents the ArgRewrite corpus1 to facilitate revision analysis research for argumentative essays. The corpus consists of a collection of three drafts of essays written by university students and employees; the drafts are manually aligned at the sentence level, then the purpose of each revision is manu"
P17-1144,W14-1818,1,0.833765,"are shown a computer interface that highlights the differences between their first and second drafts. They are asked to revise and create a third draft to improve the general quality of their essay. We experimented with two variations of revision elicitation. Chosen at random, half of the participants (10 L2 participants and 20 Native participants) are shown Interface A, the interface based on the ArgRewrite system (Zhang et al., 2016), which highlights the annotated differences between the drafts (Figure 2(a)); half of the participants are shown In5 Sentences are first automatically aligned (Zhang and Litman, 2014), then manually corrected by human. 1570 Draft1 This world has no restriction on who one can talk to. Revision Draft2 Purpose Conventions/ This world has no restrictions on Grammar/ whom one can talk to. Spelling Revision Purpose This world has no restrictions on whom one can talk to. Rebuttal/ Reservation The only aspects of communication that this new development improves are internet navigation and faux internet relatability. WordUsage/ Clarity Claims/ Ideas The only aspects of digital communication that this new development improves are internet navigation and faux internet relatability. B"
P17-1144,W15-0616,1,0.429119,"tion Most writing-related natural language processing (NLP) research focuses on the analysis of single drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), and fine-grained error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of changes between drafts – a comparison of revisions and the properties of the differences. Research on this topic can support applications involing revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014). Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision analysis, the most relevant work analyzes Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wiki"
P17-1144,P14-2066,0,\N,Missing
P17-1144,P14-2098,1,\N,Missing
P17-1144,N16-3008,1,\N,Missing
P98-1091,P98-1091,1,0.0528615,"tains all possible rules in Chomsky Normal Form constructed by the nonterminal and terminal symbols. The initial parameters associated with each rule are randomly generated subject to an admissibility constraint. As long as all the rules have a non-zero probability, any string has a non-zero chance of being generated. To train the grammar, we follow the InsideOutside re-estimation algorithm described by Lari and Young (1990). The Inside-Outside reestimation algorithm can also be extended to train PLTIGs. The equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998). As with PCFGs, the initial grammar must be able to generate any string. A simple PLTIG that fits the requirement is one that simulates a bigram model. It is represented by a tree set that contains a right auxiliary tree for each lexical item as depicted in Figure 1. Each tree has one adjunction site into which other right auxiliary trees can adjoin. The tree set has only one initial tree, which is anchored by an empty lexical item. The initial tree represents the start of the sentence. Any string can be constructed by right adjoining the words together in order. Training the parameters of th"
P98-1091,P92-1017,0,0.29857,"Missing"
P98-1091,P95-1023,0,0.0959998,"s the auxiliary tree into the target tree at a node labeled with the same nonterminal as the root and foot of the auxiliary tree. By using a tree representation, LTAGs extend the domain of locality of a grammatical primitive, so that they capture both lexical features and hierarchical structure. Moreover, the adjunction operation elegantly models intuitive linguistic concepts such as long distance dependencies be1The best theoretical upper bound on time complexity for the recognition of Tree Adjoining Languages is O(M(n2)), where M(k) is the time needed to multiply two k x k boolean matrices.(Rajasekaran and Yooseph, 1995) 558 Elem~ntwy~ ~ : tl~t t~ptl 1 t~rd 2 Example sentence: The cat twordn chases the mouse Corresponding derivation tree: £ X, ~td I X * word2 X $ tinit . ~ d J . wordn tthe Figure h A set of elementary LTIG trees that represent a bigram grammar. The arrows indicate adjunction sites. that describe the distribution of the likelihood of any left auxiliary tree adjoining into node ~/. (We need one extra parameter for the case of no left adjunction.) A similar set of parameters is constructed for the right adjunction and substitution distributions. 3 Experiments In the following experiments we show"
P98-1091,1993.iwpt-1.20,0,0.638595,"y restricting auxiliary tree structures to be in one of two forms: the left auxiliary tree, whose non-empty frontier nodes are all to the left of the foot node; or the right auxiliary tree, whose non-empty frontier nodes are all to the right of the foot node. Auxiliary trees of different types cannot adjoin into each other if the adjunction would result in a wrapping auxiliary tree. The resulting system is strongly equivalent to CFGs, yet is fully lexicalized and still O(n 3) parsable, as shown by Schabes and Waters (1994). Furthermore, LTIGs can be parameterized to form probabilistic models (Schabes and Waters, 1993). Informally speaking, a parameter is associated with each possible adjunction or substitution operation between a tree and a node. For instance, suppose there are V left auxiliary trees that might adjoin into node r/. Then there are V q- 1 parameters associated with node r/ LTAGs (and LTIGs) are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations. We distinguish two types of trees in the set of elementary trees: the initial trees and the auxiliary trees. Unlike full parse trees but reminiscent of the productions of a context-free grammar, both types of"
P98-1091,C88-2121,0,0.0720846,"xicalized context-free formalism. This method is not practical because altering the structures of the grammar damages the linguistic information stored in the original grammar (Schabes and Waters, 1994). Second, one might propagate lexical information upward through the productions. Examples of formalisms using this approach include the work of Magerman (1995), Charniak (1997), Collins (1997), and Goodman (1997). A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. The Lexicalized Tree-Adjoining Grammar (LTAG) formalism (Schabes et al., 1988), (Schabes, 1990) , although not context-free, is the most well-known instance in this category. PLTIGs belong to this third category and generate only context-free languages. tween words. Unlike the N-gram model, which only offers dependencies between neighboring words, these trees can model the interaction of structurally related words that occur far apart. Like LTAGs, LTIGs are tree-rewriting systems, but they differ from LTAGs in their generative power. LTAGs can generate some strictly context-sensitive languages. They do so by using wrapping auxiliary trees, which allow nonempty frontier"
P98-1091,P97-1003,0,0.0708348,"7 mar. There are three ways in which one might do so. First, one can modify the tree structures so that all context-free productions contain lexical items. Greibach normal form provides a well-known example of such a lexicalized context-free formalism. This method is not practical because altering the structures of the grammar damages the linguistic information stored in the original grammar (Schabes and Waters, 1994). Second, one might propagate lexical information upward through the productions. Examples of formalisms using this approach include the work of Magerman (1995), Charniak (1997), Collins (1997), and Goodman (1997). A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. The Lexicalized Tree-Adjoining Grammar (LTAG) formalism (Schabes et al., 1988), (Schabes, 1990) , although not context-free, is the most well-known instance in this category. PLTIGs belong to this third category and generate only context-free languages. tween words. Unlike the N-gram model, which only offers dependencies between neighboring words, these trees can model the interaction of structurally related words that occur far apart. Like LTAGs,"
P98-1091,1997.iwpt-1.13,0,0.180775,"ee ways in which one might do so. First, one can modify the tree structures so that all context-free productions contain lexical items. Greibach normal form provides a well-known example of such a lexicalized context-free formalism. This method is not practical because altering the structures of the grammar damages the linguistic information stored in the original grammar (Schabes and Waters, 1994). Second, one might propagate lexical information upward through the productions. Examples of formalisms using this approach include the work of Magerman (1995), Charniak (1997), Collins (1997), and Goodman (1997). A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. The Lexicalized Tree-Adjoining Grammar (LTAG) formalism (Schabes et al., 1988), (Schabes, 1990) , although not context-free, is the most well-known instance in this category. PLTIGs belong to this third category and generate only context-free languages. tween words. Unlike the N-gram model, which only offers dependencies between neighboring words, these trees can model the interaction of structurally related words that occur far apart. Like LTAGs, LTIGs are tree-rewri"
P98-1091,J95-4002,0,\N,Missing
P98-1091,P95-1037,0,\N,Missing
P99-1010,H90-1021,0,0.0149434,"Missing"
P99-1010,P98-1091,1,0.854627,"a sentential clause or a complex noun phrase. The example sentence in Table 1 contains 3 HighP constituents: a complex noun phrase made up of a BaseNP and a prepositional phrase; a sentential clause with an omitted subject NP; and the full sentence. 4 Induction Strategies To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992). The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a), which is lexicalized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b)). Second, its lexicalized representation makes the training process more efficient than a traditional P C F G while maintaining comparable parsing qualities. Two training strategies are considered: direct induction, in which a grammar is induced from scratch, learning from only the sparsely labeled training data; and adapt"
P99-1010,J93-2004,0,0.033216,"Missing"
P99-1010,P92-1017,0,0.721094,"impossible (Gold, 1967). One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979), which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992). Part of our work explores the in-between case, when only some constituent labels are available. Section 3 defines the different types of annotation we examine. Second, as supervision decreases, the learning process relies more on search. The success of the induction depends on the initial parameters of the grammar because a local search strategy may converge to a local minimum. For finding a good initial parameter set, Lari and Young (1990) suggested first estimating the probabilities with a set of regular grammar rules. Their experiments, however, indicated that the main benefit from this t"
P99-1010,1993.iwpt-1.20,0,0.149973,"l word. A typical HighP is a sentential clause or a complex noun phrase. The example sentence in Table 1 contains 3 HighP constituents: a complex noun phrase made up of a BaseNP and a prepositional phrase; a sentential clause with an omitted subject NP; and the full sentence. 4 Induction Strategies To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992). The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a), which is lexicalized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b)). Second, its lexicalized representation makes the training process more efficient than a traditional P C F G while maintaining comparable parsing qualities. Two training strategies are considered: direct induction, in which a grammar is induced from scratch, learning from only the sparsely labeled training data"
P99-1010,E93-1040,0,0.0438974,"Missing"
P99-1010,C98-1088,1,\N,Missing
W00-1306,P97-1003,0,0.0493317,"rees. Second, for most classification problems, the the number of the possible categories is relatively small, whereas the number of potential parse trees for a sentence is exponential with respect to the sentence length. 46 3 Grammar Induction The degree of difficulty of the task of learning a grammar from data depends on the quantity and quality of the training supervision. When the training corpus consists of a larg e reservoir of fully annotated parse trees, it is possible to directly extract a grammar based on these parse trees. The success of recent high-quality parsers (Charniak, 1997; Collins, 1997) relies on the availability of such treebank corpora. To work with smaller training corpora, the learning system would require even more information about the examples than their syntactic parse trees. For instance, Hermjakob and Mooney (1997) have described a learning system that can build a deterministic shiftreduce parser from a small set of training examples with the aid of detailed morphological, syntactical, and semantic knowledge databases and step-by-step guidance from human experts. The induction task becomes more challenging as the amount o f supervision in the training data and back"
W00-1306,P99-1010,1,0.795124,"Missing"
W00-1306,J93-2004,0,0.0514439,"Missing"
W00-1306,P92-1017,0,0.0441252,"ackground knowledge decreases. To compensate for the missing information, the learning process requires heuristic search to find locally optimal grammars. One form of partially supervised data might specify the phrasal boundaries without specifying their labels by bracketing each constituent unit with a pair of parentheses (McNaughton, 1967). For example, the parse tree for the sentence &apos;~Several fund managers expect a rough market this morning before prices stablize.&quot; is labeled as &quot;((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)&quot; As shown in Pereira and Schabes (1992), an essentially unsupervised learning algorithm such as the InsideOutside re-estimation process (Baker, 1979; Lari and Young, 1990) can be modified to take advantage of these bracketing constraints. For our sample selection experiment, we chose to work under the more stringent condition of partially supervised training data, as described above, because our ultimate goal is to minimize the amount of annotation done by humans in terms of b o t h the number of sentences and the number of brackets within the sentences. Thus, t h e quality of our induced grammars should not be compared to those ex"
W00-1306,1993.iwpt-1.20,0,0.146248,"periment, we chose to work under the more stringent condition of partially supervised training data, as described above, because our ultimate goal is to minimize the amount of annotation done by humans in terms of b o t h the number of sentences and the number of brackets within the sentences. Thus, t h e quality of our induced grammars should not be compared to those extracted from a fully annotated training corpus. The learning algorithm we use is a variant of the Inside-Outside algorithm that induces grammars expressed in the Probabilistic Lexicalized Tree Insertion Grammar representation (Schabes and Waters, 1993; Hwa, 1998). This formalism&apos;s Context-free equivalence and its lexicalized representation make the training process efficient and computationally plausible. 4 Selective Sampling Functions Evaluation In this paper, we propose two uncertaintybased evaluation functions for estimating the training utilities of the candidate sentences. The first is a simple heuristic that uses the length of a sentence to estimate uncertainties. The second function computes uncertainty in terms of the entropy of the parse trees that the hypothesis-grammar generated for the sentence. 4.1 S e n t e n c e L e n g t h"
W00-1306,J98-4002,0,\N,Missing
W00-1306,P97-1062,0,\N,Missing
W00-1306,P96-1042,0,\N,Missing
W01-0710,A00-2018,0,\N,Missing
W01-0710,J98-4002,0,\N,Missing
W01-0710,J93-2004,0,\N,Missing
W01-0710,P97-1003,0,\N,Missing
W01-0710,W00-1306,1,\N,Missing
W01-0710,P96-1042,0,\N,Missing
W01-0710,P00-1016,0,\N,Missing
W07-0737,W05-0909,0,0.0242516,"Missing"
W07-0737,E06-1032,0,0.0300604,"Missing"
W07-0737,N04-4038,0,0.0508963,"Missing"
W07-0737,koen-2004-pharaoh,0,0.146518,"Missing"
W07-0737,P02-1040,0,0.0787944,"f accuracy can we automatically identify DTPs? (3) To what extent do DTPs affect an MT system&apos;s performance on other (not-as-difficult) parts of the sentence? Conversely, would knowing the correct translation for the DTPs improve the system’s translation for the rest of the sentence? In this work, we model difficulty as a measurement with respect to a particular MT system. We further assume that the degree of difficulty of a phrase is directly correlated with the quality of the translation produced by the MT system, which can be approximated using an automatic evaluation metric, such as BLEU (Papineni et al., 2002). Using this formulation of difficulty, we build a framework that augments an off-the-shelf phrasebased MT system with a DTP classifier that we developed. We explore the three questions in a set of experiments, using the framework as a testbed. In the first experiment, we verify that our proposed difficulty measurement is sensible. The second experiment evaluates the classifier&apos;s accuracy in predicting whether a source phrase is a DTP. For that, we train a binary SVM classifier via a series of lexical and system dependent features. The third is an oracle study in which the DTPs are perfectly i"
W07-0737,W06-3110,0,\N,Missing
W07-0737,P03-1040,0,\N,Missing
W07-0737,C04-1046,0,\N,Missing
W07-0737,J03-1002,0,\N,Missing
W08-0330,P07-1038,1,0.853965,"ora) only a single reference is readily available. The focus of this work is on developing automatic metrics for sentence-level evaluation with at most one human reference. One way to supplement the single human reference is to use pseudo references, or sentences produced by off-the-shelf MT systems, as stand-ins for human references. However, since pseudo references may be imperfect translations themselves, the comparisons cannot be fully trusted. Previously, we have taken a learningbased approach to develop a composite metric that combines measurements taken from multiple pseudo references (Albrecht and Hwa, 2007). Experimental results suggested the approach to be promising; but those studies did not consider how well the metric might generalize across multiple years and different languages. In this paper, we investigate the applicability of the pseudo-reference metrics under these more general conditions. Using the WMT06 Workshop shared-task results (Koehn and Monz, 2006) as training examples, we train a metric that evaluates new sentences by comparing them against pseudo references produced by three off-the-shelf MT systems. We apply the learned metric to sentences from the WMT07 shared-task (Calliso"
W08-0330,W05-0909,0,0.178474,"Missing"
W08-0330,W07-0718,0,0.0261213,", 2007). Experimental results suggested the approach to be promising; but those studies did not consider how well the metric might generalize across multiple years and different languages. In this paper, we investigate the applicability of the pseudo-reference metrics under these more general conditions. Using the WMT06 Workshop shared-task results (Koehn and Monz, 2006) as training examples, we train a metric that evaluates new sentences by comparing them against pseudo references produced by three off-the-shelf MT systems. We apply the learned metric to sentences from the WMT07 shared-task (Callison-Burch et al., 2007b) and compare the metric’s predictions against human judgments. We find that additional pseudo references improve correlations for automatic metrics. 2 Background The ideal evaluation metric reports an accurate distance between an input instance and its gold standard, but even when comparing against imperfect standards, the measured distances may still convey some useful information – they may help to triangulate the input’s position relative to the true gold standard. In the context of sentence-level MT evaluations, 187 Proceedings of the Third Workshop on Statistical Machine Translation, pa"
W08-0330,P07-2045,0,0.00757835,"Missing"
W08-0330,C04-1072,0,0.0831833,"Missing"
W08-0330,P02-1040,0,0.0815931,"Missing"
W08-0330,C04-1046,0,\N,Missing
W08-0330,W07-0700,0,\N,Missing
W12-3810,W11-1701,0,0.380067,"ent 683 130 104 75 54 52 51 43 79 Table 2: Arguing and argument label statistics for the “pro” stance. subjectivity and arguments. We collected documents written both before and after the passage of the final “Patient Protection and Affordable Care Act” bill using the “Google Blog Search”3 and “Daily Op Ed”4 search portals. By choosing a relatively broad time window, from early 2009 to late 2011, we aimed to capture a wide range of arguments expressed throughout the debate. The focus of this paper is on sentence-level argument detection rather than document-level stance classification (e.g., (Anand et al., 2011), (Park et al., 2011), (Somasundaran and Wiebe, 2010), (Burfoot et al., 2011)). We treat stance classification as a separate step preceding arguing subjectivity detection, and thus provide oracle stance labels for our data. We treat documents written from the “pro” 3 4 http://www.google.com/blogsearch http://www.dailyoped.com/ arguing subjectivity objective subjective 913 575 argument labels no label diminishes quality of care too expensive unpopular hurts economy expands govt bill is politically motivated other reforms more appropriate other argument 913 122 67 60 55 52 44 35 140 Table 3: Arg"
W12-3810,P11-1151,0,0.0270021,"stics for the “pro” stance. subjectivity and arguments. We collected documents written both before and after the passage of the final “Patient Protection and Affordable Care Act” bill using the “Google Blog Search”3 and “Daily Op Ed”4 search portals. By choosing a relatively broad time window, from early 2009 to late 2011, we aimed to capture a wide range of arguments expressed throughout the debate. The focus of this paper is on sentence-level argument detection rather than document-level stance classification (e.g., (Anand et al., 2011), (Park et al., 2011), (Somasundaran and Wiebe, 2010), (Burfoot et al., 2011)). We treat stance classification as a separate step preceding arguing subjectivity detection, and thus provide oracle stance labels for our data. We treat documents written from the “pro” 3 4 http://www.google.com/blogsearch http://www.dailyoped.com/ arguing subjectivity objective subjective 913 575 argument labels no label diminishes quality of care too expensive unpopular hurts economy expands govt bill is politically motivated other reforms more appropriate other argument 913 122 67 60 55 52 44 35 140 Table 3: Arguing and argument label statistics for the “anti” stance. stance and document"
W12-3810,P11-1013,0,0.0269265,"discourse parser, and semantic similarity measures with respect to recognizing arguments. By incorporating information gained from these resources, we outperform a unigram baseline by a significant margin. In addition, we explore a two-phase approach to recognizing arguments, with promising results. 1 Introduction Subjectivity analysis is a thriving field within natural language processing. However, most research into subjectivity has focused on sentiment with respect to concrete things such as product debates (e.g., (Somasundaran and Wiebe, 2009), (Yu et al., 2011)) and movie reviews (e.g., (He et al., 2011), (Maas et al., 2011), (Pang and Lee, 2004)). Analysis often follows the opinion-target paradigm, in which expressions of sentiment are assessed with respect to the aspects of the object(s) under consideration towards which they are targeted. For example, (1) Almost everyone knows that we must start holding insurance companies accountable and give Americans a greater sense of stability and security when it comes to their health care. In a traditional opinion-target or sentimenttopic paradigm, perhaps this sentence could be labeled as containing a negative sentiment towards a topic representing"
W12-3810,P11-1016,0,0.0042768,"ecific actions may be referenced, as illustrated in Examples (2-4) from Section 1. To address this problem, we investigate expanding each instance with terms that are most similar, according to a distributional model generated from Wikipedia articles, to the nouns and verbs present within the instance (Pantel et al., 2009). We refer to these features as “expn”, where n is the number of most-similar terms with which to expand the instance for each noun or verb. We experiment with values of n = 5 and n = 10. Subjectivity classification of small units of text, such as individual microblog posts (Jiang et al., 2011) and sentences (Riloff et al., 2003), has been shown to benefit from additional context. Thus, we augment the feature representation of each target sentence with features from the two preceding and two following sentences. These additional features are modified so that they do not fall within the same feature space 5 downloaded from http://www.cs.pitt.edu/mpqa/ subj_lexicon.html feat. abbrev. unigram senti rels exp5 exp10 elaboration 2 binary features indicating positive or negative sentiment based on presence of lexicon clues 15 binary features indicating kinds of discourse relationships and"
W12-3810,W06-2915,1,0.698866,"s from Table 7. While all of the hierarchical configurations beat the best “combined” classifier, none beats the top combined classifier by a significant margin, although the best configurations approach significance (0.05 &lt; p &lt; 0.1). 7 Related Work Much recent work in ideological subjectivity detection has focused on detecting a writer’s stance in domains of varying formality, such as online forums, debating websites, and op-eds. (Anand et al., 2011) demonstrates the usefulness of dependency relations, LIWC counts (Pennebaker et al., 2001), and information about related posts for this task. (Lin et al., 2006) explores relationships between sentence-level and document-level classification for a stance-like prediction task. Among the literature on ideological subjectivity, perhaps most similar to our work is (Somasundaran and Wiebe, 2010). This paper investigates the impact of incorporating arguing-based 87 and sentiment-based features into binary stance prediction for debate posts. Also closely related to our work is (Somasundaran et al., 2007). To support answering of opinion-based questions, this work investigates the use of high-precision sentiment and arguing clues for sentence-level sentiment"
W12-3810,P11-1100,0,0.010642,"Missing"
W12-3810,P11-1015,0,0.0208271,"nd semantic similarity measures with respect to recognizing arguments. By incorporating information gained from these resources, we outperform a unigram baseline by a significant margin. In addition, we explore a two-phase approach to recognizing arguments, with promising results. 1 Introduction Subjectivity analysis is a thriving field within natural language processing. However, most research into subjectivity has focused on sentiment with respect to concrete things such as product debates (e.g., (Somasundaran and Wiebe, 2009), (Yu et al., 2011)) and movie reviews (e.g., (He et al., 2011), (Maas et al., 2011), (Pang and Lee, 2004)). Analysis often follows the opinion-target paradigm, in which expressions of sentiment are assessed with respect to the aspects of the object(s) under consideration towards which they are targeted. For example, (1) Almost everyone knows that we must start holding insurance companies accountable and give Americans a greater sense of stability and security when it comes to their health care. In a traditional opinion-target or sentimenttopic paradigm, perhaps this sentence could be labeled as containing a negative sentiment towards a topic representing “insurance companies"
W12-3810,P04-1035,0,0.00260051,"y measures with respect to recognizing arguments. By incorporating information gained from these resources, we outperform a unigram baseline by a significant margin. In addition, we explore a two-phase approach to recognizing arguments, with promising results. 1 Introduction Subjectivity analysis is a thriving field within natural language processing. However, most research into subjectivity has focused on sentiment with respect to concrete things such as product debates (e.g., (Somasundaran and Wiebe, 2009), (Yu et al., 2011)) and movie reviews (e.g., (He et al., 2011), (Maas et al., 2011), (Pang and Lee, 2004)). Analysis often follows the opinion-target paradigm, in which expressions of sentiment are assessed with respect to the aspects of the object(s) under consideration towards which they are targeted. For example, (1) Almost everyone knows that we must start holding insurance companies accountable and give Americans a greater sense of stability and security when it comes to their health care. In a traditional opinion-target or sentimenttopic paradigm, perhaps this sentence could be labeled as containing a negative sentiment towards a topic representing “insurance companies”, or a positive senti"
W12-3810,P11-1035,0,0.143022,"52 51 43 79 Table 2: Arguing and argument label statistics for the “pro” stance. subjectivity and arguments. We collected documents written both before and after the passage of the final “Patient Protection and Affordable Care Act” bill using the “Google Blog Search”3 and “Daily Op Ed”4 search portals. By choosing a relatively broad time window, from early 2009 to late 2011, we aimed to capture a wide range of arguments expressed throughout the debate. The focus of this paper is on sentence-level argument detection rather than document-level stance classification (e.g., (Anand et al., 2011), (Park et al., 2011), (Somasundaran and Wiebe, 2010), (Burfoot et al., 2011)). We treat stance classification as a separate step preceding arguing subjectivity detection, and thus provide oracle stance labels for our data. We treat documents written from the “pro” 3 4 http://www.google.com/blogsearch http://www.dailyoped.com/ arguing subjectivity objective subjective 913 575 argument labels no label diminishes quality of care too expensive unpopular hurts economy expands govt bill is politically motivated other reforms more appropriate other argument 913 122 67 60 55 52 44 35 140 Table 3: Arguing and argument lab"
W12-3810,prasad-etal-2008-penn,0,0.0370597,"Missing"
W12-3810,W03-0404,1,0.399392,"s illustrated in Examples (2-4) from Section 1. To address this problem, we investigate expanding each instance with terms that are most similar, according to a distributional model generated from Wikipedia articles, to the nouns and verbs present within the instance (Pantel et al., 2009). We refer to these features as “expn”, where n is the number of most-similar terms with which to expand the instance for each noun or verb. We experiment with values of n = 5 and n = 10. Subjectivity classification of small units of text, such as individual microblog posts (Jiang et al., 2011) and sentences (Riloff et al., 2003), has been shown to benefit from additional context. Thus, we augment the feature representation of each target sentence with features from the two preceding and two following sentences. These additional features are modified so that they do not fall within the same feature space 5 downloaded from http://www.cs.pitt.edu/mpqa/ subj_lexicon.html feat. abbrev. unigram senti rels exp5 exp10 elaboration 2 binary features indicating positive or negative sentiment based on presence of lexicon clues 15 binary features indicating kinds of discourse relationships and how they connect instance to surroun"
W12-3810,P09-1026,1,0.255916,"ne learning experiments, we investigate the utility of a sentiment lexicon, discourse parser, and semantic similarity measures with respect to recognizing arguments. By incorporating information gained from these resources, we outperform a unigram baseline by a significant margin. In addition, we explore a two-phase approach to recognizing arguments, with promising results. 1 Introduction Subjectivity analysis is a thriving field within natural language processing. However, most research into subjectivity has focused on sentiment with respect to concrete things such as product debates (e.g., (Somasundaran and Wiebe, 2009), (Yu et al., 2011)) and movie reviews (e.g., (He et al., 2011), (Maas et al., 2011), (Pang and Lee, 2004)). Analysis often follows the opinion-target paradigm, in which expressions of sentiment are assessed with respect to the aspects of the object(s) under consideration towards which they are targeted. For example, (1) Almost everyone knows that we must start holding insurance companies accountable and give Americans a greater sense of stability and security when it comes to their health care. In a traditional opinion-target or sentimenttopic paradigm, perhaps this sentence could be labeled"
W12-3810,W10-0214,1,0.935419,"nizing Arguing Subjectivity and Argument Tags Alexander Conrad, Janyce Wiebe, and Rebecca Hwa Department of Computer Science University of Pittsburgh Pittsburgh PA, 15260, USA {conrada,wiebe,hwa}@cs.pitt.edu Abstract in the domain of smartphone reviews, aspects could include product features such as the keyboard, screen quality, and battery life. Although sentiment analysis is interesting and important in its own right, this paradigm does not seem to be the best match for finegrained analysis of ideological domains. While sentiment is also present in documents from this domain, previous work (Somasundaran and Wiebe, 2010) has found that arguing subjectivity, a less-studied form of subjectivity, is more frequently employed and more relevant for a robust assessment of ideological positions. Whereas sentiment conveys the polarity of a writer’s affect towards a topic, arguing subjectivity is a type of linguistic subjectivity in which a person expresses a controversial belief about what is true or what action ought to be taken regarding a central contentious issue (Somasundaran, 2010). For example, consider this sentence about health care reform: In this paper we investigate two distinct tasks. The first task invol"
W12-3810,W03-2102,1,0.484806,"Inter-annotator span agr (top) and argument label kappa on overlapping spans (bottom). In assessing inter-annotator agreement on this subset of the corpus, we must address two levels of agreement, arguing spans and argument tags. At first glance, how to assess agreement of annotated arguing spans is not obvious. Because our annotation scheme did not enforce strict boundaries, we hypothesized that both annotators would both frequently see an instance of arguing subjectivity within a local region of text, but would disagree with respect to where the arguing begins and ends. Thus, we adopt from (Wilson and Wiebe, 2003) the agr directional agreement metric to measure the degree of annotation overlap. Given two sets of spans A and B annotated by two different annotators, this metric measures the fraction of spans in A which at least partially overlap with any spans in B. Specifically, agreement is computed as: agr (A B) = A matching B A When A is the gold standard set of annotations, agr is equivalent to recall. Similarly, when B is the gold standard, agr is equivalent to precision. For this evaluation, we treat the dataset annotated by our primary annotator as the gold standard. Table 5 presents these agr sc"
W12-3810,H05-1044,1,0.0101733,"inferred by the discourse parser with the parent top-level PDTB 85 discourse relationship class. We arrive at a total of 15 binary discourse relationship features: (4 top-level classes + “other”) x (connects to previous + connects to following + internal connection) = 15. We refer to these features as “rels”. As illustrated in our earlier examples, while arguing subjectivity is different from sentiment, the two types of subjectivity are often related. Thus, we investigate incorporating sentiment information based on the presence of unigram clues from a publically-available sentiment lexicon5 (Wilson, 2005). Each clue in the lexicon is marked as being either “strong” or “weak”. We found that this lexicon was producing many false hits for positive sentiment. Thus, a span containing a minimum of two positive clues of which at least one is marked as “strong”, or three positive “weak” clues, is augmented with a feature indicating positive sentiment. For negative sentiment the threshold is slightly lower, at one “strong” clue or two “weak” clues. These features are referred to as “senti”. A challenge to argument tag assignment is the broad diversity of language through which individual entities or sp"
W12-3810,P11-1150,0,0.0390719,"stigate the utility of a sentiment lexicon, discourse parser, and semantic similarity measures with respect to recognizing arguments. By incorporating information gained from these resources, we outperform a unigram baseline by a significant margin. In addition, we explore a two-phase approach to recognizing arguments, with promising results. 1 Introduction Subjectivity analysis is a thriving field within natural language processing. However, most research into subjectivity has focused on sentiment with respect to concrete things such as product debates (e.g., (Somasundaran and Wiebe, 2009), (Yu et al., 2011)) and movie reviews (e.g., (He et al., 2011), (Maas et al., 2011), (Pang and Lee, 2004)). Analysis often follows the opinion-target paradigm, in which expressions of sentiment are assessed with respect to the aspects of the object(s) under consideration towards which they are targeted. For example, (1) Almost everyone knows that we must start holding insurance companies accountable and give Americans a greater sense of stability and security when it comes to their health care. In a traditional opinion-target or sentimenttopic paradigm, perhaps this sentence could be labeled as containing a neg"
W12-3810,D09-1098,0,\N,Missing
