2006.amta-papers.9,W03-1004,0,0.035486,"fferent assumptions they make about the available resources. Most approaches are not applicable to the problem of inducing lexical mappings between diglossic dialects because their assumptions do not hold in this domain. One approach is to try to build small parallel corpora out of large comparable corpora. Fung and Cheung (2004) proposed a bootstrapping method that extracts parallel sentences from texts that may be unrelated on the document level. This approach requires a seed lexicon and computes lexical similarity scores. It also requires large corpora that contain some parallel sentences. Barzilay and Elhadad (2003) applied a similar method monolingually to find paraphrases. Another method proposed by Munteanu et al. (2004) requires a set of seed parallel corpora of 5000 sentences for each language. While in the world of parallel corpora 5000 sentence pairs are considered minuscule, they may not exist at all for dialect pairs such as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal webs"
2006.amta-papers.9,J90-2002,0,0.16046,"istical analyses on the corpora to find correlations between the symbols. The quality of the mapping depends on the degree of relatedness between the corpora. Parallel corpora, in which every pair of sentences is a translation of each other, facilitate the induction of a mapping between word tokens (situated occurrences); in contrast, one might only be able to glean a mapping between word types (as in a wide coverage dictionary) from non-parallel corpora. 1 Introduction A translation lexicon is an important component of multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instan"
2006.amta-papers.9,P04-1023,0,0.0150155,"multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instance, consider the problem of finding a mapping between two dialects of a diglossic language (i.e., the language exists in two forms: a “prestigious” variety for formal communications and a colloquial variety for everyday use). Because the dialects serve different social functions, parallel corpora between dialects do not naturally occur. In these cases, inducing lexical mappings from non-parallel corpora is the more challenging alternative. Even when parallel sentence pairs are not available, one might still be able to bootstrap a"
2006.amta-papers.9,W04-3208,0,0.0125752,"d statistics of the seed dictionary, the algorithm achieves a modest improvement. 2 Mapping between Comparable Corpora Several methods have been proposed to induce lexical mappings from non-parallel corpora. In this section, we provide an overview of several common approaches and discuss different assumptions they make about the available resources. Most approaches are not applicable to the problem of inducing lexical mappings between diglossic dialects because their assumptions do not hold in this domain. One approach is to try to build small parallel corpora out of large comparable corpora. Fung and Cheung (2004) proposed a bootstrapping method that extracts parallel sentences from texts that may be unrelated on the document level. This approach requires a seed lexicon and computes lexical similarity scores. It also requires large corpora that contain some parallel sentences. Barzilay and Elhadad (2003) applied a similar method monolingually to find paraphrases. Another method proposed by Munteanu et al. (2004) requires a set of seed parallel corpora of 5000 sentences for each language. While in the world of parallel corpora 5000 sentence pairs are considered minuscule, they may not exist at all for d"
2006.amta-papers.9,P04-1067,0,0.0247933,"her language. Different similarity metrics (e.g., Cosine, Jacquard, Euclidean) can be used for the comparison; Rapp used the city-block distance. A number of related algorithms have been suggested by other researchers. Diab and Finch (2000) proposed a method that does not explicitly require a seed dictionary, though they do assume that punctuations behave similarly between the two languages. This method first builds a set of similarity vectors between pairs of the 1000 most frequent words within one language; then it compares these vectors to all possible vectors pairs for the other language. Gaussier et al. (2004) proposed an extension that focused on explicitly modeling synonyms within each monolingual corpus. 76 Although this last class of methods is more flexible than filtering for parallel sentences from comparable corpora or using bridge dictionaries, its assumptions are still too stringent for the diglossic dialect domain. First, the methods assume the availability of large quantities of corpus samples in both languages. While it is not difficult to obtain a large MSA corpus, only a small Levantine corpus is available. The dependence on seed dictionary is also problematic. It is not feasible to r"
2006.amta-papers.9,N01-1020,0,0.146897,"ll for dialect pairs such as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal websites written in colloquial dialects, the methods that search the web for parallel texts typically search for pages that link to their own translation by looking for certain structures that indicate as such. It has also been proposed that one might use a bridge language to find lexical mappings (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002). The key requirement is that the language pair of interest can be related to each other via a third language with which lexical mappings have already been established. This is an unlikely situation for the diglossic language domain because it is rare to find an established dictionary purely between a colloquial dialect and some third language. An other alternative is to take aggregate word statistics over large samples of the languages in comparable corpora. An instance of this class of algorithms is a method proposed by Rapp (1999). This method requires a seed di"
2006.amta-papers.9,J00-2004,0,0.0405958,"icon is an important component of multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instance, consider the problem of finding a mapping between two dialects of a diglossic language (i.e., the language exists in two forms: a “prestigious” variety for formal communications and a colloquial variety for everyday use). Because the dialects serve different social functions, parallel corpora between dialects do not naturally occur. In these cases, inducing lexical mappings from non-parallel corpora is the more challenging alternative. Even when parallel sentence pairs are not a"
2006.amta-papers.9,N04-1034,0,0.0135162,"nducing lexical mappings between diglossic dialects because their assumptions do not hold in this domain. One approach is to try to build small parallel corpora out of large comparable corpora. Fung and Cheung (2004) proposed a bootstrapping method that extracts parallel sentences from texts that may be unrelated on the document level. This approach requires a seed lexicon and computes lexical similarity scores. It also requires large corpora that contain some parallel sentences. Barzilay and Elhadad (2003) applied a similar method monolingually to find paraphrases. Another method proposed by Munteanu et al. (2004) requires a set of seed parallel corpora of 5000 sentences for each language. While in the world of parallel corpora 5000 sentence pairs are considered minuscule, they may not exist at all for dialect pairs such as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal websites written in colloquial dialects, the methods that search the web for parallel texts typically search for p"
2006.amta-papers.9,J03-1002,0,0.00298554,"rtant component of multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instance, consider the problem of finding a mapping between two dialects of a diglossic language (i.e., the language exists in two forms: a “prestigious” variety for formal communications and a colloquial variety for everyday use). Because the dialects serve different social functions, parallel corpora between dialects do not naturally occur. In these cases, inducing lexical mappings from non-parallel corpora is the more challenging alternative. Even when parallel sentence pairs are not available, one might"
2006.amta-papers.9,P99-1067,0,0.851889,"f this work is to investigate one such scenario: finding lexical mappings between dialects of a diglossic language, in which people conduct their written communications in a prestigious formal dialect, but they communicate verbally in a colloquial dialect. Because the two dialects serve different socio-linguistic functions, parallel corpora do not naturally exist between them. An example of a diglossic dialect pair is Modern Standard Arabic (MSA) and Levantine Arabic. In this paper, we evaluate the applicability of a standard algorithm for inducing lexical mappings between comparable corpora (Rapp, 1999) to such diglossic corpora pairs. The focus of the paper is an in-depth error analysis, exploring the notion of relatedness in diglossic corpora and scrutinizing the effects of various dimensions of relatedness (such as mode, topic, style, and statistics) on the quality of the resulting translation lexicon. Abstractly speaking, a translation lexicon is a mapping between two disjoint sets of symbols. Given some corpus sample over each set of symbols, one might induce the mapping by performing statistical analyses on the corpora to find correlations between the symbols. The quality of the mappin"
2006.amta-papers.9,J03-3002,0,0.0180126,"evel. This approach requires a seed lexicon and computes lexical similarity scores. It also requires large corpora that contain some parallel sentences. Barzilay and Elhadad (2003) applied a similar method monolingually to find paraphrases. Another method proposed by Munteanu et al. (2004) requires a set of seed parallel corpora of 5000 sentences for each language. While in the world of parallel corpora 5000 sentence pairs are considered minuscule, they may not exist at all for dialect pairs such as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal websites written in colloquial dialects, the methods that search the web for parallel texts typically search for pages that link to their own translation by looking for certain structures that indicate as such. It has also been proposed that one might use a bridge language to find lexical mappings (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002). The key requirement is that the language pair of interest can be related to each other via a"
2006.amta-papers.9,W02-2026,0,0.142198,"as MSA and Levantine. The use of information on the Internet has also been shown to be promising (Resnik and Smith, 2003), but may not be applicable for spoken dialects, which are unlikely to be transcribe and published on the internet. While there may be blogs or informal websites written in colloquial dialects, the methods that search the web for parallel texts typically search for pages that link to their own translation by looking for certain structures that indicate as such. It has also been proposed that one might use a bridge language to find lexical mappings (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002). The key requirement is that the language pair of interest can be related to each other via a third language with which lexical mappings have already been established. This is an unlikely situation for the diglossic language domain because it is rare to find an established dictionary purely between a colloquial dialect and some third language. An other alternative is to take aggregate word statistics over large samples of the languages in comparable corpora. An instance of this class of algorithms is a method proposed by Rapp (1999). This method requires a seed dictionary (i.e., a collection"
2006.amta-papers.9,N01-1026,0,0.0118241,"ion of each other, facilitate the induction of a mapping between word tokens (situated occurrences); in contrast, one might only be able to glean a mapping between word types (as in a wide coverage dictionary) from non-parallel corpora. 1 Introduction A translation lexicon is an important component of multilingual processing applications such as machine translation systems (Brown et al., 1990; Al-Onaizan et al., 1999) and multilingual information retrieval systems (Sheridan and Ballerini, 1996; CLE, 2005). A translation lexicon can also facilitate cross-lingual resource building. For example, Yarowsky and Ngai (2001) have shown The induction of mappings between word tokens from parallel corpora has been extensively studied; there exist many alignment methods, both supervised and unsupervised, that yield highly accurate lexical mappings between word tokens (Melamed, 2000; Och and Ney, 2003; Callison-Burch et al., 2004). However, parallel corpora are not always available. For instance, consider the problem of finding a mapping between two dialects of a diglossic language (i.e., the language exists in two forms: a “prestigious” variety for formal communications and a colloquial variety for everyday use). Bec"
2006.amta-papers.9,E06-1047,0,\N,Missing
2010.eamt-1.28,P07-2045,0,0.00436692,"s of the European Parliament Plenary Session transcription (EuroParl). Basic training corpus statistics can be found in Table 1. Sentences Words Average sentence length Vocabulary Dutch 1.2 M 32.9 M 27.20 228 K English 1.2 M 33.0 M 27.28 104 K Table 1: Basic statistics of the English-Dutch EuroParl training corpus. Development and test datasets were randomly chosen from the corpus and consisted of 500 and 1,000 sentences, respectively. Both were provided with 1 reference translation. 5.2 Experimental setup SMT system used in the experiments is implemented within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page: http://www.statmt.org/moses/. Word alignment was estimated with GIZA++ tool4 (Och, 2003), coupled with the mkcls5 (Och, 1999) tool, which allows for statistical word clustering for better generalization. N -gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and is a 5-gram model with modified Kneser-Ney discounting. Evaluation conditions were case-sensitive and included punctuation marks. 5.3 Systems We contrast five alternative sys"
2010.eamt-1.28,E99-1010,0,0.0619318,"cs of the English-Dutch EuroParl training corpus. Development and test datasets were randomly chosen from the corpus and consisted of 500 and 1,000 sentences, respectively. Both were provided with 1 reference translation. 5.2 Experimental setup SMT system used in the experiments is implemented within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page: http://www.statmt.org/moses/. Word alignment was estimated with GIZA++ tool4 (Och, 2003), coupled with the mkcls5 (Och, 1999) tool, which allows for statistical word clustering for better generalization. N -gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and is a 5-gram model with modified Kneser-Ney discounting. Evaluation conditions were case-sensitive and included punctuation marks. 5.3 Systems We contrast five alternative system configurations differing in feature set and reordering example extraction algorithm, along with two Moses-based baseline systems (Table 2). Both baseline systems implement nondeterministic reordering algorithm providing the decoder with multiple word ord"
2010.eamt-1.28,J96-1002,0,0.0164591,"of reordering actions. For example, if a binary tree of the source is given, it is possible to define an ITG-constrained cascade of transforms (but such a binary tree is usually not available unless one commits to a certain source language parser). And secondly, it allows the conditioning of the ith step in the cascade on aspects 0 of previous permutation Si−1 . In this paper we consider Swapping/Monotone as the main transform, and in the reported experiments we limit the swapping to individual words. The swapping decisions are formulated as a binary classification task trained under MaxEnt (Berger et al., 1996). We explore a variety of lexical features of the pair of words, including surrounding words, POS tags, and supertags (Clark and Curran, 2003). The present exploration is driven by the observation that the existing phrase-based models are quite strong in local word reordering within a fixed window. In light of this observation, it becomes attractive to attempt bridging long distance reorderings as those found in English-Dutch translation. Figure 2 shows an example of typical difference in word order between Dutch and English: in contrast to English, the Dutch verb normally appears in the end o"
2010.eamt-1.28,J94-4004,0,0.0517005,"able Metric method. Features Feature functions φ1 (S) and φ2 (S), apart from the word instances themselves, include: • Context-based features. Source-side context 3-grams, 2-grams, and 1-grams, describing left- and right-hand side neighborhood of the first and the second word. These features can be seen as a contextual predictor describing word preference to change its current location. • Morpho-syntactic features. Linguistic syntax is believed to be helpful in MT, thoroughly handling word order dependencies and accurately modeling many systematic differences between word orders of languages (Bonnie, 1994). Syntax is introduced into MaxEnt reordering system using POS tags and supertags which are assigned for each word according to Lexicalized Tree-Adjoining Grammar (Abeillè and Rambow, 2000) and Combinatory Categorial Grammar (Clark and Curran, 2003), as described in (Hassan et al., 2007). In other words, supertags in compact form describe the way to the highest node of the parse tree. I Ik hoop natuurlijk Figure 3: Example of a disambiguous alignment crossing. The extraction step involves taking a decision on assigning swapping probability for those units. For example, in Figure 4, swapping of"
2010.eamt-1.28,J93-2003,0,0.023441,"en by the source reordering system. Word-alignment is performed on the latter parallel corpus and that corpus is used for training a phrase-based SMT system. In this section we describe our cascaded source reordering system that employs MaxEnt classifiers. 3.1 Creating the monotonized parallel corpus Monotonization of the parallel training corpus is done on the basis of the ”grow-diag-final“ manyto-many alignment. It is modified to one-to-one in the way described in the next lines. If a source word is aligned to two or more target words, the most probable link given lexical probability model (Brown et al., 1993) is chosen, while the others are omitted. Source side words are permuted within the scope of a sentence such that all the crossing links in the alignment are unfolded. The resulting training corpus is called {S 0 }. It is explicitly implied that the number and the set of words in {S 0 } and in {S} coincide. 3.2 Cascaded source reordering by classification Initially, we formulate the problem of defining the set of permutations and selecting the most likely permutation given a source sentence as a conditional probability that we break down into a cascade of transforms where the final permutation"
2010.eamt-1.28,P05-1033,0,0.0802948,"d under maximum entropy (MaxEnt). We experiment with features that consist of the local neighborhood of both words as well as lexico-syntactic representations known as supertags. Our experiments on the English-to-Dutch EuroParl translation task show that the cascaded alignment unfolding slightly improves the performance of a state-of-the-art phrase translation system that uses distance-based and lexicalized block-oriented reordering. 1 fined using a local window of phrases, e.g., (Tillman, 2004). Hierarchical approaches, based on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005), explore yet a wider range of reorderings defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. Finally, source sentence reordering is a preprocessing task that aims at finding a permutation of the words of the source sentence that contains the least number of crossing alignments between source words and their target sentence counterparts. This paper is concerned with the task of source language reordering as a preprocessing task. Figure 1 depicts the translation from source string S to target string T with alignm"
2010.eamt-1.28,W03-1013,0,0.16636,"such a binary tree is usually not available unless one commits to a certain source language parser). And secondly, it allows the conditioning of the ith step in the cascade on aspects 0 of previous permutation Si−1 . In this paper we consider Swapping/Monotone as the main transform, and in the reported experiments we limit the swapping to individual words. The swapping decisions are formulated as a binary classification task trained under MaxEnt (Berger et al., 1996). We explore a variety of lexical features of the pair of words, including surrounding words, POS tags, and supertags (Clark and Curran, 2003). The present exploration is driven by the observation that the existing phrase-based models are quite strong in local word reordering within a fixed window. In light of this observation, it becomes attractive to attempt bridging long distance reorderings as those found in English-Dutch translation. Figure 2 shows an example of typical difference in word order between Dutch and English: in contrast to English, the Dutch verb normally appears in the end of the relative clause. ... that he went to his house ... dat hij naar zijn huis ging Figure 2: Example of long-distance reordering for Dutch-t"
2010.eamt-1.28,P05-1066,0,0.276964,"Missing"
2010.eamt-1.28,W06-1609,0,0.0568664,"xclude the more pragmatic option of narrowing the distance between crossing alignments. 2 Related work The idea of augmenting SMT by using a reordering step prior to translation has proved to be successful in improving translation quality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-toEnglish tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Khalilov, 2009; Xia and McCord, 2004) the fundamental problem of word ordering is addressed exploiting syntactic representations of source and target texts. In (Costa-jussà and Fonollosa, 2006) source and target word order harmonization is done using well-established SMT techniques and without the use of syntactic knowledge. Other reordering models operate in a nondeterministic way providing the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. One more example of a system performin"
2010.eamt-1.28,P02-1038,0,0.137367,"es system is the first reordering technique under consideration. This model provides the decoder with a cost linear to the distance between words that should be reordered. p(allows,us,”Swapped”) = pLP (allows,mogelijk) = {(pLP (allows,maakt) + pLP (allows,mogelijk)} 4.2 MSD where pLP (word1 , word2 ) is the lexical probability of translating a source-side word1 by a targetside word2 . In the following sections, we explore the impact of this hard decision in reordering accuracy and in translation quality. 4 Baseline translation system Rather than translating single words, phrase-based systems (Och and Ney, 2002) work with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a set of constraints (Zens et al., 2002). A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any aligned pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase (Och and Ney, 2004). The probability of the phrases is estimated by counts of"
2010.eamt-1.28,J04-4002,0,0.777867,"target string T with alignment a (solid line) and the alternative of source reordering S into 0 0 S followed by the translation S → T with align0 ment a (in dashed lines). Source reordering of S is as successfull as much as it will yield a permu0 0 tation S that has as monotone an alignment a as possible with T . Introduction The word-order divergence (also called distortion) between source-target sentence pairs is a major research topic in statistical machine translation (SMT). The problem of reordering in SMT has been attacked from different angles. Standard phrase-based translation models (Och and Ney, 2004) search for the best reordering option during decoding within a limited distortion space dec 2010 European Association for Machine Translation. ° Figure 1: Translation schemes with and without a reordering step. This problem can be seen as the task of learning from a word-aligned parallel corpus a model 0 of source permutation from S to S , where the latter has monotone alignment with T . The learning task aims at learning a model that minimizes the number of non-monotone alignments in the training data and expected future data of the same sort. However, this learning problem is hampered with"
2010.eamt-1.28,P03-1021,0,0.00742671,"M 27.28 104 K Table 1: Basic statistics of the English-Dutch EuroParl training corpus. Development and test datasets were randomly chosen from the corpus and consisted of 500 and 1,000 sentences, respectively. Both were provided with 1 reference translation. 5.2 Experimental setup SMT system used in the experiments is implemented within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page: http://www.statmt.org/moses/. Word alignment was estimated with GIZA++ tool4 (Och, 2003), coupled with the mkcls5 (Och, 1999) tool, which allows for statistical word clustering for better generalization. N -gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and is a 5-gram model with modified Kneser-Ney discounting. Evaluation conditions were case-sensitive and included punctuation marks. 5.3 Systems We contrast five alternative system configurations differing in feature set and reordering example extraction algorithm, along with two Moses-based baseline systems (Table 2). Both baseline systems implement nondeterministic reordering algorithm providi"
2010.eamt-1.28,N04-4026,0,0.816145,"on to swap a pair of words is modelled as a binary classification task formulated as a log-linear model and trained under maximum entropy (MaxEnt). We experiment with features that consist of the local neighborhood of both words as well as lexico-syntactic representations known as supertags. Our experiments on the English-to-Dutch EuroParl translation task show that the cascaded alignment unfolding slightly improves the performance of a state-of-the-art phrase translation system that uses distance-based and lexicalized block-oriented reordering. 1 fined using a local window of phrases, e.g., (Tillman, 2004). Hierarchical approaches, based on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005), explore yet a wider range of reorderings defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. Finally, source sentence reordering is a preprocessing task that aims at finding a permutation of the words of the source sentence that contains the least number of crossing alignments between source words and their target sentence counterparts. This paper is concerned with the task of source language reorder"
2010.eamt-1.28,D09-1105,0,0.374388,"slation. ° Figure 1: Translation schemes with and without a reordering step. This problem can be seen as the task of learning from a word-aligned parallel corpus a model 0 of source permutation from S to S , where the latter has monotone alignment with T . The learning task aims at learning a model that minimizes the number of non-monotone alignments in the training data and expected future data of the same sort. However, this learning problem is hampered with the intractable complexity of computing the most probable permutation under a reasonable probabilistic model of the permutations (see (Tromble and Eisner, 2009) for the Linear Ordering Problem). We look at an alternative view of source reordering where we view this as a simple cascade of string transforms. At the ith step of the cascade, a single choice is made on the reordering of words/phrases in the source string and this lead 0 to a permutation Si . While this general framework does not define a priori a constrained form of the reordering graph (as, e.g., the ITG trees), it has two attractive properties. Firstly, it is efficient enough to allow exploring a variety of such constraints on the order of reordering actions. For example, if a binary tr"
2010.eamt-1.28,D07-1077,0,0.0960014,"Missing"
2010.eamt-1.28,P98-2230,0,0.282787,"ar model and trained under maximum entropy (MaxEnt). We experiment with features that consist of the local neighborhood of both words as well as lexico-syntactic representations known as supertags. Our experiments on the English-to-Dutch EuroParl translation task show that the cascaded alignment unfolding slightly improves the performance of a state-of-the-art phrase translation system that uses distance-based and lexicalized block-oriented reordering. 1 fined using a local window of phrases, e.g., (Tillman, 2004). Hierarchical approaches, based on Inversion Transduction Grammar (ITG), e.g., (Wu and Wong, 1998; Chiang, 2005), explore yet a wider range of reorderings defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence. Finally, source sentence reordering is a preprocessing task that aims at finding a permutation of the words of the source sentence that contains the least number of crossing alignments between source words and their target sentence counterparts. This paper is concerned with the task of source language reordering as a preprocessing task. Figure 1 depicts the translation from source string S to target strin"
2010.eamt-1.28,C04-1073,0,0.136004,"nge for phrase-based models. While our work aims at the general problem of learning how to untangle all crossing alignments, we do not a priori exclude the more pragmatic option of narrowing the distance between crossing alignments. 2 Related work The idea of augmenting SMT by using a reordering step prior to translation has proved to be successful in improving translation quality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-toEnglish tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Khalilov, 2009; Xia and McCord, 2004) the fundamental problem of word ordering is addressed exploiting syntactic representations of source and target texts. In (Costa-jussà and Fonollosa, 2006) source and target word order harmonization is done using well-established SMT techniques and without the use of syntactic knowledge. Other reordering models operate in a nondeterministic way providing the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the autho"
2010.eamt-1.28,2007.mtsummit-papers.16,0,0.0216738,"n is done using well-established SMT techniques and without the use of syntactic knowledge. Other reordering models operate in a nondeterministic way providing the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. One more example of a system performing reordering in this way can be found in (Crego and Mariño, 2007), where syntactic structure on the source side is exploited to reorder the input of a word lattice in an unweighted manner, slightly expanding the monotonic search space. In (Tromble and Eisner, 2009) a O(n3 ) chart parsing algorithm aimed to find the best reordering of possible word permutations is described and applied for the German-English language pair. 3 Source reordering by cascaded transforms For a translation system that employs source reordering as preprocessing step, two training stages are needed given a word-aligned parallel corpus of source and target sentences {S − T }: • Concep"
2010.eamt-1.28,P06-1066,0,0.0889719,"and Chinese-toEnglish tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Khalilov, 2009; Xia and McCord, 2004) the fundamental problem of word ordering is addressed exploiting syntactic representations of source and target texts. In (Costa-jussà and Fonollosa, 2006) source and target word order harmonization is done using well-established SMT techniques and without the use of syntactic knowledge. Other reordering models operate in a nondeterministic way providing the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. One more example of a system performing reordering in this way can be found in (Crego and Mariño, 2007), where syntactic structure on the source side is exploited to reorder the input of a word lattice in an unweighted manner, slightly expanding the monotonic search space. In (Tromble and Eisner, 2009) a O(n3 ) chart parsing algorithm aimed to find th"
2010.eamt-1.28,D08-1089,0,0.0594759,"9; Xia and McCord, 2004) the fundamental problem of word ordering is addressed exploiting syntactic representations of source and target texts. In (Costa-jussà and Fonollosa, 2006) source and target word order harmonization is done using well-established SMT techniques and without the use of syntactic knowledge. Other reordering models operate in a nondeterministic way providing the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. One more example of a system performing reordering in this way can be found in (Crego and Mariño, 2007), where syntactic structure on the source side is exploited to reorder the input of a word lattice in an unweighted manner, slightly expanding the monotonic search space. In (Tromble and Eisner, 2009) a O(n3 ) chart parsing algorithm aimed to find the best reordering of possible word permutations is described and applied for the German-English language pair. 3 Source"
2010.eamt-1.28,2002.tmi-tutorials.2,0,0.0495861,"dered. p(allows,us,”Swapped”) = pLP (allows,mogelijk) = {(pLP (allows,maakt) + pLP (allows,mogelijk)} 4.2 MSD where pLP (word1 , word2 ) is the lexical probability of translating a source-side word1 by a targetside word2 . In the following sections, we explore the impact of this hard decision in reordering accuracy and in translation quality. 4 Baseline translation system Rather than translating single words, phrase-based systems (Och and Ney, 2002) work with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a set of constraints (Zens et al., 2002). A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any aligned pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase (Och and Ney, 2004). The probability of the phrases is estimated by counts of their appearance in the training corpus. The finite, fixed inventory of phrases obtained from a parallel corpus is stored in a “phrase-table&quot;. The translation of a"
2010.eamt-1.28,P07-1037,1,0.905667,"Missing"
2012.eamt-1.63,C90-3001,0,0.0853969,"Missing"
2012.eamt-1.63,J93-2003,0,0.0340213,"rily to the meaning of a sentence. One can therefore hypothesize that this secondary status is preserved in translation, and thus that adjuncts may align consistently with their adjunct translations, suggesting they form optional phrase pairs in parallel corpora. In this paper we verify this hypothesis on French-English translation data, and explore the utility of compiling adjunct-poor data for augmenting the training data of a phrase-based machine translation model. 1 Introduction Phrase-Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003) exploits symmetrized word alignments (Brown et al., 1993) to form phrase pairs that capture the translation probabilities of idiomatic expressions. However, data sparsity is a major issue for phrase-based systems. It affects longer phrase pairs in particular, which are overestimated by the unsmoothed heuristic counts. Smoothing has been proposed to improve probability estimations in the phrase table (Kuhn et al., 2006; Foster et al., 2006), and minimal phrase pairs to alleviate data sparsity: see the tuples of Schwenk (2007) and the minimal translation units of Quirk and Menezes (2006). In both cases, these new units of translation are utilized in a"
2012.eamt-1.63,P05-1033,0,0.465329,"Missing"
2012.eamt-1.63,dorr-etal-2002-duster,0,0.0341988,"t to “il faut trouver des r`egles”. In other words, adjunct pairing can occur 288 relatively independently of the syntactical realization of the involved adjuncts and of the degree of translation equivalence of the phrases they modify. There must be rules governing existing vehicles too . Il faut aussi trouver des r`egles pour les v´ehicules existants . It is-necessary too to-find rules for the existing vehicles . Figure 1: Example sentence pair with adjunct pairs Conversely, adjuncts are not always preserved in translation. For instance, Example 1 presents a case of head swapping taken from (Dorr et al., 2002). (1) Yo entro el cuarto corriendo I enter the room running I run into the room There the manner of motion, i.e., ‘running’, is expressed by the verbal head in the English sentence and by a modifier in the Spanish sentence while the direction, i.e., ‘into’, is expressed by the head in Spanish and by a modifier in English. So, while (Dorr et al., 2002) investigated structural divergence in general and not only on modifiers, we can expect that adjuncts are not always translated as such in the target language. Another limitation on adjunct alignment is not linguistic but technical. In fact, we de"
2012.eamt-1.63,W06-1607,0,0.0243249,"poor data for augmenting the training data of a phrase-based machine translation model. 1 Introduction Phrase-Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003) exploits symmetrized word alignments (Brown et al., 1993) to form phrase pairs that capture the translation probabilities of idiomatic expressions. However, data sparsity is a major issue for phrase-based systems. It affects longer phrase pairs in particular, which are overestimated by the unsmoothed heuristic counts. Smoothing has been proposed to improve probability estimations in the phrase table (Kuhn et al., 2006; Foster et al., 2006), and minimal phrase pairs to alleviate data sparsity: see the tuples of Schwenk (2007) and the minimal translation units of Quirk and Menezes (2006). In both cases, these new units of translation are utilized in an n-gram translation model that allows to capture contextual dependencies, and their estimates are smoothed. c 2012 European Association for Machine Translation. 287 Morphology has also been proposed to reduce data sparsity, either by integrating morphological information into the translation model or as a preprocessing step. For instance, Nießen and Ney (2004) propose hierarchical l"
2012.eamt-1.63,P02-1050,0,0.825689,"Missing"
2012.eamt-1.63,P83-1002,0,0.131649,"Missing"
2012.eamt-1.63,N03-1017,0,0.00928162,"a, adjuncts are optional constituents contributing secondarily to the meaning of a sentence. One can therefore hypothesize that this secondary status is preserved in translation, and thus that adjuncts may align consistently with their adjunct translations, suggesting they form optional phrase pairs in parallel corpora. In this paper we verify this hypothesis on French-English translation data, and explore the utility of compiling adjunct-poor data for augmenting the training data of a phrase-based machine translation model. 1 Introduction Phrase-Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003) exploits symmetrized word alignments (Brown et al., 1993) to form phrase pairs that capture the translation probabilities of idiomatic expressions. However, data sparsity is a major issue for phrase-based systems. It affects longer phrase pairs in particular, which are overestimated by the unsmoothed heuristic counts. Smoothing has been proposed to improve probability estimations in the phrase table (Kuhn et al., 2006; Foster et al., 2006), and minimal phrase pairs to alleviate data sparsity: see the tuples of Schwenk (2007) and the minimal translation units of Quirk and Menezes (2006). In bo"
2012.eamt-1.63,J04-2003,0,0.0837196,"Missing"
2012.eamt-1.63,N06-1002,0,0.0169246,"on (PBSMT) (Koehn et al., 2003) exploits symmetrized word alignments (Brown et al., 1993) to form phrase pairs that capture the translation probabilities of idiomatic expressions. However, data sparsity is a major issue for phrase-based systems. It affects longer phrase pairs in particular, which are overestimated by the unsmoothed heuristic counts. Smoothing has been proposed to improve probability estimations in the phrase table (Kuhn et al., 2006; Foster et al., 2006), and minimal phrase pairs to alleviate data sparsity: see the tuples of Schwenk (2007) and the minimal translation units of Quirk and Menezes (2006). In both cases, these new units of translation are utilized in an n-gram translation model that allows to capture contextual dependencies, and their estimates are smoothed. c 2012 European Association for Machine Translation. 287 Morphology has also been proposed to reduce data sparsity, either by integrating morphological information into the translation model or as a preprocessing step. For instance, Nießen and Ney (2004) propose hierarchical lexicon models in a German-English system, with feature functions to integrate different levels of morphosyntactic abstraction, from full word forms t"
2012.eamt-1.63,W07-0412,0,0.777426,"Missing"
2012.eamt-1.63,D07-1045,0,0.0601165,"Missing"
2015.mtsummit-papers.22,P08-1087,0,0.0629855,"Missing"
2015.mtsummit-papers.22,W10-1705,0,0.123224,"ich the morphologically impoverished source side with syntactic information and translate via a factored machine translation model. In spirit, this paper is closely related to the present work; however, while their decorations are source-side syntactic information (e.g. the noun is the subject), we directly predict target morphology and learn to select the most relevant properties. A similar approach, in which source syntax is reduced to part-of-speech tags is used successfully for translation into Turkish (Yeniterzi and Oflazer, 2010). Following the tradition of two step machine translation (Bojar and Kos, 2010), Fraser et al. (2012) translate morphologically underspecified tokens and add inflections on the target side based on the predictions of discriminative classifiers. Carpuat and Wu (2007), Jeong et al. (2010), Toutanova et al. (2008) and Chahuneau et al. (2013) propose discriminative lexicon models that are able to take into account the larger context of the source sentence when making lexical choices on the target side. These proposals differ mostly in the way that the additional morphological information is integrated into the machine translation process. Jeong et al. (2010) integrate their"
2015.mtsummit-papers.22,J93-2003,0,0.0322606,"to data sparsity and will complicate the prediction task. Therefore, it is necessary to reduce this set to only morphological attributes which are helpful for a given language pair. We consider a morphological attribute to be salient if it enables the machine translation system to perform better lexical selection. It is computationally infeasible to test all possible combinations of morphological attributes in a full machine translation system; hence, we approximate the machine translation system’s ability to perform lexical selection with a word-based translation system given by IBM model 1 (Brown et al., 1993). Based on this simplified translation model, the set of salient features which improve the translation performance can be chosen by a clustering procedure. 5.1 Learning procedure Let (s, t) be a pair of parallel sentences in source and target language. IBM model 1 provides an iterative method for estimating the translation model P (t |s) from a set of parallel sentences. We add the morphological decoration s′m to this model. The translation model now takes the following form: P (t |s) = ∑ P (s′m |s)P (t |s′m ) s′m ∈Θm (s) 6 Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct"
2015.mtsummit-papers.22,E14-1061,0,0.0202373,"s. The manual and automatic selections differ mainly in the verb attributes, which our learning procedure removed from the final set. Morphological attributes in the manual selection which are marked with ‡, are attributes that in the work of Fraser et al. (2012) were transfered as part of the translated stem by their MT system. The symbol ‡ marks morphological attributes that they propagated from the noun (for example, an adjective’s case is copied from the noun it modifies). Finally, the verb attributes, which are marked with * are used by Fraser et al. (2012) but found to be problematic by Cap et al. (2014b) and dropped in later work (Cap et al., 2014a). Likewise, inspection of our model showed that verb attributes perform badly as they may be difficult to predict. Hence, our procedure successfully learnt not to model these attributes while retaining the beneficial noun and adjective attributes. Granularity of the morphological attributes When simulating the removal of a morphological attribute with this learning algorithm, all of its values are merged. In some language pairs, however, it would be useful to merge the individual values of the attributes instead. For example, from the spelling of"
2015.mtsummit-papers.22,2007.mtsummit-papers.11,0,0.0258087,"t work; however, while their decorations are source-side syntactic information (e.g. the noun is the subject), we directly predict target morphology and learn to select the most relevant properties. A similar approach, in which source syntax is reduced to part-of-speech tags is used successfully for translation into Turkish (Yeniterzi and Oflazer, 2010). Following the tradition of two step machine translation (Bojar and Kos, 2010), Fraser et al. (2012) translate morphologically underspecified tokens and add inflections on the target side based on the predictions of discriminative classifiers. Carpuat and Wu (2007), Jeong et al. (2010), Toutanova et al. (2008) and Chahuneau et al. (2013) propose discriminative lexicon models that are able to take into account the larger context of the source sentence when making lexical choices on the target side. These proposals differ mostly in the way that the additional morphological information is integrated into the machine translation process. Jeong et al. (2010) integrate their lexical selection model via features in the underlying treelet translation system (Quirk et al., 2005). Toutanova et al. (2008) survey two basic methods of integration. In the first metho"
2015.mtsummit-papers.22,N10-2003,0,0.0279034,"gainst Manual selection at p < 0.05 Table 5: Translation with predicted test decorations. At training time, only projected decorations are observed, which might not be realistic when taking into account the prediction model. 6.3 Evaluation Having introduced and evaluated the attribute selection process and the prediction of target-side morphological attributes based on source-side dependency chains, we now turn to the evaluation of the predicted morphological information within a full machine translation pipeline. Experimental details We use a standard phrase-based machine translation system (Cer et al., 2010) with a 5-gram language model and distortion-based reordering (dl=5). Features based on the source morphology predictions are learnt on either the projected morphology or the predictions of the source dependency chain model. Experiments are conducted on English– German. Source-side dependency trees are predicted based on the HamleDT treebank (Zeman et al., 2012) using TurboParser (Martins et al., 2010). The dependency parser is trained to produce pseudo-projective dependency trees (Nivre and Nilsson, 2005).5 The system is trained on the full parallel sections of Europarl (Koehn, 2005) and tune"
2015.mtsummit-papers.22,D13-1174,0,0.503919,"ource side via the word alignments, and then train a model to predict these attributes on predicate-argument aspects of source dependency trees (i.e., without the source word order). Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 283 s s′m 1. align 3. project s′m t′ t s pred. model pred. model ˆ s′m MT t MT t 2. tag (a) Morphology projection and pred. model training. (b) Machine translation system training. Figure 1: Overview of the training setup and morphology projection. Our approach differs from other approaches to predict target morphology (e.g. Chahuneau et al. (2013)) mainly in that we predict on the source side only. A related intuition underlies source-side reordering schemes, which have seen a surge of successful applications recently (e.g. Collins et al. (2005) or Lerner and Petrov (2013)). While syntax-driven source-side reordering assumes that source and target syntax are similar, here we make a weaker assumption, namely that the predicate-argument structures are similar. We explore the prediction of target morphology on the source side because we see several benefits that could potentially be exploited for further improving machine translation into"
2015.mtsummit-papers.22,P05-1066,0,0.0700872,"mit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 283 s s′m 1. align 3. project s′m t′ t s pred. model pred. model ˆ s′m MT t MT t 2. tag (a) Morphology projection and pred. model training. (b) Machine translation system training. Figure 1: Overview of the training setup and morphology projection. Our approach differs from other approaches to predict target morphology (e.g. Chahuneau et al. (2013)) mainly in that we predict on the source side only. A related intuition underlies source-side reordering schemes, which have seen a surge of successful applications recently (e.g. Collins et al. (2005) or Lerner and Petrov (2013)). While syntax-driven source-side reordering assumes that source and target syntax are similar, here we make a weaker assumption, namely that the predicate-argument structures are similar. We explore the prediction of target morphology on the source side because we see several benefits that could potentially be exploited for further improving machine translation into morphologically rich languages. Source-side prediction models can capitalize on the much reduced complexity of having to represent and process only the input source sentence instead of a large lattice"
2015.mtsummit-papers.22,W11-2107,0,0.1747,"es. In these translation systems, the target side of the test set was processed with a morphological tagger1 and subsets of the resulting morphological attributes were projected to the source side via alignments. These experiments provide a conservative indication of the potential of this approach. They are not oracle translation experiments, but simulate an optimal target morphology prediction model. The three systems listed in Table 1 differ only in the subset of morphological attributes they use. The experiment is documented in Table 1. We evaluate translation quality with METEOR and BLEU (Denkowski and Lavie, 2011; Papineni et al., 2002), word order with Kendall’s Tau (Kendall, 1938) and lexical choice with unigram BLEU. Statistical significance tests are performed for the translation scores (METEOR and BLEU) using the bootstrap resampling method (Koehn, 2004). The results show that projecting target morphological attributes improves trans1 Details of the experimental setup are provided in Section 6.3. 3 Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 285 Root Sb Peter . Adv AuxP escaped . AuxA .. from the . case=dat Peter entkam der police . case=dat Polizei F"
2015.mtsummit-papers.22,E12-1068,0,0.317303,"y impoverished source side with syntactic information and translate via a factored machine translation model. In spirit, this paper is closely related to the present work; however, while their decorations are source-side syntactic information (e.g. the noun is the subject), we directly predict target morphology and learn to select the most relevant properties. A similar approach, in which source syntax is reduced to part-of-speech tags is used successfully for translation into Turkish (Yeniterzi and Oflazer, 2010). Following the tradition of two step machine translation (Bojar and Kos, 2010), Fraser et al. (2012) translate morphologically underspecified tokens and add inflections on the target side based on the predictions of discriminative classifiers. Carpuat and Wu (2007), Jeong et al. (2010), Toutanova et al. (2008) and Chahuneau et al. (2013) propose discriminative lexicon models that are able to take into account the larger context of the source sentence when making lexical choices on the target side. These proposals differ mostly in the way that the additional morphological information is integrated into the machine translation process. Jeong et al. (2010) integrate their lexical selection mode"
2015.mtsummit-papers.22,2010.amta-papers.33,0,0.0222394,"their decorations are source-side syntactic information (e.g. the noun is the subject), we directly predict target morphology and learn to select the most relevant properties. A similar approach, in which source syntax is reduced to part-of-speech tags is used successfully for translation into Turkish (Yeniterzi and Oflazer, 2010). Following the tradition of two step machine translation (Bojar and Kos, 2010), Fraser et al. (2012) translate morphologically underspecified tokens and add inflections on the target side based on the predictions of discriminative classifiers. Carpuat and Wu (2007), Jeong et al. (2010), Toutanova et al. (2008) and Chahuneau et al. (2013) propose discriminative lexicon models that are able to take into account the larger context of the source sentence when making lexical choices on the target side. These proposals differ mostly in the way that the additional morphological information is integrated into the machine translation process. Jeong et al. (2010) integrate their lexical selection model via features in the underlying treelet translation system (Quirk et al., 2005). Toutanova et al. (2008) survey two basic methods of integration. In the first method, the inflection pre"
2015.mtsummit-papers.22,W04-3250,0,0.575698,"of the potential of this approach. They are not oracle translation experiments, but simulate an optimal target morphology prediction model. The three systems listed in Table 1 differ only in the subset of morphological attributes they use. The experiment is documented in Table 1. We evaluate translation quality with METEOR and BLEU (Denkowski and Lavie, 2011; Papineni et al., 2002), word order with Kendall’s Tau (Kendall, 1938) and lexical choice with unigram BLEU. Statistical significance tests are performed for the translation scores (METEOR and BLEU) using the bootstrap resampling method (Koehn, 2004). The results show that projecting target morphological attributes improves trans1 Details of the experimental setup are provided in Section 6.3. 3 Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 285 Root Sb Peter . Adv AuxP escaped . AuxA .. from the . case=dat Peter entkam der police . case=dat Polizei Figure 2: Morphology projection and a source dependency chain. lation. Improvements result both from better lexical choice and sometimes also better word order. Using the full set of attributes gives the best METEOR and BLEU scores, but it also contrib"
2015.mtsummit-papers.22,2005.mtsummit-papers.11,0,0.0570465,"em (Cer et al., 2010) with a 5-gram language model and distortion-based reordering (dl=5). Features based on the source morphology predictions are learnt on either the projected morphology or the predictions of the source dependency chain model. Experiments are conducted on English– German. Source-side dependency trees are predicted based on the HamleDT treebank (Zeman et al., 2012) using TurboParser (Martins et al., 2010). The dependency parser is trained to produce pseudo-projective dependency trees (Nivre and Nilsson, 2005).5 The system is trained on the full parallel sections of Europarl (Koehn, 2005) and tuned and tested on the WMT 2009 and WMT 2010 newstest sets respectively. Monolingual morphological tagging is performed using the Marmot CRF-based tagger (Mueller et al., 2013). The tagger is trained on the English and German parts of the HamleDT treebank. The morphological attributes of both languages follow the Interset standard (Zeman, 2008), which contains 45 unique attribute vectors (tags) for English and 958 for German. Discussion Table 5 shows the outcomes of using the inference strategies presented in Section 6.2. We evaluate translation quality with METEOR and BLEU (Denkowski an"
2015.mtsummit-papers.22,D13-1049,0,0.0163121,"hers' Track Miami, Oct 30 - Nov 3, 2015 |p. 283 s s′m 1. align 3. project s′m t′ t s pred. model pred. model ˆ s′m MT t MT t 2. tag (a) Morphology projection and pred. model training. (b) Machine translation system training. Figure 1: Overview of the training setup and morphology projection. Our approach differs from other approaches to predict target morphology (e.g. Chahuneau et al. (2013)) mainly in that we predict on the source side only. A related intuition underlies source-side reordering schemes, which have seen a surge of successful applications recently (e.g. Collins et al. (2005) or Lerner and Petrov (2013)). While syntax-driven source-side reordering assumes that source and target syntax are similar, here we make a weaker assumption, namely that the predicate-argument structures are similar. We explore the prediction of target morphology on the source side because we see several benefits that could potentially be exploited for further improving machine translation into morphologically rich languages. Source-side prediction models can capitalize on the much reduced complexity of having to represent and process only the input source sentence instead of a large lattice of target hypotheses. Hence,"
2015.mtsummit-papers.22,W14-3336,0,0.0206447,"achieved with a small, well-chosen set of attributes. The good performance of the manual set shows that linguistic intuition can be a good starting point for selecting this set; however, a more empirically beneficial set may be selected by enriching the source side only with attributes which help in selecting the correct target words. The fact that the automatic set produces a better METEOR score than the manual set further supports this intuition.2 We highlight the METEOR scores here, since for the language pair English-to-German, METEOR has higher correlation with human judgments than BLEU (Machacek and Bojar, 2014). Now that we established the potential of projecting target morphology on the source side, in the sequel we aim at capitalizing on this potential. In the next section, we present our model for predicting target morphology on source trees based on source side dependency chains. 4 Modeling target-side morphology Since the word order of the source and target language may differ significantly, predicting morphology in a sequential, word-by-word fashion could be inadequate. We think that source syntax and the source predicate argument structure should be informative for predicting target morpholog"
2015.mtsummit-papers.22,D10-1004,0,0.0119895,", we now turn to the evaluation of the predicted morphological information within a full machine translation pipeline. Experimental details We use a standard phrase-based machine translation system (Cer et al., 2010) with a 5-gram language model and distortion-based reordering (dl=5). Features based on the source morphology predictions are learnt on either the projected morphology or the predictions of the source dependency chain model. Experiments are conducted on English– German. Source-side dependency trees are predicted based on the HamleDT treebank (Zeman et al., 2012) using TurboParser (Martins et al., 2010). The dependency parser is trained to produce pseudo-projective dependency trees (Nivre and Nilsson, 2005).5 The system is trained on the full parallel sections of Europarl (Koehn, 2005) and tuned and tested on the WMT 2009 and WMT 2010 newstest sets respectively. Monolingual morphological tagging is performed using the Marmot CRF-based tagger (Mueller et al., 2013). The tagger is trained on the English and German parts of the HamleDT treebank. The morphological attributes of both languages follow the Interset standard (Zeman, 2008), which contains 45 unique attribute vectors (tags) for Englis"
2015.mtsummit-papers.22,D13-1032,0,0.105759,"Missing"
2015.mtsummit-papers.22,P05-1013,0,0.0223613,"ation pipeline. Experimental details We use a standard phrase-based machine translation system (Cer et al., 2010) with a 5-gram language model and distortion-based reordering (dl=5). Features based on the source morphology predictions are learnt on either the projected morphology or the predictions of the source dependency chain model. Experiments are conducted on English– German. Source-side dependency trees are predicted based on the HamleDT treebank (Zeman et al., 2012) using TurboParser (Martins et al., 2010). The dependency parser is trained to produce pseudo-projective dependency trees (Nivre and Nilsson, 2005).5 The system is trained on the full parallel sections of Europarl (Koehn, 2005) and tuned and tested on the WMT 2009 and WMT 2010 newstest sets respectively. Monolingual morphological tagging is performed using the Marmot CRF-based tagger (Mueller et al., 2013). The tagger is trained on the English and German parts of the HamleDT treebank. The morphological attributes of both languages follow the Interset standard (Zeman, 2008), which contains 45 unique attribute vectors (tags) for English and 958 for German. Discussion Table 5 shows the outcomes of using the inference strategies presented in"
2015.mtsummit-papers.22,P02-1040,0,0.0954177,"stems, the target side of the test set was processed with a morphological tagger1 and subsets of the resulting morphological attributes were projected to the source side via alignments. These experiments provide a conservative indication of the potential of this approach. They are not oracle translation experiments, but simulate an optimal target morphology prediction model. The three systems listed in Table 1 differ only in the subset of morphological attributes they use. The experiment is documented in Table 1. We evaluate translation quality with METEOR and BLEU (Denkowski and Lavie, 2011; Papineni et al., 2002), word order with Kendall’s Tau (Kendall, 1938) and lexical choice with unigram BLEU. Statistical significance tests are performed for the translation scores (METEOR and BLEU) using the bootstrap resampling method (Koehn, 2004). The results show that projecting target morphological attributes improves trans1 Details of the experimental setup are provided in Section 6.3. 3 Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 285 Root Sb Peter . Adv AuxP escaped . AuxA .. from the . case=dat Peter entkam der police . case=dat Polizei Figure 2: Morphology proj"
2015.mtsummit-papers.22,P06-1055,0,0.00740599,"s,t)∈X We found the estimates for P (s′m |s) using the full set of attributes M0 to be reasonable, with sufficient probability mass assigned to the most likely path. Therefore, we approximate this model by only using the first-best (Viterbi) assignment s′′m . The final, simplified search objective is therefore: ) ( ∑ (i) Mn = arg max log P (s′′m |s)P (t |s′′m ) Mi ⊂M0 = arg max Mi ⊂M0 (s,t)∈X ∑ log P (t |s′′m (i) ) (s,t)∈X The optimal set of attributes can now be determined with a clustering procedure starting from the full set of morphological attributes M0 . This procedure is reminiscent of Petrov et al. (2006) since as in their work, we can simulate the removal of a morphological attribute by merging the statistics of each of its occurrences.3 1. Initialization: − Estimate the source dependency chain model P (s′m |s), apply it to decorate the training and heldout set, producing T0 and H0 (datasets T and H decorated with M0 ). (0) − Estimate P (t |s′′m (0) ): perform 5 iterations of IBM Model 1 training on T0 . 2. Start with i = 0. (i) 3. Calculate P (t |s′′m ) for each sentence pair in the heldout set Hi . 3 For example, to simulate the removal of the attribute gender, we would merge the statistics"
2015.mtsummit-papers.22,P05-1034,0,0.0502073,"ections on the target side based on the predictions of discriminative classifiers. Carpuat and Wu (2007), Jeong et al. (2010), Toutanova et al. (2008) and Chahuneau et al. (2013) propose discriminative lexicon models that are able to take into account the larger context of the source sentence when making lexical choices on the target side. These proposals differ mostly in the way that the additional morphological information is integrated into the machine translation process. Jeong et al. (2010) integrate their lexical selection model via features in the underlying treelet translation system (Quirk et al., 2005). Toutanova et al. (2008) survey two basic methods of integration. In the first method, the inflection prediction model is 2 Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 284 Translation Training and test decor. None (baseline) Projected manual set Projected automatic set Projected full set Word order Lexical choice Tags MTR BLEU Kendall’s τ BLEU-1 - 35.74 15.12 45.26 49.86 77 225 846 36.34 36.50 36.67 15.86 15.73 15.96 45.79 46.45 46.27 51.30 51.24 51.52 All translation results statistically significant against baseline at p < 0.01 Table 1: Translat"
2015.mtsummit-papers.22,P08-1059,0,0.0583896,"source-side syntactic information (e.g. the noun is the subject), we directly predict target morphology and learn to select the most relevant properties. A similar approach, in which source syntax is reduced to part-of-speech tags is used successfully for translation into Turkish (Yeniterzi and Oflazer, 2010). Following the tradition of two step machine translation (Bojar and Kos, 2010), Fraser et al. (2012) translate morphologically underspecified tokens and add inflections on the target side based on the predictions of discriminative classifiers. Carpuat and Wu (2007), Jeong et al. (2010), Toutanova et al. (2008) and Chahuneau et al. (2013) propose discriminative lexicon models that are able to take into account the larger context of the source sentence when making lexical choices on the target side. These proposals differ mostly in the way that the additional morphological information is integrated into the machine translation process. Jeong et al. (2010) integrate their lexical selection model via features in the underlying treelet translation system (Quirk et al., 2005). Toutanova et al. (2008) survey two basic methods of integration. In the first method, the inflection prediction model is 2 Procee"
2015.mtsummit-papers.22,D14-1175,0,0.0438107,"Missing"
2015.mtsummit-papers.22,W11-2126,0,0.22745,"Missing"
2015.mtsummit-papers.22,P10-1047,0,0.0188445,"g between languages of varying morphological complexity. Avramidis and Koehn (2008) enrich the morphologically impoverished source side with syntactic information and translate via a factored machine translation model. In spirit, this paper is closely related to the present work; however, while their decorations are source-side syntactic information (e.g. the noun is the subject), we directly predict target morphology and learn to select the most relevant properties. A similar approach, in which source syntax is reduced to part-of-speech tags is used successfully for translation into Turkish (Yeniterzi and Oflazer, 2010). Following the tradition of two step machine translation (Bojar and Kos, 2010), Fraser et al. (2012) translate morphologically underspecified tokens and add inflections on the target side based on the predictions of discriminative classifiers. Carpuat and Wu (2007), Jeong et al. (2010), Toutanova et al. (2008) and Chahuneau et al. (2013) propose discriminative lexicon models that are able to take into account the larger context of the source sentence when making lexical choices on the target side. These proposals differ mostly in the way that the additional morphological information is integr"
2015.mtsummit-papers.22,zeman-2008-reusable,0,0.0128637,"eDT treebank (Zeman et al., 2012) using TurboParser (Martins et al., 2010). The dependency parser is trained to produce pseudo-projective dependency trees (Nivre and Nilsson, 2005).5 The system is trained on the full parallel sections of Europarl (Koehn, 2005) and tuned and tested on the WMT 2009 and WMT 2010 newstest sets respectively. Monolingual morphological tagging is performed using the Marmot CRF-based tagger (Mueller et al., 2013). The tagger is trained on the English and German parts of the HamleDT treebank. The morphological attributes of both languages follow the Interset standard (Zeman, 2008), which contains 45 unique attribute vectors (tags) for English and 958 for German. Discussion Table 5 shows the outcomes of using the inference strategies presented in Section 6.2. We evaluate translation quality with METEOR and BLEU (Denkowski and Lavie, 2011; Papineni et al., 2002), word order with Kendall’s Tau (Kendall, 1938) and lexical choice with unigram BLEU. Statistical significance tests are performed for the translation scores (METEOR and BLEU) using the bootstrap resampling method (Koehn, 2004). The results show that both attribute selections show improvements over the baseline wh"
2015.mtsummit-papers.22,zeman-etal-2012-hamledt,0,0.0287788,"Missing"
2015.tc-1.3,W15-5202,0,0.0466142,"cant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memories. 2.3 Incorporation of Language Technology in Translation Memories Translation memories are among the most successfully used tools by professional translators. However, most of these tools rely on little language processing when they match and retrieve segments. Research carried out in the EXPERT project shows that even incorporation of simple language processing such as paraphrasing can help translators (Gupta and Or˘asan, 2014). Rather than expanding the segments stored in a translation memory with all the"
2015.tc-1.3,2014.tc-1.6,1,0.723752,"metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memorie"
2015.tc-1.3,D14-1062,1,0.857312,"Missing"
2015.tc-1.3,C14-1182,1,0.796863,"Missing"
2015.tc-1.3,N15-1043,1,0.871728,"Missing"
2015.tc-1.3,2015.mtsummit-papers.22,1,0.879605,"Missing"
2015.tc-1.3,2014.eamt-1.2,0,0.0541452,"Missing"
2015.tc-1.3,W15-4905,1,0.824476,"Missing"
2015.tc-1.3,2014.amta-researchers.19,1,0.706099,"professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT, showing that for English-Chinese and English-French the proposed methods lead to better translations. Translation into morphological rich languages poses challenges to current methods in statistical machine translation. For this problem, Daiber and Sima’an (2015) propose a method which consists of two steps: first the source string is enriched with target morphological features and then fed into a translation model which takes care of reordering and lexical choice that 20 matches the provided morphologic"
2015.tc-1.3,W15-4907,1,0.836766,"by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT,"
2015.tc-1.3,P02-1040,0,0.0939051,"uggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible"
2015.tc-1.3,W15-4107,0,0.0154388,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-wptp.4,0,0.0124165,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-papers.11,0,0.0130963,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,W15-5201,0,0.0277695,"largely due to the fact that in many cases the real needs of translators were not considered when designing these tools. To this end, a survey with professional translators was carried out in order to find out their views and requirements regarding various technologies, and their current work practices. Thanks to the help of the commercial partners in the project, the survey received 736 complete responses, from a total of over 1300 responses, which is more than in other similar surveys. A first analysis of the data is presented in (Zaretskaya et al., 2015) with more analyses underway. Parra Escartín (2015) carried out another study with professional translators in an attempt to find out “missing functionalities” of translation memories that could potentially improve their productivity. An interesting feature suggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently test"
2015.tc-1.3,2014.eamt-1.21,1,0.710839,"em in translation memories and statistical machine translation. 2.4 The Human Translator in the Loop Post-editing is one of the most promising ways of integrating the output of machine translation methods in the workflows used by translation companies. Quality estimation methods are used to decide whether a sentence should be translated from scratch or it is good enough to be given to a post-editor. Most of the existing methods focus on estimating the quality of sentences, but in some cases it is necessary to estimate the quality of the translation of a whole document. The work carried out by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015)"
2015.tc-1.3,W14-3323,0,0.0605642,"Missing"
2015.tc-1.3,2015.eamt-1.6,1,\N,Missing
bastings-simaan-2014-fragments,D11-1008,0,\N,Missing
bastings-simaan-2014-fragments,J93-2004,0,\N,Missing
bastings-simaan-2014-fragments,P00-1008,0,\N,Missing
bastings-simaan-2014-fragments,J04-4004,0,\N,Missing
bastings-simaan-2014-fragments,J03-4003,0,\N,Missing
bastings-simaan-2014-fragments,P02-1040,0,\N,Missing
bastings-simaan-2014-fragments,P10-1112,0,\N,Missing
bastings-simaan-2014-fragments,P06-1055,0,\N,Missing
bastings-simaan-2014-fragments,P11-2127,0,\N,Missing
C08-1112,E03-1005,0,0.0212413,"ubstitution Grammar (STSG) at their backbone. The majority of such models belong to a Head-Driven paradigm, in which a head constituent is generated first, providing a positional anchor for subsequent (e.g., Markovian) sisters’ generation. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Constituency-based models, lexicalized and unlexicalized alike, demonstrate state-of-the-art performance for parsing English (Charniak, 1997; Collins, 2003; Klein and Manning, 2003; Bod, 2003), yet a direct application of such models to parsing less configurational languages often fails to yield comparable results. The parameters of such parsers capture generalizations that are easily stated in structural terms (e.g., subjects linearly precede predicates, VPs dominate objects, etc.) which may not be adequate for parsing languages with less configurational character. A different vein of research explores data-driven dependency-based parsing methods (e.g., (McDonald et al., 2005)) which seem to be intuitively more adequate for the task. It turns out, however, that even such models fa"
C08-1112,J03-4003,0,0.17146,"ee Grammar (PCFG) or a Stochastic Tree Substitution Grammar (STSG) at their backbone. The majority of such models belong to a Head-Driven paradigm, in which a head constituent is generated first, providing a positional anchor for subsequent (e.g., Markovian) sisters’ generation. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Constituency-based models, lexicalized and unlexicalized alike, demonstrate state-of-the-art performance for parsing English (Charniak, 1997; Collins, 2003; Klein and Manning, 2003; Bod, 2003), yet a direct application of such models to parsing less configurational languages often fails to yield comparable results. The parameters of such parsers capture generalizations that are easily stated in structural terms (e.g., subjects linearly precede predicates, VPs dominate objects, etc.) which may not be adequate for parsing languages with less configurational character. A different vein of research explores data-driven dependency-based parsing methods (e.g., (McDonald et al., 2005)) which seem to be intuitively more adequate for the task. It turns o"
C08-1112,P08-1043,1,0.228262,"(6b) recovered none of them. Both grammars make attachment mistakes internal to complex NPs, but the RR-model is better at identifying higher level constituents that correlate with meaningful grammatical functions. Our qualitative analysis suggests that our model is even more powerful than our quantitative analysis indicates, yet we leave the discussion of better ways to quantify this for future research. A Note on Related Work Studies on parsing MH to date concentrate mostly on spelling out the integration of a PCFG parser with a morphological disambiguation component (e.g., (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008)). On a setup identical to ours (gold segmentation, no PoS) the latter obtained 70pt. (Tsarfaty and Sima’an, 895 PP B.. in.. 2007) examined the contribution of horizontal and vertical conditioning to an unlexicalized MH parser and concluded that head-driven Markovization performs below the level of vertical conditioning enriched with percolated features. We do not know of existing dependency-parsers applied to parsing MH or mildly-context-sensitive broadcoverage parsers applied to parsing a Semitic language.6 To the best of our knowledge, this is the first fully generative probabilistic framew"
C08-1112,J98-4004,0,0.0831449,"ntages of the Relational-Realizational approach and its potential promise for parsing other “exotic” languages. 2 Background Recent decades have seen a surge of interest in statistical models using a body of annotated text for learning the distributions of grammatically meaningful structures, in order to assign the most likely ones to unseen sentences. Probabilistic Context Free Grammars (PCFGs) have become popular in the articulation of such models, and unlexicalized treebank grammars (or representational variations thereof) were shown to perform reasonably well on English benchmark corpora (Johnson, 1998; Klein and Manning, 2003). A major leap in the performance of PCFG-based statistical parsers has been introduced by the move towards a Head-Driven paradigm (Collins, 2003; Charniak, 1997), in which syntactic categories are enriched with head information percolated up the tree. The head-driven generation process allows one to model the relation between the information content of a constituent and the information content of its head-marked sister. At the same time, such models introduce a bias with respect to the positioning of a non-head constituent relative to its head-marked sister. The vast"
C08-1112,P03-1054,0,0.105053,"G) or a Stochastic Tree Substitution Grammar (STSG) at their backbone. The majority of such models belong to a Head-Driven paradigm, in which a head constituent is generated first, providing a positional anchor for subsequent (e.g., Markovian) sisters’ generation. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Constituency-based models, lexicalized and unlexicalized alike, demonstrate state-of-the-art performance for parsing English (Charniak, 1997; Collins, 2003; Klein and Manning, 2003; Bod, 2003), yet a direct application of such models to parsing less configurational languages often fails to yield comparable results. The parameters of such parsers capture generalizations that are easily stated in structural terms (e.g., subjects linearly precede predicates, VPs dominate objects, etc.) which may not be adequate for parsing languages with less configurational character. A different vein of research explores data-driven dependency-based parsing methods (e.g., (McDonald et al., 2005)) which seem to be intuitively more adequate for the task. It turns out, however, that even su"
C08-1112,H05-1066,0,0.0918325,"Missing"
C08-1112,W07-2220,0,0.012011,"less configurational languages often fails to yield comparable results. The parameters of such parsers capture generalizations that are easily stated in structural terms (e.g., subjects linearly precede predicates, VPs dominate objects, etc.) which may not be adequate for parsing languages with less configurational character. A different vein of research explores data-driven dependency-based parsing methods (e.g., (McDonald et al., 2005)) which seem to be intuitively more adequate for the task. It turns out, however, that even such models fail to provide the desired remedy. Recent reports by (Nivre, 2007) delineated a class of richly-inflected languages with relatively free word-order (including Greek, Basque, and Modern Standard Arabic) for which the parsers performed poorly, regardless of the parsing method used. The need for parsing methods that can effectively cope with such phenomena doesn’t seem to have been eliminated by dependency parsing — perhaps quite the contrary. The essential argument we promote here is that in order to deal with the kind of variation that is empirically observed cross-linguistically an alternative view of the generation process is required. Our Relational-Realiz"
C08-1112,W07-2219,1,0.911837,"Missing"
C08-1112,P06-3009,1,0.737153,"ly, the PCFG in (6b) recovered none of them. Both grammars make attachment mistakes internal to complex NPs, but the RR-model is better at identifying higher level constituents that correlate with meaningful grammatical functions. Our qualitative analysis suggests that our model is even more powerful than our quantitative analysis indicates, yet we leave the discussion of better ways to quantify this for future research. A Note on Related Work Studies on parsing MH to date concentrate mostly on spelling out the integration of a PCFG parser with a morphological disambiguation component (e.g., (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008)). On a setup identical to ours (gold segmentation, no PoS) the latter obtained 70pt. (Tsarfaty and Sima’an, 895 PP B.. in.. 2007) examined the contribution of horizontal and vertical conditioning to an unlexicalized MH parser and concluded that head-driven Markovization performs below the level of vertical conditioning enriched with percolated features. We do not know of existing dependency-parsers applied to parsing MH or mildly-context-sensitive broadcoverage parsers applied to parsing a Semitic language.6 To the best of our knowledge, this is the first fully g"
C08-1112,J93-2004,0,\N,Missing
C14-1182,D11-1033,0,0.660056,"mix-domain data). We report on experiments in data selection (intrinsic) and machine translation (extrinsic) on a large parallel corpus consisting of a mix of a rather diverse set of domains. Our results show that our latent domain invitation approach outperforms the existing baselines significantly. We also provide analysis of the merits of our approach relative to existing approaches. Large parallel corpora are important for training statistical MT systems. Besides size, the relevance of a parallel training corpus to the translation task at hand can be decisive for system performance, cf. (Axelrod et al., 2011; Koehn and Haddow, 2012). In this paper we look at data selection where we have access to a large parallel data repository C mix , representing a rather varied mix of domains, and we are given a sample of in-domain parallel data C in , exemplifying a target translation task. Simply concatenating C in with C mix does not always deliver best performance, because including irrelevant sentences might be more harmful than beneficial, cf. (Axelrod et al., 2011). To make the best of available data, we must select sentences from C mix for their relevance to translating sentences from C in . Axelrod e"
C14-1182,C12-1010,0,0.0919716,"Missing"
C14-1182,J93-2003,0,0.0694268,"= P (f , e, D) D∈{D1 ,D0 } P (f , e, D) P (2) 1 × P (D) × {Plm (e |D)Pt (f |e, D) + Plm (f |D)Pt (e |f , D)} 2 Viewed as learning two latent corpora C 1 and C 0 , the task is to assign every hf , ei ∈ Cmix an expected count P (Dx |f , e) that it is in C x ∈ {C0 , C1 }. Next we discuss the model components each in turn. The domain-dependent translation models Pt (· |D) can be viewed as modeling the probability that e translates as f in domain D ∈ {D0 , D1 }. Given f = f1 , f2 , . . . , fm and e = e1 , e2 , . . . , el , we assume (hidden) alignments a = a1 , a2 , . . . , am akin to IBM Model I (Brown et al., 1993): Ym  Pt (f , a |e, D) = t(fj |eaj , D) (3) m j=1 (l + 1) X Ym Xl  Pt (f |e, D) = Pt (f , a|e, D) = t(fj |ei , D). (4) m a j=1 i=0 (l + 1) 1 Earlier work on data selection exploits the contrast between in-domain and mix-domain instead of (pseudo) out-domain language models. However, the mix-domain language models trained on a mix of rather diverse set of domains could be considered kind of wide-coverage, which makes for a rather weak contrast with the in-domain language models. 1929 where t(fj |eaj , D) is the domain-dependent lexical probability of fj given eaj with respect to D. One crucia"
C14-1182,N13-1114,0,0.288664,"ions as well as translation performance than the baseline trained on the large data Cmix . 1 Invitation models of weighting and selection By now training data selection from large mix-domain data is an accepted necessity, e.g., (Axelrod et al., 2011; Gasc´o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013). Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting (Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains. Our model assigns to every sentence pair hf , ei ∈ C mix a probability as in Equation 2: P (D |f , e) = P (f , e, D) = P (f , e, D) D∈{D1 ,D0 } P (f , e, D) P (2) 1 × P (D) ×"
C14-1182,N12-1047,0,0.155539,"Missing"
C14-1182,P11-2031,0,0.136057,"Missing"
C14-1182,W11-2107,0,0.0535693,"Missing"
C14-1182,P13-2119,0,0.166196,"ge mix-domain data is an accepted necessity, e.g., (Axelrod et al., 2011; Gasc´o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013). Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting (Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains. Our model assigns to every sentence pair hf , ei ∈ C mix a probability as in Equation 2: P (D |f , e) = P (f , e, D) = P (f , e, D) D∈{D1 ,D0 } P (f , e, D) P (2) 1 × P (D) × {Plm (e |D)Pt (f |e, D) + Plm (f |D)Pt (e |f , D)} 2 Viewed as learning two latent corpora C 1 and C 0 , the task is to assign every hf , ei ∈ Cmix an expected count P (Dx |f"
C14-1182,P12-2023,0,0.0399185,"line trained on the large data Cmix . 1 Invitation models of weighting and selection By now training data selection from large mix-domain data is an accepted necessity, e.g., (Axelrod et al., 2011; Gasc´o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013). Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting (Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains. Our model assigns to every sentence pair hf , ei ∈ C mix a probability as in Equation 2: P (D |f , e) = P (f , e, D) = P (f , e, D) D∈{D1 ,D0 } P (f , e, D) P (2) 1 × P (D) × {Plm (e |D)Pt (f |e, D) + Plm (f |D)Pt (e |f , D)} 2 Vie"
C14-1182,W07-0717,0,0.0121643,"lts show that our Invitation model gives far better selections as well as translation performance than the baseline trained on the large data Cmix . 1 Invitation models of weighting and selection By now training data selection from large mix-domain data is an accepted necessity, e.g., (Axelrod et al., 2011; Gasc´o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013). Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting (Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains. Our model assigns to every sentence pair hf , ei ∈ C mix a probability as in Equation 2: P (D |f , e) = P (f , e, D) ="
C14-1182,D10-1044,0,0.0376427,"; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013). Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting (Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains. Our model assigns to every sentence pair hf , ei ∈ C mix a probability as in Equation 2: P (D |f , e) = P (f , e, D) = P (f , e, D) D∈{D1 ,D0 } P (f , e, D) P (2) 1 × P (D) × {Plm (e |D)Pt (f |e, D) + Plm (f |D)Pt (e |f , D)} 2 Viewed as learning two latent corpora C 1 and C 0 , the task is to assign every hf , ei ∈ Cmix an expected count P (Dx |f , e) that it is in C x ∈ {C0 , C1 }. Next we discuss the model components each in turn. The doma"
C14-1182,E12-1016,0,0.0608565,"Missing"
C14-1182,W12-3154,0,0.0604104,"we look at data selection where we have access to a large parallel data repository C mix , representing a rather varied mix of domains, and we are given a sample of in-domain parallel data C in , exemplifying a target translation task. Simply concatenating C in with C mix does not always deliver best performance, because including irrelevant sentences might be more harmful than beneficial, cf. (Axelrod et al., 2011). To make the best of available data, we must select sentences from C mix for their relevance to translating sentences from C in . Axelrod et al. (2011) and follow-up work, e.g., (Haddow and Koehn, 2012; Koehn and Haddow, 2012), select sentence pairs in C mix using the cross-entropy difference between in- and mix-domain language models, both source and target sides, a modification of the Moore and Lewis method (Moore and Lewis, 2010). In the translation context, however, often a source phrase has different senses/translations in different domains, which cannot be distinguished with monolingual language models. The dependence of translation choice on domain suggests that the word alignments themselves can better be conditioned on domain information. However, in the data selection setting, cor"
C14-1182,W12-3139,0,0.0247837,"report on experiments in data selection (intrinsic) and machine translation (extrinsic) on a large parallel corpus consisting of a mix of a rather diverse set of domains. Our results show that our latent domain invitation approach outperforms the existing baselines significantly. We also provide analysis of the merits of our approach relative to existing approaches. Large parallel corpora are important for training statistical MT systems. Besides size, the relevance of a parallel training corpus to the translation task at hand can be decisive for system performance, cf. (Axelrod et al., 2011; Koehn and Haddow, 2012). In this paper we look at data selection where we have access to a large parallel data repository C mix , representing a rather varied mix of domains, and we are given a sample of in-domain parallel data C in , exemplifying a target translation task. Simply concatenating C in with C mix does not always deliver best performance, because including irrelevant sentences might be more harmful than beneficial, cf. (Axelrod et al., 2011). To make the best of available data, we must select sentences from C mix for their relevance to translating sentences from C in . Axelrod et al. (2011) and follow-u"
C14-1182,W07-0733,0,0.393433,"nts over the task. The results show that our Invitation model gives far better selections as well as translation performance than the baseline trained on the large data Cmix . 1 Invitation models of weighting and selection By now training data selection from large mix-domain data is an accepted necessity, e.g., (Axelrod et al., 2011; Gasc´o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013). Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting (Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains. Our model assigns to every sentence pair hf , ei ∈ C mix a probability as in Equation 2: P (D |f"
C14-1182,P07-2045,0,0.0159584,"Missing"
C14-1182,W04-3250,0,0.113403,"Missing"
C14-1182,2005.mtsummit-papers.11,0,0.0515053,"r varied set of domains (a 3 Note that in practice, we usually use only one iteration to train IBM Model I. To simplify the implementation, we ignore  factor (l+1) m in the model (Equation 3), which serves a minor role. It should be also noted that we set a (small) threshold, e.g., t(·|·, ·) = 0.0001 for all word pairs that do not occur in the in-domain corpus to avoid over-fitting. 1931 haystack) in a way that allows us to directly measure selection quality. Starting out from a generaldomain corpus C g consisting of 4.51M sentence pairs, collected from multiple resources including EuroParl (Koehn, 2005), Common Crawl Corpus, UN Corpus, News Commentary, TAUS Software, TAUS Hardware, and TAUS Pharmacy, and a 177K in-domain (TAUS Legal) sentence pairs. We create C mix by selecting an arbitrary 100K pairs of in-domain set and adding them to C g ; the remaining 77K in-domain pairs constitute C in . We think of this as hiding in-domain data in C mix so we can evaluate our ability to retrieve it; in this setting we can evaluate selection directly using pseudoprecision/recall defined as the percentage of selected in-domain pairs to the total selected or to the hidden 100K pairs respectively. Table 1"
C14-1182,D09-1074,0,0.0493508,"011; Gasc´o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013). Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting (Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains. Our model assigns to every sentence pair hf , ei ∈ C mix a probability as in Equation 2: P (D |f , e) = P (f , e, D) = P (f , e, D) D∈{D1 ,D0 } P (f , e, D) P (2) 1 × P (D) × {Plm (e |D)Pt (f |e, D) + Plm (f |D)Pt (e |f , D)} 2 Viewed as learning two latent corpora C 1 and C 0 , the task is to assign every hf , ei ∈ Cmix an expected count P (Dx |f , e) that it is in C x ∈ {C0 , C1 }. Next we discuss the model components"
C14-1182,P10-2041,0,0.693378,". Simply concatenating C in with C mix does not always deliver best performance, because including irrelevant sentences might be more harmful than beneficial, cf. (Axelrod et al., 2011). To make the best of available data, we must select sentences from C mix for their relevance to translating sentences from C in . Axelrod et al. (2011) and follow-up work, e.g., (Haddow and Koehn, 2012; Koehn and Haddow, 2012), select sentence pairs in C mix using the cross-entropy difference between in- and mix-domain language models, both source and target sides, a modification of the Moore and Lewis method (Moore and Lewis, 2010). In the translation context, however, often a source phrase has different senses/translations in different domains, which cannot be distinguished with monolingual language models. The dependence of translation choice on domain suggests that the word alignments themselves can better be conditioned on domain information. However, in the data selection setting, corpus C mix often does not contain useful domain markers, and C in contains only a small sample of in-domain sentence pairs. In this paper we present a latent domain translation model which weights every sentence pair hf , ei ∈ Cmix with"
C14-1182,W08-0320,0,0.0177025,"e data Cmix Subset of 300K Entry φ(e|f ) φ(f |e) φ(e|f ) φ(f |e) supervisar´a monitor oversee 0.002 0.020 0.119 0.081 0.012 0.024 0.203 0.072 evaluaci´on evaluation assessment 0.579 0.429 0.391 0.403 0.487 0.357 0.338 0.417 abstendr´a de refrain from abstain 0.002 0.013 0.014 0.060 0.015 − 0.143 − Table 8: Phrase entry examples. Note that the system trained on the subset of top 300K pairs of sentences does not contain the phrase pair hrefrain from-abstaini. 5 Final Machine Translation experiments: Putting all data together For final adaptation evaluations we follow (Koehn and Schroeder, 2007; Nakov, 2008) and (Axelrod et al., 2011; Sennrich, 2012), by passing multiple phrase tables directly to the Moses decoder and tuning a system using these different tables together. Table 9 presents the result, showing the consistent improvement of adaptation with Invitation model compared to the baselines (with p-value = 0.0001 for all cases) over the mixture data Cmix . Data 50K 100K 150K 200K 250K 300K System In-domain + CE Difference (source side) + CE Difference (target side) + Bilingual CE Difference + Invitation + CE Difference (source side) + CE Difference (target side) + Bilingual CE Difference + I"
C14-1182,J03-1002,0,0.0333868,"Missing"
C14-1182,P02-1040,0,0.0894259,"Missing"
C14-1182,P11-1027,0,0.0313346,"presents the results showing substantial improvement in selection performance compared to all the baselines. Subsequently we build SMT systems over the selected subsets. We report the translation yielded by these systems over the task in Table 2 as well. It can be easily seen that the baseline approaches that simply train on in- and mix-domain data do not work that well for a difficult selection task from a mix-domain corpus consisting of a large and rather diverse set of domains. The SMT sys4 To train the LM probs, we construct interpolated 4-gram Kneser-Ney language models using BerkeleyLM (Pauls and Klein, 2011). This setting for training language models is used for all experiments in this work. 5 The script we use to train these models is developed by Luke Orland and available at: https://github.com/ lukeorland/moore_and_lewis_data_selection. 6 Note that metric scores for the systems are averages over multiple runs. 1932 Cut-off 50K 100K 150K 200K 250K 300K Model CE Difference (source side) CE Difference (target side) Bilingual CE Difference Invitation CE Difference (source side) CE Difference (target side) Bilingual CE Difference Invitation CE Difference (source side) CE Difference (target side"
C14-1182,E12-1055,0,0.570572,"ation model gives far better selections as well as translation performance than the baseline trained on the large data Cmix . 1 Invitation models of weighting and selection By now training data selection from large mix-domain data is an accepted necessity, e.g., (Axelrod et al., 2011; Gasc´o et al., 2012; Haddow and Koehn, 2012; Banerjee et al., 2012; Irvine et al., 2013). Data selection has a different (but complementary) goal than domain adaptation, which aims at adapting an existing out-domain system by focusing on, e.g., translation model (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012), reordering model (Chen et al., 2013) and/or language model adaptation (Eidelman et al., 2012). Our setting is in line with data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), and is somewhat related to phrase pair weighting (Matsoukas et al., 2009; Foster et al., 2010). In this paper we explicitly draw attention to the special case of a mix-domain parallel corpus consisting of a large and rather diverse set of domains. Our model assigns to every sentence pair hf , ei ∈ C mix a probability as in Equation 2: P (D |f , e) = P (f , e, D) = P (f , e, D) D∈{D"
C14-1182,2006.amta-papers.25,0,0.107673,"Missing"
C14-1182,Q13-1035,0,\N,Missing
C16-1204,W10-1749,0,0.0259193,"H AMMING(π) = brackets as in h2, 4, 3, 1i. Given a permutation π over n [1..n], the notation πi (1 ≤ i ≤ n) stands for the inteP 2 3 n i=1 (πi −i) ger in the ith position in π; π(i) stands for the index S PEARMAN(π) = 1 − 2 −1) n(n of the position in π where integer i appears; and πij stands for the (contiguous) sub-sequence of integers LCS(π,I Dn 1 )−1 U LAM(π) = n−1 πi , . . . πj . The length of π is simply |π |= n. The baselines are the existing metrics over permuc−1 F UZZY(π) = 1 − n−1 tations, including K ENDALL’s tau, H AMMING and U LAM where c is # of monotone sub-permutations used in (Birch and Osborne, 2010; Birch and Osborne, 2011; Birch et al., 2010; Isozaki et al., 2010); Figure 2: Common metrics over permutations S PEARMAN rho used in (Isozaki et al., 2010); and F UZZY Reordering Score used in (Talbot et al., 2011), which is a reordering measure extracted from 2 Also known as simple or non-decomposable (Brignall, 2010) – note the analogy with prime numbers. 2165 h2, 4, 1, 3i h2, 4, 1, 3i &lt;2,4,5,6,1,3> 2 4 5 6 1 3 (a) Original permutation 2 h1, 2, 3i 4 5 1 2 h1, 2i 1 h2, 4, 1, 3i 3 4 h1, 2i 3 6 (b) Factorization step 2 h1, 2i 1 3 h1, 2i 6 5 6 4 5 (c) PET (d) Another PET Figure 3: Permutation"
C16-1204,P11-1103,0,0.0553557,"fic reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman and Hamming measure correct individual position or correct relative pairs, whereas Ulam and Fuzzy Score measure monotone units (contiguous or not). A word order metric measures how similar a permutation is to the monotone (or identity) permutation. Here we advocate the idea that a suitable metric must also assign similar values to similar permutations. Crucially, factorizing a permutation into a Permutation Tree (PET) reveals its atomic building bl"
C16-1204,D08-1078,0,0.0330745,"equacy (semantic). Conceivably, MT system developers could use diagnostic tools based on metrics dedicated to each factor separately. Word order metrics are frequently used to evaluate pre-ordering components, e.g., (Herrmann et al., 2011; Bisazza and Federico, 2013), or for analyzing specific reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman and Hamming measure correct individual position or correct relative pairs, whereas Ulam and Fuzzy Score measure monotone units (contiguous or not). A word o"
C16-1204,Q13-1027,0,0.0273621,"class of metrics implementing this idea. Subsequently we define example tight metrics and empirically test them in word order evaluation. Experiments on the WMT13 data sets for ten language pairs show that a tight metric is more often than not better than the baselines. 1 Introduction MT evaluation involves at least two factors, word order (syntactic) and adequacy (semantic). Conceivably, MT system developers could use diagnostic tools based on metrics dedicated to each factor separately. Word order metrics are frequently used to evaluate pre-ordering components, e.g., (Herrmann et al., 2011; Bisazza and Federico, 2013), or for analyzing specific reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kenda"
C16-1204,2012.eamt-1.42,0,0.0171973,"tion. Experiments on the WMT13 data sets for ten language pairs show that a tight metric is more often than not better than the baselines. 1 Introduction MT evaluation involves at least two factors, word order (syntactic) and adequacy (semantic). Conceivably, MT system developers could use diagnostic tools based on metrics dedicated to each factor separately. Word order metrics are frequently used to evaluate pre-ordering components, e.g., (Herrmann et al., 2011; Bisazza and Federico, 2013), or for analyzing specific reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuz"
C16-1204,J07-2003,0,0.183602,"Missing"
C16-1204,D11-1018,0,0.0153372,"ter than the baselines. 1 Introduction MT evaluation involves at least two factors, word order (syntactic) and adequacy (semantic). Conceivably, MT system developers could use diagnostic tools based on metrics dedicated to each factor separately. Word order metrics are frequently used to evaluate pre-ordering components, e.g., (Herrmann et al., 2011; Bisazza and Federico, 2013), or for analyzing specific reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman and Hamming measure correct individual position or"
C16-1204,W11-2107,0,0.229896,"., 2010; Isozaki et al., 2010); Figure 2: Common metrics over permutations S PEARMAN rho used in (Isozaki et al., 2010); and F UZZY Reordering Score used in (Talbot et al., 2011), which is a reordering measure extracted from 2 Also known as simple or non-decomposable (Brignall, 2010) – note the analogy with prime numbers. 2165 h2, 4, 1, 3i h2, 4, 1, 3i &lt;2,4,5,6,1,3> 2 4 5 6 1 3 (a) Original permutation 2 h1, 2, 3i 4 5 1 2 h1, 2i 1 h2, 4, 1, 3i 3 4 h1, 2i 3 6 (b) Factorization step 2 h1, 2i 1 3 h1, 2i 6 5 6 4 5 (c) PET (d) Another PET Figure 3: Permutation factorization leading to PETs METEOR (Denkowski and Lavie, 2011). Figure 2 lists the definitions of these metrics. In these definitions, LCS stands for Longest Common Subsequence, Kronecker δ[a] which is 1 if (a = true) else zero, and I Dn1 = h1, · · · , ni which is the identity permutation over [1..n]. Next we present an alternative view of permutations. 3 Factorization and order complexity In factorization we seek to decompose a permutation to reveal a tree of its atomic ordering patterns. Figure 3 shows the factorization process applied to π = h2, 4, 5, 6, 1, 3i. It starts out by representing π as a tree with root decorated with π itself (Figure 3a). In"
C16-1204,D11-1079,0,0.0177176,"tight metric is more often than not better than the baselines. 1 Introduction MT evaluation involves at least two factors, word order (syntactic) and adequacy (semantic). Conceivably, MT system developers could use diagnostic tools based on metrics dedicated to each factor separately. Word order metrics are frequently used to evaluate pre-ordering components, e.g., (Herrmann et al., 2011; Bisazza and Federico, 2013), or for analyzing specific reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman a"
C16-1204,P06-2036,0,0.301306,"., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman and Hamming measure correct individual position or correct relative pairs, whereas Ulam and Fuzzy Score measure monotone units (contiguous or not). A word order metric measures how similar a permutation is to the monotone (or identity) permutation. Here we advocate the idea that a suitable metric must also assign similar values to similar permutations. Crucially, factorizing a permutation into a Permutation Tree (PET) reveals its atomic building blocks, called primal permutations (Albert and Atkinson, 2005; Gildea et al., 2006). In this view, permutations that factorize into similar PETs should be similar. Some previous work (Stanojevi´c and Sima’an, 2014a; Stanojevi´c and Sima’an, 2014b) has used PETs for evaluation, but without attempting to explain the effect of factorization. Next we motivate the idea that, all other things being equal, the more factorizable a permutation the simpler it is in terms of ordering. Informally, a PET for permutation π is a tree where the nodes are labeled with operators (Figure 1). The fringe of every subtree in a PET is a sub-permutation of π, i.e., a contiguous sub-sequence isomorp"
C16-1204,J09-4009,0,0.060634,"Missing"
C16-1204,D10-1092,0,0.143774,"azza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman and Hamming measure correct individual position or correct relative pairs, whereas Ulam and Fuzzy Score measure monotone units (contiguous or not). A word order metric measures how similar a permutation is to the monotone (or identity) permutation. Here we advocate the idea that a suitable metric must also assign similar values to similar permutations. Crucially, factorizing a permutation into a Permutation Tree (PET) reveals its atomic building blocks, called primal permutations"
C16-1204,D11-1017,0,0.0139859,"troduction MT evaluation involves at least two factors, word order (syntactic) and adequacy (semantic). Conceivably, MT system developers could use diagnostic tools based on metrics dedicated to each factor separately. Word order metrics are frequently used to evaluate pre-ordering components, e.g., (Herrmann et al., 2011; Bisazza and Federico, 2013), or for analyzing specific reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman and Hamming measure correct individual position or correct relative pairs,"
C16-1204,J06-4002,0,0.0488186,"nalyzing specific reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman and Hamming measure correct individual position or correct relative pairs, whereas Ulam and Fuzzy Score measure monotone units (contiguous or not). A word order metric measures how similar a permutation is to the monotone (or identity) permutation. Here we advocate the idea that a suitable metric must also assign similar values to similar permutations. Crucially, factorizing a permutation into a Permutation Tree (PET) reve"
C16-1204,W13-2202,0,0.0257519,"nodes receive operator length expressed as monotone decreasing in the Catalan number. M AX|Op |(π) is one minus the normalized maximum operator length in a PET of π (normalized by the range of lengths, i.e., [2..n]). Having defined tight and semi-tight metrics, next we will evaluate these metrics against a gold standard: human judgements in MT. 7 Experimental setting Data We use human rankings of translations from WMT13 (Bojar et al., 2013) for ten language pairs with a diverse set of MT systems. Meta-evaluation We conduct system level meta-evaluation by following the method used in (Mach´acˇ ek and Bojar, 2013). All MT systems were first ranked by the ratio of the times they were judged to be better than some other system. All the metrics that we tested compute system level scores for the same systems and then we rank systems by that score (per each metric). The rankings that are 2169 English-Czech English-Russian English-French English-Spanish English-German U LAM 0.868 ± 0.033 0.849 ± 0.03 0.852 ± 0.029 0.854 ± 0.03 0.851 ± 0.029 0.511 ± 0.056 0.511 ± 0.039 0.508 ± 0.041 0.498 ± 0.044 0.507 ± 0.041 0.911 ± 0.016 0.907 ± 0.014 0.907 ± 0.014 0.92 ± 0.014 0.914 ± 0.014 0.806 ± 0.056 0.844 ± 0.076 0.8"
C16-1204,D12-1077,0,0.0143978,"re often than not better than the baselines. 1 Introduction MT evaluation involves at least two factors, word order (syntactic) and adequacy (semantic). Conceivably, MT system developers could use diagnostic tools based on metrics dedicated to each factor separately. Word order metrics are frequently used to evaluate pre-ordering components, e.g., (Herrmann et al., 2011; Bisazza and Federico, 2013), or for analyzing specific reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman and Hamming measure co"
C16-1204,W14-4017,1,0.833663,"Missing"
C16-1204,D14-1025,1,0.900263,"Missing"
C16-1204,W11-2102,0,0.10262,"es include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch et al., 2010) and Fuzzy Score (Talbot et al., 2011). Approximately, Kendall’s tau, Spearman and Hamming measure correct individual position or correct relative pairs, whereas Ulam and Fuzzy Score measure monotone units (contiguous or not). A word order metric measures how similar a permutation is to the monotone (or identity) permutation. Here we advocate the idea that a suitable metric must also assign similar values to similar permutations. Crucially, factorizing a permutation into a Permutation Tree (PET) reveals its atomic building blocks, called primal permutations (Albert and Atkinson, 2005; Gildea et al., 2006). In this view, permutatio"
C16-1204,D07-1078,0,0.0488991,"Missing"
C16-1204,J97-3002,0,0.653559,"Missing"
C16-1204,W11-1007,0,0.0280128,"in word order evaluation. Experiments on the WMT13 data sets for ten language pairs show that a tight metric is more often than not better than the baselines. 1 Introduction MT evaluation involves at least two factors, word order (syntactic) and adequacy (semantic). Conceivably, MT system developers could use diagnostic tools based on metrics dedicated to each factor separately. Word order metrics are frequently used to evaluate pre-ordering components, e.g., (Herrmann et al., 2011; Bisazza and Federico, 2013), or for analyzing specific reordering phenomena, e.g., (Bisazza and Federico, 2013; Xiang et al., 2011; Braune et al., 2012). Other uses include, ordering component tuning, e.g., (Gao et al., 2011; Neubig et al., 2012; DeNero and Uszkoreit, 2011; Katz-Brown et al., 2011; Hall et al., 2011), measuring divergence between languages (Birch et al., 2008), and matching gene sequences in bioinformatics (Eres et al., 2004). For evaluating word order, a permutation is induced between a system output and the corresponding reference translation. Existing work uses metrics over permutations such as Kendall’s tau (Lapata, 2006; Birch and Osborne, 2011), Spearman (Isozaki et al., 2010), Hamming, Ulam (Birch"
C16-1204,W07-0404,0,0.0276614,"i h2, 4, 1, 3i 3 5 7 4 h2, 1i h1, 2i 1 2 6 (a) PET h5, 7, 4, 6, 3, 1, 2i. 4 h2, 1i h2, 1i 3 4 h2, 1i 2 1 h2, 1i h2, 1i h2, 1i 3 1 h2, 1i h2, 1i h2, 1i 4 2 3 1 h2, 1i h2, 1i 4 2 2 h2, 1i 1 h2, 1i 4 3 (b) Five different PETs for π = h4, 3, 2, 1i. 1 3 × h2, 1i h2, 1i 3 2 4 3 2 1 (c) Flattened PET Figure 5: In 5a and 5b PETs for different permutations. In 5c a flattened PET for the five in 5b. 5 From permutation trees to forests For many computational purposes, a single canonical PET is sufficient, cf. (Gildea et al., 2006). A single PET can be computed in linear-time, cf. (Uno and Yagiura, 2000; Zhang and Gildea, 2007). Crucial for efficiency is the uniqueness of the sub-permutations for factorizations of arity a ≥ 4 (see Albert and Atkinson’s result – Section 3), i.e., there is a single choice for a set of split points between adjacent sub-permutations. For arity a = 2, there are at most (n − 1) choices for a single split point, if |π |= n. This is also crucial for our Permutation Forest algorithm defined next. Some permutations factorize into multiple alternative PETs (see Figure 5b). The alternative PETs of π can be packed into an O(n2 ) permutation forest (PEF). Flattened PET In a PET, chains of binary"
C16-1298,J16-2001,0,0.129525,"Missing"
C16-1298,N12-1047,0,0.040367,"Missing"
C16-1298,P11-2031,0,0.0545751,"Missing"
C16-1298,P05-1066,0,0.0767091,"rov, 2013; Jehl et al., 2014). Preordering in syntax-based models (whether dependency or constituency) is done on the local level where for each constituent (or head word) the classifier decides how the children (or dependent words) should be reordered. Employing classifiers to make local decisions on each tree node is one machine learning approach to solving this problem. An alternative to employing machine learning techniques is the use of linguistic knowledge that can in some cases give clear rules for the reordering of children in the tree. An early example of rule-based preordering is by Collins et al. (2005), who develop linguistically justified rules for preordering German into English word order. Similar in spirit but much simpler is the approach of Isozaki et al. (2010), who exploit the fact that Japanese word order is in large part the mirror image of English word order—the heads of constituents in English are in final position while in Japanese they are in initial position. Preordering English sentences into Japanese word order thus only involves two simple steps: (1) Finding the parse tree of the English sentence (the authors used HPSG derivations) and (2) moving the head of each constituen"
C16-1298,W16-2213,1,0.80922,"Missing"
C16-1298,N15-1105,0,0.0328321,"Missing"
C16-1298,W14-0313,0,0.0317821,"Missing"
C16-1298,graca-etal-2008-building,0,0.0595527,"Missing"
C16-1298,P13-2121,0,0.0469064,"Missing"
C16-1298,W10-1736,0,0.0233085,"rd) the classifier decides how the children (or dependent words) should be reordered. Employing classifiers to make local decisions on each tree node is one machine learning approach to solving this problem. An alternative to employing machine learning techniques is the use of linguistic knowledge that can in some cases give clear rules for the reordering of children in the tree. An early example of rule-based preordering is by Collins et al. (2005), who develop linguistically justified rules for preordering German into English word order. Similar in spirit but much simpler is the approach of Isozaki et al. (2010), who exploit the fact that Japanese word order is in large part the mirror image of English word order—the heads of constituents in English are in final position while in Japanese they are in initial position. Preordering English sentences into Japanese word order thus only involves two simple steps: (1) Finding the parse tree of the English sentence (the authors used HPSG derivations) and (2) moving the head of each constituent to the initial position. However, this approach does not seem to scale up easily because manually encoding reordering rules for all the world’s language pairs would b"
C16-1298,E14-1026,0,0.0378304,"Missing"
C16-1298,P07-2045,0,0.00694725,"Missing"
C16-1298,2005.mtsummit-papers.11,0,0.0252774,"Missing"
C16-1298,D13-1049,0,0.0205327,"condly, they enable building more compact preordering models that should generalize to a broad set of target languages and which potentially apply for the low resource setting where no or little parallel data is available for a specific target language. 2 Related Work The most basic usage of linguistic knowledge in preordering is in restricting the search space of possible reorderings by using syntactic parse trees. Earlier work was done mostly on constituency trees (Khalilov and Sima’an, 2012; Xia and McCord, 2004) while more recent versions of preordering models mostly use dependency trees (Lerner and Petrov, 2013; Jehl et al., 2014). Preordering in syntax-based models (whether dependency or constituency) is done on the local level where for each constituent (or head word) the classifier decides how the children (or dependent words) should be reordered. Employing classifiers to make local decisions on each tree node is one machine learning approach to solving this problem. An alternative to employing machine learning techniques is the use of linguistic knowledge that can in some cases give clear rules for the reordering of children in the tree. An early example of rule-based preordering is by Collins e"
C16-1298,D11-1006,0,0.0443845,"by monotone translation (no reordering inside the decoder), our experiments show that this pipeline gives comparable or improved translation performance with a phrase-based baseline for a large number of language pairs (12 out of 22) from diverse language families. 1 Introduction Various linguistic theories and typological studies suggest that languages often share a number of properties and that their differences fall into a small set of parameter settings (Chomsky, 1965; Greenberg, 1966; Comrie, 1981). While this intuition has influenced work on multilingual parsing (Zeman and Resnik, 2008; McDonald et al., 2011), it has found less practical use in other areas of natural language processing, such as the task of machine translation. In machine translation, significant word order differences between languages often constitute a challenge to translation systems. Word order differences are frequently given special treatment, such as in the case of preordering (Xia and McCord, 2004; Neubig et al., 2012; Stanojevi´c and Sima’an, 2015, inter alia), which is a technique heavily used in practice as a means to improve both translation quality and efficiency. In preordering, word order is predicted based on manu"
C16-1298,D12-1077,0,0.0200368,"that their differences fall into a small set of parameter settings (Chomsky, 1965; Greenberg, 1966; Comrie, 1981). While this intuition has influenced work on multilingual parsing (Zeman and Resnik, 2008; McDonald et al., 2011), it has found less practical use in other areas of natural language processing, such as the task of machine translation. In machine translation, significant word order differences between languages often constitute a challenge to translation systems. Word order differences are frequently given special treatment, such as in the case of preordering (Xia and McCord, 2004; Neubig et al., 2012; Stanojevi´c and Sima’an, 2015, inter alia), which is a technique heavily used in practice as a means to improve both translation quality and efficiency. In preordering, word order is predicted based on manually created rules or based on statistical models estimated on word-aligned training data exploiting only source language features. This approach works well for some language pairs, however it usually demands a separate, dedicated preordering model for every source-target language pair, trained on a word-aligned corpus specific for the particular language pair. But if the similarities and"
C16-1298,J03-1002,0,0.0183458,"Missing"
C16-1298,P15-2034,0,0.172474,"Missing"
C16-1298,P06-1146,0,0.0634583,"Missing"
C16-1298,D15-1005,1,0.879509,"Missing"
C16-1298,tiedemann-2012-parallel,0,0.0459688,"Missing"
C16-1298,D13-1140,0,0.0615093,"Missing"
C16-1298,C04-1073,0,0.25476,"ber of properties and that their differences fall into a small set of parameter settings (Chomsky, 1965; Greenberg, 1966; Comrie, 1981). While this intuition has influenced work on multilingual parsing (Zeman and Resnik, 2008; McDonald et al., 2011), it has found less practical use in other areas of natural language processing, such as the task of machine translation. In machine translation, significant word order differences between languages often constitute a challenge to translation systems. Word order differences are frequently given special treatment, such as in the case of preordering (Xia and McCord, 2004; Neubig et al., 2012; Stanojevi´c and Sima’an, 2015, inter alia), which is a technique heavily used in practice as a means to improve both translation quality and efficiency. In preordering, word order is predicted based on manually created rules or based on statistical models estimated on word-aligned training data exploiting only source language features. This approach works well for some language pairs, however it usually demands a separate, dedicated preordering model for every source-target language pair, trained on a word-aligned corpus specific for the particular language pair. But if"
C16-1298,I08-3008,0,0.0958818,"or preordering followed by monotone translation (no reordering inside the decoder), our experiments show that this pipeline gives comparable or improved translation performance with a phrase-based baseline for a large number of language pairs (12 out of 22) from diverse language families. 1 Introduction Various linguistic theories and typological studies suggest that languages often share a number of properties and that their differences fall into a small set of parameter settings (Chomsky, 1965; Greenberg, 1966; Comrie, 1981). While this intuition has influenced work on multilingual parsing (Zeman and Resnik, 2008; McDonald et al., 2011), it has found less practical use in other areas of natural language processing, such as the task of machine translation. In machine translation, significant word order differences between languages often constitute a challenge to translation systems. Word order differences are frequently given special treatment, such as in the case of preordering (Xia and McCord, 2004; Neubig et al., 2012; Stanojevi´c and Sima’an, 2015, inter alia), which is a technique heavily used in practice as a means to improve both translation quality and efficiency. In preordering, word order is"
C96-2215,1993.iwpt-1.2,0,0.0709887,"Missing"
C96-2215,E95-1015,0,0.0264121,"et.ruu.nl. Abstract This paper studies the computational complexity of disambiguation under probabilistic tree-grammars as in (Bod, 1992; Schabes and Waters, 1993). It presents a proof that the following problems are NP-hard: computing the Most Probable Parse from a sentence or from a word-graph, and computing the Most Probable Sentence (MPS) from a wordgraph. The NP-hardness of computing the MPS from a word-graph also holds for Stochastic Context-Free G r a m mars (SCFGs). 1 Motivation Most Probable Parse ( M P P ) . The problem of computing the M P P in the DOP framework was put forward in (Bod, 1995). The solution which Bod proposes is Monte-Carlo estimation (Bod, 11993), which is essentially repeated random-sampling for minimizing errorrate. A Viterbi-style optimization for computing the M P P under I)OP is presented in (Sima'an et al., 1994), but it does not guarantee deterministic polynomial-time complexity. In this paper we present a proof that computing the M P P under the above mentioned stochastic tree grammars is NP-hard. Note that for computing the MPD there are deterministic polynomial-time algorithms (Schabes and Waters, 1993; Sima'an, 1996) 1. Another problem that turns out al"
C96-2215,C92-2065,0,0.0639884,"Missing"
C96-2215,1993.iwpt-1.20,0,0.522208,"em of computing the M P P in the DOP framework was put forward in (Bod, 1995). The solution which Bod proposes is Monte-Carlo estimation (Bod, 11993), which is essentially repeated random-sampling for minimizing errorrate. A Viterbi-style optimization for computing the M P P under I)OP is presented in (Sima'an et al., 1994), but it does not guarantee deterministic polynomial-time complexity. In this paper we present a proof that computing the M P P under the above mentioned stochastic tree grammars is NP-hard. Note that for computing the MPD there are deterministic polynomial-time algorithms (Schabes and Waters, 1993; Sima'an, 1996) 1. Another problem that turns out also NPhard is computing the Most Probable Sentence ( M P S ) from a given word-graph. But this problem turns out NP-hard even for SCFGs. Beside the m a t h e m a t i c a l interest, this work is driven by the desire to develop efficient algorithms for these problems. Such algorithms can be useflll for various applications that demand robust and faithful disambiguation e.g. Speech Recognition, information Retrieval. '['his proof provides an explanation for the source of complexity: and forms a license to redirect the research for solutions tow"
C96-2215,C92-3126,0,0.36769,"a Oriented Parsing (DOP) (Scha, 1990; nod, 1992) and Stochastic (Lexicalized) Tree-Adjoining G r a m m a r (STAG) (Schabes and Waters, 1993). These models extend the domain of locality for expressing constraints from simple Context-Free G r a m m a r (CFG) productions to deeper structures called elementary-trees. Due to this extension, the one to one mapping between a derivation and a parsetree, which holds in CFGs, does not hold any more; m a n y derivations might generate the same parse-tree, rl'his seemingly spurious ambiguity turns out crucial for statistical disambiguation as defined in (Bod, 1992) and in (Schabes and Waters, 1993), where the derivations are considered different stochastic processes and their probabilities all contribute to the probability of the generated parse. Therefore the Most Probable Derivation ( M P D ) does not necessarily generate the STSGs and SCFGs are closely related. STSGs and SCFGs are equal in weak generative ca*Special thanks to Christer Samuelsson who pointed out and helped in solving a problem with a previous version. Thanks to Remko Scha, Rens Bod and Eric Aarts for valuable comments, and t~o Steven Krauwer and the STT for the support. i The author n"
D08-1066,W06-3123,0,0.206682,"Missing"
D08-1066,P08-1024,0,0.282709,"nd Wong (Marcu and Wong, 2002) realize that the problem of extracting phrase pairs should be intertwined with the method of probability estimation. They formulate a joint phrase-based model in which a source-target sentence pair is generated jointly. However, the huge number of possible phrase-alignments prohibits scaling up the estimation by Expectation-Maximization (EM) (Dempster et al., 1977) to large corpora. Birch et al (Birch et al., 2006) provide soft measures for including wordalignments in the estimation process and obtain improved results only on small data sets. Coming up-to-date, (Blunsom et al., 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable. Also most up-to-date, (Zhang et al., 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. The latter two efforts report improved performance, alb"
D08-1066,P05-1033,0,0.708653,"rt out from a standard phrase extraction procedure based on wordalignment and aim solely at estimating the conditional probabilities for the phrase pairs and their reverse translation probabilities. Unlike preceding work, we extract all phrase pairs from the training corpus and estimate their probabilities, i.e., without limit on length. We present a novel formulation of a conditional translation model that works with a prior over segmentations and a bag of conditional phrase pairs. We use binary Synchronous ContextFree Grammar (bSCFG), based on Inversion Transduction Grammar (ITG) (Wu, 1997; Chiang, 2005a), to define the set of eligible segmentations for an aligned sentence pair. We also show how the number of spurious derivations per segmentation in this bSCFG can be used for devising a prior probability over the space of segmentations, capturing the bias in the data towards monotone translation. The heart of the estimation process is a new smoothing estimator, a penalized version of Deleted Estimation, which averages the temporary probability estimates of multiple parallel EM processes at each joint iteration. For evaluation we use a state-of-the-art baseline system (Moses) (Hoang and Koehn"
D08-1066,W06-3105,0,0.140998,"rase Translation Probabilities with ITG Priors and Smoothing as Learning Objective Khalil Sima’an Language and Computation, ILLC Faculty of Science University of Amsterdam k.simaan@uva.nl Markos Mylonakis Language and Computation, ILLC Faculty of Science University of Amsterdam m.mylonakis@uva.nl Abstract 2003). While this heuristic estimator gives good empirical results, it does not seem to optimize any intuitively reasonable objective function of the (wordaligned) parallel corpus (see e.g., (DeNero et al., 2006)) The mounting number of efforts attacking this problem over the last few years (DeNero et al., 2006; Marcu and Wong, 2002; Birch et al., 2006; Moore and Quirk, 2007; Zhang et al., 2008) exhibits its difficulty. So far, none has lead to an alternative method that performs as well as the heuristic on reasonably sized data (approx. 1000k sentence pair). Given a parallel corpus, an estimator for phrasetables in PBSMT involves two interacting decisions (1) which phrase pairs to extract, and (2) how to assign probabilities to the extracted pairs. The heuristic estimator employs word-alignment (Giza++) (Och and Ney, 2003) and a few thumb rules for defining phrase pairs, and then extracts a multi-s"
D08-1066,W08-0510,0,0.0584859,"2005a), to define the set of eligible segmentations for an aligned sentence pair. We also show how the number of spurious derivations per segmentation in this bSCFG can be used for devising a prior probability over the space of segmentations, capturing the bias in the data towards monotone translation. The heart of the estimation process is a new smoothing estimator, a penalized version of Deleted Estimation, which averages the temporary probability estimates of multiple parallel EM processes at each joint iteration. For evaluation we use a state-of-the-art baseline system (Moses) (Hoang and Koehn, 2008) which works with a log-linear interpolation of feature functions optimized by MERT (Och, 2003). We simply substitute our own estimates for the heuristic phrase translation estimates (both directions and the phrase penalty score) and compare the two within the Moses decoder. While our estimates differ substantially from the heuristic, their performance is on par with the heuristic estimates. This is remarkable given the fact that comparable previous work (DeNero et al., 2006; Moore and Quirk, 2007) did not match the performance of the heuristic estimator using large training sets. We find that"
D08-1066,N03-1017,0,0.377527,"derived from Inversion Transduction Grammar (ITG), (2) A phrase table containing all phrase pairs without length limit, and (3) Smoothing as learning objective using a novel Maximum-A-Posteriori version of Deleted Estimation working with Expectation-Maximization. Where others conclude that latent segmentations lead to overfitting and deteriorating performance, we show here that these three ingredients give performance equivalent to the heuristic method on reasonably sized training data. 1 Motivation A major component in phrase-based statistical Machine translation (PBSMT) (Zens et al., 2002; Koehn et al., 2003) is the table of conditional probabilities of phrase translation pairs. The pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multi-set of the phrase pairs extracted from the word-aligned corpus (Koehn et al., Instead of employing word-alignment to guide phrase pair extraction, it is theoretically more appealing to aim at phrase alignment as part of the estimation process (Marcu and Wong, 2002; Birch et al., 2006). This way, phrase pair extraction goes handin-hand with estimating the probabilities. However, in pr"
D08-1066,W02-1018,0,0.520822,"abilities with ITG Priors and Smoothing as Learning Objective Khalil Sima’an Language and Computation, ILLC Faculty of Science University of Amsterdam k.simaan@uva.nl Markos Mylonakis Language and Computation, ILLC Faculty of Science University of Amsterdam m.mylonakis@uva.nl Abstract 2003). While this heuristic estimator gives good empirical results, it does not seem to optimize any intuitively reasonable objective function of the (wordaligned) parallel corpus (see e.g., (DeNero et al., 2006)) The mounting number of efforts attacking this problem over the last few years (DeNero et al., 2006; Marcu and Wong, 2002; Birch et al., 2006; Moore and Quirk, 2007; Zhang et al., 2008) exhibits its difficulty. So far, none has lead to an alternative method that performs as well as the heuristic on reasonably sized data (approx. 1000k sentence pair). Given a parallel corpus, an estimator for phrasetables in PBSMT involves two interacting decisions (1) which phrase pairs to extract, and (2) how to assign probabilities to the extracted pairs. The heuristic estimator employs word-alignment (Giza++) (Och and Ney, 2003) and a few thumb rules for defining phrase pairs, and then extracts a multi-set of phrase pairs and"
D08-1066,W07-0715,0,0.755442,"Objective Khalil Sima’an Language and Computation, ILLC Faculty of Science University of Amsterdam k.simaan@uva.nl Markos Mylonakis Language and Computation, ILLC Faculty of Science University of Amsterdam m.mylonakis@uva.nl Abstract 2003). While this heuristic estimator gives good empirical results, it does not seem to optimize any intuitively reasonable objective function of the (wordaligned) parallel corpus (see e.g., (DeNero et al., 2006)) The mounting number of efforts attacking this problem over the last few years (DeNero et al., 2006; Marcu and Wong, 2002; Birch et al., 2006; Moore and Quirk, 2007; Zhang et al., 2008) exhibits its difficulty. So far, none has lead to an alternative method that performs as well as the heuristic on reasonably sized data (approx. 1000k sentence pair). Given a parallel corpus, an estimator for phrasetables in PBSMT involves two interacting decisions (1) which phrase pairs to extract, and (2) how to assign probabilities to the extracted pairs. The heuristic estimator employs word-alignment (Giza++) (Och and Ney, 2003) and a few thumb rules for defining phrase pairs, and then extracts a multi-set of phrase pairs and estimates their conditional probabilities"
D08-1066,J03-1002,0,0.00952595,"mounting number of efforts attacking this problem over the last few years (DeNero et al., 2006; Marcu and Wong, 2002; Birch et al., 2006; Moore and Quirk, 2007; Zhang et al., 2008) exhibits its difficulty. So far, none has lead to an alternative method that performs as well as the heuristic on reasonably sized data (approx. 1000k sentence pair). Given a parallel corpus, an estimator for phrasetables in PBSMT involves two interacting decisions (1) which phrase pairs to extract, and (2) how to assign probabilities to the extracted pairs. The heuristic estimator employs word-alignment (Giza++) (Och and Ney, 2003) and a few thumb rules for defining phrase pairs, and then extracts a multi-set of phrase pairs and estimates their conditional probabilities based on the counts in the multi-set. Using this method for extracting a set of phrase pairs, (DeNero et al., 2006; Moore and Quirk, 2007) aim at defining a better estimator for the probabilities. Generally speaking, both efforts report deteriorating translation performance relative to the heuristic. The conditional phrase translation probabilities constitute the principal components of phrase-based machine translation systems. These probabilities are es"
D08-1066,J04-4002,0,0.272215,"ach container σj = hlf , rf , le , re i consists of the start lf and end rf positions2 for a phrase in f and the start le and end re positions for an aligned phrase in e. 2. For a given segmentation σ1I , for every container σj (1 ≤ j ≤ I) generate the phrase-pair hfj , ej i, independently from all other phrasepairs. This leads to the following probabilistic model: P (f |e; a) = X P (σ1I ) σ1I ∈Σ(a) 3 The Translation Model Given a word-aligned parallel corpus of sourcetarget sentences, it is common practice to extract a set of phrase pairs using extraction heuristics (cf. (Koehn et al., 2003; Och and Ney, 2004)). These heuristics define a phrase pair to consist of a source and target ngrams of a word-aligned source-target sentence pair such that if one end of an alignment is in the one ngram, the other end is in the other ngram (and there is at least one such alignment) (Och and Ney, 2004; Koehn et al., 2003). For efficiency and sparseness, the practitioners of PBSMT constrain the length of the source phrase to a certain maximum number of words. 632 P (fj |ej ) (1) hfj ,ej i∈σ1I (f ,e) Where Σ(a) is the set of binarizable segmentations (defined next) that are eligible according to the word-alignment"
D08-1066,P03-1021,0,0.2699,"ow the number of spurious derivations per segmentation in this bSCFG can be used for devising a prior probability over the space of segmentations, capturing the bias in the data towards monotone translation. The heart of the estimation process is a new smoothing estimator, a penalized version of Deleted Estimation, which averages the temporary probability estimates of multiple parallel EM processes at each joint iteration. For evaluation we use a state-of-the-art baseline system (Moses) (Hoang and Koehn, 2008) which works with a log-linear interpolation of feature functions optimized by MERT (Och, 2003). We simply substitute our own estimates for the heuristic phrase translation estimates (both directions and the phrase penalty score) and compare the two within the Moses decoder. While our estimates differ substantially from the heuristic, their performance is on par with the heuristic estimates. This is remarkable given the fact that comparable previous work (DeNero et al., 2006; Moore and Quirk, 2007) did not match the performance of the heuristic estimator using large training sets. We find that smoothing is crucial for achieving good estimates. This is in line with earlier work on consis"
D08-1066,P02-1040,0,0.0767864,"ed by 3 iterations of each Model 3 and Model 4. From this aligned training corpus, we extract the phrase pairs according to the heuristics in (Koehn et al., 2003). The baseline system extracts all phrase-pairs upto a certain maximum length on both sides and employs the heuristic estimator. The language model used in all systems is a 5-gram language model trained on the English side of the parallel corpus. Minimum-Error Rate Training (MERT) is applied on the development set to obtain optimal log-linear interpolation weights for all systems. Performance is measured by computing the BLEU scores (Papineni et al., 2002) of the system’s translations, when compared against a single reference translation per sentence. Results: We compare different versions of our system against the baseline system using the heuristic estimator. We observe the effects of the ITG prior in the translation model as well as the method of estimation (Deleted Estimation vs. Penalized Deleted Estimation). Table 1 exhibits the BLEU scores for the sys10 http://www.statmt.org/wmt07 tems. Our own system (with ITG prior and Penalized Deleted Estimation and maximum phraselength ten words) scores (33.14), slightly outperforming the best basel"
D08-1066,J97-3002,0,0.877069,"e also start out from a standard phrase extraction procedure based on wordalignment and aim solely at estimating the conditional probabilities for the phrase pairs and their reverse translation probabilities. Unlike preceding work, we extract all phrase pairs from the training corpus and estimate their probabilities, i.e., without limit on length. We present a novel formulation of a conditional translation model that works with a prior over segmentations and a bag of conditional phrase pairs. We use binary Synchronous ContextFree Grammar (bSCFG), based on Inversion Transduction Grammar (ITG) (Wu, 1997; Chiang, 2005a), to define the set of eligible segmentations for an aligned sentence pair. We also show how the number of spurious derivations per segmentation in this bSCFG can be used for devising a prior probability over the space of segmentations, capturing the bias in the data towards monotone translation. The heart of the estimation process is a new smoothing estimator, a penalized version of Deleted Estimation, which averages the temporary probability estimates of multiple parallel EM processes at each joint iteration. For evaluation we use a state-of-the-art baseline system (Moses) (H"
D08-1066,2002.tmi-tutorials.2,0,0.018458,"atent segmentations derived from Inversion Transduction Grammar (ITG), (2) A phrase table containing all phrase pairs without length limit, and (3) Smoothing as learning objective using a novel Maximum-A-Posteriori version of Deleted Estimation working with Expectation-Maximization. Where others conclude that latent segmentations lead to overfitting and deteriorating performance, we show here that these three ingredients give performance equivalent to the heuristic method on reasonably sized training data. 1 Motivation A major component in phrase-based statistical Machine translation (PBSMT) (Zens et al., 2002; Koehn et al., 2003) is the table of conditional probabilities of phrase translation pairs. The pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multi-set of the phrase pairs extracted from the word-aligned corpus (Koehn et al., Instead of employing word-alignment to guide phrase pair extraction, it is theoretically more appealing to aim at phrase alignment as part of the estimation process (Marcu and Wong, 2002; Birch et al., 2006). This way, phrase pair extraction goes handin-hand with estimating the probabil"
D08-1066,N06-1033,0,0.253459,"eters, overfitting the training data and failing to generalize”. More recently, (Moore and Quirk, 2007) devise a estimator working with a model that does not include a hidden segmentation variable but works with a heuristic iterative procedure (rather than MLE or EM). The translation results remain inferior to the heuristic but the authors note an interesting trade-off between decoding speed and the various settings of this estimator. Our work expands on the general approach taken by (DeNero et al., 2006; Moore and Quirk, 2007) but arrives at insights similar to those of the most recent work (Zhang et al., 2006), albeit in a completely different manner. The present work differs from all preceding work in that it employs the set of all phrase pairs during training. It differs from (Zhang et al., 2008) in that it does postulate a latent segmentation variable and puts the prior directly over that variable rather than over the ITG synchronous rule estimates. Our method neither excludes phrase pairs before estimation nor does it prune the space of possible segmentations/analyses during training/estimation. As well as smoothing, we find (in the same vein as (Zhang et al., 2008)) that setting effective prio"
D08-1066,P08-1012,0,0.0518617,"Missing"
D08-1066,J09-4009,0,\N,Missing
D08-1066,2006.amta-papers.2,0,\N,Missing
D09-1088,P99-1065,0,0.201526,"Missing"
D09-1088,J03-4003,0,0.919227,"ve grammar that decomposes it to form and function. The RR grammar first generates a set of grammatical functions depicting the Relational Network (RN) (Perlmutter, 1982) of the clause. This 3 4 Such clauses are defined formally as exocentric in formal theories of syntax, and are used to describe syntactic structures in, e.g., Tagalog, Hungarian and Warlpiri (Bresnan, 2001, page 110). This flat representation format is characteristic of treebanks for other languages with relatively-free word-order as well, such as German (cf. (Kubler, 2008)). The success of Head-Driven models (Charniak, 1997; Collins, 2003) was initially attributed to the fact that they were fully lexicalized, but (Klein and Manning, 2003) show that an unlexicalized model combining Head-Driven Markovian processes with linguistically motivated state-splits can approach the performance of fully lexicalized models. 844 (3a) S VP-P RD natan gave NP-SBJ Dani ADVP etmol yesterday (3b) NP+D+ACC -OBJ et-hamatana the-present PP-COM le-dina to-Dina S VP-P RD natan gave NP+D+ACC -OBJ et-ha-matana the-present ADVP etmol yesterday NP-SBJ Dani Dani PP-COM le-dina to-Dina Figure 1: The State-Splits Approach for Ex. (3) (3a) S V P @S HEAD,V P @"
D09-1088,J98-4004,0,0.765139,"affordable alternative to the Head-Driven (HD) approach in the development of phrase-structure based statistical parsing models. Recently, we proposed the RelationalRealizational (RR) approach that rests upon different premises (Tsarfaty and Sima’an, 2008). The question of how the RR model fares against the HD models that have so far been predominantly used has never been tackled. Yet, it is precisely such a comparison that can shed new light on the question of adequacy we posed above. Empirically quantifying the effects of different modeling choices has been addressed for English by, e.g., (Johnson, 1998; Klein and Manning, 2003), and for German by, e.g., (Dubey, 2004; Applying statistical parsers developed for English to languages with freer wordorder has turned out to be harder than expected. This paper investigates the adequacy of different statistical parsing models for dealing with a (relatively) free word-order language. We show that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclud"
D09-1088,P03-1054,0,0.642196,"ernative to the Head-Driven (HD) approach in the development of phrase-structure based statistical parsing models. Recently, we proposed the RelationalRealizational (RR) approach that rests upon different premises (Tsarfaty and Sima’an, 2008). The question of how the RR model fares against the HD models that have so far been predominantly used has never been tackled. Yet, it is precisely such a comparison that can shed new light on the question of adequacy we posed above. Empirically quantifying the effects of different modeling choices has been addressed for English by, e.g., (Johnson, 1998; Klein and Manning, 2003), and for German by, e.g., (Dubey, 2004; Applying statistical parsers developed for English to languages with freer wordorder has turned out to be harder than expected. This paper investigates the adequacy of different statistical parsing models for dealing with a (relatively) free word-order language. We show that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclude that the form-function s"
D09-1088,C08-1112,1,0.886132,"Missing"
D09-1088,W08-1008,0,0.0606741,"parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F1 88. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F1 92.1 (McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts 1 Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F1 75, F1 79, F1 83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer to the question of parser adequacy in the face of word-order freedom. Our two empirical results are unequivocal. Firstly, RR models significantly outperform HD model"
D09-1088,P06-3009,1,0.852998,"ding off the parameters described in table 3, in accordance with the trees depicted in figures 1–3.8 For all models, we use relative frequency estimates. For lexical parameters, we use a simple smoothing procedure assigning probability to unknown words using the per-tag distribution of rare words (“rare” threshold set to &lt; 2). The input to our parser consists of morphologically segmented surface forms, and the parser has to as9 This setup is more difficult than, e.g., the Arabic parsing setup of (Bikel, 2002), as they assume gold-standard pos-tags as input. Yet it is easier than the setup of (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008) which uses unsegmented surface forms as input. The decision to use segmented and untagged forms was made to retain a realistic scenario. Morphological analysis is known to be ambiguous, and we do not assume that morphological features are known up front. Morphological segmentation is also ambiguous, but for our purposes it is unavoidable. When comparing different models on an individual sentence they may propose segmentation to sequences of different lengths, for which accuracy results cannot be faithfully compared. See (Tsarfaty, 2006) for discussion. 10 The fla"
D09-1088,P95-1037,0,0.156278,"multiple surface exponents. The question this paper addresses is therefore what kind of modeling approach would be adequate for modeling the interplay between syntax and morphology in marking grammatical relations in Hebrew, as reflected by the sentence-pair (3). They both mean, roughly, “Dani gave the present to Dina yesterday; their word-order vary, but the pattern of object marking is retained. (3) 3.2 The Head-Driven Approach Following the linguistic wisdom that the internal organization of syntactic constituents revolves around their heads, Head-Driven (HD) models have been proposed by (Magerman, 1995; Charniak, 1997; Collins, 1999). In a generative HD model, the head daughter is generated first, conditioned on properties of the mother node. Then, sisters of the head daughter are generated conditioned on the head, typically by left and right generation processes. Overall, HD processes have the modeling advantage that they capture structurallymarked positions that characterize the argument structure of the sentence. The simplest possible process uses unigram probabilities, but (Klein and Manning, 2003) show that using vertical and horizontal Markovization improves parsing accuracy.4 An unle"
D09-1088,J93-2004,0,0.0313799,"that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclude that the form-function separation ingrained in RR models makes them better suited for parsing nonconfigurational phenomena. 1 Introduction Parsing technology has come a long way since Charniak (1996) demonstrated that a simple treebank PCFG performs better than any other parser (with F1 75 accuracy) on parsing the WSJ Penn treebank (Marcus et al., 1993). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F1 88. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F1 92.1 (M"
D09-1088,C08-1071,0,0.0174641,"). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F1 88. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F1 92.1 (McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts 1 Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F1 75, F1 79, F1 83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer"
D09-1088,P06-1055,0,0.0775428,"ion separation for expressing grammatical relations. 13 The startegy of adding grammatical functions as statesplits is used in, e.g., German (Rafferty and Manning, 2008). 14 Due to the difference in the size of the grammars, one could argue that smoothing will bridge the gap between the HD and RR modeling strategies. However, the better size/accuracy trade-off shown here for RR models suggests that they provide a good bias/variance balancing point, especially for feature-rich models characterizing morphologically rich languages. A promising strategy then would be to smooth or split-and-merge (Petrov et al., 2006)) RR-based models rather than to add an elaborate smoothing component to configurationally-based HD models. 849 configurational —————– nonconfigurational Chinese&gt;English&gt;{German,Hebrew}&gt;Warlpiri 9 Acknowledgements We thank Jelle Zuidema, Inbal Tsarfati, David McCloskey and Yoav Golderg for excellent comments on earlier versions. We also thank Miles Osborne and Tikitu de Jager for comments on the camera-ready draft. All errors are our own. The work of the first author is funded by the Dutch Science Foundation (NWO) grant 017.001.271. Figure 5: The Configurationality Scale The HD assumptions tak"
D09-1088,W08-1006,0,0.0781646,"inative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F1 92.1 (McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts 1 Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F1 75, F1 79, F1 83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer to the question of parser adequacy in the face of word-order freedom. Our two empirical results are unequivocal. Firstly, RR models significantly outperform HD models (about 2 points absolute improvement in F1 ) in parsing the Modern Hebrew treebank. In particular, RR models show better performance in identifying the constituents for which syntactic positions are relatively free. Secondly, we show a novel va"
D09-1088,C04-1024,0,0.021683,"ning at a single position. Figure 3: The Relational-Realizational Approach 845 GF Description PRD SBJ OBJ COM Predicative Elements Grammatical Subjects Direct Objects Indirect Objects FInite Complements Infinitival Complements A Conjunct within a Conjunction Structure IC CNJ sign the syntactic as well as morphological analysis to the surface segments.9 We use the standard development/training/test split as in (Tsarfaty and Sima’an, 2008). Since our goal is a detailed comparison and fine-grained analysis of the results we concentrate on the development set. We use a general-purpose CKY parser (Schmid, 2004) to exhaustively parse the sentences, and we strip off all model-specific information prior to evaluation. Applicable to. . . VP, PREDP NP, SBAR NP NP, PP SBAR VP All Table 2: Grammatical Functions in the MHTB SP-PCFG Expansion P(Cln , . . . , Ch , . . . , Crn |P ) HD-PCFG Head Left Branch? Right Branch? Left Arg/Mod Right Arg/Mod Left Final? Right Final? P(Ch |P ) P(L:∆l1 , H:∆h |Ch , P ) P(Ch , R:∆r1 |∆h , Ch , P ) P(Cli , ∆li+1 |L , ∆li , Ch , P ) P(Cri , ∆ri+1 |R , ∆ri , Ch , P ) P(C1 |L , ∆ln−1 , Ch , P ) P(Cn |R , ∆rn−1 , Ch , P ) RR-PCFG Projection Configuration Realization Adjunction P"
D09-1088,P08-1043,1,\N,Missing
D09-1088,D07-1096,0,\N,Missing
D09-1123,J99-2004,0,0.090166,"alizer’) is needed in order to compose a state Si with the previous states S0 . . . Si−1 in order to obtain a fully connected intermediate dependency structure at every position in the input. To implement the incremental parsing scheme described above we use the parser described in (Hassan et al., 2008b; Hassan et al., 2009), which is based on Combinatory Categorial Grammar (CCG) (Steedman, 2000). We only briefly describe this parser as its full description is beyond the scope of this paper. The notions of a supertag as a lexical category and the process of supertagging are both crucial here (Bangalore and Joshi, 1999). Fortunately, CCG specifies the desired kind of lexical categories (supertags) sti for every word and a small set of combinatory operators oi that combine the supertag sti with a previous parse state Si−1 into the next parse state Si . In terms of CCG representations, the parse state is a CCG composite category which specifies either a functor and the arguments it expects to the right of the current word, or is itself an argument for a functor that will follow it to the right. At the first word in the sentence, the parse state consists solely of the supertag of that word. S0 Attacks rocked Ri"
D09-1123,P05-1033,0,0.0816104,"ther detailed insight into the characteristics of the systems. Finally, Section 8 concludes, and discusses future work. 2 Related Work In (Marcu et al., 2006), it is demonstrated that ‘syntactified’ target language phrases can improve translation quality for Chinese–English. A stochastic, top-down transduction process is employed that assigns a joint probability to a source sentence and each of its alternative syntactified translations; this is done by specifying a rewriting process of the target parse-tree into a source sentence. Likewise, the model in (Zollmann and Venugopal, 2006) extends (Chiang, 2005) by augmenting the hierarchical phrases with syntactic categories derived from parsing the target side of a parallel corpus. They use an existing parser to parse the target side of the parallel corpus in order to extract a syntactically motivated, bilingual synchronous grammar as in (Chiang, 2005). The above-mentioned approaches for incorporating syntax into Phrase-based SMT (Marcu et al., 2006; Zollmann and Venugopal, 2006) share common drawbacks. Firstly, they are based on syntactic phrase-structure parse trees incorporated into a Synchronous CFG or TreeSubstitution Grammar, which makes for"
D09-1123,R09-1025,1,0.886428,"Missing"
D09-1123,koen-2004-pharaoh,0,0.571843,"ities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntacticallyoriented feat"
D09-1123,W04-3250,0,0.478185,"ities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntacticallyoriented feat"
D09-1123,N03-1017,0,0.01714,"i () that capture the translation and language model effects, and (iii) the weights of the features λi that are estimated under MaxEnt (Berger et al., 1996), as in (1): P (T |S) = X P0 (T, J|S) exp λi φi (T, J, S) (1) Z i Here J is the skip reordering factor for the phrase pair captured by φi () and represents the jump from the previous source word, and Z is the per source sentence normalization term. The prior probability P0 is the prior distribution for the phrase probability which is estimated using the phrase normalized counts commonly used in conventional Phrase–based SMT systems, e.g., (Koehn et al., 2003). DTM2 differs from other Phrase–based SMT models in that it extracts from a word-aligned parallel corpus only a non-redundant set of minimal phrases in the sense that no two phrases overlap with each other. Baseline DTM2 Features: The baseline employs the following five types of features (beside the language model): • Lexical Micro Features examining source and target words of the phrases, The DTM2 approach based on MaxEnt provides a flexible framework for incorporating other available feature types as we demonstrate below. DTM2 Decoder: The decoder for the baseline is a beam search decoder s"
D09-1123,W06-1606,0,0.0241641,"to the quantitative results. This paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the DTM2 baseline model. Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2. Section 5 details our own DDTM, the dependencybased extension of the DTM2 model. Section 6 reports on extensive experiments and their results. Section 7 provides translation output to shed further detailed insight into the characteristics of the systems. Finally, Section 8 concludes, and discusses future work. 2 Related Work In (Marcu et al., 2006), it is demonstrated that ‘syntactified’ target language phrases can improve translation quality for Chinese–English. A stochastic, top-down transduction process is employed that assigns a joint probability to a source sentence and each of its alternative syntactified translations; this is done by specifying a rewriting process of the target parse-tree into a source sentence. Likewise, the model in (Zollmann and Venugopal, 2006) extends (Chiang, 2005) by augmenting the hierarchical phrases with syntactic categories derived from parsing the target side of a parallel corpus. They use an existing"
D09-1123,P02-1040,0,0.0767533,"shed to perform at a parsing level close to state1182 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182–1191, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP of-the-art cubic-time parsers. Nevertheless, the parsing information it provides allows for significant improvement in translation quality. We test the new model, called the Dependencybased Direct Translation Model (DDTM), on standard Arabic–English translation tasks used in the community, including LDC and GALE data. We show that our DDTM system provides significant improvements in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores over the already extremely competitive DTM2 system. We also provide results of manual, qualitative analysis of the system output to provide insight into the quantitative results. This paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the DTM2 baseline model. Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2. Section 5 details our own DDTM, the dependencybased extension of the DTM2 model. Section 6 reports on extensive experiments and their results."
D09-1123,P08-1066,0,0.0435443,"rate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lexicalized grammars, they never seek to build a full dependency tree or employ syntactic features in order to directly influence the reordering probabilities in the decoder. In the current work, we expand our previous work in (Hassan et al., 2007; Hassan et al., 2008a) to introduce the capabilities of building a full dependency structure and employing syntactic features to influence the decoding process. Recently, (Shen et al., 2008) introduced an approach for incorporating a dependency-based language model into SMT. They proposed to extract String-to-Dependency trees from the parallel corpus. As the dependency trees are not constituents by nature, they handle non-constituent phrases as well. While this work is in the same general direction as our work, namely aiming at incorporating dependency parsing into SMT, there remain three major differences. Firstly, (Shen et al., 2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas our approach employs the well formalized CCG grammatical theory. Secondly"
D09-1123,2006.amta-papers.25,0,0.0267791,"el close to state1182 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182–1191, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP of-the-art cubic-time parsers. Nevertheless, the parsing information it provides allows for significant improvement in translation quality. We test the new model, called the Dependencybased Direct Translation Model (DDTM), on standard Arabic–English translation tasks used in the community, including LDC and GALE data. We show that our DDTM system provides significant improvements in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores over the already extremely competitive DTM2 system. We also provide results of manual, qualitative analysis of the system output to provide insight into the quantitative results. This paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the DTM2 baseline model. Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2. Section 5 details our own DDTM, the dependencybased extension of the DTM2 model. Section 6 reports on extensive experiments and their results. Section 7 provides translation"
D09-1123,P07-1019,0,0.0181216,"are common within Phrase-based SMT. These approaches usually resort to ad hoc solutions to enrich the non-constituent phrases with syntactic structures. Secondly, they deploy chart-based decoders with a high computational cost compared with the phrase-based beam search decoders, e.g., (Tillmann and Ney, 2003; Koehn, 2004a). Thirdly, due to the large parse space, some of the proposed approaches are forced to employ small language models compared to what is usually used in phrase-based systems. To circumvent these computational limitations, various pruning techniques are usually needed, e.g., (Huang and Chiang, 2007). Other recent approaches, e.g., (Birch et al., 2007; Hassan et al., 2007; Hassan et al., 2008a) incorporate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lexicalized grammars, they never seek to build a full dependency tree or employ syntactic features in order to directly influence the reordering probabilities in the decoder. In the current work, we expand our previous work in (Hassan et al., 2007; Hassan et al., 2008a) to introduce the capabilities of building a f"
D09-1123,J03-1005,0,0.493361,"e time and space complexities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntactically"
D09-1123,W06-3119,0,0.0371993,"provides translation output to shed further detailed insight into the characteristics of the systems. Finally, Section 8 concludes, and discusses future work. 2 Related Work In (Marcu et al., 2006), it is demonstrated that ‘syntactified’ target language phrases can improve translation quality for Chinese–English. A stochastic, top-down transduction process is employed that assigns a joint probability to a source sentence and each of its alternative syntactified translations; this is done by specifying a rewriting process of the target parse-tree into a source sentence. Likewise, the model in (Zollmann and Venugopal, 2006) extends (Chiang, 2005) by augmenting the hierarchical phrases with syntactic categories derived from parsing the target side of a parallel corpus. They use an existing parser to parse the target side of the parallel corpus in order to extract a syntactically motivated, bilingual synchronous grammar as in (Chiang, 2005). The above-mentioned approaches for incorporating syntax into Phrase-based SMT (Marcu et al., 2006; Zollmann and Venugopal, 2006) share common drawbacks. Firstly, they are based on syntactic phrase-structure parse trees incorporated into a Synchronous CFG or TreeSubstitution Gr"
D09-1123,N07-1008,0,\N,Missing
D09-1123,J96-1002,0,\N,Missing
D09-1123,J90-2002,0,\N,Missing
D09-1123,W07-0702,0,\N,Missing
D14-1025,D08-1066,1,\N,Missing
D14-1025,C08-1136,0,\N,Missing
D14-1025,W12-3102,0,\N,Missing
D14-1025,P02-1040,0,\N,Missing
D14-1025,W14-3348,0,\N,Missing
D14-1025,W10-1749,0,\N,Missing
D14-1025,D10-1092,0,\N,Missing
D14-1025,J97-3002,0,\N,Missing
D14-1025,W14-3354,1,\N,Missing
D14-1025,D11-1125,0,\N,Missing
D14-1062,D11-1033,0,0.671925,"e weighting performance as well as significantly improved phrasebased translation performance. 2 Model and training by invitation Eq. 1 shows that the key to training the latent phrase-based translation models is to train the latent phrase-relevance models, P (D |e˜, f˜). As mentioned, for training P (D |e˜, f˜) on parallel sentences in Cmix we embed them in two asymmetric sentence-level models {P (D |e, f ) |D ∈ {0, 1}}. 2.1 Domain relevance sentence models Intuitively, sentence models for domain relevance P (D |e, f ) are somewhat related to data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011). The dominant approach to data selection uses the contrast between perplexities of in- and mix-domain language models.3 In the translation context, however, often a source phrase has different senses/translations in different domains, which cannot be distinguished with monolingual language models (Cuong and Sima’an, 2014). Therefore, our proposed latent sentencerelevance model includes two major latent components - monolingual domain-focused relevance models and domain-focused translation models derives as follows: P (e, f, D) , D∈{D1 ,D0 } P (e, f, D) P (D |e, f) = P (2) where P (e, f, D) ca"
D14-1062,2011.iwslt-evaluation.18,0,0.240469,"corpus Cmix the translation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix , a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from C mix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-domain” distributions i"
D14-1062,N12-1047,0,0.049778,"ation features, lexical weights (Koehn et al., 2003) and lexicalized reordering features (Koehn et al., 2005).13 Other features include the penalties for word, phrase and distance-based reordering. The mix-domain corpus is word-aligned using GIZA++ (Och and Ney, 2003) and symmetrized with grow(-diag)-final-and (Koehn et al., 2003). We limit phrase length to a maximum of seven words. The LMs are interpolated 4-grams with Kneser-Ney, trained on 2.2M English sentences from Europarl augmented with 248.8K sentences from News Commentary Corpus (WMT 2013). We tune the system using k-best batch MIRA (Cherry and Foster, 2012). Finally, we use Moses 12 We use Stanford Phrasal - a standard state-of-the-art phrase-based translation system developed by Cer et al. (2010). 13 The lexical weights and the lexical reordering features will be described in more detail in Section 6. 570 Electrics (Training Data: 1 Million) 21.2 21 20.8 20.64 20.6 20.48 20.5 20.51 20.52 Iter. 4 Iter. 5 20.4 20.2 20 19.91 19.8 Baseline Iter. 1 Iter. 2 Iter. 3 Figure 3: BLEU averaged over multiple runs. (Koehn et al., 2007) as decoder.14 We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER (Snover et al., 2006),"
D14-1062,P11-2031,0,0.0728965,"Missing"
D14-1062,C14-1182,1,0.518772,"Missing"
D14-1062,W11-2107,0,0.144708,"Missing"
D14-1062,P12-2023,0,0.0994203,") Baseline 27.4 − − BLEU Our System 28.3 +0.9 0.0001 Baseline 34.0 − − METEOR Our System 34.7 +0.7 0.0001 Baseline 51.7 − − TER Our System 50.6 -1.1 0.0001 Pharmaceuticals & Biotechnology (In-domain: 85K pairs; Dev: 920 pairs; Test: 1, 000 pairs) Baseline 31.6 − − BLEU Our System 32.4 +0.8 0.0001 Baseline 34.0 − − METEOR Our System 34.4 +0.4 0.0001 Baseline 51.4 − − TER Our System 50.6 -0.8 0.0001 Table 8: Metric scores for the systems, which are averages over multiple runs. 8 Related work A distantly related, but clearly complementary, line of research focuses on the role of document topics (Eidelman et al., 2012; Zhang et al., 2014; Hasler et al., 2014). An off-the-shelf Latent Dirichlet Allocation tool is usually used to infer document-topic distributions. On one hand, this setting may not require in-domain data as prior. On the other hand, it requires meta-information (e.g., document information). Part of this work (the latent sentence-relevance models) relates to data selection (Moore and Lewis, 2010; Axelrod et al., 2011), where sentence-relevance weights are used for hardSystem Avg ∆ Metric Professional & Business Services (In-domain: 23K pairs; Dev: 1, 000 pairs; Test: In-domain 46.5 − BLEU + M"
D14-1062,D10-1044,0,0.197997,"Missing"
D14-1062,E12-1016,0,0.0242855,"Missing"
D14-1062,E14-1035,0,0.187935,"rasal for some advantages, we prefer to use Moses for decoding. 1M 2M 4M System Baseline Our System Baseline Our System Baseline Our System Avg 19.91 20.64 20.54 21.41 21.44 22.62 ∆ − +0.73 − +0.87 − +1.18 p-value − 0.0001 − 0.0001 − 0.0001 Table 2: BLEU averaged over multiple runs. It is also interesting to consider the average entropy of phrase table entries in the domainconfused systems, i.e., P − h˜e,f˜i pt (˜ e|f˜) log pt (˜ e|f˜) number of phrasesh˜ e, f˜i against that in the domain-focused systems P − h˜e,f˜i pt (˜ e|f˜, D1 ) log pt (˜ e|f˜, D1 ) number of phrasesh˜ e, f˜i . Following (Hasler et al., 2014) in Table 3 we also show that the entropy decreases significantly in 571 the adapted tables in all cases, which indicates that the distributions over translations of phrases have become sharper. Baseline 0.210 Iter. 1 0.187 Iter. 2 0.186 Iter. 3 0.185 Iter. 4 0.185 Entries Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 Iter. 5 0.184 Table 3: Average entropy of distributions. In practice, the third iteration systems usually produce best translations. This is somewhat expected because as EM invites more pseudo indomain pairs in later iterations, it sharpens the estimates of P (D1 |e˜, f˜), mak"
D14-1062,Q13-1035,0,0.03317,"based system (Och and Ney, 2004) is a phrase table {h˜ e, f˜i} extracted from the word-aligned training data together with estimates for Pt (˜ e |f˜) ˜ and Pt (f |e˜). Because the translations of words often vary across domains, it is likely that in a mix-domain corpus Cmix the translation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix , a complementary adaptation step which might precede any further combination with other models, e.g., i"
D14-1062,W07-0733,0,0.722278,"likely that in a mix-domain corpus Cmix the translation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix , a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from C mix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-d"
D14-1062,N03-1017,0,0.0796662,"se “Consumer and Industrial Electronics” manually collected by Translation Automation Society (TAUS.com). The corpus statistics are summarized in Table 1. System We train a standard state-of-the-art Cmix Cin Domain: Dev Electronics Test Sents Words Sents Words Sents Words Sents Words English Spanish 4M 113.7M 127.1M 109K 1, 485, 558 1, 685, 716 984 13130 14, 955 982 13, 493 15, 392 Table 1: The data preparation. phrase-based system, using it as the baseline.12 There are three main kinds of features for the translation model in the baseline - phrase-based translation features, lexical weights (Koehn et al., 2003) and lexicalized reordering features (Koehn et al., 2005).13 Other features include the penalties for word, phrase and distance-based reordering. The mix-domain corpus is word-aligned using GIZA++ (Och and Ney, 2003) and symmetrized with grow(-diag)-final-and (Koehn et al., 2003). We limit phrase length to a maximum of seven words. The LMs are interpolated 4-grams with Kneser-Ney, trained on 2.2M English sentences from Europarl augmented with 248.8K sentences from News Commentary Corpus (WMT 2013). We tune the system using k-best batch MIRA (Cherry and Foster, 2012). Finally, we use Moses 12 W"
D14-1062,P07-2045,0,0.0168934,"Missing"
D14-1062,2005.mtsummit-papers.11,0,0.430827,"ecall at the sentence-level. Results We use a mix-domain corpus C g of 770K sentence pairs of different genres.10 There is also a Legal corpus of 183K pairs that serves as the in-domain data. We create C mix by selecting an arbitrary 83K pairs of in-domain pairs and adding them to C g (the hidden in-domain data); we use the remaining 100k in-domain pairs as C in . To train the baselines, we construct interpolated 4-gram Kneser-Ney LMs using BerkeleyLM (Pauls and Klein, 2011). Training our model on the data takes six EM-iterations to converge.11 10 Count of sentence pairs: European Parliament (Koehn, 2005): 183, 793; Pharmaceuticals: 190, 443, Software: 196, 168, Hardware: 196, 501. 11 After the fifth EM iteration we do not observe any significant increase in the likelihood of the data. Note that we use the same setting as for the baselines to train the latent domain-focused LMs for use in our model – interpolated 4gram Kneser-Ney LMs using BerkeleyLM. This training setting is used for all experiments in this work. 569 (b): Pseudo-Recall (Sentence-Level) (a): Pseudo-Precision (Sentence-Level) Iter. 2 Iter. 3 Iter. 4 Iter. 5 Iter. 1 100 95 90 85 80 75 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0 P"
D14-1062,D09-1074,0,0.0598684,"s are used for hardSystem Avg ∆ Metric Professional & Business Services (In-domain: 23K pairs; Dev: 1, 000 pairs; Test: In-domain 46.5 − BLEU + Mix-domain 46.6 − + Our system 47.9 +1.3 In-domain 39.8 − METEOR + Mix-domain 40.1 − + Our System 41.1 +1.0 In-domain 38.2 − TER + Mix-domain 38.0 − + Our System 36.9 -1.1 p-value 998 pairs) − − 0.0001 − − 0.0001 − − 0.0001 Table 9: Domain adaptation experiments. Metric scores for the systems, which are averages over multiple runs. filtering rather than weighting. The idea of using sentence-relevance estimates for phrase-relevance estimates relates to Matsoukas et al. (2009) who estimate the former using meta-information over documents as main features. In contrast, our work overcomes the mutual dependence of sentence and phrase estimates on one another by training both models in tandem. Adaptation using small in-domain data has a different but complementary goal to another line of research aiming at combining a domainadapted system with the another trained on the indomain data (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Our work is somewhat related to, but markedly different from, phrase pair we"
D14-1062,P10-2041,0,0.71008,"ts showing good instance weighting performance as well as significantly improved phrasebased translation performance. 2 Model and training by invitation Eq. 1 shows that the key to training the latent phrase-based translation models is to train the latent phrase-relevance models, P (D |e˜, f˜). As mentioned, for training P (D |e˜, f˜) on parallel sentences in Cmix we embed them in two asymmetric sentence-level models {P (D |e, f ) |D ∈ {0, 1}}. 2.1 Domain relevance sentence models Intuitively, sentence models for domain relevance P (D |e, f ) are somewhat related to data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011). The dominant approach to data selection uses the contrast between perplexities of in- and mix-domain language models.3 In the translation context, however, often a source phrase has different senses/translations in different domains, which cannot be distinguished with monolingual language models (Cuong and Sima’an, 2014). Therefore, our proposed latent sentencerelevance model includes two major latent components - monolingual domain-focused relevance models and domain-focused translation models derives as follows: P (e, f, D) , D∈{D1 ,D0 } P (e, f, D) P (D |e, f) = P ("
D14-1062,J03-1002,0,0.00837382,"ev Electronics Test Sents Words Sents Words Sents Words Sents Words English Spanish 4M 113.7M 127.1M 109K 1, 485, 558 1, 685, 716 984 13130 14, 955 982 13, 493 15, 392 Table 1: The data preparation. phrase-based system, using it as the baseline.12 There are three main kinds of features for the translation model in the baseline - phrase-based translation features, lexical weights (Koehn et al., 2003) and lexicalized reordering features (Koehn et al., 2005).13 Other features include the penalties for word, phrase and distance-based reordering. The mix-domain corpus is word-aligned using GIZA++ (Och and Ney, 2003) and symmetrized with grow(-diag)-final-and (Koehn et al., 2003). We limit phrase length to a maximum of seven words. The LMs are interpolated 4-grams with Kneser-Ney, trained on 2.2M English sentences from Europarl augmented with 248.8K sentences from News Commentary Corpus (WMT 2013). We tune the system using k-best batch MIRA (Cherry and Foster, 2012). Finally, we use Moses 12 We use Stanford Phrasal - a standard state-of-the-art phrase-based translation system developed by Cer et al. (2010). 13 The lexical weights and the lexical reordering features will be described in more detail in Sect"
D14-1062,J04-4002,0,0.0571742,"ormance improvement in both tasks. 1 Mix vs. Latent Domain Models Domain adaptation is usually perceived as utilizing a small seed in-domain corpus to adapt an existing system trained on an out-of-domain corpus. Here we are interested in adapting an SMT system trained on a large mix-domain corpus Cmix to an in-domain task represented by a seed parallel corpus Cin . The mix-domain scenario is interesting because often a large corpus consists of sentence pairs representing diverse domains, e.g., news, politics, finance, sports, etc. At the core of a standard state-of-the-art phrasebased system (Och and Ney, 2004) is a phrase table {h˜ e, f˜i} extracted from the word-aligned training data together with estimates for Pt (˜ e |f˜) ˜ and Pt (f |e˜). Because the translations of words often vary across domains, it is likely that in a mix-domain corpus Cmix the translation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent ada"
D14-1062,P02-1040,0,0.0909421,"Missing"
D14-1062,P11-1027,0,0.0222152,"over the data to learn the sentences with their relevance, we then rank the sentences to select top of pairs to evaluate the pseudo-precision/recall at the sentence-level. Results We use a mix-domain corpus C g of 770K sentence pairs of different genres.10 There is also a Legal corpus of 183K pairs that serves as the in-domain data. We create C mix by selecting an arbitrary 83K pairs of in-domain pairs and adding them to C g (the hidden in-domain data); we use the remaining 100k in-domain pairs as C in . To train the baselines, we construct interpolated 4-gram Kneser-Ney LMs using BerkeleyLM (Pauls and Klein, 2011). Training our model on the data takes six EM-iterations to converge.11 10 Count of sentence pairs: European Parliament (Koehn, 2005): 183, 793; Pharmaceuticals: 190, 443, Software: 196, 168, Hardware: 196, 501. 11 After the fifth EM iteration we do not observe any significant increase in the likelihood of the data. Note that we use the same setting as for the baselines to train the latent domain-focused LMs for use in our model – interpolated 4gram Kneser-Ney LMs using BerkeleyLM. This training setting is used for all experiments in this work. 569 (b): Pseudo-Recall (Sentence-Level) (a): Pseu"
D14-1062,P12-1099,0,0.386965,"will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix , a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from C mix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-domain” distributions is exploited for softly inviting (or re"
D14-1062,P13-1082,0,0.664019,"e domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix , a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from C mix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-domain” distributions is exploited for softly inviting (or recruiting) Cmix phrase pa"
D14-1062,E12-1055,0,0.715023,"lation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix , a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from C mix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-domain” distributions is exploited for"
D14-1062,2006.amta-papers.25,0,0.104244,"Missing"
D14-1062,2005.iwslt-1.8,0,\N,Missing
D15-1005,P05-1066,0,0.435599,"Missing"
D15-1005,J03-4003,0,0.0282504,"hich is a major restriction when used on the data: there are many nonbinarizable permutations in actual data (Wellington et al., 2006). In contrast, our PETs are obtained by factorizing permutations obtained from the data, i.e., they exactly fit the range of prime permutations in the parallel corpus. In practice we limit them to maximum arity 5. We can extract PCFG rules from the PETs, e.g., P 21 → P 12 P 2413. However, these rules are decorated with too coarse labels. A similar problem was encountered in non-lexicalized monolingual parsing, and one solution was to lexicalize the productions (Collins, 2003) using head words. But linguistic heads do not make sense for PETs, so we opt for the alternative approach (Matsuzaki et al., 2005), which splits the nonterminals and softly percolates the splits through the trees gradually fitting them to the training data. Splitting has a shadow side, however, because it leads to combinatorial explosion in grammar size. Suppose for example node P 21 could split into P 211 and P 212 and similarly P 2413 splits into P 24131 and 24132 . This means that rule P 21 → P 12 P 2413 will form eight new rules: P 211 → P 2111 P 2121 P 2111 → P 121 P 2111 → P 122 P 2112"
D15-1005,D11-1018,0,0.0461739,"ple parse of English sentence that predicts reordering for English-Japanese the present work, there are major differences. On the similarity side, NDTs are decomposing alignments in ways similar to PETs, and both Saluja’s and our models refine the labels on the nodes of these decompositions. However, there are major differences between the two: al (2012) trains a latent non-probabilistic discriminative model for preordering as an ITG-like grammar limited to binarizable permutations. Tromble and Eisner (2009) use ITG but do not train the grammar. They only use it to constrain the local search. DeNero and Uszkoreit (2011) present two separate consecutive steps for unsupervised induction of hierarchical structure (ITG) and the induction of a reordering function over it. In contrast, here we learn both the structure and the reordering function simultaneously. Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step process – the parser forwards to the next stage the best parse. Dyer and Resnik (2010) treat reordering as a latent var"
D15-1005,W14-3348,0,0.0300314,"n MT The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 ´ s − t. The only exception is Baseline A which is trained on original s − t. We use a 5-gram language model trained with KenLM 8 , tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics. All PETs or binary only? RGPET-forest performs significantly better than RGITG-forest (p < 0.05). Non-ITG reordering operators are predicted rarely (in only 99 sentences of the test set), but they make a difference, because these operators often appear high in the predicted PET. Furthermore, having these operators during training might allow for better fit to the data. How much reordering is resolved by the Reordering Grammar? Obviously, completely"
D15-1005,P11-1103,0,0.119963,"e dedicated to reordering. We face two technical difficulties alien to work on latent PCFGs in treebank parsing. Firstly, as mentioned above, permutations may factorize into more than one PET (a forest) leading to a latent training treebank.1 And secondly, after we parse a source string s, we are interested in ´ s, the permuted version of s, not in the best derivation/PET. Exact computation is a known NP-Complete problem (Sima’an, 2002). We solve this by a new Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations (Birch and Osborne, 2011; Isozaki et al., 2010a). In summary, this paper contributes: • A novel latent hierarchical source reordering model working over all derivations of PETs • A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations. • A fast Minimum Bayes Risk decoding over Kendall τ reordering score for selecting ´ s. We report results for extensive experiments on English-Japanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrase-based models, outperfo"
D15-1005,W98-1505,0,0.0331564,"lits into P 24131 and 24132 . This means that rule P 21 → P 12 P 2413 will form eight new rules: P 211 → P 2111 P 2121 P 2111 → P 121 P 2111 → P 122 P 2112 → P 121 P 2112 → P 122 → P 121 P 24131 → P 122 P 24131 → P 121 P 24131 → P 122 P 24131 P 211 P 211 P 212 P 212 → P 2112 P 2122 → P 24131 → P 24132 → P 24131 → P 24132 The unary trick leads to substantial reduction in grammar size, e.g., for arity 5 rules and 30 splits we could have had 306 = 729000000 split-rules, but with the unary trick we only have 30+302 ∗5 = 4530 split rules. The unary trick was used in early lexicalized parsing work (Carroll and Rooth, 1998).2 This split PCFG constitutes a latent PCFG because the splits cannot be read of a treebank. It must be learned from the latent treebank of PETs, as described next. P3142 1 P3142 P31422 P12 P121 P31423 P122 P31424 P12 P121 P21 P122 Professor Chomsky , P 211 P 211 P 212 P 212 P 212 P 2121 P 2121 P 2122 P 2122 → P 121 P 24132 → P 122 P 24132 → P 121 P 24132 → P 122 P 24132 P211 I P212 would like to thank you Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken Figure 2: Permutation Tree with unary trick Should we want to split each nonterminal into 30 subcategories, then an n-ary"
D15-1005,D10-1092,0,0.0577644,"Missing"
D15-1005,W10-1736,0,0.527316,". We face two technical difficulties alien to work on latent PCFGs in treebank parsing. Firstly, as mentioned above, permutations may factorize into more than one PET (a forest) leading to a latent training treebank.1 And secondly, after we parse a source string s, we are interested in ´ s, the permuted version of s, not in the best derivation/PET. Exact computation is a known NP-Complete problem (Sima’an, 2002). We solve this by a new Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations (Birch and Osborne, 2011; Isozaki et al., 2010a). In summary, this paper contributes: • A novel latent hierarchical source reordering model working over all derivations of PETs • A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations. • A fast Minimum Bayes Risk decoding over Kendall τ reordering score for selecting ´ s. We report results for extensive experiments on English-Japanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrase-based models, outperforming two existing bas"
D15-1005,P02-1040,0,0.0942036,"C 32.0ABC Extrinsic evaluation in MT The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 ´ s − t. The only exception is Baseline A which is trained on original s − t. We use a 5-gram language model trained with KenLM 8 , tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics. All PETs or binary only? RGPET-forest performs significantly better than RGITG-forest (p < 0.05). Non-ITG reordering operators are predicted rarely (in only 99 sentences of the test set), but they make a difference, because these operators often appear high in the predicted PET. Furthermore, having these operators during training might allow for better fit to the data. How much reordering is resolved by the Reord"
D15-1005,D11-1017,0,0.0403602,"Missing"
D15-1005,P06-1055,0,0.742047,"Albert and Atkinson, 2005). Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering. We expect this to be advantageous for learning hierarchical reordering. For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only. We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes. Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014). Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s). Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations. After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version ´ s of s. In this sense, our latent splits are dedicated to reordering. We face two technical difficulties alien to"
D15-1005,I11-1005,1,0.886523,"Missing"
D15-1005,N06-1002,0,0.0525669,"Missing"
D15-1005,N04-1022,0,0.127108,"mming over all PETs ∆ in the forest P EF (πm ), and for every PET also over all its label splits, which are given by the grammar derivations d: X X P (sm , πm ) = P (d, sm ) (1) X argmax π∈Π X P (d, πm ) ∆∈P EF (π) d∈∆ from a lattice of permutations Π using a PCFG is NP-complete (Sima’an, 2002). Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not directly applicable here. Hence, we opt for minimizing the risk of making an error under a loss function over permutations using the MBR decision rule (Kumar and Byrne, 2004): π ˆ = argmin π ∆∈P EF (πm ) d∈∆ X Loss(π, πr )P (πr ) (3) πr The loss function we minimize is Kendall τ (Birch and Osborne, 2011; Isozaki et al., 2010a) which is a ratio of wrongly ordered pairs of words (including gapped pairs) to the total number of pairs. We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss. We sample from the true distribution by sampling edges recursively The probability of a derivation d is a product of probabilities of all the rules r that build it: X XY P (sm , πm ) = P (r) (2) ∆∈P EF"
D15-1005,C12-1142,0,0.0171778,"nd the reordering function simultaneously. Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step process – the parser forwards to the next stage the best parse. Dyer and Resnik (2010) treat reordering as a latent variable and try to sum over all derivations that lead not only to the same reordering but also to the same translation. In their work they consider all permutations allowed by a given syntactic tree. Saers et al (2012) induce synchronous grammar for translation by splitting the non-terminals, but unlike our approach they split generic nonterminals and not operators. Their most expressive grammar covers only binarizable permutations. The decoder that uses this model does not try to sum over many derivations that have the same yield. They do not make independence assumption like our “unary trick” which is probably the reason they do not split more than 8 times. They do not compare their results to any other SMT system and test on a very small dataset. Saluja et al (2014) attempts inducing a refined Hiero gram"
D15-1005,D14-1210,0,0.468513,"2005). Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering. We expect this to be advantageous for learning hierarchical reordering. For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only. We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes. Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014). Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s). Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations. After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version ´ s of s. In this sense, our latent splits are dedicated to reordering. We face two technical difficulties alien to work on latent PCFGs i"
D15-1005,D13-1049,0,0.382585,"uction Preordering (Collins et al., 2005) aims at permuting the words of a source sentence s into a new order ´ s, hopefully close to a plausible target word order. Preordering is often used to bridge long distance reorderings (e.g., in Japanese- or GermanEnglish), before applying phrase-based models (Koehn et al., 2007). Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it. A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004). The (direct correspondence) assumption 44 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 44–54, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. becomes 4th and the fourth becomes 2nd . The prime permutations are non-factorizable permutations like h1, 2i, h2, 1i and h2, 4, 1, 3i. We think PETs are suitable for learning preordering for two reasons. Firstly, PETs specify exactly the phrase pairs defined by the permutation. Secondly, every permutation is factorizable into prime permuta"
D15-1005,W14-4002,1,0.869682,"Missing"
D15-1005,2006.amta-papers.25,0,0.0239807,"mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 ´ s − t. The only exception is Baseline A which is trained on original s − t. We use a 5-gram language model trained with KenLM 8 , tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics. All PETs or binary only? RGPET-forest performs significantly better than RGITG-forest (p < 0.05). Non-ITG reordering operators are predicted rarely (in only 99 sentences of the test set), but they make a difference, because these operators often appear high in the predicted PET. Furthermore, having these operators during training might allow for better fit to the data. How much reordering is resolved by the Reordering Grammar? Obviously, completely factorizing out the reorderin"
D15-1005,P05-1010,0,0.540281,"torizable into prime permutations only (Albert and Atkinson, 2005). Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering. We expect this to be advantageous for learning hierarchical reordering. For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only. We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes. Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014). Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s). Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations. After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version ´ s of s. In this sense, our latent splits are dedicated to reordering. We face"
D15-1005,N04-4026,0,0.0563023,"arge distortion limits seem to degrade the preordering choice. This shows also that the improved performance of RGPET-forest is not only a result of efficiently exploring the full space of permutations, but also a result of improved scoring of permutations. System DP BM SD EHiero RGPET-forest +MSD BLEU ↑ 29.6 32.6 32.4D METEOR ↑ 50.1 52.1 51.3D E TER ↓ 58.0 54.5 55.3D E RIBES ↑ 68.97 74.12 75.72 Table 4: Comparison to MSD and Hiero Does the improvement remain for a decoder with MSD reordering model? We compare the RGPET-forest preordered model against a decoder that uses the strong MSD model (Tillmann, 2004; Koehn et al., 2007). Table 4 shows that using Reordering Grammar as front-end to MSD reordering (full Moses) improves performance by 2.8 BLEU points. The improvement is confirmed by METEOR, TER and RIBES. Our preordering model and MSD are complementary – the Reordering Grammar captures long distance reordering, while MSD possibly does better local reorderings, especially reorderings conditioned on the lexical part of translation units. Interestingly, the MSD model (BLEU 29.6) improves over distance-based reordering (BLEU 27.8) by (BLEU 1.8), whereas the difference between these systems as ba"
D15-1005,D09-1105,0,0.0749313,"12 P01 current i flowing through the P12 P12 feeding conductor P12 3 produces magnetic field b1 . Figure 4: Example parse of English sentence that predicts reordering for English-Japanese the present work, there are major differences. On the similarity side, NDTs are decomposing alignments in ways similar to PETs, and both Saluja’s and our models refine the labels on the nodes of these decompositions. However, there are major differences between the two: al (2012) trains a latent non-probabilistic discriminative model for preordering as an ITG-like grammar limited to binarizable permutations. Tromble and Eisner (2009) use ITG but do not train the grammar. They only use it to constrain the local search. DeNero and Uszkoreit (2011) present two separate consecutive steps for unsupervised induction of hierarchical structure (ITG) and the induction of a reordering function over it. In contrast, here we learn both the structure and the reordering function simultaneously. Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step proce"
D15-1005,D12-1077,0,0.0559226,"s, 2006). 47 using their inside probabilities. An empirical distribution over permutations P (π) is given by the relative frequency of π in the sample. With large samples it is hard to efficiently compute expected Kendall τ loss for each sampled hypothesis. For sentence of length k and sample of size n the complexity of a naive algorithm is O(n2 k 2 ). Computing Kendall τ alone takes O(k 2 ). We use the fact that Kendall τ decomposes as a linear function over all skip-bigrams b that could be built for any permutation of length k: Kendall(π, πr ) = X 1 − δ(π, b) k(k−1) 2 b • Baseline C: LADER (Neubig et al., 2012): latent variable preordering that is based on ITG and large-margin training with latent variables. We used LADER in standard settings without any linguistic features (POS tags or syntactic trees). And we test four variants of our model: • RGleft - only canonical left branching PET • RGright - only canonical right branching PET • RGITG-forest - all PETs that are binary (ITG) • RGPET-forest - all PETs. We test these models on English-Japanese NTCIR-8 Patent Translation (PATMT) Task. For tuning we use all NTCIR-7 dev sets and for testing the test set from NTCIR-9 from both directions. All used d"
D15-1005,J97-3002,0,0.583913,"PETs, the latent treebank and the Reordering Grammar. Figure 1 shows examples of how PETs look like – see (Zhang and Gildea, 2007) for algorithmic details. Here we label the nodes with nonterminals which stand for prime permutations from the operators on the PETs. For example, nonterminals P 12, P 21 and P 3142 correspond respectively to reordering transducers h1, 2i, h2, 1i and h3, 1, 4, 2i. A prime permutation on a source node µ is a transduction dictating how the children of µ are reordered at the target side, e.g., P 21 inverts the child order. We must stress that any similarity with ITG (Wu, 1997) is restricted to the fact that the straight and inverted operators of ITG are the binary case of prime permutations 1 All PETs for the same permutation share the same set of prime permutations but differ only in bracketing structure (Zhang and Gildea, 2007). 45 P3142 P3142 P12 P12 P21 P21 P12 P12 Professor Chomsky , I would like to thank you Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken Professor Chomsky , I would like to thank you Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken (a) Canonical PET (b) Alternative PET Figure 1: Possible Permutation Trees"
D15-1005,C04-1073,0,0.502306,"ns et al., 2005) aims at permuting the words of a source sentence s into a new order ´ s, hopefully close to a plausible target word order. Preordering is often used to bridge long distance reorderings (e.g., in Japanese- or GermanEnglish), before applying phrase-based models (Koehn et al., 2007). Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it. A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004). The (direct correspondence) assumption 44 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 44–54, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. becomes 4th and the fourth becomes 2nd . The prime permutations are non-factorizable permutations like h1, 2i, h2, 1i and h2, 4, 1, 3i. We think PETs are suitable for learning preordering for two reasons. Firstly, PETs specify exactly the phrase pairs defined by the permutation. Secondly, every permutation is factorizable into prime permutations only (Albert and"
D15-1005,W07-0404,0,0.77532,"ucture and the transduction function in two separate, consecutive steps (DeNero and Uszkoreit, 2011). Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop, from word-aligned parallel corpora. Learning both trees and transductions jointly raises two questions. How to obtain suitable trees for the source sentence and how to learn a distribution over random variables specifically aimed at reordering in a hierarchical model? In this work we solve both challenges by using the factorizations of permutations into Permutation Trees (PETs) (Zhang and Gildea, 2007). As we explain next, PETs can be crucial for exposing the hierarchical reordering patterns found in wordalignments. We obtain permutations in the training data by segmenting every word-aligned source-target pair into minimal phrase pairs; the resulting alignment between minimal phrases is written as a permutation (1:1 and onto) on the source side. Every permutation can be factorized into a forest of PETs (over the source sentences) which we use as a latent treebank for training a Probabilistic ContextFree Grammar (PCFG) tailor made for preordering as we explain next. Figure 1 shows two altern"
D15-1005,C08-1136,0,0.148264,"t. This modifies a PET by adding a new binary branching node µ (dominating the unaligned word and the phrase it is joined to) which is labeled with a dedicated nonterminal: P 01 if the unaligned word joins to the right and P 10 if it joins to the left. 3.1 3.3 Inference We use CKY+ (Chappelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG. Unfortunately, computing the most-likely permutation (or alternatively ´ s) as in Probability model We decompose the permutation πm into a forest of permutation trees P EF (πm ) in O(n3 ), following algorithms in (Zhang et al., 2008; Zhang and Gildea, 2007) with trivial modifications. Each PET ∆ ∈ P EF (πm ) is a different bracketing (differing in binary branching structure only). We consider the bracketing hidden in the latent treebank, and apply unsupervised learning to induce a distribution over possible bracketings. Our probability model starts from the joint probability of a sequence of minimal phrases sm and a permutation πm over it. This demands summing over all PETs ∆ in the forest P EF (πm ), and for every PET also over all its label splits, which are given by the grammar derivations d: X X P (sm , πm ) = P (d,"
D15-1005,N07-1051,0,\N,Missing
D15-1005,N10-1128,0,\N,Missing
D15-1005,P06-1123,0,\N,Missing
D15-1005,P07-2045,0,\N,Missing
D15-1005,P05-1033,0,\N,Missing
D15-1005,P09-1064,0,\N,Missing
D15-1005,N12-1047,0,\N,Missing
D17-1209,N16-1024,0,0.0410979,"hanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees. Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side. Nadejde et al. (2017) multi-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG supertags. Latent structure. Hashimoto and Tsuruoka (2017) add a syntax-inspired encoder on top of a BiLSTM layer. They encode source words as a learned average of potential parents emulating a relaxed dependency tree. While their model is trained purely on translation data, they also experiment with pre-training the encoder using treebank annotation and report modest improv"
D17-1209,P17-2021,0,0.19234,"ures and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees. Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side. Nadejde et al. (2017) multi-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG s"
D17-1209,P10-1146,0,0.0329964,"riguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is natural for us to use dependency syntax."
D17-1209,W14-4012,0,0.196706,"Missing"
D17-1209,D14-1179,0,0.140705,"Missing"
D17-1209,P16-1078,0,0.267892,"y we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent"
D17-1209,P17-2012,0,0.225287,"ystems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and,"
D17-1209,D17-1012,0,0.137824,"Missing"
D17-1209,D14-1080,0,0.0742857,"Missing"
D17-1209,D13-1176,0,0.0955223,", unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees. However, in this work we solely focus on dependency syntax and leave more general investigation for future work. Our main contributions can be summarized as follows: • we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German. 2 Background Notation. We use x for vectors, x1:t for a sequence of t vectors, and X for matrices. The i-th value of vector x is denoted by xi . We use ◦ for vector concatenation. 2.1 Neural Machine Translation In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution p(y1:Ty |x1:Tx ) of translating a source sentence x1:Tx (a sequence of Tx words) into a target sentence y1:Ty . NMT models typically consist of an encoder, a decoder and some method for conditioning the decoder on the encoder, for example, an attention mechanism. We will now briefly describe the components that we use in this paper. 2.1.1 Encoders An encoder is a function that takes as input the source sentence and p"
D17-1209,Q16-1037,0,0.200605,"pturing syntactic properties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively. In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCN"
D17-1209,W16-2209,0,0.0464331,"N <=10 11-20 21-30 31-40 Sentence length 41+ Figure 4: Validation BLEU per sentence length. Discussion. Results suggest that the syntaxaware representations provided by GCNs consistently lead to improved translation performance as measured by BLEU4 (as well as TER and BEER). Consistent gains in terms of Kendall tau and BLEU1 indicate that improvements correlate with better word order and lexical/BPE selection, two phenomena that depend crucially on syntax. 5 Related Work We review various accounts to syntax in NMT as well as other convolutional encoders. Syntactic features and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-t"
D17-1209,W16-2323,0,0.105372,"as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups. 1 Introduction Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a). State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015"
D17-1209,P16-1162,0,0.532548,"as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups. 1 Introduction Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a). State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015"
D17-1209,D16-1159,0,0.0446397,"perties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively. In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require th"
D17-1209,W06-3104,0,0.0297664,"Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is natural for us to use dep"
D17-1209,D15-1166,0,0.52241,"nrich et al., 2016a). State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006"
D17-1209,2006.amta-papers.25,0,0.0452057,"Missing"
D17-1209,D17-1159,1,0.838954,"tional Linguistics det The dobj nsubj monkey • we introduce a method for incorporating structure into NMT using syntactic GCNs; det eats a banana • we show that GCNs can be used along with RNN and CNN encoders; Figure 1: A dependency tree for the example sentence: “The monkey eats a banana.” th order neighborhood (i.e. nodes at most k hops aways from the node) (Gilmer et al., 2017). They are generally simple and computationally inexpensive. We use Syntactic GCNs, a version of GCN operating on top of syntactic dependency trees, recently shown effective in the context of semantic role labeling (Marcheggiani and Titov, 2017). Since syntactic GCNs produce representations at word level, it is straightforward to use them as encoders within the attention-based encoderdecoder framework. As NMT systems are trained end-to-end, GCNs end up capturing syntactic properties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challe"
D17-1209,P02-1040,0,0.0986164,"using the syntaxnet/demo.sh shell script. 9 https://github.com/moses-smt/mosesdecoder 1962 8 English-German English-German (full) English-Czech Source Target 37824 50000 33786 8099 (BPE) 16000 (BPE) 8116 (BPE) Table 2: Vocabulary sizes. lent to the dimensionality of their input. We report results for 2-layer GCNs, as we find them most effective (see ablation studies below). Baselines. We provide three baselines, each with a different encoder: a bag-of-words encoder, a convolutional encoder with window size w = 5, and a BiRNN. See §2.1.1 for details. Evaluation. We report (cased) BLEU results (Papineni et al., 2002) using multi-bleu, as well as Kendall τ reordering scores.10 4.2.1 Results English-German. Table 3 shows test results on English-German. Unsurprisingly, the bag-ofwords baseline performs the worst. We expected the BoW+GCN model to make easy gains over this baseline, which is indeed what happens. The CNN baseline reaches a higher BLEU4 score than the BoW models, but interestingly its BLEU1 score is lower than the BoW+GCN model. The CNN+GCN model improves over the CNN baseline by +1.9 and +1.1 for BLEU1 and BLEU4 , respectively. The BiRNN, the strongest baseline, reaches a BLEU4 of 14.9. Interes"
D17-1209,D13-1170,0,0.00748132,"sion (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively. In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees. However, in this work we solely focus on dependency syntax and leave more general investigation for future work. Our main contributions can be summarized as follows: • we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German. 2 Background Notation. We use x for vectors, x1:t for a sequence of t vectors, and X for matrices. The i-th value of vector x is denoted by xi . We use ◦ for vector concatenation. 2.1 Neural Machine Translation In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014;"
D17-1209,P16-2049,0,0.03803,"better word order and lexical/BPE selection, two phenomena that depend crucially on syntax. 5 Related Work We review various accounts to syntax in NMT as well as other convolutional encoders. Syntactic features and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees. Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve"
D17-1209,D14-1025,1,0.89451,"Missing"
D17-1209,W06-3119,0,0.0765112,"learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is n"
D17-1209,W08-2121,0,\N,Missing
D17-1209,P11-2031,0,\N,Missing
D17-1209,P17-1012,0,\N,Missing
I11-1005,P03-1021,0,0.00850195,"val and 1000 resamples, following the guidelines in Koehn (2004). 6 Translation and reordering experiments Data. In our experiments we used English-Dutch and English-Spanish European Parliament data and an extraction from the English-Chinese Hong Kong Parallel Corpus. All the sets were provided with one reference translation. Basic statistics of the training data can be found in Table 2, development datasets contained 0.5K, 1.9K and 0.5K lines and test datasets contained 1K, 1.9K and 0.5K for Dutch, Spanish and English, respectively. Experimental setup. Word alignment was found using GIZA++3 (Och, 2003), supported by mkcls4 (Och, 1999) tool. The PBSMT systems we consider in this study is based on Moses toolkit (Koehn et al., 2007). We followed the guidelines provided on the Moses web page5 . Two phrase reordering methods are widely used in phrase-based systems. A distance-based reordering model providing the decoder with a cost linear to the distance between words that are being reordered. This model constitutes the default for the Moses system. And, a lexicalized block-oriented data-driven reordering model (Tillman, 2004) considers three orientations: monotone (M), swap (S), and discontinuo"
I11-1005,P02-1040,0,0.0810117,"Missing"
I11-1005,popovic-ney-2006-pos,0,0.212399,"Missing"
I11-1005,I08-1067,0,0.0191391,"and Dras, 2007; Xia and McCord, 2004). In Costa-jussà and Fonollosa (2006) statistical word classes as well as POS tags are used as patterns for reordering the input sentences and producing a new bilingual pair. A rather popular class of source reordering algorithms involves syntactic information and aims at minimizing the need for reordering during translation by permuting the source sentence (Collins et al., 2005; Wang et al., 2007; Khalilov and Sima’an, 2010; Li et al., 2007). Some systems perform source permutation using a set of handcrafted rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2008), others make use of automatically learned reordering patterns extracted from the plain training data, the corresponding parse or dependency trees and the alignment matrix (Visweswariah et al., 2010). Inspiring this work, source reordering as a pretranslation step is viewed as a word permutation learning problem in Tromble and Eisner (2009) and Li et al. (2007). The space of permutations is approached efficiently using a binary ITG-like synchronous context-free grammar put on the parallel data. Similarly, a local ITG-based tree transducer with contextual conditioning is used in Khalilov and Si"
I11-1005,N04-4026,0,0.486621,"cal Transduction Maxim Khalilov∗† and Khalil Sima’an Institute for Logic, Language and Computation University of Amsterdam P.O. Box 94242 1090 GE Amsterdam, The Netherlands {m.khalilov,k.simaan}@uva.nl Abstract et al., 2003) deals with word order differences in two subcomponents of a translation model. Firstly, using the local word reordering implicitly encoded in phrase pairs. Secondly, using an explicit reordering model which may reorder target phrases relative to their source sides, e.g., as a monotone phrase sequence generation process with the possibility of swapping neighboring phrases (Tillman, 2004). How well can a phrase translation model perform if we permute the source words to fit target word order as perfectly as word alignment might allow? And how well would it perform if we limit the allowed permutations to ITGlike tree-transduction operations on the source parse tree? First we contribute oracle results showing great potential for performance improvement by source-reordering, ranging from 1.5 to 4 BLEU points depending on language pair. Although less outspoken, the potential of tree-based source-reordering is also significant. Our second contribution is a source reordering model t"
I11-1005,D09-1105,0,0.708577,"subtrees under a node, and the other first abolishes layers in the parse tree in order to exploit sibling permutation at the remaining levels. The latter is the opposite of parse binarization using Expectation-Maximization (Huang et al., 2009). We use Maximum-Entropy training (Berger et al., 1996) to learn a sequence of tree transductions, each conditioned on contextual features in tree resulting from outcome of the preceding transduction. The conditioning on the outcome of preceding transductions is a departure from earlier approaches at learning independent source permutation steps, e.g., (Tromble and Eisner, 2009; Visweswariah et al., 2010). The aim for the rest of this paper is firstly, to quantify the potential performance improvement of a standard PBSMT system if preceded by source reordering and secondly, to show that statistical Markov approach to tree transduction, where the probability of each transduction step is conditioned on the outcome of preceding steps, can improve the quality of PBSMT output significantly. Figure 1: Example crossing alignments and long-distance reordering using a source parse tree. source string to unfold the crossing alignments is computationally intractable (see (Trom"
I11-1005,C10-1126,0,0.515884,"d the other first abolishes layers in the parse tree in order to exploit sibling permutation at the remaining levels. The latter is the opposite of parse binarization using Expectation-Maximization (Huang et al., 2009). We use Maximum-Entropy training (Berger et al., 1996) to learn a sequence of tree transductions, each conditioned on contextual features in tree resulting from outcome of the preceding transduction. The conditioning on the outcome of preceding transductions is a departure from earlier approaches at learning independent source permutation steps, e.g., (Tromble and Eisner, 2009; Visweswariah et al., 2010). The aim for the rest of this paper is firstly, to quantify the potential performance improvement of a standard PBSMT system if preceded by source reordering and secondly, to show that statistical Markov approach to tree transduction, where the probability of each transduction step is conditioned on the outcome of preceding steps, can improve the quality of PBSMT output significantly. Figure 1: Example crossing alignments and long-distance reordering using a source parse tree. source string to unfold the crossing alignments is computationally intractable (see (Tromble and Eisner, 2009)). Howe"
I11-1005,D07-1077,0,0.294755,"Missing"
I11-1005,J10-2004,0,0.0252779,"Missing"
I11-1005,P98-2230,0,0.0612851,"to exploit sibling permutation at the remaining levels.The statistical parameters of the model we introduce concern individual tree transductions conditioned on contextual features of the tree resulting from all preceding transductions. Experiments in translating from English to Spanish/Dutch/Chinese show significant improvements of respectively 0.6/1.2/2.0 BLEU points. Arguably, local phrase reordering models cannot account for long-range reordering phenomena, e.g., (Chiang, 2005; Chiang, 2007). Hierarchical models of phrase reordering employ synchronous grammars or tree transducers, e.g., (Wu and Wong, 1998; Chiang, 2005). These models explore a more varied range of reordering phenomena, e.g., defined by at most inverting the order of sibling subtrees under each node in binary source/target trees (akin to ITG (Wu and Wong, 1998)). Undoubtedly, the word order of source and target sentences is intertwined with the lexical choices on both side. Statistically speaking, however, one may first select a target word order given the source only, and then choose target words given the selected target word order and source words. One application of this idea is known as source reordering (or -permutation),"
I11-1005,C04-1073,0,0.792213,"explore a more varied range of reordering phenomena, e.g., defined by at most inverting the order of sibling subtrees under each node in binary source/target trees (akin to ITG (Wu and Wong, 1998)). Undoubtedly, the word order of source and target sentences is intertwined with the lexical choices on both side. Statistically speaking, however, one may first select a target word order given the source only, and then choose target words given the selected target word order and source words. One application of this idea is known as source reordering (or -permutation), e.g., (Collins et al., 2005; Xia and McCord, 2004; Wang et al., 2007; Li et al., 2007; Khalilov and Sima’an, 2010). Briefly, the words of the source string s are reordered to minimize word order differences with the target string t, leading to the source permuted string s`. Presumably, a standard PBSMT system trained to translate from s` to t should have an easier task than translating directly from s to t. The source reordering part, s to s`, 1 Motivation Word order differences between languages are a major challenge in Machine Translation (MT). Phrase-based Statistical Machine Translation (PBSMT) (Och and Ney, 2004; Zens et al., 2002; Koeh"
I11-1005,2002.tmi-tutorials.2,0,0.0587075,"5; Xia and McCord, 2004; Wang et al., 2007; Li et al., 2007; Khalilov and Sima’an, 2010). Briefly, the words of the source string s are reordered to minimize word order differences with the target string t, leading to the source permuted string s`. Presumably, a standard PBSMT system trained to translate from s` to t should have an easier task than translating directly from s to t. The source reordering part, s to s`, 1 Motivation Word order differences between languages are a major challenge in Machine Translation (MT). Phrase-based Statistical Machine Translation (PBSMT) (Och and Ney, 2004; Zens et al., 2002; Koehn † Currently, the first author is employed by TAUS B.V., Amsterdam (The Netherlands). 38 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 38–46, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP word alignment a. Source reordering assumes that a permutation of s, called s`, is first generated with a model Pr (` s |s) followed by a phrase translation model Pt (t |s`). The desired permutation s` is one that has minimum word order divergence from t, i.e., when word-aligned again with t would have least number of crossing alignments. Practi"
I11-1005,2007.mtsummit-papers.74,0,0.174848,"and test results on three language pairs. The majority of existing work reports encouraging performance improvements by source reordering. Next we aim at quantifying the potential improvement by oracle source reordering at the string level, if all permutations were to be allowed, and at the source syntactic tree level, by limiting the permutations with two kinds of local transductions. 3 Existing work on source permutation Source reordering has been shown useful for PBSMT for a wide variety of language pairs with high mutual word order disparity (Collins et al., 2005; Popovic’ and Ney, 2006; Zwarts and Dras, 2007; Xia and McCord, 2004). In Costa-jussà and Fonollosa (2006) statistical word classes as well as POS tags are used as patterns for reordering the input sentences and producing a new bilingual pair. A rather popular class of source reordering algorithms involves syntactic information and aims at minimizing the need for reordering during translation by permuting the source sentence (Collins et al., 2005; Wang et al., 2007; Khalilov and Sima’an, 2010; Li et al., 2007). Some systems perform source permutation using a set of handcrafted rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et"
I11-1005,J96-1002,0,0.0169978,"s confirm that reordering a single syntactic tree could be insufficient (e.g., (Huang et al., 2009)), yet they show substantial potential. Our second contribution is a novel source reordering model that manipulates the source parse tree with two kinds of tree transduction operators: the one permutes the order of sibling subtrees under a node, and the other first abolishes layers in the parse tree in order to exploit sibling permutation at the remaining levels. The latter is the opposite of parse binarization using Expectation-Maximization (Huang et al., 2009). We use Maximum-Entropy training (Berger et al., 1996) to learn a sequence of tree transductions, each conditioned on contextual features in tree resulting from outcome of the preceding transduction. The conditioning on the outcome of preceding transductions is a departure from earlier approaches at learning independent source permutation steps, e.g., (Tromble and Eisner, 2009; Visweswariah et al., 2010). The aim for the rest of this paper is firstly, to quantify the potential performance improvement of a standard PBSMT system if preceded by source reordering and secondly, to show that statistical Markov approach to tree transduction, where the p"
I11-1005,P05-1033,0,0.145603,"ns: the one permutes the order of sibling subtrees under a node, and the other first deletes layers in the parse tree in order to exploit sibling permutation at the remaining levels.The statistical parameters of the model we introduce concern individual tree transductions conditioned on contextual features of the tree resulting from all preceding transductions. Experiments in translating from English to Spanish/Dutch/Chinese show significant improvements of respectively 0.6/1.2/2.0 BLEU points. Arguably, local phrase reordering models cannot account for long-range reordering phenomena, e.g., (Chiang, 2005; Chiang, 2007). Hierarchical models of phrase reordering employ synchronous grammars or tree transducers, e.g., (Wu and Wong, 1998; Chiang, 2005). These models explore a more varied range of reordering phenomena, e.g., defined by at most inverting the order of sibling subtrees under each node in binary source/target trees (akin to ITG (Wu and Wong, 1998)). Undoubtedly, the word order of source and target sentences is intertwined with the lexical choices on both side. Statistically speaking, however, one may first select a target word order given the source only, and then choose target words g"
I11-1005,J07-2003,0,0.0983178,"rmutes the order of sibling subtrees under a node, and the other first deletes layers in the parse tree in order to exploit sibling permutation at the remaining levels.The statistical parameters of the model we introduce concern individual tree transductions conditioned on contextual features of the tree resulting from all preceding transductions. Experiments in translating from English to Spanish/Dutch/Chinese show significant improvements of respectively 0.6/1.2/2.0 BLEU points. Arguably, local phrase reordering models cannot account for long-range reordering phenomena, e.g., (Chiang, 2005; Chiang, 2007). Hierarchical models of phrase reordering employ synchronous grammars or tree transducers, e.g., (Wu and Wong, 1998; Chiang, 2005). These models explore a more varied range of reordering phenomena, e.g., defined by at most inverting the order of sibling subtrees under each node in binary source/target trees (akin to ITG (Wu and Wong, 1998)). Undoubtedly, the word order of source and target sentences is intertwined with the lexical choices on both side. Statistically speaking, however, one may first select a target word order given the source only, and then choose target words given the select"
I11-1005,P05-1066,0,0.574135,"Missing"
I11-1005,W06-1609,0,0.100936,"jority of existing work reports encouraging performance improvements by source reordering. Next we aim at quantifying the potential improvement by oracle source reordering at the string level, if all permutations were to be allowed, and at the source syntactic tree level, by limiting the permutations with two kinds of local transductions. 3 Existing work on source permutation Source reordering has been shown useful for PBSMT for a wide variety of language pairs with high mutual word order disparity (Collins et al., 2005; Popovic’ and Ney, 2006; Zwarts and Dras, 2007; Xia and McCord, 2004). In Costa-jussà and Fonollosa (2006) statistical word classes as well as POS tags are used as patterns for reordering the input sentences and producing a new bilingual pair. A rather popular class of source reordering algorithms involves syntactic information and aims at minimizing the need for reordering during translation by permuting the source sentence (Collins et al., 2005; Wang et al., 2007; Khalilov and Sima’an, 2010; Li et al., 2007). Some systems perform source permutation using a set of handcrafted rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2008), others make use of automatically learned reorder"
I11-1005,J09-4009,0,0.0234882,"SMT approach. The literature reports mixed performance improvements for different language pairs, e.g., (Collins et al., 2005; Xia and McCord, 2004; Wang et al., 2007; Li et al., 2007; Khalilov and Sima’an, 2010). But what is the potential improvement of source reordering? We contribute experiments measuring oracle performance improvement for English to Dutch/Spanish/Chinese translations. Beside string-driven oracles, we report results using ITG-like transductions over a single syntactic parse tree of s. Our results confirm that reordering a single syntactic tree could be insufficient (e.g., (Huang et al., 2009)), yet they show substantial potential. Our second contribution is a novel source reordering model that manipulates the source parse tree with two kinds of tree transduction operators: the one permutes the order of sibling subtrees under a node, and the other first abolishes layers in the parse tree in order to exploit sibling permutation at the remaining levels. The latter is the opposite of parse binarization using Expectation-Maximization (Huang et al., 2009). We use Maximum-Entropy training (Berger et al., 1996) to learn a sequence of tree transductions, each conditioned on contextual feat"
I11-1005,W10-3812,1,0.616879,"Missing"
I11-1005,P03-1054,0,0.0398147,"Missing"
I11-1005,N03-1017,0,0.0516106,"Missing"
I11-1005,P07-2045,0,0.00666458,"xperiments we used English-Dutch and English-Spanish European Parliament data and an extraction from the English-Chinese Hong Kong Parallel Corpus. All the sets were provided with one reference translation. Basic statistics of the training data can be found in Table 2, development datasets contained 0.5K, 1.9K and 0.5K lines and test datasets contained 1K, 1.9K and 0.5K for Dutch, Spanish and English, respectively. Experimental setup. Word alignment was found using GIZA++3 (Och, 2003), supported by mkcls4 (Och, 1999) tool. The PBSMT systems we consider in this study is based on Moses toolkit (Koehn et al., 2007). We followed the guidelines provided on the Moses web page5 . Two phrase reordering methods are widely used in phrase-based systems. A distance-based reordering model providing the decoder with a cost linear to the distance between words that are being reordered. This model constitutes the default for the Moses system. And, a lexicalized block-oriented data-driven reordering model (Tillman, 2004) considers three orientations: monotone (M), swap (S), and discontinuous (D), while the reordering probabilities are conditioned on the lexical context of each phrase pair. All language models were tr"
I11-1005,W04-3250,0,0.124077,"Missing"
I11-1005,P07-1091,0,0.617921,"phenomena, e.g., defined by at most inverting the order of sibling subtrees under each node in binary source/target trees (akin to ITG (Wu and Wong, 1998)). Undoubtedly, the word order of source and target sentences is intertwined with the lexical choices on both side. Statistically speaking, however, one may first select a target word order given the source only, and then choose target words given the selected target word order and source words. One application of this idea is known as source reordering (or -permutation), e.g., (Collins et al., 2005; Xia and McCord, 2004; Wang et al., 2007; Li et al., 2007; Khalilov and Sima’an, 2010). Briefly, the words of the source string s are reordered to minimize word order differences with the target string t, leading to the source permuted string s`. Presumably, a standard PBSMT system trained to translate from s` to t should have an easier task than translating directly from s to t. The source reordering part, s to s`, 1 Motivation Word order differences between languages are a major challenge in Machine Translation (MT). Phrase-based Statistical Machine Translation (PBSMT) (Och and Ney, 2004; Zens et al., 2002; Koehn † Currently, the first author is e"
I11-1005,J04-4002,0,0.032152,"Collins et al., 2005; Xia and McCord, 2004; Wang et al., 2007; Li et al., 2007; Khalilov and Sima’an, 2010). Briefly, the words of the source string s are reordered to minimize word order differences with the target string t, leading to the source permuted string s`. Presumably, a standard PBSMT system trained to translate from s` to t should have an easier task than translating directly from s to t. The source reordering part, s to s`, 1 Motivation Word order differences between languages are a major challenge in Machine Translation (MT). Phrase-based Statistical Machine Translation (PBSMT) (Och and Ney, 2004; Zens et al., 2002; Koehn † Currently, the first author is employed by TAUS B.V., Amsterdam (The Netherlands). 38 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 38–46, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP word alignment a. Source reordering assumes that a permutation of s, called s`, is first generated with a model Pr (` s |s) followed by a phrase translation model Pt (t |s`). The desired permutation s` is one that has minimum word order divergence from t, i.e., when word-aligned again with t would have least number of crossing"
I11-1005,E99-1010,0,0.0505118,"the guidelines in Koehn (2004). 6 Translation and reordering experiments Data. In our experiments we used English-Dutch and English-Spanish European Parliament data and an extraction from the English-Chinese Hong Kong Parallel Corpus. All the sets were provided with one reference translation. Basic statistics of the training data can be found in Table 2, development datasets contained 0.5K, 1.9K and 0.5K lines and test datasets contained 1K, 1.9K and 0.5K for Dutch, Spanish and English, respectively. Experimental setup. Word alignment was found using GIZA++3 (Och, 2003), supported by mkcls4 (Och, 1999) tool. The PBSMT systems we consider in this study is based on Moses toolkit (Koehn et al., 2007). We followed the guidelines provided on the Moses web page5 . Two phrase reordering methods are widely used in phrase-based systems. A distance-based reordering model providing the decoder with a cost linear to the distance between words that are being reordered. This model constitutes the default for the Moses system. And, a lexicalized block-oriented data-driven reordering model (Tillman, 2004) considers three orientations: monotone (M), swap (S), and discontinuous (D), while the reordering prob"
N15-1043,W08-0321,0,0.121561,"re more specific to few domains. This suggests that the translation probabilities for words will be as fractioned as the diversity of its translations across the domains. Furthermore, because the IBM and HMM alignment models use context-insensitive conditional probabilities, in heterogeneous corpora the estimates of these probabilities will be aggregated over different domains. Both issues could lead to suboptimal word alignment quality. Surprisingly, the insensitivity of the existing IBM and HMM alignment models to domain differences has not received much attention thus far (see the study of Bach et al. (2008) and Gao et al. (2011) for reference in the literature). We conjecture that this is because it is not fully clear how to define what constitutes a (sub)-domain. In this paper we propose to exploit the contrast between the alignment statistics in a handful of seed samples from different domains in order to induce domain-conditioned probabilities for each sentence pair in the heterogeneous corpus. Crucially, some sentence pairs will be more similar to a seed domain than others, whereas some sentence 398 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the"
N15-1043,J93-2003,0,0.134998,"rom different domains. The seed samples allow estimating sharper, domain-conditioned word alignment statistics for sentence pairs. Our experiments show that the derived domain-conditioned statistics, once combined together, produce notable improvements both in word alignment accuracy and in translation accuracy of their resulting SMT systems. 1 Introduction Word alignment currently constitutes the basis for phrase extraction and reordering in phrase-based systems, and its statistics provide lexical parameters used for smoothing the phrase pair estimates. For over two decades since IBM models (Brown et al., 1993) and the HMM alignment model (Vogel et al., 1996), word alignment remains an active research line, e.g., see recent work (Simion et al., 2013; Tamura et al., 2014; Chang et al., 2014). During the past years we witnessed an increasing need to collect and use large heterogeneous parallel corpora from different domains and sources, e.g., News, Wikipedia, Parliament Proceedings. It is tacitly assumed that assembling a larger corpus Intuitively, in heterogeneous data certain words are present across many domains, whereas others are more specific to few domains. This suggests that the translation pr"
N15-1043,W14-3363,0,0.144724,"r word alignment, a distantly related research line (Tam et al., 2007; Zhao and Xing, 2008) focuses on using document topics to improve the word alignment. In terms of learning word alignment with partial supervision, another distantly related research line focuses on semi-supervised training with partial manual alignments (Fraser and Marcu, 2006; Gao and Vogel, 2010; Gao et al., 2010). Finally, recent 12 Note that better results correspond to larger BLEU, METEOR and to smaller TER. 406 work also focuses on data selection (Kirchhoff and Bilmes, 2014; Cuong and Sima’an, 2014b), mixture models (Carpuat et al., 2014), instance weighting (Foster et al., 2010) and latent variable models (Cuong and Sima’an, 2014a) over heterogeneous corpora. One main contribution of this work is the novelty of exploring the quality of word alignment in heterogeneous corpora. This, surprisingly, has not received much attention thus far (see the study of Bach et al. (2008) and Gao et al. (2011) for reference in the literature). Another major contribution of this work is a learning framework for latent domain word alignment with partial supervision using seed domains. We present its benefits for improving not only the word alig"
N15-1043,P14-1139,0,0.105437,"oned statistics, once combined together, produce notable improvements both in word alignment accuracy and in translation accuracy of their resulting SMT systems. 1 Introduction Word alignment currently constitutes the basis for phrase extraction and reordering in phrase-based systems, and its statistics provide lexical parameters used for smoothing the phrase pair estimates. For over two decades since IBM models (Brown et al., 1993) and the HMM alignment model (Vogel et al., 1996), word alignment remains an active research line, e.g., see recent work (Simion et al., 2013; Tamura et al., 2014; Chang et al., 2014). During the past years we witnessed an increasing need to collect and use large heterogeneous parallel corpora from different domains and sources, e.g., News, Wikipedia, Parliament Proceedings. It is tacitly assumed that assembling a larger corpus Intuitively, in heterogeneous data certain words are present across many domains, whereas others are more specific to few domains. This suggests that the translation probabilities for words will be as fractioned as the diversity of its translations across the domains. Furthermore, because the IBM and HMM alignment models use context-insensitive cond"
N15-1043,N12-1047,0,0.0302566,"pairs, testing the translation accuracy over four different domain-specific test sets related to News, Pharmacy, Legal, and Hardware. We use a standard state-of-the-art phrase-based system as the baseline. Our dense features include MOSES (Koehn et al., 2007) baseline features, plus hierarchical lexicalized reordering model features (Galley and Manning, 2008), and the word-level feature derived from IBM model 1 score, c.f., (Och et al., 2004).11 The interpolated 5-grams LMs with Kneser-Ney are trained on a very large monolingual corpus of 2B words. We tune the systems using kbest batch MIRA (Cherry and Foster, 2012). Finally, we use MOSES (Koehn et al., 2007) as decoder. Our system has exactly the same setting with the baseline, except: (1) To learn the translation, we use the alignment result derived from our latent domain HMM alignment model, rather than the HMM alignment model; and (2) We replace the word-level feature with our four domain-conditioned word-level features derived from the latent domain IBM model 1. Here, note that our latent model is learned with the supervision from the combining domain knowledge of all three domain-specific seed samples. 10 Note that similar results are also observed"
N15-1043,P11-2031,0,0.0503147,"Missing"
N15-1043,D14-1062,1,0.74143,"Missing"
N15-1043,C14-1182,1,0.848827,"Missing"
N15-1043,P11-1043,0,0.0240795,"Missing"
N15-1043,W11-2107,0,0.0912236,"Missing"
N15-1043,P12-2023,0,0.0504029,"ur system produces comparably good performance to the MGIZA++-based system. When 1M data is considered, on three of four tasks, our system produces at least compatible translation accuracy to the corresponding MGIZA++-based system. Further analysis reveals that the improvement is due to not only the reduction in alignment error rate, but also the use of the domain-sensitive lexical features. Moreover, the domain-sensitive lexical features is particularly useful when the domain of the test data matches with the domain of seed samplers. This is also widely observed in the literature, e.g., see (Eidelman et al., 2012; Hasler et al., 2014; Hu et al., 2014). 9 Related Work and Conclusion In terms of domain-conditioned statistics for word alignment, a distantly related research line (Tam et al., 2007; Zhao and Xing, 2008) focuses on using document topics to improve the word alignment. In terms of learning word alignment with partial supervision, another distantly related research line focuses on semi-supervised training with partial manual alignments (Fraser and Marcu, 2006; Gao and Vogel, 2010; Gao et al., 2010). Finally, recent 12 Note that better results correspond to larger BLEU, METEOR and to smaller TE"
N15-1043,D10-1044,0,0.0609882,"rch line (Tam et al., 2007; Zhao and Xing, 2008) focuses on using document topics to improve the word alignment. In terms of learning word alignment with partial supervision, another distantly related research line focuses on semi-supervised training with partial manual alignments (Fraser and Marcu, 2006; Gao and Vogel, 2010; Gao et al., 2010). Finally, recent 12 Note that better results correspond to larger BLEU, METEOR and to smaller TER. 406 work also focuses on data selection (Kirchhoff and Bilmes, 2014; Cuong and Sima’an, 2014b), mixture models (Carpuat et al., 2014), instance weighting (Foster et al., 2010) and latent variable models (Cuong and Sima’an, 2014a) over heterogeneous corpora. One main contribution of this work is the novelty of exploring the quality of word alignment in heterogeneous corpora. This, surprisingly, has not received much attention thus far (see the study of Bach et al. (2008) and Gao et al. (2011) for reference in the literature). Another major contribution of this work is a learning framework for latent domain word alignment with partial supervision using seed domains. We present its benefits for improving not only the word alignment accuracy, but also the translation a"
N15-1043,P06-1097,0,0.0321581,"ful when the domain of the test data matches with the domain of seed samplers. This is also widely observed in the literature, e.g., see (Eidelman et al., 2012; Hasler et al., 2014; Hu et al., 2014). 9 Related Work and Conclusion In terms of domain-conditioned statistics for word alignment, a distantly related research line (Tam et al., 2007; Zhao and Xing, 2008) focuses on using document topics to improve the word alignment. In terms of learning word alignment with partial supervision, another distantly related research line focuses on semi-supervised training with partial manual alignments (Fraser and Marcu, 2006; Gao and Vogel, 2010; Gao et al., 2010). Finally, recent 12 Note that better results correspond to larger BLEU, METEOR and to smaller TER. 406 work also focuses on data selection (Kirchhoff and Bilmes, 2014; Cuong and Sima’an, 2014b), mixture models (Carpuat et al., 2014), instance weighting (Foster et al., 2010) and latent variable models (Cuong and Sima’an, 2014a) over heterogeneous corpora. One main contribution of this work is the novelty of exploring the quality of word alignment in heterogeneous corpora. This, surprisingly, has not received much attention thus far (see the study of Bach"
N15-1043,D08-1089,0,0.0699647,"t accuracy than a hard domain assignment.10 8 Translation Experiment Data 1M 2M 4M In this section, we investigate the contribution of our model in terms of the translation accuracy. Here, we run experiments on the heterogeneous corpora of 1M, 2M, and 4M sentence pairs, testing the translation accuracy over four different domain-specific test sets related to News, Pharmacy, Legal, and Hardware. We use a standard state-of-the-art phrase-based system as the baseline. Our dense features include MOSES (Koehn et al., 2007) baseline features, plus hierarchical lexicalized reordering model features (Galley and Manning, 2008), and the word-level feature derived from IBM model 1 score, c.f., (Och et al., 2004).11 The interpolated 5-grams LMs with Kneser-Ney are trained on a very large monolingual corpus of 2B words. We tune the systems using kbest batch MIRA (Cherry and Foster, 2012). Finally, we use MOSES (Koehn et al., 2007) as decoder. Our system has exactly the same setting with the baseline, except: (1) To learn the translation, we use the alignment result derived from our latent domain HMM alignment model, rather than the HMM alignment model; and (2) We replace the word-level feature with our four domain-cond"
N15-1043,W08-0509,0,0.0303515,"Missing"
N15-1043,W10-1701,0,0.0154508,"with the domain of seed samplers. This is also widely observed in the literature, e.g., see (Eidelman et al., 2012; Hasler et al., 2014; Hu et al., 2014). 9 Related Work and Conclusion In terms of domain-conditioned statistics for word alignment, a distantly related research line (Tam et al., 2007; Zhao and Xing, 2008) focuses on using document topics to improve the word alignment. In terms of learning word alignment with partial supervision, another distantly related research line focuses on semi-supervised training with partial manual alignments (Fraser and Marcu, 2006; Gao and Vogel, 2010; Gao et al., 2010). Finally, recent 12 Note that better results correspond to larger BLEU, METEOR and to smaller TER. 406 work also focuses on data selection (Kirchhoff and Bilmes, 2014; Cuong and Sima’an, 2014b), mixture models (Carpuat et al., 2014), instance weighting (Foster et al., 2010) and latent variable models (Cuong and Sima’an, 2014a) over heterogeneous corpora. One main contribution of this work is the novelty of exploring the quality of word alignment in heterogeneous corpora. This, surprisingly, has not received much attention thus far (see the study of Bach et al. (2008) and Gao et al. (2011) for"
N15-1043,2011.mtsummit-papers.10,0,0.293365,"domains. This suggests that the translation probabilities for words will be as fractioned as the diversity of its translations across the domains. Furthermore, because the IBM and HMM alignment models use context-insensitive conditional probabilities, in heterogeneous corpora the estimates of these probabilities will be aggregated over different domains. Both issues could lead to suboptimal word alignment quality. Surprisingly, the insensitivity of the existing IBM and HMM alignment models to domain differences has not received much attention thus far (see the study of Bach et al. (2008) and Gao et al. (2011) for reference in the literature). We conjecture that this is because it is not fully clear how to define what constitutes a (sub)-domain. In this paper we propose to exploit the contrast between the alignment statistics in a handful of seed samples from different domains in order to induce domain-conditioned probabilities for each sentence pair in the heterogeneous corpus. Crucially, some sentence pairs will be more similar to a seed domain than others, whereas some sentence 398 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 398–408, c"
N15-1043,graca-etal-2008-building,0,0.0177908,"4.59 61.29 61.72 62.29 63.58 63.30 +0.43 +1.00 +2.29 +2.01 32.10 36.00 35.36 35.17 33.63 33.68 -0.64 -0.83 -2.37 -2.32 +0.51 +1.64 +1.11 +3.17 65.30 61.58 62.58 64.01 63.23 63.87 +1.00 +2.43 +1.65 +2.29 30.56 35.22 34.43 33.13 33.81 32.53 -0.79 -2.09 -1.41 -2.69 +0.32 +1.14 +2.38 +2.79 65.95 64.30 62.80 63.94 64.44 64.30 -1.50 -0.36 +0.14 ±0.0 29.58 33.26 33.94 32.93 32.10 31.99 +0.68 -0.33 -1.16 -1.27 Table 1: Alignment accuracy over heterogeneous corpora. 7 Word Alignment Experiment For alignment accuracy evaluation, we use a data set of 100 sentence pairs with their “golden” alignment from Graca et al. (2008). Here, the golden alignment consists of sure links (S) and possible links (P ) for each sentence pair. Counting the set of generating alignment links (A), we report the word alignment | |A∩S| accuracy by precision ( |A∩P |P |), recall ( |S |), align|+|A∩S| ment error rate (AER) (1 − |A∩P ) (Och and |A|+|S| 9 Ney, 2003). For all experiments, we use the same training configuration for both the baseline/the latent domain alignment model: 5 iterations for IBM model 1/the latent domain model; 3 iterations for HMM alignment model/the latent domain model. For evaluation, we first align the sentence"
N15-1043,J10-3007,0,0.0179334,"ilities: YJ P (fj |eaj )P (aj |aj−1 ). (1) P (f, a |e) = j=1 Here, P (fj |eaj ) represents the word translation probabilities and P (aj |aj−1 )1 represents the transition probabilities between positions. Note that P (aj |aj−1 ) depends only on the distance (aj − aj−1 ). Note also that the first-order dependency model is an extension of the uniform dependency model and zero-order dependency model of IBM models 1 and 2, respectively. In this work, we model explicitly distances in the range ±5. Note that null-links are also explicitly added in our implementation, following Och and Ney (2003) and Graca et al. (2010). Once the HMM alignment model is trained, the ˆ for each sentence pair most probable alignment, a ˆ = argmaxa P (f, a |e). can be computed by: a Here, the search problem can be solved by the Viterbi algorithm. 3 Latent Domain HMM Alignment Model Because the heterogeneous data contains a mix of diverse domains, the induced statistics derived from word alignment models reflect translation preferences aggregated over these domains. In this sense, they can be considered domain-confused statistics (Cuong and Sima’an, 2014a). This work thus focuses on more representative statistics: the domaincondi"
N15-1043,E14-1035,0,0.0547403,"arably good performance to the MGIZA++-based system. When 1M data is considered, on three of four tasks, our system produces at least compatible translation accuracy to the corresponding MGIZA++-based system. Further analysis reveals that the improvement is due to not only the reduction in alignment error rate, but also the use of the domain-sensitive lexical features. Moreover, the domain-sensitive lexical features is particularly useful when the domain of the test data matches with the domain of seed samplers. This is also widely observed in the literature, e.g., see (Eidelman et al., 2012; Hasler et al., 2014; Hu et al., 2014). 9 Related Work and Conclusion In terms of domain-conditioned statistics for word alignment, a distantly related research line (Tam et al., 2007; Zhao and Xing, 2008) focuses on using document topics to improve the word alignment. In terms of learning word alignment with partial supervision, another distantly related research line focuses on semi-supervised training with partial manual alignments (Fraser and Marcu, 2006; Gao and Vogel, 2010; Gao et al., 2010). Finally, recent 12 Note that better results correspond to larger BLEU, METEOR and to smaller TER. 406 work also focu"
N15-1043,P14-1110,0,0.0605475,"ce to the MGIZA++-based system. When 1M data is considered, on three of four tasks, our system produces at least compatible translation accuracy to the corresponding MGIZA++-based system. Further analysis reveals that the improvement is due to not only the reduction in alignment error rate, but also the use of the domain-sensitive lexical features. Moreover, the domain-sensitive lexical features is particularly useful when the domain of the test data matches with the domain of seed samplers. This is also widely observed in the literature, e.g., see (Eidelman et al., 2012; Hasler et al., 2014; Hu et al., 2014). 9 Related Work and Conclusion In terms of domain-conditioned statistics for word alignment, a distantly related research line (Tam et al., 2007; Zhao and Xing, 2008) focuses on using document topics to improve the word alignment. In terms of learning word alignment with partial supervision, another distantly related research line focuses on semi-supervised training with partial manual alignments (Fraser and Marcu, 2006; Gao and Vogel, 2010; Gao et al., 2010). Finally, recent 12 Note that better results correspond to larger BLEU, METEOR and to smaller TER. 406 work also focuses on data select"
N15-1043,D14-1014,0,0.0867624,"elated Work and Conclusion In terms of domain-conditioned statistics for word alignment, a distantly related research line (Tam et al., 2007; Zhao and Xing, 2008) focuses on using document topics to improve the word alignment. In terms of learning word alignment with partial supervision, another distantly related research line focuses on semi-supervised training with partial manual alignments (Fraser and Marcu, 2006; Gao and Vogel, 2010; Gao et al., 2010). Finally, recent 12 Note that better results correspond to larger BLEU, METEOR and to smaller TER. 406 work also focuses on data selection (Kirchhoff and Bilmes, 2014; Cuong and Sima’an, 2014b), mixture models (Carpuat et al., 2014), instance weighting (Foster et al., 2010) and latent variable models (Cuong and Sima’an, 2014a) over heterogeneous corpora. One main contribution of this work is the novelty of exploring the quality of word alignment in heterogeneous corpora. This, surprisingly, has not received much attention thus far (see the study of Bach et al. (2008) and Gao et al. (2011) for reference in the literature). Another major contribution of this work is a learning framework for latent domain word alignment with partial supervision using seed dom"
N15-1043,N03-1017,0,0.0290085,"ch sentence pair. Counting the set of generating alignment links (A), we report the word alignment | |A∩S| accuracy by precision ( |A∩P |P |), recall ( |S |), align|+|A∩S| ment error rate (AER) (1 − |A∩P ) (Och and |A|+|S| 9 Ney, 2003). For all experiments, we use the same training configuration for both the baseline/the latent domain alignment model: 5 iterations for IBM model 1/the latent domain model; 3 iterations for HMM alignment model/the latent domain model. For evaluation, we first align the sentence pairs in both directions and then symmetrize them using the growdiag-final heuristic (Koehn et al., 2003). For reference we also report the performance of a considerably more expressive Model 4, capable of capturing more structure, but at the expense of intractable inference. Using MGIZA++ (Gao and Vo9 Note that better results correspond to larger Precision, Recall and to smaller AER. 403 gel, 2008), we run 5 iterations for training Model 1, 3 iterations for training the HMM alignment model, Model 3 and Model 4. 7.1 Learning with Single Domain We first examine the binary case, where we are given domain information in advance for each kind of samples only, e.g., Legal, or Pharmacy, or Hardware. Fo"
N15-1043,2005.mtsummit-papers.11,0,0.121525,"≤ 1, according to Jensen’s inequality. With Eq. 5, it is straightforward to design a dynamic programming algorithm to decoding, e.g., the Viterbi algorithm. In practice, we observe that the approximation yields good results. Later experiments on word alignment will present this in detail. Experimental Setup In the following experiments, we use three heterogeneous English-Spanish corpora consisting of 1M , 2M and 4M sentence pairs respectively. These corpora combine two parts. The first part respectively 0.7M , 1.7M and 3.7M is collected from multiple domains and resources including EuroParl (Koehn, 2005), Common Crawl, United Nation, News Commentary. The second part consists of three domainexemplifying samples consisting of roughly 100K sentence pairs for each one (total 300K). Each of these three samples (manually collected by a commercial partner) exemplifies a specific domain related to Legal, Hardware and Pharmacy. Outlook In Section 7 we examine the word alignment yielded by the HMM alignment model and our latent domain HMM alignment model. In Section 8 we proceed further to examine the translation produced by derived SMT systems. 8 7 X Alternative solutions could be Lagrangian relaxatio"
N15-1043,N06-1014,0,0.0441882,"decision) while predicting the translation. Formally for each sentence pair, ˆ as he, fi, we can find their best Viterbi alignment, a follows: ˆ = argmaxa a = argmaxa = argmaxa XD XD D P (f, a, D|e) P (f, a|e, D)P (D|e) P (f, a|e, D)P (e|D)P (D). Here, we derive the last equation by applying Bayes’ rule to P (D |e), i.e., P (D |e) ∝ P (e |D)P (D). Interestingly, our Viterbi decoding now relies on a mix of domain-conditioned statistics P for each sentence pair. The computing of term D (a) for all possible alignments, a, however, is intractable, making the search problem difficult. Inspired by Liang et al. (2006), we opt instead for a heuristic objective function as follows8 : Y ˆ = argmax a P (f, a |e, D)P (e |D)P (D) . (5) a 6 Q D P Here, note that p is a lower bound for p, when 0 ≤ p ≤ 1, according to Jensen’s inequality. With Eq. 5, it is straightforward to design a dynamic programming algorithm to decoding, e.g., the Viterbi algorithm. In practice, we observe that the approximation yields good results. Later experiments on word alignment will present this in detail. Experimental Setup In the following experiments, we use three heterogeneous English-Spanish corpora consisting of 1M , 2M and 4M sen"
N15-1043,J03-1002,0,0.0303973,"n and transition probabilities: YJ P (fj |eaj )P (aj |aj−1 ). (1) P (f, a |e) = j=1 Here, P (fj |eaj ) represents the word translation probabilities and P (aj |aj−1 )1 represents the transition probabilities between positions. Note that P (aj |aj−1 ) depends only on the distance (aj − aj−1 ). Note also that the first-order dependency model is an extension of the uniform dependency model and zero-order dependency model of IBM models 1 and 2, respectively. In this work, we model explicitly distances in the range ±5. Note that null-links are also explicitly added in our implementation, following Och and Ney (2003) and Graca et al. (2010). Once the HMM alignment model is trained, the ˆ for each sentence pair most probable alignment, a ˆ = argmaxa P (f, a |e). can be computed by: a Here, the search problem can be solved by the Viterbi algorithm. 3 Latent Domain HMM Alignment Model Because the heterogeneous data contains a mix of diverse domains, the induced statistics derived from word alignment models reflect translation preferences aggregated over these domains. In this sense, they can be considered domain-confused statistics (Cuong and Sima’an, 2014a). This work thus focuses on more representative sta"
N15-1043,P02-1040,0,0.0920283,"Missing"
N15-1043,P12-1099,0,0.0779181,"racy resulting SMT systems produce. We hope this study sparks a new research direction for using domain samples, which is cheap to gather, but has not been exploited before. One obvious direction for future work might be to integrate the model into fertility-based alignment models (Brown et al., 1993), as well as other recently advanced alignment frameworks, e.g., (Simion et al., 2013; Tamura et al., 2014; Chang et al., 2014). Another interesting direction might be to integrate our model into advanced mixing multiple translation models, improving SMT systems trained on the heterogeneous data (Razmara et al., 2012; Sennrich et al., 2013; Carpuat et al., 2014). Finally, an open question is whether it is possible to learn the latent domain alignment model in a fully unsupervised style. This challenge deserves more attention in future work. Acknowledgements We are indebted to Ivan Titov and three anonymous reviewers for their constructive comments on earlier versions. The first author is supported by the EXPERT (EXPloiting Empirical appRoaches to Translation) Initial Training Network (ITN) of the European Union’s Seventh Framework Programme. The second author is supported by VICI grant nr. 27789-002 from"
N15-1043,P13-1082,0,0.0596534,"tems produce. We hope this study sparks a new research direction for using domain samples, which is cheap to gather, but has not been exploited before. One obvious direction for future work might be to integrate the model into fertility-based alignment models (Brown et al., 1993), as well as other recently advanced alignment frameworks, e.g., (Simion et al., 2013; Tamura et al., 2014; Chang et al., 2014). Another interesting direction might be to integrate our model into advanced mixing multiple translation models, improving SMT systems trained on the heterogeneous data (Razmara et al., 2012; Sennrich et al., 2013; Carpuat et al., 2014). Finally, an open question is whether it is possible to learn the latent domain alignment model in a fully unsupervised style. This challenge deserves more attention in future work. Acknowledgements We are indebted to Ivan Titov and three anonymous reviewers for their constructive comments on earlier versions. The first author is supported by the EXPERT (EXPloiting Empirical appRoaches to Translation) Initial Training Network (ITN) of the European Union’s Seventh Framework Programme. The second author is supported by VICI grant nr. 27789-002 from the Netherlands Organiz"
N15-1043,D13-1164,0,0.0797273,"ments show that the derived domain-conditioned statistics, once combined together, produce notable improvements both in word alignment accuracy and in translation accuracy of their resulting SMT systems. 1 Introduction Word alignment currently constitutes the basis for phrase extraction and reordering in phrase-based systems, and its statistics provide lexical parameters used for smoothing the phrase pair estimates. For over two decades since IBM models (Brown et al., 1993) and the HMM alignment model (Vogel et al., 1996), word alignment remains an active research line, e.g., see recent work (Simion et al., 2013; Tamura et al., 2014; Chang et al., 2014). During the past years we witnessed an increasing need to collect and use large heterogeneous parallel corpora from different domains and sources, e.g., News, Wikipedia, Parliament Proceedings. It is tacitly assumed that assembling a larger corpus Intuitively, in heterogeneous data certain words are present across many domains, whereas others are more specific to few domains. This suggests that the translation probabilities for words will be as fractioned as the diversity of its translations across the domains. Furthermore, because the IBM and HMM ali"
N15-1043,2006.amta-papers.25,0,0.0818687,"Missing"
N15-1043,P14-1138,0,0.0783231,"erived domain-conditioned statistics, once combined together, produce notable improvements both in word alignment accuracy and in translation accuracy of their resulting SMT systems. 1 Introduction Word alignment currently constitutes the basis for phrase extraction and reordering in phrase-based systems, and its statistics provide lexical parameters used for smoothing the phrase pair estimates. For over two decades since IBM models (Brown et al., 1993) and the HMM alignment model (Vogel et al., 1996), word alignment remains an active research line, e.g., see recent work (Simion et al., 2013; Tamura et al., 2014; Chang et al., 2014). During the past years we witnessed an increasing need to collect and use large heterogeneous parallel corpora from different domains and sources, e.g., News, Wikipedia, Parliament Proceedings. It is tacitly assumed that assembling a larger corpus Intuitively, in heterogeneous data certain words are present across many domains, whereas others are more specific to few domains. This suggests that the translation probabilities for words will be as fractioned as the diversity of its translations across the domains. Furthermore, because the IBM and HMM alignment models use con"
N15-1043,C96-2141,0,0.552357,"imating sharper, domain-conditioned word alignment statistics for sentence pairs. Our experiments show that the derived domain-conditioned statistics, once combined together, produce notable improvements both in word alignment accuracy and in translation accuracy of their resulting SMT systems. 1 Introduction Word alignment currently constitutes the basis for phrase extraction and reordering in phrase-based systems, and its statistics provide lexical parameters used for smoothing the phrase pair estimates. For over two decades since IBM models (Brown et al., 1993) and the HMM alignment model (Vogel et al., 1996), word alignment remains an active research line, e.g., see recent work (Simion et al., 2013; Tamura et al., 2014; Chang et al., 2014). During the past years we witnessed an increasing need to collect and use large heterogeneous parallel corpora from different domains and sources, e.g., News, Wikipedia, Parliament Proceedings. It is tacitly assumed that assembling a larger corpus Intuitively, in heterogeneous data certain words are present across many domains, whereas others are more specific to few domains. This suggests that the translation probabilities for words will be as fractioned as th"
N15-1043,P07-2045,0,\N,Missing
N15-1043,W13-2201,0,\N,Missing
N15-1043,N04-1021,0,\N,Missing
N18-1092,P05-1074,0,0.0930066,"R and E N -D E . Note that these two systems perform sometimes better sometimes worse depending on the benchmark. There is no clear pattern, but differences may well come from some qualitative difference in the induced latent space. It is a known fact that different languages realise lexical ambiguities differently, thus representations induced towards different languages are likely to capture different generalisations.8 As C OMBO results show, the representations induced from different corpora are somewhat complementary. That same observation has guided paraphrasing models based on pivoting (Bannard and Callison-Burch, 2005). Once more we report a monolingual variant of E MBEDA LIGN (indicated by E N) in an attempt to illustrate how crucial the 6 http://scikit-learn.org/stable/ In Appendix A we provide bar plots marked with error bars (2 standard deviations). 8 We also acknowledge that our treatment of German is likely suboptimal due to the lack of subword features, as it can also be seen in AER results. 1017 7 translation signal is. Dataset 3.4 MTurk-771 SIMLEX-999 WS-353-ALL YP-130 VERB-143 MEN-TR-3k SimVerb-3500 RG-65 WS-353-SIM RW-STANFORD WS-353-REL MC-30 MTurk-287 Word similarity Word similarity benchmarks"
N18-1092,W14-3302,0,0.0315809,"Missing"
N18-1092,D15-1075,0,0.0323215,"compare words in context using a measure of overlap between distributions (e.g. KL divergence). We investigate our model’s performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference, paraphrasing, and text similarity. 1 Introduction Natural language processing applications often count on the availability of word representations trained on large textual data as a means to alleviate problems such as data sparsity and lack of linguistic resources (Collobert et al., 2011; Socher et al., 2011; Tu et al., 2017; Bowman et al., 2015). Traditional approaches to inducing word representations circumvent the need for explicit semantic annotation by capitalising on some form of indirect semantic supervision. A typical example is to fit a binary classifier to detect whether or not a target word is likely to co-occur with neighbouring words (Mikolov et al., 2013). If the binary classifier represents a word as a continuous vector, that vector will be trained to be discriminative of the contexts it co-occurs with, and thus words in similar contexts will have similar representations. Code available from https://github.com/ uva-slpl"
N18-1092,K16-1002,0,0.149238,"Missing"
N18-1092,J93-2003,0,0.221747,"|m, n) = p(z1m ) Pθ (xi |zi ) i=1 × n Y m X j=1 aj =1 (2) P (aj |m)Pθ (yj |zaj )dz1m Due to the conditional independences of our model, it is trivial to marginalise lexical alignments for any given latent embeddings z1m , but marginalising the embeddings themselves is intractable. Thus, we employ amortised mean field variational inference using the inference model qφ (z1m |xm 1 ) , m Y i=1 N (zi |ui , diag(si si )) (3) where each factor is a diagonal Gaussian. We map m from xm 1 to a sequence u1 of independent posterior 2 We pad L1 sentences with N ULL to account for untranslatable L2 words (Brown et al., 1993). Instead, Schulz et al. (2016) generate untranslatable words from L2 context—an alternative we leave for future work. 1012 mean (or location) vectors, where ui , µ(hi ; φ), as well as a sequence sm 1 of independent standard deviation (or scale) vectors, where si , σ(hi ; φ), m and hm 1 = enc(x1 ; φ) is a deterministic encoding of the L1 sequence (we discuss concrete architectures in §2.3). All mappings are realised by neural networks whose parameters are collectively denoted by φ (the variational parameters). Note that we choose to approximate the posterior without conditioning on y1n . This"
N18-1092,D17-1070,0,0.191218,"W 2 VEC , employ bidirectional encoders. leave it as a rather speculative point. One additional point worth highlighting: the middle section of Table 7. E NBoW and E NBiRNN show what happens when we do not give E MBEDA LIGN L2 supervision at training. That is, imagine the model of Figure 1 without the bottom plate. In that case, the model representations overfit for L1 word-byword prediction. Without the need to predict any notion of context (monolingual or otherwise), the representations drift away from semantic-driven generalisations and fail at lexical substitution. 3.3 Sentence Evaluation Conneau et al. (2017) developed a framework to evaluate unsupervised sentence level representations trained on large amounts of data on a range of supervised NLP tasks. We assess our induced representations using their framework on the following benchmarks evaluated on classification ↑accuracy (MRPC is further evaluated on ↑F1) MR classification of positive or negative movie reviews; SST fined-grained labelling of movie reviews from the Stanford sentiment treebank (SST); TREC classification of questions into k-classes; CR classification of positive or negative product reviews; SUBJ classification of a sentence int"
N18-1092,P02-1033,0,0.066345,"istributional hypothesis hinges on the definition of context and different models are based on different definitions. Importantly, the nature of the context determines the range of linguistic properties the representations may capture (Levy and Goldberg, 2014b). For example, Levy and Goldberg (2014a) propose to use syntactic context derived from dependency parses. They show that their representations are much more discriminative of syntactic function than models based on immediate neighbourhood (Mikolov et al., 2013). In this work, we take lexical translation as indirect semantic supervision (Diab and Resnik, 2002). Effectively we make two assumptions. First, that every word has a foreign equivalent that stands for its meaning. Second, that we can find this equivalent in translation data through lexical alignments.1 For that we induce both a latent mapping between words in a bilingual sentence pair and distributions over latent word representations. To summarise our contributions: • we model a joint distribution over sentence pairs that generates data from latent word representations and latent lexical alignments; • we embed words in context mining positive correlations from translation data; • we find"
N18-1092,P14-5004,0,0.0809295,"with English-French only— recall that models based on that pair performed better in terms of AER. Results in this section are based on E MBEDA LIGN (with bidirectional variational encoder) trained on the Giga web corpus (see Table 1 for statistics). Due to the scale of the experiment, we report on a single run. We trained on Giga with the same hyperparameters that we trained on Europarl, however, for 3 epochs instead of 30 (with this dataset an epoch amounts to 183, 000 updates). Again, we performed model selection on AER. Table 9 shows the results for several datasets using the framework of Faruqui and Dyer (2014a). Note that E MBE DA LIGN was designed to make use of context information, thus this evaluation setup is a bit unnatural for our model. Still, it outperforms S KIP G RAM on 5 out of 13 benchmarks, in particular, on SIMLEX-999, whose relevance has been argued by Upadhyay et al. (2016). We also remark that this model achieves 0.25 test AER and 45.16 test GAP on lexical substitution—a considerable improvement compared to models trained on Europarl and reported in Tables 4 (AER) and 7 (GAP). 4 Related work Our model is inspired by lexical alignment models such as IBM1 (Brown et al., 1993), howev"
N18-1092,E14-1049,0,0.236194,"with English-French only— recall that models based on that pair performed better in terms of AER. Results in this section are based on E MBEDA LIGN (with bidirectional variational encoder) trained on the Giga web corpus (see Table 1 for statistics). Due to the scale of the experiment, we report on a single run. We trained on Giga with the same hyperparameters that we trained on Europarl, however, for 3 epochs instead of 30 (with this dataset an epoch amounts to 183, 000 updates). Again, we performed model selection on AER. Table 9 shows the results for several datasets using the framework of Faruqui and Dyer (2014a). Note that E MBE DA LIGN was designed to make use of context information, thus this evaluation setup is a bit unnatural for our model. Still, it outperforms S KIP G RAM on 5 out of 13 benchmarks, in particular, on SIMLEX-999, whose relevance has been argued by Upadhyay et al. (2016). We also remark that this model achieves 0.25 test AER and 45.16 test GAP on lexical substitution—a considerable improvement compared to models trained on Europarl and reported in Tables 4 (AER) and 7 (GAP). 4 Related work Our model is inspired by lexical alignment models such as IBM1 (Brown et al., 1993), howev"
N18-1092,P14-1006,0,0.0229007,"elation coefficient (↑). The first column is from (Faruqui and Dyer, 2014a). There is a vast literature on exploiting multilingual context to strengthen the notion of synonymy captured by monolingual models. Roughly, the literature splits into two groups, namely, approaches that derive additional features and/or training objectives based on pre-trained alignments (Klementiev et al., 2012; Faruqui and Dyer, 2014b; Luong et al., ˇ 2015; Suster et al., 2016), and approaches that promote a joint embedding space by working with sentence level representations that dispense with explicit alignments (Hermann and Blunsom, 2014; AP et al., 2014; Gouws et al., 2015; Hill et al., 2014). The work of Koˇcisk´y et al. (2014) is closer to ours in that they also learn embeddings by marginalising alignments, however, their model is conditional—much like IBM models—and their embeddings are not part of the probabilistic model, but rather part of the architecture design. The joint formulation allows our latent embeddings to harvest learning signal from L2 while still being driven by the learning signal from L1—in a conditional model the representations can become specific to alignment deviating from the purpose of well represe"
N18-1092,C12-1089,0,0.027609,".2172 0.5384 0.6962 0.3878 0.6094 0.6258 0.6698 0.5229 0.3887 0.3968 0.4784 0.4593 0.4191 0.3539 0.6389 0.4509 0.3278 0.3494 0.5559 0.3965 Table 9: Evaluation of English word embeddings out of context in terms of Spearman’s rank correlation coefficient (↑). The first column is from (Faruqui and Dyer, 2014a). There is a vast literature on exploiting multilingual context to strengthen the notion of synonymy captured by monolingual models. Roughly, the literature splits into two groups, namely, approaches that derive additional features and/or training objectives based on pre-trained alignments (Klementiev et al., 2012; Faruqui and Dyer, 2014b; Luong et al., ˇ 2015; Suster et al., 2016), and approaches that promote a joint embedding space by working with sentence level representations that dispense with explicit alignments (Hermann and Blunsom, 2014; AP et al., 2014; Gouws et al., 2015; Hill et al., 2014). The work of Koˇcisk´y et al. (2014) is closer to ours in that they also learn embeddings by marginalising alignments, however, their model is conditional—much like IBM models—and their embeddings are not part of the probabilistic model, but rather part of the architecture design. The joint formulation all"
N18-1092,2005.mtsummit-papers.11,0,0.0605266,"max (W1 zi + b1 ) g(zaj ; θ) = softmax W2 zaj + b2  (7a) LSTM hidden states (ith step) that process the sequence in opposite directions. We use 128 units for deterministic embeddings, and 100 units for LSTMs (Hochreiter and Schmidhuber, 1997) and latent representations (i.e. d = 100). 3 We start the section describing the data used to estimate our model’s parameters as well as details about the optimiser. The remainder of the section presents results on various benchmarks. Training data We train our model on bilingual parallel data. In particular, we use parliament proceedings (Europarl-v7) (Koehn, 2005) from two language pairs: English-French and EnglishGerman.4 We employed very minimal preprocessing, namely, tokenisation and lowercasing using scripts from M OSES (Koehn et al., 2007), and have discarded sentences longer than 50 tokens. Table 1 lists more information about the training data, including the English-French Giga web corpus (Bojar et al., 2014) which we use in §3.4.5 Corpus (7b) where W1 ∈ Rvx ×d , b1 ∈ Rvx , W2 ∈ Rvy ×d , b2 ∈ Rvy , and vx (vy ) is the size of the vocabulary of L1 (L2). With the approximation of §2.2, we replace  the L1 softmax layer (7a) by exp zi> cx + bx norm"
N18-1092,P07-2045,0,0.00462417,"ddings, and 100 units for LSTMs (Hochreiter and Schmidhuber, 1997) and latent representations (i.e. d = 100). 3 We start the section describing the data used to estimate our model’s parameters as well as details about the optimiser. The remainder of the section presents results on various benchmarks. Training data We train our model on bilingual parallel data. In particular, we use parliament proceedings (Europarl-v7) (Koehn, 2005) from two language pairs: English-French and EnglishGerman.4 We employed very minimal preprocessing, namely, tokenisation and lowercasing using scripts from M OSES (Koehn et al., 2007), and have discarded sentences longer than 50 tokens. Table 1 lists more information about the training data, including the English-French Giga web corpus (Bojar et al., 2014) which we use in §3.4.5 Corpus (7b) where W1 ∈ Rvx ×d , b1 ∈ Rvx , W2 ∈ Rvy ×d , b2 ∈ Rvy , and vx (vy ) is the size of the vocabulary of L1 (L2). With the approximation of §2.2, we replace  the L1 softmax layer (7a) by exp zi> cx + bx normalised by the CSS estimate (6) at training, and similarly for the L2 softmax layer (7b). In that case, we have parameters for cx , cy ∈ Rd —deterministic embeddings for x and y, respec"
N18-1092,P14-2037,0,0.0397872,"Missing"
N18-1092,P14-2050,0,0.0335999,"inative of the contexts it co-occurs with, and thus words in similar contexts will have similar representations. Code available from https://github.com/ uva-slpl/embedalign MR and WA contributed equally. The underlying assumption is that context (e.g. neighbouring words) stands for the meaning of the target word (Harris, 1954; Firth, 1957). The success of this distributional hypothesis hinges on the definition of context and different models are based on different definitions. Importantly, the nature of the context determines the range of linguistic properties the representations may capture (Levy and Goldberg, 2014b). For example, Levy and Goldberg (2014a) propose to use syntactic context derived from dependency parses. They show that their representations are much more discriminative of syntactic function than models based on immediate neighbourhood (Mikolov et al., 2013). In this work, we take lexical translation as indirect semantic supervision (Diab and Resnik, 2002). Effectively we make two assumptions. First, that every word has a foreign equivalent that stands for its meaning. Second, that we can find this equivalent in translation data through lexical alignments.1 For that we induce both a laten"
N18-1092,W14-1618,0,0.0342058,"inative of the contexts it co-occurs with, and thus words in similar contexts will have similar representations. Code available from https://github.com/ uva-slpl/embedalign MR and WA contributed equally. The underlying assumption is that context (e.g. neighbouring words) stands for the meaning of the target word (Harris, 1954; Firth, 1957). The success of this distributional hypothesis hinges on the definition of context and different models are based on different definitions. Importantly, the nature of the context determines the range of linguistic properties the representations may capture (Levy and Goldberg, 2014b). For example, Levy and Goldberg (2014a) propose to use syntactic context derived from dependency parses. They show that their representations are much more discriminative of syntactic function than models based on immediate neighbourhood (Mikolov et al., 2013). In this work, we take lexical translation as indirect semantic supervision (Diab and Resnik, 2002). Effectively we make two assumptions. First, that every word has a foreign equivalent that stands for its meaning. Second, that we can find this equivalent in translation data through lexical alignments.1 For that we induce both a laten"
N18-1092,W15-1521,0,0.0464016,"Missing"
N18-1092,W15-1501,0,0.063635,"task dispenses with inferences about L2. Each candidate is compared to the target word in context through a measure of overlap between their inferred densities—we take KL divergence. We then rank candidates using this measure. Table 7 lists GAP scores for variants of E M BEDA LIN (bottom section) as well as some baselines and other established methods (top section). For comparison, we also compute GAP by sorting candidates in terms of cosine similarity, in which case we take the Gaussian mean as a summary of the density. The top section of the table contains systems reported by Melamud et al. (2015) (R ANDOM and S KIP G RAM) and by Brazinskas et al. (2017) (BSG). Note that both S KIP G RAM (Mikolov et al., 2013) and BSG were trained on the very large ukWaC English corpus (Ferraresi et al., 2008). S KIP G RAM is known to perform remarkably well regardless of its apparent insensitivity to context (in terms of design). BSG is a close relative of our model which gives S KIP G RAM a Bayesian treatment (also by means of amortised variational inference) and is by design sensitive to context in a manner similar to E MBEDA LIGN, that is, through its inferred posteriors. Our first observation is t"
N18-1092,W03-0301,0,0.177076,"lso important to highlight that we do not employ regularisation techniques (such as batch normalisation, dropout, or L2 penalty) for they did not seem to yield consistent results. 3.1 Word alignment Since our model leverages learning signal from parallel data by marginalising latent lexical alignments, we use alignment error rate to double check whether the model learns sensible word correspondences. Intrinsic assessment of word alignment quality requires manual annotation. For EnglishFrench, we use the NAACL English-French handaligned data (37 sentence pairs for validation and 447 for test) (Mihalcea and Pedersen, 2003). For English-German, we use the data by Pad´o and Lapata (2006) (98 sentence pairs for validation and 987 for test). Alignment quality is then measured in terms of alignment error rate (AER) (Och and Ney, 2000)—an F-measure over predicted alignment links. For prediction we condition on the posterior means E[Z1m ] which is just the predicted variational means um 1 and select the L1 position for which P (yj , aj |um 1 ) is maximum (a form of approximate Viterbi alignment). Model L1 accuracy L2 accuracy B OW B OW α B I RNN B I RNNα ↓AER 95.59 ± 2.22 99.87 ± 0.22 95.72 ± 1.28 99.97 ± 0.09 5.69 ±"
N18-1092,P00-1056,0,0.411317,"ges learning signal from parallel data by marginalising latent lexical alignments, we use alignment error rate to double check whether the model learns sensible word correspondences. Intrinsic assessment of word alignment quality requires manual annotation. For EnglishFrench, we use the NAACL English-French handaligned data (37 sentence pairs for validation and 447 for test) (Mihalcea and Pedersen, 2003). For English-German, we use the data by Pad´o and Lapata (2006) (98 sentence pairs for validation and 987 for test). Alignment quality is then measured in terms of alignment error rate (AER) (Och and Ney, 2000)—an F-measure over predicted alignment links. For prediction we condition on the posterior means E[Z1m ] which is just the predicted variational means um 1 and select the L1 position for which P (yj , aj |um 1 ) is maximum (a form of approximate Viterbi alignment). Model L1 accuracy L2 accuracy B OW B OW α B I RNN B I RNNα ↓AER 95.59 ± 2.22 99.87 ± 0.22 95.72 ± 1.28 99.97 ± 0.09 5.69 ± 2.07 6.16 ± 0.39 7.31 ± 0.64 7.25 ± 0.62 35.41 ± 1.16 30.94 ± 2.49 34.32 ± 1.08 29.18 ± 1.91 bidirectional encoder in the variational approximation. Table 2 (3) lists ↓AER for E N -F R (E N -D E) as well as accu"
N18-1092,P06-1146,0,0.0602891,"Missing"
N18-1092,P16-2028,1,0.892072,"Missing"
N18-1092,D11-1014,0,0.127503,"Missing"
N18-1092,W17-2632,0,0.027447,"ich allows us to compare words in context using a measure of overlap between distributions (e.g. KL divergence). We investigate our model’s performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference, paraphrasing, and text similarity. 1 Introduction Natural language processing applications often count on the availability of word representations trained on large textual data as a means to alleviate problems such as data sparsity and lack of linguistic resources (Collobert et al., 2011; Socher et al., 2011; Tu et al., 2017; Bowman et al., 2015). Traditional approaches to inducing word representations circumvent the need for explicit semantic annotation by capitalising on some form of indirect semantic supervision. A typical example is to fit a binary classifier to detect whether or not a target word is likely to co-occur with neighbouring words (Mikolov et al., 2013). If the binary classifier represents a word as a continuous vector, that vector will be trained to be discriminative of the contexts it co-occurs with, and thus words in similar contexts will have similar representations. Code available from https:"
N18-1092,P16-1157,0,0.0181223,"we report on a single run. We trained on Giga with the same hyperparameters that we trained on Europarl, however, for 3 epochs instead of 30 (with this dataset an epoch amounts to 183, 000 updates). Again, we performed model selection on AER. Table 9 shows the results for several datasets using the framework of Faruqui and Dyer (2014a). Note that E MBE DA LIGN was designed to make use of context information, thus this evaluation setup is a bit unnatural for our model. Still, it outperforms S KIP G RAM on 5 out of 13 benchmarks, in particular, on SIMLEX-999, whose relevance has been argued by Upadhyay et al. (2016). We also remark that this model achieves 0.25 test AER and 45.16 test GAP on lexical substitution—a considerable improvement compared to models trained on Europarl and reported in Tables 4 (AER) and 7 (GAP). 4 Related work Our model is inspired by lexical alignment models such as IBM1 (Brown et al., 1993), however, we generate words y1n from a latent vector representation z1m of xm 1 , rather than directly from the observation xm . IBM1 takes L1 sequences as con1 ditioning context and does not model their distribution. Instead, we propose a joint model, where L1 sentences are generated from l"
N18-1092,N16-1160,0,0.0389102,"Missing"
P07-1037,koen-2004-pharaoh,0,0.0266213,"tory operator violations in a sequence of supertags (cf. Figure 2). For a supertag sequence of length (L) which has (V ) operator violations (as measured by the CCG system), the language model P will be adjusted as P ∗ = P × (1 − VL ). This is of course no longer a simple smoothed maximum-likelihood estimate nor is it a true probability. Nevertheless, this mechanism provides a simple, efficient integration of a global compositionality (grammaticality) measure into the n-gram language model over supertags. Decoder The decoder used in this work is Moses, a log-linear decoder similar to Pharaoh (Koehn, 2004), modified to accommodate supertag phrase probabilities and supertag language models. 5 Experiments In this section we present a number of experiments that demonstrate the effect of lexical syntax on translation quality. We carried out experiments on the NIST open domain news translation task from Arabic into English. We performed a number of experiments to examine the effect of supertagging approaches (CCG or LTAG) with varying data sizes. Data and Settings The experiments were conducted for Arabic to English translation and tested on the NIST 2005 evaluation set. The systems were trained on"
P07-1037,N03-1017,0,0.337572,"operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robust Within the field of Machine Translation, by far the 1 most dominant paradigm is Phrase-based Statistical These operators neither carry nor presuppose further linMachine Translation (PBSMT) (Koehn et al., 2003; guistic knowledge beyond what the lexicon contains. 288 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288–295, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and efficient supertagging approach: like standard taggers, supertaggers employ probabilities based on local context and can be implemented using finite state technology, e.g. Hidden Markov Models (Bangalore & Joshi, 1999). There are currently two supertagging approaches available: LTAG-based (Bangalore & Joshi, 1999) and CCG-based (Clark & Curran, 2004"
P07-1037,W06-1606,0,0.00887724,"Related Work Until very recently, the experience with adding syntax to PBSMT systems was negative. For example, (Koehn et al., 2003) demonstrated that adding syntax actually harmed the quality of their SMT system. Among the first to demonstrate improvement when adding recursive structure was (Chiang, 2005), who allows for hierarchical phrase probabilities that handle a range of reordering phenomena in the correct fashion. Chiang’s derived grammar does not rely on any linguistic annotations or assumptions, so that the ‘syntax’ induced is not linguistically motivated. Coming right up to date, (Marcu et al., 2006) demonstrate that ‘syntactified’ target language phrases can improve translation quality for Chinese– 289 English. They employ a stochastic, top-down transduction process that assigns a joint probability to a source sentence and each of its alternative translations when rewriting the target parse-tree into a source sentence. The rewriting/transduction process is driven by “xRS rules”, each consisting of a pair of a source phrase and a (possibly only partially) lexicalized syntactified target phrase. In order to extract xRS rules, the word-to-word alignment induced from the parallel training co"
P07-1037,W02-1018,0,0.0579825,"Missing"
P07-1037,P03-1021,0,0.0204594,"riments, we used the LTAG English supertagger5 (Bangalore 4 5 http://www.speech.sri.com/projects/srilm/ http://www.cis.upenn.edu/˜xtag/gramrelease.html & Joshi, 1999) to tag the English part of the parallel data and the supertag language model data. For the CCG supertag experiments, we used the CCG supertagger of (Clark & Curran, 2004) and the Edinburgh CCG tools6 to tag the English part of the parallel corpus as well as the CCG supertag language model data. The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training (Och, 2003). Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3. We built two baseline systems with two different-sized training sets: ‘Base-SMALL’ (5 million words) and ‘Base-LARGE’ (50 million words) as described above. Both systems use a trigram language model built using 250 million words from the English GigaWord Corpus. Table 1 presents the BLEU scores (Papineni et al., 2002) of both systems on the NIST 2005 MT Evaluation test set. System Base-SMALL Base-LARGE BLEU Score 0.4008 0.4418 Table 1: Baseline systems’ BLEU scores 5.1 Baseline vs. Supertags on S"
P07-1037,J99-2004,0,0.857417,"5) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax, e.g. Lexicalized Tree-Adjoining Grammar (Joshi & Schabes, 1992; Bangalore & Joshi, 1999) and Combinary Categorial Grammar (Steedman, 2000). In these linguistic approaches, it is assumed that the grammar consists of a very rich lexicon and a tiny, impoverished1 set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robus"
P07-1037,P05-1033,0,0.0735743,"on enriching PBSMT with syntactic structure. In section 3, we describe the baseline PBSMT system which our work extends. In section 4, we detail our approach. Section 5 describes the experiments carried out, together with the results obtained. Section 6 concludes, and provides avenues for further work. 2 Related Work Until very recently, the experience with adding syntax to PBSMT systems was negative. For example, (Koehn et al., 2003) demonstrated that adding syntax actually harmed the quality of their SMT system. Among the first to demonstrate improvement when adding recursive structure was (Chiang, 2005), who allows for hierarchical phrase probabilities that handle a range of reordering phenomena in the correct fashion. Chiang’s derived grammar does not rely on any linguistic annotations or assumptions, so that the ‘syntax’ induced is not linguistically motivated. Coming right up to date, (Marcu et al., 2006) demonstrate that ‘syntactified’ target language phrases can improve translation quality for Chinese– 289 English. They employ a stochastic, top-down transduction process that assigns a joint probability to a source sentence and each of its alternative translations when rewriting the targ"
P07-1037,C04-1041,0,0.660761,"(Koehn et al., 2003; guistic knowledge beyond what the lexicon contains. 288 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288–295, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and efficient supertagging approach: like standard taggers, supertaggers employ probabilities based on local context and can be implemented using finite state technology, e.g. Hidden Markov Models (Bangalore & Joshi, 1999). There are currently two supertagging approaches available: LTAG-based (Bangalore & Joshi, 1999) and CCG-based (Clark & Curran, 2004). Both the LTAG (Chen et al., 2006) and the CCG supertag sets (Hockenmaier, 2003) were acquired from the WSJ section of the Penn-II Treebank using handbuilt extraction rules. Here we test both the LTAG and CCG supertaggers. We interpolate (log-linearly) the supertagged components (language model and phrase table) with the components of a standard PBSMT system. Our experiments on the Arabic– English NIST 2005 test suite show that each of the supertagged systems significantly improves over the baseline PBSMT system. Interestingly, combining the two taggers together diminishes the benefits of sup"
P07-1037,C92-2066,0,0.329956,"tem. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax, e.g. Lexicalized Tree-Adjoining Grammar (Joshi & Schabes, 1992; Bangalore & Joshi, 1999) and Combinary Categorial Grammar (Steedman, 2000). In these linguistic approaches, it is assumed that the grammar consists of a very rich lexicon and a tiny, impoverished1 set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory ope"
P07-1037,P02-1040,0,0.106815,"corpus as well as the CCG supertag language model data. The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training (Och, 2003). Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3. We built two baseline systems with two different-sized training sets: ‘Base-SMALL’ (5 million words) and ‘Base-LARGE’ (50 million words) as described above. Both systems use a trigram language model built using 250 million words from the English GigaWord Corpus. Table 1 presents the BLEU scores (Papineni et al., 2002) of both systems on the NIST 2005 MT Evaluation test set. System Base-SMALL Base-LARGE BLEU Score 0.4008 0.4418 Table 1: Baseline systems’ BLEU scores 5.1 Baseline vs. Supertags on Small Data Sets We compared the translation quality of the baseline systems with the LTAG and CCG supertags systems (LTAG-SMALL and CCG-SMALL). The results are System Base-SMALL LTAG-SMALL CCG-SMALL BLEU Score 0.4008 0.4205 0.4174 Table 2: LTAG and CCG systems on small data given in Table 2. All systems were trained on the same parallel data. The LTAG supertag-based system outperforms the baseline by 1.97 BLEU point"
P07-1037,N03-2036,0,0.0279423,"Missing"
P07-1037,J03-1002,0,\N,Missing
P11-1065,N09-1025,0,0.0180111,"Missing"
P11-1065,P05-1033,0,0.890334,"rovements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target. 1 Introduction Recent advances in Statistical Machine Translation (SMT) are widely centred around two concepts: (a) hierarchical translation processes, frequently employing Synchronous Context Free Grammars (SCFGs) and (b) transduction or synchronous rewrite processes over a linguistic syntactic tree. SCFGs in the form of the Inversion-Transduction Grammar (ITG) were first introduced by (Wu, 1997) as a formalism to recursively describe the translation process. The Hiero system (Chiang, 2005) 642 Khalil Sima’an ILLC University of Amsterdam k.simaan@uva.nl utilised an ITG-flavour which focused on hierarchical phrase-pairs to capture context-driven translation and reordering patterns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic st"
P11-1065,P10-1146,0,0.422063,"s a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistic"
P11-1065,D09-1037,0,0.0266163,"). However, the synchronous grammars we learn share few similarities with those that they heuristically extract. The HR-SCFG we adopt allows capturing more complex reordering phenomena and, in contrast to both (Chiang, 2005; Zollmann and Venugopal, 2006), is not exposed to the issues highlighted in section 2.1. Nevertheless, our results underline the capacity of linguistic annotations similar to those of (Zollmann and Venugopal, 2006) as part of latent translation variables. Most of the aforementioned work does concentrate on learning hierarchical, linguistically motivated translation models. Cohn and Blunsom (2009) sample rules of the form proposed in (Galley et al., 2004) from a Bayesian model, employing Dirichlet Process priors favouring smaller rules to avoid overfitting. Their grammar is however also based on the target parse-tree structure, with their system surpassing a weak baseline by a small margin. In contrast to the Bayesian approach which imposes external priors to lead estimation away from degenerate solutions, we take a data-driven approach to arrive to estimates which generalise well. The rich linguistically motivated latent variable learnt by our method delivers translation performance t"
P11-1065,W06-3105,0,0.0234601,"ped and trained for translation. We start by labelling each phrase-pair span in the word-aligned training data with multiple linguistically motivated categories, offering multi-grained abstractions from its lexical content. These phrasepair label charts are the input of our learning algorithm, which extracts the linguistically motivated rules and estimates the probabilities for a stochastic SCFG, without arbitrary constraints such as phrase or span sizes. Estimating such grammars under a Maximum Likelihood criterion is known to be plagued by strong overfitting leading to degenerate estimates (DeNero et al., 2006). In contrast, our learning objective not only avoids overfitting the training data but, most importantly, learns joint stochastic synchronous grammars which directly aim at generalisation towards yet unseen instances. By advancing from structures which mimic linguistic syntax, to learning linguistically aware latent recursive structures targeting translation, we achieve significant improvements in translation quality for 4 different language pairs in comparison with a strong hierarchical translation baseline. Our key contributions are presented in the following sections. Section 2 discusses t"
P11-1065,N04-1035,0,0.175129,"itive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to bett"
P11-1065,P06-1121,0,0.167198,"ticularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based S"
P11-1065,P07-1019,0,0.00428671,"rminals covering phrase-pairs as well as the higher level parts of the derivation. In this manner we not only constrain the translation hypotheses resulting in faster decoding time, but, more importantly, we may ground the hypotheses more closely to the available linguistic information of the source sentence. This is of particular interest as we move up the derivation tree, where 647 an initial wrong choice below could propagate towards hypotheses wildly diverging from the input sentence’s linguistic annotation. Per Non-Terminal Pruning The decoder uses a combination of beam and cube-pruning (Huang and Chiang, 2007). As our grammar uses non-terminals in the hundreds of thousands, it is important not to prune away prematurely non-terminals covering smaller spans and to leave more options to be considered as we move up the derivation tree. For this, for every cell in the decoder’s chart, we keep a separate bin per non-terminal and prune together hypotheses leading to the same non-terminal covering a cell. This allows full derivations to be found for all input sentences, as well as avoids aggressive pruning at an early stage. Given the source label constraint discussed above, this does not increase running"
P11-1065,2006.amta-papers.8,0,0.0286207,"f learning latent structure with syntax and linguistic annotations, exploring the crossroads of machine 649 learning, linguistic syntax and machine translation. Training a joint probability model was first discussed in (Marcu and Wong, 2002). We show that a translation system based on such a joint model can perform competitively in comparison with conditional probability models, when it is augmented with a rich latent hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically"
P11-1065,D10-1014,0,0.365061,"Missing"
P11-1065,N03-1017,0,0.143913,"ical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based syst"
P11-1065,W04-3250,0,0.0296604,"Missing"
P11-1065,2005.mtsummit-papers.11,0,0.00604075,"at we do not prune the phrase-pair emitting rules. Overall, we consider this a much more informed pruning criterion than those based on probability values (that are not comparable across left-hand sides) or righthand side counts (frequent symbols need many more expansions than a highly specialised one). 4.3 Experimental Setting & Baseline We evaluate our method on four different language pairs with English as the source language and French, German, Dutch and Chinese as target. The data for the first three language pairs are derived from parliament proceedings sourced from the Europarl corpus (Koehn, 2005), with WMT07 development and test data for French and German. The data for the English to Chinese task is composed of parliament proceedings and news articles. For all language pairs we employ 200K and 400K sentence pairs for training, 2K for development and 2K for testing (single reference per source sentence). Both the baseline and our method decode Training English to set size josh-base 200K lts 400K josh-base lts French BLEU NIST 29.20 7.2123 29.43 7.2611** German BLEU NIST 18.65 5.8047 19.10** 5.8714** Dutch BLEU NIST 21.97 6.2469 22.31* 6.2903* Chinese BLEU NIST 22.34 6.5540 23.67** 6.65"
P11-1065,W09-0424,0,0.016765,"a separate joint phrase-pair emission distribution per non-terminal, the smoothing features (a) above assess the conditional translation of surface phrases irrespective of any notion of recursive translation structure. The final feature is the language model score for the target sentence, mounting up to the following model used at decoding time, with the feature weights λ trained by Minimum Error Rate Training (MERT) (Och, 2003) on a development corpus. ∗ p(D ⇒ he, f i) ∝ p(e)λlm pG (D)λG n Y Y φi (r)λi i=1 r∈D 4.2 Decoding Modifications We use a customised version of the Joshua SCFG decoder (Li et al., 2009) to translate, with the following modifications: Source Labels Constraints As for this work the phrase-pair labels used to extract the grammar are based on the linguistic analysis of the source side, we can construct the label chart for every input sentence from its parse. We subsequently use it to consider only derivations with synchronous spans which are covered by non-terminals matching one of the labels for those spans. This applies both for the nonterminals covering phrase-pairs as well as the higher level parts of the derivation. In this manner we not only constrain the translation hypot"
P11-1065,P06-1077,0,0.0784363,"rns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring"
P11-1065,P09-1063,0,0.0154092,"Missing"
P11-1065,W02-1018,0,0.019363,"e but slows down decoding speed. Notably, as can be seen in Table 2(b), switching to a 4-gram LM results in performance gains for both the baseline and our system and while the margin between the two systems decreases, our system continues to deliver a considerable and significant improvement in translation BLEU scores. 5 Related Work In this work, we focus on the combination of learning latent structure with syntax and linguistic annotations, exploring the crossroads of machine 649 learning, linguistic syntax and machine translation. Training a joint probability model was first discussed in (Marcu and Wong, 2002). We show that a translation system based on such a joint model can perform competitively in comparison with conditional probability models, when it is augmented with a rich latent hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguis"
P11-1065,P08-1114,0,0.180305,"ented with a rich latent hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent syntactic categories (Huang et al., 2010). A feature-based approach and ours are not mutually exclusive, as we also employ a limited set of features next to our trained model during decoding. We find augmenting our system with a more extensive feature set an interesting research direction for the future. An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets"
P11-1065,D08-1066,1,0.895695,"Missing"
P11-1065,W10-2915,1,0.89571,"Missing"
P11-1065,P03-1021,0,0.0162761,"t the synchronous grammar derivation and learning general reordering or word emission preferences for the language pair. As an example, while our probabilistic HR-SCFG maintains a separate joint phrase-pair emission distribution per non-terminal, the smoothing features (a) above assess the conditional translation of surface phrases irrespective of any notion of recursive translation structure. The final feature is the language model score for the target sentence, mounting up to the following model used at decoding time, with the feature weights λ trained by Minimum Error Rate Training (MERT) (Och, 2003) on a development corpus. ∗ p(D ⇒ he, f i) ∝ p(e)λlm pG (D)λG n Y Y φi (r)λi i=1 r∈D 4.2 Decoding Modifications We use a customised version of the Joshua SCFG decoder (Li et al., 2009) to translate, with the following modifications: Source Labels Constraints As for this work the phrase-pair labels used to extract the grammar are based on the linguistic analysis of the source side, we can construct the label chart for every input sentence from its parse. We subsequently use it to consider only derivations with synchronous spans which are covered by non-terminals matching one of the labels for t"
P11-1065,P05-1034,0,0.0252393,"and reordering patterns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues"
P11-1065,N09-1027,0,0.307503,"mplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based system (Venugopal et al., 2009; Chiang et al., 2009), obscuring the impact of higher level syntactic processes. While it is assumed that linguistic structure does correlate with some translation phenomena, in this Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 642–652, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics work we do not employ it as the backbone of translation. In place of linguistically constrained translation imposing syntactic parse structure, we opt for linguistically motivated translation. We learn latent hierarchical struc"
P11-1065,J10-2004,0,0.012953,"ng et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based system (Venugopal et al., 2009; Chiang et al., 2009), obscuring the impact of higher level syntactic processes. While it is assumed that linguistic structure does correlate with some translation phenomena, in this Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 642–652, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics work we do no"
P11-1065,J97-3002,0,0.336407,"directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target. 1 Introduction Recent advances in Statistical Machine Translation (SMT) are widely centred around two concepts: (a) hierarchical translation processes, frequently employing Synchronous Context Free Grammars (SCFGs) and (b) transduction or synchronous rewrite processes over a linguistic syntactic tree. SCFGs in the form of the Inversion-Transduction Grammar (ITG) were first introduced by (Wu, 1997) as a formalism to recursively describe the translation process. The Hiero system (Chiang, 2005) 642 Khalil Sima’an ILLC University of Amsterdam k.simaan@uva.nl utilised an ITG-flavour which focused on hierarchical phrase-pairs to capture context-driven translation and reordering patterns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful"
P11-1065,P01-1067,0,0.453418,"ntext-driven translation and reordering patterns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to"
P11-1065,P08-1064,0,0.0643051,"ith extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 20"
P11-1065,W06-3119,0,0.854567,"xplores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed thro"
P11-1065,A00-2018,0,\N,Missing
P16-2028,J93-2003,0,0.212355,"Missing"
P16-2028,N13-1073,0,0.105968,"Missing"
P16-2028,P13-2121,0,0.0310779,"Missing"
P16-2028,N09-1036,0,0.0804932,"an auxiliary variable (Tanner and Wong, 1987) that uniformly chooses only one possible new assignment per sampled link. The sampling complexity, which would normally be linear in the size of the target sentence, thus becomes constant. In practice this speed up the sampler by several orders of magnitude, making our aligner as fast as Giza++. Unfortunately, this strategy also slightly impairs the mobility of our sampler. 3.4 Decoding Our samples contain assignments of the A and Z variables. If for a word fj we have zj = 1, we treat the word as not aligned. We then use maximum marginal decoding (Johnson and Goldwater, 2009) over alignment links to generate final word alignments. This means that we align each source word to the target word it has been aligned to most often in the samples. If the word was unaligned in most samples, we leave it unaligned in the output alignment. 4  P Zj = 0|C−Zj , H ∝ Experiments and results (6) c(fj |eaj , z = 0) + α (c(z = 0|fj−1 ) + s) P (aj ) c(eaj |z = 0) + αVf  P Zj = 1|C−Zj , H ∝ (7) c(fj |fj−1 , z = 1) + β (c(z = 1|fj−1 ) + r) c(z = 1|fj−1 ) + βVf We present translation experiments on English paired with German, French, Czech and Japanese, thereby covering four language f"
P16-2028,N06-1014,0,0.115658,"nglish and the source side is usually taken to be French. 169 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 169–174, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in the jth position (fj ) is aligned to under am 1 . In IBM model 1 P (am ) is uniform. In IBM 1 model 2, all alignment links aj are assumed to be independent and follow a categorical distribution. Here, we choose to parametrise this categorical based on the distance between the two words to be aligned, as has been done by Vogel et al. (1996) and Liang et al. (2006). Thus, in our IBM model 2    m m Y Y jl m P (a1 ) = P (aj ) = P i− (2) m j=1 q θa γ D s, r fprv θf f Sm S j=1 β Vf θe eS1 l α Ve Figure 1: A graphical representation of our model for S sentence pairs. We use Vf /e to denote the source/target vocabulary sizes and D to denote the number of possible alignment link configurations. Furthermore, Sm/l is the number of source/target words in the current sentence and fprv the source word preceding the one that we currently generate. Removing the NULL word 3.1 a Vf where i is the position of the English word that aj links to and the values l and m"
P16-2028,P11-2032,0,0.218989,"Missing"
P16-2028,J03-1002,0,0.0809498,"Missing"
P16-2028,P03-1021,0,0.2316,"Missing"
P16-2028,P02-1040,0,0.0948133,"Missing"
P16-2028,C96-2141,0,0.373139,"often identified with English and the source side is usually taken to be French. 169 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 169–174, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in the jth position (fj ) is aligned to under am 1 . In IBM model 1 P (am ) is uniform. In IBM 1 model 2, all alignment links aj are assumed to be independent and follow a categorical distribution. Here, we choose to parametrise this categorical based on the distance between the two words to be aligned, as has been done by Vogel et al. (1996) and Liang et al. (2006). Thus, in our IBM model 2    m m Y Y jl m P (a1 ) = P (aj ) = P i− (2) m j=1 q θa γ D s, r fprv θf f Sm S j=1 β Vf θe eS1 l α Ve Figure 1: A graphical representation of our model for S sentence pairs. We use Vf /e to denote the source/target vocabulary sizes and D to denote the number of possible alignment link configurations. Furthermore, Sm/l is the number of source/target words in the current sentence and fprv the source word preceding the one that we currently generate. Removing the NULL word 3.1 a Vf where i is the position of the English word that aj links to"
P16-2028,P07-2045,0,\N,Missing
P17-2004,W12-3129,0,0.0271692,"nd level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than–and on average outperforms– both models on both objectives. 1 Introduction Ever since BLEU (Papineni et al., 2002) many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made. Some proposals use additional information for extracting quality indicators, like paraphrasing (Denkowski and Lavie, 2011), syntactic trees (Liu and Gildea, 2005; Stanojevi´c and Sima’an, 2015) or shallow semantics (Rios et al., 2011; Lo et al., 2012) etc. Whereas others use different matching strategies, like n-grams (Papineni et al., 2002), treelets (Liu and Gildea, 2005) and skip-bigrams (Lin and Och, 2004). Most metrics use several indicators of translation quality which are often combined in a linear model whose weights are estimated on a training set of human judgments. 20 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 20–25 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2004 Now we can define"
P17-2004,W11-2107,0,0.0353856,"show empirical comparison against a metric trained for sentencelevel exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than–and on average outperforms– both models on both objectives. 1 Introduction Ever since BLEU (Papineni et al., 2002) many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made. Some proposals use additional information for extracting quality indicators, like paraphrasing (Denkowski and Lavie, 2011), syntactic trees (Liu and Gildea, 2005; Stanojevi´c and Sima’an, 2015) or shallow semantics (Rios et al., 2011; Lo et al., 2012) etc. Whereas others use different matching strategies, like n-grams (Papineni et al., 2002), treelets (Liu and Gildea, 2005) and skip-bigrams (Lin and Och, 2004). Most metrics use several indicators of translation quality which are often combined in a linear model whose weights are estimated on a training set of human judgments. 20 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 20–25 c Vancouver, Canada,"
P17-2004,W08-0331,0,0.0639621,"Missing"
P17-2004,N16-1001,0,0.0115894,"proves even more on both levels of RR correlation and outperforms both singleobjective models on average and on 4 out of 7 languages. Making confident conclusions from these results is difficult because, to the best of our knowledge, there is no principled way of measuring statistical significance on the RR judgments. That is why we also tested on direct assessment (DA) judgments available from WMT16. On DA we can measure statistical significance on the sentence level using Williams test (Graham et al., 2015) and on the corpus level using combination of hybrid-supersampling and Williams test (Graham and Liu, 2016). The results of correlation with human judgment are for sentence and corpus level are shown in Table 2. Feature Functions The feature functions that are used are reimplementation of many (but not all) feature functions of BEER. Because the point of this paper is about the exploration of different objective functions we did not try to experiment with more complex feature functions based on paraphrasing, function words or permutation trees. We use just simple precision, recall and 3 types of F-score (with β parameters 1, 2 and 0.5) over different “pieces” of translation: • character n-grams of"
P17-2004,W14-3336,0,0.0190825,"two corpora. For doing this we use an approach similar to Max-Margin Markov Networks (Taskar et al., 2003) where for each training instance we dynamically scale the margin that should be enforced. We want the margin between the scores ∆corp to be at least as big as the margin between the human scores ∆human assigned to these systems. In one mini-batch we will use only a randomly chosen pair of corpora with feature matrices Φcwin and Φclos for which we have a human comparison. The corpus level loss function is given by: 3 Experiments are conducted on WMT13 (Mach´acˇ ek and Bojar, 2013), WMT14 (Machacek and Bojar, 2014) and WMT16 (Bojar et al., 2016) datasets which were used as training, validation and testing datasets respectively. All of the models are implemented using TensorFlow1 and trained with L2 regularization λ = 0.001 and ADAM optimizer with learning rate 0.001. The mini-batch size for sentence level judgments is 2000 and for the corpus level is one comparison. Each model is trained for 200 epochs out of which the one performing best on the validation set for the objective function being optimized is used during the test time. We show the results for the relative ranking (RR) judgments correlation"
P17-2004,N15-1124,0,0.021574,"tives equal importance. 2.4 Experiments RR joint vs. single-objectives Training for the joint objective improves even more on both levels of RR correlation and outperforms both singleobjective models on average and on 4 out of 7 languages. Making confident conclusions from these results is difficult because, to the best of our knowledge, there is no principled way of measuring statistical significance on the RR judgments. That is why we also tested on direct assessment (DA) judgments available from WMT16. On DA we can measure statistical significance on the sentence level using Williams test (Graham et al., 2015) and on the corpus level using combination of hybrid-supersampling and Williams test (Graham and Liu, 2016). The results of correlation with human judgment are for sentence and corpus level are shown in Table 2. Feature Functions The feature functions that are used are reimplementation of many (but not all) feature functions of BEER. Because the point of this paper is about the exploration of different objective functions we did not try to experiment with more complex feature functions based on paraphrasing, function words or permutation trees. We use just simple precision, recall and 3 types"
P17-2004,W13-2202,0,0.0173233,"ould separate the scores of two corpora. For doing this we use an approach similar to Max-Margin Markov Networks (Taskar et al., 2003) where for each training instance we dynamically scale the margin that should be enforced. We want the margin between the scores ∆corp to be at least as big as the margin between the human scores ∆human assigned to these systems. In one mini-batch we will use only a randomly chosen pair of corpora with feature matrices Φcwin and Φclos for which we have a human comparison. The corpus level loss function is given by: 3 Experiments are conducted on WMT13 (Mach´acˇ ek and Bojar, 2013), WMT14 (Machacek and Bojar, 2014) and WMT16 (Bojar et al., 2016) datasets which were used as training, validation and testing datasets respectively. All of the models are implemented using TensorFlow1 and trained with L2 regularization λ = 0.001 and ADAM optimizer with learning rate 0.001. The mini-batch size for sentence level judgments is 2000 and for the corpus level is one comparison. Each model is trained for 200 epochs out of which the one performing best on the validation set for the objective function being optimized is used during the test time. We show the results for the relative r"
P17-2004,2012.iwslt-papers.5,0,0.0279994,"ce level: maximize the distance between the scores of “good” and “bad” corpora. In this case we have additional information that is not present on the sentence level: we know not only which corpus is (according to humans) better, but also by how much it is better. For f orward(Φ) = ΦT w + b 21 One final feature deals with length-disbalance. If the length of the system and reference translation are a and b respectively then this feature is computed as max(a,b)−min(a,b) . It is computed min(a,b) both for word and character length. that we can use one of the heuristics such as the Expected Wins (Koehn, 2012). We can use this information to guide the learning model by how much it should separate the scores of two corpora. For doing this we use an approach similar to Max-Margin Markov Networks (Taskar et al., 2003) where for each training instance we dynamically scale the margin that should be enforced. We want the margin between the scores ∆corp to be at least as big as the margin between the human scores ∆human assigned to these systems. In one mini-batch we will use only a randomly chosen pair of corpora with feature matrices Φcwin and Φclos for which we have a human comparison. The corpus level"
P17-2004,P02-1040,0,0.0976898,"e objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized. To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentencelevel exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than–and on average outperforms– both models on both objectives. 1 Introduction Ever since BLEU (Papineni et al., 2002) many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made. Some proposals use additional information for extracting quality indicators, like paraphrasing (Denkowski and Lavie, 2011), syntactic trees (Liu and Gildea, 2005; Stanojevi´c and Sima’an, 2015) or shallow semantics (Rios et al., 2011; Lo et al., 2012) etc. Whereas others use different matching strategies, like n-grams (Papineni et al., 2002), treelets (Liu and Gildea, 2005) and skip-bigrams (Lin and Och, 2004). Most metrics use several indicators of translation quality which are often combi"
P17-2004,W11-2112,0,0.0268731,"nguage pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than–and on average outperforms– both models on both objectives. 1 Introduction Ever since BLEU (Papineni et al., 2002) many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made. Some proposals use additional information for extracting quality indicators, like paraphrasing (Denkowski and Lavie, 2011), syntactic trees (Liu and Gildea, 2005; Stanojevi´c and Sima’an, 2015) or shallow semantics (Rios et al., 2011; Lo et al., 2012) etc. Whereas others use different matching strategies, like n-grams (Papineni et al., 2002), treelets (Liu and Gildea, 2005) and skip-bigrams (Lin and Och, 2004). Most metrics use several indicators of translation quality which are often combined in a linear model whose weights are estimated on a training set of human judgments. 20 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 20–25 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2004"
P17-2004,D14-1025,1,0.877182,"Missing"
P17-2004,W15-3050,1,0.892214,"Missing"
P17-2004,P04-1077,0,\N,Missing
P17-2004,W05-0904,0,\N,Missing
plank-simaan-2008-subdomain,J93-2004,0,\N,Missing
plank-simaan-2008-subdomain,P97-1003,0,\N,Missing
plank-simaan-2008-subdomain,W01-0521,0,\N,Missing
plank-simaan-2008-subdomain,A97-1015,0,\N,Missing
plank-simaan-2008-subdomain,J03-4003,0,\N,Missing
plank-simaan-2008-subdomain,P07-1034,0,\N,Missing
plank-simaan-2008-subdomain,P05-1022,0,\N,Missing
plank-simaan-2008-subdomain,P06-1043,0,\N,Missing
Q16-1008,D11-1033,0,0.0565742,"t translation task is used as a form of prior knowledge. Various techniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side"
Q16-1008,2011.iwslt-evaluation.18,0,0.138052,"specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain ("
Q16-1008,J93-2003,0,0.085355,"P (z) z 1 2 P (e|z) X P (f , a|e, z) a  X 1 + P (f |z) P (e, a0 |f , z) . 2 0 3. if the e-to-f direction is chosen then generate the pair relying on P (e |z)P (f |e, z); 4. otherwise, use P (f |z)P (e |f , z). 2 As we aim for a simple approach, our TMs are computed through the introduction of hidden alignments a and a0 in f -to-e and e-to-f P directions respectively, in which P (f | e, z) = a P (f , a |e, z) P 0 and P (e |f , z) = a0 P (e, a |f , z). To make the marginalization of alignments tractable, we restrict P (f , a |e, z) and P (e, a0 |f , z) to the same assumptions as IBM Model 1 (Brown et al., 1993) (i.e., a multiplication of translation of lexical probabilities with respect to latent subdomains). 1. generate the domain z from the prior P (z); 2. choose the generation direction: f -to-e or e-tof , with equal probability; 1 (4) a As there is no closed-form solution, we use the expectation-maximization (EM) algorithm (Dempster et al., 1977). In the E-step, we compute the posterior distribu2 Note that we effectively average between them which is reasonable, as there is no reason to give preference to any of them. tions P (a, z |e, f ) and P (a0 , z |e, f ) as follows  P (a, z |e, f ) ∝ P"
Q16-1008,P13-1141,0,0.026014,"3), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Carpuat et al., 2013; Carpuat et al., 2014; Mansour and Ney, 2014). Recently, there was some research on adapting simultaneously to multiple domains, the goal related to ours (Clark et al., 2012; Sennrich, 2012a). For instance, Clark et al. (2012) augment a phrase-based MT system with various domain indicator features to build a single system that performs well across a range of domains. Sennrich (2012a) proposed to cluster training data in an unsupervised fashion to build mixture models that yield good performance on multiple test domains. However, their approaches are very different from ours, that is minimizin"
Q16-1008,W14-3363,0,0.210242,"from the training data only; (2) introducing features which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural tra"
Q16-1008,N13-1114,0,0.221895,"ned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the latent subdomains they are computed f"
Q16-1008,P13-1126,0,0.225624,"ned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the latent subdomains they are computed f"
Q16-1008,N12-1047,0,0.0394474,"of-the-art phrase-based system. The Baseline system includes MOSES (Koehn et al., 2007) baseline feature functions, plus eight hierarchical lexicalized reordering model feature functions (Galley and Manning, 2008). The training data is first word-aligned using GIZA++ (Och and Ney, 2003) and then symmetrized with grow(-diag)final-and (Koehn et al., 2003). We limit the phrase length to the maximum of seven words. The lan104 In practice, we found that using this hard version leads to better performance.5 4.3 Alternative tuning scenarios In order to tune all systems, we use the k-best batch MIRA (Cherry and Foster, 2012). We report the translation accuracy with three metrics - BLEU 5 A more principled alternative would be to use posterior regularization (Ganchev et al., 2009). Task System BLEU↑ /∆ English-French Baseline 21.4 Professional & Business Services Our System 21.5/+0.1 Baseline 39.9 Leisure, Tourism and Arts Our System 40.8/+0.9 English-Spanish Baseline 32.5 Financials Our System 32.8/+0.3 Baseline 24.4 Professional & Business Services Our System 24.8/+0.4 Baseline 33.3 Legal Services Our System 33.8/+0.5 English-German Baseline 22.8 Computer Software Our System 23.1/+0.3 Baseline 20.5 Computer Hard"
Q16-1008,P11-2080,0,0.0930478,"likely safer to use. 100 Weights for these features, alongside all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic sys"
Q16-1008,P11-2031,0,0.052164,"Missing"
Q16-1008,2012.amta-papers.4,0,0.0206664,"nnotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Carpuat et al., 2013; Carpuat et al., 2014; Mansour and Ney, 2014). Recently, there was some research on adapting simultaneously to multiple domains, the goal related to ours (Clark et al., 2012; Sennrich, 2012a). For instance, Clark et al. (2012) augment a phrase-based MT system with various domain indicator features to build a single system that performs well across a range of domains. Sennrich (2012a) proposed to cluster training data in an unsupervised fashion to build mixture models that yield good performance on multiple test domains. However, their approaches are very different from ours, that is minimizing risk associated with choosing domain-specific translations. Moreover, the present work deviates radically from earlier work in that it explores the scenario where no prior"
Q16-1008,P13-1077,0,0.0214107,"i−n , z) and P (f |z) = = i P (ei | Q j−1 Here, the notation ei−1 j P (fj |fj−n , z). i−n j−1 and fj−n is used to denote the history of length n for the source and target words ei and fj , respectively. Training. For training, we maximize the loglikelihood L of the data L= Formally, it is a uniform mixture of the generative processes for the two potential translation di1 Doing that requires incorporating into the model additional hidden variables encoding phrase segmentation (DeNero et al., 2006). This would significantly complicate inference (Mylonakis and Sima’an, 2008; Neubig et al., 2011; Cohn and Haffari, 2013). 102 X e,f log X P (z) z 1 2 P (e|z) X P (f , a|e, z) a  X 1 + P (f |z) P (e, a0 |f , z) . 2 0 3. if the e-to-f direction is chosen then generate the pair relying on P (e |z)P (f |e, z); 4. otherwise, use P (f |z)P (e |f , z). 2 As we aim for a simple approach, our TMs are computed through the introduction of hidden alignments a and a0 in f -to-e and e-to-f P directions respectively, in which P (f | e, z) = a P (f , a |e, z) P 0 and P (e |f , z) = a0 P (e, a |f , z). To make the marginalization of alignments tractable, we restrict P (f , a |e, z) and P (e, a0 |f , z) to the same assumptio"
Q16-1008,D14-1062,1,0.846718,"Missing"
Q16-1008,C14-1182,1,0.889066,"Missing"
Q16-1008,N15-1043,1,0.880913,"Missing"
Q16-1008,W06-3105,0,0.03583,"3) + P (f |z)P (e |f , z) . 2 We use standard nth -order Markov model for P Q(e |z) andi−1P (f |z), in which P (e |z) ei−n , z) and P (f |z) = = i P (ei | Q j−1 Here, the notation ei−1 j P (fj |fj−n , z). i−n j−1 and fj−n is used to denote the history of length n for the source and target words ei and fj , respectively. Training. For training, we maximize the loglikelihood L of the data L= Formally, it is a uniform mixture of the generative processes for the two potential translation di1 Doing that requires incorporating into the model additional hidden variables encoding phrase segmentation (DeNero et al., 2006). This would significantly complicate inference (Mylonakis and Sima’an, 2008; Neubig et al., 2011; Cohn and Haffari, 2013). 102 X e,f log X P (z) z 1 2 P (e|z) X P (f , a|e, z) a  X 1 + P (f |z) P (e, a0 |f , z) . 2 0 3. if the e-to-f direction is chosen then generate the pair relying on P (e |z)P (f |e, z); 4. otherwise, use P (f |z)P (e |f , z). 2 As we aim for a simple approach, our TMs are computed through the introduction of hidden alignments a and a0 in f -to-e and e-to-f P directions respectively, in which P (f | e, z) = a P (f , a |e, z) P 0 and P (e |f , z) = a0 P (e, a |f , z). T"
Q16-1008,W11-2107,0,0.114396,"Missing"
Q16-1008,P13-2119,0,0.157235,"owledge. Various techniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Ca"
Q16-1008,P12-2023,0,0.107089,"he latent subdomains is likely safer to use. 100 Weights for these features, alongside all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseli"
Q16-1008,D10-1044,0,0.130054,"res which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dep"
Q16-1008,D08-1089,0,0.0721658,"P (z = i |e˜) ∝ P (z = i |f˜) ∝ Table 2: Data and adaptation tasks. to the training data. In this way, we test the stability of our results across a wide range of target domains. X he,f i X c(i; zˆhe,f i )δ(˜ e; e), he,f i c(i; zˆhe,f i )δ(f˜; f ). Here, zˆhe, f i is the “winning&quot; latent subdomain for sentence pair he, f i: zˆhe, f i = argmax P (z = i |e, f ) i∈{1, ..., K} 4.2 Systems We use a standard state-of-the-art phrase-based system. The Baseline system includes MOSES (Koehn et al., 2007) baseline feature functions, plus eight hierarchical lexicalized reordering model feature functions (Galley and Manning, 2008). The training data is first word-aligned using GIZA++ (Och and Ney, 2003) and then symmetrized with grow(-diag)final-and (Koehn et al., 2003). We limit the phrase length to the maximum of seven words. The lan104 In practice, we found that using this hard version leads to better performance.5 4.3 Alternative tuning scenarios In order to tune all systems, we use the k-best batch MIRA (Cherry and Foster, 2012). We report the translation accuracy with three metrics - BLEU 5 A more principled alternative would be to use posterior regularization (Ganchev et al., 2009). Task System BLEU↑ /∆ English-"
Q16-1008,N13-1035,0,0.0887923,"eights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the corresponding word sense). The Russian words ‘xifr’, ‘zakon’ or ‘pro"
Q16-1008,E14-1035,0,0.490374,"se features, alongside all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two"
Q16-1008,P13-2121,0,0.0218392,"https://www.taus.net/. English Professional Dev &Business Test Services Leisure, Tourism and Arts Dev Test Professional Dev &Business Test Services Dev Legal Test Dev Financials Test Professional Dev &Business Services Test Dev Legal Test Computer Software Computer Hardware Dev Test Dev Test Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words French guage models are interpolated 5-grams with KneserNey smoothing, estimated by KenLM (Heafield et al., 2013) from a large monolingual corpus of nearly 2.1B English words collected within the WMT 2015 MT Shared Task. Finally, we use MOSES as a decoder (Koehn et al., 2007). Our system is exactly the same as the baseline, plus three additional feature functions induced for the translation rules: two features for domainspecificity of phrases (both for the source side ~ (Dα (f˜)) and the target side (Dα (~e˜)), and one feature for source-target coherence across subdomains ~ (D(~e˜, f˜)). For the projection, we use K=12. We 2K 74.16K 83.85K 5K 92.84K 105.05K 2K 107.45K 117.16K 5K 101.82K 114.76K English S"
Q16-1008,P14-1110,0,0.0873844,"00 Weights for these features, alongside all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result"
Q16-1008,Q13-1035,0,0.0901055,"ng latent subdomains from the training data only; (2) introducing features which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russia"
Q16-1008,D07-1031,0,0.0299789,"80.49K 85.08K 5K 79.75K 85.28K 2K 50.54K 45.99K 5K 124.93K 111.70K 2K 40.24K 38.31K 5K 102.71K 101.12K 2K 37.40K 36.98K 5K 103.29K 98.04K also explored different values for K, but have not observed significant difference in the scores. In our experiments we do one iteration of EM with parallel LMs (as described in Section 3), before continuing with the full model for three more iterations. We did not observe a significant improvement from running EM any longer. Finally, we use hard EM, as it has been found to yield better models than the standard soft EM on a number of different task (e.g., (Johnson, 2007)). In other words, instead of standard ‘soft’ EM updates with phrase counts weighted according to the posterior P (z = i |e, f ), we use the ‘winner-takes-all’ approach: P (z = i |e˜) ∝ P (z = i |f˜) ∝ Table 2: Data and adaptation tasks. to the training data. In this way, we test the stability of our results across a wide range of target domains. X he,f i X c(i; zˆhe,f i )δ(˜ e; e), he,f i c(i; zˆhe,f i )δ(f˜; f ). Here, zˆhe, f i is the “winning&quot; latent subdomain for sentence pair he, f i: zˆhe, f i = argmax P (z = i |e, f ) i∈{1, ..., K} 4.2 Systems We use a standard state-of-the-art phrase-"
Q16-1008,D15-1147,0,0.0599944,"of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the corresponding word sense). The Russian words ‘xifr’, ‘zakon’ or ‘programma’ would perhap"
Q16-1008,D14-1014,0,0.0191632,"chniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Carpuat et al., 2013; Carpuat e"
Q16-1008,W12-3139,0,0.0253064,"sed as a form of prior knowledge. Various techniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set ("
Q16-1008,W07-0733,0,0.0351621,"in SMT can be regarded as injecting prior knowledge about the target translation task into the learning process. Various approaches have so far been exploited in the literature. They can be loosely categorized according to the type of prior knowledge exploited for adaptation. Often, a seed 109 in-domain corpus exemplifying the target translation task is used as a form of prior knowledge. Various techniques can then be used for adaptation. For example, one approach is to combine a system trained on the in-domain data with another general-domain system trained on the rest of the data (e.g., see Koehn and Schroeder (2007), Foster et al. (2010), Bisazza et al. (2011), Sennrich (2012b), Razmara et al. (2012), Sennrich et al. (2013), Haddow (2013), Joty et al. (2015)). Rather than using the entire training data, it is also common to combine the in-domain system with a system trained on a selected subset of the data (e.g., see Axelrod et al. (2011), Koehn and Haddow (2012), Duh et al. (2013), Kirchhoff and Bilmes (2014), Cuong and Sima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012;"
Q16-1008,N03-1017,0,0.405782,"more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the latent subdomains they are computed from, are useful also for the very challenging domain adaptation setting considered in this work. 2 Domain-Invariance for Phrases At the core of a standard state-of-the-art phrasebased system (Koehn et al., 2003; Och and Ney, 2004) lies a phrase table {h˜ e, f˜i} extracted from a word-aligned training corpus together with estimates for phrase translation probabilities Pcount (˜ e |f˜) and Pcount (f˜ |e˜). Typically the phrases and their probabilities are obtained from large parallel corpora, which are usually broad enough to cover a mixture of several subdomains. In such mixtures, phrase distributions may be different across different subdomains. Some phrases (whether source or target) are more specific for certain subdomains than others, while some phrases are useful across many subdomains. Moreover"
Q16-1008,P07-2045,0,0.0198822,"Financials Test Professional Dev &Business Services Test Dev Legal Test Computer Software Computer Hardware Dev Test Dev Test Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words Sents Words French guage models are interpolated 5-grams with KneserNey smoothing, estimated by KenLM (Heafield et al., 2013) from a large monolingual corpus of nearly 2.1B English words collected within the WMT 2015 MT Shared Task. Finally, we use MOSES as a decoder (Koehn et al., 2007). Our system is exactly the same as the baseline, plus three additional feature functions induced for the translation rules: two features for domainspecificity of phrases (both for the source side ~ (Dα (f˜)) and the target side (Dα (~e˜)), and one feature for source-target coherence across subdomains ~ (D(~e˜, f˜)). For the projection, we use K=12. We 2K 74.16K 83.85K 5K 92.84K 105.05K 2K 107.45K 117.16K 5K 101.82K 114.76K English Spanish 2K 31.70K 34.62K 5K 84.1K 93.4K 2K 35.06K 38.78K 5K 88.63K 102.71K 2K 37.23K 42.89K 5K 99.05K 109.81K English German 2K 80.49K 85.08K 5K 79.75K 85.28K 2K 50"
Q16-1008,W04-3250,0,0.264521,"Missing"
Q16-1008,2005.mtsummit-papers.11,0,0.0696099,"e LMs estimated in 103 Sents Words Training Data Sents Words Sents Words English French 5.01M 103.39M 125.81M English Spanish 4.00M 81.48M 89.08M English German 4.07M 93.19M 88.48M Table 1: Data Preparation. 4.1 Data We conduct experiments with large-scale SMT systems across a number of domains for three language pairs (English-Spanish, English-German and English-French). The datasets are summarized in Table 1. For English-Spanish, we run experiments with training data consisting of 4M sentence pairs collected from multiple resources within the WMT 2013 MT Shared Task. These include EuroParl (Koehn, 2005), Common Crawl Corpus, UN Corpus, and News Commentary. For English-German, our training data consists of 4.1M sentence pairs collected from the WMT 2015 MT Shared Task, including EuroParl, Common Crawl Corpus and News Commentary. Finally, for English-French, we train SMT systems on a corpus of 5M sentence pairs collected from the WMT 2015 MT Shared Task, including the 109 French-English corpus. We conducted experiments on 9 different domains (tasks) where the data was manually collected by a TAUS.4 Table 2 presents the translation tasks: each of the tasks deals with a specific domain, each of"
Q16-1008,N04-1022,0,0.0374551,"he goal and setting we are working on is markedly different (i.e., we do not have access to meta-information about the training and translation tasks at all). The domain-invariance induced is integrated into SMT systems as feature functions, redirecting the decoder to a better search space for the translation over adaptation tasks. This aims at biasing the decoder towards translations that are less domain-specific and more source-target domain coherent. There is an interesting relation between this work and extensive prior work on minimum Bayes risk (MBR) objectives (used either at test time (Kumar and Byrne, 2004) or during training (Smith and Eisner, 2006; Pauls et al., 2009)). As with our work, the goal of MBR minimization is to select translations that are less “risky&quot;. Their risk is due to the uncertainty in model predictions, and some of this uncertainty may indeed be associated with domainvariability of translations. Still, a system trained with an MBR objective will tend to output most frequent translation rather than the most domaininvariant one, and this, as we argued in the introduction, might not be the right decision when applying it across domains. We believe that the two classes of method"
Q16-1008,W14-3359,0,0.213077,"ima’an (2014b)). In some other cases, the prior knowledge lies in meta-information about the training data. This could be document-annotated training information (Eidelman et al., 2012; Hu et al., 2014; Hasler et al., 2014; Su et al., 2015; Zhang et al., 2014), and domainannotated sub-corpora (Chiang et al., 2011; Sennrich, 2012b; Chen et al., 2013b; Carpuat et al., 2014; Cuong and Sima’an, 2015). Some recent approaches perform adaptation by exploiting a target domain development, or even only the source side of the development set (Sennrich, 2012a; Carpuat et al., 2013; Carpuat et al., 2014; Mansour and Ney, 2014). Recently, there was some research on adapting simultaneously to multiple domains, the goal related to ours (Clark et al., 2012; Sennrich, 2012a). For instance, Clark et al. (2012) augment a phrase-based MT system with various domain indicator features to build a single system that performs well across a range of domains. Sennrich (2012a) proposed to cluster training data in an unsupervised fashion to build mixture models that yield good performance on multiple test domains. However, their approaches are very different from ours, that is minimizing risk associated with choosing domain-specifi"
Q16-1008,D09-1074,0,0.0279052,"f˜. In our experiments, we compare using our subdomain induction framework with relying on topic distributions provided by a standard topic model, Latent Dirichlet Allocation (Blei et al., 2003). Note that unlike LDA we rely on parallel data and word alignments when inducing domains. Our intuition is that latent variables capturing regularities in bilingual data may be more appropriate for the translation task. Inducing these probabilities directly is rather difficult as the task of designing a fully generative phrase-based model is known to be challenging.1 In order to avoid this, we follow Matsoukas et al. (2009) and Cuong and Sima’an (2014a) who “embed&quot; such a phrase-level model into a latent subdomain model that works at the sentence level. In other words, we associate latent domains with sentence pairs rather than with phrases, and use the posterior probabilities computed for the sentences with all the phrases appearing in the corresponding sentences. Given P (z |e, f ) - a latent subdomain model given sentence pairs he, f i - the estimation of P (z |e˜) and P (z |f˜), for phrases e˜ and f˜, can be simplified by computing expectations z for all z ∈ {1, . . . , K}: P e; e) e,f P (z = i|e, f )c(˜ P ("
Q16-1008,D08-1066,1,0.844454,"Missing"
Q16-1008,P11-1064,0,0.0239073,", in which P (e |z) ei−n , z) and P (f |z) = = i P (ei | Q j−1 Here, the notation ei−1 j P (fj |fj−n , z). i−n j−1 and fj−n is used to denote the history of length n for the source and target words ei and fj , respectively. Training. For training, we maximize the loglikelihood L of the data L= Formally, it is a uniform mixture of the generative processes for the two potential translation di1 Doing that requires incorporating into the model additional hidden variables encoding phrase segmentation (DeNero et al., 2006). This would significantly complicate inference (Mylonakis and Sima’an, 2008; Neubig et al., 2011; Cohn and Haffari, 2013). 102 X e,f log X P (z) z 1 2 P (e|z) X P (f , a|e, z) a  X 1 + P (f |z) P (e, a0 |f , z) . 2 0 3. if the e-to-f direction is chosen then generate the pair relying on P (e |z)P (f |e, z); 4. otherwise, use P (f |z)P (e |f , z). 2 As we aim for a simple approach, our TMs are computed through the introduction of hidden alignments a and a0 in f -to-e and e-to-f P directions respectively, in which P (f | e, z) = a P (f , a |e, z) P 0 and P (e |f , z) = a0 P (e, a |f , z). To make the marginalization of alignments tractable, we restrict P (f , a |e, z) and P (e, a0 |f ,"
Q16-1008,J03-1002,0,0.00693927,"ng data. In this way, we test the stability of our results across a wide range of target domains. X he,f i X c(i; zˆhe,f i )δ(˜ e; e), he,f i c(i; zˆhe,f i )δ(f˜; f ). Here, zˆhe, f i is the “winning&quot; latent subdomain for sentence pair he, f i: zˆhe, f i = argmax P (z = i |e, f ) i∈{1, ..., K} 4.2 Systems We use a standard state-of-the-art phrase-based system. The Baseline system includes MOSES (Koehn et al., 2007) baseline feature functions, plus eight hierarchical lexicalized reordering model feature functions (Galley and Manning, 2008). The training data is first word-aligned using GIZA++ (Och and Ney, 2003) and then symmetrized with grow(-diag)final-and (Koehn et al., 2003). We limit the phrase length to the maximum of seven words. The lan104 In practice, we found that using this hard version leads to better performance.5 4.3 Alternative tuning scenarios In order to tune all systems, we use the k-best batch MIRA (Cherry and Foster, 2012). We report the translation accuracy with three metrics - BLEU 5 A more principled alternative would be to use posterior regularization (Ganchev et al., 2009). Task System BLEU↑ /∆ English-French Baseline 21.4 Professional & Business Services Our System 21.5/+0.1"
Q16-1008,J04-4002,0,0.0827779,"d not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the latent subdomains they are computed from, are useful also for the very challenging domain adaptation setting considered in this work. 2 Domain-Invariance for Phrases At the core of a standard state-of-the-art phrasebased system (Koehn et al., 2003; Och and Ney, 2004) lies a phrase table {h˜ e, f˜i} extracted from a word-aligned training corpus together with estimates for phrase translation probabilities Pcount (˜ e |f˜) and Pcount (f˜ |e˜). Typically the phrases and their probabilities are obtained from large parallel corpora, which are usually broad enough to cover a mixture of several subdomains. In such mixtures, phrase distributions may be different across different subdomains. Some phrases (whether source or target) are more specific for certain subdomains than others, while some phrases are useful across many subdomains. Moreover, for a phrase pair,"
Q16-1008,P02-1040,0,0.0977283,"Missing"
Q16-1008,D09-1147,0,0.0243774,"we do not have access to meta-information about the training and translation tasks at all). The domain-invariance induced is integrated into SMT systems as feature functions, redirecting the decoder to a better search space for the translation over adaptation tasks. This aims at biasing the decoder towards translations that are less domain-specific and more source-target domain coherent. There is an interesting relation between this work and extensive prior work on minimum Bayes risk (MBR) objectives (used either at test time (Kumar and Byrne, 2004) or during training (Smith and Eisner, 2006; Pauls et al., 2009)). As with our work, the goal of MBR minimization is to select translations that are less “risky&quot;. Their risk is due to the uncertainty in model predictions, and some of this uncertainty may indeed be associated with domainvariability of translations. Still, a system trained with an MBR objective will tend to output most frequent translation rather than the most domaininvariant one, and this, as we argued in the introduction, might not be the right decision when applying it across domains. We believe that the two classes of methods are largely complementary, and leave further investigation for"
Q16-1008,P12-1099,0,0.106493,"induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the corresponding word sense). The"
Q16-1008,P13-1082,0,0.0784642,"3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the corresponding word sense). The Russian words ‘xifr’, ‘"
Q16-1008,2012.eamt-1.43,0,0.33738,"re to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the correspo"
Q16-1008,E12-1055,0,0.529958,"re to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance. 1 Introduction Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems (Irvine et al., 2013; Carpuat et al., 2014). Domain-adaptation methods (Foster et al., 2010; Bisazza et al., 2011; Sennrich, 2012b; Razmara et al., 2012; Sennrich et al., 2013; Haddow, 2013; Joty et al., 2015) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample. In practice, however, the target domain may not be known When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains. For example, when translating from English to Russian, the most natural translation for the word ‘code’ would be highly dependent on the domain (and the correspo"
Q16-1008,P06-2101,0,0.041292,"rkedly different (i.e., we do not have access to meta-information about the training and translation tasks at all). The domain-invariance induced is integrated into SMT systems as feature functions, redirecting the decoder to a better search space for the translation over adaptation tasks. This aims at biasing the decoder towards translations that are less domain-specific and more source-target domain coherent. There is an interesting relation between this work and extensive prior work on minimum Bayes risk (MBR) objectives (used either at test time (Kumar and Byrne, 2004) or during training (Smith and Eisner, 2006; Pauls et al., 2009)). As with our work, the goal of MBR minimization is to select translations that are less “risky&quot;. Their risk is due to the uncertainty in model predictions, and some of this uncertainty may indeed be associated with domainvariability of translations. Still, a system trained with an MBR objective will tend to output most frequent translation rather than the most domaininvariant one, and this, as we argued in the introduction, might not be the right decision when applying it across domains. We believe that the two classes of methods are largely complementary, and leave furt"
Q16-1008,2006.amta-papers.25,0,0.231338,"Missing"
Q16-1008,P15-1023,0,0.0603535,"all other standard features, are tuned on a development set. Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain. It is enough to tune them on a mixed-domain dataset sufficiently different from the training data. We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes. In contrast, for example, in Eidelman et al. (2012), Chiang et al. (2011), Hu et al. (2014), Hasler et al. (2014), Su et al. (2015), Sennrich (2012b), Chen et al. (2013b), and Carpuat et al. (2014), features capture similarities between a target domain and each of the training subdomains. Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain. We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total. We observe significant and consistent performance improvements over the baseline domain-agnostic systems. This result confirms that our two features, and the"
Q16-1008,P11-1007,1,0.54996,"the uncertainty in model predictions, and some of this uncertainty may indeed be associated with domainvariability of translations. Still, a system trained with an MBR objective will tend to output most frequent translation rather than the most domaininvariant one, and this, as we argued in the introduction, might not be the right decision when applying it across domains. We believe that the two classes of methods are largely complementary, and leave further investigation for future work. At a conceptual level it is also related to regularizers used in learning domain-invariant neural models (Titov, 2011), specifically autoencoders. Though they also consider divergences between distributions of latent variable vectors, they use these divergences at learning time to bias models to induce representations maximally invariant across domains. Moreover, they assume access to meta-information about domains and consider only classification problems. 6 Conclusion This paper aims at adapting machine translation systems to all domains at once by favoring phrases that are domain-invariant, that are safe to use across a variety of domains. While typical domain adaptation systems expect a sample of the targ"
Q16-1008,P14-1072,0,0.221592,"s a  + P (f |z)P (e, a0 |f , z) . (6) In the M-step, we use the posteriors P (a, z |e, f ) and P (a0 , z |e, f ) to re-estimate parameters of both alignment models. This is done in a very similar way to estimation of the standard IBM Model 1. We use the posteriors to re-estimate LM parameters as follows X , z) ∝ P (z|e, f )c(ei1 ; e), (7) P (ei |ei−1 1 e,f P (fi |f1i−1 , z) ∝ X P (z|e, f )c(f1i ; f ). (8) e,f To obtain better parameter estimates for word predictions and avoid overfitting, we use smoothing in the M-step. In this work, we chose to apply expected Kneser-Ney smoothing technique (Zhang and Chiang, 2014) as it is simple and achieves state-of-theart performance on the language modeling problem. Finally, P (z) can be simply estimated as follows X P (z) ∝ P (z |e, f ) (9) e, f Hierarchical Training. In practice, we found that training the full joint model leads to brittle performance, as EM is very likely to get stuck in bad local maxima. To address this difficulty, in our implementation, we start out by first jointly training P (z), P (e |z) and P (f |z). In this way in the E-step, we fix our model parameters and compute P (z |e, f ) for every sentence pair: P (z |e, f ) ∝ P (e |z)P (f |z)P (z)"
R09-1025,J99-2004,0,0.126659,"Missing"
R09-1025,J96-1002,0,0.015203,"Missing"
R09-1025,A00-2018,0,0.274514,"Missing"
R09-1025,C04-1041,0,0.130527,"Missing"
R09-1025,J07-4004,0,0.0961213,"Missing"
R09-1025,P04-1015,0,0.140279,"Missing"
R09-1025,P02-1002,0,0.0725069,"Missing"
R09-1025,J07-3004,0,0.0293549,"Missing"
R09-1025,E95-1017,0,0.15406,"Missing"
R09-1025,W04-0308,0,0.0627621,"Missing"
R09-1025,P06-2089,0,0.038448,"Missing"
R09-1025,H05-1102,0,0.0378422,"Missing"
R09-1025,W03-3023,0,0.207811,"Missing"
R09-1025,P83-1020,0,0.586462,"Missing"
R09-1025,J03-4003,0,\N,Missing
R09-1025,P05-1033,0,\N,Missing
R09-1025,J05-1004,0,\N,Missing
W03-3021,H91-1060,0,0.0869732,"Missing"
W03-3021,P97-1003,0,0.044117,"Missing"
W03-3021,P96-1024,0,0.363978,"Missing"
W03-3021,J93-2004,0,0.0301394,"Missing"
W04-0505,C02-1026,0,0.0578861,"the two classifiers. Section 7 discusses the results and considers the possibility of applying our approach to other restricted domains. We conclude in Section 8. 2 Related Work Two kinds of related work are relevant to this paper: question answering against external knowledge sources, and genre detection (using classifiers). We briefly discuss both. Many QA research groups employed External Knowledge Sources in order to improve performance. For instance, (Chu-Carroll and Prager, 2002) used WordNet to answer what is questions, using the isa hierarchy supported by WordNet. (Hovy et al., 2002; Lin, 2002) used dictionaries such as WordNet and web search results to re-rank answers. (Yang et al., 2003) preformed structure analysis of the knowledge obtained from WordNet and the Web in order to further improve performance. We refer to (Sebastiani, 2002) for extensive review about machine learning in automated text classification. (Lewis, 1992) were among the first to use machine learning for genre detection trying to categorize Reuters articles to predefined categories. Probabilistic classifiers were used by many groups (Lewis, 1998). Much current text classification research is focused on Support"
W05-0706,J95-4004,0,0.471369,"Missing"
W05-0706,A88-1019,0,0.00993528,"either a sequence of words or a sequence of morphemes, depending on the level of tokenization, and An1 are the respective nonterminals – either POS tags or word-level analyses. Thus, the disambiguator aims at finding the most probable hterminal sequence, nonterminal sequencei for the given sentence, where in the case of word-tokenization there is only one possible terminal sequence for the sentence. 3.3 HMM probabilistic model The actual probabilistic model used in this work for estimating P (en1 , An1 ) is based on Hidden Markov Models (HMMs). HMMs underly many successful POS taggers , e.g. (Church, 1988; Charniak et al., 1993). For a k-th order Markov model (k = 1 or k = 2), we rewrite (4) as: arg max P (en1 , An1 ) ≈ n en 1 ,A1 arg max n en 1 ,A1 n Y P (Ai |Ai−k , . . . , Ai−1 )P (ei |Ai ) i=1 (2) (5) where AN ALY SES(w1k ) is the set of possible analyses for the input sentence w1k (output by the For reasons of data sparseness, actual models we use work with k = 2 for the morpheme level tokenization, and with k = 1 for the word level tokenization. arg max n (mn 1 ,t1 )∈AN ALY SES(w1k ) 42 For these models, two kinds of probabilities need to be estimated: P (ei |Ai ) (lexical model) and P (A"
W05-0706,N04-4038,0,0.115124,"Missing"
W05-0706,J95-3004,0,\N,Missing
W05-0706,P03-1051,0,\N,Missing
W07-0813,W05-0706,1,0.85261,"Missing"
W07-0813,J95-4004,0,0.201834,"Missing"
W07-0813,A88-1019,0,0.0219064,"on of previous work. Section 3 describes our adaptation of Bar Haim et al.’s POS tagging system to Arabic. In section 4 we show that an architecture like Bar Haim et al.’s, which relies on a morphological analyzer, is likely to suffer from coverage problems under any configuration where it is used as a stand-alone. In section 5 we present our new architecture and the method of combining the models. Section 6 concludes. 2 Relation to Previous Works Quite a few works have dealt with extending a given POS tagger, mainly by smoothing it using extra-information about untreated words. For example, (Church, 1988) uses the simple heuristic of predicting proper nouns from capitalization. This method is not applicable to Arabic and Hebrew, which lack typographical marking of proper nouns. More advanced methods like those described by Weischedel et al. (1993) incorporate the treatment of unknown words within the probability model. Weischedel et al. use derivational and inflectional endings to infer POS tags of unknown words. Nakagawa (2004) addresses the problem of unknown words for Japanese and Chinese, and uses a hybrid method of word-level and character-level information. In his model, Nakagawa uses ch"
W07-0813,N04-4038,0,0.423612,"Missing"
W07-0813,P05-1071,0,0.324188,"Missing"
W07-0813,W04-1602,0,0.064047,"Missing"
W07-0813,C04-1067,0,0.0434305,"ion to Previous Works Quite a few works have dealt with extending a given POS tagger, mainly by smoothing it using extra-information about untreated words. For example, (Church, 1988) uses the simple heuristic of predicting proper nouns from capitalization. This method is not applicable to Arabic and Hebrew, which lack typographical marking of proper nouns. More advanced methods like those described by Weischedel et al. (1993) incorporate the treatment of unknown words within the probability model. Weischedel et al. use derivational and inflectional endings to infer POS tags of unknown words. Nakagawa (2004) addresses the problem of unknown words for Japanese and Chinese, and uses a hybrid method of word-level and character-level information. In his model, Nakagawa uses character information (only) when handling unknown words, claiming that in word-level methods information about known words helps to achieve higher accuracy compared to character-level models. On the other hand, when it comes to unknown words, Nakagawa uses a character-level method, which is hypothesized to be more robust in such cases than word-level methods. Virtually all works that dealt with coverage problems of POS taggers ha"
W07-0813,J93-2006,0,\N,Missing
W07-0813,P03-1051,0,\N,Missing
W07-2219,J97-4005,0,0.0494528,"new one and is familiar from formal theories of syntax such as HPSG (Sag et al., 2003) and LFG (Kaplan and Bresnan, 1982). Here we propose to reframe systematic morphological decoration of syntactic categories at all levels of the hierarchy as 161 (a) (b) Figure 6: The Expansion Possibilities of a Non-Terminal Node: Expanding the NP from figure 4 in a three-dimensional parameterization Space an additional dimension of statistical estimation for learning unlexicalized treebank PCFGs. Our proposal deviates from various stochastic extensions of such constraints-based grammatical formalisms (cf. (Abney, 1997)) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures. To the best of our knowledge, this proposal has not been empirically explored before. 4 Experimental Setup Our goal is to determine the optimal strategy for learning treebank grammars for MH and to contrast it with bi-dimensional strategies explored for English. The methodology we use is adopted from (Klein and Manning, 2003) and our procedure is identical to the one described in (Johnson, 1998). We define transformations over the treebank that accept as input"
W07-2219,W04-1602,0,0.029522,"Missing"
W07-2219,W00-1201,0,0.0471462,"etween these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still lag behind.1 We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a role in disambiguation at least as important as the vertical and hori"
W07-2219,P99-1065,0,0.335891,"2003) systematize the distinction between these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still lag behind.1 We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a role in disambiguation at least"
W07-2219,J03-4003,0,0.629361,"age Muidergracht 24, 1018TV Amsterdam, The Netherlands {rtsarfat,simaan}@science.uva.nl Abstract 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the na¨ıve treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. Current parameters of accurate unlexicalized parsers based on Probabilistic ContextFree Grammars (PCFGs) form a twodimensional grid in which rewrite events are conditioned on both horizontal (headoutward) and vertical (parental) histories. In Semitic languages, where arguments may move around rath"
W07-2219,P03-1013,0,0.619341,"Linguistics Klein and Manning (2003) systematize the distinction between these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still lag behind.1 We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a rol"
W07-2219,P06-1087,0,0.0178255,"figure 1, for example, it is agreement on gender and number that reveals the subject-predicate dependency between surface forms. Figure 1 also shows that agreement features help to reveal such relations between higher levels of constituents as well. Determining the child constituents that contribute each of the features is not a trivial matter either. To illustrate the extent and the complexity of that matter let us consider definiteness in MH, which is morphologically marked (as an h prefix to the stem, glossed here explicitly as “the-”) and behaves as a syntactic 3 See (Wintner, 2000) and (Goldberg et al., 2006) for formal and statistical accounts (respectively) of noun phrases in MH. 158 (b) S N.MS.D hmnhl the-manager.MS.D VP.FS V.FS htpjrh resigned.FS ShVi NPhNNTi.FS.D NNT.FS sganit deputy.FS N.MS.D hmnhl the-manager.MS.D VPhVi).FS V.FS htpjrh resigned.FS Figure 3: Phrase-Level Agreement Features and HeadDependencies in MH: The direction of percolating definiteness in MH is distinct of that of the head (marking hhead-tagi) property (Danon, 2001). Definite noun-phrases exhibit agreement with other modifying phrases, and such agreement helps to determine the internal structure, labels, and the correc"
W07-2219,J98-4004,0,0.360276,"ation for Parsing Morphologically Rich Languages Reut Tsarfaty and Khalil Sima’an Institute for Logic, Language and Computation University of Amsterdam Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlands {rtsarfat,simaan}@science.uva.nl Abstract 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the na¨ıve treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. Current parameters of accurate unlexicalized parsers based on Probabilistic ContextFree Grammars (PCFGs) form a twodimensional grid in which"
W07-2219,P03-1054,0,0.275613,"ng Morphologically Rich Languages Reut Tsarfaty and Khalil Sima’an Institute for Logic, Language and Computation University of Amsterdam Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlands {rtsarfat,simaan}@science.uva.nl Abstract 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the na¨ıve treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. Current parameters of accurate unlexicalized parsers based on Probabilistic ContextFree Grammars (PCFGs) form a twodimensional grid in which rewrite events are conditi"
W07-2219,P05-1010,0,0.0254241,"ts on a the top-down head-outward generation process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the catego"
W07-2219,P06-1055,0,0.0540878,"tion process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as linguistically motivat"
W07-2219,W05-1512,0,0.0127231,"d-outward generation process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as"
W07-2219,C04-1024,0,0.0826761,"ed headdriven baseline a` la (Collins, 2003) located on the (0, 0, 0) coordinate. We transform the treebank trees in correspondence with different points in the threedimensional space defined by (h, v, d). The models we implement are marked in the coordinate-system depicted in figure 7. The implementation details of the transformations we use are spelled out in tables 3–4. Procedure We implement different models that correspond to different instantiations of h, v and d. For each instantiation we transform the training set and learn a PCFG using Maximum Likelihood estimates, and we use BitPar (Schmidt, 2004), an efficient general-purpose parser, to parse unseen sentences. The input to the parser is a sequence of word segments where each segment corresponds to a single POS tag, possibly decorated with morphological features. This setup assumes partial morphological disambiguation (namely, segmentation) but crucially we do not disambiguate their respective POS categories. This setup is more appropriate for using general-purpose parsing tools and it makes our results comparable to studies in other languages.8 8 Our working assumption is that better performance of a parsing model in our setup will im"
W07-2219,P06-3009,1,0.688506,"ons. Figure 6(b) illustrates how the depth expansion interacts with both parent anno(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as linguistically motivated co-occurrences between them. Individual category-splits are viewed as taking place in a twodimensional space and it is hard to"
W07-2219,J04-4004,0,\N,Missing
W07-2219,H94-1020,0,\N,Missing
W09-3833,P99-1035,1,0.745505,"treebank annotated in the PTB style which marks arguments like the PTB. 3 Unsupervised Re-estimation Inside-outside (henceforth I-O) (Lari and Young, 1990), an instance of EM, is an iterative estimation method for PCFGs that, given an initial model and a corpus of unannotated data, produces models that assign increasingly higher likelihood to the corpus at each iteration. I-O often leads to sub-optimal grammars, being subject to the wellknown problem of local maxima, and dependence on initial conditions (de Marcken, 1995) (although there have been positive results using I-O as well, for e.g. Beil et al. (1999)). More recently, Deoskar (2008) re-estimated an unlexicalised PTB PCFG using unlabeled Wall Street Journal data. They compared models for which all PCFG parameters were re-estimated from raw data to models for which only lexical parameters were re-estimated, and found that the latter had better parsing results. While it is common to constrain EM either by good initial conditions or by heuristic constraints, their approach used syntactic parameters from a treebank model to constrain re-estimation of lexical parameters. Syntactic parameters are relatively well-estimated from a treebank, not bei"
W09-3833,C04-1041,0,0.0286742,"e treebank. 1 Introduction Lexical scarcity is a problem faced by all statistical NLP applications that depend on annotated training data, including parsing. One way of alleviating this problem is to supplement supervised models with lexical information from unlabeled data. In this paper, we present an approach for smoothing the lexicon of a treebank PCFG with frequencies estimated from unannotated data with Inside-outside estimation (Lari and Young, 1990). The PCFG is an unlexicalised PCFG, but contains complex lexical categories (akin to supertags in LTAG (Bangalore and Joshi, 1999) or CCG (Clark and Curran, 2004)) encoding structural preferences of words, like subcategorization. The idea behind unlexicalised parsing is that the syntax and lexicon of a language are largely independent, being mediated by “selectional” properties of open-class words. This is the intuition behind lexicalised formalisms like CCG: here lexical categories are fine-grained and syntactic in nature. Once a word is assigned a lexical category, the word itself is not taken into consideration further in the syntactic analysis. Fine-grained categories imply that lexicons estimated from treebanks will Khalil Sima’an ILLC University"
W09-3833,C08-1025,1,0.855302,"which marks arguments like the PTB. 3 Unsupervised Re-estimation Inside-outside (henceforth I-O) (Lari and Young, 1990), an instance of EM, is an iterative estimation method for PCFGs that, given an initial model and a corpus of unannotated data, produces models that assign increasingly higher likelihood to the corpus at each iteration. I-O often leads to sub-optimal grammars, being subject to the wellknown problem of local maxima, and dependence on initial conditions (de Marcken, 1995) (although there have been positive results using I-O as well, for e.g. Beil et al. (1999)). More recently, Deoskar (2008) re-estimated an unlexicalised PTB PCFG using unlabeled Wall Street Journal data. They compared models for which all PCFG parameters were re-estimated from raw data to models for which only lexical parameters were re-estimated, and found that the latter had better parsing results. While it is common to constrain EM either by good initial conditions or by heuristic constraints, their approach used syntactic parameters from a treebank model to constrain re-estimation of lexical parameters. Syntactic parameters are relatively well-estimated from a treebank, not being as sparse as lexical paramete"
W09-3833,deoskar-rooth-2008-induction,1,0.834334,"Missing"
W09-3833,E09-1038,0,0.0485815,"Missing"
W09-3833,J98-4004,0,0.0714471,"Missing"
W09-3833,P03-1054,0,0.0416148,"Missing"
W09-3833,J93-2004,0,0.0386102,"nguage are largely independent, being mediated by “selectional” properties of open-class words. This is the intuition behind lexicalised formalisms like CCG: here lexical categories are fine-grained and syntactic in nature. Once a word is assigned a lexical category, the word itself is not taken into consideration further in the syntactic analysis. Fine-grained categories imply that lexicons estimated from treebanks will Khalil Sima’an ILLC University of Amsterdam k.simaan@uva.nl be extremely sparse, even for a language like English with a large treebank resource like the Penn Treebank (PTB) (Marcus et al., 1993). Smoothing a treebank lexicon with an external wide-coverage lexicon is problematic due to their respective representations being incompatible and without an obvious mapping, assuming that the external lexicon is probabilistic to begin with. In this paper, we start with a treebank PCFG with fine-grained lexical categories and re-estimate its parameters on a large corpus of unlabeled data. We then use reestimates of lexical parameters (i.e. pre-terminal to terminal rule probabilities) to smooth the original treebank lexical parameters by interpolation between the two. Since the treebank PCFG i"
W09-3833,C04-1024,0,0.0235168,"mi (w, τ, ι) c¯emi (w, τ, ι) = P w cemi (w, τ, ι) (4) λ determines the relative weights given to the treebank and re-estimated model for a word. Since parameters of high-frequency words are likely to be more accurate in the treebank model, we parametrize λ as λf according to the treebank frequency f = t(w, τ ). 1 Note that in Eq. 4, the ratio of the two terms involving cemi is the conditional, lexical probability Pemi (w|τ, ι). 6 Experiments The treebank PCFG is trained on sections 0-22 of the PTB, with 5000 sentences held-out for evaluation. We conducted unsupervised estimation using Bitpar (Schmid, 2004) with unannotated Wall Street Journal data of 4, 8 and 12 million words, with sentence length &lt;25 words. The treebank and re-estimated models are interpolated with λ = 0.5 (in Eq. 3). We also parametrize λ for treebank frequency of words – optimizing over a development set gives us the following values of λf for different ranges of treebank word frequencies. if t(w, τ ) &lt;= 5 , λf = 0.5 if 5 &lt; t(w, τ ) &lt;= 15 , λf = 0.25 (5) if 15 &lt; t(w, τ ) &lt;= 50 , λf = 0.05 if t(w, τ ) &gt; 50 , λf = 0.005 Evaluations are on held-out data from the PTB by stripping all PTB annotation and obtaining Viterbi parses w"
W09-3833,J99-2004,0,\N,Missing
W10-1405,J97-4005,0,0.0751119,"al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for Hebrew they obtained tiny-to-no improvement. A question thus emerges whether there are any benefits in explicitly incorporating morphosyntactic agreement patterns into our models. This question is a manifestation of a greater issue, name"
W10-1405,W09-3819,0,0.0509563,"d HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for Hebrew they obtained tiny-to-no improvement. A question thus emerges whether there are any benefits in explicitly incorporating morphosyntactic agreement patterns into our models. This question is a manifestation of a greater issue, namely, whether it is beneficial to represent complex patterns of morphology in the statistical parsing model, or whether configurational information subsume the relevant patterns, as it is commonly assumed in constituencybased parsing. Here we claim that agreement features are useful for statistical parsing provided that they are represented and parametrized"
W10-1405,P06-1087,0,0.0288127,"configurational languages (Hale, 1983) where the order of words is known to be (relatively) free. Agreement features encompass information concerning the functional relations between constituents in the syntactic structure, but whether incorporating agreement features in a statistical parsing model leads to improved performance has so far remained an open question and saw contradictory results. ∗ The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 199"
W10-1405,P03-1046,0,0.0265043,"formance has so far remained an open question and saw contradictory results. ∗ The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew i"
W10-1405,P03-1054,0,0.0448913,"structure of node labels in phrase-structure trees.5 Category-label state-splits can reflect the different morphosyntactic behavior of different non-terminals of the same type. Using such supervised, linguistically motivated, state-splits, based on the phraselevel marking of morphological information is one may build an efficient implementation of a PCFGbased parsing model that takes into account morphological features. State-split models were shown to obtain state-of-the-art performance with little computational effort. Supervised state-splits for constituency-based unlexicalized parsing in (Klein and Manning, 2003) in an accurate English parser. For the pair of Hebrew sentences (2b), the morphological state-split context-free representation of the domain S is as described at the top of figure 1.6 The Relational-Realizational (RR) Model A different way to implement a syntactic model that conform to the relaxed LH is by separating the inflectional features of surface words from their grammatical functions in the syntactic representation and let5 While agreement patterns in feature-rich grammars give rise to re-entrancies that break context-freeness, GPSG shows that using feature-percolation we can get qui"
W10-1405,J08-1002,0,0.0218147,"ion and saw contradictory results. ∗ The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for"
W10-1405,P06-1055,0,0.207766,"Missing"
W10-1405,C08-1112,1,0.813509,"Missing"
W10-1405,D09-1088,1,0.904577,"Missing"
W10-1405,P08-1043,1,\N,Missing
W10-2915,P08-1024,0,0.156619,"rteenth Conference on Computational Natural Language Learning, pages 117–125, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics (3) (4) component. Other existing work, e.g. (Chiang, 2007), assumes the word-alignments are given in the parallel corpus, but the problem of learning phrase translation probabilities is usually avoided by using surface counts of phrase pairs (Koehn et al., 2003). The problem of learning the hierarchical, synchronous grammar reordering rules is oftentimes addressed as a learning problem in its own right assuming all the rest is given (Blunsom et al., 2008b). A small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs as well as hierarchical reordering, e.g., (DeNero et al., 2008; Zhang et al., 2008; Blunsom et al., 2009). Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al., 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al., 2009). However, very few evaluate on actual translation performance of induced synchronous grammars (DeNero et al., 2008). In the majority"
W10-2915,P09-1088,0,0.0506003,"he word-alignments are given in the parallel corpus, but the problem of learning phrase translation probabilities is usually avoided by using surface counts of phrase pairs (Koehn et al., 2003). The problem of learning the hierarchical, synchronous grammar reordering rules is oftentimes addressed as a learning problem in its own right assuming all the rest is given (Blunsom et al., 2008b). A small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs as well as hierarchical reordering, e.g., (DeNero et al., 2008; Zhang et al., 2008; Blunsom et al., 2009). Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al., 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al., 2009). However, very few evaluate on actual translation performance of induced synchronous grammars (DeNero et al., 2008). In the majority of cases, the Hiero system, which constitutes the yardstick by which hierarchical systems are measured, remains superior in translation performance, see e.g. (DeNero et al., 2008). This paper tackles the problem of learning generat"
W10-2915,P05-1033,0,0.630219,"translation applications focus on SCFGs of rank two (binary SCFGs), or binarisable ones witch can be converted to a binary SCFG, given that these seem to cover most of the translation phenomena encountered in language pairs (Wu, 1997) and the related processing algorithms are less demanding computationally. Although SCFGS were initially introduced for machine translation as a stochastic word-based translation process in the form of the InversionTransduction Grammar (Wu, 1997), they were actually able to offer state-of-the-art performance in their latter phrase-based implementation by Chiang (Chiang, 2005). Chiang’s Hiero hierarchical translation system is based on a synchronous grammar with a single non-terminal X covering all learned phrase-pairs. Beginning from the start symbol S, an initial phrase-span structure is constructed monotonically using a simple ‘glue grambias variance }| { z } |{ z p, pˆ) Err = KL(q, p¯) + ED KL(¯ (5) Bias is the KL-divergence between q and the mean estimate over all training data p¯ = ED pˆ(D). Variance is the expected divergence between the average estimate and the estimator’s actual choice. MLE estimators for all-fragment models are zerobiased with zero diverg"
W10-2915,J07-2003,0,0.555414,"he latter conditions reordering decisions on the surrounding lexical context of phrases, whereas our mechanism works with the lexical content of phrase pairs (akin to standard phrase-based systems). Surprisingly, our experiments on French-English data show that our learning method applied to far simpler models exhibits performance indistinguishable from the Hiero system. 1 Introduction A fundamental problem in phrase-based machine translation concerns the learning of a probabilistic synchronous context-free grammar (SCFG) over phrase pairs from an input parallel corpus. Chiang’s Hiero system (Chiang, 2007) exemplifies the gains to be had by combining phrase-based translation (Och and Ney, 2004) with the hierarchical reordering capabilities of SCFGs, particularly originating from Binary Inversion TransducStart Monotone Switching Emission S → X1 / X1 (1) X → X 1 X 2 /X 1 X 2 X → X 1 X 2 /X 2 X 1 X→e/f (2) Figure 1: A phrase-pair SCFG (BITG) 117 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117–125, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics (3) (4) component. Other existing work, e.g. (Chiang, 2007), assumes the"
W10-2915,W06-3105,0,0.633592,"superior in translation performance, see e.g. (DeNero et al., 2008). This paper tackles the problem of learning generative BITG models as translation models assuming latent segmentation and latent reordering: this is the most similar setting to the training of Hiero. Unlike all other work that heuristically selects a subset of phrase pairs, we start out from an SCFG that works with all phrase pairs in the training set and concentrate on the aspects of learning. This learning problem is fraught with the risks of overfitting and can easily result in inadequate reordering preferences (see e.g. (DeNero et al., 2006)). Almost instantly, we find that the translation performance of all-phrase probabilistic SCFGs learned in this setting crucially depends on the interplay between two aspects of learning: ing an effective, data-driven smoothed MaximumLikelihood that can cope with a model working with all phrase pair SCFGs. This builds upon our previous work on estimating parameters of a ”bag-of-phrases” model for Machine Translation (Mylonakis and Sima’an, 2008). However, learning SCFGs poses significant novel challenges, the core of which lies on the hierarchical nature of a stochastic SCFG translation model"
W10-2915,P08-1012,0,0.0289346,"Missing"
W10-2915,D08-1033,0,0.0328229,"Missing"
W10-2915,W06-3119,0,0.195348,"Missing"
W10-2915,N03-1017,0,0.440002,"from Binary Inversion TransducStart Monotone Switching Emission S → X1 / X1 (1) X → X 1 X 2 /X 1 X 2 X → X 1 X 2 /X 2 X 1 X→e/f (2) Figure 1: A phrase-pair SCFG (BITG) 117 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117–125, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics (3) (4) component. Other existing work, e.g. (Chiang, 2007), assumes the word-alignments are given in the parallel corpus, but the problem of learning phrase translation probabilities is usually avoided by using surface counts of phrase pairs (Koehn et al., 2003). The problem of learning the hierarchical, synchronous grammar reordering rules is oftentimes addressed as a learning problem in its own right assuming all the rest is given (Blunsom et al., 2008b). A small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs as well as hierarchical reordering, e.g., (DeNero et al., 2008; Zhang et al., 2008; Blunsom et al., 2009). Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al., 2008) or indirectly by evaluating a heuristically trained hierarchical translatio"
W10-2915,2005.mtsummit-papers.11,0,0.0299216,"i (r)λi Monotone Expansion X → X 1 X 2 /X 1 X 2 XSL → X 1 X 2 / X 1 X 2 XSR → X 1 X 2 /X 1 X 2 Switching Expansion X → XSL 1 XSR 2 /XSR 2 XSL 1 XSL → XSL 1 XSR 2 /XSR 2 XSL 1 XSR → XSL 1 XSR 2 /XSR 2 XSL 1 i r∈D The interpolation weights are tuned using Minimum Error Rate Training (Och, 2003). Phrase-Pair Emission X → e/f XSL → e / f XSR → e / f 5.2 We test empirically the learner’s output grammars for translating from French to English, using k = 5 for the Cross Validation data partitioning. The training material is a GIZA++ wordaligned corpus of 200K sentence-pairs from the Europarl corpus (Koehn, 2005), with our development and test parallel corpora of 2K sentencepairs stemming from the same source. Training the grammar parameters until convergence demands around 6 hours on an 8-core 2.26 GHz Intel Xeon system. Decoding employs a 4-gram language model, trained on English Europarl data of 19.5M words, smoothed using modified KneserNey discounting (Chen and Goodman, 1998), and lexical translation smoothing features based on the GIZA++ alignments. In a sense, the real baseline to which we might compare against should be a system employing the MLE estimate for the grammar extracted from the who"
W10-2915,W09-0424,0,0.111024,"slation model and the relevant additional layer of latent structure. We address these issues in this work. Another important contribution is in defining a lexicalised reordering component within BITG that captures order divergences orthogonal to Chiang’s model (Chiang, 2007) but somewhat akin to Phrase-Based Statistical Machine Translation reordering models (Koehn et al., 2003). Our analysis shows that the learning difficulties can be attributed to a rather weak generative model. Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al., 2009). Our findings should be insightful for others attempting to make the leap from shallow phrase-based systems to hierarchical SCFG-based translation models using learning methods, as opposed to heuristics. The rest of the paper is structured as follows. Section 2 briefly introduces the SCFG formalism and discusses its adoption in the context of Statistical Machine Translation (SMT). In section 3, we consider some of the pitfalls of stochastic SCFG grammar learning and address them by introducing a novel learning objective and algorithm. In the section that follows we browse through latent trans"
W10-2915,W02-1018,0,0.241257,"segmentations and latent word/phrasealignments, and the hierarchical phrase reordering can be expressed in terms of latent binary synchronous hierarchical structures (cf. Fig. 1). But each of these three kinds of latent structures may be made explicit using external resources: word-alignment could be considered solved using Giza++ (Och and Ney, 2003)), phrase pairs can be obtained from these word-alignments (Och and Ney, 2004), and the hierarchical synchronous structure can be grown over source/target linguistic syntactic trees output by an existing parser. The Joint Phrase Translation Model (Marcu and Wong, 2002) constitutes a specific case, albeit without the hierarchical, synchronous reordering Probabilistic phrase-based synchronous grammars are now considered promising devices for statistical machine translation because they can express reordering phenomena between pairs of languages. Learning these hierarchical, probabilistic devices from parallel corpora constitutes a major challenge, because of multiple latent model variables as well as the risk of data overfitting. This paper presents an effective method for learning a family of particular interest to MT, binary Synchronous Context-Free Grammar"
W10-2915,D08-1066,1,0.860813,"Missing"
W10-2915,J03-1002,0,0.003357,"ctions probabilities (for monotone or switching productions). Theoretically speaking, both kinds of preferences may involve latent structure relative to the parallel corpus. The mapping between source-target sentence pairs can be expressed in terms of latent phrase segmentations and latent word/phrasealignments, and the hierarchical phrase reordering can be expressed in terms of latent binary synchronous hierarchical structures (cf. Fig. 1). But each of these three kinds of latent structures may be made explicit using external resources: word-alignment could be considered solved using Giza++ (Och and Ney, 2003)), phrase pairs can be obtained from these word-alignments (Och and Ney, 2004), and the hierarchical synchronous structure can be grown over source/target linguistic syntactic trees output by an existing parser. The Joint Phrase Translation Model (Marcu and Wong, 2002) constitutes a specific case, albeit without the hierarchical, synchronous reordering Probabilistic phrase-based synchronous grammars are now considered promising devices for statistical machine translation because they can express reordering phenomena between pairs of languages. Learning these hierarchical, probabilistic devices"
W10-2915,J04-4002,0,0.829465,"eaking, both kinds of preferences may involve latent structure relative to the parallel corpus. The mapping between source-target sentence pairs can be expressed in terms of latent phrase segmentations and latent word/phrasealignments, and the hierarchical phrase reordering can be expressed in terms of latent binary synchronous hierarchical structures (cf. Fig. 1). But each of these three kinds of latent structures may be made explicit using external resources: word-alignment could be considered solved using Giza++ (Och and Ney, 2003)), phrase pairs can be obtained from these word-alignments (Och and Ney, 2004), and the hierarchical synchronous structure can be grown over source/target linguistic syntactic trees output by an existing parser. The Joint Phrase Translation Model (Marcu and Wong, 2002) constitutes a specific case, albeit without the hierarchical, synchronous reordering Probabilistic phrase-based synchronous grammars are now considered promising devices for statistical machine translation because they can express reordering phenomena between pairs of languages. Learning these hierarchical, probabilistic devices from parallel corpora constitutes a major challenge, because of multiple late"
W10-2915,P03-1021,0,0.0138193,"Missing"
W10-2915,J97-3002,0,0.302371,"his error decomposes into bias and variance terms (Heskes, 1998): The rank of an SCFG is defined as the maximum number of non-terminals in a grammar’s rule right-hand side. Contrary to monolingual Context Free Grammars, there does not always exist a conversion of an SCFG of a higher rank to one of a lower rank with the same language of string pairs. For this, most machine translation applications focus on SCFGs of rank two (binary SCFGs), or binarisable ones witch can be converted to a binary SCFG, given that these seem to cover most of the translation phenomena encountered in language pairs (Wu, 1997) and the related processing algorithms are less demanding computationally. Although SCFGS were initially introduced for machine translation as a stochastic word-based translation process in the form of the InversionTransduction Grammar (Wu, 1997), they were actually able to offer state-of-the-art performance in their latter phrase-based implementation by Chiang (Chiang, 2005). Chiang’s Hiero hierarchical translation system is based on a synchronous grammar with a single non-terminal X covering all learned phrase-pairs. Beginning from the start symbol S, an initial phrase-span structure is cons"
W10-3812,P05-1033,0,0.279214,"Missing"
W10-3812,P05-1066,0,0.348878,"Missing"
W10-3812,W06-1609,0,0.480679,"Missing"
W10-3812,P03-1021,0,0.333364,"ty on its specific contexts. Secondly, in every se0 0 0 quence s0 = s, . . . , sn = s resulting from a tree transductions, we prefer those local transductions 0 on τs0 that lead to source string permutation si i−1 eI1 m=1 where a feature function hm refer to a system model, and the corresponding λm refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights λm are usually optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002). Two reordering methods are widely used in phrasebased systems. 0 that are closer to target word order than si−1 ; we 0 employ s language model probability ratios as a measure of word order improvement. In how far does the assumption of source permutation provide any window for improvement over a phrase-based translation system? We conduct experiments on translating from English into Dutch, two languages which are characterized by a number of systematic divergences between them. Initially, we conduct oracle experiments with varying constraints on so"
W10-3812,P02-1040,0,0.0817805,"ondly, in every se0 0 0 quence s0 = s, . . . , sn = s resulting from a tree transductions, we prefer those local transductions 0 on τs0 that lead to source string permutation si i−1 eI1 m=1 where a feature function hm refer to a system model, and the corresponding λm refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights λm are usually optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002). Two reordering methods are widely used in phrasebased systems. 0 that are closer to target word order than si−1 ; we 0 employ s language model probability ratios as a measure of word order improvement. In how far does the assumption of source permutation provide any window for improvement over a phrase-based translation system? We conduct experiments on translating from English into Dutch, two languages which are characterized by a number of systematic divergences between them. Initially, we conduct oracle experiments with varying constraints on source permutation to set upperbounds on perfo"
W10-3812,N10-3010,0,0.259083,"nd (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Recently, Tromble and Eisner (2009) define source permutation as learning source permutations; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and a corresponding weight matrix that is fit to the parallel data. The huge space of permutations is then structured using a binary synchronous context-free grammar (Binary ITG) with O(n3 ) parsing complexity, and the permutation score is calculated recursively over the tree at ever"
W10-3812,D08-1089,0,0.0382279,"ality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Recently, Tromble and Eisner (2009) define source permutation as learning source permutations; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and a corresponding weight matrix that is fit to the parallel data. The huge space of permutations is then structured using"
W10-3812,P06-1121,0,0.0657938,"nuous (D). The reordering probabilities are conditioned on the lexical context of each phrase pair, and decoding works with a block sequence generation process with the possibility of swapping a pair of blocks. 3 Related Work on Source Permutation The integration of linguistic syntax into SMT systems offers a potential solution to reordering problem. For example, syntax is successfully integrated into hierarchical SMT (Zollmann and 93 4 Pre-Translation Source Permutation Venugopal, 2006). Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong e"
W10-3812,N04-4026,0,0.777973,"laining a source permutation can be computationally rather challenging (see (Tromble and Eisner, 2009)). Yet, Introduction From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word 92 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 92–100, COLING 2010, Beijing, August 2010. ter subtrees might turn out insufficient. from the limited perspective of source string per0 mutation (s → s ), another challenge is to inte0 grate a figure of merit that measures in how far s resembles a plausible target word-order. 2 Baseline: Phrase-based SMT Given a word-aligned parallel corpus, phrasebased systems (Och and Ney, 2002; Koehn et al., 2003) work with (in principle) arbitrarily large phrase pairs (also ca"
W10-3812,2010.eamt-1.28,1,0.615228,"Missing"
W10-3812,J03-1005,0,0.0346609,"learning a sequence of transductions for explaining a source permutation can be computationally rather challenging (see (Tromble and Eisner, 2009)). Yet, Introduction From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word 92 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 92–100, COLING 2010, Beijing, August 2010. ter subtrees might turn out insufficient. from the limited perspective of source string per0 mutation (s → s ), another challenge is to inte0 grate a figure of merit that measures in how far s resembles a plausible target word-order. 2 Baseline: Phrase-based SMT Given a word-aligned parallel corpus, phrasebased systems (Och and Ney, 2002; Koehn et al., 2003) work with (in principle) arb"
W10-3812,D09-1105,0,0.657674,"ergence from target word-order. We model the tree transfer τs → τs0 as a sequence of local, independent transduction operations, each transforming the current intermediate tree τs0 into the i next intermediate tree τs0 , with τs0 = τs and i+1 τs0 = τs0 . A transduction operation merely pern mutes the sequence of n &gt; 1 children of a single node in an intermediate tree, i.e., unlike previous work, we do not binarize the trees. The number of permutations is factorial in n, and learning a sequence of transductions for explaining a source permutation can be computationally rather challenging (see (Tromble and Eisner, 2009)). Yet, Introduction From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word 92 Proceedings of SSST-4, Fourth Workshop on Syntax and"
W10-3812,P03-1054,0,0.0445753,"ted within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page1 . The MSD model was used together with a distance-based reordering model. Word alignment was estimated with GIZA++ tool2 (Och, 2003), coupled with mkcls3 (Och, 1999), which allows for statistical word clustering for better generalization. An 5-gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and smoothed with modified Kneser-Ney discounting. We use the Stanford parser4 (Klein and Manning, 2003) as a source-side parsing engine. The parser was trained on the English treebank set provided with 14 syntactic categories and 48 POS tags. The evaluation conditions were case-sensitive and included punctuation marks. For Maximum Entropy modeling we used the maxent toolkit5 . • Local tree topology. Sub-tree instances that include parent node and the ordered sequence of child node labels. • Dependency features. Features that determine the POS tag of the head word of the current node, together with the sequence of POS tags of the head words of its child nodes. Data The experiment results were ob"
W10-3812,D07-1077,0,0.23119,"Missing"
W10-3812,N03-1017,0,0.0649499,"Missing"
W10-3812,P98-2230,0,0.555149,"Missing"
W10-3812,P07-2045,0,0.00715168,"node label Nx and its head POS tag, the child sequence αx together with the corresponding sequence of head POS tags and other features corresponding to different contextual information. We were particularly interested in those linguistic features that motivate reordering phenomena from the syntactic and linguistic perspective. The features that were used for training the permutation system are extracted for every internal node of the source tree that has more than one child: 5 Experiments and results The SMT system used in the experiments was implemented within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page1 . The MSD model was used together with a distance-based reordering model. Word alignment was estimated with GIZA++ tool2 (Och, 2003), coupled with mkcls3 (Och, 1999), which allows for statistical word clustering for better generalization. An 5-gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and smoothed with modified Kneser-Ney discounting. We use the Stanford parser4 (Klein and Manning, 2003) as a source-side parsing engine. The"
W10-3812,C04-1073,0,0.515407,"mutation via transfer of the source syntax tree. We present a novel discriminative, probabilistic tree transduction model, and contribute a set of empirical upperbounds on translation performance for Englishto-Dutch source string permutation under sequence and parse tree constraints. Finally, the translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly. 1 An alternative approach aims at minimizing the need for reordering during translation by permuting the source sentence as a pre-translation step, e.g., (Collins et al., 2005; Xia and McCord, 2004; Wang et al., 2007; Khalilov, 2009). In effect, the translation process works with a model for 0 source permutation (s → s ) followed by trans0 lation model (s → t), where s and t are source 0 and target strings and s is the target-like permuted source string. In how far can source permutation reduce the need for reordering in conjunction with translation is an empirical question. In this paper we define source permutation as the problem of learning how to transfer a given source parse-tree into a parse-tree that minimizes the divergence from target word-order. We model the tree transfer τs →"
W10-3812,W04-3250,0,0.054836,"Missing"
W10-3812,P02-1038,0,0.112224,"beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word 92 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 92–100, COLING 2010, Beijing, August 2010. ter subtrees might turn out insufficient. from the limited perspective of source string per0 mutation (s → s ), another challenge is to inte0 grate a figure of merit that measures in how far s resembles a plausible target word-order. 2 Baseline: Phrase-based SMT Given a word-aligned parallel corpus, phrasebased systems (Och and Ney, 2002; Koehn et al., 2003) work with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a simple definition of translational equivalence (Zens et al., 2002). The conditional probabilities of one phrase given its counterpart are interpolated log-linearly together with a set of other model estimates: ( M ) X I I J eˆ1 = arg max λm hm (e1 , f1 ) (1) We contribute solutions to these challenging problems. Firstly, we learn the transduction operations using a discriminative estimate of P (π(αx ) |Nx , αx , contextx ), where Nx is the label of"
W10-3812,J04-4002,0,0.122806,"nsduction operation merely pern mutes the sequence of n &gt; 1 children of a single node in an intermediate tree, i.e., unlike previous work, we do not binarize the trees. The number of permutations is factorial in n, and learning a sequence of transductions for explaining a source permutation can be computationally rather challenging (see (Tromble and Eisner, 2009)). Yet, Introduction From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word 92 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 92–100, COLING 2010, Beijing, August 2010. ter subtrees might turn out insufficient. from the limited perspective of source string per0 mutation (s → s ), another challenge is to inte0 grate a figure of merit"
W10-3812,E99-1010,0,0.228942,"inguistic perspective. The features that were used for training the permutation system are extracted for every internal node of the source tree that has more than one child: 5 Experiments and results The SMT system used in the experiments was implemented within the open-source MOSES toolkit (Koehn et al., 2007). Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page1 . The MSD model was used together with a distance-based reordering model. Word alignment was estimated with GIZA++ tool2 (Och, 2003), coupled with mkcls3 (Och, 1999), which allows for statistical word clustering for better generalization. An 5-gram target language model was estimated using the SRI LM toolkit (Stolcke, 2002) and smoothed with modified Kneser-Ney discounting. We use the Stanford parser4 (Klein and Manning, 2003) as a source-side parsing engine. The parser was trained on the English treebank set provided with 14 syntactic categories and 48 POS tags. The evaluation conditions were case-sensitive and included punctuation marks. For Maximum Entropy modeling we used the maxent toolkit5 . • Local tree topology. Sub-tree instances that include par"
W10-3812,P06-1066,0,0.0568594,", 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long-distance word-block permutations. Coming up-to-date, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Recently, Tromble and Eisner (2009) define source permutation as learning source permutations; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and"
W10-3812,P03-1019,0,0.0135266,"ransductions for explaining a source permutation can be computationally rather challenging (see (Tromble and Eisner, 2009)). Yet, Introduction From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality. While standard mechanisms embedded in phrasebased SMT systems, e.g. (Och and Ney, 2004), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g., (Tillmann and Ney, 2003; Zens and Ney, 2003; Tillman, 2004). More recent work handles word 92 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 92–100, COLING 2010, Beijing, August 2010. ter subtrees might turn out insufficient. from the limited perspective of source string per0 mutation (s → s ), another challenge is to inte0 grate a figure of merit that measures in how far s resembles a plausible target word-order. 2 Baseline: Phrase-based SMT Given a word-aligned parallel corpus, phrasebased systems (Och and Ney, 2002; Koehn et al., 2003) work with (in principle) arbitrarily large phras"
W10-3812,2002.tmi-tutorials.2,0,0.229565,"al Translation, pages 92–100, COLING 2010, Beijing, August 2010. ter subtrees might turn out insufficient. from the limited perspective of source string per0 mutation (s → s ), another challenge is to inte0 grate a figure of merit that measures in how far s resembles a plausible target word-order. 2 Baseline: Phrase-based SMT Given a word-aligned parallel corpus, phrasebased systems (Och and Ney, 2002; Koehn et al., 2003) work with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a simple definition of translational equivalence (Zens et al., 2002). The conditional probabilities of one phrase given its counterpart are interpolated log-linearly together with a set of other model estimates: ( M ) X I I J eˆ1 = arg max λm hm (e1 , f1 ) (1) We contribute solutions to these challenging problems. Firstly, we learn the transduction operations using a discriminative estimate of P (π(αx ) |Nx , αx , contextx ), where Nx is the label of node (address) x, Nx → αx is the contextfree production under x, π(αx ) is a permutation of αx and contextx represents a surrounding syntactic context. As a result, this constrains {π(αx )} only to those found in"
W10-3812,W06-3119,0,0.137013,"Missing"
W11-2150,P05-1066,0,0.0785702,"Missing"
W11-2150,D08-1089,0,0.0199153,"ang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. In (Costa-juss`a and Fonollosa, 2006) source and target word order harmonization is done using wellestablished SMT techniques and without the use of syntactic knowledge. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle longdistance word-block permutations. Coming up-todate, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Different syntax-based reordering systems can be found in (Genzel, 2010). In this system, reordering rules capable to capture many important word order transformations are automatically learned and applied in the preprocessing step. Recently, Tromble and Eisner (Tromble and Eisner, 2009) define source permutation as the wordorderin"
W11-2150,P06-1121,0,0.0139891,"ory, (2) whether the parent node is a descendant of a node annotated with the same syntactic category. 4 Related work The integration of linguistic syntax into SMT systems offers a potential solution to reordering problem. For example, syntax is successfully integrated into hierarchical SMT (Zollmann and Venugopal, 2006). In (Yamada and Knight, 2001), a set of treestring channel operations is defined over the parse tree nodes, while reordering is modeled by permutations of children nodes. Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-toEnglish and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. In (Costa-juss`a and Fonollosa, 2006) source and target word order harmonization is done using wellestablished SMT techniques and without the"
W11-2150,C10-1043,0,0.10296,"anguage and a given source-side parse tree. The difference from the baseline Moses-based translation system lies in the pre-translation step, in which we introduce a discriminative source string permutation model based on probabilistic parse tree transduction. The idea here is to permute the order of the source words in such a way that the resulting permutation allows as monotone a translation process as possible is not new. This approach to enhance SMT by using a reordering step prior to translation has proved to be successful in improving translation quality for many translation tasks, see (Genzel, 2010; Costa-juss`a and Fonollosa, 2006; Collins et al., 2005), for example. The general problem of source-side reordering is that the number of permutations is factorial in n, and learning a sequence of transductions for explaining a source permutation can be computationally rather challenging. We propose to address this problem by defining the source-side permutation process as the learning problem of how to transfer a given source parse tree into a parse tree that minimizes the divergence from target word order. Our reordering system is inspired by the direction taken in (Tromble and Eisner, 200"
W11-2150,W10-3812,1,0.728678,"Missing"
W11-2150,P03-1054,0,0.00455577,"separate MaxEnt models for the categories with potentially high reordering requirements, namely N P , SEN T and SBAR(Q). It was defines as our “primary” submission. The ranking of submission was done according to the results shown on internal testing, shown in Table 3. System Baseline Primary Secondary • GIZA++/mkcls (Och, 2003; Och, 1999) for word alignment. • SRI LM (Stolcke, 2002) for language modeling. A 3-gram target language model was estimated and smoothed with modified KneserNey discounting. • MOSES (Koehn et al., 2007) to build an unfactored translation system. • the Stanford parser (Klein and Manning, 2003) was used as a source-side parsing engine3 . • For maximum entropy modeling we used the maxent toolkit4 . The discriminative syntactic reordering model is applied to reorder training, development, and test corpora. A Moses-based translation system (corpus realignment included5 ) is then trained using the reordered input. 5.3 Internal results and submissions The outputs of two translation system were submitted. First, we piled up all feature functions into a single model as described in Section 3. It was our “secondary” submission. However, our experience tells 3 The parser was trained on the E"
W11-2150,N03-1017,0,0.0163749,"Missing"
W11-2150,P07-2045,0,0.00703438,"(version 6.0). Apart from the German portion of the EuroParl parallel corpus, two additional monolingual corpora from news domain (the News Commentary corpus (NC) and the News Crawl Corpus 2011 (NS)) were 2 http://www.statmt.org/wmt11/baseline. html used to train a language model for German. The characteristics of these datasets can be found in Table 2. Notice that the data were not de-duplicated. Data NC Ge NS Ge Sent. 161.8M 45.3M Words 3.9G 799.4M Voc. 136.7M 3.0M ASL 23.9 17.7 Table 2: Monolingual German corpora used for targetside language modeling. 5.2 Experimental setup Moses toolkit (Koehn et al., 2007) in its standard setting was used to build the SMT systems: that the system performance can increase if the set of patterns is split into partial classes conditioned on the current node label (Khalilov and Sima’an, 2010). Hence, we trained three separate MaxEnt models for the categories with potentially high reordering requirements, namely N P , SEN T and SBAR(Q). It was defines as our “primary” submission. The ranking of submission was done according to the results shown on internal testing, shown in Table 3. System Baseline Primary Secondary • GIZA++/mkcls (Och, 2003; Och, 1999) for word ali"
W11-2150,P02-1038,0,0.0602976,"omble and Eisner, 2009)). However, different kinds of constraints can be made on unfolding the crossing alignments in a. A common approach in hierarchical SMT is to assume that the source string has a binary parse tree, and the set of eligible permutations is defined by binary ITG transductions on this tree. This defines permutations that can be obtained only by at most inverting pairs of children under nodes of the source tree. t t 2.2 Phrase-based translation While first systems following this approach performed translation on the word level, modern stateof-the-art phrase-based SMT systems (Och and Ney, 2002; Koehn et al., 2003) start-out from a wordaligned parallel corpus working with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a simple definition of translational equivalence (Zens et al., 2002). The conditional probabilities of one phrase given its counterpart is estimated as the relative frequency ratio of the phrases in the multiset of phrase-pairs extracted from the parallel corpus and are interpolated log-linearly together with a set of other model estimates: ( eˆI1 = arg max eI1 M X ) λm hm (eI1 , f1J ) (1) m=1 where a f"
W11-2150,E99-1010,0,0.0113968,"it (Koehn et al., 2007) in its standard setting was used to build the SMT systems: that the system performance can increase if the set of patterns is split into partial classes conditioned on the current node label (Khalilov and Sima’an, 2010). Hence, we trained three separate MaxEnt models for the categories with potentially high reordering requirements, namely N P , SEN T and SBAR(Q). It was defines as our “primary” submission. The ranking of submission was done according to the results shown on internal testing, shown in Table 3. System Baseline Primary Secondary • GIZA++/mkcls (Och, 2003; Och, 1999) for word alignment. • SRI LM (Stolcke, 2002) for language modeling. A 3-gram target language model was estimated and smoothed with modified KneserNey discounting. • MOSES (Koehn et al., 2007) to build an unfactored translation system. • the Stanford parser (Klein and Manning, 2003) was used as a source-side parsing engine3 . • For maximum entropy modeling we used the maxent toolkit4 . The discriminative syntactic reordering model is applied to reorder training, development, and test corpora. A Moses-based translation system (corpus realignment included5 ) is then trained using the reordered i"
W11-2150,P03-1021,0,0.0993432,"equency ratio of the phrases in the multiset of phrase-pairs extracted from the parallel corpus and are interpolated log-linearly together with a set of other model estimates: ( eˆI1 = arg max eI1 M X ) λm hm (eI1 , f1J ) (1) m=1 where a feature function hm refer to a system model, and the corresponding λm refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights λm are optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002). Apart from the novel syntax-based reordering model, we consider two reordering methods that are widely used in phrase-based systems: a simple distance-based reordering and a lexicalized blockoriented data-driven reordering model (Tillman, 2004). 3 Architecture of the reordering system We approach the word order challenge by including syntactic information in a pre-translation reordering framework. This section details the general idea of our approach and details the reordering model that was used in English-to-German experiments. 414 3.2 Pre-transl"
W11-2150,P02-1040,0,0.0862569,"the multiset of phrase-pairs extracted from the parallel corpus and are interpolated log-linearly together with a set of other model estimates: ( eˆI1 = arg max eI1 M X ) λm hm (eI1 , f1J ) (1) m=1 where a feature function hm refer to a system model, and the corresponding λm refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights λm are optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002). Apart from the novel syntax-based reordering model, we consider two reordering methods that are widely used in phrase-based systems: a simple distance-based reordering and a lexicalized blockoriented data-driven reordering model (Tillman, 2004). 3 Architecture of the reordering system We approach the word order challenge by including syntactic information in a pre-translation reordering framework. This section details the general idea of our approach and details the reordering model that was used in English-to-German experiments. 414 3.2 Pre-translation reordering framework Conditional tree"
W11-2150,N10-3010,0,0.0139127,"t texts. In (Costa-juss`a and Fonollosa, 2006) source and target word order harmonization is done using wellestablished SMT techniques and without the use of syntactic knowledge. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle longdistance word-block permutations. Coming up-todate, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Different syntax-based reordering systems can be found in (Genzel, 2010). In this system, reordering rules capable to capture many important word order transformations are automatically learned and applied in the preprocessing step. Recently, Tromble and Eisner (Tromble and Eisner, 2009) define source permutation as the wordordering learning problem; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and a corresponding weight"
W11-2150,N04-4026,0,0.179905,"the corresponding λm refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights λm are optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002). Apart from the novel syntax-based reordering model, we consider two reordering methods that are widely used in phrase-based systems: a simple distance-based reordering and a lexicalized blockoriented data-driven reordering model (Tillman, 2004). 3 Architecture of the reordering system We approach the word order challenge by including syntactic information in a pre-translation reordering framework. This section details the general idea of our approach and details the reordering model that was used in English-to-German experiments. 414 3.2 Pre-translation reordering framework Conditional tree reordering model Given a parallel corpus with string pairs s → t with word alignment a, the source strings s are parsed, leading to a single parse tree τs per source string. We 0 create a source permuted parallel corpus s → s by unfolding the cro"
W11-2150,D09-1105,0,0.120622,"asks, see (Genzel, 2010; Costa-juss`a and Fonollosa, 2006; Collins et al., 2005), for example. The general problem of source-side reordering is that the number of permutations is factorial in n, and learning a sequence of transductions for explaining a source permutation can be computationally rather challenging. We propose to address this problem by defining the source-side permutation process as the learning problem of how to transfer a given source parse tree into a parse tree that minimizes the divergence from target word order. Our reordering system is inspired by the direction taken in (Tromble and Eisner, 2009), but differs in defining the space of permutations, using local probabilistic tree transductions, as well as in the learning objective aiming at scoring permutations based on a log-linear interpolation of a local syntax-based model with a global string-based (language) model. The reordering (novel) and translation (standard) components are described in the following sections. The rest of this paper is structured as follows. After a brief description of the phrase-based translation system in Section 2, we present the architecture and details of our reordering system (Section 3), Section 4 revi"
W11-2150,C10-1126,0,0.0118318,"proach. Syntax-driven reordering, as described in this paper, involves large contextual information applied cumulatively. Under conditions of scarce data, alignment and parsing errors, it introduces noise to the reordering system and distorts the feature probability space. At the same time, many reorderings can be performed more efficiently based on fixed (hand-crafted) rules (as it is done in (Collins et al., 2005)). A possible remedy to this problem is to combine automatically extracted features with fixed (hand-crafted) rules. Our last claims are supported by the observations described in (Visweswariah et al., 2010). During post-evaluation period we analyzed the reasons why the system performance has slightly improved when separate MaxEnt models are applied. The outline of reordered nodes for each of syntactic categories considered (SEN T , SBAR(Q) and N P ) can be found in Table 4 (the size of the corpus is 1.7 M of sentences). Category NP SBAR(Q) SENT # of applications 497,186 106,243 221,568 7 Acknowledgements Both authors are supported by a VIDI grant (nr. 639.022.604) from The Netherlands Organization for Scientific Research (NWO). References Table 4: Application of reorderings for separate syntacti"
W11-2150,D07-1077,0,0.0417973,"Missing"
W11-2150,C04-1073,0,0.0320265,"ht, 2001), a set of treestring channel operations is defined over the parse tree nodes, while reordering is modeled by permutations of children nodes. Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-toEnglish and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. In (Costa-juss`a and Fonollosa, 2006) source and target word order harmonization is done using wellestablished SMT techniques and without the use of syntactic knowledge. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the fa"
W11-2150,P06-1066,0,0.0242741,"nd-crafted reordering rules for German-toEnglish and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts. In (Costa-juss`a and Fonollosa, 2006) source and target word order harmonization is done using wellestablished SMT techniques and without the use of syntactic knowledge. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle longdistance word-block permutations. Coming up-todate, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented. Different syntax-based reordering systems can be found in (Genzel, 2010). In this system, reordering rules capable to capture many important word order transformations are automatically learned and applied in the p"
W11-2150,P01-1067,0,0.0933916,"f the head word of the current node, together with the sequence of POS tags of the head words of its child nodes. • Syntactic features. Two binary features from this class describe: (1) whether the parent node is a child of the node annotated with the same syntactic category, (2) whether the parent node is a descendant of a node annotated with the same syntactic category. 4 Related work The integration of linguistic syntax into SMT systems offers a potential solution to reordering problem. For example, syntax is successfully integrated into hierarchical SMT (Zollmann and Venugopal, 2006). In (Yamada and Knight, 2001), a set of treestring channel operations is defined over the parse tree nodes, while reordering is modeled by permutations of children nodes. Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-toEnglish and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and M"
W11-2150,2002.tmi-tutorials.2,0,0.0365225,"efined by binary ITG transductions on this tree. This defines permutations that can be obtained only by at most inverting pairs of children under nodes of the source tree. t t 2.2 Phrase-based translation While first systems following this approach performed translation on the word level, modern stateof-the-art phrase-based SMT systems (Och and Ney, 2002; Koehn et al., 2003) start-out from a wordaligned parallel corpus working with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a simple definition of translational equivalence (Zens et al., 2002). The conditional probabilities of one phrase given its counterpart is estimated as the relative frequency ratio of the phrases in the multiset of phrase-pairs extracted from the parallel corpus and are interpolated log-linearly together with a set of other model estimates: ( eˆI1 = arg max eI1 M X ) λm hm (eI1 , f1J ) (1) m=1 where a feature function hm refer to a system model, and the corresponding λm refers to the relative weight given to this model. A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to scor"
W11-2150,W06-3119,0,0.032042,"atures that determine the POS tag of the head word of the current node, together with the sequence of POS tags of the head words of its child nodes. • Syntactic features. Two binary features from this class describe: (1) whether the parent node is a child of the node annotated with the same syntactic category, (2) whether the parent node is a descendant of a node annotated with the same syntactic category. 4 Related work The integration of linguistic syntax into SMT systems offers a potential solution to reordering problem. For example, syntax is successfully integrated into hierarchical SMT (Zollmann and Venugopal, 2006). In (Yamada and Knight, 2001), a set of treestring channel operations is defined over the parse tree nodes, while reordering is modeled by permutations of children nodes. Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006). The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-toEnglish and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007"
W11-2150,W06-1609,0,\N,Missing
W11-2911,D10-1068,0,0.0360402,"Missing"
W11-2911,P11-1070,0,0.13792,"M in particular, has been successful for classification-based NLP tasks (e.g. Nigam et al. (1998), Blum and Mitchell (1998), Yarowsky (1995)). For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (Klein and Manning, 2004; Steedman et al., 2003; Druck et al., 2009a; Ganchev et al., 2010; Reichart and Rappoport, 2007). There have been some instances of successful large-scale semi-supervised learning for structured models (McClosky et al., 2006; Deoskar, 2008; Koo et al., 2008; Bansal and Klein, 2011), where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further improvement from unlabeled data. These methods have typically depended on the complementarity of multiple views Introduction Computational models of natural language trained on labeled data contain many parameters that are not estimated accurately, due to the data sparsity inherent in labeled data. This is especially true of complex structured models like parsers, which contain a large number of parameters, and where labeled training data is expensive to create.These models employ"
W11-2911,P09-1041,0,0.0186802,"of a generative structured model already trained over large labeled data. 1 Khalil Sima’an ILLC University of Amsterdam k.simaan@uva.nl From the machine learning point of view, semi-supervised learning in general, and semisupervised EM in particular, has been successful for classification-based NLP tasks (e.g. Nigam et al. (1998), Blum and Mitchell (1998), Yarowsky (1995)). For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (Klein and Manning, 2004; Steedman et al., 2003; Druck et al., 2009a; Ganchev et al., 2010; Reichart and Rappoport, 2007). There have been some instances of successful large-scale semi-supervised learning for structured models (McClosky et al., 2006; Deoskar, 2008; Koo et al., 2008; Bansal and Klein, 2011), where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further improvement from unlabeled data. These methods have typically depended on the complementarity of multiple views Introduction Computational models of natural language trained on labeled data contain many parameters that are not estimated accurate"
W11-2911,A00-2031,0,0.109635,"Missing"
W11-2911,P02-1043,0,0.0386335,"tags), the problem of smoothing becomes more severe. In some other generative models containing fine-grained lexical categories, such as CCG, smoothing is done by replacing unseen words and words below a cut-off 1 2 This number holds for the case when lexicalized prepositions are not projected into the supertag. The complete list is available in Deoskar (2009) (Appendix D). Merlo and Musillo (2005)’s work uses a subset of the functional tags in the PTB, and hence their results are not comparable to ours. 82 3 frequency with POS tags. This cut-off frequency is in fact very high – for instance, Hockenmaier and Steedman (2002) find that the optimal cutoff is 30 for their generative parser. In our work, such a method is not an option: we are interested precisely in learning supertags for low frequency and unseen words from the unlabeled corpus. Secondly, POS tags are not a parameter of the PCFG, only supertags are. We adopt a smoothing method first described in Deoskar (2008), that specifically aims at introducing parameters for unseen words from the unlabeled corpus into the PCFG3 . In this method, every word from the unlabeled corpus is assigned with all those supertags that have been seen in the labeled corpus wi"
W11-2911,J98-4004,0,0.218828,"Missing"
W11-2911,P04-1061,0,0.0434009,"st successful improvement via semi-supervised EM of a generative structured model already trained over large labeled data. 1 Khalil Sima’an ILLC University of Amsterdam k.simaan@uva.nl From the machine learning point of view, semi-supervised learning in general, and semisupervised EM in particular, has been successful for classification-based NLP tasks (e.g. Nigam et al. (1998), Blum and Mitchell (1998), Yarowsky (1995)). For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (Klein and Manning, 2004; Steedman et al., 2003; Druck et al., 2009a; Ganchev et al., 2010; Reichart and Rappoport, 2007). There have been some instances of successful large-scale semi-supervised learning for structured models (McClosky et al., 2006; Deoskar, 2008; Koo et al., 2008; Bansal and Klein, 2011), where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further improvement from unlabeled data. These methods have typically depended on the complementarity of multiple views Introduction Computational models of natural language trained on labeled data contain many"
W11-2911,P03-1054,0,0.0283115,"Missing"
W11-2911,P05-1022,0,0.0625749,"ly lexicalised formalisms like CCG and LTAG, of which statistical models suffer from severe sparsity and have not been successfully trained using semisupervised methods. Another area of future work will be to incorporate supertags that encode other forms of lexico-structural dependencies, such as noun subcategorization or adverb attachment. McClosky et al. (2006) enhance the performance of a state-of-the-art parser-reranker combination by self-training on large amounts of unlabeled data. Much of the improvement in their case comes from the ability of an external maximumentropy Parse Reranker (Charniak and Johnson, 2005) to select parses from the parser’s output for the unannotated sentences. Our work differs from McClosky et al. (2006) in that, firstly, they employ a fully lexicalized parser, whereas our parser is unlexicalised with supertags as pre-terminals. We are thus isolating lexico-syntactic dependencies, rather than word-word dependencies. All our improvements come from enhancing the lexical component of the PCFG. They find in their analysis that lexical learning does not play a large role in the improvements they obtain. Secondly, in contrast with their somewhat complex self-training objective, we r"
W11-2911,P08-1068,0,0.70078,"d semisupervised EM in particular, has been successful for classification-based NLP tasks (e.g. Nigam et al. (1998), Blum and Mitchell (1998), Yarowsky (1995)). For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (Klein and Manning, 2004; Steedman et al., 2003; Druck et al., 2009a; Ganchev et al., 2010; Reichart and Rappoport, 2007). There have been some instances of successful large-scale semi-supervised learning for structured models (McClosky et al., 2006; Deoskar, 2008; Koo et al., 2008; Bansal and Klein, 2011), where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further improvement from unlabeled data. These methods have typically depended on the complementarity of multiple views Introduction Computational models of natural language trained on labeled data contain many parameters that are not estimated accurately, due to the data sparsity inherent in labeled data. This is especially true of complex structured models like parsers, which contain a large number of parameters, and where labeled training data is expensive to c"
W11-2911,C04-1041,0,0.0344745,"n framework, which incorporates data-dependent constraints encoded as model posteriors on the observed data. The Generalized Expectation criteria (Mann and McCallum, 2010, 2007) incorporates weakly labeled data or ‘sideinformation’ such as marginal label distributions to inform estimation from unlabeled data. These methods have been shown to work for some structured tasks but have not been applied to a large scale grammar yet, and whether they can be used to improve a high baseline model is an open question. There is also a substantial body of work on supertagging (Bangalore and Joshi (1999); Clark and Curran (2004), amongst several others), but their PP-CLR for getting back into operation (a) Incorrect parse from baseline model VP VBG.n NP exceeding NP PP its goals for getting back into operation (b) Correct parse from EM-estimated model Figure 4: Improvement in PP attachment. VP VBZ.d PP-DIR aims to profit PP-MNR by selling borrowed shares (a) Incorrect parse from baseline model VP VBZ.s.e.to aims S *NP* VP VP TO to VB.z Related Work PP-MNR profit by selling borrowed shares (b) Correct parse from EM-estimated model Figure 5: Detection of an S structure for aims ments in a common PP attachment case (som"
W11-2911,P97-1003,0,0.391429,"Missing"
W11-2911,C08-1025,1,0.876962,"in general, and semisupervised EM in particular, has been successful for classification-based NLP tasks (e.g. Nigam et al. (1998), Blum and Mitchell (1998), Yarowsky (1995)). For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (Klein and Manning, 2004; Steedman et al., 2003; Druck et al., 2009a; Ganchev et al., 2010; Reichart and Rappoport, 2007). There have been some instances of successful large-scale semi-supervised learning for structured models (McClosky et al., 2006; Deoskar, 2008; Koo et al., 2008; Bansal and Klein, 2011), where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further improvement from unlabeled data. These methods have typically depended on the complementarity of multiple views Introduction Computational models of natural language trained on labeled data contain many parameters that are not estimated accurately, due to the data sparsity inherent in labeled data. This is especially true of complex structured models like parsers, which contain a large number of parameters, and where labeled training data"
W11-2911,J93-2004,0,0.0374024,"Missing"
W11-2911,N06-1020,0,0.779697,"emi-supervised learning in general, and semisupervised EM in particular, has been successful for classification-based NLP tasks (e.g. Nigam et al. (1998), Blum and Mitchell (1998), Yarowsky (1995)). For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (Klein and Manning, 2004; Steedman et al., 2003; Druck et al., 2009a; Ganchev et al., 2010; Reichart and Rappoport, 2007). There have been some instances of successful large-scale semi-supervised learning for structured models (McClosky et al., 2006; Deoskar, 2008; Koo et al., 2008; Bansal and Klein, 2011), where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further improvement from unlabeled data. These methods have typically depended on the complementarity of multiple views Introduction Computational models of natural language trained on labeled data contain many parameters that are not estimated accurately, due to the data sparsity inherent in labeled data. This is especially true of complex structured models like parsers, which contain a large number of parameters, and where labele"
W11-2911,deoskar-rooth-2008-induction,1,0.905912,"Missing"
W11-2911,J94-2001,0,0.190047,"(Steedman, 2000). A supertag encodes structure that is distributed over the tree and localises it onto a single parameter of the model. Our learning problem is cast very simply as estimating the parameters p(w|τ ) (where w is a word and τ a supertag) from labeled and unlabeled data. The problem is, however, more complex than a sequence labeling task because these supertags are highly ambiguous and encode argument-adjunct distinctions as well as long-distance dependencies (illustrated later in examples). Semi-supervised EM is known to often give models that are worse than the supervised model (Merialdo, 1994; Charniak, 1993; Ng and Cardie, 2003). To address this, we incorporate probabilistic constraints on unsupervised estimation by using labeled data to derive prior knowledge at two levels: (a) structural constraints in the form of higher PCFG rules (b) preferences over the distributions p(w|τ ) themselves. We obtain large improvements in assigning correct structures to unseen verbs, and also a statistically significant improvement in labeled bracketing over a smoothed supervised model. The rest of the paper is structured as follows: a description of the Treebank PCFG model and its smoothing is"
W11-2911,H05-1078,0,0.0231568,"ords. This is usually done by backing off from a more articulated level (such as words) to a less articulated one (such as POS-tags), or by interpolating between the two. In the case of finegrained lexical categories (supertags), the problem of smoothing becomes more severe. In some other generative models containing fine-grained lexical categories, such as CCG, smoothing is done by replacing unseen words and words below a cut-off 1 2 This number holds for the case when lexicalized prepositions are not projected into the supertag. The complete list is available in Deoskar (2009) (Appendix D). Merlo and Musillo (2005)’s work uses a subset of the functional tags in the PTB, and hence their results are not comparable to ours. 82 3 frequency with POS tags. This cut-off frequency is in fact very high – for instance, Hockenmaier and Steedman (2002) find that the optimal cutoff is 30 for their generative parser. In our work, such a method is not an option: we are interested precisely in learning supertags for low frequency and unseen words from the unlabeled corpus. Secondly, POS tags are not a parameter of the PCFG, only supertags are. We adopt a smoothing method first described in Deoskar (2008), that specific"
W11-2911,N03-1023,0,0.027389,"odes structure that is distributed over the tree and localises it onto a single parameter of the model. Our learning problem is cast very simply as estimating the parameters p(w|τ ) (where w is a word and τ a supertag) from labeled and unlabeled data. The problem is, however, more complex than a sequence labeling task because these supertags are highly ambiguous and encode argument-adjunct distinctions as well as long-distance dependencies (illustrated later in examples). Semi-supervised EM is known to often give models that are worse than the supervised model (Merialdo, 1994; Charniak, 1993; Ng and Cardie, 2003). To address this, we incorporate probabilistic constraints on unsupervised estimation by using labeled data to derive prior knowledge at two levels: (a) structural constraints in the form of higher PCFG rules (b) preferences over the distributions p(w|τ ) themselves. We obtain large improvements in assigning correct structures to unseen verbs, and also a statistically significant improvement in labeled bracketing over a smoothed supervised model. The rest of the paper is structured as follows: a description of the Treebank PCFG model and its smoothing is in §2. §3 describes the semisupervised"
W11-2911,N07-1051,0,0.0716665,"Missing"
W11-2911,P10-1051,0,0.0451146,"Missing"
W11-2911,P07-1078,0,0.0165125,"rained over large labeled data. 1 Khalil Sima’an ILLC University of Amsterdam k.simaan@uva.nl From the machine learning point of view, semi-supervised learning in general, and semisupervised EM in particular, has been successful for classification-based NLP tasks (e.g. Nigam et al. (1998), Blum and Mitchell (1998), Yarowsky (1995)). For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (Klein and Manning, 2004; Steedman et al., 2003; Druck et al., 2009a; Ganchev et al., 2010; Reichart and Rappoport, 2007). There have been some instances of successful large-scale semi-supervised learning for structured models (McClosky et al., 2006; Deoskar, 2008; Koo et al., 2008; Bansal and Klein, 2011), where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further improvement from unlabeled data. These methods have typically depended on the complementarity of multiple views Introduction Computational models of natural language trained on labeled data contain many parameters that are not estimated accurately, due to the data sparsity inherent in labeled data."
W11-2911,C04-1024,0,0.0215638,"ity mass is (1 − πfixed ) and is distributed to the free parameters in proportion to the related (expected) counts c(w, τ ). We skip the proof due to space limitations. 4 Experiments We report experiments using a treebank PCFG trained on approximately 36,000 sentences from sections 0-22 of the Wall Street Journal (WSJ) portion of the PTB, with about 5000 sentences heldout for testing and development. Semi-supervised training is carried out using 4, 8, 12 and 16 million words of unlabeled WSJ data, after limiting sentence length to <25 words. Inside-outside estimation is implemented in Bitpar (Schmid, 2004). The corpus scaling factor for labeled data is set to 8 (i.e., a = 1 and b = 8 in Eq. 3; this value makes our labeled data ( 1 million words) weigh about twice as much as our smallest unlabeled corpus of 4 million words. We experimented with setting the scaling factor to 4, making the labeled corpus of 1 θi (w|τ ) = C TB a ∗ cU i−1 (w, τ ) + b ∗ ci−1 (w, τ ) (1 − πfixed ) X C 0 TB 0 a ∗ cU i−1 (w , τ ) + b ∗ ci−1 (w , τ ) τ w0 ∈Wfree (4) 3.5 (5) Semi-Supervised Learning as Maximum A Posteriori Estimation In this subsection, we discuss an interpretation of our learning method (i.e. maximum-lik"
W11-2911,P06-1023,0,0.0299567,"Missing"
W11-2911,E03-1008,0,0.0300183,"via semi-supervised EM of a generative structured model already trained over large labeled data. 1 Khalil Sima’an ILLC University of Amsterdam k.simaan@uva.nl From the machine learning point of view, semi-supervised learning in general, and semisupervised EM in particular, has been successful for classification-based NLP tasks (e.g. Nigam et al. (1998), Blum and Mitchell (1998), Yarowsky (1995)). For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (Klein and Manning, 2004; Steedman et al., 2003; Druck et al., 2009a; Ganchev et al., 2010; Reichart and Rappoport, 2007). There have been some instances of successful large-scale semi-supervised learning for structured models (McClosky et al., 2006; Deoskar, 2008; Koo et al., 2008; Bansal and Klein, 2011), where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further improvement from unlabeled data. These methods have typically depended on the complementarity of multiple views Introduction Computational models of natural language trained on labeled data contain many parameters that are no"
W11-2911,P95-1026,0,0.155122,"seen verbs, the most important case of learning lexico-structural dependencies. We also obtain a statistically significant improvement in labeled bracketing score of the treebank PCFG, the first successful improvement via semi-supervised EM of a generative structured model already trained over large labeled data. 1 Khalil Sima’an ILLC University of Amsterdam k.simaan@uva.nl From the machine learning point of view, semi-supervised learning in general, and semisupervised EM in particular, has been successful for classification-based NLP tasks (e.g. Nigam et al. (1998), Blum and Mitchell (1998), Yarowsky (1995)). For more structured tasks such as partof-speech tagging and grammar learning, semisupervised learning has worked largely in the case where the labeled data is small in size (Klein and Manning, 2004; Steedman et al., 2003; Druck et al., 2009a; Ganchev et al., 2010; Reichart and Rappoport, 2007). There have been some instances of successful large-scale semi-supervised learning for structured models (McClosky et al., 2006; Deoskar, 2008; Koo et al., 2008; Bansal and Klein, 2011), where a grammar model trained on a large amount of labeled data such as the full Penn Treebank has shown further im"
W11-2911,J99-2004,0,\N,Missing
W13-0803,2011.eamt-1.38,0,0.709738,"eline, exemplifying certain merits of this novel approach. 1 Introduction The Hiero model (Chiang, 2007; Chiang, 2005) formulates phrase-based translation in terms of a synchronous context-free grammar (SCFG) limited to the inversion transduction grammar (ITG) (Wu, 1997) family. While the original Hiero approach works with a single nonterminal label (X) (besides the start nonterminal S ), more recent work is dedicated to devising methods for extracting more elaborate labels for the phrase-pairs and their abstractions into SCFG productions, e.g., (Zollmann and Venugopal, 2006; Li et al., 2012; Almaghout et al., 2011). All labeling approaches exploit monolingual parsers of some kind, e.g., syntactic, semanKhalil Sima’an Institute for Logic, Language and Computation University of Amsterdam Science Park 904, 1098 XH Amsterdam The Netherlands k.simaan AT uva.nl tic or sense-oriented. The rationale behind monolingual labeling is often to make the probability distributions over alternative synchronous derivations of the Hiero model more sensitive to linguistically justified monolingual phrase context. For example, syntactic target-language labels in many approaches are aimed at improved target language modeling"
W13-0803,N12-1047,0,0.141643,"best-dev” in Moses to actually get the weights from the best development run, our initial experiments had not used this. This explains the somewhat unfortunate drop in performance in our Analysis Experiments. Decoding Type Lattice MBR Viterbi System Name Hiero Hiero-RL Hiero Hiero-RL-PPL 5.1.2 200K 26.44 26.72 26.23 26.16 Table 1: Initial Results. Lowercase BLEU results for German-English trained on 200K sentence pairs.4 Top rows display results for our experiments using Moses (Hoang et al., 2007) with Lattice Minimum Bayes-Risk Decoding5 (Tromble et al., 2008) in combination with Batch Mira (Cherry and Foster, 2012) for tuning. Below are results for experiments with Joshua (Ganitkevitch et al., 2012) using Viterbi decoding (i.e. no MBR) and PRO (Hopkins and May, 2011) for tuning. were done on Joshua (Ganitkevitch et al., 2012), using the Viterbi best derivation. The second set of experiments was done on Moses (Hoang et al., 2007) using Lattice Minimum Bayes-Risk Decoding5 (Tromble et al., 2008) to sum over derivations. 5.1.1 General Settings Based on experiments reported in (Mylonakis and Sima’an, 2011; Mylonakis, 2012) we opted to not label the (fully lexicalized) phrase pairs, but instead label them wi"
W13-0803,P05-1033,0,0.871582,"nature to monolingual labels. In this paper we explore a first version of this idea based on a hierarchical decomposition of word alignments into recursive tree representations. We identify five clusters of alignment patterns in which the children of a node in a decomposition tree are found and employ these five as nonterminal labels for the Hiero productions. Although this is our first non-optimized instantiation of the idea, our experiments show competitive performance with the Hiero baseline, exemplifying certain merits of this novel approach. 1 Introduction The Hiero model (Chiang, 2007; Chiang, 2005) formulates phrase-based translation in terms of a synchronous context-free grammar (SCFG) limited to the inversion transduction grammar (ITG) (Wu, 1997) family. While the original Hiero approach works with a single nonterminal label (X) (besides the start nonterminal S ), more recent work is dedicated to devising methods for extracting more elaborate labels for the phrase-pairs and their abstractions into SCFG productions, e.g., (Zollmann and Venugopal, 2006; Li et al., 2012; Almaghout et al., 2011). All labeling approaches exploit monolingual parsers of some kind, e.g., syntactic, semanKhali"
W13-0803,J07-2003,0,0.802825,"complementary) nature to monolingual labels. In this paper we explore a first version of this idea based on a hierarchical decomposition of word alignments into recursive tree representations. We identify five clusters of alignment patterns in which the children of a node in a decomposition tree are found and employ these five as nonterminal labels for the Hiero productions. Although this is our first non-optimized instantiation of the idea, our experiments show competitive performance with the Hiero baseline, exemplifying certain merits of this novel approach. 1 Introduction The Hiero model (Chiang, 2007; Chiang, 2005) formulates phrase-based translation in terms of a synchronous context-free grammar (SCFG) limited to the inversion transduction grammar (ITG) (Wu, 1997) family. While the original Hiero approach works with a single nonterminal label (X) (besides the start nonterminal S ), more recent work is dedicated to devising methods for extracting more elaborate labels for the phrase-pairs and their abstractions into SCFG productions, e.g., (Zollmann and Venugopal, 2006; Li et al., 2012; Almaghout et al., 2011). All labeling approaches exploit monolingual parsers of some kind, e.g., syntac"
W13-0803,P10-1146,0,0.310663,"t |s) can be optimized through as single best derivation as follows: arg max P(t |s) = arg max t t X d∈G ≈ arg max P(t, d |s) d∈G 20 (2) This approximation can be notoriously problematic for labelled Hiero models because the labels tend to lead to many more derivations than in the original model, thereby aggravating the effects of this assumption. This problem is relevant for our work and approaches to deal with it are Minimum BayesRisk decoding (Kumar and Byrne, 2004; Tromble et al., 2008), Variational Decoding (Li et al., 2009) and soft labeling (Venugopal et al., 2009; Marton et al., 2012; Chiang, 2010). Given a derivation d, most existing phrasebased models approximate the derivation probability through a linear interpolation of a finite set of feature functions (Φ(d)) of the derivation d, mostly working with local feature functions φi of individual productions, the target side yield string t of d (target language model features) and other heuristic features discussed in the experimental section: Hierarchical SMT models Hierarchical SMT usually works with weighted instantiations of Synchronous Context-Free Grammars (SCFGs) (Aho and Ullman, 1969). SCFGs are defined over a finite set of nonte"
W13-0803,W12-3134,0,0.35451,"2005), with WMT-07 development and test data. We used a maximum sentence length of 40 for filtering. We employ either 200K or (approximately) 1000K sentence pairs for training, 1K for development and 2K for testing (single reference per source sentence). Both the baseline and our method decode with a 3-gram language model smoothed with modified Knesser-Ney discounting (Chen and Goodman, 1998), trained on the target side of the full original training set (approximately 1000K sentences). We compare against state-of-the-art hierarchical translation (Chiang, 2005) baselines, based on the Joshua (Ganitkevitch et al., 2012) and Moses (Hoang et al., 2007) translation systems with default decoding settings. We use our own grammar extrac23 Figure 3: Example of a labeled Hiero rule X Complex → hwe owe X Atomic 1 to X Monotone 2 , X Atomic 1 sind wir X Monotone 2 schuldig i extracted from the Complex example in Figure 2 by replacing the phrase pairs hthis, dasi and hour citizens , unsern burgerni with (labeled) variables. tor for the generation of all grammars, including the baseline Hiero grammars. This enables us to use the same features (as far as applicable given the grammar formalism) and assure true comparabili"
W13-0803,P07-1037,1,0.899816,"Missing"
W13-0803,P07-2045,0,0.0736618,"st data. We used a maximum sentence length of 40 for filtering. We employ either 200K or (approximately) 1000K sentence pairs for training, 1K for development and 2K for testing (single reference per source sentence). Both the baseline and our method decode with a 3-gram language model smoothed with modified Knesser-Ney discounting (Chen and Goodman, 1998), trained on the target side of the full original training set (approximately 1000K sentences). We compare against state-of-the-art hierarchical translation (Chiang, 2005) baselines, based on the Joshua (Ganitkevitch et al., 2012) and Moses (Hoang et al., 2007) translation systems with default decoding settings. We use our own grammar extrac23 Figure 3: Example of a labeled Hiero rule X Complex → hwe owe X Atomic 1 to X Monotone 2 , X Atomic 1 sind wir X Monotone 2 schuldig i extracted from the Complex example in Figure 2 by replacing the phrase pairs hthis, dasi and hour citizens , unsern burgerni with (labeled) variables. tor for the generation of all grammars, including the baseline Hiero grammars. This enables us to use the same features (as far as applicable given the grammar formalism) and assure true comparability of the grammars under compar"
W13-0803,D11-1125,0,0.0123681,"te drop in performance in our Analysis Experiments. Decoding Type Lattice MBR Viterbi System Name Hiero Hiero-RL Hiero Hiero-RL-PPL 5.1.2 200K 26.44 26.72 26.23 26.16 Table 1: Initial Results. Lowercase BLEU results for German-English trained on 200K sentence pairs.4 Top rows display results for our experiments using Moses (Hoang et al., 2007) with Lattice Minimum Bayes-Risk Decoding5 (Tromble et al., 2008) in combination with Batch Mira (Cherry and Foster, 2012) for tuning. Below are results for experiments with Joshua (Ganitkevitch et al., 2012) using Viterbi decoding (i.e. no MBR) and PRO (Hopkins and May, 2011) for tuning. were done on Joshua (Ganitkevitch et al., 2012), using the Viterbi best derivation. The second set of experiments was done on Moses (Hoang et al., 2007) using Lattice Minimum Bayes-Risk Decoding5 (Tromble et al., 2008) to sum over derivations. 5.1.1 General Settings Based on experiments reported in (Mylonakis and Sima’an, 2011; Mylonakis, 2012) we opted to not label the (fully lexicalized) phrase pairs, but instead label them with a generic PhrasePair label and use a set of switch rules from all other labels to the PhrasePair label to enable transition between Hiero rules and phra"
W13-0803,N04-1022,0,0.0229145,"synchronous productions from G, see (Chiang, 2006). Standardly, for complexity reasons, most models used make the assumption that the probability P(t |s) can be optimized through as single best derivation as follows: arg max P(t |s) = arg max t t X d∈G ≈ arg max P(t, d |s) d∈G 20 (2) This approximation can be notoriously problematic for labelled Hiero models because the labels tend to lead to many more derivations than in the original model, thereby aggravating the effects of this assumption. This problem is relevant for our work and approaches to deal with it are Minimum BayesRisk decoding (Kumar and Byrne, 2004; Tromble et al., 2008), Variational Decoding (Li et al., 2009) and soft labeling (Venugopal et al., 2009; Marton et al., 2012; Chiang, 2010). Given a derivation d, most existing phrasebased models approximate the derivation probability through a linear interpolation of a finite set of feature functions (Φ(d)) of the derivation d, mostly working with local feature functions φi of individual productions, the target side yield string t of d (target language model features) and other heuristic features discussed in the experimental section: Hierarchical SMT models Hierarchical SMT usually works w"
W13-0803,P09-1067,0,0.11512,"complexity reasons, most models used make the assumption that the probability P(t |s) can be optimized through as single best derivation as follows: arg max P(t |s) = arg max t t X d∈G ≈ arg max P(t, d |s) d∈G 20 (2) This approximation can be notoriously problematic for labelled Hiero models because the labels tend to lead to many more derivations than in the original model, thereby aggravating the effects of this assumption. This problem is relevant for our work and approaches to deal with it are Minimum BayesRisk decoding (Kumar and Byrne, 2004; Tromble et al., 2008), Variational Decoding (Li et al., 2009) and soft labeling (Venugopal et al., 2009; Marton et al., 2012; Chiang, 2010). Given a derivation d, most existing phrasebased models approximate the derivation probability through a linear interpolation of a finite set of feature functions (Φ(d)) of the derivation d, mostly working with local feature functions φi of individual productions, the target side yield string t of d (target language model features) and other heuristic features discussed in the experimental section: Hierarchical SMT models Hierarchical SMT usually works with weighted instantiations of Synchronous Context-Free Grammar"
W13-0803,W12-3128,0,0.28668,"Missing"
W13-0803,P11-1065,1,0.845769,"Missing"
W13-0803,P03-1021,0,0.114022,"side, with a bijective (1-to-1 and onto) function between the source and target nonterminals. Like the standard Hiero model (Chiang, 2007), we constrain our work to SCFGs which involve at most two nonterminals in every lexicalized production. Given an SCFG G, a source sentence s is translated into a target sentence t by synchronous derivations d, each is a finite sequence of well-formed P(t, d |s) (1) arg max P(t, d |s) ≈ arg max d∈G d∈G |Φ(d)| X λi × φi (3) i=1 Where λi is the weight of feature φi optimized over a held-out parallel corpus by some direct errorminimization procedure like MERT (Och, 2003). 3 Baseline: Hiero Grammars (single label) Hiero Grammars (Chiang, 2005; Chiang, 2007) are a particular form of SCFGs that generalize phrasebased translation models to hierarchical phrasebased Translation models. They allow only up to two (pairs of) nonterminals on the right-hand-side of rules. Hierarchical rules are formed from fully lexicalized base rules (i.e. phrase pairs) by replacing a sub-span of the phrase pair that corresponds itself to a valid phrase pair with variable X called “gap”. Two gaps may be maximally introduced in this way1 , labeled as X 1 and X 2 respectively for distinc"
W13-0803,D08-1065,0,0.102857,"s from G, see (Chiang, 2006). Standardly, for complexity reasons, most models used make the assumption that the probability P(t |s) can be optimized through as single best derivation as follows: arg max P(t |s) = arg max t t X d∈G ≈ arg max P(t, d |s) d∈G 20 (2) This approximation can be notoriously problematic for labelled Hiero models because the labels tend to lead to many more derivations than in the original model, thereby aggravating the effects of this assumption. This problem is relevant for our work and approaches to deal with it are Minimum BayesRisk decoding (Kumar and Byrne, 2004; Tromble et al., 2008), Variational Decoding (Li et al., 2009) and soft labeling (Venugopal et al., 2009; Marton et al., 2012; Chiang, 2010). Given a derivation d, most existing phrasebased models approximate the derivation probability through a linear interpolation of a finite set of feature functions (Φ(d)) of the derivation d, mostly working with local feature functions φi of individual productions, the target side yield string t of d (target language model features) and other heuristic features discussed in the experimental section: Hierarchical SMT models Hierarchical SMT usually works with weighted instantiat"
W13-0803,N09-1027,0,0.113574,"d make the assumption that the probability P(t |s) can be optimized through as single best derivation as follows: arg max P(t |s) = arg max t t X d∈G ≈ arg max P(t, d |s) d∈G 20 (2) This approximation can be notoriously problematic for labelled Hiero models because the labels tend to lead to many more derivations than in the original model, thereby aggravating the effects of this assumption. This problem is relevant for our work and approaches to deal with it are Minimum BayesRisk decoding (Kumar and Byrne, 2004; Tromble et al., 2008), Variational Decoding (Li et al., 2009) and soft labeling (Venugopal et al., 2009; Marton et al., 2012; Chiang, 2010). Given a derivation d, most existing phrasebased models approximate the derivation probability through a linear interpolation of a finite set of feature functions (Φ(d)) of the derivation d, mostly working with local feature functions φi of individual productions, the target side yield string t of d (target language model features) and other heuristic features discussed in the experimental section: Hierarchical SMT models Hierarchical SMT usually works with weighted instantiations of Synchronous Context-Free Grammars (SCFGs) (Aho and Ullman, 1969). SCFGs ar"
W13-0803,J97-3002,0,0.311541,"tree representations. We identify five clusters of alignment patterns in which the children of a node in a decomposition tree are found and employ these five as nonterminal labels for the Hiero productions. Although this is our first non-optimized instantiation of the idea, our experiments show competitive performance with the Hiero baseline, exemplifying certain merits of this novel approach. 1 Introduction The Hiero model (Chiang, 2007; Chiang, 2005) formulates phrase-based translation in terms of a synchronous context-free grammar (SCFG) limited to the inversion transduction grammar (ITG) (Wu, 1997) family. While the original Hiero approach works with a single nonterminal label (X) (besides the start nonterminal S ), more recent work is dedicated to devising methods for extracting more elaborate labels for the phrase-pairs and their abstractions into SCFG productions, e.g., (Zollmann and Venugopal, 2006; Li et al., 2012; Almaghout et al., 2011). All labeling approaches exploit monolingual parsers of some kind, e.g., syntactic, semanKhalil Sima’an Institute for Logic, Language and Computation University of Amsterdam Science Park 904, 1098 XH Amsterdam The Netherlands k.simaan AT uva.nl ti"
W13-0803,C08-1136,0,0.621906,"emantics and Structure in Statistical Translation, pages 19–28, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics • The labels come from the word alignments only, • The labels are coarse-grained, pre-defined clusters and not optimized for performance, • The labels extend the binary set of ITG operators (monotone and inverted) into five such labels in order to cover non-binarizable alignment patterns. Our labels are based on our own tree decompositions of word alignments (Sima’an and Maillette de Buy Wenniger, 2011), akin to Normalized Decomposition Trees (NDTs) (Zhang et al., 2008). In this first attempt we explore a set of five nonterminal labels that characterize alignment patterns found directly under nodes in the NDT projected for every word alignment in the parallel corpus during training. There is a range of work that exploits the monotone and inverted orientations of binary ITG within hierarchical phrase-based models, either as feature functions of lexicalized Hiero productions (Chiang, 2007; Zollmann and Venugopal, 2006), or as labels on non-lexicalized ITG productions, e.g., (Mylonakis and Sima’an, 2011). As far as we are aware, this is the first attempt at exp"
W13-0803,W06-3119,0,0.7617,"show competitive performance with the Hiero baseline, exemplifying certain merits of this novel approach. 1 Introduction The Hiero model (Chiang, 2007; Chiang, 2005) formulates phrase-based translation in terms of a synchronous context-free grammar (SCFG) limited to the inversion transduction grammar (ITG) (Wu, 1997) family. While the original Hiero approach works with a single nonterminal label (X) (besides the start nonterminal S ), more recent work is dedicated to devising methods for extracting more elaborate labels for the phrase-pairs and their abstractions into SCFG productions, e.g., (Zollmann and Venugopal, 2006; Li et al., 2012; Almaghout et al., 2011). All labeling approaches exploit monolingual parsers of some kind, e.g., syntactic, semanKhalil Sima’an Institute for Logic, Language and Computation University of Amsterdam Science Park 904, 1098 XH Amsterdam The Netherlands k.simaan AT uva.nl tic or sense-oriented. The rationale behind monolingual labeling is often to make the probability distributions over alternative synchronous derivations of the Hiero model more sensitive to linguistically justified monolingual phrase context. For example, syntactic target-language labels in many approaches are"
W13-0807,P06-1002,0,0.0276662,"ng/excluding complex alignment phenomena known formally to be beyond (NF-)ITG. None of these earlier efforts discussed explicitly the dilemmas of instantiating a grammar formalism or how to formally parse word alignments. The work in (Zens and Ney, 2003; Søgaard and Wu, 2009), defining and counting TEUs, provides a far tighter upperbound than (Wellington et al., 2006), who use the disjunctive interpretation of word alignments, interpreting multiple alignment links of the same word as alternatives. We adopt the conjunctive interpretation of word alignments like a majority of work in MT, e.g., (Ayan and Dorr, 2006; Fox, 2002; Søgaard and Wu, 2009; Søgaard, 2010). In deviation from earlier work, the work in (Søgaard and Kuhn, 2009; Søgaard and Wu, 2009; Søgaard, 2010) discusses TEUs defined over word alignments explicitly, and defines evaluation metrics based on TEUs. In particular, Sogaard (Søgaard, 2010) writes that he employs ""a more aggressive search"" for TEUs than earlier work, thereby leading 65 Conclusions In this paper we provide a formal characterization for the problem of determining the coverage of a word alignment by a given grammar formalism as the intersection of two partially ordered sets"
W13-0807,P09-1088,0,0.017989,"t1 perceived in the word alignment. And finally, intersect the sets of nodes in the two sets of synchronous trees to check whether the grammar can generate (parts of) the word alignment. The formal detail of each of these three steps is provided in sections 3 to 5. We think that alignment parsing is relevant for current research because it highlights the difference between alignments in training data and alignments accepted by a synchronous grammar (learned from data). This is useful for literature on learning from word aligned parallel corpora (e.g., (Zens and Ney, 2003; DeNero et al., 2006; Blunsom et al., 2009; Cohn and Blunsom, 2009; Riesa and Marcu, 2010; Mylonakis and Sima’an, 2011; Haghighi et al., 2009; McCarley et al., 2011)). A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well. We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis. Our study highlights some of the technical choices left implicit in preceding work as explained in the next section. 2 First application to the ITG hypothesis A grammar formalism is a whole set/family of synchronous grammars. Fo"
W13-0807,D07-1007,0,0.0204789,"On the one hand, this is because of computational complexity reasons. On the other, this choice relies on existing empirical evidence of what we will call the “ITG hypothesis"", freely rephrased as follows: the ITG formalism is sufficient for representing a major percentage of reorderings in translation data in general. Although checking whether a word alignment can be generated by ITG is far simpler than for arbitrary synchronous grammars, there is a striking variation in the approaches taken in the existing literature, e.g., (Zens and Ney, 2003; Wellington et al., 2006; Søgaard and Wu, 2009; Carpuat and Wu, 2007; Søgaard and Kuhn, 2009; Søgaard, 2010). Søgaard and Wu (Søgaard and Wu, 2009) observe justifiably that the literature studying the ITG alignment coverage makes conflicting choices in method and data, and reports significantly diverging alignment coverage scores. We hypothesize here that the major conflicting choices in method (what to count and how to parse) are likely due to the absence of a well-understood, formalized method for parsing word alignments even under ITG. In this paper we apply our formal approach to the ITG case, contributing new empirical evidence concerning the ITG hypothes"
W13-0807,P05-1033,0,0.0275535,"d Eisner, 2009). Our results also exhibit the importance of explicitly defining the units of translation equivalence when studying (ITG) coverage of word alignments. The more complex the choice of translation equivalence relations, the more difficult it is to parse the word alignments. 3 Translation equivalence in MT In (Koehn et al., 2003), a translation equivalence unit (TEU) is a phrase pair: a pair of contiguous substrings of the source and target sentences such that the words on the one side align only with words on the other side (formal definitions next). The hierarchical phrase pairs (Chiang, 2005; Chiang, 2007) are extracted by replacing one or more sub-phrase pairs, that are contained within a phrase pair, by pairs of linked variables. This defines a subsumption relation between hierarchical phrase pairs (Zhang et al., 2008). Actual systems, e.g., (Koehn et al., 2003; Chiang, 2007) set an upperbound on length or the number of variables in the synchronous productions. For the purposes of our theoretical study, these practical limitations are irrelevant. We give two definitions of translation equivalence for word alignments.2 The first one makes no assumptions about the contiguity of T"
W13-0807,J07-2003,0,0.513922,"er defined by the word alignment. As a first sanity check, we report extensive coverage results for ITG on automatic and manual alignments. Even for the ITG formalism, our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work. 1 hX → α X (1) β X (2) γ X (3) , X → σ X (2) τ X (1) µ X (3) i Introduction The training data used by current statistical machine translation (SMT) models consists of source and target sentence pairs aligned together at the word level (word alignments). For the hierarchical and syntactically-enriched SMT models, e.g., (Chiang, 2007; Zollmann and Venugopal, 2006), this training data is used for extracting statistically weighted Synchronous Context-Free Grammars (SCFGs). Formally speaking, a synchronous grammar defines a set of (source-target) sentence pairs derived synchronously by the grammar. Contrary to common ∗ Khalil Sima’an∗ University of Amsterdam k.simaan@uva.nl Institute for Logic, Language and Computation. where superscript (i) stands for aligned instances of nonterminal X and all Greek symbols stand for arbitrary non-empty terminals sequences. Given a word aligned sentence pair it is necessary to bind the term"
W13-0807,D09-1037,0,0.0163185,"rd alignment. And finally, intersect the sets of nodes in the two sets of synchronous trees to check whether the grammar can generate (parts of) the word alignment. The formal detail of each of these three steps is provided in sections 3 to 5. We think that alignment parsing is relevant for current research because it highlights the difference between alignments in training data and alignments accepted by a synchronous grammar (learned from data). This is useful for literature on learning from word aligned parallel corpora (e.g., (Zens and Ney, 2003; DeNero et al., 2006; Blunsom et al., 2009; Cohn and Blunsom, 2009; Riesa and Marcu, 2010; Mylonakis and Sima’an, 2011; Haghighi et al., 2009; McCarley et al., 2011)). A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well. We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis. Our study highlights some of the technical choices left implicit in preceding work as explained in the next section. 2 First application to the ITG hypothesis A grammar formalism is a whole set/family of synchronous grammars. For example, ITG (Wu, 1997"
W13-0807,W06-3105,0,0.0177662,"relations of interest1 perceived in the word alignment. And finally, intersect the sets of nodes in the two sets of synchronous trees to check whether the grammar can generate (parts of) the word alignment. The formal detail of each of these three steps is provided in sections 3 to 5. We think that alignment parsing is relevant for current research because it highlights the difference between alignments in training data and alignments accepted by a synchronous grammar (learned from data). This is useful for literature on learning from word aligned parallel corpora (e.g., (Zens and Ney, 2003; DeNero et al., 2006; Blunsom et al., 2009; Cohn and Blunsom, 2009; Riesa and Marcu, 2010; Mylonakis and Sima’an, 2011; Haghighi et al., 2009; McCarley et al., 2011)). A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well. We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis. Our study highlights some of the technical choices left implicit in preceding work as explained in the next section. 2 First application to the ITG hypothesis A grammar formalism is a whole set/family of sy"
W13-0807,W02-1039,0,0.0246277,"alignment phenomena known formally to be beyond (NF-)ITG. None of these earlier efforts discussed explicitly the dilemmas of instantiating a grammar formalism or how to formally parse word alignments. The work in (Zens and Ney, 2003; Søgaard and Wu, 2009), defining and counting TEUs, provides a far tighter upperbound than (Wellington et al., 2006), who use the disjunctive interpretation of word alignments, interpreting multiple alignment links of the same word as alternatives. We adopt the conjunctive interpretation of word alignments like a majority of work in MT, e.g., (Ayan and Dorr, 2006; Fox, 2002; Søgaard and Wu, 2009; Søgaard, 2010). In deviation from earlier work, the work in (Søgaard and Kuhn, 2009; Søgaard and Wu, 2009; Søgaard, 2010) discusses TEUs defined over word alignments explicitly, and defines evaluation metrics based on TEUs. In particular, Sogaard (Søgaard, 2010) writes that he employs ""a more aggressive search"" for TEUs than earlier work, thereby leading 65 Conclusions In this paper we provide a formal characterization for the problem of determining the coverage of a word alignment by a given grammar formalism as the intersection of two partially ordered sets. These par"
W13-0807,graca-etal-2008-building,0,0.0603092,"Missing"
W13-0807,P09-1104,0,0.022055,"nchronous trees to check whether the grammar can generate (parts of) the word alignment. The formal detail of each of these three steps is provided in sections 3 to 5. We think that alignment parsing is relevant for current research because it highlights the difference between alignments in training data and alignments accepted by a synchronous grammar (learned from data). This is useful for literature on learning from word aligned parallel corpora (e.g., (Zens and Ney, 2003; DeNero et al., 2006; Blunsom et al., 2009; Cohn and Blunsom, 2009; Riesa and Marcu, 2010; Mylonakis and Sima’an, 2011; Haghighi et al., 2009; McCarley et al., 2011)). A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well. We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis. Our study highlights some of the technical choices left implicit in preceding work as explained in the next section. 2 First application to the ITG hypothesis A grammar formalism is a whole set/family of synchronous grammars. For example, ITG (Wu, 1997) defines a family of inversion-transduction grammars differing among them"
W13-0807,J09-4009,0,0.218926,"erage scores. We hypothesize here that the major conflicting choices in method (what to count and how to parse) are likely due to the absence of a well-understood, formalized method for parsing word alignments even under ITG. In this paper we apply our formal approach to the ITG case, contributing new empirical evidence concerning the ITG hypothesis. For our empirical study we exemplify our approach by detailing an algorithm dedicated to ITG in Normal-Form (NF-ITG). While our algorithm is in essence equivalent to existing algorithms for checking binarizability of permutations, e.g.,(Wu, 1997; Huang et al., 2009), the formal foundations preceding it concern nailing down the choices made in parsing arbitrary word alignments, as opposed to (bijective) permutations. The formalization is our way to resolve some of the major points of differences in existing literature. We report new coverage results for ITG parsing of manual as well as automatic alignments, showing the contrast between the two kinds. While the latter seems built for phrase extraction, trading-off precision for recall, the former is heavily marked with idiomatic expressions. Our coverage results make explicit a relevant dilemma. To hierarc"
W13-0807,N03-1017,0,0.0320579,"synchronous reordering mechanisms than ITG, with increased risk of exponential parsing algorithms (Wu, 1997; Satta and Peserico, 2005). But if we abandon these word alignments, we will face the exponential problem of learning reordering arbitrary permutations, cf. (Tromble and Eisner, 2009). Our results also exhibit the importance of explicitly defining the units of translation equivalence when studying (ITG) coverage of word alignments. The more complex the choice of translation equivalence relations, the more difficult it is to parse the word alignments. 3 Translation equivalence in MT In (Koehn et al., 2003), a translation equivalence unit (TEU) is a phrase pair: a pair of contiguous substrings of the source and target sentences such that the words on the one side align only with words on the other side (formal definitions next). The hierarchical phrase pairs (Chiang, 2005; Chiang, 2007) are extracted by replacing one or more sub-phrase pairs, that are contained within a phrase pair, by pairs of linked variables. This defines a subsumption relation between hierarchical phrase pairs (Zhang et al., 2008). Actual systems, e.g., (Koehn et al., 2003; Chiang, 2007) set an upperbound on length or the nu"
W13-0807,2005.mtsummit-papers.11,0,0.0055575,"ora come from two datasets. The first (Graca ˛ et al., 2008) consists of six language pairs: Portuguese– English, Portuguese–French, Portuguese–Spanish, English–Spanish, English–French and French– Spanish. These datasets contain 100 sentence pairs each and distinguish Sure and Possible alignments. Following (Søgaard and Kuhn, 2009), we treat these two equally. The second manually aligned dataset (Padó and Lapata, 2006) contains 987 sentence pairs from the English-German part of Europarl annotated using the Blinker guidelines (Melamed, 1998). The automatically aligned data comes from Europarl (Koehn, 2005) in three language pairs (English– Dutch, English–French and English–German). The corpora are automatically aligned using GIZA++ (Och and Ney, 2003) in combination with the growdiag-final-and heuristic. With sentence length cutoff 40 on both sides these contain respectively 945k, 949k and 995k sentence pairs. Grammatical Coverage (GC) is defined as the percentage word alignments (sentence pairs) in a parallel corpus that can be covered by an instance of the grammar (NF-ITG) (cf. Section 5). Clearly, GC depends on the chosen semantic interpretation of word alignments: contiguous TE’s (phrase pa"
W13-0807,C88-1075,0,0.523697,"stent with the given word alignment, and then parse the word alignment with the thus enriched grammar rules. This is not complex if we assume that each of the source terminal sequences is contiguously aligned with a target contiguous sequence, but difficult if we assume arbitrary alignments, including many-to-one and non-contiguously aligned chunks. One important goal of this paper is to propose a formal characterization of what it means to synchronously parse a word alignment. Our formal characterization is borrowed from the “parsing as intersection"" paradigm, e.g., (Bar-Hillel et al., 1964; Lang, 1988; van Noord, 1995; Nederhof and Satta, 58 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 58–67, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics 2004). Conceptually, our characterization makes use of three algorithms. Firstly, parse the unaligned sentence pair with the synchronous grammar to obtain a set of synchronous derivations, i.e., trees. Secondly, interpret a word alignment as generating a set of synchronous trees representing the recursive translation equivalence relations of interest1 perceived in th"
W13-0807,D11-1082,0,0.0263679,"k whether the grammar can generate (parts of) the word alignment. The formal detail of each of these three steps is provided in sections 3 to 5. We think that alignment parsing is relevant for current research because it highlights the difference between alignments in training data and alignments accepted by a synchronous grammar (learned from data). This is useful for literature on learning from word aligned parallel corpora (e.g., (Zens and Ney, 2003; DeNero et al., 2006; Blunsom et al., 2009; Cohn and Blunsom, 2009; Riesa and Marcu, 2010; Mylonakis and Sima’an, 2011; Haghighi et al., 2009; McCarley et al., 2011)). A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well. We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis. Our study highlights some of the technical choices left implicit in preceding work as explained in the next section. 2 First application to the ITG hypothesis A grammar formalism is a whole set/family of synchronous grammars. For example, ITG (Wu, 1997) defines a family of inversion-transduction grammars differing among them in the exact set of sync"
W13-0807,P11-1065,1,0.900086,"Missing"
W13-0807,J03-1002,0,0.0235061,"e–Spanish, English–Spanish, English–French and French– Spanish. These datasets contain 100 sentence pairs each and distinguish Sure and Possible alignments. Following (Søgaard and Kuhn, 2009), we treat these two equally. The second manually aligned dataset (Padó and Lapata, 2006) contains 987 sentence pairs from the English-German part of Europarl annotated using the Blinker guidelines (Melamed, 1998). The automatically aligned data comes from Europarl (Koehn, 2005) in three language pairs (English– Dutch, English–French and English–German). The corpora are automatically aligned using GIZA++ (Och and Ney, 2003) in combination with the growdiag-final-and heuristic. With sentence length cutoff 40 on both sides these contain respectively 945k, 949k and 995k sentence pairs. Grammatical Coverage (GC) is defined as the percentage word alignments (sentence pairs) in a parallel corpus that can be covered by an instance of the grammar (NF-ITG) (cf. Section 5). Clearly, GC depends on the chosen semantic interpretation of word alignments: contiguous TE’s (phrase pairs) or discontiguous TE’s. Alignments Set GC contiguous TEs Hand aligned corpora English–French 76.0 English–Portuguese 78.0 English–Spanish 83.0 P"
W13-0807,P06-1146,0,0.026277,"nce(chart[i][ j], a0 ) end Algorithm 3: Algorithm that adds a TEU and associated Inference to the chart 7 Experiments Data Sets We use manually and automatically aligned corpora. Manually aligned corpora come from two datasets. The first (Graca ˛ et al., 2008) consists of six language pairs: Portuguese– English, Portuguese–French, Portuguese–Spanish, English–Spanish, English–French and French– Spanish. These datasets contain 100 sentence pairs each and distinguish Sure and Possible alignments. Following (Søgaard and Kuhn, 2009), we treat these two equally. The second manually aligned dataset (Padó and Lapata, 2006) contains 987 sentence pairs from the English-German part of Europarl annotated using the Blinker guidelines (Melamed, 1998). The automatically aligned data comes from Europarl (Koehn, 2005) in three language pairs (English– Dutch, English–French and English–German). The corpora are automatically aligned using GIZA++ (Och and Ney, 2003) in combination with the growdiag-final-and heuristic. With sentence length cutoff 40 on both sides these contain respectively 945k, 949k and 995k sentence pairs. Grammatical Coverage (GC) is defined as the percentage word alignments (sentence pairs) in a parall"
W13-0807,P10-1017,0,0.0209559,"y, intersect the sets of nodes in the two sets of synchronous trees to check whether the grammar can generate (parts of) the word alignment. The formal detail of each of these three steps is provided in sections 3 to 5. We think that alignment parsing is relevant for current research because it highlights the difference between alignments in training data and alignments accepted by a synchronous grammar (learned from data). This is useful for literature on learning from word aligned parallel corpora (e.g., (Zens and Ney, 2003; DeNero et al., 2006; Blunsom et al., 2009; Cohn and Blunsom, 2009; Riesa and Marcu, 2010; Mylonakis and Sima’an, 2011; Haghighi et al., 2009; McCarley et al., 2011)). A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well. We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis. Our study highlights some of the technical choices left implicit in preceding work as explained in the next section. 2 First application to the ITG hypothesis A grammar formalism is a whole set/family of synchronous grammars. For example, ITG (Wu, 1997) defines a family of i"
W13-0807,H05-1101,0,0.0298731,"me of the major points of differences in existing literature. We report new coverage results for ITG parsing of manual as well as automatic alignments, showing the contrast between the two kinds. While the latter seems built for phrase extraction, trading-off precision for recall, the former is heavily marked with idiomatic expressions. Our coverage results make explicit a relevant dilemma. To hierarchically parse the current automatic word alignments exactly, we will need more general synchronous reordering mechanisms than ITG, with increased risk of exponential parsing algorithms (Wu, 1997; Satta and Peserico, 2005). But if we abandon these word alignments, we will face the exponential problem of learning reordering arbitrary permutations, cf. (Tromble and Eisner, 2009). Our results also exhibit the importance of explicitly defining the units of translation equivalence when studying (ITG) coverage of word alignments. The more complex the choice of translation equivalence relations, the more difficult it is to parse the word alignments. 3 Translation equivalence in MT In (Koehn et al., 2003), a translation equivalence unit (TEU) is a phrase pair: a pair of contiguous substrings of the source and target se"
W13-0807,W09-2303,0,0.303388,"is because of computational complexity reasons. On the other, this choice relies on existing empirical evidence of what we will call the “ITG hypothesis"", freely rephrased as follows: the ITG formalism is sufficient for representing a major percentage of reorderings in translation data in general. Although checking whether a word alignment can be generated by ITG is far simpler than for arbitrary synchronous grammars, there is a striking variation in the approaches taken in the existing literature, e.g., (Zens and Ney, 2003; Wellington et al., 2006; Søgaard and Wu, 2009; Carpuat and Wu, 2007; Søgaard and Kuhn, 2009; Søgaard, 2010). Søgaard and Wu (Søgaard and Wu, 2009) observe justifiably that the literature studying the ITG alignment coverage makes conflicting choices in method and data, and reports significantly diverging alignment coverage scores. We hypothesize here that the major conflicting choices in method (what to count and how to parse) are likely due to the absence of a well-understood, formalized method for parsing word alignments even under ITG. In this paper we apply our formal approach to the ITG case, contributing new empirical evidence concerning the ITG hypothesis. For our empirical st"
W13-0807,W09-3805,0,0.20389,"formalism (Wu, 1997). On the one hand, this is because of computational complexity reasons. On the other, this choice relies on existing empirical evidence of what we will call the “ITG hypothesis"", freely rephrased as follows: the ITG formalism is sufficient for representing a major percentage of reorderings in translation data in general. Although checking whether a word alignment can be generated by ITG is far simpler than for arbitrary synchronous grammars, there is a striking variation in the approaches taken in the existing literature, e.g., (Zens and Ney, 2003; Wellington et al., 2006; Søgaard and Wu, 2009; Carpuat and Wu, 2007; Søgaard and Kuhn, 2009; Søgaard, 2010). Søgaard and Wu (Søgaard and Wu, 2009) observe justifiably that the literature studying the ITG alignment coverage makes conflicting choices in method and data, and reports significantly diverging alignment coverage scores. We hypothesize here that the major conflicting choices in method (what to count and how to parse) are likely due to the absence of a well-understood, formalized method for parsing word alignments even under ITG. In this paper we apply our formal approach to the ITG case, contributing new empirical evidence conce"
W13-0807,2010.eamt-1.5,0,0.317738,"nal complexity reasons. On the other, this choice relies on existing empirical evidence of what we will call the “ITG hypothesis"", freely rephrased as follows: the ITG formalism is sufficient for representing a major percentage of reorderings in translation data in general. Although checking whether a word alignment can be generated by ITG is far simpler than for arbitrary synchronous grammars, there is a striking variation in the approaches taken in the existing literature, e.g., (Zens and Ney, 2003; Wellington et al., 2006; Søgaard and Wu, 2009; Carpuat and Wu, 2007; Søgaard and Kuhn, 2009; Søgaard, 2010). Søgaard and Wu (Søgaard and Wu, 2009) observe justifiably that the literature studying the ITG alignment coverage makes conflicting choices in method and data, and reports significantly diverging alignment coverage scores. We hypothesize here that the major conflicting choices in method (what to count and how to parse) are likely due to the absence of a well-understood, formalized method for parsing word alignments even under ITG. In this paper we apply our formal approach to the ITG case, contributing new empirical evidence concerning the ITG hypothesis. For our empirical study we exemplify"
W13-0807,D09-1105,0,0.0134363,"ng the contrast between the two kinds. While the latter seems built for phrase extraction, trading-off precision for recall, the former is heavily marked with idiomatic expressions. Our coverage results make explicit a relevant dilemma. To hierarchically parse the current automatic word alignments exactly, we will need more general synchronous reordering mechanisms than ITG, with increased risk of exponential parsing algorithms (Wu, 1997; Satta and Peserico, 2005). But if we abandon these word alignments, we will face the exponential problem of learning reordering arbitrary permutations, cf. (Tromble and Eisner, 2009). Our results also exhibit the importance of explicitly defining the units of translation equivalence when studying (ITG) coverage of word alignments. The more complex the choice of translation equivalence relations, the more difficult it is to parse the word alignments. 3 Translation equivalence in MT In (Koehn et al., 2003), a translation equivalence unit (TEU) is a phrase pair: a pair of contiguous substrings of the source and target sentences such that the words on the one side align only with words on the other side (formal definitions next). The hierarchical phrase pairs (Chiang, 2005; C"
W13-0807,P95-1022,0,0.315837,"Missing"
W13-0807,P06-1123,0,0.135221,"the confines of the ITG formalism (Wu, 1997). On the one hand, this is because of computational complexity reasons. On the other, this choice relies on existing empirical evidence of what we will call the “ITG hypothesis"", freely rephrased as follows: the ITG formalism is sufficient for representing a major percentage of reorderings in translation data in general. Although checking whether a word alignment can be generated by ITG is far simpler than for arbitrary synchronous grammars, there is a striking variation in the approaches taken in the existing literature, e.g., (Zens and Ney, 2003; Wellington et al., 2006; Søgaard and Wu, 2009; Carpuat and Wu, 2007; Søgaard and Kuhn, 2009; Søgaard, 2010). Søgaard and Wu (Søgaard and Wu, 2009) observe justifiably that the literature studying the ITG alignment coverage makes conflicting choices in method and data, and reports significantly diverging alignment coverage scores. We hypothesize here that the major conflicting choices in method (what to count and how to parse) are likely due to the absence of a well-understood, formalized method for parsing word alignments even under ITG. In this paper we apply our formal approach to the ITG case, contributing new em"
W13-0807,J97-3002,0,0.884707,"om, 2009; Riesa and Marcu, 2010; Mylonakis and Sima’an, 2011; Haghighi et al., 2009; McCarley et al., 2011)). A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well. We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis. Our study highlights some of the technical choices left implicit in preceding work as explained in the next section. 2 First application to the ITG hypothesis A grammar formalism is a whole set/family of synchronous grammars. For example, ITG (Wu, 1997) defines a family of inversion-transduction grammars differing among them in the exact set of synchronous productions, terminals and non-terminals. Given a synchronous grammar formalism and an input word alignment, a relevant theoretical question is whether there exists an instance synchronous grammar that generates the word alignment exactly. We will refer to this question as the alignment coverage problem. In this paper we propose an approach to the alignment coverage problem using the threestep solution proposed above for parsing word align1 The translation equivalence relations of interest"
W13-0807,P03-1019,0,0.516195,"nslation equivalence relations of interest1 perceived in the word alignment. And finally, intersect the sets of nodes in the two sets of synchronous trees to check whether the grammar can generate (parts of) the word alignment. The formal detail of each of these three steps is provided in sections 3 to 5. We think that alignment parsing is relevant for current research because it highlights the difference between alignments in training data and alignments accepted by a synchronous grammar (learned from data). This is useful for literature on learning from word aligned parallel corpora (e.g., (Zens and Ney, 2003; DeNero et al., 2006; Blunsom et al., 2009; Cohn and Blunsom, 2009; Riesa and Marcu, 2010; Mylonakis and Sima’an, 2011; Haghighi et al., 2009; McCarley et al., 2011)). A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well. We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis. Our study highlights some of the technical choices left implicit in preceding work as explained in the next section. 2 First application to the ITG hypothesis A grammar formalism is a w"
W13-0807,C08-1136,0,0.0941133,"ons, the more difficult it is to parse the word alignments. 3 Translation equivalence in MT In (Koehn et al., 2003), a translation equivalence unit (TEU) is a phrase pair: a pair of contiguous substrings of the source and target sentences such that the words on the one side align only with words on the other side (formal definitions next). The hierarchical phrase pairs (Chiang, 2005; Chiang, 2007) are extracted by replacing one or more sub-phrase pairs, that are contained within a phrase pair, by pairs of linked variables. This defines a subsumption relation between hierarchical phrase pairs (Zhang et al., 2008). Actual systems, e.g., (Koehn et al., 2003; Chiang, 2007) set an upperbound on length or the number of variables in the synchronous productions. For the purposes of our theoretical study, these practical limitations are irrelevant. We give two definitions of translation equivalence for word alignments.2 The first one makes no assumptions about the contiguity of TEUs, while the second does require them to be contiguous substrings on both sides (i.e., phrase pairs). As usual, s = s1 ...sm and t = t1 ...tn are source and target sentences respectively. Let sσ be the source word at position σ in s"
W13-0807,W06-3119,0,0.16438,"the word alignment. As a first sanity check, we report extensive coverage results for ITG on automatic and manual alignments. Even for the ITG formalism, our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work. 1 hX → α X (1) β X (2) γ X (3) , X → σ X (2) τ X (1) µ X (3) i Introduction The training data used by current statistical machine translation (SMT) models consists of source and target sentence pairs aligned together at the word level (word alignments). For the hierarchical and syntactically-enriched SMT models, e.g., (Chiang, 2007; Zollmann and Venugopal, 2006), this training data is used for extracting statistically weighted Synchronous Context-Free Grammars (SCFGs). Formally speaking, a synchronous grammar defines a set of (source-target) sentence pairs derived synchronously by the grammar. Contrary to common ∗ Khalil Sima’an∗ University of Amsterdam k.simaan@uva.nl Institute for Logic, Language and Computation. where superscript (i) stands for aligned instances of nonterminal X and all Greek symbols stand for arbitrary non-empty terminals sequences. Given a word aligned sentence pair it is necessary to bind the terminal sequence by alignments con"
W14-3354,W10-1749,0,0.0507195,"orkshop on Statistical Machine Translation, pages 414–419, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2.2 to each hypothesis individually which can be used for ranking (local ranking model) (Li, 2011). Local ranking models are preferable because they provide absolute distance between hypotheses like most existing evaluation metrics. In this paper we follow the learning-to-rank approach which produces a local ranking model in a similar way to PRO MT systems tuning (Hopkins and May, 2011). 2 To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) in representing reordering as a permutation and then measuring the distance to the ideal monotone permutation. Here we take one feature from previous work – Kendall τ distance from the monotone permutation. This metrics on the permutation level has been shown to have high correlation with human judgment on language pairs with very different word order. Additionally, we add novel features with an even less sparse view of word order by exploiting hierarchical structure that exists in permutations (Zhang and Gildea, 2007). The trees that represent this structure are called PETs (PErmutation Tree"
W14-3354,W09-0404,0,0.0362766,"Missing"
W14-3354,W12-3102,0,0.0203988,"Missing"
W14-3354,P02-1040,0,0.114431,"er similar objective – e.g., tuning for absolute adequacy and fluency scores instead on rankings, or 2. is assumed by MT tuning algorithms (Hopkins and May, 2011). 2. training for rankings directly but with metaheuristic approaches like hill-climbing, or 3. facilitates easier statistical testing using sign test or t-test (Collins et al., 2005) 3. training for pairwise rankings using learningto-rank techniques We think that the root cause for most of the difficulty in creating a good sentence level metric is the sparseness of the features often used. Consider the n-gram counting metrics (BLEU (Papineni et al., 2002)): counts of higher order n-grams are usually rather small, if not zero, when counted at the individual sentence level. Metrics based on such counts are brittle at the sentence level even when they might be good at the corpus level. Ideally we should have features of varying granularity that we can optimize on the actual evaluation task: relative ranking of system outputs. Therefore, in this paper we explore two kinds of less sparse features: Approach (1) has two disadvantages. One is the inconsistency between the training and the testing objectives. The other, is that absolute rankings are no"
W14-3354,P05-1066,0,0.0554382,"Missing"
W14-3354,W11-2113,0,0.019258,"earning algorithm produce the metric that has a very high correlation with human judgment. For future research we plan to investigate some more linguistically inspired features and also explore how this metric could be tuned for better tuning of statistical machine translation systems. From metrics that participated in all language pairs on the sentence level on average BEER has the best correlation with the human judgment. 6 Approach 3 Some methods, like ours, allow training of a large number of parameters for ranking. Global ranking models that directly rank hypotheses are used in ROSErank (Song and Cohn, 2011) and PAIR metric of (Pad´o et al., 2009). Our work is more similar to the training method for local ranking models that give score directly (as it is usually expected from an evaluation metric) which was originally proposed in (Ye et al., 2007) and later applied in (Duh, 2008) and (Yang et al., 2013). Related work The main contribution of our metric is a linear combination of features with far less sparse statistics than earlier work. In particular, we employ novel ordering features over PETs, a range of character n-gram features for adequancy, and direct tuning for human ranking. There are in"
W14-3354,W11-2107,0,0.0341948,"nslation. This differs from the majority of earlier work which we explain in Section 6. 4 #comparisons 85469 128668 67832 80741 151422 102842 77286 60464 100783 87323 Table 1: Number of human judgments in WMT13 language pair en-cs en-fr en-de en-es cs-en fr-en de-en es-en Experiments on WMT12 data We conducted experiments for the metric which in total has 33 features (27 for adequacy and 6 for word order). Some of the features in the metric depend on external sources of information. For function words we use listings that are created for many languages and are distributed with METEOR toolkit (Denkowski and Lavie, 2011). The permutations are extracted using METEOR aligner which does fuzzy matching using resources such as WordNet, paraphrase tables and stemmers. METEOR is not used for any scoring, but only for aligning hypothesis and reference. For training we used the data from WMT13 human evaluation of the systems (Mach´acˇ ek and Bojar, 2013). Before evaluation, all data was lowercased and tokenized. After preprocessing, we extract training examples for our binary classifier. The number of non-tied human judgments per language pair are shown in Table 1. Each human judgment produces two training instances :"
W14-3354,2010.amta-papers.3,0,0.0889039,"Missing"
W14-3354,W08-0331,0,0.0201865,"From metrics that participated in all language pairs on the sentence level on average BEER has the best correlation with the human judgment. 6 Approach 3 Some methods, like ours, allow training of a large number of parameters for ranking. Global ranking models that directly rank hypotheses are used in ROSErank (Song and Cohn, 2011) and PAIR metric of (Pad´o et al., 2009). Our work is more similar to the training method for local ranking models that give score directly (as it is usually expected from an evaluation metric) which was originally proposed in (Ye et al., 2007) and later applied in (Duh, 2008) and (Yang et al., 2013). Related work The main contribution of our metric is a linear combination of features with far less sparse statistics than earlier work. In particular, we employ novel ordering features over PETs, a range of character n-gram features for adequancy, and direct tuning for human ranking. There are in the literature three main approaches for tuning the machine translation metrics. Approach 1 SPEDE (Wang and Manning, 2012), metric of (Specia and Gim´enez, 2010), ROSE-reg (Song and Cohn, 2011), ABS metric of (Pad´o et al., 2009) and many others train their regression models"
W14-3354,D11-1125,0,0.218613,"n for optimal Kendall τ correlation with rankings by human evaluators. The models in the literature tackle this problem by Introduction The quality of sentence level (also called segment level) evaluation metrics in machine translation is often considered inferior to the quality of corpus (or system) level metrics. Yet, a sentence level metrics has important advantages as it: 1. provides an informative score to individual translations 1. training for another similar objective – e.g., tuning for absolute adequacy and fluency scores instead on rankings, or 2. is assumed by MT tuning algorithms (Hopkins and May, 2011). 2. training for rankings directly but with metaheuristic approaches like hill-climbing, or 3. facilitates easier statistical testing using sign test or t-test (Collins et al., 2005) 3. training for pairwise rankings using learningto-rank techniques We think that the root cause for most of the difficulty in creating a good sentence level metric is the sparseness of the features often used. Consider the n-gram counting metrics (BLEU (Papineni et al., 2002)): counts of higher order n-grams are usually rather small, if not zero, when counted at the individual sentence level. Metrics based on suc"
W14-3354,W12-3107,0,0.0547474,"Missing"
W14-3354,D10-1092,0,0.00621788,"eedings of the Ninth Workshop on Statistical Machine Translation, pages 414–419, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2.2 to each hypothesis individually which can be used for ranking (local ranking model) (Li, 2011). Local ranking models are preferable because they provide absolute distance between hypotheses like most existing evaluation metrics. In this paper we follow the learning-to-rank approach which produces a local ranking model in a similar way to PRO MT systems tuning (Hopkins and May, 2011). 2 To evaluate word order we follow (Isozaki et al., 2010; Birch and Osborne, 2010) in representing reordering as a permutation and then measuring the distance to the ideal monotone permutation. Here we take one feature from previous work – Kendall τ distance from the monotone permutation. This metrics on the permutation level has been shown to have high correlation with human judgment on language pairs with very different word order. Additionally, we add novel features with an even less sparse view of word order by exploiting hierarchical structure that exists in permutations (Zhang and Gildea, 2007). The trees that represent this structure are cal"
W14-3354,J97-3002,0,0.116231,"firstly, the PET operators show the minimal units of ordering that constitute the permutation itself, and secondly the higher level operators capture hidden patterns of ordering that cannot be observed without factorization. Statistics over patterns of ordering using PETs are non-lexical and hence far less sparse than word or character n-gram statistics. In PETs, the minimal operators on the node stand for ordering that cannot be broken down any further. The binary monotone operator is the simplest, binary inverted is the second in line, followed by operators of length four like h2, 4, 1, 3i (Wu, 1997), and then operators longer than four. The larger the branching factor under a PET node (the length of the operator on that node) the more complex the ordering. Hence, we devise possible branching feature functions over the operator length for the nodes in PETs: (we ignore unigrams now). But, it is clear that 2143 is somewhat better since it has at least some words in more or less the right order. These “abstract n-grams” pertaining to correct ordering of full phrases could be counted using ∆[ ] which would recognize that on top of the PET in 1b there is the monotone node unlike the PET in 1c"
W14-3354,W07-0404,0,0.772411,"ord level that provide evidence for translation adequacy - for example whether the stem is correctly translated, We present the UvA-ILLC submission of the B EER metric to WMT 14 metrics task. B EER is a sentence level metric that can incorporate a large number of features combined in a linear model. Novel contributions are (1) efficient tuning of a large number of features for maximizing correlation with human system ranking, and (2) novel features that give smoother sentence level scores. 1 Abstract ordering patterns found in tree factorizations of permutations into Permutation Trees (PETs) (Zhang and Gildea, 2007), including non-lexical alignment patterns. The B EER metric combines features of both kinds (presented in Section 2). With the growing number of adequacy and ordering features we need a model that facilitates efficient training. We would like to train for optimal Kendall τ correlation with rankings by human evaluators. The models in the literature tackle this problem by Introduction The quality of sentence level (also called segment level) evaluation metrics in machine translation is often considered inferior to the quality of corpus (or system) level metrics. Yet, a sentence level metrics ha"
W14-3354,W13-2202,0,0.0594864,"n 1c which has no monotone nodes at all. • factor 2 - with two features: ∆[ ] and ∆&lt;> (there are no nodes with factor 3 (Wu, 1997)) Let us take any pair of hypotheses that have the same reference r where one is better (hgood ) than the other one (hbad ) as judged by human evaluator. In order for our metric to give the same ranking as human judges do, it needs to give the higher score to the hgood hypothesis. Given that our model is linear we can derive: 3 Tuning for human judgment The task of correlation with human judgment on the sentence level is usually posed in the following way (Mach´acˇ ek and Bojar, 2013): • Translate all source sentences using the available machine translation systems • Let human evaluators rank them by quality compared to the reference translation • Each evaluation metric should do the same task of ranking the hypothesis translations • The metric with higher Kendall τ correlation with human judgment is considered better • factor 4 - feature ∆=4 • factor bigger than 4 - feature ∆>4 All of the mentioned PETs node features, except ∆[ ] and ∆count , signify the wrong word order but of different magnitude. Ideally all nodes in a PET would be binary monotone, but when that is not"
W14-3354,W07-0734,0,\N,Missing
W14-3354,W07-0718,0,\N,Missing
W14-4002,P07-2045,0,0.00543887,"ingle labeled version per Hiero rule, which we call the “canonical labeled rule”. Following (Chiang, 2010), this canonical form is the most frequent labeled variant. 4 The data for our German-English experiments is derived from parliament proceedings sourced from the Europarl corpus (Koehn, 2005), with WMT-07 development and test data. We used a maximum sentence length of 40 for filtering the training data. We employ 1M sentence pairs for training, 1K for development and 2K for testing (single reference per source sentence). Both source and target of all datasets are tokenized using the Moses(Hoang et al., 2007) tokenization script. For these experiments both the baseline and our method use a language model trained on the target side of the full original training set (approximately 1M sentences). Experiments We evaluate our method on two language pairs: using German/Chinese as source and English as target. In all experiments we decode with a 4-gram language model smoothed with modified Knesser-Ney discounting (Chen and Goodman, 1998). The data used for training the language models differs per language pair, details are given in the next paragraphs. All data is lowercased as a last pre-processing step"
W14-4002,W08-0336,0,0.0525206,"Missing"
W14-4002,W06-3601,0,0.232853,"λi × φi . The parameters {λi } are optimized on a held-out parallel corpus by direct error-minimization (Och, 2003). The labeling approach presented next differs from existing approaches. It is inspired by soft labeling but employs novel, non-linguistic bilingual labels. And it shares the bilingual intuition with phrase orientation models but it is based on a Markov approach for SCFG labeling, thereby remaining within the confines of Hiero SCFG, avoiding the need to make changes inside the decoder.1 A range of (distantly) related work exploits syntax for Hiero models, e.g. (Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zollmann and Venugopal, 2006; Wu and Hkust, 1998). In terms of labeling Hiero rules, SAMT (Zollmann and Venugopal, 2006; Mylonakis and Sima’an, 2011) exploits a “softer notion” of syntax by fitting the CCG-like syntactic labels to non-constituent phrases. The work of (Xiao et al., 2011) adds a lexicalized orientation model to Hiero, akin to (Tillmann, 1 Soft constraint decoding can easily be implemented without adapting the decoder, through a smart application of “label bridging” unary rules. In practice however, adapting the decoder turns out to be compu"
W14-4002,P05-1033,0,0.192623,", and combine these preference distributions during decoding, thus achieving a summation rather than competition between compatible label configurations. The latter approach requires significant changes to the decoder and comes at a considerable computational cost. An alternative approach (Chiang, 2010) uses labels similar to (Zollmann and Venugopal, 2006) together with boolean features for rule-label and substitutedlabel combinations; using discriminative training (MIRA) it is learned what combinations are associated with better translations. Hierarchical models and related work Hiero SCFGs (Chiang, 2005; Chiang, 2007) allow only up to two (pairs of) nonterminals on the righthand-side (RHS) of synchronous rules. The types of permissible Hiero rules are: X → hα, γi (1) X → hα X 1 β, δ X 1 ζi (2) X → hα X 1 β X 2 γ , δ X 1 ζ X 2 η i (3) X → hα X 1 β X 2 γ , δ X 2 ζ X 1 η i (4) Here α, β, γ, δ, ζ, η are terminal sequences, possibly empty. Equation 1 corresponds to a normal phrase pair, 2 to a rule with one gap and 3 and 4 to the monotone- and inverting rules respectively. Given an Hiero SCFG G, a source sentence s is translated into a target sentence t by synchronous derivations d, each is a fin"
W14-4002,W13-2258,0,0.213354,"Missing"
W14-4002,J07-2003,0,0.80819,"s in Hiero’s own training data. In this paper we take a Markovian view on synchronous top-down derivations over these factorizations which allows us to extract 0th - and 1 st -order bilingual reordering labels. Using exactly the same training data as Hiero we show that the Markovian interpretation of word alignment factorization offers major benefits over the unlabeled version. We report extensive experiments with strict and soft bilingual labeled Hiero showing improved performance up to 1 BLEU points for ChineseEnglish and about 0.1 BLEU points for German-English. Phrase reordering in Hiero (Chiang, 2007) is modelled with synchronous rules consisting of phrase pairs with at most two nonterminal gaps, thereby embedding ITG permutations (Wu, 1997) in lexical context. It is by now recognized that Hiero’s reordering can be strengthened either by labeling (e.g., (Zollmann and Venugopal, 2006)) or by supplementing the grammar with extra-grammatical reordering models, e.g., (Xiao et al., 2011; Huck et al., 2013; Nguyen and Vogel, 2013). In this paper we concentrate on labeling approaches. 11 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 11"
W14-4002,P03-1054,0,0.0113841,"with explicit phrase permutation operators also extracted from the original word alignment (Sima’an and Maillette de Buy Wenniger, 2013); Every node in the NDT is equipped with a node operator that specifies how the order of the target phrases (children of this node) is produced from the corresponding source phrases. Subsequently, we cluster the node operators in these enriched NDTs according to their complexity, e.g., monotone (straight), inverted, non-binary but one-to-one, and the more complex case of discontinuous (Maillette de Buy Wenniger and Sima’an, 2013). Inspired by work on parsing (Klein and Manning, 2003), we explore a vertical Markovian labeling approach: intuitively, 0th -order labels signify the reordering of the sub-phrases inside the phrase pair (Zhang et al., 2008), 1 st -order labels signify reordering aspects of the direct context (an embedding, parent phrase pair) of the phrase pair, and so on. Like the phrase orientation models this labeling approach does not employ external resources (e.g., taggers, parsers) beyond the training data used by Hiero. We empirically explore this bucketing for 0th - Earlier work on labeling Hiero grammars with monolingual syntax reports improved performa"
W14-4002,P10-1146,0,0.336366,"he Markov labeling approach on operatorlabelled NDTs as proposed in this paper. 2004) and achieves significant gains. The work of (Huck et al., 2013; Nguyen and Vogel, 2013) overcomes technical limitations of (Xiao et al., 2011), making necessary changes to the decoder, which involves delayed (re-)scoring at hypernodes up in the derivation of nodes lower in the chart whose orientations are affected by them. This goes to show that phrase-orientation models are not mere labelings of Hiero. 1 Soft syntactic constraints has been around for some time now (Zhou et al., 2008; Venugopal et al., 2009; Chiang, 2010). In (Zhou et al., 2008) Hiero is reinforced with a linguistically motivated prior. This prior is based on the level of syntactic homogeneity between pairs of non-terminals and the associated syntactic forests rooted at these nonterminals, whereby tree-kernels are applied to efficiently measure the amount of overlap between all pairs of sub-trees induced by the pairs of syntactic forests. Crucially, the syntactic prior encourages derivations that are more syntactically coherent but does not block derivations when they are not. In (Venugopal et al., 2009) the authors associate distributions ove"
W14-4002,2005.mtsummit-papers.11,0,0.242263,"mplexity (see examples in Figure 4): (1) Atomic: non-decomposable phrases, (2) Monotonic(Mono): binarizable, fully monotone, (3) Inverted(Inv): binarizable, fully inverted (4) Permutation(Perm): factorizes into a permutation of four or more sub-phrases (5) Complex(Comp): does not factorize into a permutation and contains at least one embedded phrase. In Figure 3, we show a phrase-complexity labeled derivation for the example of Figure 1. Observe how the phrase-centric labels reflect the relative reordering at the node. For example, the Figure 1 shows an alignment from Europarl German-English (Koehn, 2005) along with a tree showing corresponding maximally decomposed phrase pairs. Phrase pairs can be grouped into a maximally decomposed tree (called Normalized Decomposition Tree – NDT) (Zhang et al., 2008). Figure 2 shows the NDT for Figure 1, extended with pointers to the original alignment structure in Figure 2. The numbered boxes indicate how the phrases in the two representations correspond. In an NDT every phrase pair is recursively split up at every level into a minimum number (two or greater) of contiguous parts. In this example the root node splits into three phrase pairs, but these phras"
W14-4002,P11-2031,0,0.0162311,"BLEU, and after tuning we use the configuration with the highest scores on the dev set with actual (corpus level) BLEU evaluation. We report lowercase BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006) scores for the tuned test set and also for the tuned dev set, the latter mainly to observe any possible overfitting. We use Multeval version 0.5.1.12 for computing these metrics. We also use MultEval’s implementation of statistical significance testing between systems, which is based on multiple optimizer runs and approximate randomization. Multeval (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. Differences that are statistically significant and correspond to improvement/worsening with respect to the baseline are marked with N /H at the p ≤ .05 level and NN /HH at the p ≤ .01 level. We also report the Kendall Reordering Score (KRS), which is the reordering-only variant of the LRscore (Birch and Osborne, 2010) (without the optional interpolation with BLEU) and which is a sentence-level score. For the computation of statistical significance of this metric we use our"
W14-4002,P06-1077,0,0.101271,"maxd∈G |Φ(d)| i=1 λi × φi . The parameters {λi } are optimized on a held-out parallel corpus by direct error-minimization (Och, 2003). The labeling approach presented next differs from existing approaches. It is inspired by soft labeling but employs novel, non-linguistic bilingual labels. And it shares the bilingual intuition with phrase orientation models but it is based on a Markov approach for SCFG labeling, thereby remaining within the confines of Hiero SCFG, avoiding the need to make changes inside the decoder.1 A range of (distantly) related work exploits syntax for Hiero models, e.g. (Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zollmann and Venugopal, 2006; Wu and Hkust, 1998). In terms of labeling Hiero rules, SAMT (Zollmann and Venugopal, 2006; Mylonakis and Sima’an, 2011) exploits a “softer notion” of syntax by fitting the CCG-like syntactic labels to non-constituent phrases. The work of (Xiao et al., 2011) adds a lexicalized orientation model to Hiero, akin to (Tillmann, 1 Soft constraint decoding can easily be implemented without adapting the decoder, through a smart application of “label bridging” unary rules. In practice however, adapting the decoder t"
W14-4002,W11-2107,0,0.0142961,", +0.28 METEOR and +1.42 KRS. TER is the only metric that worsens, and considerably so with +1.48 point. Hiero-1 st achieves the highest improvement of KRS, namely 1.86 point higher than the Hiero baseline. Overall, this preliminary experiment shows that strict labeling sometimes gives improvements over Hiero, but sometimes it leads to worsening in terms of some of the metrics. standard practice, we tune on BLEU, and after tuning we use the configuration with the highest scores on the dev set with actual (corpus level) BLEU evaluation. We report lowercase BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006) scores for the tuned test set and also for the tuned dev set, the latter mainly to observe any possible overfitting. We use Multeval version 0.5.1.12 for computing these metrics. We also use MultEval’s implementation of statistical significance testing between systems, which is based on multiple optimizer runs and approximate randomization. Multeval (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. Differences that are statistically significant and correspond to improvemen"
W14-4002,W13-0803,1,0.877848,"Missing"
W14-4002,eisele-chen-2010-multiun,0,0.0133393,"er-Ney discounting (Chen and Goodman, 1998). The data used for training the language models differs per language pair, details are given in the next paragraphs. All data is lowercased as a last pre-processing step. In all experiments we use our own grammar extractor for the generation of all grammars, including the baseline Hiero grammars. This enables us to use the same features (as far as applicable given the grammar formalism) and assure true comparability of the grammars under comparison. Chinese-English The data for our Chinese-English experiments is derived from a combination of MultiUn(Eisele and Chen, 2010; Tiedemann, 2012)5 data and Hong Kong Parallel Text data from the Linguistic Data Consortium6 . The Hong Kong Parallel Text data is in traditional Chinese and is thus first converted to simplified Chinese to be compatible German-English 5 Freely available and downloaded from http://opus.lingfil.uu.se/ 6 The LDC catalog number of this dataset is LDC2004T08 4 Statistical significance is dependent on variance of resampled scores, and hence sometimes different for same mean scores across different systems. 16 BLEU ↑ DEV METEOR ↑ TER ↓ Hiero SAMT Hiero-0th ITG+ -Sft Hiero-0th -Sft Hiero-1 st Coars"
W14-4002,W12-3134,0,0.0177581,"ecoding {Strict,Soft} Combining these dimensions gives 8 different reordering labeled systems per language pair. On top of that we use two baseline systems, namely Hiero and Syntax Augmented Machine Translation (SAMT) to measure these systems against. An overview of the naming of our reordering labeled systems is given in Table 1. For these experiments both the baseline and our method use a language model trained on 5.4M sentences of domain specific10 news data taken from the “Xinhua” subcorpus of the English Gigaword corpus of LDC. 11 Training and decoding details Our experiments use Joshua (Ganitkevitch et al., 2012) with Viterbi best derivation. Baseline experiments use normal decoding whereas soft labeling experiments use soft constraint decoding. For training we use standard Hiero grammar extraction constraints (Chiang, 2007) (phrase pairs with source spans up to 10 words; abstract rules are forbidden). During decoding maximum span 10 on the source side is maintained. Following common practice, we use relative frequency estimates for phrase probabilities, lexical probabilities and generative rule probability. We train our systems using (batch-kbest) Mira as borrowed by Joshua from the Moses codebase, a"
W14-4002,J97-3002,0,0.730968,"s to extract 0th - and 1 st -order bilingual reordering labels. Using exactly the same training data as Hiero we show that the Markovian interpretation of word alignment factorization offers major benefits over the unlabeled version. We report extensive experiments with strict and soft bilingual labeled Hiero showing improved performance up to 1 BLEU points for ChineseEnglish and about 0.1 BLEU points for German-English. Phrase reordering in Hiero (Chiang, 2007) is modelled with synchronous rules consisting of phrase pairs with at most two nonterminal gaps, thereby embedding ITG permutations (Wu, 1997) in lexical context. It is by now recognized that Hiero’s reordering can be strengthened either by labeling (e.g., (Zollmann and Venugopal, 2006)) or by supplementing the grammar with extra-grammatical reordering models, e.g., (Xiao et al., 2011; Huck et al., 2013; Nguyen and Vogel, 2013). In this paper we concentrate on labeling approaches. 11 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 11–21, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and 1 st -order labels both as hard and soft labels. In ex"
W14-4002,P11-1065,1,0.928932,"Missing"
W14-4002,2011.mtsummit-papers.43,0,0.0527403,"report extensive experiments with strict and soft bilingual labeled Hiero showing improved performance up to 1 BLEU points for ChineseEnglish and about 0.1 BLEU points for German-English. Phrase reordering in Hiero (Chiang, 2007) is modelled with synchronous rules consisting of phrase pairs with at most two nonterminal gaps, thereby embedding ITG permutations (Wu, 1997) in lexical context. It is by now recognized that Hiero’s reordering can be strengthened either by labeling (e.g., (Zollmann and Venugopal, 2006)) or by supplementing the grammar with extra-grammatical reordering models, e.g., (Xiao et al., 2011; Huck et al., 2013; Nguyen and Vogel, 2013). In this paper we concentrate on labeling approaches. 11 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 11–21, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics and 1 st -order labels both as hard and soft labels. In experiments on German-English and ChineseEnglish we show that this extension of Hiero often significantly outperforms the unlabeled model while using no external data or monolingual labeling mechanisms. This suggests the viability of automatically"
W14-4002,C08-1136,0,0.359986,"Missing"
W14-4002,P13-1156,0,0.33835,"Missing"
W14-4002,W08-0403,0,0.0216446,"cally inducing bilingual labels following the Markov labeling approach on operatorlabelled NDTs as proposed in this paper. 2004) and achieves significant gains. The work of (Huck et al., 2013; Nguyen and Vogel, 2013) overcomes technical limitations of (Xiao et al., 2011), making necessary changes to the decoder, which involves delayed (re-)scoring at hypernodes up in the derivation of nodes lower in the chart whose orientations are affected by them. This goes to show that phrase-orientation models are not mere labelings of Hiero. 1 Soft syntactic constraints has been around for some time now (Zhou et al., 2008; Venugopal et al., 2009; Chiang, 2010). In (Zhou et al., 2008) Hiero is reinforced with a linguistically motivated prior. This prior is based on the level of syntactic homogeneity between pairs of non-terminals and the associated syntactic forests rooted at these nonterminals, whereby tree-kernels are applied to efficiently measure the amount of overlap between all pairs of sub-trees induced by the pairs of syntactic forests. Crucially, the syntactic prior encourages derivations that are more syntactically coherent but does not block derivations when they are not. In (Venugopal et al., 2009)"
W14-4002,P03-1021,0,0.021532,"ns d, each is a finite sequence of wellformed substitutions of synchronous productions from G, see (Chiang, 2006). Existing phrasebased models score a derivation der with linear interpolation of a finite set of feature functions (Φ(d)) of the derivation d, mostly working with local feature functions φi of individual productions, the target side yield string t of d (target language model features) and other features (see experimental section): arg maxd∈G P(t, d |s) ≈ P arg maxd∈G |Φ(d)| i=1 λi × φi . The parameters {λi } are optimized on a held-out parallel corpus by direct error-minimization (Och, 2003). The labeling approach presented next differs from existing approaches. It is inspired by soft labeling but employs novel, non-linguistic bilingual labels. And it shares the bilingual intuition with phrase orientation models but it is based on a Markov approach for SCFG labeling, thereby remaining within the confines of Hiero SCFG, avoiding the need to make changes inside the decoder.1 A range of (distantly) related work exploits syntax for Hiero models, e.g. (Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zollmann and Venugopal, 2006; Wu and Hkust, 1998). In terms"
W14-4002,P02-1040,0,0.0915807,"icant improvements of +0.31 BLEU, +0.28 METEOR and +1.42 KRS. TER is the only metric that worsens, and considerably so with +1.48 point. Hiero-1 st achieves the highest improvement of KRS, namely 1.86 point higher than the Hiero baseline. Overall, this preliminary experiment shows that strict labeling sometimes gives improvements over Hiero, but sometimes it leads to worsening in terms of some of the metrics. standard practice, we tune on BLEU, and after tuning we use the configuration with the highest scores on the dev set with actual (corpus level) BLEU evaluation. We report lowercase BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006) scores for the tuned test set and also for the tuned dev set, the latter mainly to observe any possible overfitting. We use Multeval version 0.5.1.12 for computing these metrics. We also use MultEval’s implementation of statistical significance testing between systems, which is based on multiple optimizer runs and approximate randomization. Multeval (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. Differences that are statistically sign"
W14-4002,W06-3119,0,0.270661,"Missing"
W14-4002,2006.amta-papers.25,0,0.056327,"the only metric that worsens, and considerably so with +1.48 point. Hiero-1 st achieves the highest improvement of KRS, namely 1.86 point higher than the Hiero baseline. Overall, this preliminary experiment shows that strict labeling sometimes gives improvements over Hiero, but sometimes it leads to worsening in terms of some of the metrics. standard practice, we tune on BLEU, and after tuning we use the configuration with the highest scores on the dev set with actual (corpus level) BLEU evaluation. We report lowercase BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006) scores for the tuned test set and also for the tuned dev set, the latter mainly to observe any possible overfitting. We use Multeval version 0.5.1.12 for computing these metrics. We also use MultEval’s implementation of statistical significance testing between systems, which is based on multiple optimizer runs and approximate randomization. Multeval (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. Differences that are statistically significant and correspond to improvement/worsening with respect to th"
W14-4002,tiedemann-2012-parallel,0,0.0143441,"n and Goodman, 1998). The data used for training the language models differs per language pair, details are given in the next paragraphs. All data is lowercased as a last pre-processing step. In all experiments we use our own grammar extractor for the generation of all grammars, including the baseline Hiero grammars. This enables us to use the same features (as far as applicable given the grammar formalism) and assure true comparability of the grammars under comparison. Chinese-English The data for our Chinese-English experiments is derived from a combination of MultiUn(Eisele and Chen, 2010; Tiedemann, 2012)5 data and Hong Kong Parallel Text data from the Linguistic Data Consortium6 . The Hong Kong Parallel Text data is in traditional Chinese and is thus first converted to simplified Chinese to be compatible German-English 5 Freely available and downloaded from http://opus.lingfil.uu.se/ 6 The LDC catalog number of this dataset is LDC2004T08 4 Statistical significance is dependent on variance of resampled scores, and hence sometimes different for same mean scores across different systems. 16 BLEU ↑ DEV METEOR ↑ TER ↓ Hiero SAMT Hiero-0th ITG+ -Sft Hiero-0th -Sft Hiero-1 st Coarse -Sft Hiero-1 st"
W14-4002,N04-4026,0,0.425033,"Missing"
W14-4002,N09-1027,0,0.0886549,"ngual labels following the Markov labeling approach on operatorlabelled NDTs as proposed in this paper. 2004) and achieves significant gains. The work of (Huck et al., 2013; Nguyen and Vogel, 2013) overcomes technical limitations of (Xiao et al., 2011), making necessary changes to the decoder, which involves delayed (re-)scoring at hypernodes up in the derivation of nodes lower in the chart whose orientations are affected by them. This goes to show that phrase-orientation models are not mere labelings of Hiero. 1 Soft syntactic constraints has been around for some time now (Zhou et al., 2008; Venugopal et al., 2009; Chiang, 2010). In (Zhou et al., 2008) Hiero is reinforced with a linguistically motivated prior. This prior is based on the level of syntactic homogeneity between pairs of non-terminals and the associated syntactic forests rooted at these nonterminals, whereby tree-kernels are applied to efficiently measure the amount of overlap between all pairs of sub-trees induced by the pairs of syntactic forests. Crucially, the syntactic prior encourages derivations that are more syntactically coherent but does not block derivations when they are not. In (Venugopal et al., 2009) the authors associate di"
W14-4002,D08-1022,0,\N,Missing
W14-4002,W10-1749,0,\N,Missing
W14-4002,P98-2230,0,\N,Missing
W14-4002,C98-2225,0,\N,Missing
W14-4002,N13-1029,0,\N,Missing
W14-4017,W10-1749,0,0.223607,"torize a permutation. There is yet a more profoud reasoning behind PETs than only accounting for long-range reorderings. The example in Figure 1 gives the flavor of PETs. Observe how every internal node in this PET dominates a subtree whose fringe1 is itself a permutation over an integer sub-range of the original permutation. Every node is decorated with a permutation over the child positions (called operator). For example h4, 5, 6i constitutes a contiguous range of integers (corresponding to a phrase pair), and hence will be grouped into a subtree; 1 2 Measures on permutations: Baselines In (Birch and Osborne, 2010; Birch and Osborne, 2011) Kendall’s tau and Hamming distance are combined with unigram BLEU (BLEU-1) leading to LRscore showing better correlation with human judgment than BLEU-4. Birch et al. (2010) additionally tests Ulam distance (longest common subsequence – LCS – normalized by the permutation length) and the square root of Kendall’s tau. Isozaki et al. (2010) presents a similar approach to (Birch and Osborne, 2011) additionally testing Spearman rho as a distance measure. Talbot et al. (2011) extracts a reordering measure from METEOR (Denkowski and Lavie, 2011) dubbed Fuzzy Reordering Sco"
W14-4017,W12-3102,0,0.0715363,"Missing"
W14-4017,W11-2107,0,0.35112,"on permutations: Baselines In (Birch and Osborne, 2010; Birch and Osborne, 2011) Kendall’s tau and Hamming distance are combined with unigram BLEU (BLEU-1) leading to LRscore showing better correlation with human judgment than BLEU-4. Birch et al. (2010) additionally tests Ulam distance (longest common subsequence – LCS – normalized by the permutation length) and the square root of Kendall’s tau. Isozaki et al. (2010) presents a similar approach to (Birch and Osborne, 2011) additionally testing Spearman rho as a distance measure. Talbot et al. (2011) extracts a reordering measure from METEOR (Denkowski and Lavie, 2011) dubbed Fuzzy Reordering Score and evaluates it on MT reordering quality. 2 Ordered sequence of leaf nodes. 139 https://github.com/stanojevic/beer For an evaluation metric we need a function which would have the standard behaviour of evaluation metrics - the higher the score the better. Bellow we define the baseline metrics that were used in our experiments. PETs to permutation forests (PEFs). A non-empty sub-sequence πij of a permutation π is isomorphic with a permutation over [1..(j − i + 1)] iff the set {πi , . . . , πj } is a contiguous range of positive integers. We will use the term a su"
W14-4017,P06-2036,0,0.831526,"permutations also contain latent atomic units that govern the recursive reordering of phrase-like units. Accounting for these latent reorderings could actually be far simpler than the flat view of a permutation. Isozaki et al. (2010) argue that the conventional metrics cannot measure well the long distance reordering between an English reference sentence “A because B” and a Japanese-English hypothesis translation “B because A”, where A and B are blocks of any length with internal monotonic alignments. In this paper we explore the idea of factorizing permutations into permutation-trees (PETs) (Gildea et al., 2006) and defining new Automatically evaluating word order of MT system output at the sentence-level is challenging. At the sentence-level, ngram counts are rather sparse which makes it difficult to measure word order quality effectively using lexicalized units. Recent approaches abstract away from lexicalization by assigning a score to the permutation representing how word positions in system output move around relative to a reference translation. Metrics over permutations exist (e.g., Kendal tau or Spearman Rho) and have been shown to be useful in earlier work. However, none of the existing metri"
W14-4017,D10-1092,0,0.64169,"t al., 2011). Existing work computes the correlation between the ranking of the outputs of different systems by an evaluation metric to human ranking, on e.g., the WMT evaluation data. For evaluating reordering, it is necessary to word align system output with the corresponding reference translation. For convenience, a 1:1 alignment (a permutation) is induced between the words on both sides (Birch and Osborne, 2011), possibly leaving words unaligned on either side. Existing work then concentrates on defining measures of reordering over permutations, cf. (Lapata, 2006; Birch and Osborne, 2011; Isozaki et al., 2010; Talbot et al., 2011). Popular metrics over permutations are: Kendall’s tau, Spearman, Hamming distance, Ulam and Fuzzy score. These metrics treat a permutation as a flat sequence of integers or blocks, disregarding the possibility of hierarchical grouping into phrase-like units, making it difficult to measure long-range order divergence. Next we will show by example that permutations also contain latent atomic units that govern the recursive reordering of phrase-like units. Accounting for these latent reorderings could actually be far simpler than the flat view of a permutation. Isozaki et a"
W14-4017,lavie-etal-2004-significance,0,0.0387563,"er or by automatically aligning sentences), and also alignments between the source sentence and the reference translation (manually or automatically aligned). Subsequently we must make those alignments 1-to-1 and merge them into a permutation. That is the approach that was followed in previous work (Birch and Osborne, 2011; Talbot et al., 5 Empirical results The results are shown in Table 1 and Table 2. These scores could be much higher if we used some more sophisticated measure than unigram BLEU for the lexical part (for example recall is very useful in evaluation of the system translations (Lavie et al., 2004)). However, this is not the issue here since our goal is merely to compare different ways to evaluate word order. All metrics that we tested have the same lexical component, get the same permutation as their input and have the same value for α. 4 Note that for reordering evaluation it does not make sense to tune α because that would blur the individual contributions of reordering and adequacy during meta evaluation, which is confirmed by Figure 7 showing that α  0.5 leads to similar performance for all metrics. 143 English-Czech English-Spanish English-German English-Russian English-French Ke"
W14-4017,W13-2202,0,0.0360351,"on-binary branching? English-Czech 6 6.2 0.156 0.157 0.157 0.158 0.173 0.175 0.165 0.165 0.185 0.183 0.182 0.183 0.196 0.195 0.195 0.195 0.219 0.219 0.216 0.217 Table 4: Sentence level Kendall tau score for translation out of English different γ with α = 0.5 and β = 0.6 The English-Czech language pair turned out to be the hardest one to evaluate for all metrics. All metrics that were used in the meta-evaluation that we conducted give much lower Kendall tau correlation coefficient compared to the other language pairs. The experiments conducted by other researchers on the same dataset (Mach´acˇ ek and Bojar, 2013), using full evaluation metrics, also get far lower Kendall tau correlation coefficient for English-Czech than for other language pairs. In the description of WMT13 data that we used (Bojar et al., 2013), it is shown that annotatorIntuitively, we might expect that inverted binary operators are preferred by human evaluators over non-binary ones. So instead of assigning zero as a score to inverted nodes we give them 0.5, while for non-binary nodes we remain with zero. The experiments with the inverted operator scored with 0.5 (i.e., γ = 0.5) are shown in Tables 4 and 5. The results show that the"
W14-4017,P02-1040,0,0.0907275,"metrics computed over Permutation Forests (PEFs), packed charts of Permutation Trees (PETs), which are tree decompositions of a permutation into primitive ordering units. We empirically compare PEFs metric against five known reordering metrics on WMT13 data for ten language pairs. The PEFs metric shows better correlation with human ranking than the other metrics almost on all language pairs. None of the other metrics exhibits as stable behavior across language pairs. 1 Introduction Evaluating word order (also reordering) in MT is one of the main ingredients in automatic MT evaluation, e.g., (Papineni et al., 2002; Denkowski 138 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 138–147, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics which in turn can be internally re-grouped into a binary branching subtree. Every node in a PET is minimum branching, i.e., the permutation factorizes into a minimum number of adjacent permutations over integer sub-ranges (Albert and Atkinson, 2005). The node operators in a PET are known to be the atomic building blocks of all permutations (called primal permutations). Because these a"
W14-4017,W14-3354,1,0.852995,"Missing"
W14-4017,W11-2102,0,0.424061,"work computes the correlation between the ranking of the outputs of different systems by an evaluation metric to human ranking, on e.g., the WMT evaluation data. For evaluating reordering, it is necessary to word align system output with the corresponding reference translation. For convenience, a 1:1 alignment (a permutation) is induced between the words on both sides (Birch and Osborne, 2011), possibly leaving words unaligned on either side. Existing work then concentrates on defining measures of reordering over permutations, cf. (Lapata, 2006; Birch and Osborne, 2011; Isozaki et al., 2010; Talbot et al., 2011). Popular metrics over permutations are: Kendall’s tau, Spearman, Hamming distance, Ulam and Fuzzy score. These metrics treat a permutation as a flat sequence of integers or blocks, disregarding the possibility of hierarchical grouping into phrase-like units, making it difficult to measure long-range order divergence. Next we will show by example that permutations also contain latent atomic units that govern the recursive reordering of phrase-like units. Accounting for these latent reorderings could actually be far simpler than the flat view of a permutation. Isozaki et al. (2010) argue that t"
W14-4017,J97-3002,0,0.804599,"Missing"
W14-4017,W07-0404,0,0.654273,"of πi+1 . If πi+1 is into a canonical permutation tree (PET). Here we a sub-permutation and it has arity a ≤ (j − (i + will summarize the relevant aspects and extend 140 h2,1i h1,2i h2,1i 3 h2,4,1,3i 5 7 4 h2,1i 1 4 h2,1i 4 h2,1i 3 2 h2,1i 2 6 1 h2,1i h2,1i h2,1i h2,1i 3 2 1 4 3 h2,1i 2 1 1 h2,1i h2,1i h2,1i 4 h2,1i 2 3 1 h2,1i 4 h2,1i 3 2 Figure 3: A PET for π = h5, 7, 4, 6, 3, 1, 2i. And five different PETs for π = h4, 3, 2, 1i. the scope of this paper, but this naive algorithm has worst case time complexity O(n3 ), and when computing only a single canonical PET this can be O(n) (see e.g., (Zhang and Gildea, 2007)). 1)), then each inference consists of a a − 1-tuple [l1 , . . . , la−1 ], where for each 1 ≤ x ≤ (a − 1), lx is a “split point” which is given by the index of the last integer in the xth sub-permutation in π. The permutation of the a sub-permutations (“children” j of πi+1 ) is stored in Oij and it is the same for all inferences of that span (Zhang et al., 2008). h2,1i 4 321 h2,1i 43 Function P EF (i, j, π, F); # Args: sub-perm. π over [i..j] and forest F Output: Parse-Forest F(π) for π; h2,1i 21 432 1 begin if ([[i, j, ?]] ∈ F) then return F; #memoization a := a(π); if a = 1 return F := F ∪"
W14-4017,C08-1136,0,0.0653582,"Missing"
W14-4017,P11-1103,0,\N,Missing
W14-4017,J06-4002,0,\N,Missing
W14-4017,W13-2201,0,\N,Missing
W14-4019,2012.eamt-1.63,1,0.453914,"Missing"
W14-4019,J93-2003,0,0.0663747,"Missing"
W14-4019,candito-etal-2010-statistical,0,0.186661,"Missing"
W14-4019,J03-4003,0,0.0173854,"more unrelated adjuncts, resulting in a drop of divergent adjuncts. 4.5 europarl 1-15 Table 9: Effect of alignment simplification on adjunct translation equivalence in the Europarl data bky std -fw 5 giza std -fw null div 6.3 21.8 7.5 21.5 2.3 24.2 3.1 24.0 we-nm we-11 11.0 12.4 9.1 10.0 12.7 14.6 10.8 13.2 eq-nm eq-11 3.2 45.0 4.0 47.5 4.0 42.0 4.8 43.7 Related work While adjunction is a formal operation that may be applied to non-linguistic adjuncts in STAG, DeNeefe and Knight (2009) restrict it to syntactic adjuncts in a Synchronous Tree Insertion Grammar. They identify complements using (Collins, 2003)’s rules, and regard all other non-head constituents as adjuncts. Their model is able to generalize to unseen adjunction patterns, and to beat a string-to-tree baseline in an Arabic-English translation task. Arnoult and Sima’an (2012) exploit adjunct optionality to generate new training data for a phrasebased model, by removing phrase pairs with an English adjunct from the training data. They identify adjuncts using syntactic heuristics in phrasestructure parses. They found that few of the generated phrase pairs were actually used at decoding, leading to marginal improvement over the baseline"
W14-4019,D09-1076,0,0.448158,"slation mismatches being mostly local. As sentence length increases however, word and adjunct alignment errors are also likely to link more unrelated adjuncts, resulting in a drop of divergent adjuncts. 4.5 europarl 1-15 Table 9: Effect of alignment simplification on adjunct translation equivalence in the Europarl data bky std -fw 5 giza std -fw null div 6.3 21.8 7.5 21.5 2.3 24.2 3.1 24.0 we-nm we-11 11.0 12.4 9.1 10.0 12.7 14.6 10.8 13.2 eq-nm eq-11 3.2 45.0 4.0 47.5 4.0 42.0 4.8 43.7 Related work While adjunction is a formal operation that may be applied to non-linguistic adjuncts in STAG, DeNeefe and Knight (2009) restrict it to syntactic adjuncts in a Synchronous Tree Insertion Grammar. They identify complements using (Collins, 2003)’s rules, and regard all other non-head constituents as adjuncts. Their model is able to generalize to unseen adjunction patterns, and to beat a string-to-tree baseline in an Arabic-English translation task. Arnoult and Sima’an (2012) exploit adjunct optionality to generate new training data for a phrasebased model, by removing phrase pairs with an English adjunct from the training data. They identify adjuncts using syntactic heuristics in phrasestructure parses. They foun"
W14-4019,J94-4004,0,0.768813,"Hwa et al. (2002) for dependency relations, Arnoult and Sima’an (2012) for adjuncts, and Pad´o and Lapata (2009) and Wu and Fung (2009) for semantic roles–that the Direct Correspondence Assumption does not always hold. A question that has not received much attention is the degree to which the assumption of synchronous adjunction is supported in human translation data. This is crucial for the succesful application of linguistically-motivated STAG, but attempts at answering this question empirically are hampered by a variety of difficulties. Linguistic structures may diverge between languages (Dorr, 1994), translations may be more or less literal, and annotation resources may be inaccurate, when they are available at all. Besides, automatic word alignments are known to be noisy and manual alignments are rather scarse. The work of Arnoult and Sima’an (2012) reports lower and upper bounds of one-to-one adjunct correspondence, using rather limited resources to identify French adjuncts making their results not directly applicable for measuring the stability of the synchronous adjunction assumption. In this paper we aim at redefining the translation equivalence of adjuncts in ways that allow us to"
W14-4019,W08-0509,0,0.0147775,"ique [en s´ecurit´e] [dans nos communaut´es] de tout le pays des opinions bien arrˆet´ees un moment heureux pour le Canada nous ne suivons pas le bon processus un espace et des moyens communs les dossiers confidentiels et prot´eg´es les noms que je viens de mentionner au Canada , un emploi sur trois tions. All four corpora except the manual Hansards are preprocessed to keep sentences with up to 80 words, and all four data sets are used jointly to train unsupervised alignments, both with the Berkeley aligner (Liang et al., 2006) and GIZA++ (Brown et al., 1993; Och and Ney, 2003) through mgiza (Gao and Vogel, 2008), using 5 iterations of Model 1 and 5 iterations of HMM for the Berkeley aligner, and 5 iterations of Model 1 and HMM and 3 iterations of Model 3 and Model 4 for GIZA++. The GIZA++ alignments are symmetrized using the grow-diag-final heuristics. Besides, the manual Hansards corpus is aligned with Sure Only (SO) and Sure and Possible (SP) manual alignments. Table 4: Adjunct identification F scores prec. vous me faites un grand honneur Experiments Experimental set-up We measure adjunct translation equivalence in four data sets: the manually-aligned Canadian Hansards corpus (Och and Ney, 2003), c"
W14-4019,P02-1050,0,0.818919,"Missing"
W14-4019,W07-2416,0,0.0440596,"th the word alignments. Equivalent types notably differ from weakly equivalent ones by being bijectively 159 3 aligned; With the notations of section 2.2.1, two adjunct sequences σ1n and τ1m with respective pro0 0 jections φn1 and ψ1m are translation equivalent iff 0 0 I(φn1 ) = I(τ1m ) and I(ψ1m ) = I(σ1n ). Adjunct identification We identify adjuncts in dependency trees obtained by conversion from phrase-structure trees: we map modifier labels to adjuncts, except when the dependent is a closed-class word. For English, we use the Berkeley parser and convert its output with the pennconverter (Johansson and Nugues, 2007; Surdeanu et al., 2008); for French, we use the Berkeley parser and the functional role labeller of Candito et al. (2010). The pennconverter with default options and the French converter make similar structural choices concerning the representation of coordination and the choice of heads. Table 2: Adjunct pairing types divergent null empty projection div no aligned target adjuncts weakly equivalent we-nm many-to-many non-bijective we-11 one-to-one non-bijective equivalent eq-nm many-to-many bijective eq-11 one-to-one bijective 3.1 English adjuncts We first identify closed-class words by their"
W14-4019,N13-1060,0,0.204878,"Missing"
W14-4019,N06-1014,0,0.0477148,"tte tˆache r´eformes des forces [arm´ees] [canadiennes] un pays [encore] [plus] magnifique [en s´ecurit´e] [dans nos communaut´es] de tout le pays des opinions bien arrˆet´ees un moment heureux pour le Canada nous ne suivons pas le bon processus un espace et des moyens communs les dossiers confidentiels et prot´eg´es les noms que je viens de mentionner au Canada , un emploi sur trois tions. All four corpora except the manual Hansards are preprocessed to keep sentences with up to 80 words, and all four data sets are used jointly to train unsupervised alignments, both with the Berkeley aligner (Liang et al., 2006) and GIZA++ (Brown et al., 1993; Och and Ney, 2003) through mgiza (Gao and Vogel, 2008), using 5 iterations of Model 1 and 5 iterations of HMM for the Berkeley aligner, and 5 iterations of Model 1 and HMM and 3 iterations of Model 3 and Model 4 for GIZA++. The GIZA++ alignments are symmetrized using the grow-diag-final heuristics. Besides, the manual Hansards corpus is aligned with Sure Only (SO) and Sure and Possible (SP) manual alignments. Table 4: Adjunct identification F scores prec. vous me faites un grand honneur Experiments Experimental set-up We measure adjunct translation equivalence"
W14-4019,W12-3129,0,0.0709486,"Missing"
W14-4019,J03-1002,0,0.0112676,"es] un pays [encore] [plus] magnifique [en s´ecurit´e] [dans nos communaut´es] de tout le pays des opinions bien arrˆet´ees un moment heureux pour le Canada nous ne suivons pas le bon processus un espace et des moyens communs les dossiers confidentiels et prot´eg´es les noms que je viens de mentionner au Canada , un emploi sur trois tions. All four corpora except the manual Hansards are preprocessed to keep sentences with up to 80 words, and all four data sets are used jointly to train unsupervised alignments, both with the Berkeley aligner (Liang et al., 2006) and GIZA++ (Brown et al., 1993; Och and Ney, 2003) through mgiza (Gao and Vogel, 2008), using 5 iterations of Model 1 and 5 iterations of HMM for the Berkeley aligner, and 5 iterations of Model 1 and HMM and 3 iterations of Model 3 and Model 4 for GIZA++. The GIZA++ alignments are symmetrized using the grow-diag-final heuristics. Besides, the manual Hansards corpus is aligned with Sure Only (SO) and Sure and Possible (SP) manual alignments. Table 4: Adjunct identification F scores prec. vous me faites un grand honneur Experiments Experimental set-up We measure adjunct translation equivalence in four data sets: the manually-aligned Canadian Ha"
W14-4019,C90-3045,0,0.92018,"Missing"
W14-4019,2009.eamt-1.30,0,0.0723821,"Missing"
W14-4019,P12-1095,0,0.176151,"Missing"
W14-4019,W08-2121,0,\N,Missing
W14-4019,W90-0102,0,\N,Missing
W15-3050,W14-3354,1,0.882797,"Missing"
W15-3050,W10-1749,0,0.0195015,"omputations we have an additional requirement that our sentence level scores need to be scaled proportional to the translation quality. 3.1 New BEER scoring function To make the scores on the sentence level better scaled we transform our linear model into a probabilistic linear model – logistic regression with the following scoring function: Reordering component based on PETs The word alignments between system and reference translation can be simplified and considered as permutation of words from the reference translation in the system translation. Previous work by (Isozaki et al., 2010) and (Birch and Osborne, 2010) used this permutation view of word order and applied Kendall τ for evaluating its distance from ideal (monotone) word order. BEER goes beyond this skip-gram based evaluation and decomposes permutation into a hierarchical structure which shows how subparts of permutation form small groups that can be reordered all together. Figure 1a shows PET for permutation h2, 5, 6, 4, 1, 3i. Ideally the permutation tree will be filled with nodes h1, 2i which would say score(h, r) = 1 1 + e− P i wi ×φi (h,r) There is still a problem with this formulation. During training, the model is trained on the differ~"
W15-3050,W14-4017,1,0.859475,"Missing"
W15-3050,D14-1082,0,0.0173973,"standard version of BEER does not use any syntactic knowledge. Since the training method of BEER allows the usage of a large number of features, it is trivial to integrate new features that would measure the matching between some syntax attributes of system and reference translations. The syntactic representation we exploit is a dependency tree. The reason for that is that we can easily connect the structure with the lexical content and it is fast to compute which can often be very important for evaluation metrics when they need to evaluate on large data. We used Stanford’s dependency parser (Chen and Manning, 2014) because it gives high accuracy parses in a very short time. The features we compute on the dependency trees of the system and its reference translation are: 5 BEER for tuning The metrics that perform well on metrics task are very often not good for tuning. This is because recall has much more importance for human judgment than precision. The metrics that put more weight on recall than precision will be better with 398 tuning metric BEER BLEU BEER no bias BLEU 16.4 18.2 18.0 MTR 28.4 28.1 27.7 BEER 10.2 10.1 9.8 System Name TrueSkill Score Tuning-Only All BLEU -MIRA- DENSE 0.153 -0.177 ILLC-U"
W15-3050,D14-1025,1,0.88796,"Missing"
W15-3050,W11-2107,0,0.0900113,"Missing"
W15-3050,2009.mtsummit-posters.8,0,0.0191127,"Missing"
W15-3050,C14-1193,0,0.0383694,"ng 3. arc type matching 4. valency matching For each of these we compute precision, recall and F1-score. It has been shown by other researchers (Popovi´c and Ney, 2009) that POS tags are useful for abstracting away from concrete words and measure the grammatical aspect of translation (for example it can captures agreement). Dependency word bigrams (bigrams connected by a dependency arc) are also useful for capturing long distance dependencies. Most of the previous metrics that work with dependency trees usually ignore the type of the dependency that is (un)matched and treat all types equally (Yu et al., 2014). This is clearly not the case. Surely subject and complement arcs are more important than modifier arc. To capture this we created individual features for precision, recall and F1-score matching of each arc type so our system could learn on which arc type to put more weight. All words take some number of arguments (valency), and not matching that number of arguments is a sign of a, potentially, bad translation. With this feature we hope to capture the aspect of not producing the right number of arguments for all words (and especially verbs) in the sentence. This model BEER Treepel contains in"
W15-3050,W07-0404,0,0.021903,"the sentence level than word n-grams 2.2 Learning-to-rank Since the task on which our model is going to be evaluated is ranking translations it comes natural to train the model using learning-to-rank techniques. Our training data consists of pairs of “good” and “bad” translations. By using a feature vector ~ good for a good translation and a feature vector φ ~ bad for a bad translation then using the following φ equations we can transform the ranking problem into a binary classification problem (Herbrich et al., 1999): 3. permutation trees - hierarchical decomposition of word order based on (Zhang and Gildea, 2007) A deeper analysis of (2) is presented in (Stanojevi´c and Sima’an, 2014c) and of (3) in (Stanojevi´c and Sima’an, 2014b). Here we modify BEER by 1. incorporating a better scoring function that give scores that are better scaled 396 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 396–401, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. that there is no need to do any reordering (everything is in the right place). BEER has features that compute the number of different node types and for each different type it assigns a differ"
W15-3050,D10-1092,0,0.0161576,"this kind of corpus level computations we have an additional requirement that our sentence level scores need to be scaled proportional to the translation quality. 3.1 New BEER scoring function To make the scores on the sentence level better scaled we transform our linear model into a probabilistic linear model – logistic regression with the following scoring function: Reordering component based on PETs The word alignments between system and reference translation can be simplified and considered as permutation of words from the reference translation in the system translation. Previous work by (Isozaki et al., 2010) and (Birch and Osborne, 2010) used this permutation view of word order and applied Kendall τ for evaluating its distance from ideal (monotone) word order. BEER goes beyond this skip-gram based evaluation and decomposes permutation into a hierarchical structure which shows how subparts of permutation form small groups that can be reordered all together. Figure 1a shows PET for permutation h2, 5, 6, 4, 1, 3i. Ideally the permutation tree will be filled with nodes h1, 2i which would say score(h, r) = 1 1 + e− P i wi ×φi (h,r) There is still a problem with this formulation. During training, the m"
W15-3050,W14-3336,0,0.0623342,"r in Sections 3, 4 and 5. We show the results from the metric and tuning tasks in Section 6, and conclude in Section 7. 2 BEER basics The model underying the BEER metric is flexible for the integration of an arbitrary number of new features and has a training method that is targeted for producing good rankings among systems. Two other characteristic properties of BEER are its hierarchical reordering component and character ngrams lexical matching component. Introduction In the 2014 WMT metrics task, BEER turned up as the best sentence level evaluation metric on average over 10 language pairs (Machacek and Bojar, 2014). We believe that this was due to: 2.1 Old BEER scoring BEER is essentially a linear model with which the score can be computed in the following way: 1. learning-to-rank - type of training that allows a large number of features and also training on the same objective on which the model is going to be evaluated : ranking of translations score(h, r) = X ~ wi × φi (h, r) = w ~ ·φ i ~ is a feature where w ~ is a weight vector and φ vector. 2. dense features - character n-grams and skipbigrams that are less sparse on the sentence level than word n-grams 2.2 Learning-to-rank Since the task on which"
W15-3050,W14-3348,0,0.245718,"Missing"
W15-3050,P02-1040,0,0.0955018,"kes in ranking on the training set. 2.3 3 Our goal here is to create corpus level extension of BEER that decomposes trivially at the sentence level. More concretely we wanted to have a corpus level BEER that would be the average of the sentence level BEER of all sentences in the corpus: Lexical component based on character n-grams Lexical scoring of BEER relies heavily on character n-grams. Precision, Recall and F1-score are used with character n-gram orders from 1 until 6. These scores are more smooth on the sentence level than word n-gram matching that is present in other metrics like BLEU (Papineni et al., 2002) or METEOR (Michael Denkowski and Alon Lavie, 2014). BEER also uses precision, recall and F1-score on word level (but not with word n-grams). Matching of words is computed over METEOR alignments that use WordNet, paraphrasing and stemming to have more accurate alignment. We also make distinction between function and content words. The more precise description of used features and their effectiveness is presented in (Stanojevi´c and Sima’an, 2014c). 2.4 Corpus level BEER P BEERcorpus (c) = si ∈c BEERsent (si ) |c| (1) In order to do so it is not suitable to to use previous scoring function of B"
W15-3050,W09-0402,0,0.0595695,"Missing"
W15-5701,2011.eamt-1.38,0,0.0133912,"001) 6 One can also note that, as the right-hand-side distance is not normalized, it takes lower values than the left-hand-side distance; we have not attempted to weigh them differently, but more experiments in this direction might be worth the while. 8 used constituency trees, recent work has come to use a larger array of linguistic formalisms: besides applications of TAG (DeNeefe and Knight, 2009; Liu et al., 2011), Xie et al. (2011) apply dependency syntax for tree-to-string modelling and Li et al. (2012) for labelling Hiero; Hassan et al. (2007) apply CCG supertags to phrase-based SMT and Almaghout et al. (2011) to Hiero; Xiong et al. (2012) apply predicate-argument structures in a hierarchical phrase-based model and Li et al.(2013) for in Hiero. Labelling hierarchical models introduces new constraints while providing the opportunity to relax innate Hiero constraints. New constraints are: a limitation of the grammar to observed substitutions, which can be remedied by relaxing matching constraints using features to learn subsitution preferences (Chiang, 2010); an increase of rule sparsity and computational constraints, which can be remedied by label clustering (Hanneman and Lavie, 2013; Mino et al., 2"
W15-5701,2012.eamt-1.63,1,0.905592,"Missing"
W15-5701,W14-4019,1,0.830822,"Missing"
W15-5701,candito-etal-2010-statistical,0,0.0526148,"Missing"
W15-5701,P05-1033,0,0.603945,"cts and arguments. Our labels are bilingual obtained from dependency annotations and extended to cover nonsyntactic phrases. The label set we derive in this manner is extremely small, as it contains only thirty-six labels, and yet we find it useful to cluster these labels even further. We present a clustering method that uses label similarity based on left-hand-side/right-hand-side joint trained-model estimates. The results of initial experiments show that our model performs similarly to Hiero on in-domain French-English data. 1 Introduction Labelling Hierarchical Phrase-Based models (Hiero) (Chiang, 2005) allows to disambiguate Hiero, while benefitting from its broad coverage. Using syntactic labels for labelling as Zollmann and Venugopal (2006) do with Syntax-Augmented Machine Translation (SAMT) or, e.g., Li et al. (2012) in an inspired approach, yields however unwieldy models with large non-terminal vocabularies. We propose to approach the labelling problem from the other end, using the adjunct-argument distinction to minimally label Hiero. We interpret adjuncts in the general sense of modifiers, and not only of adjuncts in semantic frames. Generally speaking, the adjunct-argument distinctio"
W15-5701,P10-1146,0,0.159065,"ro are marked with one ▿ for p = 0.05 and two for p = 0.01; significant differences with the original label-set model are marked with one ▴ for p = 0.05 and two for p = 0.01; dev 6 BLEU test METEOR dev test dev TER test Hiero AA-Bi 32.1 31.9 31.8 31.5 34.9 34.8 34.8 34.7 52.9 53.0 53.3 53.5 AA-Cn AA-Cu 31.8▿▿/▾ 31.9▿▿ 31.4▿▿ 31.8▴▴ 34.8 34.9 34.7▿ 34.8▴▴ 53.1▿▿ 53.0 53.6▿▿ 53.3▴▴ Discussion Our first results show that direct matching of source and target labels leads to sub-optimal performance. Our solution uses rule estimates to cluster bilingual labels. This is orthogonal to the approach of Chiang (2010), who applies rule-matching features on both sides of the data, without explicitely matching source and target labels. While using bilingual labels is appealing as these labels are directly interpretable in terms of syntactic correspondence, clustering only allows to merge labels. A more refined method would allow to both split and merge labels, with the original adjunct/argument labels as a starting point for characterizing synchronous recursion linguistically. As far as the current clustering procedure is concerned, we have shown that a distance based on rewriting patterns of left-hand-side"
W15-5701,P11-2031,0,0.0240604,"the Berkeley Parser4 –the data are then true-cased–, and then convert parses to dependency parses: with the Pennconverter of Johansson and Nugues (2007) for English, and the Functional Role Labeller of Candito et al. (2010) for French. 5.1.3 Model Training and decoding We train models using an in-house grammar extractor, and a decoder based on Joshua5 . Training and decoding constraints and defaults are the same as for Hiero, but we disallow consecutive non-terminals on both sides, and not only on the source side. Model parameters are tuned with Mira, allowing up to 20 iterations. Following (Clark et al., 2011) we average results over three rounds of tuning/decoding. 5.2 First results Table 3 reports tests on adjunct/argument label sets, where we use source-language labels only (AA-Src), target-language labels (AA-Trg), or combined, bilingual labels (AA-Bi). Table 3: Performance of monolingual and bilingual labelling schemes with regard to Hiero; significant differences are marked with one ▿ for p = 0.05 and two for p = 0.01 BLEU dev test METEOR dev test TER dev test Hiero 32.1 31.8 34.9 34.8 52.9 53.3 AA-Src AA-Trg AA-Bi 31.9▿▿ 32.0▿ 31.9▿ 31.3▿▿ 31.6▿▿ 31.5▿▿ 34.8▿ 34.9 34.8 34.7▿▿ 34.7▿ 34.7▿▿ 53"
W15-5701,D09-1076,0,0.162557,"the general sense of modifiers, and not only of adjuncts in semantic frames. Generally speaking, the adjunct-argument distinction accounts for a difference in selectional preferences: arguments are selected by their heads, while adjuncts select their heads. This distinction is modelled in Tree-Adjoining Grammar (Joshi et al., 1975; Joshi and Schabes, 1997), through substitution and adjunction. Shieber and Schabes (1990) and Shieber (2007) have proposed Synchronous Tree-Adjoining Grammar (STAG) by for SMT, and the adjunct/argument distinction has been applied to Syntax-Based models notably by DeNeefe and Knight (2009) and Liu et al. (2011). We do not attempt here to model adjunction in Hiero, rather we reduce the adjunct-argument distinction to one of type. The semantic aspect of this distinction–adjuncts modify the meaning of a phrase, while arguments complete it–makes it appealing for Machine Translation, as one may expect that it can be preserved across a bitext. To circumvene mismatches, we label both sides of the data to derive bilingual labels. The label set that we derive is minimal as we start from two labels for adjuncts and arguments on both sides of the data, and derive only four new labels for"
W15-5701,J94-4004,0,0.201426,"A phrase label for φ if φ matches a dependent D then Label(φ) ← Label(D) else if φ matches a sequence of dependents Di then if all Di are adjuncts then Label(φ) ← A else Label(φ) ← CS else if φ matches a dependent D less some left and/or right sub-dependents SDi then if all SDi are adjuncts then Label(φ) ← Label(D) else if D is an adjunct then Label(φ) ← AI else Label(φ) ← CI else Label(φ) ← P 2.2 Bilingual labelling The adjunct/argument label set presented thus far can be equally applied on the source and target sides of the data. To account for parsing differences and linguistic divergence (Dorr, 1994; Hwa et al., 2002; Arnoult and Sima’an, 2014), we combine source and target labels into composite, bilingual labels. The resulting label set consists of 36 labels. Table 2 shows some phrase pairs for the example of Figure 1. Table 2: Adjunct/argument-based phrasal labels label French phrase English phrase label French phrase English phrase AA CC CA CI AI CI P enfin , les armes de par le monde de par les conflits de par finally , arms all over the world all over conflicts all over CI CI CI CI CS CS PP alimentent monde le monde . monde . fuel world the world . world . The phrase “finally ,” is"
W15-5701,N13-1029,0,0.108315,"ature also applies for adjunct/argument-labelled models, and is then computed on unlabelled rule equivalents, i.e., on lexical content only. Like SAMT (Zollmann, 2011), the model uses features for left-hand-side-conditioned rule weights, labelled-rule translation weights, rule-rarity penalty, and flags for lexical-only rules, abstract rules, monotone rules, and abstract-target rules. Unlike SAMT, we do not condition labelled on unlabelled sides: phrase-translation weights are computed on labelled rules on the one hand, and on unlabelled rules on the other hand. 4 4 Adjunction-label clustering Hanneman and Lavie (2013) propose a clustering method for SAMT labels to reduce their amount and the resulting computational load. Their method employs source labels next to the usual SAMT target labels: combining source and target labels allows them to compute relative-frequency estimates of source/target labels, which serve to compute distance measures between source labels on one hand, and target labels on the other. The distance measure between two source labels s1 and s2 is defined as the marginal difference between P (t∣s1 ) and P (t∣s2 ) estimates; the distance between target labels is defined similarly. Cluste"
W15-5701,P07-1037,1,0.86839,"Missing"
W15-5701,2012.eamt-1.66,0,0.0241685,"al.(2013) for in Hiero. Labelling hierarchical models introduces new constraints while providing the opportunity to relax innate Hiero constraints. New constraints are: a limitation of the grammar to observed substitutions, which can be remedied by relaxing matching constraints using features to learn subsitution preferences (Chiang, 2010); an increase of rule sparsity and computational constraints, which can be remedied by label clustering (Hanneman and Lavie, 2013; Mino et al., 2014). The Hiero constraints that one attempts to relax are the monotonic top-level ordering, first and foremost (Huck et al., 2012; Li et al., 2012; Li et al., 2013). Li et al. (2012) also relax the source non-terminal adjacency constraint, while Li et al. (2013) relax the phrase-length constraint for extraction and decoding. 8 Conclusion We have presented a bilingual labelling scheme for Hiero that is based on the adjunct/argument distinction. Even though our label set is very small, containing only thirty-six labels, we find that clustering these labels is useful. As it is, our model is able to perform similarly to Hiero on in-domain test data for French-English. For future work, we plan to refine our labelling method,"
W15-5701,P02-1050,0,0.0547996,"el for φ if φ matches a dependent D then Label(φ) ← Label(D) else if φ matches a sequence of dependents Di then if all Di are adjuncts then Label(φ) ← A else Label(φ) ← CS else if φ matches a dependent D less some left and/or right sub-dependents SDi then if all SDi are adjuncts then Label(φ) ← Label(D) else if D is an adjunct then Label(φ) ← AI else Label(φ) ← CI else Label(φ) ← P 2.2 Bilingual labelling The adjunct/argument label set presented thus far can be equally applied on the source and target sides of the data. To account for parsing differences and linguistic divergence (Dorr, 1994; Hwa et al., 2002; Arnoult and Sima’an, 2014), we combine source and target labels into composite, bilingual labels. The resulting label set consists of 36 labels. Table 2 shows some phrase pairs for the example of Figure 1. Table 2: Adjunct/argument-based phrasal labels label French phrase English phrase label French phrase English phrase AA CC CA CI AI CI P enfin , les armes de par le monde de par les conflits de par finally , arms all over the world all over conflicts all over CI CI CI CI CS CS PP alimentent monde le monde . monde . fuel world the world . world . The phrase “finally ,” is labelled as an adj"
W15-5701,W07-2416,0,0.024727,"art from sentence pairs that have been parsed on both sides of the data into dependencies, and we map dependency labels to either adjuncts or arguments, as is Figure 11 . enfin , les armes alimentent les conflits de par le monde . finally , arms fuel conflicts all over the world . Figure 1: Example sentence pair. Adjunct dependencies are indicated with dashes. Dependency labels vary per parser, but we broadly map modifier and punctuation labels to adjuncts, and remaining labels to arguments. Table 1 presents the mapping from the dependency converters of Candito et al. (2010) for French and of Johansson and Nugues (2007) for English. Table 1: Adjunct-mapping criteria for English and French head-governor relation English French 2.1 ADV, APPO, PRN AMOD PMOD NMOD other constraints on head h, governor g, etc. P relation(g, governor(g)) ! = ADV h precedes g POS(h) ∉ {CC, DT, EX, POS, MD, PRP, PRP$, RP, SYM, WDT, WP, WP$, WRB} h has no dependents mod, mod rel ponct h has no dependents Phrase-Labelling scheme Next, we define phrase labels to allow for recursion over non-syntactic phrases. Our phrase labelling procedure is summarized as Algorithm 1. This scheme follows SAMT and beyond that Combinatorial Categorial Gr"
W15-5701,W12-3128,0,0.0383444,"Missing"
W15-5701,N13-1060,0,0.0423257,"Missing"
W15-5701,P11-1128,0,0.0245975,"Missing"
W15-5701,D14-1019,0,0.0121663,"et al. (2011) to Hiero; Xiong et al. (2012) apply predicate-argument structures in a hierarchical phrase-based model and Li et al.(2013) for in Hiero. Labelling hierarchical models introduces new constraints while providing the opportunity to relax innate Hiero constraints. New constraints are: a limitation of the grammar to observed substitutions, which can be remedied by relaxing matching constraints using features to learn subsitution preferences (Chiang, 2010); an increase of rule sparsity and computational constraints, which can be remedied by label clustering (Hanneman and Lavie, 2013; Mino et al., 2014). The Hiero constraints that one attempts to relax are the monotonic top-level ordering, first and foremost (Huck et al., 2012; Li et al., 2012; Li et al., 2013). Li et al. (2012) also relax the source non-terminal adjacency constraint, while Li et al. (2013) relax the phrase-length constraint for extraction and decoding. 8 Conclusion We have presented a bilingual labelling scheme for Hiero that is based on the adjunct/argument distinction. Even though our label set is very small, containing only thirty-six labels, we find that clustering these labels is useful. As it is, our model is able to"
W15-5701,C00-2092,0,0.0629977,"way to lift Hiero’s standard phrase-length constraint. 7 Related Work Most work on adjunction in SMT takes place in a syntax-based framework, which forms a natural ground for STAG. DeNeefe and Knight (2009) and Liu et al. (2011) for instance have proposed tree-to-string models that differentiate between adjunction and substitution. The only application of adjunction to string-to-string models we know of is that of Arnoult and Sima’an (2012), who exploit the optional character of adjuncts to extract more rules for a Phrase-Based model. While the first applications of syntax for SMT (Wu, 1997; Poutsma, 2000; Yamada and Knight, 2001) 6 One can also note that, as the right-hand-side distance is not normalized, it takes lower values than the left-hand-side distance; we have not attempted to weigh them differently, but more experiments in this direction might be worth the while. 8 used constituency trees, recent work has come to use a larger array of linguistic formalisms: besides applications of TAG (DeNeefe and Knight, 2009; Liu et al., 2011), Xie et al. (2011) apply dependency syntax for tree-to-string modelling and Li et al. (2012) for labelling Hiero; Hassan et al. (2007) apply CCG supertags to"
W15-5701,C90-3045,0,0.110662,"dy models with large non-terminal vocabularies. We propose to approach the labelling problem from the other end, using the adjunct-argument distinction to minimally label Hiero. We interpret adjuncts in the general sense of modifiers, and not only of adjuncts in semantic frames. Generally speaking, the adjunct-argument distinction accounts for a difference in selectional preferences: arguments are selected by their heads, while adjuncts select their heads. This distinction is modelled in Tree-Adjoining Grammar (Joshi et al., 1975; Joshi and Schabes, 1997), through substitution and adjunction. Shieber and Schabes (1990) and Shieber (2007) have proposed Synchronous Tree-Adjoining Grammar (STAG) by for SMT, and the adjunct/argument distinction has been applied to Syntax-Based models notably by DeNeefe and Knight (2009) and Liu et al. (2011). We do not attempt here to model adjunction in Hiero, rather we reduce the adjunct-argument distinction to one of type. The semantic aspect of this distinction–adjuncts modify the meaning of a phrase, while arguments complete it–makes it appealing for Machine Translation, as one may expect that it can be preserved across a bitext. To circumvene mismatches, we label both sid"
W15-5701,W07-0412,0,0.0276328,"al vocabularies. We propose to approach the labelling problem from the other end, using the adjunct-argument distinction to minimally label Hiero. We interpret adjuncts in the general sense of modifiers, and not only of adjuncts in semantic frames. Generally speaking, the adjunct-argument distinction accounts for a difference in selectional preferences: arguments are selected by their heads, while adjuncts select their heads. This distinction is modelled in Tree-Adjoining Grammar (Joshi et al., 1975; Joshi and Schabes, 1997), through substitution and adjunction. Shieber and Schabes (1990) and Shieber (2007) have proposed Synchronous Tree-Adjoining Grammar (STAG) by for SMT, and the adjunct/argument distinction has been applied to Syntax-Based models notably by DeNeefe and Knight (2009) and Liu et al. (2011). We do not attempt here to model adjunction in Hiero, rather we reduce the adjunct-argument distinction to one of type. The semantic aspect of this distinction–adjuncts modify the meaning of a phrase, while arguments complete it–makes it appealing for Machine Translation, as one may expect that it can be preserved across a bitext. To circumvene mismatches, we label both sides of the data to d"
W15-5701,J97-3002,0,0.4968,"eby making way to lift Hiero’s standard phrase-length constraint. 7 Related Work Most work on adjunction in SMT takes place in a syntax-based framework, which forms a natural ground for STAG. DeNeefe and Knight (2009) and Liu et al. (2011) for instance have proposed tree-to-string models that differentiate between adjunction and substitution. The only application of adjunction to string-to-string models we know of is that of Arnoult and Sima’an (2012), who exploit the optional character of adjuncts to extract more rules for a Phrase-Based model. While the first applications of syntax for SMT (Wu, 1997; Poutsma, 2000; Yamada and Knight, 2001) 6 One can also note that, as the right-hand-side distance is not normalized, it takes lower values than the left-hand-side distance; we have not attempted to weigh them differently, but more experiments in this direction might be worth the while. 8 used constituency trees, recent work has come to use a larger array of linguistic formalisms: besides applications of TAG (DeNeefe and Knight, 2009; Liu et al., 2011), Xie et al. (2011) apply dependency syntax for tree-to-string modelling and Li et al. (2012) for labelling Hiero; Hassan et al. (2007) apply C"
W15-5701,D11-1020,0,0.025522,"t the optional character of adjuncts to extract more rules for a Phrase-Based model. While the first applications of syntax for SMT (Wu, 1997; Poutsma, 2000; Yamada and Knight, 2001) 6 One can also note that, as the right-hand-side distance is not normalized, it takes lower values than the left-hand-side distance; we have not attempted to weigh them differently, but more experiments in this direction might be worth the while. 8 used constituency trees, recent work has come to use a larger array of linguistic formalisms: besides applications of TAG (DeNeefe and Knight, 2009; Liu et al., 2011), Xie et al. (2011) apply dependency syntax for tree-to-string modelling and Li et al. (2012) for labelling Hiero; Hassan et al. (2007) apply CCG supertags to phrase-based SMT and Almaghout et al. (2011) to Hiero; Xiong et al. (2012) apply predicate-argument structures in a hierarchical phrase-based model and Li et al.(2013) for in Hiero. Labelling hierarchical models introduces new constraints while providing the opportunity to relax innate Hiero constraints. New constraints are: a limitation of the grammar to observed substitutions, which can be remedied by relaxing matching constraints using features to learn"
W15-5701,P12-1095,0,0.0176697,"the right-hand-side distance is not normalized, it takes lower values than the left-hand-side distance; we have not attempted to weigh them differently, but more experiments in this direction might be worth the while. 8 used constituency trees, recent work has come to use a larger array of linguistic formalisms: besides applications of TAG (DeNeefe and Knight, 2009; Liu et al., 2011), Xie et al. (2011) apply dependency syntax for tree-to-string modelling and Li et al. (2012) for labelling Hiero; Hassan et al. (2007) apply CCG supertags to phrase-based SMT and Almaghout et al. (2011) to Hiero; Xiong et al. (2012) apply predicate-argument structures in a hierarchical phrase-based model and Li et al.(2013) for in Hiero. Labelling hierarchical models introduces new constraints while providing the opportunity to relax innate Hiero constraints. New constraints are: a limitation of the grammar to observed substitutions, which can be remedied by relaxing matching constraints using features to learn subsitution preferences (Chiang, 2010); an increase of rule sparsity and computational constraints, which can be remedied by label clustering (Hanneman and Lavie, 2013; Mino et al., 2014). The Hiero constraints th"
W15-5701,P01-1067,0,0.195556,"ero’s standard phrase-length constraint. 7 Related Work Most work on adjunction in SMT takes place in a syntax-based framework, which forms a natural ground for STAG. DeNeefe and Knight (2009) and Liu et al. (2011) for instance have proposed tree-to-string models that differentiate between adjunction and substitution. The only application of adjunction to string-to-string models we know of is that of Arnoult and Sima’an (2012), who exploit the optional character of adjuncts to extract more rules for a Phrase-Based model. While the first applications of syntax for SMT (Wu, 1997; Poutsma, 2000; Yamada and Knight, 2001) 6 One can also note that, as the right-hand-side distance is not normalized, it takes lower values than the left-hand-side distance; we have not attempted to weigh them differently, but more experiments in this direction might be worth the while. 8 used constituency trees, recent work has come to use a larger array of linguistic formalisms: besides applications of TAG (DeNeefe and Knight, 2009; Liu et al., 2011), Xie et al. (2011) apply dependency syntax for tree-to-string modelling and Li et al. (2012) for labelling Hiero; Hassan et al. (2007) apply CCG supertags to phrase-based SMT and Alma"
W15-5701,W06-3119,0,0.151135,"e label set we derive in this manner is extremely small, as it contains only thirty-six labels, and yet we find it useful to cluster these labels even further. We present a clustering method that uses label similarity based on left-hand-side/right-hand-side joint trained-model estimates. The results of initial experiments show that our model performs similarly to Hiero on in-domain French-English data. 1 Introduction Labelling Hierarchical Phrase-Based models (Hiero) (Chiang, 2005) allows to disambiguate Hiero, while benefitting from its broad coverage. Using syntactic labels for labelling as Zollmann and Venugopal (2006) do with Syntax-Augmented Machine Translation (SAMT) or, e.g., Li et al. (2012) in an inspired approach, yields however unwieldy models with large non-terminal vocabularies. We propose to approach the labelling problem from the other end, using the adjunct-argument distinction to minimally label Hiero. We interpret adjuncts in the general sense of modifiers, and not only of adjuncts in semantic frames. Generally speaking, the adjunct-argument distinction accounts for a difference in selectional preferences: arguments are selected by their heads, while adjuncts select their heads. This distinct"
W15-5704,J07-2003,0,0.413379,"ord order predictions s′ can improve preordering performance. Since we use projective dependency trees, which are internally converted to a flat phrase structure representation, the model can be expressed in the form of a weighted context-free grammar in which labels encode the order of the constituents. One method to weaken the independence assumptions of this grammar is the direct integration of a language model (LM). This idea is reminiscent of the integration of the finite state language model with the synchronous context-free grammar used in hierarchical phrase-based machine translation (Chiang, 2007). 32 Hence, instead of searching for ˆs′ = arg maxs′ P (s′ |s, τ ), the search will now include the ngram language model, such that: ˆs′ = arg maxs′ P (s′ |s, τ )PLM (s′ ). This integration can be performed in three ways: the simplest form of integration, which is fast but allows for significant search errors, is to generate an n-best list of word order predictions using the −LM preordering model (i.e., without the LM or other non-local features) and re-score this list using the language model. On the other end of the spectrum, the language model can be integrated by performing a full intersec"
W15-5704,P05-1066,0,0.0610585,"ng more exhaustive search algorithms or specialized reordering models, preordering provides several benefits: Apart from facilitating the integration of additional information sources such as paraphrases, preordering approaches provide significant improvements in runtime performance. Jehl et al. (2014), for example, report an 80-fold speed improvement using their preordering system compared to a standard system producing translations of the same quality. Preordering systems can be compared along several dimensions. The main distinctions are whether the reordering rules are specified manually (Collins et al., 2005) or automatically learnt from data (Lerner and Petrov, 2013; Khalilov and Sima’an, 2012). Furthermore, approaches differ in the types of syntactic structures they assume. Systems may use either source or target syntax (Lerner and Petrov, 2013; Khalilov and Sima’an, 2012), both source and target syntax or no syntax at all (e.g. DeNero and Uszkoreit (2011)). In this paper, we focus on approaches using only source-side syntax. Dependency grammar offers a flexible and light-weight syntactic framework that can cover a large number of languages and This work is licenced under a Creative Commons Attr"
W15-5704,W06-1609,0,0.0836468,"Missing"
W15-5704,N15-1105,0,0.0344703,"Missing"
W15-5704,D11-1018,0,0.137098,"ent using their preordering system compared to a standard system producing translations of the same quality. Preordering systems can be compared along several dimensions. The main distinctions are whether the reordering rules are specified manually (Collins et al., 2005) or automatically learnt from data (Lerner and Petrov, 2013; Khalilov and Sima’an, 2012). Furthermore, approaches differ in the types of syntactic structures they assume. Systems may use either source or target syntax (Lerner and Petrov, 2013; Khalilov and Sima’an, 2012), both source and target syntax or no syntax at all (e.g. DeNero and Uszkoreit (2011)). In this paper, we focus on approaches using only source-side syntax. Dependency grammar offers a flexible and light-weight syntactic framework that can cover a large number of languages and This work is licenced under a Creative Commons Attribution 4.0 International License. 29 Proceedings of the 1st Deep Machine Translation Workshop (DMTW 2015), pages 29–38, Praha, Czech Republic, 3–4 September 2015. Atr ... AuxP AuxA Atr AuxA the . house . das Haus of. . the . green . man . des gr¨unen Mannes case=gen case=gen case=gen Figure 1: Translation of an English prepositional phrase as a genitive"
W15-5704,N13-1073,0,0.0319799,"ction (en–de, scores from predicted to gold-ordered en). word alignment using MGIZA++ (Gao and Vogel, 2008). We perform 6 iterations of IBM model 1 training followed by 6 iterations of HMM word alignment and 3 iterations each of IBM model 3 and 4. After initial training, the preordering model is applied to s, obtaining the preordered corpus ˆs′ . Since the word order differences between ˆs′ and t should be less acute, less computationally expensive word alignment tools are sufficient to re-align the corpus. We align ˆs′ and t using fast align,4 an efficient re-parameterization of IBM model 2 (Dyer et al., 2013). Improvements in word order can lead to improvements in alignments and hence the training and word alignment process can be performed repeatedly. Lerner and Petrov (2013) report no significant improvements after the initial re-alignment. Accordingly, we do not iterate the training process either. The underlying translation system is Moses (Koehn et al., 2007) using the standard feature setup and using only the distortion-based reordering model. Tuning is performed using MERT (Och, 2003). The system is trained on the full parallel sections of the Europarl corpus (Koehn, 2005) and tuned and tes"
W15-5704,W08-0509,0,0.0960665,"Missing"
W15-5704,2013.iwslt-papers.11,0,0.0286464,"experimental setup in Section 4.1. Section 4.2 and 4.3 present results of the experimental evaluation and a discussion of these results. We conclude in Section 5. 2 Related work Various approaches to preordering have been explored in the literature. A brief overview of the work establishing the background for the presented method will be given in this section. 2.1 Gold experiments To investigate the upper bounds of preordering in terms of quality and integration with translation systems, several researchers have performed studies with gold reorderings. Khalilov and Sima’an (2012), as well as Herrmann et al. (2013) compare various systems and provide oracle scores for syntax-based preordering models. These studies show that perfect gold reorderings estimated via automatic alignments enable translation systems enormous jumps in translation quality and further provide improvements in the size of the downstream translation models. Additionally, it was found that properties of the source syntax representation, such as how deeply phrase structure trees are nested, can significantly hamper the quality of these approaches. 2.2 Preordering with source syntax Jehl et al. (2014) learn order decisions for sibling"
W15-5704,D11-1125,0,0.0271003,"fined as (Birch et al., 2010): ∑n ∑n j=1 zij i=1 dτ (π, σ) = 1 − Z { 1 if π(i) < π(j) and σ(i) &gt; σ(j) (n2 − n) where zij = and Z = 2 0 otherwise The metric indicates the ratio of pairwise order differences between two permutations. An alternative to this ordering measure is the simulation of a full machine translation system, as first proposed by Tromble and Eisner (2009). To ensure that the changes in word order do not affect this mock translation system and to limit its complexity, the system is limited to phrases of length 1. Tuning is performed using the tuning as ranking (PRO) framework (Hopkins and May, 2011). At tuning time, k−LM and k+LM are set to 15 and 100 respectively. PRO requires the unweighted values of all feature functions; hence, during tuning only, we remember the unweighted feature values on each node and sum over intermediate values to arrive at the overall scores. Training instances for ranking are sampled from the best 100 word order predictions for each sentence in the tuning set. We perform 6 iterations and interpolate the weights of each iteration with the weights from the previous iteration by the recommended factor of Ψ = 0.1. Translation setup To evaluate the model in a full"
W15-5704,E14-1026,0,0.0351728,"Missing"
W15-5704,P07-2045,0,0.00467429,"rd order differences between ˆs′ and t should be less acute, less computationally expensive word alignment tools are sufficient to re-align the corpus. We align ˆs′ and t using fast align,4 an efficient re-parameterization of IBM model 2 (Dyer et al., 2013). Improvements in word order can lead to improvements in alignments and hence the training and word alignment process can be performed repeatedly. Lerner and Petrov (2013) report no significant improvements after the initial re-alignment. Accordingly, we do not iterate the training process either. The underlying translation system is Moses (Koehn et al., 2007) using the standard feature setup and using only the distortion-based reordering model. Tuning is performed using MERT (Och, 2003). The system is trained on the full parallel sections of the Europarl corpus (Koehn, 2005) and tuned and tested on the WMT 2009 and WMT 2010 newstest sets respectively. The language model is a 5-gram ngram model trained on the target side of Europarl and the news commentary corpus.5 4.2 Testing the effectiveness of non-local features While our preliminary results showed that the integration of a language model might be helpful, we now consider this question in more"
W15-5704,W04-3250,0,0.0265726,"U score. Table 2 shows results for this setup and for a baseline system without preordering. Both systems use a distortion limit of 7 and use only the standard 4 5 http://github.com/clab/fast_align http://statmt.org/wmt13/translation-task.html 35 Baseline Best out of k (k = 10) ∗ Distortion limit BLEU METEOR TER 7 15.20 17.26∗ 35.43 37.97∗ 66.62 62.64∗ Result is statistically significant against baseline at p < 0.05. Table 2: Estimation of the quality of the k best word order predictions. distance-based reordering model. Statistical significance tests are performed using bootstrap resampling (Koehn, 2004) and statistically significant results (p < 0.05) are marked with an asterisk. These results show that significant improvements in translation quality measured in terms of BLEU, METEOR and TER are possible based on the space of word order choices provided by our model. 4.4 Discussion Having introduced our preordering method and having evaluated the influence of non-local features, we are now interested in two basic aspects of the output space provided by this system: The first aspect is the quality of the space of the space delimited by the preordering system. Since we plan to pass the output"
W15-5704,2005.mtsummit-papers.11,0,0.0533209,"BM model 2 (Dyer et al., 2013). Improvements in word order can lead to improvements in alignments and hence the training and word alignment process can be performed repeatedly. Lerner and Petrov (2013) report no significant improvements after the initial re-alignment. Accordingly, we do not iterate the training process either. The underlying translation system is Moses (Koehn et al., 2007) using the standard feature setup and using only the distortion-based reordering model. Tuning is performed using MERT (Och, 2003). The system is trained on the full parallel sections of the Europarl corpus (Koehn, 2005) and tuned and tested on the WMT 2009 and WMT 2010 newstest sets respectively. The language model is a 5-gram ngram model trained on the target side of Europarl and the news commentary corpus.5 4.2 Testing the effectiveness of non-local features While our preliminary results showed that the integration of a language model might be helpful, we now consider this question in more detail. To test whether the language model features are beneficial to the reordering model, we compare two versions of the same system: first-best −LM is the reordering system without a language model and first-best +LM"
W15-5704,D13-1049,0,0.0855004,"in translation quality. For many language pairs, however—especially for translation into morphologically rich languages—the assumptions of these models may be too crude. But while such language pairs call for more complex models, these could increase the search space to an extent that would diminish their benefits. In this paper, we examine the question whether purely syntaxoriented adaptation models (i.e., models only considering word order) can be used as a means to delimit the search space for more complex morphosyntactic models. We propose a model based on a popular preordering algorithm (Lerner and Petrov, 2013). This novel preordering model is able to produce both n-best word order predictions as well as distributions over possible word order choices in the form of a lattice and is therefore a good fit for use by richer models taking into account aspects of both syntax and morphology. We show that the integration of non-local language model features can be beneficial for the model’s preordering quality and evaluate the space of potential word order choices the model produces. 1 Introduction A significant amount of research in machine translation has recently focused on methods for effectively restri"
W15-5704,J06-4004,0,0.108178,"Missing"
W15-5704,P09-1039,0,0.035,"e treebank collection and transformation tool HamleDT (Zeman et al., 2012) for this purpose. Model training For training the model, we mostly follow the process from Lerner and Petrov (2013). Training instances are extracted from the automatically aligned training data based on a small set of manually defined rules. To ensure high quality training data, only subtrees that are fully connected by high confidence alignments are considered. The preordering classifiers are trained on the intersection of high-confidence word alignments and the first-best output of the TurboParser dependency parser (Martins et al., 2009). The alignments are created using the Berkeley aligner2 with the hard intersection setting. This setting ensures that only high confidence alignment links are produced. While this will lead to a reduction in the number of alignment links, it creates more reliable training data for the preordering model. The dependency parser is trained to produce pseudo-projective dependency trees (Nivre and Nilsson, 2005).3 Appropriate values for k+LM and k−LM are determined using grid search. We found that beam sizes above k+LM = 15 and k−LM = 5 did not improve first-best preordering quality. Model tuning T"
W15-5704,P05-1013,0,0.0449656,"idence alignments are considered. The preordering classifiers are trained on the intersection of high-confidence word alignments and the first-best output of the TurboParser dependency parser (Martins et al., 2009). The alignments are created using the Berkeley aligner2 with the hard intersection setting. This setting ensures that only high confidence alignment links are produced. While this will lead to a reduction in the number of alignment links, it creates more reliable training data for the preordering model. The dependency parser is trained to produce pseudo-projective dependency trees (Nivre and Nilsson, 2005).3 Appropriate values for k+LM and k−LM are determined using grid search. We found that beam sizes above k+LM = 15 and k−LM = 5 did not improve first-best preordering quality. Model tuning The set of weights λ for the combination of the preordering model and the language models are optimized for a selected target metric on heldout data. The straight-forward choice for this metric is Kendall τ , which indicates the similarity of the word order of both sides. The Kendall τ distance dτ (π, σ) between two permutations π and σ is defined as (Birch et al., 2010): ∑n ∑n j=1 zij i=1 dτ (π, σ) = 1 − Z"
W15-5704,P02-1038,0,0.133973,"finite state automaton that defines the language model (Bar-Hillel et al., 1961). While this would allow for exact search, this method is found to be too slow in practice. A compromise between these two extremes is cube pruning (Chiang, 2007), in which the inner LM cost as well as the left and right LM states are stored on each node, so that it is possible to perform bottom-up dynamic programming to efficiently determine the total LM cost by combining the intermediate node costs. Keeping the properties required for performing cube pruning, we use the more general log-linear model formulation (Och and Ney, 2002) by defining the search for the best word order prediction ˆs′ as follows: ∏ ∑ ˆs′ = arg max P (s′ |s, τ )λRM PLM (s)λLM ... = arg max ϕi (s′ )λi = arg max λi log ϕi (s′ ) s′ s′ i s′ i On every source tree node, cube pruning is performed with a beam size of k+LM word order predictions. The best k−LM preordering labels are considered for expansion. Additionally, we prune all preordering labels for which the language model cost is higher than the language model cost of the original source tree order (i.e., performing no reordering). To make individual configurations comparable, we follow Chiang"
W15-5704,P03-1021,0,0.0273743,"the corpus. We align ˆs′ and t using fast align,4 an efficient re-parameterization of IBM model 2 (Dyer et al., 2013). Improvements in word order can lead to improvements in alignments and hence the training and word alignment process can be performed repeatedly. Lerner and Petrov (2013) report no significant improvements after the initial re-alignment. Accordingly, we do not iterate the training process either. The underlying translation system is Moses (Koehn et al., 2007) using the standard feature setup and using only the distortion-based reordering model. Tuning is performed using MERT (Och, 2003). The system is trained on the full parallel sections of the Europarl corpus (Koehn, 2005) and tuned and tested on the WMT 2009 and WMT 2010 newstest sets respectively. The language model is a 5-gram ngram model trained on the target side of Europarl and the news commentary corpus.5 4.2 Testing the effectiveness of non-local features While our preliminary results showed that the integration of a language model might be helpful, we now consider this question in more detail. To test whether the language model features are beneficial to the reordering model, we compare two versions of the same sy"
W15-5704,D09-1105,0,0.28682,"oved by replacing the logistic regression classifier with a feed-forward neural network (de Gispert et al., 2015). This modification shows both improved empirical results and eliminates the need for feature engineering. Similarly, Lerner and Petrov (2013) learn classifiers to permute the tree nodes of a dependency tree. The main difference here is that the permutation of up to 6 tree nodes is predicted directly instead of predicting the orientation of individual node pairs. Figure 1 shows an example dependency tree that can serve as input to such systems. 2.3 Preordering without source syntax Tromble and Eisner (2009) apply machine learning techniques to learn ITG-like orientations (straight or inverted order) for each pair of input words in the sentence. The best reordering is then determined using a standard O(n3 ) chart parsing algorithm. Generally, systems not relying on syntactic information fill the full spectrum from simple to advanced approaches. A simple approach is the application of multiple MT systems (Costa-juss`a and Fonollosa, 2006): one MT system learns the preordering (i.e. the translation of the source sentence to its preordered form, s → s′ ) and the second MT system learns to translate"
W15-5704,J97-3002,0,0.390328,"criminative classifiers are trained to directly predict the target-side word order based on source-side dependency trees. This is done by traversing the dependency tree in a top-down fashion and predicting the target order for each tree family (a family consists of a syntactic head and its children). To address sparsity issues, two models are introduced. For each subtree, the 1-step model directly predicts the target order of the child nodes. Unlike other preordering models, which often restrict the space of possible permutations, e.g. by the permutations permissible under the ITG constraint (Wu, 1997), the space of possible permutations for each subtree is restricted to the k permutations most commonly observed in the data. The blow-up in permutation space with growing numbers of children is addressed by a second model, the 2-step model. This model decreases the number of nodes involved in any single word order decision. A binary classifier (pivot classifier, in analogy to quicksort) first predicts whether a child node should occur to the left or to the right of the head of the subtree. The order of the set of nodes to the left and to the right of the head is then directly predicted as in"
W15-5704,zeman-etal-2012-hamledt,0,0.0297559,"Missing"
W16-2213,N12-1047,0,0.02632,". Let (s, t) be a training instance consisting of a source sentence s and a target sentence t and let s′ be the target-order source sentence obtained via the word alignments. For each training instance, we select the preordered source ˆs′ as follows: In our translation experiments, we use the following experimental setup, datasets and parameters. Translation system Translation experiments are performed with a phrase-based machine translation system, a version of Moses (Koehn et al., 2007) with extended lattice support.10 We use the basic Moses features and perform 15 iterations of batch MIRA (Cherry and Foster, 2012). English–Japanese Our experiments are performed on the NTCIR-8 Patent Translation (PATMT) Task. Tuning is performed on the NTCIR-7 dev sets, and translation is evaluated on the test set from NTCIR-9. All data is tokenized (using the Moses tokenizer for English and KyTea 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the paralle"
W16-2213,P05-1066,0,0.186788,"ereas lattices enable further improvement for preordering English into the strict word order language Japanese, lattices in conjunction with our proposed lattice silver training scheme turn out to be crucial to reach satisfactory empirical performance for English–German. This result highlights that when predicting word order of free word order languages given source clues only, it is important to ensure that the word order predictions and the backend system are suitably fitted together. Many approaches to preordering have made use of syntactic representations of the source sentence, including Collins et al. (2005) who restructure the source phrase structure parse tree by applying a sequence of transformation rules. More recently, Jehl et al. (2014) learn to order sibling nodes in the source-side dependency parse tree. The space of possible permutations is explored via depth-first branch-and-bound search (Balas and Toth, 1983). In later work, the authors further improve this model by replacing the logistic regression classifier with a feed-forward neural network (de Gispert et al., 2015), which results in improved empirical results and eliminates the need for feature engineering. Lerner and Petrov (2013"
W16-2213,D08-1089,0,0.0903304,"2007) with extended lattice support.10 We use the basic Moses features and perform 15 iterations of batch MIRA (Cherry and Foster, 2012). English–Japanese Our experiments are performed on the NTCIR-8 Patent Translation (PATMT) Task. Tuning is performed on the NTCIR-7 dev sets, and translation is evaluated on the test set from NTCIR-9. All data is tokenized (using the Moses tokenizer for English and KyTea 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the parallel corpus. English–German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.11 The system is trained on all available parallel data, consisting of 4.5m sentence pairs from Europarl (Koehn, 2005), Common Crawl (Smith et al., 2013) and the News Commentary corpus. We removed all sentences longer than 80 words and tokenization and truecasing is performed using the standard Moses tokenizer and truecaser. We use a 5-gram Kneser-Ne"
W16-2213,graca-etal-2008-building,0,0.380872,"Missing"
W16-2213,W06-1609,0,0.053661,"Missing"
W16-2213,P13-2121,0,0.0729712,"Missing"
W16-2213,N15-1105,0,0.173681,"Missing"
W16-2213,2013.iwslt-papers.11,0,0.213256,"a-juss`a and Fonollosa, 2006), where a first system learns preordering and a second learns to translate the preordered sentence into the target sentence. Finally, there have been successful attempts at the automatic induction of parse trees from aligned data (DeNero and Uszkoreit, 2011) and the estimation of latent reordering grammars (Stanojevi´c and Sima’an, 2015) based on permutation trees (Zhang and Gildea, 2007). 2 Related Work Preordering has been explored from the perspective of the upper-bound achievable translation quality in several studies, including Khalilov and Sima’an (2012) and Herrmann et al. (2013), which compare various systems and provide oracle scores for syntax-based preordering models. Target-order source sentences, in which the word order is determined via automatic alignments, enable translation systems great jumps in translation quality and provide improvements in compactness and efficiency of downstream phrase-based translation models. Approaches have largely followed two directions: (1) predicting word order based on some form of source-syntactic representation and (2) approaches which do not depend on source syntax. 2.3 Lattice Translation A lattice is an acyclic finite-state"
W16-2213,E14-1026,0,0.201465,"Missing"
W16-2213,D11-1018,0,0.0161892,"ring without Source Syntax Tromble and Eisner (2009) learn to predict the orientation of any two words (straight or inverted order) using a perceptron. The search for the best reordering is performed with a O(n3 ) chart parsing algorithm. More basic approaches to syntax-less preordering include the application of multiple MT systems (Costa-juss`a and Fonollosa, 2006), where a first system learns preordering and a second learns to translate the preordered sentence into the target sentence. Finally, there have been successful attempts at the automatic induction of parse trees from aligned data (DeNero and Uszkoreit, 2011) and the estimation of latent reordering grammars (Stanojevi´c and Sima’an, 2015) based on permutation trees (Zhang and Gildea, 2007). 2 Related Work Preordering has been explored from the perspective of the upper-bound achievable translation quality in several studies, including Khalilov and Sima’an (2012) and Herrmann et al. (2013), which compare various systems and provide oracle scores for syntax-based preordering models. Target-order source sentences, in which the word order is determined via automatic alignments, enable translation systems great jumps in translation quality and provide i"
W16-2213,P09-1064,0,0.0320201,"18:18 664 18:18 608 627 646 665 (a) Linear form. (b) Minimized lattice. Figure 3: Example permutation lattice. 5 Machine Translation with Permutation Lattices 5.1 Permutation Lattices ∏ where P (d) = r∈d P (r) is the probability of a derivation, and Chart(s) is the space of all possible derivations of all possible permutation trees for source sentence s. Two main modifications to this formula are made in order to make inference fast: First, Kendall τ is used as a cost function because it decomposes well,8 which allows usage of efficient dynamic programming minimum Bayesrisk (MBR) computation (DeNero et al., 2009). Second, instead of computing the MBR derivation over the full chart, computation is done over 10,000 unbiased samples from the chart. To build the permutation lattice with this model we use the top n permutations which have the lowest expected Kendall τ cost. We call a permutation lattice for sentence s = ⟨s1 , . . . , sn ⟩ an acyclic finite-state automaton where every path from the initial state reaches an accepting state in exactly n uniquely labeled transitions. Transitions are labeled with pairs in {(i, si )ni=1 } and each path represents an arbitrary permutation of the source’s n tokens"
W16-2213,W09-2310,0,0.0203734,", 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Klein and Manning, 2001). In this work, we focus on phrase-based models. The space of translation options in standard phrase-based decoding with a distortion limit d gr"
W16-2213,P08-1115,0,0.108666,"y due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Klein and Manning, 2001). In this work, we focus on phrase-based models. The space of translation options in standard phrase-based decoding with a distortion limit d grows with O(stack size × n × 2d ) where n represents the inp"
W16-2213,W01-1812,0,0.108479,"3). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Klein and Manning, 2001). In this work, we focus on phrase-based models. The space of translation options in standard phrase-based decoding with a distortion limit d grows with O(stack size × n × 2d ) where n represents the input length, and the number of translation options is capped due to beam search (Koehn et al., 2003). With lattice input, the dependency on n is replaced by |Q |where Q is the set of states of the lattice. The stack size makes the number of translation options explored by the decoder independent of the number of transitions in the lattice. As in standard decoding, the states of a lattice can also"
W16-2213,W07-0729,0,0.0350462,"ctions: (1) predicting word order based on some form of source-syntactic representation and (2) approaches which do not depend on source syntax. 2.3 Lattice Translation A lattice is an acyclic finite-state automaton defining a finite language. A more restricted class of lattices, namely, confusion networks (Bertoldi et al., 2007), has been extensively used to pack alternative input sequences for decoding.1 However, applications mostly focused on speech translation (Ney, 1999; Bertoldi et al., 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation bo"
W16-2213,knight-al-onaizan-1998-translation,0,0.243919,"tworks (Bertoldi et al., 2007), has been extensively used to pack alternative input sequences for decoding.1 However, applications mostly focused on speech translation (Ney, 1999; Bertoldi et al., 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a general"
W16-2213,W14-0313,0,0.173881,"ge they consider. Our estimation only involves word-aligned bilingual sentence pairs with a source dependency tree. Manual alignments are available for a limited number of language pairs and often only for a diminishingly small number of sentences. Consequently the question arises, whether automatic word alignments are sufficient for this task. To answer this question, we apply our measure to a set of manually aligned as well as a larger set of automatically aligned sentence pairs. In addition to the German and Japanese alignments mentioned above, we use manual alignments for English–Italian (Farajian et al., 2014), English–French (Och and Ney, 2003), English–Spanish (Grac¸a et al., 2008) and English– Portuguese (Grac¸a et al., 2008). 3.2 Bilingual Head Direction Entropy While such a qualitative comparison provides insight into the order differences of selected language pairs, it is not straight-forward to compare across many language pairs. From a linguistic perspective, Futrell et al. (2015) use entropy to compare word order freedom in dependency corpora across various languages. While the authors observed that artifacts of the data such as treebank annotation style can hamper comparability, they foun"
W16-2213,N03-1017,0,0.0335767,"sed MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Klein and Manning, 2001). In this work, we focus on phrase-based models. The space of translation options in standard phrase-based decoding with a distortion limit d grows with O(stack size × n × 2d ) where n represents the input length, and the number of translation options is capped due to beam search (Koehn et al., 2003). With lattice input, the dependency on n is replaced by |Q |where Q is the set of states of the lattice. The stack size makes the number of translation options explored by the decoder independent of the number of transitions in the lattice. As in standard decoding, the states of a lattice can also be visited non-monotonically. However, two states in a lattice are not always connected by a path, and, in general, paths connecting two nodes might differ in length. Dyer et al. (2008) proposed to pick the shortest path between two nodes to be representative of the distance between them.2 Just like"
W16-2213,W15-2112,0,0.377773,", we can reach far more complex word orders. Crucially, our models are better predictors of word order than standard distortion-based reordering, thus we manage to decode with relatively small permutation lattices. While varying degrees of word order freedom are a well-studied topic in linguistics, word order freedom has only recently been studied from a quantitative perspective. This has been enabled partly by the increasing availability of syntactic treebanks. Kuboˇn and Lopatkov´a (2015) propose a measure of word order freedom based on a set of six common word order types (SVO, SOV, etc.). Futrell et al. (2015) define various entropy measures based on the prediction of word order given unordered dependency trees. Both approaches require a dependency treebank for each language. In practical applications such as machine translation, it is difficult to quantify the influence of word order freedom. For an arbitrary language pair, our goal is to quantify a notion of the target language’s word order freedom based only on parallel sentences and source syntax. In their head direction entropy measure, Futrell et al. (2015) approach the problem of quantifying word order freedom by measuring the difficulty of"
W16-2213,P11-2093,0,0.0267283,"ntal setup, datasets and parameters. Translation system Translation experiments are performed with a phrase-based machine translation system, a version of Moses (Koehn et al., 2007) with extended lattice support.10 We use the basic Moses features and perform 15 iterations of batch MIRA (Cherry and Foster, 2012). English–Japanese Our experiments are performed on the NTCIR-8 Patent Translation (PATMT) Task. Tuning is performed on the NTCIR-7 dev sets, and translation is evaluated on the test set from NTCIR-9. All data is tokenized (using the Moses tokenizer for English and KyTea 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the parallel corpus. English–German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.11 The system is trained on all available parallel data, consisting of 4.5m sentence pairs from Europarl (Koehn, 2005), Common Crawl (Smith et al., 2013) and the News Comme"
W16-2213,W04-3250,0,0.126483,"he training data is extracted. Table 3 shows that for English– German, lattice silver training is successful in bridging the gap between the preordering model and the alignment-based target word order, both for monotonic translation and when allowing the decoder to additionally reorder translations. 6.2 Translation Experiments Distortion limit We report lowercased BLEU (Papineni et al., 2002) and Kendall τ calculated from the forcealigned hypothesis and reference. Statistical significance tests are performed for the translation scores using the bootstrap resampling method with p-value < 0.05 (Koehn, 2004). The standard preordering systems (“first-best” in Table 2 and 4) use an additional lexicalized reordering model (MSD), while the lattice systems use only lattice distortion. For training preordered translation models, we recreate word alignments from the original MGIZA alignments and the permutation for En– De and re-align preordered and target sentences for En–Ja using MGIZA.13 Gold training Lattice silver training 0 3 21.44 21.88 21.60 21.88 Table 3: Lattice silver training (BLEU, En–De). English–Japanese Results for translation into Japanese are shown in Table 4. Discussion Although preor"
W16-2213,2005.mtsummit-papers.11,0,0.0474772,"kenizer for English and KyTea 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the parallel corpus. English–German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.11 The system is trained on all available parallel data, consisting of 4.5m sentence pairs from Europarl (Koehn, 2005), Common Crawl (Smith et al., 2013) and the News Commentary corpus. We removed all sentences longer than 80 words and tokenization and truecasing is performed using the standard Moses tokenizer and truecaser. We use a 5-gram Kneser-Ney language model, estimated using lmplz (Heafield ˆs′ = arg max overlap(ˆs′L , s′ ) ˆ s′L ∈ πk (s) where πk (s) is the set of k-best permutations predicted by the preordering model. Each ˆs′L ∈ πk (s) represents a single path through the lattice. As 10 Made available at https://github.com/ wilkeraziz/mosesdecoder. 11 http://statmt.org/wmt16/ 125 et al., 2013). The"
W16-2213,J03-1002,0,0.0184192,"volves word-aligned bilingual sentence pairs with a source dependency tree. Manual alignments are available for a limited number of language pairs and often only for a diminishingly small number of sentences. Consequently the question arises, whether automatic word alignments are sufficient for this task. To answer this question, we apply our measure to a set of manually aligned as well as a larger set of automatically aligned sentence pairs. In addition to the German and Japanese alignments mentioned above, we use manual alignments for English–Italian (Farajian et al., 2014), English–French (Och and Ney, 2003), English–Spanish (Grac¸a et al., 2008) and English– Portuguese (Grac¸a et al., 2008). 3.2 Bilingual Head Direction Entropy While such a qualitative comparison provides insight into the order differences of selected language pairs, it is not straight-forward to compare across many language pairs. From a linguistic perspective, Futrell et al. (2015) use entropy to compare word order freedom in dependency corpora across various languages. While the authors observed that artifacts of the data such as treebank annotation style can hamper comparability, they found that a simple entropy measure for"
W16-2213,P06-1146,0,0.314577,"Missing"
W16-2213,P02-1040,0,0.0985882,"alignments. performs better even when translating monotonically with a distortion limit of 0. Lattice silver training To examine the utility of the lattice silver training scheme, we train systems which differ only in the way the training data is extracted. Table 3 shows that for English– German, lattice silver training is successful in bridging the gap between the preordering model and the alignment-based target word order, both for monotonic translation and when allowing the decoder to additionally reorder translations. 6.2 Translation Experiments Distortion limit We report lowercased BLEU (Papineni et al., 2002) and Kendall τ calculated from the forcealigned hypothesis and reference. Statistical significance tests are performed for the translation scores using the bootstrap resampling method with p-value < 0.05 (Koehn, 2004). The standard preordering systems (“first-best” in Table 2 and 4) use an additional lexicalized reordering model (MSD), while the lattice systems use only lattice distortion. For training preordered translation models, we recreate word alignments from the original MGIZA alignments and the permutation for En– De and re-align preordered and target sentences for En–Ja using MGIZA.13"
W16-2213,N03-1019,0,0.0274531,"), has been extensively used to pack alternative input sequences for decoding.1 However, applications mostly focused on speech translation (Ney, 1999; Bertoldi et al., 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Kl"
W16-2213,P06-1055,0,0.0221854,"ial permutation π ′ of length k ′ by 7 Our implementation is based on http://nlg.isi. edu/software/nplm/. 123 sentence and its permutation are observed during training. The exact PET that generated this permutation is not observed and there could be (exponentially) many PETs that could have generated the observed permutation. Hence, the bracketings of potential PETs are treated as latent variables. The second source of latent variables is state splitting of non-terminals (labels that indicate how to reorder the children) in a similar way as done in monolingual parsing (Matsuzaki et al., 2005; Petrov et al., 2006; Prescher, 2005). Each latent permutation tree has many latent derivations and the generative probabilistic model needs to account for them. The probability of the observed permutation π is defined in the following way: 11:11 16:16 15:15 1 0:0 20 0:0 39 0:0 58 0:0 77 0:0 96 0:0 115 0:0 134 0:0 153 0:0 172 0:0 191 0:0 210 0:0 229 0:0 248 0:0 267 0:0 286 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 2 21 40 59 78 97 116 135 154 173 192 211 230 249 268 287 2:2 2:2 2:2 2:2 2:2 2:2 79 98 2:2 117 2:2 136 2:2 155 2:2 174 2:2 193 2:2 2:2 2:2 2:2 2:2 3:3 4 3:3 23 7:7 6:6 6 7:7 4:4 7"
W16-2213,D13-1049,0,0.419988,"g Collins et al. (2005) who restructure the source phrase structure parse tree by applying a sequence of transformation rules. More recently, Jehl et al. (2014) learn to order sibling nodes in the source-side dependency parse tree. The space of possible permutations is explored via depth-first branch-and-bound search (Balas and Toth, 1983). In later work, the authors further improve this model by replacing the logistic regression classifier with a feed-forward neural network (de Gispert et al., 2015), which results in improved empirical results and eliminates the need for feature engineering. Lerner and Petrov (2013) train classifiers to predict the permutations of up to 6 tree nodes in the source dependency tree. The authors found that by only predicting the best 20 permutations of n nodes, they could cover a large majority of the reorderings in their data. 2.2 Preordering without Source Syntax Tromble and Eisner (2009) learn to predict the orientation of any two words (straight or inverted order) using a perceptron. The search for the best reordering is performed with a O(n3 ) chart parsing algorithm. More basic approaches to syntax-less preordering include the application of multiple MT systems (Costa-"
W16-2213,P13-1135,0,0.0167523,"a 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the parallel corpus. English–German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.11 The system is trained on all available parallel data, consisting of 4.5m sentence pairs from Europarl (Koehn, 2005), Common Crawl (Smith et al., 2013) and the News Commentary corpus. We removed all sentences longer than 80 words and tokenization and truecasing is performed using the standard Moses tokenizer and truecaser. We use a 5-gram Kneser-Ney language model, estimated using lmplz (Heafield ˆs′ = arg max overlap(ˆs′L , s′ ) ˆ s′L ∈ πk (s) where πk (s) is the set of k-best permutations predicted by the preordering model. Each ˆs′L ∈ πk (s) represents a single path through the lattice. As 10 Made available at https://github.com/ wilkeraziz/mosesdecoder. 11 http://statmt.org/wmt16/ 125 et al., 2013). The language model is trained on 189m"
W16-2213,P05-1010,0,0.0104461,"ined by extending a partial permutation π ′ of length k ′ by 7 Our implementation is based on http://nlg.isi. edu/software/nplm/. 123 sentence and its permutation are observed during training. The exact PET that generated this permutation is not observed and there could be (exponentially) many PETs that could have generated the observed permutation. Hence, the bracketings of potential PETs are treated as latent variables. The second source of latent variables is state splitting of non-terminals (labels that indicate how to reorder the children) in a similar way as done in monolingual parsing (Matsuzaki et al., 2005; Petrov et al., 2006; Prescher, 2005). Each latent permutation tree has many latent derivations and the generative probabilistic model needs to account for them. The probability of the observed permutation π is defined in the following way: 11:11 16:16 15:15 1 0:0 20 0:0 39 0:0 58 0:0 77 0:0 96 0:0 115 0:0 134 0:0 153 0:0 172 0:0 191 0:0 210 0:0 229 0:0 248 0:0 267 0:0 286 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 2 21 40 59 78 97 116 135 154 173 192 211 230 249 268 287 2:2 2:2 2:2 2:2 2:2 2:2 79 98 2:2 117 2:2 136 2:2 155 2:2 174 2:2 193 2:2 2:2 2:2 2:2 2:2 3:3 4 3:3 23"
W16-2213,D15-1005,1,0.825977,"Missing"
W16-2213,tiedemann-2012-parallel,0,0.0169245,"entences. Finally, while dependency treebanks rarely cover the same corpora or even domains, our method can utilize sentences from the same or similar corpora for each language, thus minimizing potential corpus biases. Berber Turkish Hebrew German Russian French Spanish Italian Portuguese Esperanto Japanese Mandarin 0 0.2 0.4 0.6 Translation from English Figure 2 plots bilingual head direction entropy for an English source side and a set of typologically diverse languages on the target side. For each language pair, we use 18,000 sentence pairs and automatic alignments from the Tatoeba corpus (Tiedemann, 2012).6 Languages at the top of the plot in Figure 2 show a greater degree of word order freedom with respect to the English source syntax. Thus, predicting their word order from English source clues alone is likely to be difficult. We argue that in such cases it is crucial to pass on the ambiguity over the space of predictions to the translation model. By doing so, word order decisions can be influenced by translation decisions, while still shaping the space of reachable translations. Figure 2: Bilingual head direction entropy with English source side. Since a limited number of manually aligned se"
W16-2213,D09-1105,0,0.0233312,"nd search (Balas and Toth, 1983). In later work, the authors further improve this model by replacing the logistic regression classifier with a feed-forward neural network (de Gispert et al., 2015), which results in improved empirical results and eliminates the need for feature engineering. Lerner and Petrov (2013) train classifiers to predict the permutations of up to 6 tree nodes in the source dependency tree. The authors found that by only predicting the best 20 permutations of n nodes, they could cover a large majority of the reorderings in their data. 2.2 Preordering without Source Syntax Tromble and Eisner (2009) learn to predict the orientation of any two words (straight or inverted order) using a perceptron. The search for the best reordering is performed with a O(n3 ) chart parsing algorithm. More basic approaches to syntax-less preordering include the application of multiple MT systems (Costa-juss`a and Fonollosa, 2006), where a first system learns preordering and a second learns to translate the preordered sentence into the target sentence. Finally, there have been successful attempts at the automatic induction of parse trees from aligned data (DeNero and Uszkoreit, 2011) and the estimation of la"
W16-2213,J97-3002,0,0.221249,"he input layer, 2 on the output layer, and 50 and 100 on the two hidden layers. We use a learning rate of 0.01, batch size of 1000 and perform 20 training epochs. 4.2 Reordering Grammar Induction Reordering Grammar (RG) (Stanojevi´c and Sima’an, 2015) is a recent approach for preordering that is hierarchical and fully unsupervised. It is based on inducing a probabilistic context-free grammar from aligned parallel data. This grammar can predict permutation trees (PETs) (Zhang and Gildea, 2007) — projective constituency trees that can fully describe any permutation. PETs are reminiscent of ITG (Wu, 1997) with the important distinction that PETs can handle any permutation, unlike ITG which can only handle binarizable ones. As in ITG, constituents in PETs are labeled with the permutation of their children. Induction of RGs is performed by specifying a generative probabilistic model and then estimating its parameters using the EM algorithm. The reasoning behind using EM is that many latent variables are present in the model. Only the source Search Search in this model consists of finding the sequence of swaps leading to the best overall score according to the model. Let a partial permutation of"
W16-2213,2005.iwslt-1.18,0,0.0371916,"followed two directions: (1) predicting word order based on some form of source-syntactic representation and (2) approaches which do not depend on source syntax. 2.3 Lattice Translation A lattice is an acyclic finite-state automaton defining a finite language. A more restricted class of lattices, namely, confusion networks (Bertoldi et al., 2007), has been extensively used to pack alternative input sequences for decoding.1 However, applications mostly focused on speech translation (Ney, 1999; Bertoldi et al., 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice t"
W16-2213,zeman-etal-2012-hamledt,0,0.031049,"Missing"
W16-2213,W07-0404,0,0.181631,"a perceptron. The search for the best reordering is performed with a O(n3 ) chart parsing algorithm. More basic approaches to syntax-less preordering include the application of multiple MT systems (Costa-juss`a and Fonollosa, 2006), where a first system learns preordering and a second learns to translate the preordered sentence into the target sentence. Finally, there have been successful attempts at the automatic induction of parse trees from aligned data (DeNero and Uszkoreit, 2011) and the estimation of latent reordering grammars (Stanojevi´c and Sima’an, 2015) based on permutation trees (Zhang and Gildea, 2007). 2 Related Work Preordering has been explored from the perspective of the upper-bound achievable translation quality in several studies, including Khalilov and Sima’an (2012) and Herrmann et al. (2013), which compare various systems and provide oracle scores for syntax-based preordering models. Target-order source sentences, in which the word order is determined via automatic alignments, enable translation systems great jumps in translation quality and provide improvements in compactness and efficiency of downstream phrase-based translation models. Approaches have largely followed two directi"
W16-2213,W07-0401,0,0.0776075,"Missing"
W16-2213,P07-2045,0,\N,Missing
W16-2330,C14-1182,1,0.919895,"Missing"
W16-2330,N15-1043,1,0.838383,"Missing"
W16-2330,Q16-1008,1,0.837161,"Missing"
W16-2330,W11-2107,0,0.0195071,"ural, as the most effective adaptation method always comes from providing more in-domain data. 4 Results Our baseline, as described earlier, is created from the concatenation of all parallel data provided 425 by the organizer. The language models are also trained by concatenating all monolingual data provided by the organizer. The baseline has 17 translation and language modeling features in total. Meanwhile, our system has 23 features (17 + 6 adapted features). Table 4 and 5 present translation results on the internal dev and test sets respectively, with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006) and finally BEER (Stanojevi´c and Sima’an, 2014). System Baseline Scorpio English-Dutch BLEU METEOR 28.1 28.7 30.1 29.9 TER 53.3 51.6 BEER 18.4 20.9 Table 4: Results on Dev set System Baseline Scorpio English-Dutch BLEU METEOR 34.5 32.9 36.8 34.5 TER 45.3 43.1 References Daniel Cer, Michel Galley, Daniel Jurafsky, and Christopher D. Manning. 2010. Phrasal: A toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features. In NAACL HLT 2010 Demonstration Session. Colin Cherry and George Foster. 2012. Batch tuni"
W16-2330,D08-1089,0,0.0154349,"ive; (2): a quite big difference between BLEU scores on the final test set and our (admittedly small) validation set. This suggests that there is still lots of room for improving our final translations with better de-tokenization. However, this was not the focus of our submission. 3 System Description We first train a baseline with standard phrasebased system, using all the parallel data, i.e. the concatenation of in-domain and general-domain data. The system includes MOSES (Koehn et al., 2007) baseline feature functions, plus eight hierarchical lexicalized reordering model feature functions (Galley and Manning, 2008). The training data is first word-aligned using GIZA++ (Och and Ney, 2003) and then symmetrized with grow(diag)-final-and (Koehn et al., 2003). We limit the phrase length to a maximum of seven words. Somewhat surprisingly, we find that increasing the maximum number of words for phrases from three to seven significantly improves the baseline on the adaptation task. We believe this is quite important. It suggests that for validating domain adaptation methods over a phrase-based system, 1 However, we are not sure whether these “heuristics” rules are correct or not, as there is no way to verify th"
W16-2330,P13-2121,0,0.0295566,"Missing"
W16-2330,N03-1017,0,0.0307166,"ill lots of room for improving our final translations with better de-tokenization. However, this was not the focus of our submission. 3 System Description We first train a baseline with standard phrasebased system, using all the parallel data, i.e. the concatenation of in-domain and general-domain data. The system includes MOSES (Koehn et al., 2007) baseline feature functions, plus eight hierarchical lexicalized reordering model feature functions (Galley and Manning, 2008). The training data is first word-aligned using GIZA++ (Och and Ney, 2003) and then symmetrized with grow(diag)-final-and (Koehn et al., 2003). We limit the phrase length to a maximum of seven words. Somewhat surprisingly, we find that increasing the maximum number of words for phrases from three to seven significantly improves the baseline on the adaptation task. We believe this is quite important. It suggests that for validating domain adaptation methods over a phrase-based system, 1 However, we are not sure whether these “heuristics” rules are correct or not, as there is no way to verify them. the system itself should be built over phrases with a reasonable maximum length (e.g. seven words). We use the phrase extraction component"
W16-2330,2005.mtsummit-papers.11,0,0.0629089,"p much for this language pair. Several additional adapted features proposed in Cuong and Sima’an (2015) and Cuong et al. (2016) are also deployed, including domain-specific and domain-invariant translation features. Despite the simplicity of our adaptation models, our results show effective adaptation performance for the task. This system consolidates the Data English-Dutch Sents 211K Words 1.69M 1.65M Sents 1.95M Words 52.60M 52.95M Sents 1800 Words 41.35K 42.06K Sents 200 Words 6.4K 6.3K Table 1: Data Preparation. More specifically, we use the European Parliament (Europarl) parallel corpus (Koehn, 2005) as general-domain data. We use the corpora of ITrelated terms from Wikipedia and Localization PO files as the in-domain data. For training Dutch language models we use the monolingual Dutch side of Europarl, together with in-domain data. We split the provided development data (2K sentence pairs) into two different internal datasets: • A dev set of 1800 sentence pairs used for system optimization. • A test set of 200 sentence pairs used for evaluation. Preprocessing All the data is preprocessed before training. For preprocessing we remove all sentences that have 423 Proceedings of the First Co"
W16-2330,J03-1002,0,0.0113835,"r (admittedly small) validation set. This suggests that there is still lots of room for improving our final translations with better de-tokenization. However, this was not the focus of our submission. 3 System Description We first train a baseline with standard phrasebased system, using all the parallel data, i.e. the concatenation of in-domain and general-domain data. The system includes MOSES (Koehn et al., 2007) baseline feature functions, plus eight hierarchical lexicalized reordering model feature functions (Galley and Manning, 2008). The training data is first word-aligned using GIZA++ (Och and Ney, 2003) and then symmetrized with grow(diag)-final-and (Koehn et al., 2003). We limit the phrase length to a maximum of seven words. Somewhat surprisingly, we find that increasing the maximum number of words for phrases from three to seven significantly improves the baseline on the adaptation task. We believe this is quite important. It suggests that for validating domain adaptation methods over a phrase-based system, 1 However, we are not sure whether these “heuristics” rules are correct or not, as there is no way to verify them. the system itself should be built over phrases with a reasonable maxim"
W16-2330,P02-1040,0,0.09483,"ess, we believe this is very natural, as the most effective adaptation method always comes from providing more in-domain data. 4 Results Our baseline, as described earlier, is created from the concatenation of all parallel data provided 425 by the organizer. The language models are also trained by concatenating all monolingual data provided by the organizer. The baseline has 17 translation and language modeling features in total. Meanwhile, our system has 23 features (17 + 6 adapted features). Table 4 and 5 present translation results on the internal dev and test sets respectively, with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006) and finally BEER (Stanojevi´c and Sima’an, 2014). System Baseline Scorpio English-Dutch BLEU METEOR 28.1 28.7 30.1 29.9 TER 53.3 51.6 BEER 18.4 20.9 Table 4: Results on Dev set System Baseline Scorpio English-Dutch BLEU METEOR 34.5 32.9 36.8 34.5 TER 45.3 43.1 References Daniel Cer, Michel Galley, Daniel Jurafsky, and Christopher D. Manning. 2010. Phrasal: A toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features. In NAACL HLT 2010 Demonstration Session. Colin Cherry"
W16-2330,E12-1055,0,0.0196462,"provided in-domain data (Indomain). Note that our biased translation models are sharp in terms of having low entropies in translation distributions. Meanwhile, the translation statistics we induce from the in-domain are even sharper. Meanwhile, the translation statistics we induce from the in-domain are even sharper. Our 424 experience suggests the statistics induced from indomain data still incrementally contributes to the adaptation. We combine all three different types of translation models together. The combination is optimized over the (internal) development set using linear combination (Sennrich, 2012). To have an idea what the combining weights look like, Table 2 presents results for four translation features, i.e. the translation models (TM) and lexical weights (LEX) in both directions (ennl and nl-en). Combining weights for translation features Concat. Weighted In-Domain TM en-nl 0.002 0.724 0.274 Lex en-nl 0.001 0.594 0.405 TM nl-en 0.002 0.755 0.243 0.573 0.426 Lex nl-en 0.001 Table 2: Combining weights We see that most of the adaptation is credited to the models trained with biased weighting. The models trained on the in-domain data still partially contributes to the adaptation. On th"
W16-2330,2006.amta-papers.25,0,0.103109,"ation method always comes from providing more in-domain data. 4 Results Our baseline, as described earlier, is created from the concatenation of all parallel data provided 425 by the organizer. The language models are also trained by concatenating all monolingual data provided by the organizer. The baseline has 17 translation and language modeling features in total. Meanwhile, our system has 23 features (17 + 6 adapted features). Table 4 and 5 present translation results on the internal dev and test sets respectively, with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006) and finally BEER (Stanojevi´c and Sima’an, 2014). System Baseline Scorpio English-Dutch BLEU METEOR 28.1 28.7 30.1 29.9 TER 53.3 51.6 BEER 18.4 20.9 Table 4: Results on Dev set System Baseline Scorpio English-Dutch BLEU METEOR 34.5 32.9 36.8 34.5 TER 45.3 43.1 References Daniel Cer, Michel Galley, Daniel Jurafsky, and Christopher D. Manning. 2010. Phrasal: A toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features. In NAACL HLT 2010 Demonstration Session. Colin Cherry and George Foster. 2012. Batch tuning strategies for statistic"
W16-2330,W14-3354,1,0.900629,"Missing"
W16-2330,P14-1072,0,0.0150549,"the model trained on the simple concatenation of the data does not contribute much. 3.2 Biasing language models Along with biasing the translation models, we find it useful to bias the language models as well. With similar simple latent domain variable models (but in this case, trained on target side data only), we learn the relevance of each sentence with respect to the target domain. We train 3gram language models with relevance weights. To avoid overfitting, we find that it is necessary to apply an expected smoothing approach in training. We choose expected Kneser-Ney smoothing technique (Zhang and Chiang, 2014) as it is simple and achieves state-of-the-art performance on the language modeling problem. Note that we also train a 3-gram language model directly on the provided in-domain data, as well as another one trained on the concatenation of in- and general- domain data. This results in three different language models, similar to the three translation models we trained above. They are treated as separate dense features for our system. We provide the combining weights (after tuning) in Table 3, in order to demonstrate the relative importance of the different language models. Tuning weights for langu"
W16-2330,P07-2045,0,\N,Missing
W16-2330,D14-1062,1,\N,Missing
W16-2330,N04-1021,0,\N,Missing
W16-2330,N12-1047,0,\N,Missing
W16-2346,P14-2074,1,0.681895,"https://github.com/Z -TANG/re-scorer. 10 https://github.com/elliottd/Grounded Translation 11 548 https://github.com/jhclark/multeval 4.2 System submissions that preserved casing or had been tokenised were further processed for lowercasing and detokenisation.12 For all of these preprocessing steps, we used Moses scripts.13 Task 2: Crosslingual Image Description Table 4 presents the final results for the Crosslingual Image Description task. Meteor is the primary evaluation measure because it has been shown to have a much stronger correlation with human judgements than BLEU or TER for this task (Elliott and Keller, 2014). The data for this task was lowercased and had punctuation removed where necessary. The strongest performing constrained submission (LIUM 2 TextNMT C) does not use any visual features. Including multimodal features (i.e., LIUM 2 MultimodalNMT C) results in a 2.8 Meteor drop in performance for that model type. The baseline system 2 GroundedTranslation C outperformed all but these two systems. In general, there is a wide range of performances, and an intriguing discrepancy between Meteor and BLEU rankings. This discrepancy was much larger than the one observed in Task 1, where the overall ranki"
W16-2346,W16-3210,1,0.489205,"Missing"
W16-2346,W15-3001,1,0.437372,"Missing"
W16-2346,N10-1125,0,0.0165056,"he following main goals: • To push existing work on multimodal language processing towards multilingual multimodal language processing. Introduction • To investigate the effectiveness of information from images in machine translation. In recent years, significant research has been done to address problems that require joint modelling of language and vision. Examples of popular applications involving both Natural Language Processing (NLP) and Computer Vision (CV) include image description generation and video captioning (Bernardi et al., 2016), image retrieval based on textual and visual cues (Feng and Lapata, 2010), visual question answering (Yang et al., 2015), among many others (see (Ramisa et al., 2016) for more examples). With very few exceptions (Grubinger et al., 2006; Funaki and Nakayama, 2015; • To investigate the effectiveness of crosslingual textual information in image description generation. The challenge was organised in the framework of the well-established WMT series of shared tasks.1 Participants were called to submit systems focusing on either or both of these task variants. The tasks differ in the training data and in 1 http://www.statmt.org/wmt16/ 543 Proceedings of the First Conferen"
W16-2346,W16-2358,0,0.527348,"Missing"
W16-2346,D15-1070,0,0.0302355,"information from images in machine translation. In recent years, significant research has been done to address problems that require joint modelling of language and vision. Examples of popular applications involving both Natural Language Processing (NLP) and Computer Vision (CV) include image description generation and video captioning (Bernardi et al., 2016), image retrieval based on textual and visual cues (Feng and Lapata, 2010), visual question answering (Yang et al., 2015), among many others (see (Ramisa et al., 2016) for more examples). With very few exceptions (Grubinger et al., 2006; Funaki and Nakayama, 2015; • To investigate the effectiveness of crosslingual textual information in image description generation. The challenge was organised in the framework of the well-established WMT series of shared tasks.1 Participants were called to submit systems focusing on either or both of these task variants. The tasks differ in the training data and in 1 http://www.statmt.org/wmt16/ 543 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 543–553, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics translate Ein brauner Hund ..."
W16-2346,W16-2359,1,0.497561,"Missing"
W16-2346,P11-2031,0,0.0597769,"fter that, an SVM-based model decides which one is better according to the sentence’s score from a language model and the score from the model that generated the sentence. The only difference between the two submissions is that the unconstrained one used Task 1 dataset in the training of text translator. 4 Results Tables 3 and 4 present the official results for the Multimodal Machine Translation and Crosslingual Image Description tasks. We evaluated the submissions based on Meteor (Denkowski and Lavie, 2014) (primary), BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) using MultEval (Clark et al., 2011)11 with default parameters. 4.1 Baseline - GroundedTranslation (Tasks 1 & 2) This method follows (Elliott et al., 2015):10 A source language multimodal RNN model is initialised with a visual feature vector (i.e., a multimodal model for the source language). The final hidden state is then used to initialise a target Task 1 Table 4 shows the final results for the Multimodal Machine Translation task on the official test set, where systems are ranked by their Meteor scores. Meteor, BLEU and TER were computed based on the single reference (human translation) provided for the test set. For Meteor, w"
W16-2346,P16-1227,0,0.269311,"ken classes (Stewart et al., 2014). 4 http://github.com/BVLC/caffe/releases /tag/rc2 5 http://github.com/karpathy/neuraltalk /tree/master/matlab_features_reference 6 https://glosbe.com/en/de/ http://www.crowdflower.com http://illinois.edu/fb/sec/229675 545 ID CMU+NTU CUNI DCU DCU-UVA HUCL IBM-IITM-Montreal-NYU LIUM SHEF UPC UPCb Participating team Carnegie Melon University (Huang et al., 2016) Univerzita Karlova v Praze (Libovick´y et al., 2016) Dublin City University (Hokamp and Calixto, 2016) Dublin City University & Universiteit van Amsterdam (Calixto et al., 2016) Universit¨at Heidelberg (Hitschler et al., 2016) IBM Research India, IIT Madras, Universit´e de Montr´eal & New York University Laboratoire d’Informatique de l’Universit´e du Maine (Caglayan et al., 2016) University of Sheffield (Shah et al., 2016) Universitat Polit`ecnica de Catalunya (Rodr´ıguez Guasch and Costa-juss`a, 2016) Universitat Polit`ecnica de Catalunya Table 2: Participants in the WMT16 multimodal machine translation shared task. DCU (Task 1) Both submissions from DCU are neural MT systems with an attention mechanism on the source-side representation (Bahdanau et al., 2014). The first submission is text-only, and the second sub"
W16-2346,W14-3348,0,0.307313,", 2014). The other one is generated based on the image feature using method proposed in (Vinyals et al., 2015). After that, an SVM-based model decides which one is better according to the sentence’s score from a language model and the score from the model that generated the sentence. The only difference between the two submissions is that the unconstrained one used Task 1 dataset in the training of text translator. 4 Results Tables 3 and 4 present the official results for the Multimodal Machine Translation and Crosslingual Image Description tasks. We evaluated the submissions based on Meteor (Denkowski and Lavie, 2014) (primary), BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) using MultEval (Clark et al., 2011)11 with default parameters. 4.1 Baseline - GroundedTranslation (Tasks 1 & 2) This method follows (Elliott et al., 2015):10 A source language multimodal RNN model is initialised with a visual feature vector (i.e., a multimodal model for the source language). The final hidden state is then used to initialise a target Task 1 Table 4 shows the final results for the Multimodal Machine Translation task on the official test set, where systems are ranked by their Meteor scores. Meteor, BLEU and TE"
W16-2346,P16-1159,0,0.00608079,"Missing"
W16-2346,W16-2360,0,0.440474,"Missing"
W16-2346,2006.amta-papers.25,0,0.156429,"proposed in (Vinyals et al., 2015). After that, an SVM-based model decides which one is better according to the sentence’s score from a language model and the score from the model that generated the sentence. The only difference between the two submissions is that the unconstrained one used Task 1 dataset in the training of text translator. 4 Results Tables 3 and 4 present the official results for the Multimodal Machine Translation and Crosslingual Image Description tasks. We evaluated the submissions based on Meteor (Denkowski and Lavie, 2014) (primary), BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) using MultEval (Clark et al., 2011)11 with default parameters. 4.1 Baseline - GroundedTranslation (Tasks 1 & 2) This method follows (Elliott et al., 2015):10 A source language multimodal RNN model is initialised with a visual feature vector (i.e., a multimodal model for the source language). The final hidden state is then used to initialise a target Task 1 Table 4 shows the final results for the Multimodal Machine Translation task on the official test set, where systems are ranked by their Meteor scores. Meteor, BLEU and TER were computed based on the single reference (human translation) prov"
W16-2346,P07-2045,0,0.0201639,"five descriptions of the same image in the target language, created independently from the corresponding source description (image description variant). The data used for both tasks is an extended version of the Flickr30K dataset. Participants were also allowed to use external data and resources for unconstrained submissions. Participants were encouraged to make use of both the sentences and the images as part of their submissions but they were not required to do so. The baseline systems for the translation task were a text-only Moses phrase-based statistical machine translation (SMT) model (Koehn et al., 2007) and the GroundedTranslation multilingual image description model (Elliott et al., 2015) (in particular, the MLM→LM variant). The baseline system for the description generation task was also the GroundedTranslation model. In this paper we describe the data, image features and participants of the shared task (Sections 2 and 3), present its main findings (Section 4), and discuss interesting issues and directions for future research (Section 5). 2 courage participation we released two types of features extracted from the images. The use of such features was not mandatory, and participants could a"
W16-2346,2014.amta-researchers.3,0,0.0922404,"Missing"
W16-2346,W16-2361,0,0.218958,"Missing"
W16-2346,P02-1040,0,0.10153,"1) The approach integrates separate attention mechanisms over the source language and the CONV5,4 visual features in a single decoder. The source language was represented using a bidirectional RNN with Gated Recurrent Units (GRU); the images were represented as 196x512 matrix from the pre-trained VGG-19 convolutional network. A separate, time546 VGG16 deep convolutional model (Simonyan and Zisserman, 2015), supplied by the task organisers) for retrieval of matches. The retrieval model architecture was identical to that in Hitschler et al. (2016). Instead of TF-IDF, a modified version of BLEU (Papineni et al., 2002) was used in order to re-score hypotheses based on the target-language text of retrieved captions. Fixed settings were used for some parameters (d = 90, b = 0.01 and km = 20), while kr and λ were optimised on the validation set (parameters as defined in (Hitschler et al., 2016)). LIUM 1 MosesNMTRnnLMSent2Vec C and LIUM 1 MosesNMTRnnLMSent2VecVGGFC7 C are phrase-based systems based on Moses (14 standard features plus operation sequence models. They include re-scoring with several models and more particularly with a continuous space language model (CSLM) and a neural MT system (see TextNMT syste"
W16-2346,Q14-1006,0,0.409641,"a regionbased convolution neural network (RCNN) are designed to be appended in the head/tail of the textual feature or dissipated in parallel long short term memory (LSTM) threads to assist the LSTM reader in computing a representation. For rescoring, an additional bilingual dictionary is used to select the best sentence from candidates generated by five different models. The submission is thus unconstrained, with the German-English Dictionary from GLOSBE6 used as additional resource. Datasets and image features We created a new dataset for the shared task by extending the Flickr30K dataset (Young et al., 2014) into another language. The Multi30K dataset (Elliott et al., 2016) contains two types of multilingual data: a corpus of English sentences translated into German (used for Task 1), and a corpus of independently collected English and German sentences (used for Task 2). For the translation corpus, one sentence (of five) was chosen for professional translation such that the final dataset is a combination of short, medium, and long length sentences. The second corpus consists of crowdsourced descriptions gathered from Crowdflower,2 where each worker generated an independent description of the imag"
W16-2346,W16-2363,1,0.301031,"uilt using only the shared task data. This is a remarkable result, given the size of the dataset: 29,000 parallel segments. They all use additional features to re-rank the k-best output of a text-only phrase-based system, including visual features, although these seem to play a minor role and lead to only marginally better results. Submissions based on the output of a Moses translation model – like the main baseline (1 en-de-Moses C) – have very similar Meteor scores. In fact, SHEF 1 en-de-Moses-rerank C and CMU+NTU 1 MNMT+RERANK U are not considered significantly different from this baseline Shah et al. (2016) provide some analysis on the differences between SHEF 1 en-de-Mosesrerank C and 1 en-de-Moses C. They show that the output of these systems differ in 260 out of the 1,000 segments. However, despite differences in the actual translations, the Meteor scores for many of these cases may be the same/close. Disappointingly, truly multimodal systems, which in most cases use neural MT approaches (e.g. CUNI 1 MMS2S-1 C, DCU 1 min-riskmultimodal C) do not fare as well as the text-only SMT systems (or those followed by multimodalbased translation rescoring), except when additional resources are used for"
W16-2346,N12-1017,0,\N,Missing
W16-2346,P10-4002,0,\N,Missing
W16-2346,W16-2362,0,\N,Missing
W16-3210,P05-1033,0,0.0128193,"relationship between the sentences in different languages. In the translated corpus, we know there is a strong correspondence between the sentences in both languages. In the descriptions corpus, we only know that the sentences, regardless of the language, are supposed to describe the same image. A dataset of images paired with sentences in multiple languages broadens the scope of multimodal NLP research. Image description with multilingual data can also be seen as machine translation in a multimodal context. This opens up new avenues for researchers in machine translation (Koehn et al., 2003; Chiang, 2005; Sutskever et al., 2014; Bahdanau et al., 2015, inter-alia) to work with multilingual multimodal data. Image– sentence ranking using monolingual multimodal datasets (Hodosh et al., 2013, inter-alia) is also a natural task for multilingual modelling. The only existing datasets of images paired with multilingual sentences were created by professionally translating English into the target language: IAPR-TC12 with 20,000 English-German described images (Grubinger et al., 2006), and the Pascal Sentences Dataset of 1,000 JapaneseEnglish described images (Funaki and Nakayama, 2015). Multi30K dataset"
W16-3210,D13-1128,1,0.045395,"Missing"
W16-3210,D15-1070,0,0.217749,"ranslation (Koehn et al., 2003; Chiang, 2005; Sutskever et al., 2014; Bahdanau et al., 2015, inter-alia) to work with multilingual multimodal data. Image– sentence ranking using monolingual multimodal datasets (Hodosh et al., 2013, inter-alia) is also a natural task for multilingual modelling. The only existing datasets of images paired with multilingual sentences were created by professionally translating English into the target language: IAPR-TC12 with 20,000 English-German described images (Grubinger et al., 2006), and the Pascal Sentences Dataset of 1,000 JapaneseEnglish described images (Funaki and Nakayama, 2015). Multi30K dataset is larger than both of these and contains both independent and translated sentences. We hope this dataset will be of broad interest across NLP and CV research and anticipate that these communities will put the data to use in a broader range of tasks than we can foresee. 70 Proceedings of the 5th Workshop on Vision and Language, pages 70–74, c Berlin, Germany, August 12 2016. 2016 Association for Computational Linguistics 1. Brick layers constructing a wall. 1. The two men on the scaffolding are helping to build a red brick wall. 2. Maurer bauen eine Wand. 2. Zwei Mauerer mau"
W16-3210,P16-1227,0,0.103727,"slation Machine translation is typically performed using only textual data, for example news data, the Europarl corpora, or corpora harvested from the Web (CommonCrawl, Wikipedia, etc.). The Multi30K dataset makes it possible to further develop ma73 chine translation in a setting where multimodal data, such as images or video, are observed alongside text. The potential advantages of using multimodal information for machine translation include the ability to better deal with ambiguous source text and to avoid (untranslated) out-of-vocabulary words in the target language (Calixto et al., 2012). Hitschler and Riezler (2016) have demonstrated the potential of multimodal features in a targetside translation reranking model. Their approach is initially trained over large text-only translation copora and then fine-tuned with a small amount of in-domain data, such as our dataset. We expect a variety of translation models can be adapted to take advantage of multimodal data as features in a translation model or as feature vectors in neural machine translation models. 4 Conclusions We introduced Multi30K: a large-scale multilingual multimodal dataset for interdisciplinary machine learning research. Our dataset is an ext"
W16-3210,N03-1017,0,0.00435756,"hese corpora is the relationship between the sentences in different languages. In the translated corpus, we know there is a strong correspondence between the sentences in both languages. In the descriptions corpus, we only know that the sentences, regardless of the language, are supposed to describe the same image. A dataset of images paired with sentences in multiple languages broadens the scope of multimodal NLP research. Image description with multilingual data can also be seen as machine translation in a multimodal context. This opens up new avenues for researchers in machine translation (Koehn et al., 2003; Chiang, 2005; Sutskever et al., 2014; Bahdanau et al., 2015, inter-alia) to work with multilingual multimodal data. Image– sentence ranking using monolingual multimodal datasets (Hodosh et al., 2013, inter-alia) is also a natural task for multilingual modelling. The only existing datasets of images paired with multilingual sentences were created by professionally translating English into the target language: IAPR-TC12 with 20,000 English-German described images (Grubinger et al., 2006), and the Pascal Sentences Dataset of 1,000 JapaneseEnglish described images (Funaki and Nakayama, 2015). Mu"
W16-3210,W10-0721,0,0.0981994,"Missing"
W16-3210,W16-2346,1,0.477489,"Missing"
W16-3210,Q14-1006,0,0.793454,"s crowdsourced independently of the original English descriptions. We describe the data and outline how it can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks. 1 Introduction Image description is one of the core challenges at the intersection of Natural Language Processing (NLP) and Computer Vision (CV) (Bernardi et al., 2016). This task has only received attention in a monolingual English setting, helped by the availability of English datasets, e.g. Flickr8K (Hodosh et al., 2013), Flickr30K (Young et al., 2014), and MS COCO (Chen et al., 2015). However, the possible applications of image description are useful for all languages, such as searching for images using natural language, or providing alternativedescription text for visually impaired Web users. We introduce a large-scale dataset of images paired with sentences in English and German as an initial step towards studying the value and the characteristics of multilingual-multimodal data1 . 1 The dataset is freely available under the Creative Commons Attribution NonCommercial ShareAlike 4.0 International license from http://www.statmt.org/wmt 16/"
W16-3210,W10-0707,0,\N,Missing
W16-6402,2011.eamt-1.38,0,0.0195123,"ation models (Koehn et al., 2003) by allowing phrase pairs to rewrite through a Synchronous CFG mechanism. Rewriting is unconstrained, and the model thus learns local dependencies and reorderings in a very general manner. This lack of restrictions allows the grammar to achieve good coverage, but begs the question of how to guide Hiero with linguistic information. Since SAMT (Zollmann and Venugopal, 2006), a branch of work has focused on labelling Hiero, with different types of labels: phrase-structure labels (Zollmann and Venugopal, 2006), dependency head labels (Li et al., 2012), CCG labels (Almaghout et al., 2011), (non-syntactic) hierarchical alignment labels (Maillette de Buy Wenniger and Sima’an, 2013), etc. Most of these models use large nonterminal vocabularies, as syntactic labels or POS tags are combined into phrase labels. The general character of Hiero is balanced by constraints on the extraction and the form of rules, and another branch of work has focused on rebalancing such constraints. For instance, Li et al. (2013) constrain rewriting to constituents or sequences of constituents, allowing them to relax phrase length at extraction. Perhaps the most obvious limitation of Hiero is its limite"
W16-6402,2012.eamt-1.63,1,0.892072,"Missing"
W16-6402,W14-4019,1,0.881423,"Missing"
W16-6402,P11-1103,0,0.0255928,"three times. The LR-KB1 scores were computed giving equal weight to BLEU-1 and Kendall’s tau (α = 0.5) While extending extraction spans with the adj model decreases performance, both labelling and the base adjunction features allow to guide decoding in the adjunct-driven models, and outperform Hiero, both in terms of BLEU and BEER (Stanojevi´c and Sima’an, 2014). The highest improvements are obtained for the models employing both labelling and features, with little or no difference between the full-label model adj-FL and the label-to-feature model adj-L2F. The lack of improvement in LR score (Birch and Osborne, 2011) suggest that the adjunct-driven models improve lexical selection rather than reordering. Effect of training set size Table 5 presents results for the larger English-Chinese data set (2M training sentence pairs). With a larger data set, relaxing the decoding span for Hiero (H-100) is beneficial for English-Chinese–locally learned rules are useful when applied to larger spans. As before, extending extraction spans alone decreases performance, but labels and features allow to guide the model and improve performance; the adj-L2F model outperforms Hiero by 0.6 BLEU point. Table 5: Experimental res"
W16-6402,N12-1047,0,0.059534,"Missing"
W16-6402,P00-1058,0,0.144841,"re corresponding to the probability that a rule was extracted from a (shorter) phrase pair violating the non-adjunct-crossing constraint. Besides, we tested a version of the model with a simplified labelling for adjunct sequences. These sequences are then labelled with A instead of Ax, while their size x appears in the following feature: fx = e1−x (1) For other rules (adjuncts and other phrase pairs), fx is taken to be 1. 3.4 Factoring out adjuncts TAG factors adjunction by extracting auxiliary trees and initial trees separately (Joshi and Schabes, 1997). This leads to a more compact grammar (Chiang, 2000) that is able to generate unseen adjunction patterns. Synchronous Tree Adjunction Grammar (STAG) (Shieber and Schabes, 1990) applies TAG to translation, and DeNeefe and Knight (2009) propose a probabilistic implementation for string-to-tree translation. Their model identifies target-side adjuncts and takes their projection on the source side as a basis for auxilliary-tree extraction. In the case of Hiero, one cannot directly implement STAG, as CFG rules do not have the (tree) structure that is necessary for modelling adjunction. One can still however extract generalized versions of rules, by f"
W16-6402,P05-1033,0,0.0982938,"r these models. But as Shieber (2007) points out, capturing this distinction allows to abstract over ‘intervening’ adjuncts, and is thus relevant for (machine) translation in general. We contribute an adjunction-driven approach to hierarchical phrase-based modelling that uses source-side adjuncts to relax extraction constraints–allowing to capturing long-distance dependencies–, and to guide translation through labelling. The labelling scheme can be reduced to two adjunct/non-adjunct labels, and improves translation over Hiero by up to 0.6 BLEU points for English-Chinese. 1 Introduction Hiero (Chiang, 2005) extends phrase-based Statistical Machine Translation models (Koehn et al., 2003) by allowing phrase pairs to rewrite through a Synchronous CFG mechanism. Rewriting is unconstrained, and the model thus learns local dependencies and reorderings in a very general manner. This lack of restrictions allows the grammar to achieve good coverage, but begs the question of how to guide Hiero with linguistic information. Since SAMT (Zollmann and Venugopal, 2006), a branch of work has focused on labelling Hiero, with different types of labels: phrase-structure labels (Zollmann and Venugopal, 2006), depend"
W16-6402,D09-1076,0,0.153179,"tence; adjuncts cause long-distance dependencies (10 tokens separate ‘workers’ from ‘have’ in the English sentence) and complex reorderings (adjunction introduces a 2-4-1-3 permutation). those workers A have shown that dat hebben de medewerkers bewezen A who have made the switch A to the solar power industry die zijn overgestapt A naar de zonne-energiesector phrase pairs that can be extracted by Hiero, we find that this labelling is useful, allowing to gain up to 0.6 BLEU points on English-Chinese combined with a basic feature set. Factoring adjunction also allows to learn more general rules. DeNeefe and Knight (2009) show this improves translation for syntax-based models. We propose to extend the Hiero grammar by excising adjuncts from extraction phrases. This is similar in spirit to the approach of (Arnoult and Sima’an, 2012) for phrase-based models, but with the added capacity to extract SCFG rules from modified phrases. In our example, this allows to extract rules from the (adjunct-free) phrase “those workers have shown that”/“dat hebben de medewerkers bewezen”. The rest of this article is organized as follows: section 2 deals with adjunction, and how we identify it; section 3 presents our extensions t"
W16-6402,eisele-chen-2010-multiun,0,0.0267957,"onsequently, our grammar increases in size rather than becoming more compact. 4 4.1 Experiments Data We performed experiments on three language pairs: English-Chinese, English-Dutch and EnglishFrench. For all experiments, word alignments were obtained using GIZA++ with ‘grow-diag-final-and’ symmetrization (Och and Ney, 2003). The English side of the data were parsed using the Turbo parser, and converted to adjunct parses following the criteria of section 2. We used a 4-gram language model, trained with KenLM (Heafield et al., 2013). The English-Chinese data were taken from the MultiUN corpus (Eisele and Chen, 2010), limited to sentences of up to 40 tokens. We first extracted an in-domain development and test set by randomly drawing 4000 sentences without replacement from the corpus (after having removed English-side duplicates), and splitting the resulting set in two. Word alignments were trained on the rest of the corpus (ca. 5.6M sentence pairs). The language model was trained on the Xinhua section of the Chinese Gigaword corpus (LDC2003T09). The English-Dutch data were taken from the Europarl corpus (v7). We extracted a development and test set of 2000 sentence pairs each following the same method as"
W16-6402,P13-2121,0,0.0286355,"lized adjunction patterns, rather than separating ‘auxiliary’ from ‘initial’ rules. Consequently, our grammar increases in size rather than becoming more compact. 4 4.1 Experiments Data We performed experiments on three language pairs: English-Chinese, English-Dutch and EnglishFrench. For all experiments, word alignments were obtained using GIZA++ with ‘grow-diag-final-and’ symmetrization (Och and Ney, 2003). The English side of the data were parsed using the Turbo parser, and converted to adjunct parses following the criteria of section 2. We used a 4-gram language model, trained with KenLM (Heafield et al., 2013). The English-Chinese data were taken from the MultiUN corpus (Eisele and Chen, 2010), limited to sentences of up to 40 tokens. We first extracted an in-domain development and test set by randomly drawing 4000 sentences without replacement from the corpus (after having removed English-side duplicates), and splitting the resulting set in two. Word alignments were trained on the rest of the corpus (ca. 5.6M sentence pairs). The language model was trained on the Xinhua section of the Chinese Gigaword corpus (LDC2003T09). The English-Dutch data were taken from the Europarl corpus (v7). We extracte"
W16-6402,2012.eamt-1.66,0,0.016802,"S tags are combined into phrase labels. The general character of Hiero is balanced by constraints on the extraction and the form of rules, and another branch of work has focused on rebalancing such constraints. For instance, Li et al. (2013) constrain rewriting to constituents or sequences of constituents, allowing them to relax phrase length at extraction. Perhaps the most obvious limitation of Hiero is its limited capacity to capture sentencelevel reordering, as it can only monotonically concatenate larger fragments. This has motivated work on reordering, e.g., (Mylonakis and Sima’an, 2011; Huck et al., 2012). We propose to extend the scope of rule extraction in Hiero around adjuncts. As adjunction introduces long-distance dependencies, allowing the extraction of larger phrases that contain adjuncts should lead to phrases that still capture useful dependencies. This is akin to the linguistic motivation for Tree-Adjoining Grammar (Joshi et al., 1975), where factoring recursion allows to keep dependencies local (Joshi and Schabes, 1997). Our model relaxes length constraints for phrase extraction by discounting the length of adjuncts contained in a phrase. This allows to learn phrases that Hiero may"
W16-6402,P02-1050,0,0.0921776,"ven 17 Table 8: Experimental results for the adjunct-optionality model, for English-Chinese training=500k Hiero adj-Opt a training=2M BLEU BEER TER LEN BLEU BEER TER LEN 21.7 21.0∗∗ 11.1 10.9 64.6 64.1∗ 100.0 97.9∗∗ 23.4 22.8∗∗ 12.6 12.3 62.0 61.8 98.8 97.5∗∗ Results are based on a single tuning round though our means to identify adjuncts are coarse (in the example of Figure 1 for instance, “to the solar power industry” is argueably an argument of “made the switch”, and not an adjunct). Beside we assumed here that source adjuncts project into target adjuncts. This is an optimistic assumption (Hwa et al., 2002; Arnoult and Sima’an, 2014), and we are bound to extract many erroneous rules. The adjunct-driven model is however able to guide the model sufficiently well to ward off these rules for English-Chinese. Refining the labels and features is likely to further enhance the model. While our feature set may be improved, we face the difficulty that the current features are informative of the extraction of a rule, and we accordingly store their values along with the rules in the grammar. This increases the size occupied by the grammar in memory, making it harder to extract grammars for larger training"
W16-6402,N03-1017,0,0.0413481,"n allows to abstract over ‘intervening’ adjuncts, and is thus relevant for (machine) translation in general. We contribute an adjunction-driven approach to hierarchical phrase-based modelling that uses source-side adjuncts to relax extraction constraints–allowing to capturing long-distance dependencies–, and to guide translation through labelling. The labelling scheme can be reduced to two adjunct/non-adjunct labels, and improves translation over Hiero by up to 0.6 BLEU points for English-Chinese. 1 Introduction Hiero (Chiang, 2005) extends phrase-based Statistical Machine Translation models (Koehn et al., 2003) by allowing phrase pairs to rewrite through a Synchronous CFG mechanism. Rewriting is unconstrained, and the model thus learns local dependencies and reorderings in a very general manner. This lack of restrictions allows the grammar to achieve good coverage, but begs the question of how to guide Hiero with linguistic information. Since SAMT (Zollmann and Venugopal, 2006), a branch of work has focused on labelling Hiero, with different types of labels: phrase-structure labels (Zollmann and Venugopal, 2006), dependency head labels (Li et al., 2012), CCG labels (Almaghout et al., 2011), (non-syn"
W16-6402,W09-0424,0,0.0176374,"sets of two different sizes. Table 3 summarizes the sizes and average sentence length of the different data sets. Table 3: Data-set sizes 4.2 train dev test fr sentences avg. tokens 500k 20.6 2k 29.0 2k 29.7 nl sentences avg. tokens 500k 27.4 2k 27.6 2k 27.1 zh sentences avg. tokens 2k 22.7 2k 22.6 500k 22.5 2M 22.5 Tuning and Decoding All models use an extended set of dense features (not counting adjunction features), following Maillette de Buy Wenniger and Sima’an (2013). Feature weights are tuned with MIRA (Cherry and Foster, 2012), for 20 iterations. 15 Decoding is performed with Joshua (Li et al., 2009), with a relaxation of the decoding span to 100 tokens. This allows hierarchical rules to span an entire sentence in the case of the extended models. 4.3 Adjunction-based-model results Results for English-Chinese, with a small training set Table 4 presents test results on the smaller English-Chinese training set (500k sentence pairs). These tests compare the adjunction-based models, with and without labelling or features, to a Hiero baseline. We also tested the effect of relaxing the decoding span on Hiero. We use the following identifiers for the models: H-100 is a Hiero model with a relaxed"
W16-6402,W12-3128,0,0.0387599,"Missing"
W16-6402,N13-1060,0,0.0504087,"Missing"
W16-6402,W13-0803,1,0.890436,"Missing"
W16-6402,P11-1065,1,0.902646,"Missing"
W16-6402,J03-1002,0,0.0117461,"time in other phrases. The adjunct factorization we propose for Hiero is incomplete as it does not fully extract adjuncts from phrase pairs. Compared to STAG, our grammar extracts ‘derived’ rules with generalized adjunction patterns, rather than separating ‘auxiliary’ from ‘initial’ rules. Consequently, our grammar increases in size rather than becoming more compact. 4 4.1 Experiments Data We performed experiments on three language pairs: English-Chinese, English-Dutch and EnglishFrench. For all experiments, word alignments were obtained using GIZA++ with ‘grow-diag-final-and’ symmetrization (Och and Ney, 2003). The English side of the data were parsed using the Turbo parser, and converted to adjunct parses following the criteria of section 2. We used a 4-gram language model, trained with KenLM (Heafield et al., 2013). The English-Chinese data were taken from the MultiUN corpus (Eisele and Chen, 2010), limited to sentences of up to 40 tokens. We first extracted an in-domain development and test set by randomly drawing 4000 sentences without replacement from the corpus (after having removed English-side duplicates), and splitting the resulting set in two. Word alignments were trained on the rest of t"
W16-6402,C90-3045,0,0.73691,"junct-crossing constraint. Besides, we tested a version of the model with a simplified labelling for adjunct sequences. These sequences are then labelled with A instead of Ax, while their size x appears in the following feature: fx = e1−x (1) For other rules (adjuncts and other phrase pairs), fx is taken to be 1. 3.4 Factoring out adjuncts TAG factors adjunction by extracting auxiliary trees and initial trees separately (Joshi and Schabes, 1997). This leads to a more compact grammar (Chiang, 2000) that is able to generate unseen adjunction patterns. Synchronous Tree Adjunction Grammar (STAG) (Shieber and Schabes, 1990) applies TAG to translation, and DeNeefe and Knight (2009) propose a probabilistic implementation for string-to-tree translation. Their model identifies target-side adjuncts and takes their projection on the source side as a basis for auxilliary-tree extraction. In the case of Hiero, one cannot directly implement STAG, as CFG rules do not have the (tree) structure that is necessary for modelling adjunction. One can still however extract generalized versions of rules, by factoring out adjuncts contained in extraction phrases. This follows (Arnoult and Sima’an, 2012), who apply this idea to a ph"
W16-6402,W07-0412,0,0.0672966,"Missing"
W16-6402,W14-4017,1,0.898166,"Missing"
W16-6402,W06-3119,0,0.0484677,"scheme can be reduced to two adjunct/non-adjunct labels, and improves translation over Hiero by up to 0.6 BLEU points for English-Chinese. 1 Introduction Hiero (Chiang, 2005) extends phrase-based Statistical Machine Translation models (Koehn et al., 2003) by allowing phrase pairs to rewrite through a Synchronous CFG mechanism. Rewriting is unconstrained, and the model thus learns local dependencies and reorderings in a very general manner. This lack of restrictions allows the grammar to achieve good coverage, but begs the question of how to guide Hiero with linguistic information. Since SAMT (Zollmann and Venugopal, 2006), a branch of work has focused on labelling Hiero, with different types of labels: phrase-structure labels (Zollmann and Venugopal, 2006), dependency head labels (Li et al., 2012), CCG labels (Almaghout et al., 2011), (non-syntactic) hierarchical alignment labels (Maillette de Buy Wenniger and Sima’an, 2013), etc. Most of these models use large nonterminal vocabularies, as syntactic labels or POS tags are combined into phrase labels. The general character of Hiero is balanced by constraints on the extraction and the form of rules, and another branch of work has focused on rebalancing such cons"
