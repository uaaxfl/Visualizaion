2021.wat-1.1,Overview of the 8th Workshop on {A}sian Translation,2021,-1,-1,4,0,283,toshiaki nakazawa,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"This paper presents the results of the shared tasks from the 8th workshop on Asian translation (WAT2021). For the WAT2021, 28 teams participated in the shared tasks and 24 teams submitted their translation results for the human evaluation. We also accepted 5 research papers. About 2,100 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated."
2021.wat-1.23,{NICT}-5{'}s Submission To {WAT} 2021: {MBART} Pre-training And In-Domain Fine Tuning For Indic Languages,2021,-1,-1,1,1,286,raj dabre,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"In this paper we describe our submission to the multilingual Indic language translation wtask {``}MultiIndicMT{''} under the team name {``}NICT-5{''}. This task involves translation from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training."
2021.mtsummit-research.10,Investigating Softmax Tempering for Training Neural Machine Translation Models,2021,-1,-1,1,1,286,raj dabre,Proceedings of Machine Translation Summit XVIII: Research Track,0,Neural machine translation (NMT) models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against the gold labels. In low-resource scenarios and NMT models tend to perform poorly because the model training quickly converges to a point where the softmax distribution computed using logits approaches the gold label distribution. Although label smoothing is a well-known solution to address this issue and we further propose to divide the logits by a temperature coefficient greater than one and forcing the softmax distribution to be smoother during training. This makes it harder for the model to quickly over-fit. In our experiments on 11 language pairs in the low-resource Asian Language Treebank dataset and we observed significant improvements in translation quality. Our analysis focuses on finding the right balance of label smoothing and softmax tempering which indicates that they are orthogonal methods. Finally and a study of softmax entropies and gradients reveal the impact of our method on the internal behavior of our NMT models.
2021.mtsummit-research.17,Studying The Impact Of Document-level Context On Simultaneous Neural Machine Translation,2021,-1,-1,1,1,286,raj dabre,Proceedings of Machine Translation Summit XVIII: Research Track,0,In a real-time simultaneous translation setting and neural machine translation (NMT) models start generating target language tokens from incomplete source language sentences and making them harder to translate and leading to poor translation quality. Previous research has shown that document-level NMT and comprising of sentence and context encoders and a decoder and leverages context from neighboring sentences and helps improve translation quality. In simultaneous translation settings and the context from previous sentences should be even more critical. To this end and in this paper and we propose wait-k simultaneous document-level NMT where we keep the context encoder as it is and replace the source sentence encoder and target language decoder with their wait-k equivalents. We experiment with low and high resource settings using the ALT and OpenSubtitles2018 corpora and where we observe minor improvements in translation quality. We then perform an analysis of the translations obtained using our models by focusing on sentences that should benefit from the context where we found out that the model does and in fact and benefit from context but is unable to effectively leverage it and especially in a low-resource setting. This shows that there is a need for further innovation in the way useful context is identified and leveraged.
2020.wmt-1.61,Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models,2020,-1,-1,1,1,286,raj dabre,Proceedings of the Fifth Conference on Machine Translation,0,"In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the other hand, transfer learning (TL) by leveraging larger helping corpora greatly improves translation quality in general. This paper investigates a combination of SD and TL for training efficient NMT models for ELR settings, where we utilize TL with helping corpora twice: once for distilling the ELR corpora and then during compact model training. We experimented with two ELR settings: Vietnamese{--}English and Hindi{--}English from the Asian Language Treebank dataset with 18k training sentence pairs. Using the compact models with 40{\%} smaller parameters trained on the distilled ELR corpora, greedy search achieved 3.6 BLEU points improvement in average while reducing 40{\%} of decoding time. We also confirmed that using both the distilled ELR and helping corpora in the second round of TL further improves translation quality. Our work highlights the importance of stage-wise application of SD and TL for efficient NMT modeling for ELR settings."
2020.wat-1.1,Overview of the 7th Workshop on {A}sian Translation,2020,-1,-1,4,0,283,toshiaki nakazawa,Proceedings of the 7th Workshop on Asian Translation,0,"This paper presents the results of the shared tasks from the 7th workshop on Asian translation (WAT2020). For the WAT2020, 20 teams participated in the shared tasks and 14 teams submitted their translation results for the human evaluation. We also received 12 research paper submissions out of which 7 were accepted. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated."
2020.wat-1.9,{NICT}{`}s Submission To {WAT} 2020: How Effective Are Simple Many-To-Many Neural Machine Translation Models?,2020,-1,-1,1,1,286,raj dabre,Proceedings of the 7th Workshop on Asian Translation,0,In this paper we describe our team{`}s (NICT-5) Neural Machine Translation (NMT) models whose translations were submitted to shared tasks of the 7th Workshop on Asian Translation. We participated in the Indic language multilingual sub-task as well as the NICT-SAP multilingual multi-domain sub-task. We focused on naive many-to-many NMT models which gave reasonable translation quality despite their simplicity. Our observations are twofold: (a.) Many-to-many models suffer from a lack of consistency where the translation quality for some language pairs is very good but for some others it is terrible when compared against one-to-many and many-to-one baselines. (b.) Oversampling smaller corpora does not necessarily give the best translation quality for the language pair associated with that pair.
2020.ngt-1.3,Balancing Cost and Benefit with Tied-Multi Transformers,2020,-1,-1,1,1,286,raj dabre,Proceedings of the Fourth Workshop on Neural Generation and Translation,0,"We propose a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. In training an encoder-decoder model, typically, the output of the last layer of the N-layer encoder is fed to the M-layer decoder, and the output of the last decoder layer is used to compute loss. Instead, our method computes a single loss consisting of NxM losses, where each loss is computed from the output of one of the M decoder layers connected to one of the N encoder layers. Such a model subsumes NxM models with different number of encoder and decoder layers, and can be used for decoding with fewer than the maximum number of encoder and decoder layers. Given our flexible tied model, we also address to a-priori selection of the number of encoder and decoder layers for faster decoding, and explore recurrent stacking of layers and knowledge distillation for model compression. We present a cost-benefit analysis of applying the proposed approaches for neural machine translation and show that they reduce decoding costs while preserving translation quality."
2020.lrec-1.449,{C}oursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation,2020,-1,-1,2,0,12440,haiyue song,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Lectures translation is a case of spoken language translation and there is a lack of publicly available parallel corpora for this purpose. To address this, we examine a framework for parallel corpus mining which is a quick and effective way to mine a parallel corpus from publicly available lectures at Coursera. Our approach determines sentence alignments, relying on machine translation and cosine similarity over continuous-space sentence representations. We also show how to use the resulting corpora in a multistage fine-tuning based domain adaptation for high-quality lectures translation. For Japanese{--}English lectures translation, we extracted parallel data of approximately 40,000 lines and created development and test sets through manual filtering for benchmarking translation performance. We demonstrate that the mined corpus greatly enhances the quality of translation when used in conjunction with out-of-domain parallel corpora via multistage training. This paper also suggests some guidelines to gather and clean corpora, mine parallel sentences, address noise in the mined data, and create high-quality evaluation splits. For the sake of reproducibility, we have released our code for parallel data creation."
2020.lrec-1.454,{JASS}: {J}apanese-specific Sequence to Sequence Pre-training for Neural Machine Translation,2020,32,0,3,0,13030,zhuoyuan mao,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Neural machine translation (NMT) needs large parallel corpora for state-of-the-art translation quality. Low-resource NMT is typically addressed by transfer learning which leverages large monolingual or parallel corpora for pre-training. Monolingual pre-training approaches such as MASS (MAsked Sequence to Sequence) are extremely effective in boosting NMT quality for languages with small parallel corpora. However, they do not account for linguistic information obtained using syntactic analyzers which is known to be invaluable for several Natural Language Processing (NLP) tasks. To this end, we propose JASS, Japanese-specific Sequence to Sequence, as a novel pre-training alternative to MASS for NMT involving Japanese as the source or target language. JASS is joint BMASS (Bunsetsu MASS) and BRSS (Bunsetsu Reordering Sequence to Sequence) pre-training which focuses on Japanese linguistic units called bunsetsus. In our experiments on ASPEC Japanese{--}English and News Commentary Japanese{--}Russian translation we show that JASS can give results that are competitive with if not better than those given by MASS. Furthermore, we show for the first time that joint MASS and JASS pre-training gives results that significantly surpass the individual methods indicating their complementary nature. We will release our code, pre-trained models and bunsetsu annotated data as resources for researchers to use in their own NLP tasks."
2020.coling-tutorials.3,Multilingual Neural Machine Translation,2020,0,28,1,1,286,raj dabre,Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts,0,"The advent of neural machine translation (NMT) has opened up exciting research in building multilingual translation systems i.e. translation models that can handle more than one language pair. Many advances have been made which have enabled (1) improving translation for low-resource languages via transfer learning from high resource languages; and (2) building compact translation models spanning multiple languages. In this tutorial, we will cover the latest advances in NMT approaches that leverage multilingualism, especially to enhance low-resource translation. In particular, we will focus on the following topics: modeling parameter sharing for multi-way models, massively multilingual models, training protocols, language divergence, transfer learning, zero-shot/zero-resource learning, pivoting, multilingual pre-training and multi-source translation."
2020.coling-main.119,Harnessing Cross-lingual Features to Improve Cognate Detection for Low-resource Languages,2020,-1,-1,2,0.284605,8127,diptesh kanojia,Proceedings of the 28th International Conference on Computational Linguistics,0,"Cognates are variants of the same lexical form across different languages; for example {``}fonema{''} in Spanish and {``}phoneme{''} in English are cognates, both of which mean {``}a unit of sound{''}. The task of automatic detection of cognates among any two languages can help downstream NLP tasks such as Cross-lingual Information Retrieval, Computational Phylogenetics, and Machine Translation. In this paper, we demonstrate the use of cross-lingual word embeddings for detecting cognates among fourteen Indian Languages. Our approach introduces the use of context from a knowledge graph to generate improved feature representations for cognate detection. We, then, evaluate the impact of our cognate detection mechanism on neural machine translation (NMT), as a downstream task. We evaluate our methods to detect cognates on a challenging dataset of twelve Indian languages, namely, Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. Additionally, we create evaluation datasets for two more Indian languages, Konkani and Nepali. We observe an improvement of up to 18{\%} points, in terms of F-score, for cognate detection. Furthermore, we observe that cognates extracted using our method help improve NMT quality by up to 2.76 BLEU. We also release our code, newly constructed datasets and cross-lingual models publicly."
2020.coling-main.376,Improving Low-Resource {NMT} through Relevance Based Linguistic Features Incorporation,2020,-1,-1,2,0,373,abhisek chakrabarty,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this study, linguistic knowledge at different levels are incorporated into the neural machine translation (NMT) framework to improve translation quality for language pairs with extremely limited data. Integrating manually designed or automatically extracted features into the NMT framework is known to be beneficial. However, this study emphasizes that the relevance of the features is crucial to the performance. Specifically, we propose two methods, 1) self relevance and 2) word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation."
2020.acl-srw.37,Pre-training via Leveraging Assisting Languages for Neural Machine Translation,2020,17,0,2,0,12440,haiyue song,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks. However, large monolingual corpora might not always be available for the languages of interest (LOI). Thus, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. We utilize script mapping (Chinese to Japanese) to increase the similarity (number of cognates) between the monolingual corpora of helping languages and LOI. An empirical case study of low-resource Japanese-English neural machine translation (NMT) reveals that leveraging large Chinese and French monolingual corpora can help overcome the shortage of Japanese and English monolingual corpora, respectively, for S2S pre-training. Using only Chinese and French monolingual corpora, we were able to improve Japanese-English translation quality by up to 8.5 BLEU in low-resource scenarios."
W19-6613,Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation,2019,25,0,2,0,3853,aizhan imankulova,Proceedings of Machine Translation Summit XVII: Research Track,0,"This paper proposes a novel multilingual multistage fine-tuning approach for low-resource neural machine translation (NMT), taking a challenging Japanese--Russian pair for benchmarking. Although there are many solutions for low-resource scenarios, such as multilingual NMT and back-translation, we have empirically confirmed their limited success when restricted to in-domain data. We therefore propose to exploit out-of-domain data through transfer learning, by using it to first train a multilingual NMT model followed by multistage fine-tuning on in-domain parallel and back-translated pseudo-parallel data. Our approach, which combines domain adaptation, multilingualism, and back-translation, helps improve the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario."
W19-5428,{NICT}{'}s Machine Translation Systems for the {WMT}19 Similar Language Translation Task,2019,0,1,2,0,8610,benjamin marie,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",0,"This paper presents the NICT{'}s participation in the WMT19 shared Similar Language Translation Task. We participated in the Spanish-Portuguese task. For both translation directions, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems with the Transformer architecture were trained on the provided parallel data enlarged with a large quantity of back-translated monolingual data. Our primary submission to the task is the result of a simple combination of our SMT and NMT systems. According to BLEU, our systems were ranked second and third respectively for the Portuguese-to-Spanish and Spanish-to-Portuguese translation directions. For contrastive experiments, we also submitted outputs generated with an unsupervised SMT system."
W19-5313,{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 News Translation Task,2019,0,2,1,1,286,raj dabre,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"In this paper, we describe our supervised neural machine translation (NMT) systems that we developed for the news translation task for KazakhâEnglish, GujaratiâEnglish, ChineseâEnglish, and EnglishâFinnish translation directions. We focused on leveraging multilingual transfer learning and back-translation for the extremely low-resource language pairs: KazakhâEnglish and GujaratiâEnglish translation. For the ChineseâEnglish translation, we used the provided parallel data augmented with a large quantity of back-translated monolingual data to train state-of-the-art NMT systems. We then employed techniques that have been proven to be most effective, such as back-translation, fine-tuning, and model ensembling, to generate the primary submissions of ChineseâEnglish. For EnglishâFinnish, our submission from WMT18 remains a strong baseline despite the increase in parallel corpora for this year{'}s task."
W19-5362,{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 Translation Robustness Task,2019,0,2,1,1,286,raj dabre,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"In this paper we describe our neural machine translation (NMT) systems for JapaneseâEnglish translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et. al., 2017) to improve translation quality for JapaneseâEnglish. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest."
D19-5201,Overview of the 6th Workshop on {A}sian Translation,2019,0,1,5,0,283,toshiaki nakazawa,Proceedings of the 6th Workshop on Asian Translation,0,"This paper presents the results of the shared tasks from the 6th workshop on Asian translation (WAT2019) including JaâEn, JaâZh scientific paper translation subtasks, JaâEn, JaâKo, JaâEn patent translation subtasks, HiâEn, MyâEn, KmâEn, TaâEn mixed domain subtasks and RuâJa news commentary translation task. For the WAT2019, 25 teams participated in the shared tasks. We also received 10 research paper submissions out of which 61 were accepted. About 400 translation results were submitted to the automatic evaluation server, and selected submis- sions were manually evaluated."
D19-5207,{NICT}{'}s participation to {WAT} 2019: Multilingualism and Multi-step Fine-Tuning for Low Resource {NMT},2019,0,0,1,1,286,raj dabre,Proceedings of the 6th Workshop on Asian Translation,0,"In this paper we describe our submissions to WAT 2019 for the following tasks: English{--}Tamil translation and Russian{--}Japanese translation. Our team,{``}NICT-5{''}, focused on multilingual domain adaptation and back-translation for Russian{--}Japanese translation and on simple fine-tuning for English{--}Tamil translation . We noted that multi-stage fine tuning is essential in leveraging the power of multilingualism for an extremely low-resource language like Russian{--}Japanese. Furthermore, we can improve the performance of such a low-resource language pair by exploiting a small but in-domain monolingual corpus via back-translation. We managed to obtain second rank in both tasks for all translation directions."
D19-1146,Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation,2019,0,5,1,1,286,raj dabre,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"This paper highlights the impressive utility of multi-parallel corpora for transfer learning in a one-to-many low-resource neural machine translation (NMT) setting. We report on a systematic comparison of multistage fine-tuning configurations, consisting of (1) pre-training on an external large (209k{--}440k) parallel corpus for English and a helping target language, (2) mixed pre-training or fine-tuning on a mixture of the external and low-resource (18k) target parallel corpora, and (3) pure fine-tuning on the target parallel corpora. Our experiments confirm that multi-parallel corpora are extremely useful despite their scarcity and content-wise redundancy thus exhibiting the true power of multilingualism. Even when the helping target language is not one of the target languages of our concern, our multistage fine-tuning can give 3{--}9 BLEU score gains over a simple one-to-one model."
Y18-3001,Overview of the 5th Workshop on {A}sian Translation,2018,0,5,5,0,283,toshiaki nakazawa,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Y18-3003,{NICT}{'}s Participation in {WAT} 2018: Approaches Using Multilingualism and Recurrently Stacked Layers,2018,0,3,1,1,286,raj dabre,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Y17-1038,An Empirical Study of Language Relatedness for Transfer Learning in Neural Machine Translation,2017,0,12,1,1,286,raj dabre,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
W17-5714,{K}yoto {U}niversity Participation to {WAT} 2017,2017,-1,-1,2,0.694451,17599,fabien cromieres,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"We describe here our approaches and results on the WAT 2017 shared translation tasks. Following our good results with Neural Machine Translation in the previous shared task, we continue this approach this year, with incremental improvements in models and training methods. We focused on the ASPEC dataset and could improve the state-of-the-art results for Chinese-to-Japanese and Japanese-to-Chinese translations."
P17-2061,An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation,2017,16,47,2,0.416667,293,chenhui chu,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we propose a novel domain adaptation method named {``}mixed fine tuning{''} for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings."
I17-5004,"Neural Machine Translation: Basics, Practical Aspects and Recent Trends",2017,0,0,3,0.694451,17599,fabien cromieres,"Proceedings of the {IJCNLP} 2017, Tutorial Abstracts",0,"Machine Translation (MT) is a sub-field of NLP which has experienced a number of paradigm shifts since its inception. Up until 2014, Phrase Based Statistical Machine Translation (PBSMT) approaches used to be the state of the art. In late 2014, Neural Machine Translation (NMT) was introduced and was proven to outperform all PBSMT approaches by a significant margin. Since then, the NMT approaches have undergone several transformations which have pushed the state of the art even further. This tutorial is primarily aimed at researchers who are either interested in or are fairly new to the world of NMT and want to obtain a deep understanding of NMT fundamentals. Because it will also cover the latest developments in NMT, it should also be useful to attendees with some experience in NMT."
W16-2349,The {K}yoto {U}niversity Cross-Lingual Pronoun Translation System,2016,8,1,1,1,286,raj dabre,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"In this paper we describe our system we designed and implemented for the crosslingual pronoun prediction task as a part of WMT 2016. The majority of the paper will be dedicated to the system whose outputs we submitted wherein we describe the simplified mathematical model, the details of the components and the working by means of an architecture diagram which also serves as a flowchart. We then discuss the results of the official scores and our observations on the same."
L16-1468,Parallel Sentence Extraction from Comparable Corpora with Neural Network Features,2016,7,4,2,0.416667,293,chenhui chu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Parallel corpora are crucial for machine translation (MT), however they are quite scarce for most language pairs and domains. As comparable corpora are far more available, many studies have been conducted to extract parallel sentences from them for MT. In this paper, we exploit the neural network features acquired from neural MT for parallel sentence extraction. We observe significant improvements for both accuracy in sentence extraction and MT performance."
2016.gwc-1.22,Sophisticated Lexical Databases - Simplified Usage: Mobile Applications and Browser Plugins For Wordnets,2016,-1,-1,2,0.588235,8127,diptesh kanojia,Proceedings of the 8th Global WordNet Conference (GWC),0,"India is a country with 22 officially recognized languages and 17 of these have WordNets, a crucial resource. Web browser based interfaces are available for these WordNets, but are not suited for mobile devices which deters people from effectively using this resource. We present our initial work on developing mobile applications and browser extensions to access WordNets for Indian Languages. Our contribution is two fold: (1) We develop mobile applications for the Android, iOS and Windows Phone OS platforms for Hindi, Marathi and Sanskrit WordNets which allow users to search for words and obtain more information along with their translations in English and other Indian languages. (2) We also develop browser extensions for English, Hindi, Marathi, and Sanskrit WordNets, for both Mozilla Firefox, and Google Chrome. We believe that such applications can be quite helpful in a classroom scenario, where students would be able to access the WordNets as dictionaries as well as lexical knowledge bases. This can help in overcoming the language barrier along with furthering language understanding."
Y15-1033,Large-scale Dictionary Construction via Pivot-based Statistical Machine Translation with Significance Pruning and Neural Network Features,2015,15,0,1,1,286,raj dabre,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"We present our ongoing work on large-scale Japanese-Chinese bilingual dictionary construction via pivot-based statistical machine translation. We utilize statistical significance pruning to control noisy translation pairs that are induced by pivoting. We construct a large dictionary which we manually verify to be of a high quality. We then use this dictionary and a parallel corpus to learn bilingual neural network language models to obtain features for reranking the n-best list, which leads to an absolute improvement of 5% in accuracy when compared to a setting that does not use significance pruning and reranking."
W15-5944,Augmenting Pivot based {SMT} with word segmentation,2015,0,2,4,0,36405,rohit more,Proceedings of the 12th International Conference on Natural Language Processing,0,None
W15-5006,{K}yoto{EBMT} System Description for the 2nd Workshop on {A}sian Translation,2015,-1,-1,2,0,30411,john richardson,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
N15-1125,Leveraging Small Multilingual Corpora for {SMT} Using Many Pivot Languages,2015,24,10,1,1,286,raj dabre,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present our work on leveraging multilingual parallel corpora of small sizes for Statistical Machine Translation between Japanese and Hindi using multiple pivot languages. In our setting, the source and target part of the corpus remains the same, but we show that using several different pivot to extract phrase pairs from these source and target parts lead to large BLEU improvements. We focus on a variety of ways to exploit phrase tables generated using multiple pivots to support a direct source-target phrase table. Our main method uses the Multiple Decoding Paths (MDP) feature of Moses, which we empirically verify as the best compared to the other methods we used. We compare and contrast our various results to show that one can overcome the limitations of small corpora by using as many pivot languages as possible in a multilingual setting. Most importantly, we show that such pivoting aids in learning of additional phrase pairs which are not learned when the direct sourcetarget corpus is small. We obtained improvements of up to 3 BLEU points using multiple pivots for Japanese to Hindi translation compared to when only one pivot is used. To the best of our knowledge, this work is also the first of its kind to attempt the simultaneous utilization of 7 pivot languages at decoding time."
W14-5103,Tackling Close Cousins: Experiences In Developing Statistical Machine Translation Systems For {M}arathi And {H}indi,2014,5,0,1,1,286,raj dabre,Proceedings of the 11th International Conference on Natural Language Processing,0,None
W14-5113,"Anou Tradir: Experiences In Building Statistical Machine Translation Systems For Mauritian Languages {--} Creole, {E}nglish, {F}rench",2014,9,0,1,1,286,raj dabre,Proceedings of the 11th International Conference on Natural Language Processing,0,"We present, in this paper, our experiences in developing Statistical Machine Translation (SMT) systems involving English, French and Mauritian Creole, the languages most spoken in Mauritius. We give a brief overview of the peculiarities of the language phenomena in Mauritian Creole and indicate the differences between it and English and French. We then give descriptions of the developed corpora used for the various MT systems where we also explore the possibility of using French as a bridge language when translating from English to Creole. We evaluate these systems using the standard objective evaluation measure, BLEU. We postulate and through an error analysis, indicated by examples, verify that when English to French translations are perfect, the subsequent translation of French to Creole results in better quality translations than direct English to Creole translation."
W14-5126,{P}a{CM}an : Parallel Corpus Management Workbench,2014,6,0,3,0.588235,8127,diptesh kanojia,Proceedings of the 11th International Conference on Natural Language Processing,0,"We present a Parallel Corpora Management tool that aides parallel corpora generation for the task of Machine Translation (MT). It takes source and target text of a corpus for any language pair in text file format, or zip archives containing multiple corresponding text files. Then, it provides with a helpful interface to lexicographers for manual translation / validation, and gives out the corrected text files as output. It provides various dictionary references as help within the interface which increase the productivity and efficiency of a lexicographer. It also provides automatic translation of the source sentence using an integrated MT system. The tool interface includes a corpora management system which facilitates maintenance of parallel corpora by assigning roles such as manager, lexicographer etc. We have designed a novel tool that provides aides like references to various dictionary sources such as Wordnets, Shabdkosh, Wikitionary etc. We also provide manual word alignment correction which is visualized in the tool and can lead to its gamification in the future, thus, providing a valuable source of word / phrase alignments."
W14-0126,"Do not do processing, when you can look up: Towards a Discrimination Net for {WSD}",2014,4,0,3,0.588235,8127,diptesh kanojia,Proceedings of the Seventh Global {W}ordnet Conference,0,"The task of Word Sense Disambiguation (WSD) incorporates in its definition the role of xe2x80x98contextxe2x80x99. We present our work on the development of a tool which allows for automatic acquisition and ranking of xe2x80x98context cluesxe2x80x99 for WSD. These clue words are extracted from the contexts of words appearing in a large monolingual corpus. These mined collection of contextual clues form a discrimination net in the sense that for targeted WSD, navigation of the net leads to the correct sense of a word given its context. Utilizing this resource we intend to develop efficient and light weight WSD based on look up and navigation of memory-resident knowledge base, thereby avoiding heavy computation which often prevents incorporation of any serious WSD in MT and search. The need for large quantities of sense marked data too can be reduced."
C12-2023,Morphological Analyzer for Affix Stacking Languages: A Case Study of {M}arathi,2012,7,2,1,1,286,raj dabre,Proceedings of {COLING} 2012: Posters,0,"In this paper we describe and evaluate a Finite State Machine (FSM) based Morphological Analyzer (MA) for Marathi, a highly inflectional language with agglutinative su ffixes. Marathi belongs to the Indo-European family and is considerably influenced by Dravidian languages. Adroit handling of participial constructions and other derived forms ( Krudantas and Taddhitas) in addition to inflected forms is crucial to NLP and MT of Marathi. We firs t describe Marathi morphological phenomena, detailing the complexities of inflectional and derivational morphology, and then go into the construction and working of the MA. The MA produces the root word and the features. A thorough evaluation against gold standard data establish es the efficacy of this MA. To the best of our knowledge, this work is t he first of its kind on a systematic and exhaustive study of the Morphotactics of a suffix-stac king language, leading to high quality morph analyzer. The system forms part of a Marathi -Hindi transfer based machine translation system. The methodology delineated in the paper can be replicated fo r other languages showing similar suffix stacking behaviour as Marathi."
