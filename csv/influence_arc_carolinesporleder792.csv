2015.lilt-12.4,W12-2513,0,0.0161837,"ions and interactions). Celikyilmaz et al. (2010) extracted dialogue interactions in order to analyze semantic orientation of social networks from literature. In order to perform large-scale analyses of the works, both Rydberg-Cox (2011) and Suen et al. (2013) extract networks from structured text: Greek tragedies the former, plays and movie scripts the latter. All the approaches mentioned above produce static networks which are flat representations of the novel. Past, present and future are displayed simultaneously in such networks: time turns into space. The recent work by Elsner (2012) and Agarwal et al. (2012) questions the validity of static network analysis. Agarwal et al. introduce the concept of dynamic network analysis for literature, motivated by the idea that static networks can distort the characters’ importance (exemplified by a case analysis of Lewis Carroll’s Alice in Wonderland ). A dynamic social network is but the collection of independent networks for each of the parts into which the novel is divided. 2.4 Contributions Our paper makes several contributions. It is one of the first attempts to solve in a quantitative fashion the challenging task of clustering novels according to genre,"
2015.lilt-12.4,P14-1035,0,0.0847927,"is yields better results. 24 / LiLT volume 12, issue 4 October 2015 We could also ask whether we worked with a suitable corpus. Attempting a classification of the Western literary canon may always lead to frustration, for many of the novels that entered it broke the mould of the literary production of their time, setting new standards. As future work, we plan to apply our method to present-day literature, which we expect in most of the cases to yield a more clear-cut classification of the novels. Finally, we have seen how the author’s fingerprints are visible in the social networks of novels. Bamman et al. (2014) take first steps toward accounting for the influence of unwanted signals (such as, in our genre experiment, the author of the novel) over the wanted information. Any further work should take this into consideration. 8 Conclusion This work is a contribution to the field of quantitative literary analysis. We have presented a method of building static and dynamic social networks from novels as a way of representing structure and plot. Our main goal was to understand the role that the network structure of a novel plays in determining the genre to which the novel belongs. A secondary goal was to l"
2015.lilt-12.4,E12-1065,0,0.0186321,"l events (observations and interactions). Celikyilmaz et al. (2010) extracted dialogue interactions in order to analyze semantic orientation of social networks from literature. In order to perform large-scale analyses of the works, both Rydberg-Cox (2011) and Suen et al. (2013) extract networks from structured text: Greek tragedies the former, plays and movie scripts the latter. All the approaches mentioned above produce static networks which are flat representations of the novel. Past, present and future are displayed simultaneously in such networks: time turns into space. The recent work by Elsner (2012) and Agarwal et al. (2012) questions the validity of static network analysis. Agarwal et al. introduce the concept of dynamic network analysis for literature, motivated by the idea that static networks can distort the characters’ importance (exemplified by a case analysis of Lewis Carroll’s Alice in Wonderland ). A dynamic social network is but the collection of independent networks for each of the parts into which the novel is divided. 2.4 Contributions Our paper makes several contributions. It is one of the first attempts to solve in a quantitative fashion the challenging task of clustering"
2015.lilt-12.4,P10-1015,0,0.391759,"lots artificially. Alberich et al. (2002) made one of the first attempts to combine social networks and literature. They built a social network from the Marvel comics, in which characters are the nodes linked by their co-occurrence 6 / LiLT volume 12, issue 4 October 2015 in the same book. The authors note that the resulting network is very similar to a real social network. Newman and Girvan (2003) used a hand-built social network with the main characters of Victor Hugo’s Les Misérables to detect communities of characters, densely connected, that reproduced the subplot structure of the novel. Elson et al. (2010) introduced a new approach to create networks from novels: characters are linked if they converse, instead of being linked if they occur in the same window of text. The networks are built automatically, and heuristics are used to generate variations of the names of the characters and to cluster the coreferent names. The analysis of the networks is then used to refute some literary hypotheses. Jayannavar et al. (2015), revisiting the work of Elson et al., revised the set of hypotheses, which they validated using networks based on social events (observations and interactions). Celikyilmaz et al."
2015.lilt-12.4,P05-1045,0,0.0481167,"Missing"
2015.lilt-12.4,W15-0704,0,0.0126632,"uilt social network with the main characters of Victor Hugo’s Les Misérables to detect communities of characters, densely connected, that reproduced the subplot structure of the novel. Elson et al. (2010) introduced a new approach to create networks from novels: characters are linked if they converse, instead of being linked if they occur in the same window of text. The networks are built automatically, and heuristics are used to generate variations of the names of the characters and to cluster the coreferent names. The analysis of the networks is then used to refute some literary hypotheses. Jayannavar et al. (2015), revisiting the work of Elson et al., revised the set of hypotheses, which they validated using networks based on social events (observations and interactions). Celikyilmaz et al. (2010) extracted dialogue interactions in order to analyze semantic orientation of social networks from literature. In order to perform large-scale analyses of the works, both Rydberg-Cox (2011) and Suen et al. (2013) extract networks from structured text: Greek tragedies the former, plays and movie scripts the latter. All the approaches mentioned above produce static networks which are flat representations of the n"
C04-1007,J91-1002,0,\N,Missing
C04-1007,P88-1014,0,\N,Missing
C04-1007,J88-2006,0,\N,Missing
C04-1007,N03-1030,0,\N,Missing
C08-1084,W06-1617,0,0.307641,"Missing"
C08-1084,P03-1068,1,0.875425,"Missing"
C08-1084,W03-1007,0,0.0595431,"Missing"
C08-1084,J02-3001,0,0.793508,"less clear how the combined meaning of phrases can be described. Semantic roles describe an important aspect of phrasal meaning by characterising the relationship between predicates and their arguments on a semantic level (e.g., agent, patient). They generalise over surface categories (such as subject, object) and variations (such as diathesis alternations). Two frameworks for semantic roles have found wide use in the community, PropBank (Palmer et al., 2005) and FrameNet (Fillmore et al., 2003). Their corpora are used to train supervised models for semantic role labelling (SRL) of new text (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). The resulting analysis can benefit a number of applications, such c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. as Information Extraction (Moschitti et al., 2003) or Question Answering (Frank et al., 2007). A commonly encountered criticism of semantic roles, and arguably a major obstacle to their adoption in NLP, is their limited coverage. Since manual semantic role tagging is costly, it is hardly conceivable that gold standard anno"
C08-1084,P07-1025,0,0.0844952,"o generalise from seen predicates to unseen predicates for which no training data is available. Techniques for extending the coverage of SRL therefore address an important need. Unfortunately, pioneering work in unsupervised SRL (Swier and Stevenson, 2004; Grenager and Manning, 2006) currently either relies on a small number of semantic roles, or cannot identify equivalent roles across predicates. A promising alternative direction is automatic data expansion, i.e., leveraging existing annotations to classify unseen, but similar, predicates. The feasibility of this approach was demonstrated by Gordon and Swanson (2007) for syntactically similar verbs. However, their approach requires at least one annotated instance of each new predicate, limiting its practicability. In this paper, we present a pilot study on the application of automatic data expansion to event nominalisations of verbs, such as agreement for agree or destruction for destroy. While event nominalisations often afford the same semantic roles as verbs, and often replace them in written language (Gurevich et al., 2006), they have played a largely marginal role in annotation. PropBank has only annotated verbs.1 FrameNet annotates nouns, but covers"
C08-1084,W06-1601,0,0.0172172,"bstacle to their adoption in NLP, is their limited coverage. Since manual semantic role tagging is costly, it is hardly conceivable that gold standard annotation will ultimately be available for every predicate of English. In addition, the lexically specific nature of the mapping between surface syntax and semantic roles makes it difficult to generalise from seen predicates to unseen predicates for which no training data is available. Techniques for extending the coverage of SRL therefore address an important need. Unfortunately, pioneering work in unsupervised SRL (Swier and Stevenson, 2004; Grenager and Manning, 2006) currently either relies on a small number of semantic roles, or cannot identify equivalent roles across predicates. A promising alternative direction is automatic data expansion, i.e., leveraging existing annotations to classify unseen, but similar, predicates. The feasibility of this approach was demonstrated by Gordon and Swanson (2007) for syntactically similar verbs. However, their approach requires at least one annotated instance of each new predicate, limiting its practicability. In this paper, we present a pilot study on the application of automatic data expansion to event nominalisati"
C08-1084,P07-1027,0,0.528749,"undamental intuition is that it is possible to increase the annotation coverage of event nominalisations by data expansion from verbal instances, since the verbal and nominal predicates share a large part of the underlying argument structure. We assume that annotation is available for verbal instances. Then, for a given instance of a nominalisation and its arguments, the aim is to assign semantic role labels to these arguments. We solve this task by constructing mappings between the arguments of the noun and the semantic roles realised by the verb’s arguments. Crucially, unlike previous work (Liu and Ng, 2007), we do not employ a classical supervised approach, and thus do not require any nominal annotations. Structure of the paper. Sec. 2 provides background on nominalisations and SRL. Sec. 3 provides concrete details on our expansion-based approach to SRL for nominalisations. The second part of the paper (Sec. 4–6) provides a first evaluation of different mapping strategies based on syntactic, semantic, and hybrid information. Sec. 8 concludes. 2 Nominalisations Nominalisations (or deverbal nouns) are commonly defined as nouns morphologically derived from verbs, usually by suffixation (Quirk et al"
C08-1084,meyers-etal-2004-annotating,0,0.0617771,"least one annotated instance of each new predicate, limiting its practicability. In this paper, we present a pilot study on the application of automatic data expansion to event nominalisations of verbs, such as agreement for agree or destruction for destroy. While event nominalisations often afford the same semantic roles as verbs, and often replace them in written language (Gurevich et al., 2006), they have played a largely marginal role in annotation. PropBank has only annotated verbs.1 FrameNet annotates nouns, but covers far fewer nouns than verbs. The same 1 A follow-up project, NomBank (Meyers et al., 2004), has since provided annotations for nominal instances, too. 665 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 665–672 Manchester, August 2008 situation holds in other languages (Erk et al., 2003). Our fundamental intuition is that it is possible to increase the annotation coverage of event nominalisations by data expansion from verbal instances, since the verbal and nominal predicates share a large part of the underlying argument structure. We assume that annotation is available for verbal instances. Then, for a given instance of a nominali"
C08-1084,J07-2002,1,0.837428,"Missing"
C08-1084,J05-1004,0,0.0534777,"c properties of individual words, both in the form of hand-built resources like WordNet and data-driven methods like semantic space models. It is still much less clear how the combined meaning of phrases can be described. Semantic roles describe an important aspect of phrasal meaning by characterising the relationship between predicates and their arguments on a semantic level (e.g., agent, patient). They generalise over surface categories (such as subject, object) and variations (such as diathesis alternations). Two frameworks for semantic roles have found wide use in the community, PropBank (Palmer et al., 2005) and FrameNet (Fillmore et al., 2003). Their corpora are used to train supervised models for semantic role labelling (SRL) of new text (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). The resulting analysis can benefit a number of applications, such c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. as Information Extraction (Moschitti et al., 2003) or Question Answering (Frank et al., 2007). A commonly encountered criticism of semantic roles, and arguably a m"
C08-1084,N04-4036,0,0.506059,"Missing"
C08-1084,W04-3212,0,0.0953581,"Missing"
C08-1084,W04-3213,0,\N,Missing
C08-1084,J02-3004,0,\N,Missing
C10-2078,baldwin-etal-2004-road,0,0.0237657,"different features and found that features encoding lexical cohesion as well as some syntactic features can generalize well across idioms. 1 Introduction Nonliteral expressions are a major challenge in NLP because they are (i) fairly frequent and (ii) often behave idiosyncratically. Apart from typically being semantically more or less opaque, they can also disobey grammatical constraints (e.g., by and large, lie in wait). Hence, idiomatic expressions are not only a problem for semantic analysis but can also have a negative effect on other NLP applications (Sag et al., 2001), such as parsing (Baldwin et al., 2004). To process non-literal language correctly, NLP systems need to recognise such expressions automatically. While there has been a significant body of work on idiom (and more generally multiword expression) detection (see Section 2), until recently most approaches have focused on a type-based classification, dividing expressions into “idiomatic” or “not idiomatic” irrespective of their actual use in a discourse context. However, while some expressions, such as by and large, always have a non-compositional, idiomatic meaning, many other expressions, such as break the ice or spill the beans, can"
C10-2078,E06-1042,0,0.327653,"n which the idiom normally occurs (Riehemann, 2001).The canonical form allows for inflectional variation of the head verb but not for other variations (such as nominal inflection, choice of determiner etc.). It has been observed that if an expression is used idiomatically, it typically occurs in its canonical form (Riehemann, 2001). Cook et al. exploit this behaviour and propose an unsupervised method in which an expression is classified as idiomatic if it occurs in canonical form and literal otherwise. Canonical forms are determined automatically using a statistical, frequency-based measure. Birke and Sarkar (2006) model literal vs. nonliteral classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. Sporleder and Li (2009) propose another unsupervised method which detects the presence or absence of cohesive links between the component words of the idiom and the surrounding discourse. If such links can be found the expression is classified as literal otherwise as non-literal. Li and Sporleder (2009) later extended this work by combining t"
C10-2078,D09-1049,0,0.0137675,"n the component words of the idiom and the surrounding discourse. If such links can be found the expression is classified as literal otherwise as non-literal. Li and Sporleder (2009) later extended this work by combining the unsupervised classifier with a secondstage supervised classifier. Hashimoto and Kawahara (2008) present a supervised approach to token-based idiom distinction for Japanese, in which they implement several features, such as features known from other word sense disambiguation tasks (e.g., collocations) and idiom-specific features taken from Hashimoto et al. (2006). Finally, Boukobza and Rappoport (2009) also experimented with a supervised classifier, which takes into account various surface features. In the present work, we also investigate supervised models for token-based idiom detection. We are specifically interested in which types of features (e.g., local context, global context, syntactic properties) perform best on this task and more specifically which features generalize across idioms. 3 Data We used the data set created by Sporleder and Li (2009), which consists of 13 English expressions (mainly V+PP or V+NP) that can be used both literally and idiomatically, such as break the ice o"
C10-2078,P06-2006,0,0.0146007,"al reading is more frequent. The number of instances varies, ranging from 15 for pull the trigger to 903 for drop the ball. While the instances were extracted from a news corpus, none of them are domain-specific and all expressions also occur in the BNC, which is a balanced, multi-domain corpus. To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger (Ratnaparkhi, 1996), parsed with the MaltParser2 , and named entity tagged with the Stanford NE tagger (Finkel et al., 2005). The lemmatization was done by RASP (Briscoe and Carroll, 2006). 4 Indicators of Idiomatic and Literal Usage In this study we are particularly interested in which linguistic indicators work well for the task of distinguishing literal and idiomatic language use. The few previous studies have mainly looked at the lexical context in which and expression occurs (Katz and Giesbrecht, 2006; Birke and Sarkar, 2006). However, other properties of the linguistic context might also be useful. We distinguish these features into different groups and discuss them in the following sections. 4.1 Global Lexical Context (glc) That the lexical context might be a good indica"
C10-2078,W07-1106,0,0.151723,"s encode information about the passivisation, argument movement, and the ability of the target expression to be negated or modified. Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples. This approach was later extended by Diab and Krishna (2009), who take a larger context into account when computing the feature vectors (e.g., the whole paragraph) and who also include prepositions and determiners in addition to content words. Cook et al. (2007) and Fazly et al. (2009) take a different approach, which crucially relies on the concept of canonical form (CForm). It is assumed that for each idiom there is a fixed form (or a small set of those) corresponding to the syntactic pattern(s) in which the idiom normally occurs (Riehemann, 2001).The canonical form allows for inflectional variation of the head verb but not for other variations (such as nominal inflection, choice of determiner etc.). It has been observed that if an expression is used idiomatically, it typically occurs in its canonical form (Riehemann, 2001). Cook et al. exploit thi"
C10-2078,W09-2903,0,0.273361,"e feature vector. Indicative Terms (iTerm) Some words such as literally, proverbially are also indicative of literal or idiomatic usages. We encoded the frequencies of those indicative terms as features. Scare Quotes (quote) This feature encodes whether the idiom is marked off by scare quotes, which often indicates non-literal usage (15). (15) 5 Other Features Named Entities (ne) can also indicate the usage of an expression. For instance, a country name in the subject position of the target expression break the ice is a strong indicator of this phrase being used idiomatically (see Example 7). Diab and Bhutada (2009) find that NE-features perform best. They used a commercial NE-tagger Experiments In the previous section we discussed different linguistic cues for idiom usage. To determine which of these cues work best for the task and which ones generalize across different idioms, we carried out three experiments. In the first one (Section 5.1) we trained one model for each idiom (see Section 3) and tested the predictiveness of each feature type individually as well as all features together. In the second experiment (Section 5.2), we trained one generic model for all idioms and determined how the performan"
C10-2078,J09-1005,0,0.152026,"out the passivisation, argument movement, and the ability of the target expression to be negated or modified. Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples. This approach was later extended by Diab and Krishna (2009), who take a larger context into account when computing the feature vectors (e.g., the whole paragraph) and who also include prepositions and determiners in addition to content words. Cook et al. (2007) and Fazly et al. (2009) take a different approach, which crucially relies on the concept of canonical form (CForm). It is assumed that for each idiom there is a fixed form (or a small set of those) corresponding to the syntactic pattern(s) in which the idiom normally occurs (Riehemann, 2001).The canonical form allows for inflectional variation of the head verb but not for other variations (such as nominal inflection, choice of determiner etc.). It has been observed that if an expression is used idiomatically, it typically occurs in its canonical form (Riehemann, 2001). Cook et al. exploit this behaviour and propose"
C10-2078,P05-1045,0,0.0498089,"pretation, however for some, like drop the ball, the literal reading is more frequent. The number of instances varies, ranging from 15 for pull the trigger to 903 for drop the ball. While the instances were extracted from a news corpus, none of them are domain-specific and all expressions also occur in the BNC, which is a balanced, multi-domain corpus. To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger (Ratnaparkhi, 1996), parsed with the MaltParser2 , and named entity tagged with the Stanford NE tagger (Finkel et al., 2005). The lemmatization was done by RASP (Briscoe and Carroll, 2006). 4 Indicators of Idiomatic and Literal Usage In this study we are particularly interested in which linguistic indicators work well for the task of distinguishing literal and idiomatic language use. The few previous studies have mainly looked at the lexical context in which and expression occurs (Katz and Giesbrecht, 2006; Birke and Sarkar, 2006). However, other properties of the linguistic context might also be useful. We distinguish these features into different groups and discuss them in the following sections. 4.1 Global Lexic"
C10-2078,D08-1104,0,0.0602089,"Missing"
C10-2078,P06-2046,0,0.0976038,"s problem by employing a generic feature space that 683 Coling 2010: Poster Volume, pages 683–691, Beijing, August 2010 looks at the cohesive ties between the potential idiom and its surrounding discourse. Such features generalize well across different expressions and lead to acceptable performance even on expressions unseen in the training set. 2 Related Work Until recently, most studies on idiom classification focus on type-based classification; sofar there are only comparably few studies on tokenbased classification. Among the earliest studies on token-based classification were the ones by Hashimoto et al. (2006) on Japanese and Katz and Giesbrecht (2006) on German. Hashimoto et al. (2006) present a rule-based system in which lexico-syntactic features of different idioms are hard-coded in a lexicon and then used to distinguish literal and non-literal usages. The features encode information about the passivisation, argument movement, and the ability of the target expression to be negated or modified. Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the"
C10-2078,W06-1203,0,0.466194,"space that 683 Coling 2010: Poster Volume, pages 683–691, Beijing, August 2010 looks at the cohesive ties between the potential idiom and its surrounding discourse. Such features generalize well across different expressions and lead to acceptable performance even on expressions unseen in the training set. 2 Related Work Until recently, most studies on idiom classification focus on type-based classification; sofar there are only comparably few studies on tokenbased classification. Among the earliest studies on token-based classification were the ones by Hashimoto et al. (2006) on Japanese and Katz and Giesbrecht (2006) on German. Hashimoto et al. (2006) present a rule-based system in which lexico-syntactic features of different idioms are hard-coded in a lexicon and then used to distinguish literal and non-literal usages. The features encode information about the passivisation, argument movement, and the ability of the target expression to be negated or modified. Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples. This approach was later"
C10-2078,D09-1033,1,0.878004,"a statistical, frequency-based measure. Birke and Sarkar (2006) model literal vs. nonliteral classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. Sporleder and Li (2009) propose another unsupervised method which detects the presence or absence of cohesive links between the component words of the idiom and the surrounding discourse. If such links can be found the expression is classified as literal otherwise as non-literal. Li and Sporleder (2009) later extended this work by combining the unsupervised classifier with a secondstage supervised classifier. Hashimoto and Kawahara (2008) present a supervised approach to token-based idiom distinction for Japanese, in which they implement several features, such as features known from other word sense disambiguation tasks (e.g., collocations) and idiom-specific features taken from Hashimoto et al. (2006). Finally, Boukobza and Rappoport (2009) also experimented with a supervised classifier, which takes into account various surface features. In the present work, we also investigate supervised m"
C10-2078,E09-1086,1,0.940316,"pically occurs in its canonical form (Riehemann, 2001). Cook et al. exploit this behaviour and propose an unsupervised method in which an expression is classified as idiomatic if it occurs in canonical form and literal otherwise. Canonical forms are determined automatically using a statistical, frequency-based measure. Birke and Sarkar (2006) model literal vs. nonliteral classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. Sporleder and Li (2009) propose another unsupervised method which detects the presence or absence of cohesive links between the component words of the idiom and the surrounding discourse. If such links can be found the expression is classified as literal otherwise as non-literal. Li and Sporleder (2009) later extended this work by combining the unsupervised classifier with a secondstage supervised classifier. Hashimoto and Kawahara (2008) present a supervised approach to token-based idiom distinction for Japanese, in which they implement several features, such as features known from other word sense disambiguation t"
C10-2078,W96-0213,0,\N,Missing
C10-2107,S07-1018,0,0.0848176,"Missing"
C10-2107,W08-2208,0,0.0332928,"t al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes our approach to coverage evaluation, Section 3 discusses the texts analyzed, and the analysis itself appears in Section 5. Section 6 then looks at one possibility for addre"
C10-2107,D09-1003,0,0.0267383,"ion answering. However, studies also found that state-of-the-art FrameNet-style SRL systems perform too poorly to provide any substantial benefit to real applications (Burchardt et al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes ou"
C10-2107,erk-pado-2006-shalmaneser,0,0.371376,"Missing"
C10-2107,D09-1002,0,0.202434,"Missing"
C10-2107,E09-1026,0,0.0593346,"Missing"
C10-2107,J02-3001,0,0.432239,"itself appears in Section 5. Section 6 then looks at one possibility for addressing the coverage problem. The final section presents some discussion and conclusions. 929 (a) (b) Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a lexicographic project whose aim it is to create a l"
C10-2107,P07-1025,0,0.0206462,"dies also found that state-of-the-art FrameNet-style SRL systems perform too poorly to provide any substantial benefit to real applications (Burchardt et al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes our approach to coverage eva"
C10-2107,J08-2003,0,0.0166903,"lyzed, and the analysis itself appears in Section 5. Section 6 then looks at one possibility for addressing the coverage problem. The final section presents some discussion and conclusions. 929 (a) (b) Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a lexicographic project w"
C10-2107,C08-1084,1,0.903036,"Missing"
C10-2107,J05-1004,0,0.109455,"ity for addressing the coverage problem. The final section presents some discussion and conclusions. 929 (a) (b) Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a lexicographic project whose aim it is to create a lexical resource documenting valence structures for differen"
C10-2107,D08-1048,0,0.112106,"any substantial benefit to real applications (Burchardt et al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes our approach to coverage evaluation, Section 3 discusses the texts analyzed, and the analysis itself appears in Section 5. S"
C10-2107,pennacchiotti-etal-2008-towards,0,0.0127403,"any substantial benefit to real applications (Burchardt et al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes our approach to coverage evaluation, Section 3 discusses the texts analyzed, and the analysis itself appears in Section 5. S"
C10-2107,D07-1002,0,0.414803,"g data for each sense. This approach to evaluation arises from the Automated frame-semantic analysis aims to extract from text the key event-denoting predicates and the semantic argument structure for those predicates. The semantic argument structure of a predicate describing an event encodes relationships between the participants involved in the event, e.g. who did what to whom. Knowledge of semantic argument structure is essential for language understanding and thus important for applications such as information extraction (Moschitti et al., 2003; Surdeanu et al., 2003), question answering (Shen and Lapata, 2007), or recognizing textual entailment (Burchardt et al., 2009). Evaluating an existing system for its ability to aid such tasks is unrealistic if the evaluation is lemmabased rather than text-based. Consequently, there continues to be significant interest in developing semantic role labeling (SRL) systems able to automatically compute the semantic argument structures in an input text. Performance on the full text task, though, is typically much lower than for the more restricted evaluations. The SemEval 2007 Task on “Frame Semantic Structure Extraction,” for example, required systems to identify"
C10-2107,P03-1002,0,0.0134661,"d (ii) there is a suitable amount of training data for each sense. This approach to evaluation arises from the Automated frame-semantic analysis aims to extract from text the key event-denoting predicates and the semantic argument structure for those predicates. The semantic argument structure of a predicate describing an event encodes relationships between the participants involved in the event, e.g. who did what to whom. Knowledge of semantic argument structure is essential for language understanding and thus important for applications such as information extraction (Moschitti et al., 2003; Surdeanu et al., 2003), question answering (Shen and Lapata, 2007), or recognizing textual entailment (Burchardt et al., 2009). Evaluating an existing system for its ability to aid such tasks is unrealistic if the evaluation is lemmabased rather than text-based. Consequently, there continues to be significant interest in developing semantic role labeling (SRL) systems able to automatically compute the semantic argument structures in an input text. Performance on the full text task, though, is typically much lower than for the more restricted evaluations. The SemEval 2007 Task on “Frame Semantic Structure Extraction"
C10-2107,J08-2002,0,0.0130768,"discusses the texts analyzed, and the analysis itself appears in Section 5. Section 6 then looks at one possibility for addressing the coverage problem. The final section presents some discussion and conclusions. 929 (a) (b) Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a"
C10-2107,H94-1020,0,0.0333817,"core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a lexicographic project whose aim it is to create a lexical resource documenting valence structures for different word senses and their possible mappings to underlying semantic argument structure (Ruppenhofer et al., 2006). In contrast to PropBank, FrameNet is pr"
C10-2107,S10-1008,1,\N,Missing
C10-2107,ruppenhofer-etal-2010-generating,0,\N,Missing
C14-1059,W98-0706,0,0.0572767,"ification has also employed deeper features, such as distributions of syntactic constructions (see e.g., Kim et al. (2010)). However, not all features that work well for prose carry over to song lyrics. Syntax, for example, is strongly constrained by meter. On the other hand, additional features like meter and rhyme properties might be useful. So far, most studies on lyrics classification have used rather simple features, for example (tf-idf weighted) bags-of-words (Neumayer and Rauber, 2007; Mahedero et al., 2005; Logan et al., 2004), sometimes enriched by synonymy and hypernymy information (Scott and Matwin, 1998). Mayer et al. (2008a; 2008b) also include POS tag distributions, simple text statistics (avg. word length, proportion of hapax legomena per document/line, distribution of punctuation marks and digits, words per minute) and simple (end-of-line) rhyme features. Li and Ogihara (2004) use a similar feature set but also include function word distributions. Finally, Hirjee and Brown (2010) analyse Rap lyrics and focus exclusively on rhyme features, providing a sophisticated statistical rhyme detector which can also identify in-line and slant rhymes. We build on this work but extend the feature spac"
D07-1087,P05-1046,0,0.157693,"s contain the string but is not about a Hyla minuta specimen but about a specimen of type Leptophis ahaetulla (the string Hyla minuta just happens to occur in the SPECIAL REMARKS field). On the other hand, if the genus and species information in an entry was explicitly marked, it would be possible to query specifically for entries whose GENUS is Hyla and whose SPECIES is minuta, thus avoiding the retrieval of entries in which this string occurs in another field. The task of automatically finding and labelling segments in object or event descriptions has been referred to as field segmentation (Grenager et al., 2005).1 It can be seen as a sequence labelling problem, where each text is viewed as a sequence of tokens and the aim is to assign each token a label indicating to what segment the token belongs (e.g., BIOTOPE or LOCATION ). If training data in the form of texts annotated with segment information was readily available, the problem could be approached by training a sequence labeller in a supervised machine learning set-up. However, manually annotated data is rarely available. Creating it from scratch is not only time consuming but usually also requires a certain amount of expert knowledge. Moreover,"
D07-1087,J02-1002,0,0.0128912,"described in Section 3.2. All resulting scores are listed in Table 2. Performance of the systems was measured using a number of different metrics, each reflecting different qualities of the output. The most basic one, token accuracy, simply measures the percentage of tokens that were assigned the correct field type. It has the disadvantage that it does not reflect the quality of the segments that were found. For a more segment-oriented evaluation, we used precision, recall and F-score on correctly identified and labelled segments. As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. In comparison with F-score, it is more forgiving with respect to segment boundaries that are slightly off. The baseline performance scores support our assumption that the contents of the database can be used to learn how to segment and label field book entries, i.e. the increasingly more sophisticated database matching strategies each cause a substantial performance improvement up to 45.1 token accuracy for the trigram lookup with voting strategy. The biggest problem of all baseline approaches is that their performance"
D07-1087,W06-3206,1,0.862908,"Missing"
D07-1087,W05-0611,0,0.0403569,"Missing"
D09-1033,W03-1812,0,0.0412291,"Missing"
D09-1033,W07-1101,0,0.0245773,"Missing"
D09-1033,E06-1042,0,0.229739,"Missing"
D09-1033,W07-1106,0,0.128755,"Missing"
D09-1033,E06-1043,0,0.0696726,"Missing"
D09-1033,J09-1005,0,0.170335,"Missing"
D09-1033,W06-1203,0,0.566517,"Missing"
D09-1033,P99-1041,0,0.173446,"Missing"
D09-1033,N03-1023,0,0.0219678,"Missing"
D09-1033,E09-1086,1,0.824362,"s for drop the ball, Dad had to break the ice on the chicken troughs so that they could get water. Somehow I always end up spilling the beans all over the floor and looking foolish when the clerk comes to sweep them up. Hence, whether a particular occurrence of a potentially ambiguous expression has literal or nonliteral meaning has to be inferred from the context (token-based idiom classification). Recently, there has been increasing interest in this classification task and both supervised and unsupervised techniques have been proposed. The work we present here builds on previous research by Sporleder and Li (2009), who describe an unsupervised method that exploits the presence or absence of cohesive ties between the component words of a potential idiom and its context to distinguish between literal and non-literal use. If strong ties can be found the expression is classified as literal otherwise as non-literal. While this approach often works fairly well, it has the disadvantage that it focuses exclusively on lexical cohesion, other linguistic cues that might influence the classification decision are disregarded. We show that it is possible to improve on Sporleder and Li’s (2009) results by employing a"
D09-1033,D07-1110,0,0.0475428,"Missing"
E09-1086,W03-1812,0,0.497569,"Missing"
E09-1086,W06-1203,0,0.639116,"chose 17 idioms from the Oxford Dictionary of Idiomatic English (Cowie et al., 1997) and other idiom lists found on the internet. The idioms were more or less selected randomly, subject to two constraints: First, because the focus of the present study is on distinguishing literal and non-literal usage, we chose expressions for which we assumed that the literal meaning was not too infrequent. We thus disregarded expressions like play the second fiddle or sail under false colours. Second, in line with many previous approaches to idiom classification (Fazly et al., To appear; Cook et al., 2007; Katz and Giesbrecht, 2006), we focused mainly on expressions of the form V+NP or V+PP as this is a fairly large group and many of these expressions can be used literally as well, making them an ideal test set for our purpose. However, our approach also works for expressions which match a different syntactic pattern and to test the generality of our method we included a couple of these in the data set (e.g., get one’s feet wet). For the same reason, we also included some expressions for which we could not find a literal use in the corpus (e.g., back the wrong horse). For each of the 17 expressions shown in Table 1, we e"
E09-1086,W07-1101,0,0.0773436,"Missing"
E09-1086,W97-0703,0,0.0698517,"e sure that we queried for all combinations of inflected forms (for example Cohesion-based Classifiers We implemented two cohesion-based classifiers: the first one computes the lexical chains for the input text and classifies an expression as literal or non-literal depending on whether its component words participate in any of the chains, the second classifier builds a cohesion graph and determines how this graph changes when the expression is inserted or left out. Chain-based classifier Various methods for building lexical chains have been proposed in the literature (Hirst and St-Onge, 1998; Barzilay and Elhadad, 1997; Silber and McCoy, 2002) but the basic idea is as follows: the content words of the text are considered in sequence and for each word it is determined whether it is similar enough to (the words in) one of the existing chains to be placed in that chain, if not it is placed in a chain of its own. Depending on the chain building algorithm used, a word is placed in a chain if it is related to one other word in the chain or to all of them. The latter strategy is more conservative and tends to lead to shorter but more reliable chains and it is the method we adopted here.6 Note that the chaining alg"
E09-1086,P98-2127,0,0.1965,"omputing semantic relatedness. This is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modelling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). All approaches have advantages and disadvantages. WordNet-based approaches, for instance, typically have a low coverage and only work for so-called “classical relations” like hypernymy, antonymy etc. Distributional approaches usually conflate different word senses and may therefore lead to unintuitive results. For our task, we need to model a wide range of semantic relations (Morris and Hirst, 2004"
E09-1086,E06-1042,0,0.547012,"Missing"
E09-1086,P99-1041,0,0.290279,"Missing"
E09-1086,W06-1605,0,0.0162241,"antic relatedness. This is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modelling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). All approaches have advantages and disadvantages. WordNet-based approaches, for instance, typically have a low coverage and only work for so-called “classical relations” like hypernymy, antonymy etc. Distributional approaches usually conflate different word senses and may therefore lead to unintuitive results. For our task, we need to model a wide range of semantic relations (Morris and Hirst, 2004), for example, relations b"
E09-1086,W04-2607,0,0.00955063,"dle (1990), Lin (1998), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modelling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). All approaches have advantages and disadvantages. WordNet-based approaches, for instance, typically have a low coverage and only work for so-called “classical relations” like hypernymy, antonymy etc. Distributional approaches usually conflate different word senses and may therefore lead to unintuitive results. For our task, we need to model a wide range of semantic relations (Morris and Hirst, 2004), for example, relations based on some kind of functional or situational association, as between fire and coal in (3) or between ice and water in example (1). Likewise we also need to model relations between non-nouns, for instance between spill and sweep up in example (2). Some relations also require world-knowledge, as in example (7), where the literal usage of drop the ball is not only indicated by the presence of goalkeeper but also by knowing that Wayne Rooney and Kevin Campbell are both football players. Unfortunately, there are also a few cases in which a cohesion-based approach fails."
E09-1086,W07-1106,0,0.59697,"ering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. While the seed sets are created without immediate human intervention they do rely on manually created resources such as databases of known idioms. 1 This is also the form in which an idiom is usually listed in a dictionary. 2 Decomposable idioms are expressions such as spill the beans which have a composite meaning whose parts can be mapped to the words of the expression (e.g., spill→’reveal’, beans→’secret’). Cook et al. (2007) and Fazly et al. (To appear) propose an alternative method which crucially re755 3.2 gree of lexical cohesion with their context, at least not if one assumes a literal meaning for their component words. Hence if the component words of a potentially idiomatic expression do not participate in any lexical chain, it is likely that the expression is indeed used idiomatically, otherwise it is probably used literally. For instance, in example (3), where the expression play with fire is used in a literal sense, the word fire does participate in a chain (shown in bold face) that also includes the word"
E09-1086,J02-4004,0,0.0416361,"ll combinations of inflected forms (for example Cohesion-based Classifiers We implemented two cohesion-based classifiers: the first one computes the lexical chains for the input text and classifies an expression as literal or non-literal depending on whether its component words participate in any of the chains, the second classifier builds a cohesion graph and determines how this graph changes when the expression is inserted or left out. Chain-based classifier Various methods for building lexical chains have been proposed in the literature (Hirst and St-Onge, 1998; Barzilay and Elhadad, 1997; Silber and McCoy, 2002) but the basic idea is as follows: the content words of the text are considered in sequence and for each word it is determined whether it is similar enough to (the words in) one of the existing chains to be placed in that chain, if not it is placed in a chain of its own. Depending on the chain building algorithm used, a word is placed in a chain if it is related to one other word in the chain or to all of them. The latter strategy is more conservative and tends to lead to shorter but more reliable chains and it is the method we adopted here.6 Note that the chaining algorithm has a free paramet"
E09-1086,E06-1043,0,0.525483,"Missing"
E09-1086,D07-1110,0,0.0466045,"Missing"
E09-1086,P90-1034,0,0.48679,"le method for computing semantic relatedness. This is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modelling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). All approaches have advantages and disadvantages. WordNet-based approaches, for instance, typically have a low coverage and only work for so-called “classical relations” like hypernymy, antonymy etc. Distributional approaches usually conflate different word senses and may therefore lead to unintuitive results. For our task, we need to model a wide range of semantic relations (Morris and"
E09-1086,J09-1005,0,\N,Missing
E09-1086,J06-1003,0,\N,Missing
E09-1086,C98-2122,0,\N,Missing
H05-1033,W04-2504,0,0.0517817,"Missing"
H05-1033,A00-2018,0,0.0349467,"Missing"
H05-1033,W97-1311,0,0.0605045,"Missing"
H05-1033,J02-1002,0,0.0174422,"l humans agree on discourse chunk segmentation and labelling in order to establish an upper bound for the task. We measured both unlabelled and labelled agreement on the 52 doubly annotated RST-DT texts. The former measures whether humans agree in placing chunk boundaries, whereas the latter additionally measures whether humans agree in assigning chunk labels. To facilitate comparison with our models we report inter-annotator agreement in terms of accuracy and F-score.3 For the unlabelled case we also report Window Difference (WDiff), a commonly used evaluation measure for segmentation tasks (Pevzner and Hearst, 2002). It returns values between 0 (identical segmentations) and 1 (maximally different segmentations) and differs from accuracy in that predicted boundaries which are only slightly off are penalised less than those which are completely wrong. Human agreement is relatively high4 on both segmentation and span labelling (see Table 1), which can be explained by the fact that (i) the RST-DT annotators were given very detailed and precise instructions and (ii) assigning boundaries and labels is an easier task than creating full-scale discourse trees. 4.2 One-Step Chunking For the one-step chunking metho"
H05-1033,W95-0107,0,0.0105372,"t plunged. and determine that the first of these functions as a nucleus at the lowest level of the tree whereas the latter two function as satellites. We do not try to determine that the first two edus are merged at a higher level and then function as the overall nucleus of the sentence. The discourse chunking task assumes a nonhierarchical representation. We converted each sentence-level discourse tree into a flat chunk representation by assigning each token (i.e., word or punctuation mark) a tag encoding its nuclearity status at the edu level. We adopted the chunk representation proposed by Ramshaw and Marcus (1995) and used four different tags: B - NUC and B - SAT for nucleus and satellite-initial tokens, and I - NUC and I - SAT for non-initial tokens, i.e., tokens inside a nucleus and satellite span. As all tokens belong either to a nucleus or a satellite span, we do not need a special tag (typically denoted by O in syntactic chunking) to indicate elements outside a chunk. The chunk representation for the sentence in Figure 1 is thus: “/B - NUC I/I - NUC am/I - NUC optimistic/I - NUC ”/I - NUC said/B - SAT Mr./I - SAT Smith/I - SAT as/B - SAT the/I - SAT market/I - SAT plunged/I SAT ./ I - SAT Discours"
H05-1033,N03-1026,0,0.0133272,"nlabelled Labelled 95 F-score 90 85 80 75 70 65 60 0 472 949 1,428 1,8872,350 2,823 3,290 3,8524,258 4,734 Number of sentences in training data Figure 2: Learning curve for discourse segmentation (unlabelled) and span labelling (labelled) 4.4 Sentence Compression Sentence compression can be likened to summarisation at the sentence level. The task has an immediate impact on several applications ranging from summarisation to audio scanning devices for the blind and caption generation (see Knight and Marcu, 2002 and the references therein). Previous datadriven approaches (Knight and Marcu, 2003; Riezler et al., 2003) relied on parallel corpora to determine what is important in a sentence. The models learned correspondences between long sentences and their shorter counterparts, typically employing a rich feature space induced from parse trees. The task is challenging since the compressed sentences should retain essential information and convey it grammatically. Here, we propose a complementary approach which utilises discourse chunking. A compressed sentence can be obtained from the output of the chunker simply by removing satellites. We thus capitalise on RST’s (Mann and Thompson, 1987) notion of nucleari"
H05-1033,N03-1030,0,0.865213,"segments) are linked to each other by rhetorical relations (e.g., Contrast, Elaboration). Discourse units are further characterised in terms of their text importance: nuclei denote central segments, whereas satellites denote peripheral ones. Recent advances in discourse modelling have greatly benefited from the availability of resources annotated with discourse-level information such as the RST Discourse Treebank (RST-DT, Carlson et al., 2002). Even though discourse parsing at the document-level still poses a significant challenge to data-driven methods, sentence-level discourse models (e.g., Soricut and Marcu, 2003) trained on the RST-DT have attained accuracies comparable to human performance. The availability of discourse annotations is partly responsible for the success of these models. Another important reason is the development of robust syntactic parsers (e.g., Charniak, 2000) that can be used to provide critical structural and lexical information to the discourse parser. Unfortunately, discourse annotated corpora are largely absent for languages other than English. Furthermore, reliance on syntactic parsing renders discourse parsing practically impossible for languages for which state-of-the-art p"
H05-1033,J02-4002,0,0.0684054,"Missing"
H05-1033,W00-0733,0,0.0172664,"n be achieved by incorporating additional features into the labeller, such as the number of chunks in the sentence or the length of the current chunk. A two-step approach also avoids the creation of illegal chunk sequences, such as “B - SAT I - NUC”. However, a potential drawback is that the number of training examples for the labeller is reduced as the instances to be classified are chunks rather than tokens. We explore the performance of the one-step and the two-step methods in Sections 4.2 and 4.3, respectively. 1 A similar approach has been proposed for syntactic chunking, e.g., Tjong Kim Sang (2000). 259 A variety of learning schemes can be employed for the discourse chunking task. We have experimented with Boosting (Schapire and Singer, 2000), Conditional Random Fields (Lafferty et al., 2001), and Support Vector Machines (Vapnik, 1998). Discussion of our results focuses exclusively on boosting, since it had a slight advantage over the other methods. Boosting combines many simple, moderately accurate categorisation rules into a single, highly accurate rule. We used BoosTexter’s (Schapire and Singer, 2000) implementation, which combines boosting with simple decision rules. The system perm"
H05-1033,A00-2024,0,\N,Missing
I11-1021,S07-1018,0,0.028347,"ces required to reach a given performance level using supervised machine learning techniques. This is accomplished by allowing the learner to guide the selection of examples to be annotated and added to the training set; at each iteration the learner queries for the example (or set of examples) that will be most informative to its present state. AL is an attractive idea for natural language processing (NLP) because of its potential to dramatically reduce the 1 For recent work on SRL, see, among others: (Das et al., 2010; Hajiˇc et al., 2009; Surdeanu et al., 2008; Carreras and M`arquez, 2005; Baker et al., 2007). 183 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 183–191, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP (1) compressed dependency trees encode the target predicate and the key dependents of the verb complex in a sentence. As illustrated in Section 3, the structural relationships defined by the compressed dependency trees well encapsulate key features used in automatic SRL. For a more complete picture of the potential for AL with respect to SRL, we investigate a set of strategies designed to select the most informative training exampl"
I11-1021,N04-1012,0,0.0730589,"Missing"
I11-1021,J05-1004,0,0.156725,"sentence involves identification and disambiguation of target predicates as well as identification and labeling of their arguments. Because our focus is on the active learning more so than on the semantic role labeling itself, we address only the argument labeling stage of the process, assuming that predicates and argument spans alike have already been identified and correctly labeled. Broadly speaking, there are two different styles of semantic parsing and semantic role labeling (SRL): those based on FrameNet-style analysis (Ruppenhofer et al., 2006) and those using PropBank-style analysis (Palmer et al., 2005). This work takes the PropBank approach, which considers only verbal predicates and is strongly tied to syntactic structure. In (1), for example, the two arguments of the predicate idolize are labeled as Arg0 and Arg1. 2 Note that logistic regression is used together with a regularized term to avoid the overfitting problem by penalizing the complexity of the trained model. Generally, the regularized term is defined as a function of the learned parameters over the weights. The L1 regularization, also called lasso penalty, is used to penalize both large and small weights. 3 In ongoing work, we r"
I11-1021,P05-1072,0,0.0237605,"y’, ’as a result of’) Active or passive LOC, TMP, etc. 1) Sbj*, obj* are defined as: Sbj* ← Obj Passive Sbj* ← LGS passive Sbj* ← Active vt sbj date not set . 3 5 7 8 Alternation if applicable Compressed Dependency Tree OBJ* P ADV date not set . 3 5 7 8 Figure 1: Producing compressed dependency tree Obj* ← Sbj Passive Obj* ← Sbj VI (intransitive verb) Obj* ← Obj Active VT = 1; transitive VI = 2; intransitive TO IM=3; begins with ’to’ V Adj = 4; verb followed by adjective words (e.g. ’sounds good’, ’looks pretty’) PV = 5; phrasal verb (e.g. ’pick up’) e.g. ”has not been set” in figure 1 2008b; Pradhan et al., 2005). At the same time, much of the structural and relational information represented in a dependency tree is not relevant for the SRL task. We use a compressed dependency tree (CDT) to encode just the relationships between a target predicate and the key dependents of the verb complex. The new tree is always rooted in the target predicate, which often means resetting the root from an auxiliary or other finite main verb. We generate the CDT from the output of an existing dependency parser through the process described in a simplified form below, using the example sentence in Fig. 1. adjectival comp"
I11-1021,J08-2006,0,0.0326744,"may be due to a conflict between the two selection criteria. In any event, there is clearly a trade-off between informativity and representativeness, and results are influenced by the details of the manner of combining the two. The results of other INF / REP combinations are presented in Table 2, in terms of their reduction in error compared to random selection. 6.3 7 Related Work Much research efforts have been devoted to statistical machine learning methodologies for SRL (Bjkelund et al., 2009; Gildea and Jurafsky, 2002; Shi et al., 2009; Johansson and Nugues, 2008a; Lang and Lapata, 2010; Pradhan et al., 2008; F¨urstenau and Lapata, 2009; Titov and Klementiev, 2011, among others). For example, Johansson et al. (Johansson and Nugues, 2008a) applied logistic regression with L2 norm to dependency-based SRL. Similarly, we also use logistic regression to train the classifier with a probabilistic explanation. However, we use L1 normed Weighting the two criteria Finally, we set α with different values (i.e., 0.3, 0.5 and 0.7) to investigate how the trade-off between informativity and representativeness may affect the SRL performance. We also compare our solution to the information density solution propos"
I11-1021,W07-1516,0,0.063528,"Missing"
I11-1021,D08-1112,0,0.558796,"pool P of unlabeled sentences, for every unlabeled sentence s ∈ P , the representativeness of the sentence, denoted as rep(s), is defined as nsimilar , representing the number of edges in the pool that are similar to the edges of the CDT for s. Intuitively, the larger the number of similar CDT edges in the unlabeled pool, the more representative the sentence is overall of the input data. Representativeness A disadvantage of selecting examples based only on informativity is the tendency of the learner to query outliers (Settles, 2010). It has therefore been proposed (Dredze and Crammer, 2008; Settles and Craven, 2008) to temper such selection strategies with a notion of relevance or representativeness. Ours is the first work to use such a combined strategy for SRL. We measure the representativeness of unlabeled sentences based on sentence similarity, taking two different approaches: cosine similarity, and a measure based on CDTs. COS: Cosine Similarity. Given two sentences s and s0 , let i1 , i2 , . . . , im , and i01 , i02 ,. . . ,i0n be their instances, respectively. The similarity of the two sentences, denoted as similarity(s, s0 ), is defined Pm P as j=1 nk=1 sim(ij , i0k ), where sim(ij , i0k ) is the"
I11-1021,D07-1002,0,0.0574311,"and experiment on SRL task. They defined structured output by constraining the relations among class labels, e.g., one predicate only has one of the labels. The classification problem is defined via constraints among output labels. The most uncertain instances are selected to satisfy predefined constraints. Rather than a structured relation between output labels, our work exploits the structure of the sentences themselves via compressed dependency trees. In the area of sentence similarity measurement, most current work focuses on semantic similarity (Haghighi et al., 2005; Tang et al., 2002; Shen and Lapata, 2007). We define similarity between sentences in terms of the nodes and edges in the dependency tree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the specific dependency relations and collapse the t"
I11-1021,P04-1075,0,0.0687189,"Missing"
I11-1021,P02-1016,0,0.0398086,"r structured output and experiment on SRL task. They defined structured output by constraining the relations among class labels, e.g., one predicate only has one of the labels. The classification problem is defined via constraints among output labels. The most uncertain instances are selected to satisfy predefined constraints. Rather than a structured relation between output labels, our work exploits the structure of the sentences themselves via compressed dependency trees. In the area of sentence similarity measurement, most current work focuses on semantic similarity (Haghighi et al., 2005; Tang et al., 2002; Shen and Lapata, 2007). We define similarity between sentences in terms of the nodes and edges in the dependency tree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the specific dependency rela"
I11-1021,P11-1145,0,0.0598375,"cally by training classifiers using large annotated corpora.1 Although such approaches can achieve reasonably good performance, annotating a large corpus is still expensive and time consuming. Moreover, the performance of trained classifiers may degrade remarkably when they are applied to out-of-domain data (Johansson and Nugues, 2008a). There is very little work on AL for SRL (e.g. Roth and Small (2006)), although much interesting work has been done with semi-supervised and unsupervised approaches to the problem (Grenager and Manning, 2006; F¨urstenau and Lapata, 2009; Lang and Lapata, 2010; Titov and Klementiev, 2011, among others). In this paper we explore the use of compressed dependency trees (CDTs) as features for supervised semantic role labeling and, most importantly, as a way of measuring how representative an individual instance is of the input data. We then incorporate representativeness as part of the metric used for sample selection in active learning. The This paper explores new approaches to active learning (AL) for semantic role labeling (SRL), focusing in particular on combining typical informativity-based sampling strategies with a novel measure of representativeness based on compressed de"
I11-1021,W09-1906,0,0.0134804,"dependency relations (e.g., subject, modifier). Thus, a dependency tree of a sentence encodes the dependency relation between the head words and their dependents. It has been reported that SRL can benefit from phrase-structure and dependency-based syntactic parsing (Hacioglu, 2004; Johansson and Nugues, 185 representativeness, or how well a training example represents the overall input patterns of the unlabeled data. While some results from AL are robust across different datasets and even different tasks, it is clear that there is no single approach to AL that is suitable for all situations (Tomanek and Olsson, 2009). Because there is very little previous work on AL for the task of semantic role labeling, we do not assume previous solutions but rather investigate a number of different strategies. 5. Heuristically determine voice of clause and alter some CDT dependency labels(e.g. S BJ PASSIVE becomes O BJ *); these are the asterisk-marked relations in Table. 1. For example, in (2): (2) At the same time, the government did not want to appear to favor GM by allowing a minority stake that might preclude a full bid by Ford. 4.2 the verb complex is {did, n’t, want, appear, favor}. The subject phrase the govern"
I11-1021,W07-1406,0,0.0316028,"tisfy predefined constraints. Rather than a structured relation between output labels, our work exploits the structure of the sentences themselves via compressed dependency trees. In the area of sentence similarity measurement, most current work focuses on semantic similarity (Haghighi et al., 2005; Tang et al., 2002; Shen and Lapata, 2007). We define similarity between sentences in terms of the nodes and edges in the dependency tree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the specific dependency relations and collapse the trees, aiming for structural rather than semantic similarity. In addition, Filippova et al. (Filippova and Strube, 2008) proposed to compress a sentence using dependency trees and take the importance of words as weight. They found compressed dependency tree can better ensure the gr"
I11-1021,D09-1031,1,0.902092,"Missing"
I11-1021,W05-0620,0,0.0851586,"Missing"
I11-1021,N10-1138,0,0.0227454,"Missing"
I11-1021,de-marneffe-etal-2006-generating,0,0.0106804,"Missing"
I11-1021,P08-2059,0,0.0219528,"art-of-speech tag. Given a pool P of unlabeled sentences, for every unlabeled sentence s ∈ P , the representativeness of the sentence, denoted as rep(s), is defined as nsimilar , representing the number of edges in the pool that are similar to the edges of the CDT for s. Intuitively, the larger the number of similar CDT edges in the unlabeled pool, the more representative the sentence is overall of the input data. Representativeness A disadvantage of selecting examples based only on informativity is the tendency of the learner to query outliers (Settles, 2010). It has therefore been proposed (Dredze and Crammer, 2008; Settles and Craven, 2008) to temper such selection strategies with a notion of relevance or representativeness. Ours is the first work to use such a combined strategy for SRL. We measure the representativeness of unlabeled sentences based on sentence similarity, taking two different approaches: cosine similarity, and a measure based on CDTs. COS: Cosine Similarity. Given two sentences s and s0 , let i1 , i2 , . . . , im , and i01 , i02 ,. . . ,i0n be their instances, respectively. The similarity of the two sentences, denoted as similarity(s, s0 ), is defined Pm P as j=1 nk=1 sim(ij , i0k ),"
I11-1021,W08-1105,0,0.0289968,"ree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the specific dependency relations and collapse the trees, aiming for structural rather than semantic similarity. In addition, Filippova et al. (Filippova and Strube, 2008) proposed to compress a sentence using dependency trees and take the importance of words as weight. They found compressed dependency tree can better ensure the grammaticality of the sentences to preserve the same lexical meaning as much as possible. In our work, we are more interested in the explicit dependency relation of predicate-argument pairs. Our goal is to apply compressed dependency tree to extract 8 Conclusions This paper investigates the use of active learning for semantic role labeling. To improve the learning accuracy and reduce the size of training set, compressed dependency trees"
I11-1021,E09-1026,0,0.0383099,"Missing"
I11-1021,J02-3001,0,0.0435082,"350, the solution of using INF 2 only achieves a higher accuracy than the combined solution. This may be due to a conflict between the two selection criteria. In any event, there is clearly a trade-off between informativity and representativeness, and results are influenced by the details of the manner of combining the two. The results of other INF / REP combinations are presented in Table 2, in terms of their reduction in error compared to random selection. 6.3 7 Related Work Much research efforts have been devoted to statistical machine learning methodologies for SRL (Bjkelund et al., 2009; Gildea and Jurafsky, 2002; Shi et al., 2009; Johansson and Nugues, 2008a; Lang and Lapata, 2010; Pradhan et al., 2008; F¨urstenau and Lapata, 2009; Titov and Klementiev, 2011, among others). For example, Johansson et al. (Johansson and Nugues, 2008a) applied logistic regression with L2 norm to dependency-based SRL. Similarly, we also use logistic regression to train the classifier with a probabilistic explanation. However, we use L1 normed Weighting the two criteria Finally, we set α with different values (i.e., 0.3, 0.5 and 0.7) to investigate how the trade-off between informativity and representativeness may affect"
I11-1021,W06-1601,0,0.204667,"Missing"
I11-1021,C04-1186,0,0.0326564,"sion process, we use output from the Stanford parser to complement the dependency relations found in the gold-standard data. Dependency Tree Compression Given a sentence, the task of dependency parsing is to identify the head word and its corresponding dependents and to classify their functional relationships according to a set of dependency relations (e.g., subject, modifier). Thus, a dependency tree of a sentence encodes the dependency relation between the head words and their dependents. It has been reported that SRL can benefit from phrase-structure and dependency-based syntactic parsing (Hacioglu, 2004; Johansson and Nugues, 185 representativeness, or how well a training example represents the overall input patterns of the unlabeled data. While some results from AL are robust across different datasets and even different tasks, it is clear that there is no single approach to AL that is suitable for all situations (Tomanek and Olsson, 2009). Because there is very little previous work on AL for the task of semantic role labeling, we do not assume previous solutions but rather investigate a number of different strategies. 5. Heuristically determine voice of clause and alter some CDT dependency"
I11-1021,H05-1049,0,0.0348322,"e learning framework for structured output and experiment on SRL task. They defined structured output by constraining the relations among class labels, e.g., one predicate only has one of the labels. The classification problem is defined via constraints among output labels. The most uncertain instances are selected to satisfy predefined constraints. Rather than a structured relation between output labels, our work exploits the structure of the sentences themselves via compressed dependency trees. In the area of sentence similarity measurement, most current work focuses on semantic similarity (Haghighi et al., 2005; Tang et al., 2002; Shen and Lapata, 2007). We define similarity between sentences in terms of the nodes and edges in the dependency tree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the speci"
I11-1021,W09-1201,0,0.109155,"Missing"
I11-1021,J04-3001,0,0.0623597,"Missing"
I11-1021,D08-1008,0,0.284553,"annotated data and the expense of annotating new data are at least as relevant for semantic role labeling (SRL) as for the above-mentioned areas of NLP. Existing work on automatic SRL usually explores supervised machine learning approaches to mark the semantic roles of predicates automatically by training classifiers using large annotated corpora.1 Although such approaches can achieve reasonably good performance, annotating a large corpus is still expensive and time consuming. Moreover, the performance of trained classifiers may degrade remarkably when they are applied to out-of-domain data (Johansson and Nugues, 2008a). There is very little work on AL for SRL (e.g. Roth and Small (2006)), although much interesting work has been done with semi-supervised and unsupervised approaches to the problem (Grenager and Manning, 2006; F¨urstenau and Lapata, 2009; Lang and Lapata, 2010; Titov and Klementiev, 2011, among others). In this paper we explore the use of compressed dependency trees (CDTs) as features for supervised semantic role labeling and, most importantly, as a way of measuring how representative an individual instance is of the input data. We then incorporate representativeness as part of the metric us"
I11-1021,C08-1050,0,0.0873161,"annotated data and the expense of annotating new data are at least as relevant for semantic role labeling (SRL) as for the above-mentioned areas of NLP. Existing work on automatic SRL usually explores supervised machine learning approaches to mark the semantic roles of predicates automatically by training classifiers using large annotated corpora.1 Although such approaches can achieve reasonably good performance, annotating a large corpus is still expensive and time consuming. Moreover, the performance of trained classifiers may degrade remarkably when they are applied to out-of-domain data (Johansson and Nugues, 2008a). There is very little work on AL for SRL (e.g. Roth and Small (2006)), although much interesting work has been done with semi-supervised and unsupervised approaches to the problem (Grenager and Manning, 2006; F¨urstenau and Lapata, 2009; Lang and Lapata, 2010; Titov and Klementiev, 2011, among others). In this paper we explore the use of compressed dependency trees (CDTs) as features for supervised semantic role labeling and, most importantly, as a way of measuring how representative an individual instance is of the input data. We then incorporate representativeness as part of the metric us"
I11-1021,N10-1137,0,0.198237,"of predicates automatically by training classifiers using large annotated corpora.1 Although such approaches can achieve reasonably good performance, annotating a large corpus is still expensive and time consuming. Moreover, the performance of trained classifiers may degrade remarkably when they are applied to out-of-domain data (Johansson and Nugues, 2008a). There is very little work on AL for SRL (e.g. Roth and Small (2006)), although much interesting work has been done with semi-supervised and unsupervised approaches to the problem (Grenager and Manning, 2006; F¨urstenau and Lapata, 2009; Lang and Lapata, 2010; Titov and Klementiev, 2011, among others). In this paper we explore the use of compressed dependency trees (CDTs) as features for supervised semantic role labeling and, most importantly, as a way of measuring how representative an individual instance is of the input data. We then incorporate representativeness as part of the metric used for sample selection in active learning. The This paper explores new approaches to active learning (AL) for semantic role labeling (SRL), focusing in particular on combining typical informativity-based sampling strategies with a novel measure of representativ"
I11-1021,W08-2121,0,\N,Missing
I11-1021,N07-1070,0,\N,Missing
J12-2001,dalianis-velupillai-2010-certain,0,0.0252184,"/rgai/bioscope. Last accessed on 8 December 2011. 237 Computational Linguistics Volume 38, Number 2 levels of certainty are deﬁned: low confidence or considerable speculation, high confidence or slight speculation, and no expression of uncertainty or speculation. Information about negation is encoded in the LEXICAL POLARITY dimension, which identiﬁes negated events. Negation is deﬁned here as “the absence or non-existence of an entity or a process.” For languages other than English there are much fewer resources. A corpus of 6,740 sentences from the Stockholm Electronic Patient Record Corpus (Dalianis and Velupillai 2010) has been annotated with certain and uncertain expressions as well as speculative and negation cues, with the purpose of creating a resource for the development of automatic detection of speculative language in Swedish clinical text. The categories used are: certain, uncertain, and undefined at sentence level, and negation, speculative words, and undefined speculative words at token level. Inter-annotator agreement for certain sentences and negation are high, but for the rest of the classes results are lower. 5. Detection of Speculative Sentences Initial work on processing speculation focuses"
J12-2001,P08-1118,0,0.0290135,"Missing"
J12-2001,W09-3012,0,0.0577069,"nd the predicate 245 Computational Linguistics Volume 38, Number 2 that is within their scope (target). They describe two modality taggers that identify modality cues and modality targets, a string-based tagger and a structure-based tagger, and compare their performances. The string-based tagger takes as input text tagged with PoS and marks as modality cues words or phrases that match exactly cues from a modality lexicon. More information about the modality taggers and their application in machine translation can be found in the article by Baker et al. included in this special issue. Finally, Diab et al. (2009) model belief categorization as a sequence labeling task, which allows them to treat cue detection and scope recognition in a uniﬁed fashion. Diab et al. distinguish three belief categories. For committed belief the writer indicates clearly that he or she believes a proposition. In the case of non-committed belief the writer identiﬁes the proposition as something in which he or she could believe but about which the belief is not strong. This category is further subdivided into weak belief, which is often indicated by modals, such as may, and reported speech. The ﬁnal category, not applicable,"
J12-2001,W08-0607,0,0.0495649,"lection procedure that allows a reduction of the number of keyword candidates from 2,407 to 253. The system is trained on the data set of Medlock and Briscoe and evaluated on four newly annotated biomedical full articles8 and on radiology reports. The best results of the system are achieved by performing automatic and manual feature selection consecutively and by adding external dictionaries. The ﬁnal results on biomedical articles are 85.29 BEP and 85.08 F1 score. The results for the external corpus of radiology reports are lower, at 82.07 F1 score. A different type of system is presented by Kilicoglu and Bergler (2008), who apply a linguistically motivated approach to the same classiﬁcation task by using knowledge from existing lexical resources and incorporating syntactic patterns, including unhedgers, lexical cues, and patterns that strongly suggest non-speculation. Additionally, hedge cues are weighted by automatically assigning an information gain measure to them and by assigning weights semi-automatically based on their types and centrality to hedging. The hypothesis behind this approach is that “a more linguistically oriented approach can enhance recognition of speculative language.” The results are e"
J12-2001,W09-1418,0,0.0226542,"Missing"
J12-2001,W09-1401,0,0.0763386,"Missing"
J12-2001,C04-1200,0,0.0171358,"polarity of an expression (Kennedy and Inkpen 2006; Polanyi and Zaenen 2006), or by introducing speciﬁc negation features (Wilson, Wiebe, and Hoffman 2005; Wilson, Wiebe, and Hwa 2006; Wilson 2008). It was found that these more sophisticated models typically lead to a signiﬁcant improvement over a simple bag-of-words model with negation preﬁxes. This improvement can to a large extent be directly attributed to the better modeling of negation (Wilson, Wiebe, and Hoffman 2009). Whereas modeling negation in opinion mining frequently involves determining the polarity of opinions (Hu and Liu 2004; Kim and Hovy 2004; Wilson, Wiebe, and Hoffman 2005; Wilson 2008), some researchers have also used negation models to 14 The three terms are used sometimes interchangeably and sometimes reserved for somewhat different contexts. We follow here the deﬁnitions of Pang and Lee (2008) who use “opinion mining” and “sentiment analysis” as largely synonymous terms and “subjectivity analysis” as a cover term for both. 251 Computational Linguistics Volume 38, Number 2 determine the strength of opinions (Popescu and Etzioni 2005; Wilson, Wiebe, and Hwa 2006). Choi and Cardie (2010) found that performing both tasks jointly"
J12-2001,W04-3103,0,0.63478,"Missing"
J12-2001,matsuyoshi-etal-2010-annotating,0,0.0713208,"SSP is underlined. 234 Morante and Sporleder Modality and Negation for the same event are found in Example (16). Discriminatory co-predication tests are provided for the annotators to determine the factuality of events. The interannotator agreement reported for assigning factuality values is κcohen 0.81. (15) John knows whether Mary came. (16) a. John does not know whether Mary came. b. John does not know that Mary came. c. John knows that Paul said that Mary came. A corpus of 50,108 event mentions in blogs and Web posts in Japanese has been annotated with information about extended modality (Matsuyoshi et al. 2010). The annotation scheme of extended modality is based on four desiderata: information should be assigned to the event mention; the modality system has to be language independent; polarity should be divided into two classes: POLARITY ON THE ACTUALITY of the event and SUBJECTIVE POLARITY from the perspective of the source’s evaluation; and the annotation labels should not be too ﬁne-grained. In Example (17) the polarity on actuality is negative for the events STUDY and PASS because they did not occur, but the subjective polarity for the PASS event is positive. Extended modality is characterized"
J12-2001,P07-1125,0,0.755745,"peculative is not. The annotation work by Wilbur, Rzhetsky, and Shatkay (2006) is motivated by the need to identify and characterize parts of scientiﬁc documents where reliable information can be found. They deﬁne ﬁve dimensions to characterize scientiﬁc sentences: FOCUS (scientific versus general), POLARITY (positive versus negative statement), LEVEL OF CERTAINTY in the range 0–3, STRENGTH of evidence, and DIRECTION / TREND (increase or decrease in certain measurement). A corpus5 of six articles from the functional genomics literature has been annotated at the sentence level for speculation (Medlock and Briscoe 2007). Sentences are annotated as being speculative or not. Of the 1,157 sentences, 380 were found to be speculative. An inter-annotator agreement of 0.93 κcohen is reported. BioInfer (Pyysalo et al. 2007) is a corpus of 1,100 sentences from abstracts of biomedical research articles annotated with protein, gene, and RNA relationships. The annotation scheme captures information about the absence of a relation. Statements expressing absence of a relation such as not affected by or independent of are annotated using a predicate NOT, as in this example: not:NOT(affect:AFFECT(deletion of SIR3, silencing"
J12-2001,morante-2010-descriptive,1,0.823267,"cal opacity]. b. This result [suggests that the valency of Bi in the material is smaller than +3]. Because the Genia Event and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al. (2010). Their study shows that the scopes of BioScope are not directly useful to detect the certainty status of the events in Genia, and that the BioScope annotation is more easily adaptable to non-biomedical applications. A description of negation cues and their scope in biomedical texts, based on the cues that occur in the BioScope corpus, can be found in Morante (2010), where information is provided relative to the ambiguity of the negation cue and to the type of scope, as well as examples. The description shows that the scope depends mostly on the PoS of the cue and on the syntactic features of the clause. The NaCTeM team has annotated events in biomedical texts with meta-knowledge that includes polarity and modality (Thompson et al. 2008). The modality categorization scheme covers epistemic modality and speculation and contains information about the following dimensions: KNOWLEDGE TYPE, LEVEL OF CERTAINTY, and POINT OF VIEW. Four types of knowledge are de"
J12-2001,W09-1304,1,0.924573,"presentation of text chunking as a tagging problem and by the standard CoNLL representation format (Buchholz and Marsi 2006). By setting up the task in this way they show that the task can be modeled as a sequence labeling problem, and by conforming to the existing CoNLL standards they show that scope resolution could be integrated in a joint learning setting with dependency parsing and semantic role labeling. Their system is a memory-based scope ﬁnder that tackles the task in two phases: cue identiﬁcation and scope resolution, which are modeled as consecutive token level classiﬁcation tasks. Morante and Daelemans (2009b) present another scope resolution system that uses a different architecture, can deal with multiword negation cues, and is tested on the three subcorpora of the BioScope corpus. For resolving the scope, three classiﬁers (kNN, SVM, CRF++) predict whether a token is the ﬁrst token in the scope sequence, the last, or neither. A fourth classiﬁer is a metalearner that uses the predictions of the 246 Morante and Sporleder Modality and Negation three classiﬁers to predict the scope classes. The system is evaluated on three corpora using as measure the percentage of fully correct scopes (PCS), which"
J12-2001,W09-1105,1,0.898782,"presentation of text chunking as a tagging problem and by the standard CoNLL representation format (Buchholz and Marsi 2006). By setting up the task in this way they show that the task can be modeled as a sequence labeling problem, and by conforming to the existing CoNLL standards they show that scope resolution could be integrated in a joint learning setting with dependency parsing and semantic role labeling. Their system is a memory-based scope ﬁnder that tackles the task in two phases: cue identiﬁcation and scope resolution, which are modeled as consecutive token level classiﬁcation tasks. Morante and Daelemans (2009b) present another scope resolution system that uses a different architecture, can deal with multiword negation cues, and is tested on the three subcorpora of the BioScope corpus. For resolving the scope, three classiﬁers (kNN, SVM, CRF++) predict whether a token is the ﬁrst token in the scope sequence, the last, or neither. A fourth classiﬁer is a metalearner that uses the predictions of the 246 Morante and Sporleder Modality and Negation three classiﬁers to predict the scope classes. The system is evaluated on three corpora using as measure the percentage of fully correct scopes (PCS), which"
J12-2001,D08-1075,1,0.923051,"Missing"
J12-2001,W10-3006,1,0.910251,"Missing"
J12-2001,W10-3112,0,0.195715,"Missing"
J12-2001,C10-1155,0,0.0417093,"Missing"
J12-2001,D09-1145,0,0.0667183,"Missing"
J12-2001,W02-1011,0,0.0197905,"Missing"
J12-2001,P09-1077,0,0.0330926,"Missing"
J12-2001,H05-1043,0,0.0225137,"Missing"
J12-2001,C10-2117,0,0.0342491,"Missing"
J12-2001,W06-0305,0,0.0595174,"Missing"
J12-2001,prasad-etal-2008-penn,0,0.057837,"Missing"
J12-2001,W95-0107,0,0.0504303,"Missing"
J12-2001,D08-1002,0,0.0155781,", which varies depending on whether the negated object is an event, an entity, or a state. For events, the negation is assumed to scope over the whole predicate–argument structure. For entities and for states realized by nominalizations the negation is assumed to scope over the whole NP. Implicit negations are detected by searching for antonymy chains in WordNet. de Marneffe, Rafferty, and Manning (2008) also make use of negation detection to discover contradictions. They do so rather implicitly, however, by using a number of features which check for explicit negation, polarity, and antonymy. Ritter et al. (2008) present a contradiction detection system that uses the T EXT R UNNER system (Banko et al. 2007) to extract relations of the form R(x,y) (e.g., was born in(Mozart,Salzburg)). They then inspect potential contradictions (i.e., relations which overlap in one variable but not in the other) and ﬁlter out non-contradictions by looking, for example, for synonyms and meronyms. In the context of contrast detection in discourse processing, negation detection is rarely used as an explicit step. An exception is Kim et al. (2006), who are concerned with discovering contrastive information about protein int"
J12-2001,N07-2036,0,0.0423535,"Missing"
J12-2001,S10-1008,1,0.792855,"ieve P?). The annotation guidelines to annotate the modalities are deﬁned in Baker et al. (2009). 3 Web site of the modality lexicon: http://www.umiacs.umd.edu/∼bonnie/ModalityLexicon.txt. Last accessed on 8 December 2011. 235 Computational Linguistics Volume 38, Number 2 The scope of negation has been annotated on a corpus of Conan Doyle stories (Morante, Schrauwen, and Daelemans 2011)4 (The Hound of the Baskervilles and The Adventure of Wisteria Lodge), which have also been annotated with coreference and semantic roles for the SemEval Task Linking Events and Their Participants in Discourse (Ruppenhofer et al. 2010). As for negation, the corpus is annotated with negation cues and their scope in a way similar to the BioScope corpus (Vincze et al. 2008) described subsequently, and in addition negated events are also marked, if they occur in factual statements. Blanco and Moldovan (2011) take a different approach by annotating the focus, “that part of the scope that is most prominently or explicitly negated,” in the 3,993 verbal negations signaled with MNEG in the PropBank corpus. According to the authors, the annotation of the focus allows the derivation of the implicit positive meaning of negated statemen"
J12-2001,W10-3113,0,0.0406808,"Missing"
J12-2001,W10-1103,0,0.0291232,"Missing"
J12-2001,N06-1005,0,0.0848299,"Missing"
J12-2001,W10-2102,0,0.0554897,"Missing"
J12-2001,P08-1033,0,0.187289,"on interact with mood and tense markers, and also with each other. Finally, discourse factors also add to the complexity of these phenomena. Incorporating information about modality and negation has been shown to be useful for a number of applications such as recognizing textual entailment (de Marneffe et al. 2006; Snow, Vanderwende, and Menezes 2006; Hickl and Bensley 2007), machine translation (Baker et al. 2010), trustworthiness detection (Su, Huang, and Chen 2010), classiﬁcation of citations (Di Marco, Kroon, and Mercer 2006), clinical and biomedical text processing (Friedman et al. 1994; Szarvas 2008), and identiﬁcation of text structure (Grabar and Hamon 2009). This overview is organized as follows: Sections 2 and 3 deﬁne modality and negation, respectively. Section 4 gives details of linguistic resources annotated with various aspects of negation and modality. We also discuss properties of the different annotation schemes that have been proposed. Having discussed the linguistic basis as well as the available resources, the remainder of the article then provides an overview of automated methods for dealing with modality and negation. Most of the work in this area has been carried out at t"
J12-2001,W10-3002,0,0.0240341,"words in Wikipedia: http://en.wikipedia.org/wiki/Weasel word. Last accessed on 8 December 2011. 10 Wikipedia instructions about weasel words are available at http://simple.wikipedia.org/wiki/ Wikipedia:Avoid weasel words. Last accessed on 8 December 2011. 240 Morante and Sporleder Modality and Negation top-ranked systems for Wikipedia data follow a bag-of-words approach. None of the top-ranked systems uses features derived from syntactic parsing. The best system for Wikipedia data (Georgescul 2010) implements an SVM and obtains an F1 score of 60.2, whereas the best system for biological data (Tang et al. 2010) incorporates conditional random ﬁelds (CRF) and obtains an F1 score of 86.4. As a follow-up of the CoNLL Shared Task, Velldal (2011) proposes to handle the hedge detection task as a simple disambiguation problem, restricted to the words that have previously been observed as hedge cues. This reduces the number of examples that need to be considered and the relevant feature space. Velldal develops a largemargin SVM classiﬁer based on simple sequence-oriented n-gram features collected for PoS-tags, lemmas, and surface forms. This system produces better results (86.64 F1 ) than the best system of"
J12-2001,W08-0606,0,0.781475,"Missing"
J12-2001,J94-2004,0,0.337443,"Missing"
J12-2001,J04-3002,0,0.124068,"Missing"
J12-2001,W10-3111,0,0.0846081,"ity is also not always entirely straightforward. For example, whereas negation can change the polarity of an expression from positive to negative (e.g., good vs. not good in Examples (32a) vs. (32b)) it can also shift negative polarity to neutral or even positive polarity (32c). (32) a. This is a good camera. b. This is not a good camera. c. This is by no means a bad camera. In this section, we discuss some approaches that make explicit use of negation in the context of sentiment analysis. For a recent general overview of work on sentiment analysis, we refer the reader to Pang and Lee (2008). Wiegand et al. (2010) present a survey of the role of negation in sentiment analysis. They indicate that it is necessary to perform ﬁne-grained linguistic analysis in order to extract features for machine learning or rule-based opinion analysis systems. The features allow the incorporation of information about linguistic phenomena such as negation (Wiegand et al. 2010, page 60). Early approaches made use of negation in a bag-of-words model by preﬁxing a word x with a negation marker if a negation word was detected immediately preceding x (Pang, Lee, and Vaithyanathan 2002). Thus x and NOT x were treated as two com"
J12-2001,H05-2018,0,0.0588463,"Missing"
J12-2001,H05-1044,0,0.0184724,"Missing"
J12-2001,J09-3003,0,0.0235573,"Missing"
J12-2001,D10-1070,0,0.0565376,"mpetitive performance. Øvrelid et al. report that the errors of their system are mostly of two classes: (i) failing to recognize phrase and clause boundaries, as in Example (28a), and (ii) not dealing successfully with 248 Morante and Sporleder Modality and Negation relatively superﬁcial properties of the text as in Example (28b). The scope boundaries produced by the system are marked with ‘’. (28) a. ... [the reverse complement mR of m will be considered to be ...]. b. This [might affect the results] if there is a systematic bias on the composition of a protein interaction set. Finally, Zhu et al. (2010) approach the scope learning problem via simpliﬁed shallow semantic parsing. The cue is regarded as the predicate and its scope is mapped into several constituents as the arguments of the cue. The system resolves the scope of negation and modality cues in the standard two phase approach. For cue identiﬁcation they apply an SVM that uses features from the surrounding words and from the structure of the syntax tree. The scope resolution task is different than in previous systems. The task is addressed in three consecutive phases: (1) argument pruning, consisting of collecting as argument candida"
J12-2001,baker-etal-2010-modality,0,\N,Missing
J12-2001,W06-2920,0,\N,Missing
J12-2001,W10-3004,0,\N,Missing
J12-2001,H05-2017,0,\N,Missing
J12-2001,W07-1428,0,\N,Missing
J12-2001,W10-3001,0,\N,Missing
J12-2001,W07-1401,0,\N,Missing
J12-2001,E99-1043,0,\N,Missing
J12-2001,W10-3110,0,\N,Missing
J12-2001,P11-1059,0,\N,Missing
J12-2001,P09-2044,0,\N,Missing
J12-2001,sauri-etal-2006-slinket,0,\N,Missing
J12-2001,W10-2900,0,\N,Missing
J12-2001,P10-2050,0,\N,Missing
J14-3007,W06-3814,0,0.0491604,"Missing"
J14-3007,S07-1002,0,0.0519592,"Missing"
J14-3007,H05-1004,0,0.136661,"Missing"
J14-3007,W09-2419,0,0.0864926,"H(c) and H(k). As a result, the V-measure will be positively biased, and this bias would be especially high for systems predicting a large number of clusters. This phenomenon has been previously noticed (Manandhar et al. 2010) but no satisfactory explanation has been given. The shortcomings of the ML estimator are especially easy to see on the example of a baseline system that assigns every instance in the testing set to an individual cluster. This baseline, when averaged over the 100 target words, outperforms all the participants’ systems of the SemEval-2010 task on the standard testing set (Manandhar and Klapaftis 2009). Though we cannot compute the true bias for any real system, the computation is trivial for this baseline. The true V-measure is equal to 0, 1 V-measure can be expressed via entropies in a number of different ways, although, for ML estimation they are all equivalent. For some more complex estimators, including some of the ones considered here, the resulting estimates will be somewhat different depending on the decomposition. We will focus on the symmetric form presented here. 673 Computational Linguistics Volume 40, Number 3 as the baseline can be regarded as a limiting case of a stochastic s"
J14-3007,S10-1011,0,0.389128,"4286 Trier, Germany. E-mail: sporledc@uni-trier.de. Submission received: 14 March 2013; revised version received: 13 September 2013; accepted for publication: 20 November 2013 doi:10.1162/COLI a 00196 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 proposed but information theoretic measures have been among the most successful and widely used techniques. One example is the normalized mutual information, also known as V-measure (Strehl and Gosh 2002; Rosenberg and Hirschberg 2007), which has, for example, been adopted in the SemEval 2010 WSI task (Manandhar et al. 2010). All information theoretic measures of cluster quality essentially rely on samplebased estimates of entropy. For instance, the mutual information I(c, k) between a gold standard class c and an output cluster k can be written H(c) + H(k) − H(k, c), where H(c) and H(k) are the marginal entropies of c and k, respectively, and H(k, c) is their joint entropy. The most standard estimator is the maximum-likelihood (ML) estimator, which substitutes the probability of each event (cluster, classes, or cluster-class pair occurrence) with its normalized empirical frequency. Entropy estimators, even thoug"
J14-3007,W04-2406,0,0.0405453,"ly to the setting of WSI evaluation where a ground-truth cluster sense number arguably does not exist and the standard evaluation scenarios use a small number of instances of each word to compute the score. We describe more accurate entropy estimators and analyze their performance both in simulations and on evaluation of WSI systems. 1. Introduction The task of word sense induction (WSI) has grown in popularity recently. WSI has the advantage of not assuming a predefined inventory of senses. Rather, senses are induced in an unsupervised fashion on the basis of corpus evidence (Schutze ¨ 1998; Purandare and Pedersen 2004). WSI systems can therefore better adapt to different target domains that may require sense inventories of different granularities. However, the fact that WSI systems do not rely on fixed inventories also makes it notoriously difficult to evaluate and compare their performance. WSI evaluation is a type of cluster evaluation problem. Although cluster evaluation has received much attention (see, e.g., Dom 2001; Strehl and Gosh 2002; Meila 2007), it is still not a solved problem. Finding a good way to score partially incorrect clusters is particularly difficult. Several solutions have been ∗ Micr"
J14-3007,D07-1043,0,0.375199,"tion. E-mail: titov@uva.nl. † Computational Linguistics and Digital Humanities, Trier University, 54286 Trier, Germany. E-mail: sporledc@uni-trier.de. Submission received: 14 March 2013; revised version received: 13 September 2013; accepted for publication: 20 November 2013 doi:10.1162/COLI a 00196 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 proposed but information theoretic measures have been among the most successful and widely used techniques. One example is the normalized mutual information, also known as V-measure (Strehl and Gosh 2002; Rosenberg and Hirschberg 2007), which has, for example, been adopted in the SemEval 2010 WSI task (Manandhar et al. 2010). All information theoretic measures of cluster quality essentially rely on samplebased estimates of entropy. For instance, the mutual information I(c, k) between a gold standard class c and an output cluster k can be written H(c) + H(k) − H(k, c), where H(c) and H(k) are the marginal entropies of c and k, respectively, and H(k, c) is their joint entropy. The most standard estimator is the maximum-likelihood (ML) estimator, which substitutes the probability of each event (cluster, classes, or cluster-cla"
J14-3007,J98-1004,0,0.734873,"Missing"
N10-1039,E06-1042,0,0.269448,"rties which differentiate them from other expressions, e.g., they often exhibit a degree of syntactic and lexical fixedness. These properties can be used to identify potential idioms, for instance, by employing measures of association strength between the elements of an expression (Lin, 1999). Type-based approaches are unsuitable for expressions which can be used both figuratively and literally. These have to be disambiguated in context. Token-based classification aims to do this. A number of token-based approaches have been proposed: supervised (Katz and Giesbrecht, 2006), weakly supervised (Birke and Sarkar, 2006), and unsupervised (Fazly et al., 2009; Sporleder and Li, 2009). Finally, token-based detection can be viewed as a two stage task which is the combination of type-based extraction and token-based classification. There has been relatively little work on this so far. One exception are Fazly et al. (2009) who detect idiom types by using statistical methods that model the general idiomaticity of an expression and then combine this with a simple second-stage process that detects whether the target expression is used figuratively in a given context, based on whether the expression occurs in canonica"
N10-1039,J09-1005,0,0.078587,"xpressions, e.g., they often exhibit a degree of syntactic and lexical fixedness. These properties can be used to identify potential idioms, for instance, by employing measures of association strength between the elements of an expression (Lin, 1999). Type-based approaches are unsuitable for expressions which can be used both figuratively and literally. These have to be disambiguated in context. Token-based classification aims to do this. A number of token-based approaches have been proposed: supervised (Katz and Giesbrecht, 2006), weakly supervised (Birke and Sarkar, 2006), and unsupervised (Fazly et al., 2009; Sporleder and Li, 2009). Finally, token-based detection can be viewed as a two stage task which is the combination of type-based extraction and token-based classification. There has been relatively little work on this so far. One exception are Fazly et al. (2009) who detect idiom types by using statistical methods that model the general idiomaticity of an expression and then combine this with a simple second-stage process that detects whether the target expression is used figuratively in a given context, based on whether the expression occurs in canonical form or not. However, modeling token"
N10-1039,W06-1203,0,0.0450148,"ns exploit the fact that idioms have many properties which differentiate them from other expressions, e.g., they often exhibit a degree of syntactic and lexical fixedness. These properties can be used to identify potential idioms, for instance, by employing measures of association strength between the elements of an expression (Lin, 1999). Type-based approaches are unsuitable for expressions which can be used both figuratively and literally. These have to be disambiguated in context. Token-based classification aims to do this. A number of token-based approaches have been proposed: supervised (Katz and Giesbrecht, 2006), weakly supervised (Birke and Sarkar, 2006), and unsupervised (Fazly et al., 2009; Sporleder and Li, 2009). Finally, token-based detection can be viewed as a two stage task which is the combination of type-based extraction and token-based classification. There has been relatively little work on this so far. One exception are Fazly et al. (2009) who detect idiom types by using statistical methods that model the general idiomaticity of an expression and then combine this with a simple second-stage process that detects whether the target expression is used figuratively in a given context, based"
N10-1039,D09-1033,1,0.778633,"t weak cohesive ties with the context even if though they are used literally. Using a named-entity tagger before applying the GMM might solve the problem. bite one’s tongue 4.3 Model GMM+f GMM+f+s C n l n l Pre. 42.22 92.71 41.38 92.54 Rec. 73.08 77.39 54.55 87.94 F-S. 53.52 84.36 47.06 90.18 Acc. 76.60 83.44 Table 3: Results on the V+NP data set, Gaussian component parameters estimated by annotated data Finally, Table 4 shows the result when using different idioms to generate the nonliteral Gaussian. The literal Gaussian can be generated from the automatically obtained nonliteral examples by Li and Sporleder (2009). We found the estimation of the GMM is not sensitive to idioms; our model is robust and can use any existing idiom data to discover new figurative expressions. Furthermore, Table 4 also shows that the GMM does not need a large amount of annotated data for parameter estimation. A few hundred instances are sufficient. 5 Conclusion We described a GMM based approach for detecting figurative expressions. This method not only works 300 (166) break the ice (541) C n l n l Pre. 40.79 94.10 39.05 88.36 Rec. 79.49 73.91 52.56 81.45 F-S. 53.91 82.79 44.81 84.77 Acc. 74.94 76.12 Table 4: Results on the V"
N10-1039,P99-1041,0,0.0104071,"s: type-based extraction (detect idioms on the type level), token-based classification (given a potentially idiomatic phrase in context, decide whether it is used idiomatically), token-based detection (detect figurative expressions in running text). Type-based extractions exploit the fact that idioms have many properties which differentiate them from other expressions, e.g., they often exhibit a degree of syntactic and lexical fixedness. These properties can be used to identify potential idioms, for instance, by employing measures of association strength between the elements of an expression (Lin, 1999). Type-based approaches are unsuitable for expressions which can be used both figuratively and literally. These have to be disambiguated in context. Token-based classification aims to do this. A number of token-based approaches have been proposed: supervised (Katz and Giesbrecht, 2006), weakly supervised (Birke and Sarkar, 2006), and unsupervised (Fazly et al., 2009; Sporleder and Li, 2009). Finally, token-based detection can be viewed as a two stage task which is the combination of type-based extraction and token-based classification. There has been relatively little work on this so far. One"
N10-1039,E09-1086,1,0.863037,"ey often exhibit a degree of syntactic and lexical fixedness. These properties can be used to identify potential idioms, for instance, by employing measures of association strength between the elements of an expression (Lin, 1999). Type-based approaches are unsuitable for expressions which can be used both figuratively and literally. These have to be disambiguated in context. Token-based classification aims to do this. A number of token-based approaches have been proposed: supervised (Katz and Giesbrecht, 2006), weakly supervised (Birke and Sarkar, 2006), and unsupervised (Fazly et al., 2009; Sporleder and Li, 2009). Finally, token-based detection can be viewed as a two stage task which is the combination of type-based extraction and token-based classification. There has been relatively little work on this so far. One exception are Fazly et al. (2009) who detect idiom types by using statistical methods that model the general idiomaticity of an expression and then combine this with a simple second-stage process that detects whether the target expression is used figuratively in a given context, based on whether the expression occurs in canonical form or not. However, modeling token-based detection as a 297"
P10-1116,S07-1070,0,0.204781,"Missing"
P10-1116,W09-2002,0,0.0298023,"which use hierarchical lexicons to model semantic distance (Budanitsky and Hirst, 2006). In our approach, we circumvent this problem by exploiting paraphrase information for the target senses rather than relying on the structure of WordNet as a whole. Topic models have also been applied to the related task of word sense induction. Brody and Lapata (2009) propose a method that integrates a number of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to distinguish literal and nonliteral usages of potential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that"
P10-1116,E06-1042,0,0.046316,"ber of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to distinguish literal and nonliteral usages of potential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that computes cohesive links between the component words of the target expression and its context have been proposed (Sporleder and Li, 2009; Li and Sporleder, 2009). Their system predicts literal usages when strong links can be found. 3 3.1 The Sense Disambiguation Model Topic Model As pointed out by Hofmann (1999), the starting point of topic models is to decompose the conditional word-document pr"
P10-1116,S07-1060,0,0.0953556,"ed systems automatically group word tokens into similar groups using clustering algorithms, and then assign labels to each sense cluster. Knowledge-based approaches exploit information contained in existing resources. They can be combined with supervised machinelearning models to assemble semi-supervised approaches. Recently, a number of systems have been proposed that make use of topic models for sense disambiguation. Cai et al. (2007), for example, use LDA to capture global context. They compute topic models from a large unlabelled corpus and include them as features in a supervised system. Boyd-Graber and Blei (2007) propose an unsupervised approach that integrates McCarthy et al.’s (2004) method for finding predominant word senses into a topic modelling framework. In addition to generating a topic from the document’s topic distribution and sampling a word from that topic, the enhanced model also generates a distributional neighbour for the chosen word and then assigns a sense based on the word, its neighbour and the topic. Boyd-Graber and Blei (2007) test their method on WSD and information retrieval tasks and find that it can lead to modest improvements over state-of-the-art results. In another unsuperv"
P10-1116,D07-1109,0,0.629267,"task, and despite the fact that it has been the focus of much research over the years, stateof-the-art systems are still often not good enough for real-world applications. One major factor that makes WSD difficult is a relative lack of manually annotated corpora, which hampers the performance of supervised systems. To address this problem, there has been a significant amount of work on unsupervised WSD that does not require manually sensedisambiguated training data (see McCarthy (2009) for an overview). Recently, several researchers have experimented with topic models (Brody and Lapata, 2009; Boyd-Graber et al., 2007; BoydGraber and Blei, 2007; Cai et al., 2007) for sense disambiguation and induction. Topic models are generative probabilistic models of text corpora in which each document is modelled as a mixture over (latent) topics, which are in turn represented by a distribution over words. Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al., 2007) or rely heavily on the structure of hierarchical lexicons such as WordNet (Boyd-Graber et al., 2007). In this paper, we propose a novel framework which is fairly resource-poor in that i"
P10-1116,P06-2006,0,0.0164129,"Missing"
P10-1116,E09-1013,0,0.188728,"ver, WSD is a difficult task, and despite the fact that it has been the focus of much research over the years, stateof-the-art systems are still often not good enough for real-world applications. One major factor that makes WSD difficult is a relative lack of manually annotated corpora, which hampers the performance of supervised systems. To address this problem, there has been a significant amount of work on unsupervised WSD that does not require manually sensedisambiguated training data (see McCarthy (2009) for an overview). Recently, several researchers have experimented with topic models (Brody and Lapata, 2009; Boyd-Graber et al., 2007; BoydGraber and Blei, 2007; Cai et al., 2007) for sense disambiguation and induction. Topic models are generative probabilistic models of text corpora in which each document is modelled as a mixture over (latent) topics, which are in turn represented by a distribution over words. Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al., 2007) or rely heavily on the structure of hierarchical lexicons such as WordNet (Boyd-Graber et al., 2007). In this paper, we propose a novel framework which is fair"
P10-1116,J06-1003,0,0.00649911,"(2007) enhance the basic LDA algorithm by incorporating WordNet senses as an additional latent variable. Instead of generating words directly from a topic, each topic is associated with a random walk through the WordNet hierarchy which generates the observed word. Topics and synsets are then inferred together. While Boyd-Graber et al. (2007) show that this method can lead to improvements in accuracy, they also find that idiosyncracies in the hierarchical structure of WordNet can harm performance. This is a general problem for methods which use hierarchical lexicons to model semantic distance (Budanitsky and Hirst, 2006). In our approach, we circumvent this problem by exploiting paraphrase information for the target senses rather than relying on the structure of WordNet as a whole. Topic models have also been applied to the related task of word sense induction. Brody and Lapata (2009) propose a method that integrates a number of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to d"
P10-1116,S07-1097,0,0.0195966,"ccurrences between different word senses, which was obtained from a number of resources (SSI+LKB) including: (i) SemCor (manually annotated); (ii) LDCDSO (partly manually annotated); (iii) collocation dictionaries which are then disambiguated semiautomatically. Even though the system is not “trained”, it needs a lot of information which is largely dependent on manually annotated data, so it does not fit neatly into the categories Type II or Type III either. Table 2 lists the best participating systems of each type in the SemEval-2007 task (Type I: NUS-PT (Chan et al., 2007); Type II: UPV-WSD (Buscaldi and Rosso, 2007); Type III: TKB-UO (Anaya-S´anchez et al., 2007)). Our Model I belongs to Type II, and our Model II belongs to Type III. Table 2 compares the performance of our models with the Semeval-2007 participating systems. We only compare the F-score, since all the compared systems have an attempted rate7 of 1.0, 7 Attempted rate is defined as the total number of disambiguated output instances divided by the total number of input 1143 which makes both the precision and recall rates the same as the F-score. We focus on comparisons between our models and the best SemEval-2007 participating systems within"
P10-1116,D07-1108,0,0.293271,"of much research over the years, stateof-the-art systems are still often not good enough for real-world applications. One major factor that makes WSD difficult is a relative lack of manually annotated corpora, which hampers the performance of supervised systems. To address this problem, there has been a significant amount of work on unsupervised WSD that does not require manually sensedisambiguated training data (see McCarthy (2009) for an overview). Recently, several researchers have experimented with topic models (Brody and Lapata, 2009; Boyd-Graber et al., 2007; BoydGraber and Blei, 2007; Cai et al., 2007) for sense disambiguation and induction. Topic models are generative probabilistic models of text corpora in which each document is modelled as a mixture over (latent) topics, which are in turn represented by a distribution over words. Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al., 2007) or rely heavily on the structure of hierarchical lexicons such as WordNet (Boyd-Graber et al., 2007). In this paper, we propose a novel framework which is fairly resource-poor in that it requires only 1) a large unlabelled corpus f"
P10-1116,S07-1054,0,0.0172795,"e, specifically information about co-occurrences between different word senses, which was obtained from a number of resources (SSI+LKB) including: (i) SemCor (manually annotated); (ii) LDCDSO (partly manually annotated); (iii) collocation dictionaries which are then disambiguated semiautomatically. Even though the system is not “trained”, it needs a lot of information which is largely dependent on manually annotated data, so it does not fit neatly into the categories Type II or Type III either. Table 2 lists the best participating systems of each type in the SemEval-2007 task (Type I: NUS-PT (Chan et al., 2007); Type II: UPV-WSD (Buscaldi and Rosso, 2007); Type III: TKB-UO (Anaya-S´anchez et al., 2007)). Our Model I belongs to Type II, and our Model II belongs to Type III. Table 2 compares the performance of our models with the Semeval-2007 participating systems. We only compare the F-score, since all the compared systems have an attempted rate7 of 1.0, 7 Attempted rate is defined as the total number of disambiguated output instances divided by the total number of input 1143 which makes both the precision and recall rates the same as the F-score. We focus on comparisons between our models and the be"
P10-1116,S07-1006,0,0.0367902,"Missing"
P10-1116,P06-1014,0,0.046176,"Missing"
P10-1116,S07-1061,0,0.0609561,"Missing"
P10-1116,W06-1203,0,0.0495933,"he structure of WordNet as a whole. Topic models have also been applied to the related task of word sense induction. Brody and Lapata (2009) propose a method that integrates a number of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to distinguish literal and nonliteral usages of potential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that computes cohesive links between the component words of the target expression and its context have been proposed (Sporleder and Li, 2009; Li and Sporleder, 2009). Their system predicts literal usages when strong links can"
P10-1116,E09-1086,1,0.920859,"ential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that computes cohesive links between the component words of the target expression and its context have been proposed (Sporleder and Li, 2009; Li and Sporleder, 2009). Their system predicts literal usages when strong links can be found. 3 3.1 The Sense Disambiguation Model Topic Model As pointed out by Hofmann (1999), the starting point of topic models is to decompose the conditional word-document probability distribution p(w|d) into two different distributions: the wordtopic distribution p(w|z), and the topic-document distribution p(z|d) (see Equation 1). This allows each semantic topic z to be represented as a multinominal distribution of words p(w|z), and each document d to be represented as a multinominal distribution of semant"
P10-1116,W09-2001,0,0.0191887,"ral problem for methods which use hierarchical lexicons to model semantic distance (Budanitsky and Hirst, 2006). In our approach, we circumvent this problem by exploiting paraphrase information for the target senses rather than relying on the structure of WordNet as a whole. Topic models have also been applied to the related task of word sense induction. Brody and Lapata (2009) propose a method that integrates a number of different linguistic features into a single generative model. Topic models have been previously considered for metaphor extraction and estimating the frequency of metaphors (Klebanov et al., 2009; Bethard et al., 2009). However, we have a different focus in this paper, which aims to distinguish literal and nonliteral usages of potential idiomatic expressions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An u"
P10-1116,D09-1033,1,0.937647,"ions. A number of methods have been applied to this task. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and nonliteral usages of a given expression in the trainning data. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one nonliteral), assigning the label of the closest set. An unsupervised method that computes cohesive links between the component words of the target expression and its context have been proposed (Sporleder and Li, 2009; Li and Sporleder, 2009). Their system predicts literal usages when strong links can be found. 3 3.1 The Sense Disambiguation Model Topic Model As pointed out by Hofmann (1999), the starting point of topic models is to decompose the conditional word-document probability distribution p(w|d) into two different distributions: the wordtopic distribution p(w|z), and the topic-document distribution p(z|d) (see Equation 1). This allows each semantic topic z to be represented as a multinominal distribution of words p(w|z), and each document d to be represented as a multinominal distribution of semantic topics p(z|d). The mod"
P10-1116,P04-1036,0,0.261853,"Missing"
P13-1160,S07-1002,0,0.0124771,"ed in our annotation scheme (see Table 2). The hyperpriors were chosen in a qualitative experiment over a subset of our dataset by manually inspecting the produced languages models. The resulting values are: α = 10−3 , β = 5 ∗ 10−4 , τ = 5 ∗ 10−4 , η = 10−3 , ν4 = 103 , ν¯4 = 10−4 , ωθ = 85 and ωθ0 = ωψ = ωψ0 = 5. 5.1 Direct clustering evaluation Our labels encoding aspect and sentiment level can be regarded as clusters. Consequently we can apply techniques developed in the context of clustering evaluation. We use a version of the standard metrics considered for the word sense induction task (Agirre and Soroa, 2007) where a clustering is converted to a classification problem. This is achieved by splitting the gold standard into two subsets; the training portion is used to choose oneto-one correspondence from the gold classes to the induced clusters and then the chosen mapping is applied to the testing portion. We perform 10-fold cross validation and report precision, recall and F1 score. Our dataset is very skewed and the majority class (rest) is arguably the least important, so we use macro-averaging over labels and then average those across folds to arrive to the reported numbers. We compare the discou"
P13-1160,C08-2002,0,0.0857708,"earns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on senti"
P13-1160,D08-1035,0,0.0100928,"ic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encoded in this way provides a useful signal for prediction of sentiment, but they leave automatic discourse relation predict"
P13-1160,E12-1021,0,0.148328,"Missing"
P13-1160,N10-1120,0,0.453483,"on for Computational Linguistics, pages 1630–1639, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics count for these discourse phenomena and cannot rely solely on local lexical information. These issues have not gone unnoticed to the research community. Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Nakagawa et al., 2010) that operate on the lexical level or by using discourse relations (Taboada et al., 2008; Zhou et al., 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to e"
P13-1160,prasad-etal-2008-penn,0,0.0143278,"(or negative) is also vaguely defined. To gain insight into our model, we conducted an experiment similar to the one presented in Somasundaran et al. (2009). We divide the dataset in two subsets; one containing all EDUs starting with a discourse cue (“marked”) and one containing the remaining EDUs (“unmarked”). We hypothesize that the effect of the discourse-aware model should be stronger on the first subset, since the presence of the connective indicates the possibility of a discourse relation with the previous EDU. The set of discourse connectives is taken from the Penn Discourse Treebank (Prasad et al., 2008), thus creating a list of 240 potential connectives. Table 5 presents a subset of “marked” EDUs for which trying to assign the sentiment and aspect out of context (i.e. without the previous EDU) is a difficult task. In examples 1-3 there is no explicit mention of the aspect. However, there is an anaphoric expression (marked in bold) which 1635 refers to a mention of the aspect in some previous EDU. On the other hand, in examples 4 and 5 there is an ambiguity in the choice of aspect; in example 5, tea making facilities can refer to a breakfast at the hotel (label food) or to facilities in the r"
P13-1160,sadamitsu-etal-2008-sentiment,0,0.0271863,"e beneficial. 6 Related Work Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either usi"
P13-1160,N07-1038,0,0.110215,"shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated"
P13-1160,D11-1014,0,0.0612942,"ork Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relatio"
P13-1160,C08-1101,0,0.0120621,"ven better) or both (the room was nice but the breakfast was awful). Therefore, we do not explicitly model 3 The ‘explanation’ relation, for example, can occur with a polarity change (We were upgraded to a really nice room because the hotel made a terrible blunder with our booking) but does not have to (The room was really nice because the hotel was newly renovated). 1631 Name AltSame SameAlt AltAlt Description different polarity, same aspect same polarity, different aspect different polarity and aspect Table 1: Discourse relations generic discourse relations; instead, inspired by the work of Somasundaran et al. (2008), we define three very general relations which encode how polarity and aspect change (Table 1). Note that we do not have a discourse relation SameSame since we do not expect to have strong linguistic evidence which states that an EDU contains the same sentiment information as the previous one.4 However, we assume that the sentiment and topic flow is fairly smooth in general. In other words, for two adjacent EDUs not connected by any of the above three relations, the prior probability of staying at the same topic and sentiment level is higher than picking a new topic and sentiment level (i.e. w"
P13-1160,D09-1018,0,0.725925,"st, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. Furthermore, these techniques will not necessarily be able to induce discourse relations informative for the sentiment analysis domain (Voll and Taboada, 2007). An alternative approach is to define a taskspecific scheme of discourse relations (Somasundaran et al., 2009). This previous work showed that task-specific discourse relations are helpful in predicting sentiment, however, in doing so they relied on gold-standard discourse annotation at test time rather than predicting it automatically or inducing it jointly with sentiment polarity. We take a different approach and induce discourse and sentiment information jointly in an unsupervised (or weakly supervised) manner. This has the advantage of not having to pre-specify a mapping from discourse cues to discourse relations; our model induces this automatically, which makes it portable to new domains and lan"
P13-1160,N03-1030,0,0.0509325,"re intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encode"
P13-1160,J11-2001,0,0.0477964,"EDUs are higher than the ones for the “marked” dataset clearly suggests that the latter constitute a hard case for sentiment analysis, in which exploiting discourse signal proves to be beneficial. 6 Related Work Recently, there has been significant interest in leveraging content structure for a number of NLP tasks (Webber et al., 2011). Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al., 2011). One way to deal with this problem is to model the interactions by using a precompiled set of polarity shifters (Nakagawa et al., 2010; Polanyi and Zaenen, 2006; Sadamitsu et al., 2008). Socher et al. (2011) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. This may be problematic for the noisy sentiment-analysis domain"
P13-1160,P11-2100,0,0.140484,"Missing"
P13-1160,P08-1036,1,0.665334,"the wheat from the chaff, it is necessary to structure the available information. In the review domain, this is done in aspectbased sentiment analysis which aims at identifying text fragments in which opinions are expressed about ratable aspects of products, such as ‘room quality’ or ‘service quality’. Such fine-grained analysis can serve as the first step in aspect-based sentiment summarization (Hu and Liu, 2004), a task with many practical applications. Aspect-based summarization is an active research area for which various techniques have been developed, both statistical (Mei et al., 2007; Titov and McDonald, 2008b) and not (Hu and Liu, 2004), and relying on different types of supervision sources, such as sentiment-annotated texts or polarity lexica (Turney and Littman, 2002). Most methods rely on local information (bag-of-words, short ngrams or elementary syntactic fragments) and do not attempt to account for more complex interactions. However, these local lexical representations by themselves are often not sufficient to infer a sentiment or aspect for a fragment of text. For instance, in the following example taken from a TripAdvisor1 review: Example 1. The room was nice but let’s not talk about the"
P13-1160,N13-1100,0,0.137549,"e noisy sentiment-analysis domain and especially for poor-resource languages. Similar to our work, others have focused on modeling interactions between phrases and sentences. However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al., 2011; Asher et al., 2008; Snyder and Barzilay, 2007) or by using directly (Taboada et al., 2008) the output of discourse parsers (Soricut and Marcu, 2003). Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. Another related line of research was presented in Somasundaran et al. (2009) where a domainspecific discourse scheme is considered. Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. Somasundaran et al. (2009) show that gold-standard discourse information encoded in this way provides a useful signal for prediction of sentiment, but they leave automatic discourse relation prediction for future work. They use an integer linear programmi"
P13-1160,D11-1015,0,0.432291,"utational Linguistics count for these discourse phenomena and cannot rely solely on local lexical information. These issues have not gone unnoticed to the research community. Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Nakagawa et al., 2010) that operate on the lexical level or by using discourse relations (Taboada et al., 2008; Zhou et al., 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Such approaches have a number of disadvantages. First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. These resources are available only for a handful of languages. Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. Furthermore, these techniques will not necessarily be able to induce discourse relations i"
P13-1160,H05-2017,0,\N,Missing
P13-1160,H05-1043,0,\N,Missing
P13-1160,N10-1122,0,\N,Missing
P13-1160,D10-1006,0,\N,Missing
P13-1160,P09-2020,0,\N,Missing
penas-etal-2012-evaluating,P11-1142,1,\N,Missing
R11-1046,S10-1059,0,0.42829,"Missing"
R11-1046,N10-1138,0,0.0632222,"Missing"
R11-1046,P10-1160,0,0.157854,"rpus provides annotations for running texts not for individual occurrences of selected target predicates. It thus treats many different generallanguage predicates of all parts of speech. While the overall size of the corpus in terms of sentences is comparable to Gerber and Chai’s corpus, the SemEval corpus contains many more target predicates and fewer instances for each.3 These properties make it much harder to obtain good results on the SemEval corpus, which is supported by the fact that the NI resolution results obtained by the Task-10 participants are significantly below those reported by Gerber and Chai (2010). While the SemEval-10 Task-10 is harder than the problem tackled by Gerber and Chai (2010), we also believe it is more realistic. Given the complexity of annotating semantic argument structures in general and null instantiations in particular, it seems infeasible to annotate large amounts of text with the required information. Hence, automated systems will always have to make do with scarce resources. We investigate different strategies of incorporating linguistic background knowledge to overcome this data sparseness problem, e.g., by explicitly modeling the DNI v. INI distinction, which is i"
R11-1046,J01-4005,0,0.065965,"Missing"
R11-1046,P10-1005,0,0.0832063,"Missing"
R11-1046,S10-1008,1,0.600884,"ate. The other arguments are so-called null instantiations (NIs). Even core arguments of a predicate, i.e., those that express participants which are necessarily present in the situation which the predicate evokes (see Section 2 for a more detailed explanation of core vs. peripheral arguments), are frequently not instantiated in the local context. While null instantiated arguments are not locally realized, they can often be inferred from the context. Consider examples (1) and (2) below (taken from Arthur Conan Doyle’s “The Adventure of Wisteria Lodge” and part of the SemEval-10 Task10 corpus (Ruppenhofer et al., 2010)). We use A and B in the examples to indicate speakers.1 In a frame-semantic analysis of (1) interesting evokes the Mental stimulus stimulus focus (1) A. [“A white cock,”]Stim said [he]Exp . “[Most]Deg interestingMssf !” (2) A. [“Your powers seem superior to your opportunities.”]Inf “[You]Src ’re rightCorr , Mr. Holmes.” B. While humans have no problem inferring uninstantiated roles that can be filled from the linguistic context, this is beyond the capacity of state-ofthe-art semantic role labeling systems, which tacitly ignore all roles that are not instantiated locally. SRL systems thus disr"
R11-1046,S10-1065,0,0.258966,"Missing"
R11-1090,W04-0817,0,0.0614556,"Missing"
R11-1090,W08-2208,0,0.0207302,"the arguments. ies have addressed the coverage issue. For example, Das et al. (2010) introduce a latent variable ranging over seen targets, allowing them to infer likely frames for unseen words, and the SRL system of Johansson and Nugues (2007) uses WordNet to generalise to unseen lemmas. In a similar vein, Burchardt et al. (2005) propose a system that generalizes over WordNet synsets to guess frames for unknown words. Pennacchiotti et al. (2008) compare WordNet-based and distributional approaches to inferring frames and conclude that a combination of the two leads to the best results, while (Cao et al., 2008) discuss how different distributional models can be utilised. Several approaches have also addressed other coverage problems, e.g., how to automatically expand the number of example sentences for a given lexical unit (Pad´o et al., 2008; F¨urstenau and Lapata, 2009). Another related approach is that of generalizing over semantic roles. Baldewein et al. (2004) use the FrameNet hierarchy to model the similarity of roles, boosting seldom-seen instances by reusing training data for similar roles, though without significant gains in performance. The most extensive study on role generalization to da"
R11-1090,N10-1138,0,0.0122428,"t-in difference between predicting a frame label or semantic role for seen versus unseen instances. Naturally, the outcome of prediction will be more accurate if the model has seen several instances similar to a test instance (i.e., from the same lexical unit or lemma). But even for unseen instances, the model is still capable of generalizing the properties of the training instances given that there are similarities between their available features, such as the syntactic pattern and the semantic properties of the predicate and the arguments. ies have addressed the coverage issue. For example, Das et al. (2010) introduce a latent variable ranging over seen targets, allowing them to infer likely frames for unseen words, and the SRL system of Johansson and Nugues (2007) uses WordNet to generalise to unseen lemmas. In a similar vein, Burchardt et al. (2005) propose a system that generalizes over WordNet synsets to guess frames for unknown words. Pennacchiotti et al. (2008) compare WordNet-based and distributional approaches to inferring frames and conclude that a combination of the two leads to the best results, while (Cao et al., 2008) discuss how different distributional models can be utilised. Sever"
R11-1090,erk-pado-2006-shalmaneser,0,0.0488517,"Missing"
R11-1090,E09-1026,0,0.0377882,"Missing"
R11-1090,J02-3001,0,0.0666911,"se (frame) to a noun and a verb as in (1), where both competition and play are assigned the C OMPETITION frame. Also, FrameNet assigns semantic roles not only to syntactic arguments of the target but also to constituents which are not directly syntactically dependent on the target but can be semantically understood as filling a role, e.g., Wivenhoe Town in (1a). (1) a. b. 2 [Wivenhoe Town]Participant1 have never won the competitionCompetition . [Olympiakos]Participant1 playsCompetition [against Aris Salonica]Participant1 [in Piraeus]Place . Related Work While early FrameNet-style SRL systems (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006, among others) are unable to make predictions for LUs not seen in the training data, several more recent studA major challenge for FrameNet-style SRL is posed by the limited coverage of available annotated data. The FrameNet lexicographic corpus 1 Under the SemEval-07 partial matching scheme, a majority of the other frame predictions receive partial credit. 628 Proceedings of Recent Advances in Natural Language Processing, pages 628–633, Hissar, Bulgaria, 12-14 September 2011. are predicted based on the acquired constructions (or clusters), and the extracted features from"
R11-1090,P09-1003,0,0.0587843,"cuss how different distributional models can be utilised. Several approaches have also addressed other coverage problems, e.g., how to automatically expand the number of example sentences for a given lexical unit (Pad´o et al., 2008; F¨urstenau and Lapata, 2009). Another related approach is that of generalizing over semantic roles. Baldewein et al. (2004) use the FrameNet hierarchy to model the similarity of roles, boosting seldom-seen instances by reusing training data for similar roles, though without significant gains in performance. The most extensive study on role generalization to date (Matsubayashi et al., 2009) compares different ways of grouping roles—exploiting hierarchical relations in FrameNet, generalizing via role names, utilising role types, and using thematic roles from VerbNet—with the best results from using all groups together. 3 Model 3.1 We formalize frame and role assignment using an extended version of the construction learning model of Alishahi and Stevenson (2010). The model uses Bayesian clustering for learning argument structure constructions: each construction is a grouping of individual predicate usages which probabilistically share form-meaning associations. These groupings typ"
R11-1090,C08-1084,1,0.908726,"Missing"
R11-1090,C10-2107,1,0.810969,"Missing"
R11-1090,J05-1004,0,0.0515042,"ed coverage of available annotated data. Our SRL model is based on Bayesian clustering and has the advantage of being very robust in the face of unseen and incomplete data. Frame labeling and role labeling are modeled in like fashions, allowing cascading classification scenarios. The model is shown to perform especially well on unseen data. In addition, we show that for seen data, predicting semantic types for roles improves role labeling performance. 1 Introduction The majority of recent work in semantic role labeling (SRL) has been carried out on PropBankstyle semantic argument annotations (Palmer et al., 2005), rather than on FrameNet-style annotations (Ruppenhofer et al., 2006). FrameNet differs from PropBank in that FrameNet annotations are more strongly semantically driven. FrameNet generalizes over different parts of speech and can assign the same sense (frame) to a noun and a verb as in (1), where both competition and play are assigned the C OMPETITION frame. Also, FrameNet assigns semantic roles not only to syntactic arguments of the target but also to constituents which are not directly syntactically dependent on the target but can be semantically understood as filling a role, e.g., Wivenhoe"
R11-1090,D08-1048,0,0.0169724,"ies of the training instances given that there are similarities between their available features, such as the syntactic pattern and the semantic properties of the predicate and the arguments. ies have addressed the coverage issue. For example, Das et al. (2010) introduce a latent variable ranging over seen targets, allowing them to infer likely frames for unseen words, and the SRL system of Johansson and Nugues (2007) uses WordNet to generalise to unseen lemmas. In a similar vein, Burchardt et al. (2005) propose a system that generalizes over WordNet synsets to guess frames for unknown words. Pennacchiotti et al. (2008) compare WordNet-based and distributional approaches to inferring frames and conclude that a combination of the two leads to the best results, while (Cao et al., 2008) discuss how different distributional models can be utilised. Several approaches have also addressed other coverage problems, e.g., how to automatically expand the number of example sentences for a given lexical unit (Pad´o et al., 2008; F¨urstenau and Lapata, 2009). Another related approach is that of generalizing over semantic roles. Baldewein et al. (2004) use the FrameNet hierarchy to model the similarity of roles, boosting s"
ruppenhofer-etal-2010-speaker,krestel-etal-2008-minding,0,\N,Missing
ruppenhofer-etal-2010-speaker,W06-0301,0,\N,Missing
ruppenhofer-etal-2010-speaker,H05-1045,0,\N,Missing
S07-1039,W96-0102,0,0.0707357,"ous way to model their lexical semantics was by utilizing WordNet3.0 (Fellbaum, 1998) (WN). One of the systems followed this route. We also entered a second system, which did not rely on WN but instead made use of automatically 2 System Description The development of the system consists of a preprocessing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, c Prague, June 2007. 2007 Association for Computatio"
S07-1039,J93-2004,0,0.0321182,"ing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, c Prague, June 2007. 2007 Association for Computational Linguistics The features extracted are of three types: semantic, lexical, and morpho-syntactic. The features that apply to the entities in a relation (e1,e2) are extracted for term 1 (t1) and term 2 (t2) of the relation, where t1 is the first term in the relation name, and t2 is the second term."
S10-1008,S07-1018,1,0.754355,"rd word senses (i.e., frames) for the target words and the participants had to perform role recognition/labeling and null instantiation linking, and a NI only task, in which the test set was already annotated with gold standard semantic argument structures and the participants only had to recognize definite null instantiations and find links to antecedents in the wider context (NI linking). However, it turned out that the basic semantic role labeling task was already quite challenging for our data set. Previous shared tasks have shown that frame-semantic SRL of running text is a hard problem (Baker et al., 2007), partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available. In our case the difficulty was increased because our data came from a new genre and domain (i.e., crime fiction, see Section 3.2). Hence, we decided to add standard SRL, i.e., role recognition and labeling, as a third task (SRL only). This task did not involve NI linking. The theory of null complementation used here is the one adopted by FrameNet, which derives from the work of Fillmore (1986).3 Briefly, omissions of core arguments of predicates are categoriz"
S10-1008,erk-pado-2004-powerful,0,0.102782,"Missing"
S10-1008,J02-3001,0,0.0764846,"scourse entities or events. In the shared task we looked at one particular aspect of cross-sentence links between argument structures, namely linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications, such as information extraction, question answering or text summarization. 1 Introduction Semantic role labeling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. However, semantic role labeling as it is currently defined misses a lot of information due to the fact that it is viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded. This view of SRL as a sentence-internal task is partly due to the fact that large-scale manual annotation 1 http://framenet.icsi.berkeley.edu/ http://verbs.colorado.edu/˜mpalm"
S10-1008,P86-1004,1,0.741997,"fer and Caroline Sporleder Roser Morante Computational Linguistics CNTS Saarland University University of Antwerp {josefr,csporled}@coli.uni-sb.de Roser.Morante@ua.ac.be Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu Collin Baker ICSI Berkeley, CA 94704 collin@icsi.berkeley.edu projects such as FrameNet1 and PropBank2 typically present their annotations lexicographically by lemma rather than by source text. It is clear that there is an interplay between local argument structure and the surrounding discourse (Fillmore, 1977). In early work, Palmer et al. (1986) discussed filling null complements from context by using knowledge about individual predicates and tendencies of referential chaining across sentences. But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semantic relations between the predicates involved. Two notable exceptions are Fillmore and Baker (2001) and Burchardt et al. (2005). Fillmore and Baker (2001) analyse a short newspaper article and discuss how frame semantics could benefit discourse processing but without making concrete suggestions"
S10-1008,W09-2417,1,0.499792,"Missing"
S10-1008,H86-1011,1,\N,Missing
sporleder-etal-2006-identifying,A97-1028,0,\N,Missing
sporleder-etal-2006-identifying,E99-1001,0,\N,Missing
sporleder-etal-2006-identifying,W02-1017,0,\N,Missing
sporleder-etal-2006-identifying,W03-0425,0,\N,Missing
sporleder-etal-2006-identifying,W03-1505,0,\N,Missing
sporleder-etal-2006-identifying,buchholz-van-den-bosch-2000-integrating,1,\N,Missing
sporleder-etal-2006-identifying,W03-0419,0,\N,Missing
sporleder-etal-2006-identifying,W99-0613,0,\N,Missing
sporleder-etal-2006-identifying,W99-0612,0,\N,Missing
sporleder-etal-2010-idioms,burchardt-etal-2006-salsa,0,\N,Missing
sporleder-etal-2010-idioms,W09-3211,1,\N,Missing
sporleder-etal-2010-idioms,W09-0213,0,\N,Missing
sporleder-etal-2010-idioms,W07-1106,0,\N,Missing
sporleder-etal-2010-idioms,D08-1104,0,\N,Missing
sporleder-etal-2010-idioms,E09-1086,1,\N,Missing
sporleder-etal-2010-idioms,W07-1101,0,\N,Missing
sporleder-etal-2010-idioms,E06-1042,0,\N,Missing
sporleder-etal-2010-idioms,N10-1039,1,\N,Missing
sporleder-etal-2010-idioms,J09-1005,0,\N,Missing
sporleder-etal-2010-idioms,W03-1812,0,\N,Missing
sporleder-etal-2010-idioms,P06-4020,0,\N,Missing
sporleder-etal-2010-idioms,P06-2046,0,\N,Missing
sporleder-etal-2010-idioms,baldwin-etal-2004-road,0,\N,Missing
sporleder-etal-2010-idioms,2005.mtsummit-papers.11,0,\N,Missing
sporleder-etal-2010-idioms,E06-1043,0,\N,Missing
sporleder-etal-2010-idioms,M98-1006,0,\N,Missing
W04-3210,P01-1017,0,0.0184608,"Missing"
W04-3210,H92-1019,0,0.227927,"Missing"
W04-3210,W03-1009,0,0.358882,"liable cue for paragraph identification.5 3.2.1 Non-syntactic Features Distance (Ds , Dw ): These features encode the distance of the current sentence from the previous paragraph break. We measured distance in terms of the number of intervening sentences (D s ) as well as in terms of the number of intervening words (D w ). If paragraph breaks were driven purely by aesthetics one would expect this feature to be among the most successful ones.6 Sentence Length (Length): This feature encodes the number of words in the current sentence. Average sentence length is known to vary with text position (Genzel and Charniak, 2003) and it is possible that it also varies with paragraph position. Relative Position (Pos): The relative position of a sentence in the text is calculated by dividing the current sentence number by the number of sentences in the text. The motivation for this feature is that paragraph length may vary with text position. For example, it is possible that paragraphs at the beginning and end of a text are shorter than paragraphs in the middle and hence a paragraph break is more likely at the two former text positions. Quotes (Quote p , Quotec , Quotei ): These features encode whether the previous or c"
W04-3210,J97-1003,0,0.353531,"Missing"
W04-3210,J02-1002,0,0.072683,"Missing"
W04-3210,A97-1004,0,0.100203,"Missing"
W04-3210,A00-1012,0,0.0213962,"y hard to read, which can cause processing difficulties, especially if speech recognition is used to provide deaf students with real-time transcripts of lectures. Furthermore, sometimes the output of a speech recogniser needs to be processed automatically by applications such as information extraction or summarisation. Most of these applications (e.g., Christensen et al., (2004)) port techniques developed for written texts to spoken texts and therefore require input that is punctuated and broken into paragraphs. While there has been some research on finding sentence boundaries in spoken text (Stevenson and Gaizauskas, 2000), there has been little research on determining paragraph boundaries.1 If paragraph boundaries were mainly an aesthetic device for visually breaking up long texts into smaller chunks, as has previously been suggested (see Longacre (1979)), paragraph boundaries could be easily inserted by splitting a text into several equal-size segments. Psycho-linguistic research, however, indicates that paragraph boundaries are not purely aesthetic. For example, Stark (1988) 1 There has been research on using phonetic cues to segment speech into “acoustic paragraphs” (Hauptmann and Smith, 1995). However, the"
W04-3210,P01-1064,0,0.0360184,"anguages and across domains. We also assess human performance on the same task and whether it differs across domains. 2 Related Work Previous work has focused extensively on the task of automatic text segmentation whose primary goal is to divide individual texts into sub-topics. Despite their differences, most methods are unsupervised and typically rely on the distribution of words in a given text to provide cues for topic segmentation.2 Hearst’s (1997) TextTiling algorithm, for example, determines sub-topic boundaries on the basis of term overlap in adjacent text blocks. In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability. Beeferman et al. (1999) use supervised learning methods to infer boundaries between texts. They employ language models to detect topic shifts and combine them with cue word features. 2 Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview. Our work differs from these previous approaches in that paragraphs do not always corre"
W09-2417,S07-1018,1,0.851873,"nce resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank,"
W09-2417,W04-2412,0,0.0812467,"Missing"
W09-2417,W05-0620,0,0.313707,"Missing"
W09-2417,J02-3001,0,0.0966796,"s sentence boundaries. Specifically, the task aims at linking locally uninstantiated roles to their coreferents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inf"
W09-2417,W04-0803,0,0.0306698,"so from co-reference resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (B"
W09-2417,S07-1008,0,0.0681195,"Missing"
W09-2417,C08-1084,1,0.879566,"Missing"
W09-2417,P86-1004,1,0.770233,"he fact that large-scale manual annotation projects such as FrameNet1 and PropBank2 typically present their annotations lexicographically by lemma rather than by source text. Furthermore, in the case of FrameNet, the annotation effort did not start out with the goal of exhaustive corpus annotation but instead focused on isolated instances of the target words sampled from a very large corpus, which did not allow for a view of the data as ‘full-text annotation’. It is clear that there is an interplay between local argument structure and the surrounding discourse (Fillmore, 1977). In early work, Palmer et al. (1986) discussed filling null complements from context by using knowledge about individual predicates and ten1 http://framenet.icsi.berkeley.edu/ http://verbs.colorado.edu/˜mpalmer/ projects/ace.html 2 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 106–111, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics dencies of referential chaining across sentences. But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semanti"
W09-2417,D07-1002,0,0.0290019,"in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). However, semantic role labelling as it is currently defined also misses a lot of information that would be beneficial for NLP applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or question answering. The reason for this is that SRL has traditionally been viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded and this leads to a loss of important semantic information. This view of SRL as a sentence-"
W09-2417,P03-1002,0,0.0620561,"years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). However, semantic role labelling as it is currently defined also misses a lot of information that would be beneficial for NLP applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or question answering. The reason for this is that SRL has traditionally been viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded and this leads to a loss of important semantic"
W09-2417,W08-2121,0,0.0907887,"Missing"
W09-2417,S07-1017,1,\N,Missing
W09-2417,H86-1011,1,\N,Missing
W09-2417,erk-pado-2004-powerful,0,\N,Missing
W09-3003,W04-3202,0,0.070424,"Missing"
W09-3003,brants-plaehn-2000-interactive,0,0.0682926,"Missing"
W09-3003,burchardt-etal-2006-salto,0,0.0276165,"Missing"
W09-3003,H01-1026,0,0.0727584,"Missing"
W09-3003,W06-0602,0,0.0520171,"Missing"
W09-3003,W07-1509,0,0.0260804,"Missing"
W09-3003,J93-2004,0,0.0329044,"ion and for Shalmaneser error types are made by human annotators throughout all three annotation trials, and that these errors are different from the ones made by the ASRL. Indicated by f-score, the most difficult frames in our data set are Scrutiny, Fluidic motion, Seeking, Make noise and Communication noise. This shows that automatic pre-annotation, even if noisy and of low quality, does not corrupt human annotators on a grand scale. Furthermore, if the preannotation is good it can even improve the overall annotation quality. This is in line with previous studies for other annotation tasks (Marcus et al., 1993). 4.3 pre-annotation decreases annotation quality. Most interestingly, the two annotators who showed a decrease in f-score on the text segments pre-annotated by Shalmaneser (compared to the text segments with no pre-annotation provided) had been assigned to the same group (Group I). Both had first annotated the enhanced, high-quality pre-annotation, in the second trial the sentences pre-annotated by Shalmaneser, and finally the texts with no pre-annotation. It might be possible that they benefitted from the ongoing training, resulting in a higher f-score for the third text segment (no pre-anno"
W09-3003,P02-1045,0,0.0339238,"cquisition bottleneck is a well-known problem and there have been numerous efforts to address it on the algorithmic side. Examples include the development of weakly supervised learning methods such as co-training and active learning. However, addressing only the algorithmic side is not always possible and not always desirable in all scenarios. First, some machine learning solutions are not as generally applicable or widely re-usable as one might think. It has been shown, for example, that co-training does not work well for problems which cannot easily be factorized into two independent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly 19 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19–26, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP frame typically also requires fewer physical operations from the annotator than correcting a number of wrongly assigned frame elements. We aim to answer three research questions in our study: First, we explore whether pre-annotation of frame labels can indeed speed up the annotation process. This question is important because frame as"
W09-3003,W03-1015,0,0.0167118,"is a well-known problem and there have been numerous efforts to address it on the algorithmic side. Examples include the development of weakly supervised learning methods such as co-training and active learning. However, addressing only the algorithmic side is not always possible and not always desirable in all scenarios. First, some machine learning solutions are not as generally applicable or widely re-usable as one might think. It has been shown, for example, that co-training does not work well for problems which cannot easily be factorized into two independent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly 19 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19–26, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP frame typically also requires fewer physical operations from the annotator than correcting a number of wrongly assigned frame elements. We aim to answer three research questions in our study: First, we explore whether pre-annotation of frame labels can indeed speed up the annotation process. This question is important because frame assignment, in terms of"
W09-3003,C02-1145,0,0.0741674,"Missing"
W09-3003,P98-1013,0,0.00732122,"number of scenarios for which there is simply no alternative to high-quality, manually annotated data; for example, if the annotated corpus is used for empirical research in linguistics (Meurers and M¨uller, 2007; Meurers, 2005). In this paper, we look at this problem from the data creation side. Specifically we explore whether a semi-automatic annotation set-up in which a human expert corrects the output of an automatic system can help to speed up the annotation process without sacrificing annotation quality. For our study, we explore the task of framesemantic argument structure annotation (Baker et al., 1998). We chose this particular task because it is a rather complex – and therefore time-consuming – undertaking, and it involves making a number of different but interdependent annotation decisions for each instance to be labeled (e.g. frame assignment and labeling of frame elements, see Section 3.1). Semi-automatic support would thus be of real benefit. More specifically, we explore the usefulness of automatic pre-annotation for the first step in the annotation process, namely frame assignment (word sense disambiguation). Since the available inventory of frame elements is dependent on the chosen"
W09-3003,C98-1013,0,\N,Missing
W09-3211,W02-2001,0,0.0168621,"example in 4, the word ice does not contribute to the overall cohesion as it is poorly connected to all the other (content) words in this specific context (play, party, games). 2 Related Work Type-based MWE classification aims to extract multiword expression types in text from observations of the token distribution. It aims to pick up on word combinations which occur with comparatively high frequencies when compared to the frequencies of the individual words (Evert and Krenn, 2001; Smadja, 19993). The lexical and syntactic fixedness property can also be utilized to automatically extract MWEs (Baldwin and Villavicencio, 2002). The study of semantic compositionality of MWEs focuses on the degree to which the semantics of the parts of an MWE contribute towards the meaning of the whole. The aim is a binary classification of the MWEs as idiosyncratically decomposable (e.g. spill the beans) or non-decomposable (e.g. kick the bucket). Several approaches have been proposed. Lin (1999) uses the substitution test2 and mutual information (MI) to determine the compositionality of the phrase. An obvious change of the MI value of the phrase in the substitution test is taken as the evidence of the MWEs being non-compositional."
W09-3211,baldwin-etal-2004-road,0,0.0817822,"Missing"
W09-3211,W03-1809,0,0.0135091,". The study of semantic compositionality of MWEs focuses on the degree to which the semantics of the parts of an MWE contribute towards the meaning of the whole. The aim is a binary classification of the MWEs as idiosyncratically decomposable (e.g. spill the beans) or non-decomposable (e.g. kick the bucket). Several approaches have been proposed. Lin (1999) uses the substitution test2 and mutual information (MI) to determine the compositionality of the phrase. An obvious change of the MI value of the phrase in the substitution test is taken as the evidence of the MWEs being non-compositional. Bannard et al. (2003) assume that compositional MWEs occur in similar lexical context as their component parts. The co-occurrence vector representations of verb particle construction (VPC) and the component words are utilized to determine the compositionality of the MWE. There have also been a few token-based classification approaches, aimed at classifying individual instances of a potential idiom as literal or nonliteral. Katz and Giesbrecht (2006) make use of latent semantic analysis (LSA) to explore the local linguistic context that can serve to identify multiword expressions that have non-compositional meaning"
W09-3211,E06-1042,0,0.110003,"Missing"
W09-3211,W07-1106,0,0.245046,"Missing"
W09-3211,W06-1605,0,0.0176728,"(ti , tj )). Modeling semantic relatedness between two terms is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we need to model a wide range of relations, e.g., between ice and water. Hence we opted for a distributional approach. We experimented with two different approaches, one (DV ) based on syntactic co-occurrences in a large text corpus and the other (N GD) based on search engine"
W09-3211,P01-1025,0,0.0174765,"rall semantic connectivity of the whole sentence by the fact that ice is semantically related to water. In contrast, in the non-literal example in 4, the word ice does not contribute to the overall cohesion as it is poorly connected to all the other (content) words in this specific context (play, party, games). 2 Related Work Type-based MWE classification aims to extract multiword expression types in text from observations of the token distribution. It aims to pick up on word combinations which occur with comparatively high frequencies when compared to the frequencies of the individual words (Evert and Krenn, 2001; Smadja, 19993). The lexical and syntactic fixedness property can also be utilized to automatically extract MWEs (Baldwin and Villavicencio, 2002). The study of semantic compositionality of MWEs focuses on the degree to which the semantics of the parts of an MWE contribute towards the meaning of the whole. The aim is a binary classification of the MWEs as idiosyncratically decomposable (e.g. spill the beans) or non-decomposable (e.g. kick the bucket). Several approaches have been proposed. Lin (1999) uses the substitution test2 and mutual information (MI) to determine the compositionality of"
W09-3211,J07-2002,0,0.0238211,"Missing"
W09-3211,J09-1005,0,0.447706,"Missing"
W09-3211,W03-1807,0,0.0128542,"g., I handed in my thesis = I handed my thesis in vs. Kim kicked the bucket 6= *the bucket was kicked by Kim), variation in productivity (there are various levels of productivity for different MWEs, e.g., kick/*beat/*hit the bucket, call/ring/phone/*telephone up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown (1) When the members of De la Guarda aren’t hanging around, they’re yelling and bouncing off the wall. (2) Blinded by the sun, Erstad leaped at the wall, but the ball bounced off the wall well below his glove. Our work aims to distinguish the literal and non-literal usages of idiomatic expressions in a 75 Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 75–83, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP we provide a formalization of the graph and experiment with different vertex and edge weightin"
W09-3211,1997.mtsummit-papers.19,0,0.103157,"Missing"
W09-3211,P90-1034,0,0.171924,"edness between two tokens (h(ti , tj )). Modeling semantic relatedness between two terms is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we need to model a wide range of relations, e.g., between ice and water. Hence we opted for a distributional approach. We experimented with two different approaches, one (DV ) based on syntactic co-occurrences in a large text corpus and"
W09-3211,W06-1203,0,0.154813,"e the compositionality of the phrase. An obvious change of the MI value of the phrase in the substitution test is taken as the evidence of the MWEs being non-compositional. Bannard et al. (2003) assume that compositional MWEs occur in similar lexical context as their component parts. The co-occurrence vector representations of verb particle construction (VPC) and the component words are utilized to determine the compositionality of the MWE. There have also been a few token-based classification approaches, aimed at classifying individual instances of a potential idiom as literal or nonliteral. Katz and Giesbrecht (2006) make use of latent semantic analysis (LSA) to explore the local linguistic context that can serve to identify multiword expressions that have non-compositional meaning. They measure the cosine vector similarity between the vectors associated with an MWE as a whole and the vectors associated with its constituent parts and interpret it as the degree to which the MWE is compositional. They report an av(3) The water would break the ice into floes with its accumulated energy. (4) We played a couple of party games to break the ice. Our approach bears similarities to Hirst and StOnge’s (1998) method"
W09-3211,E09-1086,1,0.816542,"hich makes this method weakly supervised. We propose an alternative, parameter-free method in which we model the cohesive structure of a discourse as a graph structure (called cohesion graph), where the vertices of the graph correspond to the content words of the text and the edges encode the semantic relatedness between pairs of words. To distinguish between literal and non-literal use of MWEs, we look at how the average relatedness of the graph changes when the component words of the MWE are excluded or included in the graph (see Section 3).1 We first introduced the cohesion graph method in Sporleder and Li (2009). In the present paper, 1 By modeling lexical cohesion as a graph structure, we follow earlier approaches in information retrieval, notably by Salton and colleagues (Salton et al., 1994). The difference is that these works aim at representing similarity between larger text segments (e.g., paragraphs) in a so-called ’text’ or ’paragraph relation map’, whose vertices correspond to a text segment and whose edges represent the similarity between the segments (modeled as weighted term overlap). 2 The substitution test aims to replace part of the idiom’s component words with semantically similar wor"
W09-3211,N03-1035,0,0.0126262,"ht), variation in syntactic flexibility (e.g., I handed in my thesis = I handed my thesis in vs. Kim kicked the bucket 6= *the bucket was kicked by Kim), variation in productivity (there are various levels of productivity for different MWEs, e.g., kick/*beat/*hit the bucket, call/ring/phone/*telephone up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown (1) When the members of De la Guarda aren’t hanging around, they’re yelling and bouncing off the wall. (2) Blinded by the sun, Erstad leaped at the wall, but the ball bounced off the wall well below his glove. Our work aims to distinguish the literal and non-literal usages of idiomatic expressions in a 75 Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 75–83, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP we provide a formalization of the graph and experiment wit"
W09-3211,P98-2127,0,0.0748798,"two tokens (h(ti , tj )). Modeling semantic relatedness between two terms is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we need to model a wide range of relations, e.g., between ice and water. Hence we opted for a distributional approach. We experimented with two different approaches, one (DV ) based on syntactic co-occurrences in a large text corpus and the other ("
W09-3211,M98-1006,0,0.137802,"two tokens (h(ti , tj )). Modeling semantic relatedness between two terms is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we need to model a wide range of relations, e.g., between ice and water. Hence we opted for a distributional approach. We experimented with two different approaches, one (DV ) based on syntactic co-occurrences in a large text corpus and the other ("
W09-3211,P99-1041,0,0.0221561,"atively high frequencies when compared to the frequencies of the individual words (Evert and Krenn, 2001; Smadja, 19993). The lexical and syntactic fixedness property can also be utilized to automatically extract MWEs (Baldwin and Villavicencio, 2002). The study of semantic compositionality of MWEs focuses on the degree to which the semantics of the parts of an MWE contribute towards the meaning of the whole. The aim is a binary classification of the MWEs as idiosyncratically decomposable (e.g. spill the beans) or non-decomposable (e.g. kick the bucket). Several approaches have been proposed. Lin (1999) uses the substitution test2 and mutual information (MI) to determine the compositionality of the phrase. An obvious change of the MI value of the phrase in the substitution test is taken as the evidence of the MWEs being non-compositional. Bannard et al. (2003) assume that compositional MWEs occur in similar lexical context as their component parts. The co-occurrence vector representations of verb particle construction (VPC) and the component words are utilized to determine the compositionality of the MWE. There have also been a few token-based classification approaches, aimed at classifying"
W09-3211,J91-1002,0,\N,Missing
W09-3211,J93-1007,0,\N,Missing
W09-3211,C94-2121,0,\N,Missing
W09-3211,J02-4004,0,\N,Missing
W09-3211,S07-1106,0,\N,Missing
W09-3211,P03-1017,0,\N,Missing
W09-3211,P06-4020,0,\N,Missing
W09-3211,J06-1003,0,\N,Missing
W09-3211,J03-1005,0,\N,Missing
W09-3211,C98-2122,0,\N,Missing
W09-3738,S07-1018,0,0.0236359,"n Computational Semantics, pages 333–337, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Theories of semantic argument structure, such as Frame Semantics, model relations within individual sentences, namely the relation between a lexical item and its semantic arguments such as agent or patient. During the last five to ten years there has been much research in this area, as witnessed by several large scale projects aimed at providing lexicons and annotated corpora (e.g., FrameNet,1 PropBank,2 and SALSA3 ), and numerous shared tasks on semantic role labelling (Baker et al., 2007; Carreras and M`arquez, 2005; Carreras and M`arques, 2004). While the performance of semantic parsers is still lower than that of syntactic parsers, it is now good enough that NLP tasks such as information extraction or question answering can be shown to benefit from automatically computed semantic argument structures (Moschitti et al., 2003; Shen and Lapata, 2007). While Frame Semantics was originally seen as being grounded in discourse (Fillmore, 1977), its computational treatment has largely been restricted to the sentence level, which may also be due to the fact that annotated data typica"
W09-3738,W04-2412,0,0.0272691,"Missing"
W09-3738,W05-0620,0,0.0257181,"Missing"
W09-3738,W04-2322,0,0.0461791,"Missing"
W09-3738,D07-1002,0,0.0326333,"there has been much research in this area, as witnessed by several large scale projects aimed at providing lexicons and annotated corpora (e.g., FrameNet,1 PropBank,2 and SALSA3 ), and numerous shared tasks on semantic role labelling (Baker et al., 2007; Carreras and M`arquez, 2005; Carreras and M`arques, 2004). While the performance of semantic parsers is still lower than that of syntactic parsers, it is now good enough that NLP tasks such as information extraction or question answering can be shown to benefit from automatically computed semantic argument structures (Moschitti et al., 2003; Shen and Lapata, 2007). While Frame Semantics was originally seen as being grounded in discourse (Fillmore, 1977), its computational treatment has largely been restricted to the sentence level, which may also be due to the fact that annotated data typically consists of sets of individual sentences rather than of running text, though there has been some effort recently to create full text annotations as well. Few studies tried to connect frame semantic annotations across sentences. Two notable exceptions are Fillmore and Baker (2001) and Burchardt et al. (2005). Fillmore and Baker (2001) analyse a short newspaper ar"
W09-3738,N03-1030,0,0.045361,"linguistic analysis. The model should be sophisticated enough to aid applications such as text mining, information extraction, question answering, and text summarisation. Discourse processing deals with modelling the meaning of multisentence units. Early approaches (e.g. Hobbs et al., 1993) were heavily knowledge-based and, while these systems worked well on small, well-defined domains, they generally did not scale up very well. More recent research largely abandoned the knowledge-based approach in favour of much shallower systems, either rule-based (Polanyi et al., 2004) or machine-learned (Soricut and Marcu, 2003). These systems rely largely on surface cues. While shallow models can be quite successful, they also have clear limitations. For example, progress on discourse parsing has stagnated in the last years and text summarisation is still a challenge, especially from multiple input documents. 333 Proceedings of the 8th International Conference on Computational Semantics, pages 333–337, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Theories of semantic argument structure, such as Frame Semantics, model relations within individual sentences, namely the relation betw"
W13-0111,S10-1059,0,0.523964,"“[You]Src ’re rightCorr , Mr. Holmes.” Semantic role labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven PropBank roles. Second, the SemEval 2010 data set consists of running text in which null instantiations are marked and resolved, while the data set used by Gerber and Chai (2010, 2012) consists of annotated examples sentences for just a few predi"
W13-0111,P10-1160,0,0.511495,"oken by a different speaker, namely Holmes), which provides details of the fact about which Holmes was right. (1) [“A white cock,”]Stim said [he]Exp . “[Most]Deg interestingMssf !” (2) A. [“Your powers seem superior to your opportunities.”]Inf B. “[You]Src ’re rightCorr , Mr. Holmes.” Semantic role labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven Prop"
W13-0111,J12-4003,0,0.151593,"nal results we report are for the unseen Hound data (the test set in SemEval). 5 Modeling NI Resolution While the complete NI resolution task consists of three steps, detecting NIs, classifying NIs as DNIs or INIs, and resolving DNIs, in this paper, we focus exclusively on the third task as this is by far the most difficult one. We model the problem as a weakly supervised task, where the only type of supervision is the use of a corpus annotated with overtly realised semantic roles. We do not make use of the NI annotations in the training set. This distinguishes our work from the approaches by Gerber and Chai (2012; 2010) and Silberer and Frank (2012). However, like these two we employ an entity mention model, that is, we take into account the whole coreference chain for a discourse entity when assessing its likelihood of filling a null instantiated role. For this, we make use of the gold standard coreference chains in the SemEval data. So as not to have an unfair advantage, we also create singleton chains for all noun phrases without an overt co-referent, since such cases could, in theory, be antecedents for omitted arguments. Finally, since NIs can also refer to complete sentences, we augment the enti"
W13-0111,S10-1008,1,0.624077,"our opportunities.”]Inf B. “[You]Src ’re rightCorr , Mr. Holmes.” Semantic role labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven PropBank roles. Second, the SemEval 2010 data set consists of running text in which null instantiations are marked and resolved, while the data set used by Gerber and Chai (2010, 2012) consists of annotated examples sentences f"
W13-0111,S12-1001,0,0.460718,"labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven PropBank roles. Second, the SemEval 2010 data set consists of running text in which null instantiations are marked and resolved, while the data set used by Gerber and Chai (2010, 2012) consists of annotated examples sentences for just a few predicates. This makes the latter data set easier as there are few"
W13-0111,S10-1065,0,0.270962,"tCorr , Mr. Holmes.” Semantic role labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven PropBank roles. Second, the SemEval 2010 data set consists of running text in which null instantiations are marked and resolved, while the data set used by Gerber and Chai (2010, 2012) consists of annotated examples sentences for just a few predicates. This makes the latter"
W13-0111,W11-0908,0,0.786859,"Missing"
W13-5602,sporleder-etal-2010-idioms,1,\N,Missing
W13-5602,W07-1106,0,\N,Missing
W13-5602,W06-1203,0,\N,Missing
W13-5602,E09-1086,1,\N,Missing
W13-5602,E06-1042,0,\N,Missing
W13-5602,N10-1039,1,\N,Missing
W13-5602,C10-2078,1,\N,Missing
W14-0905,W12-2513,0,0.299747,"ry hypotheses. Celikyilmaz et al. (2010) extracts dialogue interactions in order to analyze semantic orientations of social networks from literature. In order to perform large-scale analyses of the works, both Rydberg-Cox (2011) and Suen et al. (2013) extract networks from structured text: Greek tragedies the first, plays and movie scripts the latter. All the approaches mentioned above produce static networks which are flat representations of the novel as a whole. In them, past, present, and future are represented at once. By means of static networks, time turns into space. The recent work by Agarwal et al. (2012) questions the validity of static network analysis. Their authors introduce the concept of dynamic network analysis for literature, motivated by the idea that static networks can distort the importance of the characters (exemplified through an analysis of Lewis Carroll’s Alice in Wonderland). A dynamic social network is but the collection of independent networks for each of the parts in which the novel is divided. ments is the task of automatically grouping texts that share the same author, by determining the set of features that distinguish one author from any other. The first approaches focu"
W14-0905,P10-1015,0,0.103892,"Missing"
W16-2107,D07-1020,0,0.0209122,"likely to be populated by many people that are absent from historical accounts and, therefore, also from KBs. We intentionally refrain from linking entities to a knowledge base to avoid the bias towards entities which are present in it. Ter Braake and Fokkens (2015) discuss the problem of biases in historiography and the importance of rescuing long-neglected individuals from the oblivion of history. sions (see Mann and Yarowsky (2003), Niu et al. (2004), Al-Kamha and Embley (2004), Bollegala et al. (2006)). The most exploited source of evidence for clustering is named entities (Blume (2005), Chen and Martin (2007), Popescu and Magnini (2007), Kalashnikov et al. (2007)). Artiles et al. (2009a) thoroughly study the role of named entities in the task and conclude that they often increase precision at the expense of recall, even though they leave the door open to more sophisticated approaches using named entities, such as in combination with other levels of features (Yoshida et al., 2010) or in graph-based approaches (Kalashnikov et al. (2008), Jiang et al. (2009), Chen et al. (2012)). Over the last years, the trend has moved towards using resource-based approaches, such as a knowledge base (KB) (Dutta and"
W16-2107,D12-1076,0,0.0637851,"(2004), Bollegala et al. (2006)). The most exploited source of evidence for clustering is named entities (Blume (2005), Chen and Martin (2007), Popescu and Magnini (2007), Kalashnikov et al. (2007)). Artiles et al. (2009a) thoroughly study the role of named entities in the task and conclude that they often increase precision at the expense of recall, even though they leave the door open to more sophisticated approaches using named entities, such as in combination with other levels of features (Yoshida et al., 2010) or in graph-based approaches (Kalashnikov et al. (2008), Jiang et al. (2009), Chen et al. (2012)). Over the last years, the trend has moved towards using resource-based approaches, such as a knowledge base (KB) (Dutta and Weikum, 2015) or Wikipedia, and the person name disambiguation task has been in most cases subsumed by entity linking. Bunescu and Pasca (2006), Cucerzan (2007) and Han and Zhao (2009) are only some of the many approaches that exploit the wide coverage of Wikipedia by linking entity mentions to the referring Wikipedia articles. 4 The model Given the assumption that a person name always refers to the same entity in a given document,3 person name clustering amounts to doc"
W16-2107,D07-1074,0,0.101819,"ude that they often increase precision at the expense of recall, even though they leave the door open to more sophisticated approaches using named entities, such as in combination with other levels of features (Yoshida et al., 2010) or in graph-based approaches (Kalashnikov et al. (2008), Jiang et al. (2009), Chen et al. (2012)). Over the last years, the trend has moved towards using resource-based approaches, such as a knowledge base (KB) (Dutta and Weikum, 2015) or Wikipedia, and the person name disambiguation task has been in most cases subsumed by entity linking. Bunescu and Pasca (2006), Cucerzan (2007) and Han and Zhao (2009) are only some of the many approaches that exploit the wide coverage of Wikipedia by linking entity mentions to the referring Wikipedia articles. 4 The model Given the assumption that a person name always refers to the same entity in a given document,3 person name clustering amounts to document clustering. In order to cluster documents, a similarity measure is needed. The core idea is that two documents should be clustered together if they are similar enough, i.e. if there exists enough evidence that they belong together. The evidence needed, though, may vary greatly de"
W16-2107,Q15-1002,0,0.0149655,"in (2007), Popescu and Magnini (2007), Kalashnikov et al. (2007)). Artiles et al. (2009a) thoroughly study the role of named entities in the task and conclude that they often increase precision at the expense of recall, even though they leave the door open to more sophisticated approaches using named entities, such as in combination with other levels of features (Yoshida et al., 2010) or in graph-based approaches (Kalashnikov et al. (2008), Jiang et al. (2009), Chen et al. (2012)). Over the last years, the trend has moved towards using resource-based approaches, such as a knowledge base (KB) (Dutta and Weikum, 2015) or Wikipedia, and the person name disambiguation task has been in most cases subsumed by entity linking. Bunescu and Pasca (2006), Cucerzan (2007) and Han and Zhao (2009) are only some of the many approaches that exploit the wide coverage of Wikipedia by linking entity mentions to the referring Wikipedia articles. 4 The model Given the assumption that a person name always refers to the same entity in a given document,3 person name clustering amounts to document clustering. In order to cluster documents, a similarity measure is needed. The core idea is that two documents should be clustered to"
W16-2107,S07-1012,0,0.0376475,"for which we learn a threshold from a small manually labeled data set) and the ambiguity of the overlapping nodes (for which we manually set a penalty function). Network overlap is not always a sufficient source of information (in particular, small overlap does not mean that the documents involved should not be clustered together), and we additionally make use of further features in those cases where networks do not provide sufficient evAn evaluation campaign was organized in 2007 to tackle the problem of name ambiguity on the WWW and the interest of this task moved largely to the web domain (Artiles et al., 2007). However, web pages and news articles differ greatly in their form. Even though more heterogeneous, web pages tend to be more structured and provide additional features that can be exploited (url, e-mail addresses, phone numbers, etc.). In 2011 a similar evaluation campaign was proposed at EVALITA 2011 in order to evaluate CDCR in Italian in the news domain (Bentivogli et al., 2013). Pairwise clustering has been the most popular clustering method: two documents are grouped together if their similarity is higher than a certain threshold. To date, most approaches have used a fixed similarity th"
W16-2107,P05-1045,0,0.023507,"ohn Smith Corpus. 5.3 4 http://nlp.stanford.edu/software/CRF-NER.shtml http://textpro.fbk.eu/ 6 http://www.corpusitaliano.it/ 7 http://wiki.dbpedia.org/Downloads 5 8 The fifteen training instances for each range are: ‘Isabella Bossi Fedrigotti’ (0.0-0.1); ‘Marta Sala’, ‘Alberto Sighele’, ‘Roberto Baggio’, ‘Bruno Degasperi’, ‘Ombretta Colli’, and ‘Leonardo da Vinci’ (0.2-0.3); ‘Luisa Costa’, ‘Mario Monti’, and ‘Andrea Barbieri’ (0.3-0.4); ‘Antonio Conte’, ‘Antonio de Luca’, and ‘Antonio Russo’ (0.4-0.5); ‘Paolo Rossi’ (0.5-0.6); and ‘Giuseppe Rossi’ (0.6-0.7). Settings We use the Stanford NER (Finkel et al., 2005) and TextPro (Pianta et al., 2008) to identify NEs in En69 Approach SNsimple TopicModel Zanoli et al. 2013 Rao et al. 2010 [1] Rao et al. 2010 [2] SNcomplete P 0.94 0.91 0.89 – – 0.87 cripco R 0.67 0.44 0.97 – – 0.95 F 0.78 0.55 0.93 – – 0.91 P 0.65 0.76 – 0.61 0.82 0.63 nytac sel R 0.74 0.27 – 0.78 0.24 0.75 F 0.67 0.37 – 0.68 0.37 0.68 P 0.65 0.71 – 0.60 0.85 0.79 johnsmith R F 0.6 0.62 0.51 0.59 – – 0.63 0.61 0.59 0.70 0.60 0.68 Table 3: Evaluation results. 5.4 Using the ambiguity of the query name to dynamically decide on a clustering strategy is crucial for the success of our method. Fail"
W16-2107,D09-1056,0,0.0451006,"Missing"
W16-2107,N04-1002,0,0.0561027,"o, Powell switched coaches from Randy Huntington to John Smith, who is renowned for his work with sprinters from 100 to 400 meters. These examples are drawn from The John Smith Corpus, the first reference set for CDCR, which was introduced by Bagga and Baldwin (1998). The authors also proposed a new scoring algorithm, B-Cubed, in order to evaluate the task, which was modeled as a document clustering problem. To solve the problem, the authors applied the standard vector space model based on context similarity. Several subsequent studies adapted and extended the approach (Ravin and Kazi (1999), Gooi and Allan (2004)). More recent methods apply LDA and other topic models (Song et al. (2007), Kozareva and Ravi (2011)). Yoshida et al. (2010) distinguish between weak and strong features. Weak features are the context words of the document, as opposed to strong features such as named entities, biographical information, key phrases, or temporal expresRelated work The idea of using social networks to find information from historical texts is not a new one. One of the first and more influential works is Padgett and Ansell (1993), in which its authors use networks of marriages between the most eminent Florentine"
W16-2107,P98-1012,0,0.166817,"Chrysler Corp., Alex Trotman of Ford Motor Co. and finally with John Smith Jr. of General Motors Corp. (3) Blair became Labour leader after the sudden death of his successor John Smith in 1994 and since then has steadily purged the party of its high-spend and high-tax policies and its commitment to national ownership of industrial assets. (4) Two years ago, Powell switched coaches from Randy Huntington to John Smith, who is renowned for his work with sprinters from 100 to 400 meters. These examples are drawn from The John Smith Corpus, the first reference set for CDCR, which was introduced by Bagga and Baldwin (1998). The authors also proposed a new scoring algorithm, B-Cubed, in order to evaluate the task, which was modeled as a document clustering problem. To solve the problem, the authors applied the standard vector space model based on context similarity. Several subsequent studies adapted and extended the approach (Ravin and Kazi (1999), Gooi and Allan (2004)). More recent methods apply LDA and other topic models (Song et al. (2007), Kozareva and Ravi (2011)). Yoshida et al. (2010) distinguish between weak and strong features. Weak features are the context words of the document, as opposed to strong"
W16-2107,W06-0803,0,0.0340383,"tomatically constructing social networks from historical newspapers. The articles that constitute our corpus are likely to be populated by many people that are absent from historical accounts and, therefore, also from KBs. We intentionally refrain from linking entities to a knowledge base to avoid the bias towards entities which are present in it. Ter Braake and Fokkens (2015) discuss the problem of biases in historiography and the importance of rescuing long-neglected individuals from the oblivion of history. sions (see Mann and Yarowsky (2003), Niu et al. (2004), Al-Kamha and Embley (2004), Bollegala et al. (2006)). The most exploited source of evidence for clustering is named entities (Blume (2005), Chen and Martin (2007), Popescu and Magnini (2007), Kalashnikov et al. (2007)). Artiles et al. (2009a) thoroughly study the role of named entities in the task and conclude that they often increase precision at the expense of recall, even though they leave the door open to more sophisticated approaches using named entities, such as in combination with other levels of features (Yoshida et al., 2010) or in graph-based approaches (Kalashnikov et al. (2008), Jiang et al. (2009), Chen et al. (2012)). Over the la"
W16-2107,W03-0405,0,0.0356524,"solution for the problem of person name disambiguation in the task of automatically constructing social networks from historical newspapers. The articles that constitute our corpus are likely to be populated by many people that are absent from historical accounts and, therefore, also from KBs. We intentionally refrain from linking entities to a knowledge base to avoid the bias towards entities which are present in it. Ter Braake and Fokkens (2015) discuss the problem of biases in historiography and the importance of rescuing long-neglected individuals from the oblivion of history. sions (see Mann and Yarowsky (2003), Niu et al. (2004), Al-Kamha and Embley (2004), Bollegala et al. (2006)). The most exploited source of evidence for clustering is named entities (Blume (2005), Chen and Martin (2007), Popescu and Magnini (2007), Kalashnikov et al. (2007)). Artiles et al. (2009a) thoroughly study the role of named entities in the task and conclude that they often increase precision at the expense of recall, even though they leave the door open to more sophisticated approaches using named entities, such as in combination with other levels of features (Yoshida et al., 2010) or in graph-based approaches (Kalashni"
W16-2107,E06-1002,0,0.0886051,"ties in the task and conclude that they often increase precision at the expense of recall, even though they leave the door open to more sophisticated approaches using named entities, such as in combination with other levels of features (Yoshida et al., 2010) or in graph-based approaches (Kalashnikov et al. (2008), Jiang et al. (2009), Chen et al. (2012)). Over the last years, the trend has moved towards using resource-based approaches, such as a knowledge base (KB) (Dutta and Weikum, 2015) or Wikipedia, and the person name disambiguation task has been in most cases subsumed by entity linking. Bunescu and Pasca (2006), Cucerzan (2007) and Han and Zhao (2009) are only some of the many approaches that exploit the wide coverage of Wikipedia by linking entity mentions to the referring Wikipedia articles. 4 The model Given the assumption that a person name always refers to the same entity in a given document,3 person name clustering amounts to document clustering. In order to cluster documents, a similarity measure is needed. The core idea is that two documents should be clustered together if they are similar enough, i.e. if there exists enough evidence that they belong together. The evidence needed, though, ma"
W16-2107,P04-1076,0,0.0531973,"of person name disambiguation in the task of automatically constructing social networks from historical newspapers. The articles that constitute our corpus are likely to be populated by many people that are absent from historical accounts and, therefore, also from KBs. We intentionally refrain from linking entities to a knowledge base to avoid the bias towards entities which are present in it. Ter Braake and Fokkens (2015) discuss the problem of biases in historiography and the importance of rescuing long-neglected individuals from the oblivion of history. sions (see Mann and Yarowsky (2003), Niu et al. (2004), Al-Kamha and Embley (2004), Bollegala et al. (2006)). The most exploited source of evidence for clustering is named entities (Blume (2005), Chen and Martin (2007), Popescu and Magnini (2007), Kalashnikov et al. (2007)). Artiles et al. (2009a) thoroughly study the role of named entities in the task and conclude that they often increase precision at the expense of recall, even though they leave the door open to more sophisticated approaches using named entities, such as in combination with other levels of features (Yoshida et al., 2010) or in graph-based approaches (Kalashnikov et al. (2008),"
W16-2107,pianta-etal-2008-textpro,0,0.0287274,"Missing"
W16-2107,S07-1041,0,0.0244667,"by many people that are absent from historical accounts and, therefore, also from KBs. We intentionally refrain from linking entities to a knowledge base to avoid the bias towards entities which are present in it. Ter Braake and Fokkens (2015) discuss the problem of biases in historiography and the importance of rescuing long-neglected individuals from the oblivion of history. sions (see Mann and Yarowsky (2003), Niu et al. (2004), Al-Kamha and Embley (2004), Bollegala et al. (2006)). The most exploited source of evidence for clustering is named entities (Blume (2005), Chen and Martin (2007), Popescu and Magnini (2007), Kalashnikov et al. (2007)). Artiles et al. (2009a) thoroughly study the role of named entities in the task and conclude that they often increase precision at the expense of recall, even though they leave the door open to more sophisticated approaches using named entities, such as in combination with other levels of features (Yoshida et al., 2010) or in graph-based approaches (Kalashnikov et al. (2008), Jiang et al. (2009), Chen et al. (2012)). Over the last years, the trend has moved towards using resource-based approaches, such as a knowledge base (KB) (Dutta and Weikum, 2015) or Wikipedia,"
W16-2107,D09-1104,0,0.0153592,"articles differ greatly in their form. Even though more heterogeneous, web pages tend to be more structured and provide additional features that can be exploited (url, e-mail addresses, phone numbers, etc.). In 2011 a similar evaluation campaign was proposed at EVALITA 2011 in order to evaluate CDCR in Italian in the news domain (Bentivogli et al., 2013). Pairwise clustering has been the most popular clustering method: two documents are grouped together if their similarity is higher than a certain threshold. To date, most approaches have used a fixed similarity threshold. Very few approaches (Popescu (2009), Bentivogli et al. (2013)) have warned of the importance of determining the ambiguity degree of a person name in order to be able to estimate the number of output clusters. In Zanoli et al. (2013), a dynamic threshold similarity is introduced by estimating the ambiguity of the query name. This work, which in this aspect is the most similar to ours, differs greatly from ours with respect to the clustering strategy, since they rely on a KB, whereas we exploit only the context. 3 This is an assumption made by previous approaches and reminiscent of the ‘one sense per discourse’ assumption in word"
W16-2107,C10-2121,0,0.0359727,"Missing"
W16-2107,W99-0202,0,0.123711,"ssets. (4) Two years ago, Powell switched coaches from Randy Huntington to John Smith, who is renowned for his work with sprinters from 100 to 400 meters. These examples are drawn from The John Smith Corpus, the first reference set for CDCR, which was introduced by Bagga and Baldwin (1998). The authors also proposed a new scoring algorithm, B-Cubed, in order to evaluate the task, which was modeled as a document clustering problem. To solve the problem, the authors applied the standard vector space model based on context similarity. Several subsequent studies adapted and extended the approach (Ravin and Kazi (1999), Gooi and Allan (2004)). More recent methods apply LDA and other topic models (Song et al. (2007), Kozareva and Ravi (2011)). Yoshida et al. (2010) distinguish between weak and strong features. Weak features are the context words of the document, as opposed to strong features such as named entities, biographical information, key phrases, or temporal expresRelated work The idea of using social networks to find information from historical texts is not a new one. One of the first and more influential works is Padgett and Ansell (1993), in which its authors use networks of marriages between the m"
W16-2107,P11-1080,0,0.0161082,"closely related (e.g. Luciano Pavarotti and Pl´acido Domingo, two names that very often appear mentioned in the same text). Therefore, their social networks have much less predictive power than in natural data, where we assume that two people with the exact same name have low probability to share a big portion of their social networks. That would explain why we report low precision for this dataset, and yet the results obtained are comparable to those from the best of the two models introduced by Rao et al. (2010). The result reported for John Smith Corpus improves upon recent models, such as Singh et al. (2011), who obtained 0.664, but is far from the most recent approach (Rahimian et al., 2014), who obtained around 0.80. This might be well due to the fact that there was only one query name in our development set that had high ambiguity, which was, still, far from being as ambiguous as ‘John Smith’. Our method works overall better than any of the two methods from Rao et al. (2010) when we average the results for both English datasets. 6 Impact in the social sciences: a case study on Dutch religious history To assess the impact of this approach in the social sciences, we introduce here a case study t"
W16-2107,W11-2213,0,\N,Missing
wang-sporleder-2010-constructing,D09-1082,1,\N,Missing
wang-sporleder-2010-constructing,W07-1431,0,\N,Missing
wang-sporleder-2010-constructing,W05-1209,0,\N,Missing
wang-sporleder-2010-constructing,W09-3027,0,\N,Missing
wang-sporleder-2010-constructing,N03-1003,0,\N,Missing
wang-sporleder-2010-constructing,W07-1401,0,\N,Missing
