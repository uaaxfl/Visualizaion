1999.mtsummit-1.34,C96-1070,0,0.132529,"Missing"
1999.mtsummit-1.34,C96-1075,0,0.0608382,"Missing"
1999.mtsummit-1.34,P89-1013,0,0.0183586,"Missing"
1999.mtsummit-1.34,C88-2118,0,0.0291956,"Missing"
1999.mtsummit-1.34,P91-1024,1,0.749748,"Missing"
1999.mtsummit-1.34,P98-2233,1,0.883543,"Missing"
1999.mtsummit-1.34,W97-0404,0,0.260944,"Missing"
1999.mtsummit-1.34,W99-0207,1,\N,Missing
1999.mtsummit-1.34,C98-2228,1,\N,Missing
2001.mtsummit-papers.48,2001.mtsummit-papers.10,0,0.0492874,"Missing"
2001.mtsummit-papers.48,1999.mtsummit-1.34,1,0.915509,"ation E ngine The translation engine used for our experiments consists of a spoken-language machine translation system capable of bilingual translations between Japanese/English (JE). This transfer-driven translation system (TDMT) uses a constituent boundary parsing method (CBP) in an examplebased framework. The input sentence is incrementally parsed by matching meaningful units of linguistic structure (patterns) with a chart-parsing algorithm. Given a set of translation examples, TDMT tries to find the “closest” examples to the structured input by using a semantic distance calculation (SDC) (Sumita et al., 1999). By simulating the translation of the closest examples, the empirical transfer knowledge is applied to the source structure, resulting in a corresponding target structure, that can be used to generate the translation (cf. Figure 3). 3.2 Recycling sys tem The input of the proposed system (JeG) consists of the linguistic knowledge contained in the target representation of the JE system. The mapping algorithm for recycling the English translation knowledge is introduced in Section 3.2.3. First, it substitutes the English words in the English parse tree with corresponding German words by using a"
2001.mtsummit-papers.48,P98-2233,0,0.0131247,"imit the number of language pairs to language representatives. Moreover, the costs of multiple full-scale translation engines can be reduced to those of developing a transfer dictionary and a generation dictionary, and these knowledge resources are already frequently available, at least for common languages like English. The most difficult part of the translation process is carried out within the translation engine, e.g., a Japaneseto-English translation engine has to deal with problems like the recovery of the sentence subject, which is frequently omitted in Japanese but required in English (Yamamoto & Sumita, 1998). Similar to English, German is also a language that requires a subject. Thus we could benefit from the Japanese-to-English efforts by simply mapping and reusing the recovered subject for the generation of German translations. Furthermore, the number of generation markers utilized in a specific language is limited. Therefore the compilation of mapping rules for related languages becomes inexpensive. The disadvantage of knowledge resource recycling is the possible lack of translation knowledge required in the target language, i.e., grammatical information of the target language omitted or witho"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.5,0,0.0434217,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.6,0,0.0792214,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.8,1,0.857983,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.11,0,0.00856795,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.13,0,0.00858588,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.14,0,0.0294958,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.7,0,0.0434735,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.10,0,0.0223145,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.12,0,0.0295568,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.15,0,0.0273582,"Missing"
2004.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,\N,Missing
2004.iwslt-evaluation.1,2004.iwslt-evaluation.2,1,\N,Missing
2004.iwslt-evaluation.1,P02-1040,0,\N,Missing
2004.iwslt-evaluation.1,W05-0909,0,\N,Missing
2004.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2004.iwslt-evaluation.1,2005.iwslt-1.6,0,\N,Missing
2004.iwslt-evaluation.1,2004.iwslt-evaluation.4,0,\N,Missing
2004.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,\N,Missing
2004.iwslt-evaluation.1,P03-1021,0,\N,Missing
2004.iwslt-evaluation.1,2004.iwslt-evaluation.9,0,\N,Missing
2004.iwslt-evaluation.2,J90-2002,0,0.724612,"imal solution due to the enormous search space. However, SMT can sort translations in the order of their quality according to its statistical models. We show two different EBMT systems here, briefly explain each system, and then compare them. Finally, we ex1. Introduction There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. By using the IWSLT04 task, this paper describes two endeavors that are independent at this moment: (a) a hybridization of EBMT and statistical models, and (b) a new approach for SMT, phrase-based HMM. (a) is used in the “unrestricted” Japanese-to-English track (Section 2), and (b) is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technolog"
2004.iwslt-evaluation.2,W01-1401,1,0.863919,"on is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven transDucer), which exploits DP-matching between word sequences. Let’s illustrate the process with a simple sample below. Suppose we are translating a Japanese sentence into English. The Japanese input sentence (1-j) is translated into the English sentence (1-e) by utilizing the English sentence (2-e), whose source sentence (2-j) is similar to (1-j). The common parts are unchanged, and the different portions, shown in bold face, are substituted by consulting a bilingual dictionary. ;;; A Japanese input (1-j) iro/ga/ki/ni/iri/masen ;;; the most similar example in co"
2004.iwslt-evaluation.2,P91-1024,1,0.754066,"source sentence of examples from a bilingual corpus. For this, we use DP-matching, which tells us the edit distance between word sequences while giving us the matched portions between the input and the example. The edit distance is calculated as follows. The count of the inserted words, the count of the deleted words, and the semantic distance of the substituted words are summed. Then, this total is normalized by the sum of the lengths of the input and the source part of translation example. The semantic distance between two substituted words is calculated by using the hierachy of a thesaurus[4]. Our language resources in addition to a bilingual corpus are a bilingual dictionary, which is used for generating target sentences, and thesauri of both languages, which are used for incorporating the semantic distance between words into the distance between word sequences. Furthermore, lexical resources are also used for word alignment. Table 1: Resources used for two EBMTs in IWSLT04 unresticted Japanese-to-English track. bilingual corpus bilingual dictionary thesaurus grammar D3 travel domain (20K) in-house in-house N.A. HPAT travel domain (20K) in-house in-house in-house D3 achieves a go"
2004.iwslt-evaluation.2,C94-1015,0,0.0154509,"is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technologies, which are not used in the IWSLT04 task but boost translation performance, are also introduced in Section 4. 13 plain the selector used to determine the best from multiple translations based on SMT models. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial"
2004.iwslt-evaluation.2,P03-1057,1,0.787621,"s. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven tran"
2004.iwslt-evaluation.2,C02-1076,1,0.866736,"7.00 70.00 77.60 83.40 16.60 HPAT 38.60 59.80 77.40 83.40 16.60 SELECT 59.80 73.00 82.40 87.80 12.20 DIFF. +2.80 +3.00 +4.80 +4.40 -4.40 Next, the relationship between translation quality of element systems and gain by the selector was analyzed. Table 5 shows that the proposed selector reduces the number of low-quality translations (ranked “D”) while it increases the number of high-quality translations (ranked “S” to “B”). 2.3. SMT-based Selector We proposed an SMT-based method of automatically selecting the best translation among outputs generated by multiple machine translation (MT) systems [9]. Conventional approaches to the selection problem include a method that automatically selects the output to which the highest probability is assigned according to a language model (LM). [10] These existing methods have two problems. First, they do not check whether information on source sentences is adequately translated into MT outputs, although they do check the fluency of MT outputs. Second, they do not take the statistical behavior of assigned scores into consideration. The proposed approach scores MT outputs by using not only the language but also a translation model (TM). To conduct a s"
2004.iwslt-evaluation.2,hogan-frederking-1998-evaluation,0,0.0713962,"e drastic reduction in mWER has been demonstrated (Table 6). However, the quality with the small corpus is not so bad in the subjective evaluation shown in Table 7. We conjecture that adequacy is not low even with the supplied corpus, and the translation become similar to native English, that is, its fluency improves as the size of corpus increases. 2.4. Results 2.4.1. Selecting Effect 2.5. Discussion As shown in Table 4, all of the metrics taken together show that the proposed selector outperforms both element transRelated works have proposed ways to merge MT outputs from multiple MT systems [12] in order to output better translations. When the source language and the target language have similar sentence structures, this merging apGood: easy to understand, with either some unimportant information missing or flawed grammar; (C) Fair: broken, but understandable with effort; (D) Unacceptable: important information has been translated incorrectly. 15 translations with additional constraints [17, 18, 19]:  P (¯fi |¯ ea i ) P (f |e) ≈ Table 7: ATR’s Overall Subjective Evaluation - IWSLT supplied corpus. S S,A S,A,B S,A,B,C D D3 34.80 47.40 62.60 73.40 26.60 HPAT 25.20 44.20 70.40 80.40 19"
2004.iwslt-evaluation.2,P01-1050,0,0.0620593,"age and the target language have different sentence structures, such as English and Japanese, we often have translations whose structures are different from each other for a single input sentences. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sent"
2004.iwslt-evaluation.2,P01-1030,0,0.0810071,"nces. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihoo"
2004.iwslt-evaluation.2,2003.mtsummit-papers.54,1,0.856638,"s regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihood of the phrase-segmen"
2004.iwslt-evaluation.2,N03-1017,0,0.00448749,"ndling long Japanese input. The latter was attributed to the fact that we tuned our parameter to mWER and we exploited phrase models as well. Table 8: Evaluation - IWSLT Chinese-to-English supplied task. System Top Our Bottom (15) where count(¯ e, f¯) is the cooccurrence frequency of the two phrases e¯ and f¯. The basic idea of Equation 15 is to capture the bilingual correspondence while considering two directions. Additional phrases were exhaustively induced based on the intersection/union of the viterbi word alignments of the two directional models, P (e|f ) and P (f |e), computed by GIZA++ [17]. After the extraction of phrase translation pairs, their monolingual phrase lexicons were extracted and used as the possible segmentation for the source and target sentences. mWER 45.59 46.99 61.69 Fluency 38.20 38.20 25.04 Adequacy 33.38 29.50 29.06 4. Other Features of C3 This section introduces another feature of C3: paraphrasing and filtering corpora, which are not used in the IWSLT04 task but are useful for boosting MT performance. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. Specifically, theis variety makes it m"
2004.iwslt-evaluation.2,2003.mtsummit-papers.53,0,0.0727491,"Missing"
2004.iwslt-evaluation.2,W03-1001,0,0.0212046,"Missing"
2004.iwslt-evaluation.2,W04-3216,0,0.0140311,"regarded as the distortion tion 5, the term P (f |¯f , e probability of how a phrase segmented sentence ¯f will be reordered to form the source sentence f . Instead, we model this as the likelihood of a particular phrase segment ¯fj observed in f : ¯, e) ∝ P (f |¯f , e P (¯f |f )  P (¯fj |f ) ≈ Equation 12 can be regarded as a Hidden Markov Model in ¯ j in the lattice F ¯ is treated as an which each source phrase F ¯ observation emitted from a state Ei , a target phrase, in the ¯ as shown in Figure 2. lattice E, (8) (9) j The use of the phrase-based HMM structure has already been proposed in [20] in the context of aligning documents and abstracts. In their approach, jump probabilities were explicitly encoded as the state transitions that roughly corresponded to the alignment probabilities in the context of the word-based statistical translation model. The use of the explicit jump or alignment probabilities served for the completeness of the translation modeling at the cost of the enormous search space needed to train the phrase-based HMM structure. The segmentation model is realized as the unigram posterior probability of the phrase ngram model presented in Section 3.1. To briefly sum"
2004.iwslt-evaluation.2,J03-1002,0,0.00549884,"rocedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (14) To overcome the problem of local convergence often observed in the EM algorithm [21], we use the lexicon model from the GIZA++ [22] training as the initial parameters for the phrase translation model. In addition, the phrase ngram model and the phrase segmentation models are individually trained over the monolingual corpus and remained fixed during the HMM iterations. 3.8. Results The results appear strange in two points: (1) Our proposal didn’t work well for the Japanese-to-English track but did work well for the Chinese-to-English track; (2) Our proposal achieved high fluency but marked low adequacy. 3.6. Phrase Segment Induction Equations 13 and 14 involve summation over all possible contexts, either in its left-hand-s"
2004.iwslt-evaluation.2,P02-1038,0,0.0308911,"e segmentation model, and the phrase translation model – Equation 4 can be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure ge"
2004.iwslt-evaluation.2,P03-1021,0,0.00794031,"an be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input"
2004.iwslt-evaluation.2,W02-1021,0,0.0253988,"λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (1"
2004.iwslt-evaluation.2,shimohata-sumita-2002-automatic,1,0.850547,"xtract good transfer patterns for HPAT, and to estimate the parameters for SMT. 3.7. Decoder The decision rule to compute the best translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created o"
2004.iwslt-evaluation.2,2002.tmi-tutorials.2,0,0.02481,"st translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created on the normalized target sentences had a reduced word-error rate [31]. Finch et al. [32] demonstrated that the expansion of refer"
2004.iwslt-evaluation.2,C04-1017,1,0.875016,"Missing"
2004.iwslt-evaluation.2,E03-1029,1,0.895696,"Missing"
2004.iwslt-evaluation.2,W02-1611,1,\N,Missing
2004.iwslt-evaluation.2,watanabe-etal-2002-statistical,1,\N,Missing
2005.iwslt-1.5,J90-2002,0,0.320474,"aining data conditions showed the potential of the proposed hybrid approach and revealed new directions in how to improve the current system performance. 1. Introduction Corpus-based approaches to machine translation (MT) have achieved much progress over the last decades. There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. Despite a high performance on average, these approaches can often produce translations with severe errors. However, different MT engines not always do the same error. Due to the particular output characteristics of each MT engine, quite different translation hypotheses are produced. Thus, combining multiple MT systems can boost the system performance by exploiting the strengths of each MT engine. W"
2005.iwslt-1.5,2003.mtsummit-papers.54,1,0.79351,"-based SMT engine [MSEP] (cf. Section 2.1.3), an SMT engine based on syntactic transfer [HPATR2] (cf. Section 2.1.4), an EBMT engine that incorporates word-level SMT methods [HPATR] (cf. Section 2.1.5), an EBMT engine based on hierarchical phrase alignments [HPAT] (cf. Section 2.1.6), an DP-match-driven EBMT engine [D3 ] (cf. Section 2.1.7), and a translation memory system [EM] (cf. Section 2.1.8). The translation knowledge of the eight MT systems is automatically acquired from a parallel corpus. The characteristics of the element MTs are summarized in Table 1. 2.1.1. SAT SAT is an SMT system [3]. The decoder searches for an optimal translation by using SMT models starting from a decoder seed, i.e., the source language input paired with an initial translation hypothesis. In SAT, the search is initiated from Preprocessing Translation Tagger Selection MT1 Hypothesis MTm Hypothesis 1 Input SELECTOR Chunker Parser Resources m TM LM 1 Output TM LM n Statistical Models (LM, TM) Thesaurus Dictionary Corpus (monolingual) Corpus (bilingual) Figure 1: System outline Table 1: Features of element MT engines U nit Coverage Quality Speed Resources SAT SMT PBHMTM sentence&word wide excellent modest"
2005.iwslt-1.5,P01-1030,0,0.0372404,"1: Features of element MT engines U nit Coverage Quality Speed Resources SAT SMT PBHMTM sentence&word wide excellent modest corpus phrase wide good slow corpus MSEP HPATR2 HPATR phrase wide good slow corpus, chunker phrase wide good modest corpus, parser phrase wide good modest corpus, parser similar translation examples retrieved from a parallel corpus. The similarity measure used here is a combination of an editdistance and tf/idf criteria as seen in the information retrieval framework [4]. The retrieved translations are modified by using a greedy search approach to find better translations [5]. 2.1.2. PBHMTM PBHMTM is a statistical MT system that is based on a phrase-based HMM translation model [6]. The model directly structures the phrase-based SMT approach in a Hidden Markov structure. The probability P (f |e) of translating a foreign source sentence f into a target language sentence e using noisy channel modeling is approximated by introduc¯, to explicitly capture ing two new hidden variables, ¯f and e the phrase translation relationship: X P(f |¯ f, ¯ e, e)P(¯ f |¯ e, e)P(¯ e|e) (1) P (f |e) = ¯ f ,¯ e The first term represents the probability that a phrasesegmented source lang"
2005.iwslt-1.5,2004.iwslt-evaluation.2,1,0.800582,"excellent modest corpus phrase wide good slow corpus MSEP HPATR2 HPATR phrase wide good slow corpus, chunker phrase wide good modest corpus, parser phrase wide good modest corpus, parser similar translation examples retrieved from a parallel corpus. The similarity measure used here is a combination of an editdistance and tf/idf criteria as seen in the information retrieval framework [4]. The retrieved translations are modified by using a greedy search approach to find better translations [5]. 2.1.2. PBHMTM PBHMTM is a statistical MT system that is based on a phrase-based HMM translation model [6]. The model directly structures the phrase-based SMT approach in a Hidden Markov structure. The probability P (f |e) of translating a foreign source sentence f into a target language sentence e using noisy channel modeling is approximated by introduc¯, to explicitly capture ing two new hidden variables, ¯f and e the phrase translation relationship: X P(f |¯ f, ¯ e, e)P(¯ f |¯ e, e)P(¯ e|e) (1) P (f |e) = ¯ f ,¯ e The first term represents the probability that a phrasesegmented source language sentence ¯f can be reordered and generated as the source text of f (Phrase Segmentation Model). The se"
2005.iwslt-1.5,W02-1021,0,0.0245308,"nted target language sentence e (Phrase Ngram Model). ¯ and ¯f are expanded If the phrase segmented sentences e ¯ ¯ then the apinto corresponding lattice structures E and F, proximation of the proposed models can be regarded as a Hidden Markov Model in which each source phrase in the EBMT HPAT D3 phrase wide good fast corpus, parser, thesaurus sentence narrow excellent fast corpus, thesaurus, bilingual dictionary EM sentence narrow excellent fast corpus ¯ is treated as an observation emitted from a state, a lattice F ¯ target phrase, in the lattice E. The decoder is a word-graph-based decoder [7], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the word-based trigram language model and the class 5-gram model. The second pass uses the A* strategy to search for the best path for translation on the generated word-graph. 2.1.3. MSEP MSEP is a phrase-based SMT system that utilizes morphosyntactic informat"
2005.iwslt-1.5,2005.mtsummit-papers.35,1,0.73862,"p1 ), we incorporate the following features into the loglinear translation model: • Class-based n-gram Q model: P r(eI1 ) = i Pr(ei |ci )P r(ci |ci−1 1 ) • Length model: P r(l|eI1 , f1J ), whereby l is the length (number of words) of a translated target sentence. • Phrase matching score: The translated target sentence is matched with phrase translation examples that are extracted from a parallel corpus based on bidirectional word alignment of phrase translation pairs. A score is derived based on the number of matches. 2.1.4. HPATR2 HPATR2 is a statistical MT system based on syntactic transfer [9]. The translation model of HPATR2 is defined as an inside probability of two parse trees, which is used to create probabilistic context-free grammar rules. The system searches for the best translation that maximizes the product of the following probabilities, where F, E are a source and a target parse trees, and θ, π are context-free grammar rules of the source and the target language, respectively. • Probability of Source Tree Model Y P (F) = P (θ) (3) θ:θ∈F • Probability of Target Tree Model Y P (E) = P (π) 2.1.6. HPAT HPAT is an example-based MT system based on syntactic transfer [11]. The"
2005.iwslt-1.5,C04-1015,1,0.7436,"π:π∈E A characteristic of HPATR2 is that not only word translations but also the translation of multi-word sequences is carried out by the syntactic transfer. Parsing hypotheses, which are multi-word sequences connected by context-free grammar rules, are created. The best hypothesis (parse tree and translation) is selected based on the above models. Therefore, HPATR2 is an MT system that contains features of phrase-based SMT as well as syntax-based SMT. 2.1.5. HPATR HPATR is an extension of the example-based HPAT system (cf. Section 2.1.6) that incorporates a word-based statistical MT system [10]. Similar to HPAT, an EBMT module based on syntactic transfer is used to generate translation candidates that have minimum semantic distances. However, word D3 (DP-match Driven transDucer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic d"
2005.iwslt-1.5,2002.tmi-papers.9,1,0.840155,"ansfer [9]. The translation model of HPATR2 is defined as an inside probability of two parse trees, which is used to create probabilistic context-free grammar rules. The system searches for the best translation that maximizes the product of the following probabilities, where F, E are a source and a target parse trees, and θ, π are context-free grammar rules of the source and the target language, respectively. • Probability of Source Tree Model Y P (F) = P (θ) (3) θ:θ∈F • Probability of Target Tree Model Y P (E) = P (π) 2.1.6. HPAT HPAT is an example-based MT system based on syntactic transfer [11]. The most important knowledge in HPAT are transfer rules, which define the correspondences between source and target patterns. The transfer rules can be regarded as synchronized context-free grammar rules. When the system translates an input sentence, the sentence is first parsed by using the source side of the transfer rules. Next, a tree structure of the target language is generated by mapping the source grammar rules to the corresponding target rules. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which"
2005.iwslt-1.5,P03-1057,1,0.847084,"the target language is generated by mapping the source grammar rules to the corresponding target rules. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which occur during parsing or mapping, are resolved by selecting the rules that minimize the semantic distance between the input words and source examples of the transfer rules. In general, the automatic acquisition process generates many redundant rules. To avoid this problem, HPAT optimizes the transfer rules by removing redundant rules (feedback cleaning, [12]) in order to increase an automatic evaluation score. 2.1.7. D3 (4) π:π∈E • Probability of Tree-mapping Model Y P (F|E)P (E|F) = P (θ|π)P (π|θ) selection is not performed during transfer, but all possible word translation candidates are generated. In a second step, an SMT module using a lexicon model and an n-gram language model is exploited to search for the best translation that maximizes the product of the probabilities. Therefore, HPATR selects the best translation among the output of example-based MT using models of statistical MT from the viewpoints of adequacy of word translation and fl"
2005.iwslt-1.5,W01-1401,1,0.85314,"ted. The best hypothesis (parse tree and translation) is selected based on the above models. Therefore, HPATR2 is an MT system that contains features of phrase-based SMT as well as syntax-based SMT. 2.1.5. HPATR HPATR is an extension of the example-based HPAT system (cf. Section 2.1.6) that incorporates a word-based statistical MT system [10]. Similar to HPAT, an EBMT module based on syntactic transfer is used to generate translation candidates that have minimum semantic distances. However, word D3 (DP-match Driven transDucer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic distance between two substituted words and is defined as the division of K, the level of the least common abstraction in the thesaurus of two words, by N, the height of the thesaurus [14]. According to the difference between the input sentence and the retrieved"
2005.iwslt-1.5,P91-1024,1,0.660049,"Ducer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic distance between two substituted words and is defined as the division of K, the level of the least common abstraction in the thesaurus of two words, by N, the height of the thesaurus [14]. According to the difference between the input sentence and the retrieved source sentence, the translation of the retrieved source sentence is modified by using dictionaries. 2.1.8. EM EM is a translation memory system that matches a given source sentence against the source language parts of translation examples extracted from a parallel corpus. In case an exact match can be achieved, the corresponding target language sentence will be used. Otherwise, the system fails to output a translation. 2.2. Selection of the Best MT Engine Output We use an SMT-based method of automatically selecting the"
2005.iwslt-1.5,C02-1076,1,0.892551,"retrieved source sentence, the translation of the retrieved source sentence is modified by using dictionaries. 2.1.8. EM EM is a translation memory system that matches a given source sentence against the source language parts of translation examples extracted from a parallel corpus. In case an exact match can be achieved, the corresponding target language sentence will be used. Otherwise, the system fails to output a translation. 2.2. Selection of the Best MT Engine Output We use an SMT-based method of automatically selecting the best translation among outputs generated by multiple MT systems [15]. This approach scores MT outputs by using multiple language (LM) and translation model (TM) pairs trained on different subsets of the training data. It uses a statistical test to check whether the average TM·LM score of one MT output is significantly higher than those of another MT output. The SELECTOR algorithm is summarized in Figure 2. (1) proc SELECTOR( Input, Corpus, n, M T 1 , . . . , M T m ) ; (2) begin (3) (∗ initalize statistical models ∗) (4) for each i in {1, . . . , n} do (5) Corpusi ← subset(Corpus) ; (6) T Mi ← translation-model(Corpusi ) ; (7) LMi ← language-model(Corpusi ) ; ("
2005.mtsummit-ebmt.15,J93-2003,0,0.00436194,"end NULL another another hotel NULL  Lexicon Model |hotel) p( Greedy Decoding for SMT In this section, we explain the outline of SMT and greedy decoding in short. 2.1 Statistical Machine Translation Statistical machine translation formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem: argmaxT p(S|T ) ∗ p(T ), (1) where p(S|T ) is called a translation model (T M ), representing the generation probability from T into S, and p(T ) is called a language model (LM ), which represents the likelihood of the target language (Brown et al., 1993). During the translation process (decoding), a statistical score based on T M and LM is assigned to each translation. In this paper, we call this score TM·LM. The translation with the highest TM·LM score is selected as the output. We used the IBM-4 translation model (Brown et al., 1993) in the experiments in Section 4, which consists of probabilities for word translations (lexicon model), the number of source words produced by a target word (fertility model), word insertions (generation model), and word order changes (distortion model). LM is based on the frequency of consecutive word sequence"
2005.mtsummit-ebmt.15,P01-1020,0,0.0223056,"tance between the given hypotheses and those of other MT engines, whereby the edit-distance is defined as the sum of the costs of insertion, deletion, and substitution operations required to map one word sequence into the other (Wagner, 1974). – differences in the length of a given initial translation hypothesis toward the shortest/longest initial translation hypothesis. Moreover, we added also statistical features and syntactic/semantic features for the experiments described in this paper, some of which were used in previous research on the automatic evaluation of machine translation output (Corston-Oliver et al., 2001). (→ i would prefer the hyatt regency please and if possible i want a single room) (initial translation hypothesis) MT1 : i ’m asked do i want to stay to room single room MT2 : i ’ll send a hyatt i ’d like to stay in a single room MT3 : i want to stay at the single room which asks you for the hyatt regency hotel MT4 : i want to stay at a single room in which it asks for the hyatt regency hotel MT5 : i want to stay at the single room which you may ask for hyatt regency hotel with • Perplexity of the source language input and the initial translation hypothesis calculated on the basis of trigram"
2005.mtsummit-ebmt.15,P01-1030,0,0.0246424,"), and word order changes (distortion model). LM is based on the frequency of consecutive word sequences (n-gram). The T M and LM probabilities are trained automatically from a parallel text corpus. Figure 1 gives an example for the process of transferring a Japanese source sentence into an English target sentence and illustrates which translation knowledge is captured by the respective statistical models mentioned above. 2.2 Greedy Decoding Various decoding algorithms have been proposed, including stack-based (Wang and Waibel, 1997), beam search (Tillmann and Ney, 2000), and greedy decoding (Germann et al., 2001). This paper concentrates on the greedy decoding approach described in details in Section 2.2.1. The local optima problem of this approach is illustrated in Section 2.2.2. 118 x could could recommend another another hotel         Distortion Model p(src_pos=3|trg_pos=5) [source]         Language Model (LM): p(could you recommend another hotel) = p(could) p(you |could) p(recommend |could you) p(another |you recommend) p(hotel |recommend another) Figure 1: Statistical Models 2.2.1 Algorithm Figure 2 illustrates the decoding algorithm, which is described in detail in"
2005.mtsummit-ebmt.15,2002.tmi-papers.9,0,0.0373438,"Missing"
2005.mtsummit-ebmt.15,P01-1050,0,0.0195919,"a high performance on average, the greedy decoding approach can often produce translations with severe errors. A major problem of the greedy decoding approach is that the translation output depends 117 on the initial translation hypothesis to start the search, which may lead to a local optimum translation but not to the global optimum translation. Therefore, the selection of the starting point is crucial to avoid local optima in the search. Previous methods addressed this problem by creating an initial translation hypothesis based on translation examples obtained from a parallel text corpus (Marcu, 2001), (Watanabe and Sumita, 2003) or by using diverse starting points generated by multiple translation engines (Paul et al., 2004). Combining multiple MT systems has the advantage of exploiting the strengths of each MT engine. Quite different initial translation hypotheses are produced due to particular output characteristics of each MT engine. Therefore, larger parts of the search space can be explored while avoiding local optima problems of the search algorithm. This method outperforms conventional greedy decoding approaches using initial translation hypotheses based on translation examples ret"
2005.mtsummit-ebmt.15,N04-4003,1,0.783157,"roblem of the greedy decoding approach is that the translation output depends 117 on the initial translation hypothesis to start the search, which may lead to a local optimum translation but not to the global optimum translation. Therefore, the selection of the starting point is crucial to avoid local optima in the search. Previous methods addressed this problem by creating an initial translation hypothesis based on translation examples obtained from a parallel text corpus (Marcu, 2001), (Watanabe and Sumita, 2003) or by using diverse starting points generated by multiple translation engines (Paul et al., 2004). Combining multiple MT systems has the advantage of exploiting the strengths of each MT engine. Quite different initial translation hypotheses are produced due to particular output characteristics of each MT engine. Therefore, larger parts of the search space can be explored while avoiding local optima problems of the search algorithm. This method outperforms conventional greedy decoding approaches using initial translation hypotheses based on translation examples retrieved from a parallel text corpus. However, the sequential decoding of multiple decoder seeds is computationally expensive. In"
2005.mtsummit-ebmt.15,quirk-2004-training,0,0.049879,"Missing"
2005.mtsummit-ebmt.15,C92-2067,0,0.0202221,"orpus Statistics word corpus sentence lang uage tokens count J 1,114,186 BTEC 162,318 952,300 E 62,529 J MAD 4,894 57,500 E word words per types sentence 6.9 18,781 5.9 12,404 10.0 2,607 10.3 2,158 The BTEC corpus was used for the acquisition of translation knowledge (training set) and the MAD corpus was used for the training of the decision tree classifier. In addition, we used 502 sentences from the MAD corpus reserved for evaluation purposes as the test set. 4.1.2 Evaluation Metrics For the evaluation, we used the following automatic scoring measure and human assessment. • Word Error Rate (Su et al., 1992) (WER), which penalizes edit operations against reference translations.. • Translation Accuracy (Sumita et al., 1999) (ABC): subjective evaluation ranks ranging from A to D (A: perfect, B: fair, C: acceptable and D: nonsense), judged by a native speaker. Hereafter, we use the total count of translations ranked A, B, or C as the ABC score. 4 Parts of the BTEC corpus were used in the International Workshop of Spoken Language Translation (http://www.slt.atr.jp/IWSLT2004/) and will be made publicly available through GSK (http://www.gsk.or.jp). 122 In contrast to WER, higher ABC scores indicate bet"
2005.mtsummit-ebmt.15,1999.mtsummit-1.34,1,0.814857,"4 57,500 E word words per types sentence 6.9 18,781 5.9 12,404 10.0 2,607 10.3 2,158 The BTEC corpus was used for the acquisition of translation knowledge (training set) and the MAD corpus was used for the training of the decision tree classifier. In addition, we used 502 sentences from the MAD corpus reserved for evaluation purposes as the test set. 4.1.2 Evaluation Metrics For the evaluation, we used the following automatic scoring measure and human assessment. • Word Error Rate (Su et al., 1992) (WER), which penalizes edit operations against reference translations.. • Translation Accuracy (Sumita et al., 1999) (ABC): subjective evaluation ranks ranging from A to D (A: perfect, B: fair, C: acceptable and D: nonsense), judged by a native speaker. Hereafter, we use the total count of translations ranked A, B, or C as the ABC score. 4 Parts of the BTEC corpus were used in the International Workshop of Spoken Language Translation (http://www.slt.atr.jp/IWSLT2004/) and will be made publicly available through GSK (http://www.gsk.or.jp). 122 In contrast to WER, higher ABC scores indicate better translations. For the automatic scoring measure we utilized up to 16 human reference translations. 4.2 Translatio"
2005.mtsummit-ebmt.15,W01-1401,1,0.890601,"Missing"
2005.mtsummit-ebmt.15,takezawa-etal-2002-toward,1,0.829973,"ng two Japanese(J)-English(E) parallel corpora of the travel domain. 2 The translation models are trained using the GIZA++ toolkit, http://www.fjoch.com 3 The language models are trained using the CMUCambridge Statistical Language Modeling Toolkit v2, http://mi.eng.cam.ac.uk/∼prc14/toolkit.html • Basic Travel Expression Corpus (BTEC) The BTEC corpus is a large collection of sentences4 that bilingual travel experts consider useful for people going to or coming from countries with different languages. The BTEC sentences are not transcriptions of actual interactions, but were written by experts (Takezawa et al., 2002). • Machine Aided Dialogue Corpus (MAD) The MAD corpus is a collection of dialogues between a native speaker of Japanese and a native speaker of English that is mediated by a speech-to-speech translation system (Kikui et al., 2003). The statistics of the corpora are given in Table 3, where word token refers to the number of words in the corpus and word type refers to the vocabulary size. Since the MAD corpus consists of dialogues, it contains more complex and compound sentences as well as filled pauses, resulting in longer sentences that are more difficult to translate. Table 3: Corpus Statist"
2005.mtsummit-ebmt.15,C00-2123,0,0.02089,"lity model), word insertions (generation model), and word order changes (distortion model). LM is based on the frequency of consecutive word sequences (n-gram). The T M and LM probabilities are trained automatically from a parallel text corpus. Figure 1 gives an example for the process of transferring a Japanese source sentence into an English target sentence and illustrates which translation knowledge is captured by the respective statistical models mentioned above. 2.2 Greedy Decoding Various decoding algorithms have been proposed, including stack-based (Wang and Waibel, 1997), beam search (Tillmann and Ney, 2000), and greedy decoding (Germann et al., 2001). This paper concentrates on the greedy decoding approach described in details in Section 2.2.1. The local optima problem of this approach is illustrated in Section 2.2.2. 118 x could could recommend another another hotel         Distortion Model p(src_pos=3|trg_pos=5) [source]         Language Model (LM): p(could you recommend another hotel) = p(could) p(you |could) p(recommend |could you) p(another |you recommend) p(hotel |recommend another) Figure 1: Statistical Models 2.2.1 Algorithm Figure 2 illustrates the decoding"
2005.mtsummit-ebmt.15,P97-1047,0,0.0296968,"ords produced by a target word (fertility model), word insertions (generation model), and word order changes (distortion model). LM is based on the frequency of consecutive word sequences (n-gram). The T M and LM probabilities are trained automatically from a parallel text corpus. Figure 1 gives an example for the process of transferring a Japanese source sentence into an English target sentence and illustrates which translation knowledge is captured by the respective statistical models mentioned above. 2.2 Greedy Decoding Various decoding algorithms have been proposed, including stack-based (Wang and Waibel, 1997), beam search (Tillmann and Ney, 2000), and greedy decoding (Germann et al., 2001). This paper concentrates on the greedy decoding approach described in details in Section 2.2.1. The local optima problem of this approach is illustrated in Section 2.2.2. 118 x could could recommend another another hotel         Distortion Model p(src_pos=3|trg_pos=5) [source]         Language Model (LM): p(could you recommend another hotel) = p(could) p(you |could) p(recommend |could you) p(another |you recommend) p(hotel |recommend another) Figure 1: Statistical Models 2.2.1 Algor"
2005.mtsummit-ebmt.15,2003.mtsummit-papers.54,1,0.8222,"ance on average, the greedy decoding approach can often produce translations with severe errors. A major problem of the greedy decoding approach is that the translation output depends 117 on the initial translation hypothesis to start the search, which may lead to a local optimum translation but not to the global optimum translation. Therefore, the selection of the starting point is crucial to avoid local optima in the search. Previous methods addressed this problem by creating an initial translation hypothesis based on translation examples obtained from a parallel text corpus (Marcu, 2001), (Watanabe and Sumita, 2003) or by using diverse starting points generated by multiple translation engines (Paul et al., 2004). Combining multiple MT systems has the advantage of exploiting the strengths of each MT engine. Quite different initial translation hypotheses are produced due to particular output characteristics of each MT engine. Therefore, larger parts of the search space can be explored while avoiding local optima problems of the search algorithm. This method outperforms conventional greedy decoding approaches using initial translation hypotheses based on translation examples retrieved from a parallel text c"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.16,0,0.0110982,"plied word segmentations. However, resegmenting the data set proved to be effective for increasing the vocabulary coverage and improving translation quality [14]. For Japanese, the highest recognition accuracy was obtained. However, due to large differences in syntactic structure and word order, the JE translation task seems to be one of the most difficult tasks and the best performing systems obtained lower scores compared to the AE and IE results. Interestingly, the JE task featured the largest number of nonSMT engines including a commercial system that achieved quite good performances (see [24, 17]). For Chinese, the recognition accuracy for read speech is similar to the Arabic recognition results, but the automatic evaluation scores obtained for the top-scoring MT engines are much lower. The complexity of the CE translation task seems to be similar to JE. Altogether, the complexity14 of the translation tasks of this year’s IWSLT evaluation campaign can be summarized as: 6. Acknowledgments I thank the C-STAR partners for their accomplishments during the preparation of this workshop and the subjective evaluation task. In particular, I would like to thank Roldano Cattoni, Roger Hsiao, Gen"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.20,0,0.0410896,"Missing"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.21,0,0.0201369,"data, the same set of English reference translations were used for the evaluation of all translations outputs. Therefore, the translation results of MT engines using different source languages as the input can be directly compared. 7 5. Conclusion Looking at the automatic evaluation results of the Open Data Track, the highest scores were obtained on the IE transThis year’s workshop provided a testbed for applying novel ideas on how to deal with problems in the area of spontaneous speech translation. Various innovative ideas were explored, most notably the usage of out-of-domain training data [14, 29], new methods for distortion modeling [15, 26], topic-dependent model adaptation [20, 23], efficient decoding of word lattices [16], and rescoring/reranking methods of NBEST list [22, 23, 29]. Although not all ideas proved to be effective, new insights into the complexity of combining speech recognition and machine translation technologies were obtained that will help to advance the current state-ofthe-art in speech translation. lation task for the CRR and the ASR output translation conditions. The latter was surprising given Italian had the worst recognition accuracies. One reason might be th"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.22,0,0.069962,"Missing"
2006.iwslt-evaluation.1,W05-0909,0,0.0439056,"n task. A total of 38,198 grading operations were performed. Table 8: Automatic evaluation metrics BLEU4: the geometric mean of n-gram precision by the system output with respect to reference translations. Scores range between 0 (worst) and 1 (best) [6] NIST: a variant of BLEU4 using the arithmetic mean of weighted n-gram precision values. Scores are positive with 0 being the worst possible [7] METEOR: calculates unigram overlaps between a translations and reference texts using various levels of matches (exact, stem, synonym) are taken into account. Scores range between 0 (worst) and 1 (best) [8] 3. Evaluation Results The evaluation results of the IWSLT 2006 workshop are summarized in Appendix B (human assessment) and Appendix C (automatic evaluation). For each translation condition/evaluation metric, the best score is marked in bold-face. Based on the obtained evaluation results, the respective MT engines were ranked. In order to decide whether the translation output of one MT engine is significantly better than another one, we used the bootStrap12 method that (1) performs a random sampling with replacement from the eval data set, (2) calculates the respective evaluation metric score"
2006.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,0.0193586,"Missing"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.2,0,0.056287,"Missing"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.3,0,0.0547443,"Missing"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.4,0,0.0581553,"Missing"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.5,0,0.0644358,"Missing"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.6,0,0.0184441,"data, the same set of English reference translations were used for the evaluation of all translations outputs. Therefore, the translation results of MT engines using different source languages as the input can be directly compared. 7 5. Conclusion Looking at the automatic evaluation results of the Open Data Track, the highest scores were obtained on the IE transThis year’s workshop provided a testbed for applying novel ideas on how to deal with problems in the area of spontaneous speech translation. Various innovative ideas were explored, most notably the usage of out-of-domain training data [14, 29], new methods for distortion modeling [15, 26], topic-dependent model adaptation [20, 23], efficient decoding of word lattices [16], and rescoring/reranking methods of NBEST list [22, 23, 29]. Although not all ideas proved to be effective, new insights into the complexity of combining speech recognition and machine translation technologies were obtained that will help to advance the current state-ofthe-art in speech translation. lation task for the CRR and the ASR output translation conditions. The latter was surprising given Italian had the worst recognition accuracies. One reason might be th"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.9,0,0.0211685,"plied word segmentations. However, resegmenting the data set proved to be effective for increasing the vocabulary coverage and improving translation quality [14]. For Japanese, the highest recognition accuracy was obtained. However, due to large differences in syntactic structure and word order, the JE translation task seems to be one of the most difficult tasks and the best performing systems obtained lower scores compared to the AE and IE results. Interestingly, the JE task featured the largest number of nonSMT engines including a commercial system that achieved quite good performances (see [24, 17]). For Chinese, the recognition accuracy for read speech is similar to the Arabic recognition results, but the automatic evaluation scores obtained for the top-scoring MT engines are much lower. The complexity of the CE translation task seems to be similar to JE. Altogether, the complexity14 of the translation tasks of this year’s IWSLT evaluation campaign can be summarized as: 6. Acknowledgments I thank the C-STAR partners for their accomplishments during the preparation of this workshop and the subjective evaluation task. In particular, I would like to thank Roldano Cattoni, Roger Hsiao, Gen"
2006.iwslt-evaluation.1,2006.iwslt-evaluation.11,0,0.0573763,"Missing"
2006.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,\N,Missing
2006.iwslt-evaluation.1,2006.iwslt-evaluation.7,0,\N,Missing
2006.iwslt-evaluation.1,2006.iwslt-evaluation.17,0,\N,Missing
2006.iwslt-evaluation.1,P02-1040,0,\N,Missing
2006.iwslt-evaluation.1,2006.iwslt-evaluation.18,0,\N,Missing
2006.iwslt-evaluation.1,2006.iwslt-evaluation.13,0,\N,Missing
2006.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2006.iwslt-evaluation.1,2004.iwslt-evaluation.1,1,\N,Missing
2006.iwslt-evaluation.1,2006.iwslt-evaluation.14,0,\N,Missing
2006.iwslt-evaluation.1,2005.iwslt-1.6,0,\N,Missing
2006.iwslt-evaluation.1,2006.iwslt-evaluation.12,1,\N,Missing
2006.iwslt-evaluation.1,2006.iwslt-evaluation.15,0,\N,Missing
2006.iwslt-evaluation.1,P03-1021,0,\N,Missing
2006.iwslt-evaluation.1,2006.iwslt-evaluation.8,0,\N,Missing
2006.iwslt-evaluation.1,2006.iwslt-evaluation.10,0,\N,Missing
2006.iwslt-evaluation.12,C02-1076,1,0.827325,"ntactic transfer; and EM by exact match. For the OPEN track, only TATR was used; for the CSTAR track, a hybrid system using three engines and Selector was used. See Figure 1. 5.4. Selector 5.1. TATR In order to select the best translation among outputs generated by multiple MT systems, we employ an SMT-based method that scores MT outputs by using multiple language (LM) and translation model (TM) pairs trained on different subsets of the training data. It uses a statistical test to check whether the obtained TM·LM scores of one MT output are significantly higher than those of another MT output [9]. Given an input sentence, m translation hypotheses are produced by the component MT engines (m = 1 for this evaluation), whereby n different TM·LM scores are assigned to each hypothesis. In order to check whether the highest scoring hypothesis is significantly better then the other MT outputs, a multiple comparison test based on the Kruskal-Wallis test is used [10]. If one of the MT outputs is significantly better, this output is selected. Otherwise, the output of the MT engine that performs best on a development set is selected. TATR is a phrase-based SMT system built within the framework of"
2006.iwslt-evaluation.12,J03-1002,0,0.0511227,"Missing"
2006.iwslt-evaluation.12,2006.iwslt-evaluation.15,0,0.0505268,"Missing"
2006.iwslt-evaluation.12,2005.mtsummit-papers.35,1,0.807212,"Missing"
2006.iwslt-evaluation.12,2005.iwslt-1.5,1,0.8833,"Missing"
2006.iwslt-evaluation.12,N06-2049,1,0.870022,"ation. Our main translation engine for this year’s IWSLT evaluation, TATR, is also a phrase-based SMT. The hybrid multiple engine approach, that was used last year [1], was used again this year. But we replaced the 2005 SMTs (PBHMTM and SAT) with TATR, partly for simplification reasons. In addition to TATR, two other engines are included in this year’s hybrid system: HPATR3, a SMT based on syntactic transfer; and EM, the translation memory based on exact match. We employed new approaches for pre-processing, postprocessing, and language modeling. We used subword-based Chinese word segmentation [2]. This word segmentation achieved the highest F-score rate for the second Sighan test data, and can recognize numerical expressions and foreign names. We built a conversion model to implement capitalization and punctuation by using the maximum entropy principle and the conditional random field (CRF) approach, which can integrate long-range features to enhance performance. We applied sentence-splitting techniques to all languages. This approach significantly improved CE and JE translation. 2. Preprocessing 2.1. Arabic segmentation Of the released data, we threw away all end-of-utterance markers"
2006.iwslt-evaluation.12,N06-2013,0,\N,Missing
2007.iwslt-1.15,N06-2049,1,0.896044,"Missing"
2007.iwslt-1.15,W06-1626,0,0.0485171,"/MX is/AL CEO/AU of/AL a/AL British/IU company/AL. The CRF tagging model is expressed by the following equation: 1 http://www.speech.sri.com/projects/srilm 6. Hit-rate-based Skip n-gram Rescoring This section describes the re-scoring of the statistical MT decoder hypotheses based on skip n-gram counts extracted from a large-scale corpus consisting of collections of webpages. In order to handle very large amounts of training data to build language models, recent research focuses on distributed language modeling that use a two-pass approach to store corpora in suffix arrays and serve raw counts [6, 7] or a single-pass approach that provides smoothed probabilities 2 http://www.chasen.org/˜taku/software/CRF++ using simple smoothing techniques [8]. Although such approaches are to be preferred when available, the computational and hardware requirements are still immense and not always practicable. In order to make use of very large training corpora with fewer resources, we use a method based on n-gram occurrence counts . The hit-rate of a word sequence is defined to be: X HitRate(w1L ) = δ(wij ) (2) i,j;i<j δ(wij ) =  1 0 : f (wij ) &gt; 0 : f (wij ) = 0 The hit-rate counts can be easily calcula"
2007.iwslt-1.15,D07-1054,1,0.871459,"e in the additional corpus (Tanaka corpus, Yomiuri News corpus, SLDB corpus, and Beijing Olympic corpus included in ChineseLDC) was calculated 3. Only those sentences for which the perplexity was lower than 100 were used as training sentences. After the selection process we were left with 40K sentences from the supplied corpus and 117K additional sentences from the external corpora, giving us a total of 157K sentences for training. 2.4. Modeling Issues Word-trigram language models were used, these were smoothed using Knesser-Ney discounting. In addition, topicdependent models were constructed [1]. We built bilingual cluster-based models from 157K bilingual training sentence pairs. The sentence pairs were clustered into 10 sub-corpora. These sub-corpora intutitively represent sub-domains of the main corpus. The motivation behind this strategy was to build models specific to these sub-domains and then predict the sub-domain of the text to be translated, and use the appropriate model for the translation process. A strong improvement was demonstrated using this technique for all language pairs in the IWSLT06 evaluation campaign. In this year’s campaign we only apply this technique to the"
2007.iwslt-1.15,P07-2046,1,0.884488,"a method for re-segmenting the tokens in the confusion network. 3. Chinese-English 3.1. Corpora We used the supplied corpus in combination with the Beijing Olympic Corpus, and other corpora provided by the LDC. These corpora and their respective sizes are shown in Table 1. 3.2. Lemmatization Data sparseness is one of the key factors that degrade statistical machine translation (SMT). Especially for a translation task like IWSLT, where collecting a large amount of indomain data is very expensive. One method to reduce the translation degradation caused by this approach is by using lemmatization [2]. Lemmatization is shallow morphological analysis, which uses single a lexical entry to replace a whole range of derived inflected words. For example, the three words: “doing”, “did” and “done”, can be replaced by one word: “do”. In fact, they should all be mapped to the same Chinese target word during alignment. It is easy to see that as a result, the process reduces the number of types observed in the data, thereby easing the problems associate with sparse data, and in Chinese at least we expect the process to preserve as much of the semantic information as possible. We used Moses to impleme"
2007.iwslt-1.15,W07-0717,0,0.0516508,"he only difference in training with lemmatization from that without is the alignment factor. The former uses Chinese surface words and English lemmas as the alignment factor, but the latter uses Chinese surface words and English surface words. Therefore, the lemmatized English is only used in the word alignment stage of the training. All the other aspects of the training process are the same for both the lemmatized translation training and nonlemmatized training. 3.3. Translation model combination Linear interpolation of translation models has been shown to be effective in machine translation [2, 3]. In this campaign we apply this approach as the main means of integrating models built from the external resources with the primary models built from the supplied corpus. More formally, we use the following equation for model combination: p(e|f ) = α1 p1 (e|f ) + α2 p2 (e|f ) where p1 and p2 are two models to be integrated, and the weight α1 and α2 must sum to unity. We did not use automatic optimization methods to select the α1 and α2 . Instead, we hand-selected the values by evaluating the performance of multiple runs on the development data. We consider this approach reasonable since the s"
2007.iwslt-1.15,D07-1090,0,0.0387915,"/srilm 6. Hit-rate-based Skip n-gram Rescoring This section describes the re-scoring of the statistical MT decoder hypotheses based on skip n-gram counts extracted from a large-scale corpus consisting of collections of webpages. In order to handle very large amounts of training data to build language models, recent research focuses on distributed language modeling that use a two-pass approach to store corpora in suffix arrays and serve raw counts [6, 7] or a single-pass approach that provides smoothed probabilities 2 http://www.chasen.org/˜taku/software/CRF++ using simple smoothing techniques [8]. Although such approaches are to be preferred when available, the computational and hardware requirements are still immense and not always practicable. In order to make use of very large training corpora with fewer resources, we use a method based on n-gram occurrence counts . The hit-rate of a word sequence is defined to be: X HitRate(w1L ) = δ(wij ) (2) i,j;i<j δ(wij ) =  1 0 : f (wij ) &gt; 0 : f (wij ) = 0 The hit-rate counts can be easily calculated, even for very large training corpora like the Web-Corpus introduced in Section 6.1. For the IWSLT experiments, we calculated the hitrate feat"
2007.iwslt-1.15,W08-0334,1,\N,Missing
2007.iwslt-1.15,P02-1040,0,\N,Missing
2007.iwslt-1.15,N07-1061,0,\N,Missing
2007.iwslt-1.15,W05-0909,0,\N,Missing
2007.iwslt-1.15,P07-2045,0,\N,Missing
2007.iwslt-1.15,J03-1002,0,\N,Missing
2007.iwslt-1.15,W08-0335,1,\N,Missing
2007.iwslt-1.15,2006.iwslt-evaluation.12,1,\N,Missing
2007.iwslt-1.15,I05-3017,0,\N,Missing
2007.iwslt-1.15,P03-1021,0,\N,Missing
2007.tmi-papers.19,2001.mtsummit-papers.3,1,0.893267,"ers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various learning approaches for the multiclass classification problem for a very small data set in the domain of technical documentation. (Akiba et al., 2001) utilized DT classifiers trained on multiple edit-distance features where combinations of lexical (stem, word, part-of-speech) and semantic (thesaususbased semantic class) matches were used to compare MT system outputs with reference translations and to approximate human scores of acceptability directly. (Kulesza and Shieber, 2004) trained a binary SVM classifier based on automatic scoring features in order to distinguish between “human-produced” and “machine-generated” translations of newswire data instead of predicting human judgments directly. The approach proposed in this paper also utiliz"
2007.tmi-papers.19,W05-0909,0,0.0288594,"cceptable Translation Nonsense Table 2: Automatic Evaluation Metrics BLEU: NIST: METEOR: GTM: WER: PER: TER: the geometric mean of n-gram precision of the system output with respect to reference translations. Scores range between 0 (worst) and 1 (best) (Papineni et al., 2002) a variant of BLEU using the arithmetic mean of weighted n-gram precision values. Scores are positive with 0 being the worst possible (Doddington, 2002) calculates unigram overlaps between a translation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discrimina"
2007.tmi-papers.19,2004.tmi-1.8,0,0.153171,"automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various learning approaches for the multiclass classification problem for a very small data set in the domain of technical documentation. (Akiba et al., 2001) utilized DT classifiers trained on multiple edit-distance features where combinations of lexical (stem, word, part-of-speech) and semantic (thesaususbased semantic class) matches were used to compare MT system outputs with reference translations and to approximate human scores of acceptability directly. (Kulesza and Shieber, 2004) trained a binary SVM classifier based on automatic scoring features in order to distinguish between “human-produced” and “machine-generated” translations of newswire data instead of predicting human judgments directly. The approach proposed in this paper also utilizes a supervised learning method to predict human assessments of translation quality, but differs in the following two aspects: (1) Reduction of Classification Perplexity: The decomposition of a multiclass classification task into a set of binary classification problems reduces the complexity of the learning task resulting in higher"
2007.tmi-papers.19,N07-1006,0,0.115946,"Missing"
2007.tmi-papers.19,niessen-etal-2000-evaluation,0,0.0624038,"tive with 0 being the worst possible (Doddington, 2002) calculates unigram overlaps between a translation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binar"
2007.tmi-papers.19,2001.mtsummit-papers.46,0,0.0107964,"anslation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguis"
2007.tmi-papers.19,P02-1040,0,0.0751806,"Missing"
2007.tmi-papers.19,quirk-2004-training,0,0.0918346,"system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various learning approaches for the multiclass classification problem for a very small data set in the domain of technical documentation. (Akiba et al., 2001) utilized DT classifiers trained on multiple edit-distance features where combinations of lexical (stem, w"
2007.tmi-papers.19,2006.amta-papers.25,0,0.0596957,"cores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various"
2007.tmi-papers.19,1999.mtsummit-1.34,1,0.721734,"uality of a translation have been proposed. In this paper, human assessments of translation quality with respect to the fluency, the adequacy and the acceptability of the translation are investigated. Fluency indicates how natural the evaluation segment sounds to a native speaker of English. For adequacy, the evaluator was presented with the source language input as well as a “gold standard” translation and has to judge how much of the information from the original translation is expressed in the translation (White et al., 1994). Acceptability judges how easy-to-understand the translation is (Sumita et al., 1999). The fluency, adequacy and acceptability judgments consist of one of the grades listed in Table 1. The high cost of such human evaluation metrics has triggered a huge interest in the development of automatic evaluation metrics for machine translation. Table 2 introduces some metrics that are widely used in the MT research community. 3 Prediction of Human Assessments Most of the previously proposed approaches to predict human assessments of translation quality utilize supervised learning methods like decision trees (DT), support vector ma5 4 3 2 1 acceptability Perfect Translation Good Transla"
2007.tmi-papers.19,2003.mtsummit-papers.51,0,0.0312584,"ion of the system output with respect to reference translations. Scores range between 0 (worst) and 1 (best) (Papineni et al., 2002) a variant of BLEU using the arithmetic mean of weighted n-gram precision values. Scores are positive with 0 being the worst possible (Doddington, 2002) calculates unigram overlaps between a translation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from hu"
2007.tmi-papers.19,1994.amta-1.25,0,0.116906,"Missing"
2008.iwslt-evaluation.1,1994.amta-1.25,0,0.43504,"was also not allowed. In total, 19 research groups participated in this year’s evaluation campaign. A total of 58 MT engines were built 3 http://www.slc.atr.jp/IWSLT2008/archives/2008/10/resources.html -1- Proceedings of IWSLT 2008, Hawaii - U.S.A. to cover six different data tracks. The translation quality of all primary run submissions was evaluated using automatic evaluation metrics (BLEU [5], METEOR [6]) and a subjective evaluation metric that ranks each whole sentence translation from best to worst relative to the other choices [7]. In addition, human assessments of fluency and adequacy [8] were carried out for four selected MT system outputs for each of the data tracks. Based on the evaluation results, the impact of the spontaneity aspects of speech in real situations on the ASR and MT system performance as well as the robustness of state-of-the-art MT systems against speech recognition errors were investigated. particpants to carry out additional experiments on the evaluation testset. The schedule of the evaluation campaign is summarized in Table 2. 2. IWSLT 2008 Evaluation Campaign The IWSLT 2008 evaluation campaign was carried out using a multilingual spoken language corpus."
2008.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,0.0856615,"Missing"
2008.iwslt-evaluation.1,P03-1021,0,0.0544683,"Missing"
2008.iwslt-evaluation.1,2006.amta-papers.25,0,0.097732,"Missing"
2008.iwslt-evaluation.1,2003.mtsummit-papers.51,0,0.189854,"Missing"
2008.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,0.0986538,"Missing"
2008.iwslt-evaluation.1,2008.iwslt-evaluation.19,0,0.0439359,"Missing"
2008.iwslt-evaluation.1,2008.iwslt-evaluation.8,0,0.053344,"Missing"
2008.iwslt-evaluation.1,2008.iwslt-evaluation.9,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.3,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.17,0,\N,Missing
2008.iwslt-evaluation.1,2007.iwslt-1.1,0,\N,Missing
2008.iwslt-evaluation.1,P02-1040,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.18,0,\N,Missing
2008.iwslt-evaluation.1,W09-0410,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.6,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.14,0,\N,Missing
2008.iwslt-evaluation.1,W05-0909,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.11,1,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.12,0,\N,Missing
2008.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2008.iwslt-evaluation.1,2006.iwslt-evaluation.4,0,\N,Missing
2008.iwslt-evaluation.1,2004.iwslt-evaluation.1,1,\N,Missing
2008.iwslt-evaluation.1,W07-0718,0,\N,Missing
2008.iwslt-evaluation.1,2005.iwslt-1.16,0,\N,Missing
2008.iwslt-evaluation.1,2005.iwslt-1.15,0,\N,Missing
2008.iwslt-evaluation.1,2006.iwslt-evaluation.1,1,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.16,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.5,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.2,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.7,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.13,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.4,0,\N,Missing
2008.iwslt-evaluation.1,2008.iwslt-evaluation.10,0,\N,Missing
2008.iwslt-evaluation.11,W08-0334,1,0.872755,"ation task, we integrated two strategies for pivot translation by linear interpolation. 1. Introduction This paper describes the NICT/ATR SMT system used in the International Workshop on Spoken Language Translation (IWSLT) 2008 evaluation campaign. We participated in the following translation tasks: Chinese–English (Challenge Task), English–Chinese (Challenge Task), Chinese–English (BTEC Task), Chinese–Spanish (BTEC Task), and Chinese– English–Spanish (PIVOT Task). Although our theme for each task was different, our systems were based on a fairly common phrase-based machine translation system [1], which was built within the framework of a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapte"
2008.iwslt-evaluation.11,P07-2045,0,0.0149744,"e based on a fairly common phrase-based machine translation system [1], which was built within the framework of a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese tran"
2008.iwslt-evaluation.11,J03-1002,0,0.00224261,"a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various fact"
2008.iwslt-evaluation.11,P03-1021,0,0.0223499,"r can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various factors affecting English–Chinese translation. Table 1 summarizes the BLEU scores for correct recognition results (CRR). The BLEU scores [6] for “devset” are obtained with the small Challenge Task devset corpus (comprising 251 sentences). The devset corpus was also used for MERT.1 Thus, the results in Table 1 for devset were obtained from closed experiments. The results for “devset3” (506 sentences) were obtained by using the param"
2008.iwslt-evaluation.11,P02-1040,0,0.0774265,"The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various factors affecting English–Chinese translation. Table 1 summarizes the BLEU scores for correct recognition results (CRR). The BLEU scores [6] for “devset” are obtained with the small Challenge Task devset corpus (comprising 251 sentences). The devset corpus was also used for MERT.1 Thus, the results in Table 1 for devset were obtained from closed experiments. The results for “devset3” (506 sentences) were obtained by using the parameters tuned on devset (open experiments). The BLEU scores were calculated based on Chinese character n-grams. When calculating BLEU scores, we removed out-of-vocabulary (OOV) words from the machine translated text and ignored punctuation. 1 We used 3-gram language models for performing MERT and used 5gra"
2008.iwslt-evaluation.11,W08-0335,1,0.86956,"Missing"
2008.iwslt-evaluation.11,N06-2049,1,0.880518,"Missing"
2008.iwslt-evaluation.11,I05-3017,0,0.087631,"Missing"
2008.iwslt-evaluation.11,D07-1054,1,0.880592,"Missing"
2008.iwslt-evaluation.11,N07-1061,1,0.865684,"of the language X to language Y SMT system by using corpus X of the “X-E corpus” and the newly created corpus Y’. After developing the models, as described above, we remove all the phrase table entries that have OOV words on the target side of the phrase table. We will call the system developed above the X2Y’ system and the strategy PseudoCorpusY. In this system, the target side of the phrase table is not completely reliable. For training these systems, we develop and use a language model using corpus Y of the “Y-E corpus.” 4.4. Phrase Table Composition This strategy was introduced by Utiyama [12]. In order to implement this strategy, we first develop the X2E system using the “X-E corpus” and the E2Y system using the “Y-E corpus.” Then, we compose a new phrase table from the phrase tables of the X2E and E2Y systems. For the purpose of integrating two models, we extend this strategy to include the lexicalized reordering model. 4.5. Linear Interpolation This strategy is used to develop new models from those described above, by linear interpolation: the phrase translation model and the lexicalized reordering model. First, we interpolate two PseudoCorpus models. These models are de- 82 - n"
2008.iwslt-evaluation.11,W05-0909,0,0.115895,"Missing"
2008.iwslt-papers.2,P05-1074,0,0.0857514,"ons o the same meaning it it can be derived from preexisting training corpora and automatically aligned with the same target training sentence in a bilingual corpora. Including generated paraphrases as additional training data gives an SMT system the ability to make a richer model and thus positively affecting model quality during evaluation. There are several areas where paraphrases can be readily introduced into standard phrase-based SMT systems: source and target sides and even during the parameter tuning phase as references. Work has been done to extract paraphrases from bilingual corpora [1] and to extract paraphrase patterns as in [2]. By pivoting on target language phrases, source phrases and potential paraphrases can be found; for extracting patterns the task extends to a generalization of phrases with slots instead Proceedings of IWSLT 2008, Hawaii - U.S.A. of words. [3] found that it is possible to translate unknown source words by paraphrasing them and then do a translation on the paraphrase. The classic example of the pivot approach from [3] follows: (2) what is more, the relevant cost dynamic is completely under control im u¨ brigen ist die diesbez¨ugliche kostenentwicklu"
2008.iwslt-papers.2,P08-1089,0,0.0117653,"rom preexisting training corpora and automatically aligned with the same target training sentence in a bilingual corpora. Including generated paraphrases as additional training data gives an SMT system the ability to make a richer model and thus positively affecting model quality during evaluation. There are several areas where paraphrases can be readily introduced into standard phrase-based SMT systems: source and target sides and even during the parameter tuning phase as references. Work has been done to extract paraphrases from bilingual corpora [1] and to extract paraphrase patterns as in [2]. By pivoting on target language phrases, source phrases and potential paraphrases can be found; for extracting patterns the task extends to a generalization of phrases with slots instead Proceedings of IWSLT 2008, Hawaii - U.S.A. of words. [3] found that it is possible to translate unknown source words by paraphrasing them and then do a translation on the paraphrase. The classic example of the pivot approach from [3] follows: (2) what is more, the relevant cost dynamic is completely under control im u¨ brigen ist die diesbez¨ugliche kostenentwicklung v¨ollig unter kontrolle (3) wir sind es de"
2008.iwslt-papers.2,N06-1003,0,0.529709,"s positively affecting model quality during evaluation. There are several areas where paraphrases can be readily introduced into standard phrase-based SMT systems: source and target sides and even during the parameter tuning phase as references. Work has been done to extract paraphrases from bilingual corpora [1] and to extract paraphrase patterns as in [2]. By pivoting on target language phrases, source phrases and potential paraphrases can be found; for extracting patterns the task extends to a generalization of phrases with slots instead Proceedings of IWSLT 2008, Hawaii - U.S.A. of words. [3] found that it is possible to translate unknown source words by paraphrasing them and then do a translation on the paraphrase. The classic example of the pivot approach from [3] follows: (2) what is more, the relevant cost dynamic is completely under control im u¨ brigen ist die diesbez¨ugliche kostenentwicklung v¨ollig unter kontrolle (3) wir sind es den steuerzahlern die kosten schuldig unter kontrolle zu haben we owe it to the taxpayers to keep the costs in check By holding the german phrase unter kontrolle as a pivot the English phrase under control can be paraphrased as in check. [2] exte"
2008.iwslt-papers.2,2006.iwslt-evaluation.11,0,0.101827,"e directly to the training corpus. As for the BLEU scores, they only ever achieve an increase of about 1 BLEU point and this is on limited corpora sizes. Some examples from [4] are as follows: (5) of members of the Irish parliament of irish parliament members of irish parliament’s members (6) action at community level community level action To the extent that paraphrasing techniques are comparable to more implicit methods of language to language manipulation we explore previous research related to reordering models where a form of paraphras- 151 - ing does occur in how translation is done. In [5] proposed a reordering model that took into account predicateargument structure in Japanese and followed a heuristic for reordering sentences in the training data as a preprocessing step. This sort of reordering, while unnatural to native speakers, is still grammatically correct and easier to align to English during model training, it is also a type of paraphrasing. [6] made use of parses of source sentences and then applied a reordering heuristic as well. [7] also discusses the use altering German word order to correspond to English word order there is also some use of annotations on verbs wi"
2008.iwslt-papers.2,P05-1066,0,0.0326932,"ng techniques are comparable to more implicit methods of language to language manipulation we explore previous research related to reordering models where a form of paraphras- 151 - ing does occur in how translation is done. In [5] proposed a reordering model that took into account predicateargument structure in Japanese and followed a heuristic for reordering sentences in the training data as a preprocessing step. This sort of reordering, while unnatural to native speakers, is still grammatically correct and easier to align to English during model training, it is also a type of paraphrasing. [6] made use of parses of source sentences and then applied a reordering heuristic as well. [7] also discusses the use altering German word order to correspond to English word order there is also some use of annotations on verbs with identifying prefixes to solve the long distance dependency of German verbs types that allow for separation of the prefix from the verb in the sentence structure. 3. Resources In this section we describe the major resources used. For the SMT system we used the open source Moses system1 . For paraphrasing we used the open source English Resource Grammar. We tested on t"
2008.iwslt-papers.2,2001.mtsummit-papers.45,0,0.030743,"e explore previous research related to reordering models where a form of paraphras- 151 - ing does occur in how translation is done. In [5] proposed a reordering model that took into account predicateargument structure in Japanese and followed a heuristic for reordering sentences in the training data as a preprocessing step. This sort of reordering, while unnatural to native speakers, is still grammatically correct and easier to align to English during model training, it is also a type of paraphrasing. [6] made use of parses of source sentences and then applied a reordering heuristic as well. [7] also discusses the use altering German word order to correspond to English word order there is also some use of annotations on verbs with identifying prefixes to solve the long distance dependency of German verbs types that allow for separation of the prefix from the verb in the sentence structure. 3. Resources In this section we describe the major resources used. For the SMT system we used the open source Moses system1 . For paraphrasing we used the open source English Resource Grammar. We tested on two JapaneseEnglish corpora, the Tanaka Corpus and the IWSLT corpus. We chose the Tanaka corp"
2008.iwslt-papers.2,P07-2045,0,0.00468382,"For paraphrasing we used the open source English Resource Grammar. We tested on two JapaneseEnglish corpora, the Tanaka Corpus and the IWSLT corpus. We chose the Tanaka corpus primarily because of its easy availability (it is in the public domain). This will make our results easy to reproduce. We also tested on the IWSLT corpus, as it has been used in several competitions, in order to facilitate comparisons with other systems. In the spirit of open science, the paraphrased Tanaka Corpus data and our scripts will be put on line at www2. nict.go.jp/x/x161/en/member/bond/data/. 3.1. Moses Moses [8] is an open-source toolkit for phrase-based statistical machine translation with support for factors. The toolkit is one of the first highly efficient and free SMT decoders and tool kits; it supports building factored statistical models. Factors such as part-of-speech, morphology, and lemmas are applied using translation models and generation models, two features of moses which in conjunction with the user specified decoding steps help to create stories of how one language might translate best to another. We used the multi-threaded Giza++ [9] as it fixed a bug in how probabilities are assigned"
2008.iwslt-papers.2,W06-1661,0,0.0282395,"ersity since 1993. The ERG was originally developed within the Verbmobil machine translation effort, but over the past few years has been ported to additional domains and significantly extended. The grammar includes a hand-built lexicon of around 43,000 lexemes. We are using the development release LinGO (Apr-08). The ERG and the associated parsers and generators are freely available from the Deep Linguistic Processing with HPSG Initiative (DELPH-IN: www. delph-in.net/). Generally, we use the default settings and the language models trained in the LOGON project both for parsing and generation [12]. However, we set the root condition, which controls which sentences are treated as grammatical, to be robust for parsing and strict for generation. This means that robust rules (for example a rule to allow verbs not to agree in number with their subject) will apply in parsing but not in generation. The grammar will thus parse The dog bark or The dog barks but only generate The dog barks. 3.3. Corpora We used two corpora, one freely available, and one standard test set. 3.3.1. Tanaka Corpus The Tanaka corpus is an open corpus of Japanese-English sentence pairs compiled by Professor Yasuhito Ta"
2008.iwslt-papers.2,2004.iwslt-evaluation.1,1,0.799203,"ong sentences (> 40 tokens) as part of the SMT cleaning process, there were 147,007 sentences in the training set. The average number of tokens per sentence is 11.6 for Japanese and 9.1 for English (with the tokenization used in the SMT). 3.3.2. The IWSLT Corpus We also tested our system on the IWSLT 2005 evaluation corpus [15]. This is a subset of the Basic Travel Expression Corpus (BTEC), which contains tourismrelated sentences similar to those that are usually found in phrase books for tourists going abroad [16]. Parts of this corpus were already used in previous IWSLT evaluation campaigns [17]. We used the evaluation and development data sets of 2004, although only with the first of the multiple reference translations, for our development corpora and the 500 sentence IWSLT 2005 evaluation set, again with only the first of the 16 references, as the evaluation corpus. The IWSLT corpus has 42,682 sentence pairs. The average number of tokens per sentence is 9.0 for Japanese and 8.0 for English (with the tokenization used in the SMT). The sentences are both shorter and more homogeneous than those in the Tanaka Corpus. 4. Method 4.1. Paraphrasing We paraphrase by parsing a sentence to an"
2008.iwslt-papers.2,W04-3230,0,0.0113819,"82 0.74 0.79 0.83 0.77 0.82 0.81 0.80 0.82 +0.14 +0.29 +0.67 +0.20 +0.14 +0.32 +0.17 -0.13 +0.34 -0.33 -0.04 +0.15 +0.34 +0.17 +0.27 +0.44 Table 2: Results of adding paraphrases to Tanaka Corpus training data We replicated the baseline in the ACL 2007 Second Workshop on Statistical Machine Translation. The baseline is a factorless Moses system with a 5-gram language model. We followed the online tutorial3 as-is, with the exception that we used external morphological analyzers to tokenize our data instead of using the provided scripts. We used the Tree Tagger [21] for English and MeCab [22] for Japanese. Part-of-speech information was discarded after tokenization. All data was tokenized, separating punctuation from words and converted to lowercase prior to training and translation. Translations were detokenized and recased prior to evaluation using the helper scripts distributed as part of the baseline system for the ACL 2007 SMT Workshop. Prior to evaluation we conducted Minimum Error Rate Training on each system using the development data from the target corpus. We used the MERT implementation distributed with Moses. All results reported in this paper are post-mert Bleu scores"
2008.iwslt-papers.2,W04-3250,0,0.00898498,"ata 5.2. Results We compared a baseline of no paraphrases added (d:0) to systems with progressively larger numbers of new paraphrased sentence pairs added to the training data. We tested three distributions (d, f and v ). v always gave results below the baseline, so we do not report them in more detail. The results for d and f are summarized in Tables 2 and 3 with 2, 4, 6 and 8 paraphrases. All deltas and significance results are calculated against the baseline of no paraphrases (0). We calculated Bleu score variance and measured statistical significance with the bootstrap methods outlined in [23] using Jun-ya Norimatsu’s MIT-Licensed Bleu Kit.4 Variance scores are reported with p = 0:05 in Tables 2 and 3. In Tables 2 and 3 results with an improvement of p < 0:10 over the baseline are shown in bold. 6. Discussion The results for En!Ja show gains of up to 0.67 Bleu points on the Tanaka Corpus and 0.19 on the IWSLT 4 www.mibel.cs.tsukuba.ac.jp/˜norimatsu/ bleu_kit/ - 155 - 2005 evaluation data. The results for Ja!En show gains of 0.44 on the Tanaka Corpus and 0.61 on the IWSLT 2005 evaluation data. There is a statistically significant improvement for each language pair and paraphrase dis"
2008.iwslt-papers.2,2007.tmi-papers.18,0,0.0109645,"raphrases. Compared to [3] or [4] we are very conservative in our paraphrasing, and this is probably why we get a slightly lower improvement in quality. We could do more extravagant paraphrasing, but would have to retrain the generation model. At the moment, it expects fully specified input MRSs, if we were going to allow variation in, for example, noun phrase structure or open class lexical variation, then we should treat it as a monolingual translation problem, and also train a transfer (paraphrase) model. An example of how to do this (for bilingual transfer (Norwegian-English)) is given in [24]. Our syntactic reordering is not aimed at matching the target language like [5]. We correspondingly get a slighter improvement, but can hope to get a similar improvement even for different language pairs. Also, our improvement is still there after MERT training, whereas theirs did not survive the optimization. 7. Further Work MT system [26]. We can easily write noun phrase rewriting rules of the type used by [4]. For lexical substitution we will try using WordNet, after first disambiguating the input. Finally, we would like to enhance Moses (primarily GIZA++) so that input sentences can be we"
2008.iwslt-papers.2,2005.mtsummit-osmtw.3,1,0.82132,"xample, noun phrase structure or open class lexical variation, then we should treat it as a monolingual translation problem, and also train a transfer (paraphrase) model. An example of how to do this (for bilingual transfer (Norwegian-English)) is given in [24]. Our syntactic reordering is not aimed at matching the target language like [5]. We correspondingly get a slighter improvement, but can hope to get a similar improvement even for different language pairs. Also, our improvement is still there after MERT training, whereas theirs did not survive the optimization. 7. Further Work MT system [26]. We can easily write noun phrase rewriting rules of the type used by [4]. For lexical substitution we will try using WordNet, after first disambiguating the input. Finally, we would like to enhance Moses (primarily GIZA++) so that input sentences can be weighted. That way, if we have n paraphrases for one sentence and m for another, each can just be entered with a weight of 1=n and 1=m respectively. If we could do this, we could then experiment with setting a probability based threshold on the number of paraphrases, for example, to select all paraphrases within of the probability of the origi"
2008.iwslt-papers.2,2005.iwslt-1.1,0,\N,Missing
2009.iwslt-evaluation.1,W07-0718,0,0.133717,"it was difficult to distinguish whether system improvements were triggered by better suited (or simply more) language resources or by improvements in the underlying decoding algorithms and statistical models. In order to focus more on the research aspects, only the supplied resources listed in Appendix B were allowed for the training of the MT engines for the IWSLT 2009 official run submission. All primary run submissions were judged and compared according to the Ranking metric where human graders were asked to rank whole sentence translations from best to worst relative to the other choices [2]. Then, human assessments of Fluency and Adequacy [3] were carried out for the topranked MT outputs of each translation task. In addition, a modified version of the Adequacy metrics that takes into account information beyond the current input sentence was applied to the translation results of the Challenge Task in order to judge the overall translation quality of a given MT output in the context of the respective dialog. The translation quality of all primary and contrastive run submissions was also evaluated using various standard automatic evaluation metrics. In addition to the single-metric"
2009.iwslt-evaluation.1,1994.amta-1.25,0,0.752462,"vements were triggered by better suited (or simply more) language resources or by improvements in the underlying decoding algorithms and statistical models. In order to focus more on the research aspects, only the supplied resources listed in Appendix B were allowed for the training of the MT engines for the IWSLT 2009 official run submission. All primary run submissions were judged and compared according to the Ranking metric where human graders were asked to rank whole sentence translations from best to worst relative to the other choices [2]. Then, human assessments of Fluency and Adequacy [3] were carried out for the topranked MT outputs of each translation task. In addition, a modified version of the Adequacy metrics that takes into account information beyond the current input sentence was applied to the translation results of the Challenge Task in order to judge the overall translation quality of a given MT output in the context of the respective dialog. The translation quality of all primary and contrastive run submissions was also evaluated using various standard automatic evaluation metrics. In addition to the single-metric scores, all automatic metric scores for the MT outpu"
2009.iwslt-evaluation.1,2003.mtsummit-papers.51,0,0.0461112,"respect to reference translations. Scores range between 0 (worst) and 1 (best) [7] → ’mteval-v13.pl’ NIST: a variant of BLEU using the arithmetic mean of weighted n-gram precision values. Scores are positive with 0 being the worst possible [8] → ’mteval-v13.pl’ METEOR: calculates unigram overlaps between a translation and reference texts taking into account various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) [9] → ’meteor-v0.8.3’ GTM: measures the similarity between texts by using a unigram-based F-measure. Scores range between 0 (worst) and 1 (best) [10] → ’gtm-v1.4’ WER: Word Error Rate: the edit distance between the system output and the closest reference translation. Scores are positive with 0 being the best possible [11] PER: Position independent WER: a variant of WER that disregards word ordering [12] TER: Translation Edit Rate: a variant of WER that allows phrasal shifts [13] → ’tercom-0.7.25’ where di is the difference between the rank of the system i and n is the number of systems. 3. Evaluation Results The evaluation results of the IWSLT 2009 workshop are summarized in Appendix C (human assessment) and Appendix D (automatic evaluatio"
2009.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,0.011628,"cision values. Scores are positive with 0 being the worst possible [8] → ’mteval-v13.pl’ METEOR: calculates unigram overlaps between a translation and reference texts taking into account various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) [9] → ’meteor-v0.8.3’ GTM: measures the similarity between texts by using a unigram-based F-measure. Scores range between 0 (worst) and 1 (best) [10] → ’gtm-v1.4’ WER: Word Error Rate: the edit distance between the system output and the closest reference translation. Scores are positive with 0 being the best possible [11] PER: Position independent WER: a variant of WER that disregards word ordering [12] TER: Translation Edit Rate: a variant of WER that allows phrasal shifts [13] → ’tercom-0.7.25’ where di is the difference between the rank of the system i and n is the number of systems. 3. Evaluation Results The evaluation results of the IWSLT 2009 workshop are summarized in Appendix C (human assessment) and Appendix D (automatic evaluation). The rank correlation coefficients of subjective and automatic evaluation results are given in Appendix E. For each evaluation metric, the best correlation coefficient of"
2009.iwslt-evaluation.1,2006.amta-papers.25,0,0.0253191,"texts taking into account various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) [9] → ’meteor-v0.8.3’ GTM: measures the similarity between texts by using a unigram-based F-measure. Scores range between 0 (worst) and 1 (best) [10] → ’gtm-v1.4’ WER: Word Error Rate: the edit distance between the system output and the closest reference translation. Scores are positive with 0 being the best possible [11] PER: Position independent WER: a variant of WER that disregards word ordering [12] TER: Translation Edit Rate: a variant of WER that allows phrasal shifts [13] → ’tercom-0.7.25’ where di is the difference between the rank of the system i and n is the number of systems. 3. Evaluation Results The evaluation results of the IWSLT 2009 workshop are summarized in Appendix C (human assessment) and Appendix D (automatic evaluation). The rank correlation coefficients of subjective and automatic evaluation results are given in Appendix E. For each evaluation metric, the best correlation coefficient of each translation task is marked in boldface. 3.1. Subjective Evaluation Results are language independent. For the automatic evaluation results of the CTEC task,"
2009.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,0.153066,"Missing"
2009.iwslt-evaluation.1,2009.iwslt-evaluation.2,0,0.0505765,"Missing"
2009.iwslt-evaluation.1,2009.iwslt-evaluation.4,0,0.0249339,"Missing"
2009.iwslt-evaluation.1,2009.iwslt-evaluation.6,0,0.0249068,"Missing"
2009.iwslt-evaluation.1,2009.iwslt-evaluation.12,0,0.0257032,"Missing"
2009.iwslt-evaluation.1,2009.iwslt-evaluation.16,0,0.0605538,"Missing"
2009.iwslt-evaluation.1,2009.iwslt-papers.2,0,0.0366312,"Missing"
2009.iwslt-evaluation.1,2009.iwslt-papers.5,0,0.0572077,"Missing"
2009.iwslt-evaluation.1,2009.iwslt-papers.6,1,0.806902,"Missing"
2009.iwslt-evaluation.1,P02-1040,0,\N,Missing
2009.iwslt-evaluation.1,W07-0734,0,\N,Missing
2009.iwslt-evaluation.1,2009.iwslt-evaluation.7,0,\N,Missing
2009.iwslt-evaluation.1,2009.iwslt-evaluation.18,0,\N,Missing
2009.iwslt-evaluation.1,2009.iwslt-evaluation.5,0,\N,Missing
2009.iwslt-evaluation.1,2009.iwslt-evaluation.19,0,\N,Missing
2009.iwslt-evaluation.1,2008.iwslt-evaluation.1,1,\N,Missing
2009.iwslt-evaluation.1,2009.iwslt-evaluation.3,0,\N,Missing
2010.iwslt-evaluation.1,2005.iwslt-1.19,0,0.0489962,"Missing"
2010.iwslt-evaluation.1,W07-0718,0,0.0381577,"tic evaluation scores were also calculated for case-insensitive (lower-case only) MT outputs with punctuation marks removed (no_case+no_punc). 2.4. Evaluation Specifications In this section, we summarize the subjective and automatic evaluation metrics used to assess the translation quality of the primary run submissions. 2.4.1. Subjective Evaluation Human assessments of translation quality were carried out using the Ranking metrics. For the Ranking evaluation, human graders were asked to “rank each whole sentence translation from Best to Worst relative to the other choices (ties are allowed)” [5]. The Ranking evaluation was carried out using a web-browser interface and graders had to order up to five system outputs by assigning a grade between 5 (best) and 1 (worse). This year’s evaluations were carried out by paid evaluation experts, i.e., three graders for each of the target languages. The Ranking scores were obtained as the average number of times that a system was judged better than any other system. In addition, normalized ranks (NormRank) on a per-judge basis using the method of [6] were calculated for each run submission. The Ranking metric was applied to all submitted primary"
2010.iwslt-evaluation.1,2006.amta-papers.25,0,0.0771934,"Missing"
2010.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,0.0672413,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.2,0,0.0412904,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.4,0,0.0757522,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.20,0,0.0355769,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.21,0,0.0337253,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.23,0,0.0323627,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.11,0,0.0546484,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.26,0,0.0352774,"Missing"
2010.iwslt-evaluation.1,2010.iwslt-evaluation.28,0,0.039409,"Missing"
2010.iwslt-evaluation.18,N03-1017,0,0.00288212,"words by replacing them with known words that have the same lemmas but different inflections. The structure of the remainder of the paper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous cont"
2010.iwslt-evaluation.18,P05-1033,0,0.0604414,"that have the same lemmas but different inflections. The structure of the remainder of the paper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY alg"
2010.iwslt-evaluation.18,koen-2004-pharaoh,0,0.0968916,"aper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language m"
2010.iwslt-evaluation.18,P03-1021,0,0.0147245,"translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language model, a hierarchical phrase translation model, and phrase penalty. The feature weights are also tuned using MERT [4]. 2.2. Integration of Multiple Segmentation Schemes The task of word segmentation, i.e., i"
2010.iwslt-evaluation.18,J07-2003,0,0.0812834,"en in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language model, a hierarchical phrase translation model, and phrase penalty. The feature weights are also tuned using MERT [4]. 2.2. Integration of Multiple Segmentation Schemes The task of word segmentation, i.e., identifying word boundaries in continuous text, is one of the fundamental preprocessing steps of data-"
2010.iwslt-evaluation.18,W10-1760,1,0.791009,"on of Multiple Segmentation Schemes The task of word segmentation, i.e., identifying word boundaries in continuous text, is one of the fundamental preprocessing steps of data-driven NLP applications like Machine Translation (MT). In contrast to Indo-European languages like English, many Asian languages like Chinese do not use a whitespace character to separate meaningful word units. We use an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches [6]. Word segmentations that are consistent with the phrasal segmentations of SMT translation models are learned from the SMT training corpus by aligning character-wise source language sentences to word units separated by a whitespace in the target language. Successive characters aligned to the same target words are merged into a larger source language unit. Therefore, the granularity of the translation unit is defined in the given bitext context. In order to minimize the side effects of alignment errors and to achieve segmentation consistency, a Maximum-Entropy (ME) algorithm is applied to learn"
2010.iwslt-evaluation.18,P07-1040,0,0.0206781,"er the same surface string but differ only in the segmentation of the source language phrase. Therefore, the more often such a translation pair is learned by different iterative models, the more often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorith"
2010.iwslt-evaluation.18,N04-1022,0,0.0213991,"language phrase. Therefore, the more often such a translation pair is learned by different iterative models, the more often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm"
2010.iwslt-evaluation.18,2006.amta-papers.25,0,0.017203,"often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argm"
2010.iwslt-evaluation.18,P02-1034,0,0.0171555,"output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, w"
2010.iwslt-evaluation.18,P05-1012,0,0.0195886,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,D07-1080,1,0.900751,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,D08-1024,0,0.0536492,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,2008.iwslt-evaluation.13,1,0.855795,"n using the loss biased maximization in Equation 2 largely inspired by [14]. For the loss function lij and the underlying BLEU score b(·), we applied document scaled BLEU which computes BLEU by replacing one translation ei1 with another eij in a set of 1-best translations {ei1 }i=1...m [13]. Oracle translations are selected with respect to b(·). When multiple oracle translations are found, we select the one which maximizes ∆h(eij ) · w [14]. 2.4.2. Feature Functions for Re-ranking We used a large number of sparse binary features together with real valued features from decoders as described in [17]. Word pair features We used all possible pairs of source word and target word as our primary features. POS pairs were also extracted by replacing source words and target words with their corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignme"
2010.iwslt-evaluation.18,N03-1033,0,0.0440416,"t of 1-best translations {ei1 }i=1...m [13]. Oracle translations are selected with respect to b(·). When multiple oracle translations are found, we select the one which maximizes ∆h(eij ) · w [14]. 2.4.2. Feature Functions for Re-ranking We used a large number of sparse binary features together with real valued features from decoders as described in [17]. Word pair features We used all possible pairs of source word and target word as our primary features. POS pairs were also extracted by replacing source words and target words with their corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-c"
2010.iwslt-evaluation.18,N06-1014,0,0.0794113,"ir corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the"
2010.iwslt-evaluation.18,P08-1012,0,0.0154613,". As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from"
2010.iwslt-evaluation.18,P03-1054,0,0.00620711,"our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from the current translated utterance and bags of words (BOW) from the previously “translated” la"
2010.iwslt-evaluation.18,P08-1067,0,0.0126776,"a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from the current translated utterance and bags of words (BOW) from the previously “translated” last utterance from both speakers. The BOWs were collected from the n-best list of the tra"
2010.iwslt-evaluation.18,2009.iwslt-evaluation.12,1,0.830023,"DIALOG corpus, the BTEC corpus and the DEVSET corpus. All the data in the DEVSET for the BTEC task, using on single reference, was included for training. Only the devset for DIALOG was reserved for development testing. All of our experiment results presented in this paper are based on this testset. In total, we had around 35K sentence pairs for training. The devset used for MERT is sampled from all of the DEVSET for BTEC. In the last year’s IWSLT campaign, we introduced a devset sampling technique in which the development data were sampled from training data that are similar to the input text [24]. The similarity is measured by the BLEU using the input sentences as references. This year, we sampled from bilingual data with multiple reference translations, rather than from large amounts of DIALOG data with single reference translations, in order to avoid overfitting. We extracted 500 sentences for each translation direction. During MERT, only the training corpus for DIALOG and BTEC were used to train the translation model, but all of the data was used to build final translation model. Some pre-processing was also carried out on the corpus before training. First, in order to avoid ambigu"
2010.iwslt-evaluation.18,I08-4033,1,0.828764,"ces are split if multiple sentences are found in one line. At the end of translation, these multiple sentences are concatenated into a single line. We also did some normalization to the text. For English text, all the words were lowercased, any hyphens or commas were removed from between numeral words and tokenized using the standard tools provided by the Moses toolkit1 . The Chinese word segmentation originally provided contained inconsistencies and was not usable to build the translation model. The Chinese word segmentation was therefore redone using three methods: character-based, Achilles [25] and ICTCLAS2 . We will explain the usage of different segmentation standards in the next section. Basically, the numeral words in Chinese can be written either using Chinese characters or Arabic numbers. We converted all of the Arabic numbers to Chinese characters using a simple set of heuristics. Our translation model was built from data containing the punctuation for both source and target languages. In the official testing, the test data is provided without punctuation to remain consistent with the format of ASR output. So, before sending the test data for translation, we restored the punc"
2011.iwslt-evaluation.1,P02-1040,0,0.111082,"4.2. In addition, for development purposes, ASR outputs for the IWSLT 2010 development and test sets were also made available to participants. 3.3. Evaluation Specifications The participants had to provide the result of the translation of the English audio in NIST XML format. The output had to be true-cased and had to contain punctuation. The participants could either use the audio files directly, or use the output— either first best hypotheses in CTM format or word lattices in SLF — of KIT, LIUM, and FBK from the ASR task. The quality of the translations was measured automatically with BLEU [1] by scoring against the human translations created by the TED open translation project, and by human subjective evaluation (paired comparison, Section 7). Since the reference translations from the TED website match the segmentation of the reference transcriptions of the talks, 12 automatic evaluation scores for the MT outputs could be directly computed. The evaluation specifications for the SLT task were defined as case-sensitive with punctuation marks (case+punc). Tokenization scripts were applied automatically to all run submissions prior to evaluation. Moreover, automatic evaluation scores"
2011.iwslt-evaluation.1,W07-0734,0,0.031722,"Missing"
2011.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,0.0557796,"Missing"
2011.iwslt-evaluation.1,2006.amta-papers.25,0,0.148776,"Missing"
2011.iwslt-evaluation.1,2003.mtsummit-papers.51,0,0.142051,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.8,0,0.040341,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.7,0,0.030857,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.10,0,0.0773305,"Missing"
2011.iwslt-evaluation.1,2011.mtsummit-papers.59,1,0.858689,"xception of sentences with less than 5 words, which were excluded from the subjective evaluation. The IWSLT 2011 subjective evaluation focused solely on the Ranking task10 and a number of novelties were introduced with respect to the traditional system ranking evaluation carried out in previous campaigns. Firstly, this year’s evaluation was not carried out by hired expert graders but by relying on crowdsourced data. The feasibility of using crowdsourcing methodologies as an effective way to reduce the costs of MT evaluation without sacrificing quality was investigated in a previous experiment [23], where the ranking evaluation of the IWSLT 2010 Arabic-English BTEC task was replicated by hiring non-experts through Amazon’s Mechanical Turk. The analysis of the collected data showed that agreement rates for non-experts were comparable to those for experts, and that the crowd-based system ranking had a very strong correlation with expert-based ranking. Secondly, the cost reduction obtained by using crowdsourcing allowed us to focus on modifying and extending the ranking methodology in different respects, with the aim of maximizing the overall evaluation reliability. The goal of the Ranking"
2011.iwslt-evaluation.1,W07-0718,0,0.0308834,"uation is to produce a complete ordering of the systems participating in a given task. The ranking task requires human judges to decide whether one system output is better than another for a given source sentence. The judgments collected through these comparisons are used to obtain the ranking scores, which are calculated as the average number of times that a system was judged better than any other system. Traditionally, in the ranking task, the judge was presented with the output of five submissions for a given source sentence and was asked to rank them from best to worst (ties were allowed) [24]. Each evaluation block contained the implicit pairwise comparisons (i.e. each system against the other systems presented in the same block) which constituted the basis of the ranking scores. 10 Last year human evaluation was also carried out for the Fluency and Adequacy metrics. In the following sections we analyze the data that we collected by posting the ranking task on Amazon’s Mechanical Although ranking a number of translated sentences relative to each other is quite intuitive, a 5-fold ranking task is less reliable than a direct comparison between only two translated sentences due to th"
2011.iwslt-evaluation.1,D09-1030,0,0.0421625,"Missing"
2011.iwslt-evaluation.1,2011.iwslt-evaluation.6,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.3,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.9,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.13,0,\N,Missing
2011.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.4,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.5,0,\N,Missing
2011.iwslt-evaluation.1,2011.iwslt-evaluation.11,0,\N,Missing
2011.mtsummit-papers.59,S10-1007,0,0.0268043,"tasks, including the creation of parallel corpora, the word-level alignment of parallel sentences, the creation of paraphrases of existing reference translations, and the creation of translation lexica for low resource languages (Callison-Burch and Dredze, 2010). Annotated data is also crucial for evaluation purposes. Very recently, crowdsourced data has started being ofﬁcially used in international evaluation campaigns. In the CLEF 2010 Web People Search Clustering Task (Artiles et al., 2010) and the SemEval2010 Task of Noun Compounds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for different types of human evaluation tasks. In (Callison-Burch, 2009), it is shown that MTurk can be effectively used to collect relative rankings, to perform human-mediated translation edit rate (HTER), and to carry out evaluation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessment"
2011.mtsummit-papers.59,W10-0701,0,0.0210162,"Vision, and Natural Language Processing tasks such as relation extraction, word sense disambiguation, textual entailment, named entity annotation, and natural language generation. Machine Translation is one of the ﬁelds where research on crowdsourcing is most active. The feasibility of collecting good quality crowdsourced data has been explored for many different MT tasks, including the creation of parallel corpora, the word-level alignment of parallel sentences, the creation of paraphrases of existing reference translations, and the creation of translation lexica for low resource languages (Callison-Burch and Dredze, 2010). Annotated data is also crucial for evaluation purposes. Very recently, crowdsourced data has started being ofﬁcially used in international evaluation campaigns. In the CLEF 2010 Web People Search Clustering Task (Artiles et al., 2010) and the SemEval2010 Task of Noun Compounds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for"
2011.mtsummit-papers.59,W07-0718,0,0.230666,"a given translation depends on numerous factors like the intended use of the translation, the characteristics of the MT software, and the nature of the translation process. Early attempts tried to manually produce numerical judgements of MT quality with respect to a set of reference translations (White et al., 1994). Recently, human assessment of MT quality has been carried out by either assigning a single grade on a scale of 5 or 7 specifying the ﬂuency or adequacy of a given translation (Przybocki et al., 2008), or by relatively ranking to each other multiple translations of the same input (Callison-Burch et al., 2007). 521 Although human evaluation of MT output provides the most direct and reliable assessment, it is time consuming, costly and subjective, i.e., evaluation results might vary from person to person due to different backgrounds, bilingual experience, and inconsistent judgements caused by the high complexity of the multi-class grading task. These drawbacks to human assessment schemes have encouraged many researchers to seek reliable methods for estimating such measures automatically. Various automatic evaluation measures have been proposed to make the evaluation of MT outputs cheaper and faster."
2011.mtsummit-papers.59,D09-1030,0,0.0704963,"purposes. Very recently, crowdsourced data has started being ofﬁcially used in international evaluation campaigns. In the CLEF 2010 Web People Search Clustering Task (Artiles et al., 2010) and the SemEval2010 Task of Noun Compounds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for different types of human evaluation tasks. In (Callison-Burch, 2009), it is shown that MTurk can be effectively used to collect relative rankings, to perform human-mediated translation edit rate (HTER), and to carry out evaluation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have mo"
2011.mtsummit-papers.59,W10-0709,0,0.112325,"ds Interpretation Using Paraphrasing Verbs and Prepositions (Butnariu et al., 2010), for example, MTurk was used for the annotation of the training/test data sets. For MT evaluation, a number of studies have been carried out with the aim of understanding the feasibility of substituting expert data with non-expert data for different types of human evaluation tasks. In (Callison-Burch, 2009), it is shown that MTurk can be effectively used to collect relative rankings, to perform human-mediated translation edit rate (HTER), and to carry out evaluation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none"
2011.mtsummit-papers.59,W10-0713,0,0.0166157,"ehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none of them addressing the task of MT evaluation. 3 IWSLT 2010 BTEC Task Evaluation The International Workshop on Spoken Language Translation (IWSLT) is a yearly, open evaluation campaign for spoken language translation. IWSLT’s evaluations are not competition-oriented; their goal is to foster cooperative work and scientiﬁc exchange. In this respect, IWSLT proposes challenging research tasks and an open experimental infrastructure for the scientiﬁc community working on spoken and written language translation. In the IWSLT 2010 campaign (Paul et al., 2010), three dif"
2011.mtsummit-papers.59,W10-0734,0,0.0133446,"In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none of them addressing the task of MT evaluation. 3 IWSLT 2010 BTEC Task Evaluation The International Workshop on Spoken Language Translation (IWSLT) is a yearly, open evaluation campaign for spoken language translation. IWSLT’s evaluations are not competition-oriented; their goal is to foster cooperative work and scientiﬁc exchange. In this respect, IWSLT proposes challenging research tasks and an open experimental infrastructure for the scientiﬁc community working on spoken and written language translation. In the IWSLT 2010 campaign (Paul et al., 2010), three different shared tasks addre"
2011.mtsummit-papers.59,J82-2005,0,0.718625,"Missing"
2011.mtsummit-papers.59,W10-0725,0,0.0281807,"valuation through reading comprehension questions. In (Denkowski and Lavie, 2010) MTurk was used to obtain translation adequacy assessments. Finally, in (Callison-Burch et al., 2010) a subset of the ofﬁcial WMT10 relative ranking evaluation was reproduced with non-expert judges and various methods to improve the quality of the collected data are presented. All these MT evaluation experiments have been conducted using MTurk directly, as have most of the available studies on the effectiveness of crowdsourcing. Up to now, only a few papers have reported on the use of CF as an interface to MTurk (Wang and Callison-Burch, 2010; Finin et al., 2010; Negri and Mehdad, 2010), none of them addressing the task of MT evaluation. 3 IWSLT 2010 BTEC Task Evaluation The International Workshop on Spoken Language Translation (IWSLT) is a yearly, open evaluation campaign for spoken language translation. IWSLT’s evaluations are not competition-oriented; their goal is to foster cooperative work and scientiﬁc exchange. In this respect, IWSLT proposes challenging research tasks and an open experimental infrastructure for the scientiﬁc community working on spoken and written language translation. In the IWSLT 2010 campaign (Paul et a"
2011.mtsummit-papers.59,1994.amta-1.25,0,0.69757,"Missing"
2020.acl-main.201,N19-1253,0,0.0358768,"to the final layer, run Adam (Kingma and Ba, 2015) with default parameters for ten epochs, and report the average accuracy of ten runs. 2 https://github.com/facebookresearch/ MUSE/issues/24 3 A pilot study confirms that retrofitting to infrequent word pairs is less effective. Dependency Parsing We also test on dependency parsing, a structured prediction task. We use Universal Dependencies (Nivre et al., 2019, 4.2 Intrinsic Evaluation: BLI 2217 v2.4) with the standard split. We use the biaffine parser (Dozat and Manning, 2017) in AllenNLP (Gardner et al., 2017) with the same hyperparameters as Ahmad et al. (2019). To focus on the influence of CLWE, we remove part-of-speech features (Ammar et al., 2016). We report the average unlabeled attachment score (UAS) of five runs. Results Although training dictionary retrofitting lowers BLI test accuracy, it improves both downstream tasks’ test accuracy (Figure 3). This confirms that over-optimizing the test BLI accuracy can hurt downstream tasks because training dictionary words are also important. The synthetic dictionary further improves downstream models, showing that generalization to downstream tasks must balance between BLI training and test accuracy. Qu"
2020.acl-main.201,N19-1391,0,0.0225807,"ely refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accuracy, which further helps downstream models on average. BLI test accuracy does not always correlate with downstream task accuracy because words from the"
2020.acl-main.201,D16-1250,0,0.0392399,"ained by a projection-based method, where X′ = WX are the projected source embeddings and Z′ = Z are ˆ and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly aligned. In practice, we use a non-zero α for regularization, but the upd"
2020.acl-main.201,P17-1042,0,0.0458555,"s. Our pilot experiments found similar trends when replacing retrofitting with Counter-fitting (Mrkši´c et al., 2016) and Attract-Repel (Mrkši´c et al., 2017), so we focus on retrofitting. Recent work applies semantic specialization to CLWE by using multilingual ontologies (Mrkši´c et al., 2017), transferring a monolingual ontology across languages (Ponti et al., 2019), and asking bilingual speakers to annotate task-specific keywords (Yuan et al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wan"
2020.acl-main.201,D18-1366,0,0.0250512,"and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly aligned. In practice, we use a non-zero α for regularization, but the updated CLWE still have perfect training BLI accuracy (Figure 2). If the training dictionary covers predict"
2020.acl-main.201,2020.acl-main.747,0,0.0490558,"t al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accuracy, which further helps"
2020.acl-main.201,D19-1572,0,0.0142572,"Ponti et al., 2019), and asking bilingual speakers to annotate task-specific keywords (Yuan et al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then"
2020.acl-main.201,E14-1049,0,0.0627564,"target languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We use 300-dimensional fastText vectors trained on Wikipedia and Common Crawl (Grave et al., 2018). We lowercase all words, only keep the 200K most frequent words, and apply five rounds of Iterative Normalization (Zhang et al., 2019). We use dictionaries from MUSE (Conneau et al., 2018), a popular BLI benchmark, with standard splits: train on 5K source word translations and test on 1.5K words for BLI. For each language, we train three projection-based CLWE: canonical correlation analysis (Faruqui and Dyer, 2014, CCA), 2216 1 Code at https://go.umd.edu/retro_clwe. Original +retrofit +synthetic 100 80 60 60 40 40 DE ES FR IT JA RU ZH 20 AVG Original +retrofit +synthetic 80 DE Document classification with PROC 60 60 40 DE ES FR IT JA RU ZH 20 AVG Original +retrofit +synthetic DE ES 60 40 ES FR IT JA RU FR IT ZH AVG JA RU ZH AVG ZH 20 AVG Original +retrofit +synthetic 80 60 DE RU Dependency parsing with CCA 80 40 JA Original +retrofit +synthetic Document classification with CCA 100 IT 80 80 40 FR Dependency parsing with PROC Original +retrofit +synthetic 100 ES DE Document classification with RCSLS ES F"
2020.acl-main.201,P19-1489,1,0.850029,"dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation. 1 Target Embedding Original CLWE Retrofit Updated CLWE Figure 1: To fully exploit the training dictionary, we retrofit projection-based CLWE to the training dictionary as a post-processing step (pink parts). To preserve correctly aligned translations in the original CLWE, we optionally retrofit to a synthetic dictionary induced from the original CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g.,"
2020.acl-main.201,P19-1070,0,0.289461,"m tasks and explains why BLI is a flawed CLWE evaluation. 1 Target Embedding Original CLWE Retrofit Updated CLWE Figure 1: To fully exploit the training dictionary, we retrofit projection-based CLWE to the training dictionary as a post-processing step (pink parts). To preserve correctly aligned translations in the original CLWE, we optionally retrofit to a synthetic dictionary induced from the original CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor searc"
2020.acl-main.201,L18-1550,0,0.0372087,"onary keeps closely aligned word pairs in the original CLWE, which sometimes improves downstream models. Experiments We retrofit three projection-based CLWE to their training dictionaries and synthetic dictionaries.1 We evaluate on BLI and two downstream tasks. While retrofitting decreases test BLI accuracy, it often improves downstream models. 4.1 Embeddings and Dictionaries We align English embeddings with six target languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We use 300-dimensional fastText vectors trained on Wikipedia and Common Crawl (Grave et al., 2018). We lowercase all words, only keep the 200K most frequent words, and apply five rounds of Iterative Normalization (Zhang et al., 2019). We use dictionaries from MUSE (Conneau et al., 2018), a popular BLI benchmark, with standard splits: train on 5K source word translations and test on 1.5K words for BLI. For each language, we train three projection-based CLWE: canonical correlation analysis (Faruqui and Dyer, 2014, CCA), 2216 1 Code at https://go.umd.edu/retro_clwe. Original +retrofit +synthetic 100 80 60 60 40 40 DE ES FR IT JA RU ZH 20 AVG Original +retrofit +synthetic 80 DE Document classi"
2020.acl-main.201,P15-1119,0,0.0414222,"ced from the original CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor search) and compared against gold translations. BLI accuracy is easy to compute and captures the desired property of CLWE that translation pairs should be close. However, BLI accuracy Equal contribution Project Training Dictionary Introduction ⋆ ∗ Synthetic Dictionary Let’s think about why that might be. BLI accuracy is only computed on test words. Consequently, BLI hides linear pro"
2020.acl-main.201,D19-1090,0,0.111892,"istances between translation pairs in a training dictionary: ∑ min ∥Wxi − zj ∥22 . (1) W weakness is not transparent when using BLI as the standard evaluation for CLWE, because BLI test sets omit training dictionary words. However, when the training dictionary covers words that help downstream tasks, underfitting limits generalization to other tasks. Some BLI benchmarks use frequent words for training and infrequent words for testing (Mikolov et al., 2013; Conneau et al., 2018). This mismatch often appears in real-world data, because frequent words are easier to find in digital dicitonaries (Czarnowska et al., 2019). Therefore, training dictionary words are often more important in downstream tasks than test words. 3 Retrofitting to Dictionaries To fully exploit the training dictionary, we explore a simple post-processing step that overfits the dictionary: we first train projection-based CLWE and then retrofit to the training dictionary (pink parts in Figure 1). Retrofitting was originally introduced for refining monolingual word embeddings with synonym constraints from a lexical ontology (Faruqui et al., 2015). For CLWE, we retrofit using the training dictionary D as the ontology. Intuitively, retrofitti"
2020.acl-main.201,D19-1252,0,0.0167114,"o annotate task-specific keywords (Yuan et al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test"
2020.acl-main.201,D18-1330,0,0.326515,"minimizing deviation from the original CLWE. Let X′ and Z′ be CLWE trained by a projection-based method, where X′ = WX are the projected source embeddings and Z′ = Z are ˆ and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly ali"
2020.acl-main.201,D14-1181,0,0.00501098,"t language. We first compare BLI accuracy on both training and test dictionaries (Figure 2). We use CSLS to translate words with default parameters. The original projection-based CLWE have the highest test accuracy but underfit the training dictionary. Retrofitting to the training dictionary perfectly Document Classification Our first downstream task is document-level classification. We use MLDoc, a multilingual classification benchmark (Schwenk and Li, 2018) using the standard split with 1K training and 4K test documents. Following Glavas et al. (2019), we use a convolutional neural network (Kim, 2014). We apply 0.5 dropout to the final layer, run Adam (Kingma and Ba, 2015) with default parameters for ten epochs, and report the average accuracy of ten runs. 2 https://github.com/facebookresearch/ MUSE/issues/24 3 A pilot study confirms that retrofitting to infrequent word pairs is less effective. Dependency Parsing We also test on dependency parsing, a structured prediction task. We use Universal Dependencies (Nivre et al., 2019, 4.2 Intrinsic Evaluation: BLI 2217 v2.4) with the standard split. We use the biaffine parser (Dozat and Manning, 2017) in AllenNLP (Gardner et al., 2017) with the s"
2020.acl-main.201,C12-1089,0,0.0937553,"synthetic dictionary induced from the original CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor search) and compared against gold translations. BLI accuracy is easy to compute and captures the desired property of CLWE that translation pairs should be close. However, BLI accuracy Equal contribution Project Training Dictionary Introduction ⋆ ∗ Synthetic Dictionary Let’s think about why that might be. BLI accuracy is only computed on test words. Consequently, BL"
2020.acl-main.201,N16-1018,0,0.124468,"Missing"
2020.acl-main.201,Q17-1022,0,0.121936,"Missing"
2020.acl-main.201,P17-1135,0,0.0140593,"ts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor search) and compared against gold translations. BLI accuracy is easy to compute and captures the desired property of CLWE that translation pairs should be close. However, BLI accuracy Equal contribution Project Training Dictionary Introduction ⋆ ∗ Synthetic Dictionary Let’s think about why that might be. BLI accuracy is only computed on test words. Consequently, BLI hides linear projection’s inability to align all train"
2020.acl-main.201,D19-1226,0,0.167317,"Missing"
2020.acl-main.201,L18-1560,0,0.133547,"pendency parsing. We fix the embeddng layer of the model to CLWE and use the zero-shot setting, where a model is trained in English and evaluated in the target language. We first compare BLI accuracy on both training and test dictionaries (Figure 2). We use CSLS to translate words with default parameters. The original projection-based CLWE have the highest test accuracy but underfit the training dictionary. Retrofitting to the training dictionary perfectly Document Classification Our first downstream task is document-level classification. We use MLDoc, a multilingual classification benchmark (Schwenk and Li, 2018) using the standard split with 1K training and 4K test documents. Following Glavas et al. (2019), we use a convolutional neural network (Kim, 2014). We apply 0.5 dropout to the final layer, run Adam (Kingma and Ba, 2015) with default parameters for ten epochs, and report the average accuracy of ten runs. 2 https://github.com/facebookresearch/ MUSE/issues/24 3 A pilot study confirms that retrofitting to infrequent word pairs is less effective. Dependency Parsing We also test on dependency parsing, a structured prediction task. We use Universal Dependencies (Nivre et al., 2019, 4.2 Intrinsic Eva"
2020.acl-main.201,2020.acl-main.536,0,0.0150763,", 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accuracy, which further helps downstream models on average. BLI test accuracy does not always correlate with downstream task accuracy because words from the training dictionary are ignored. An obvious fix is adding t"
2020.acl-main.201,D19-1077,0,0.0295053,"ific keywords (Yuan et al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accurac"
2020.acl-main.201,N15-1104,0,0.0985627,"′ and Z′ be CLWE trained by a projection-based method, where X′ = WX are the projected source embeddings and Z′ = Z are ˆ and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly aligned. In practice, we use a non-zero α for regul"
2020.acl-main.201,P19-1307,1,0.909316,"rojected source embeddings and Z′ = Z are ˆ and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly aligned. In practice, we use a non-zero α for regularization, but the updated CLWE still have perfect training BLI accuracy (Figure"
2020.acl-main.201,N16-1156,0,0.0499522,"nal CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor search) and compared against gold translations. BLI accuracy is easy to compute and captures the desired property of CLWE that translation pairs should be close. However, BLI accuracy Equal contribution Project Training Dictionary Introduction ⋆ ∗ Synthetic Dictionary Let’s think about why that might be. BLI accuracy is only computed on test words. Consequently, BLI hides linear projection’s inability"
2020.acl-main.201,N19-1162,0,0.021627,"tion (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accuracy, which further helps downstream models on average. BLI test accuracy does not always correlate with downstream task accuracy because words from the training dictionary ar"
2020.acl-main.201,N15-1184,0,\N,Missing
2020.acl-main.201,W18-2501,0,\N,Missing
2020.cl-1.3,P18-1073,0,0.0249956,"Missing"
2020.cl-1.3,E14-1049,0,0.0415614,"(Artetxe, Labaka, and Agirre 2018; Lample et al. 2018), which is a large step toward generalization of crosslingual modeling. Although it is an open problem on how to interpret the results and how to reduce the heavy computing resources required, embedding based methods are a promising research direction. Relations to Topic Models. A very common strategy for learning crosslingual embeddings is to use a projection matrix as supervision or sub-objective to learn a projection matrix that projects independently trained monolingual embeddings into a shared crosslingual space (Dinu and Baroni 2014; Faruqui and Dyer 2014; Tsvetkov and Dyer 2016; Vuli´c and Korhonen 2016). In multilingual topic models, the supervision matrix δ plays the role of a projection matrix between languages. In DOCLINK, for example, δd`2 ,d`1 projects document d`2 to the document space of `1 (Equation (15)). SOFTLINK provides a simple extension by forming δ to a matrix of transfer distirbutions based on word-level document similarities. VOCLINK applies projections in the form of word translations. Thus, we can see that the formation of projection matrices in multilingual topic models is still static and restricted to an identity matrix"
2020.cl-1.3,Q16-1004,0,0.060999,"Missing"
2020.cl-1.3,N18-1099,1,0.859695,"Missing"
2020.cl-1.3,C18-1220,1,0.888047,"Missing"
2020.cl-1.3,N19-1158,1,0.884055,"Missing"
2020.cl-1.3,P14-1110,0,0.0607378,"Missing"
2020.cl-1.3,2005.mtsummit-papers.11,0,0.0665631,"models with enough data, and, therefore, we choose languages that are understudied in natural language processing literature. Preprocessing in this language group needs more consideration. Because they represent low-resource languages that most natural language processing tools are not available for, we do not use a fixed stopword list. Stemmers are also not available for these languages, so we do not apply stemming. 5.2 Training Sets and Model Configurations There are many resources available for multilingual research, such as the European Parliament Proceedings parallel corpus (E URO PARL; Koehn 2005), the Bible, and Wikipedia. EuroParl provides a perfectly parallel corpus with precise translations, but it only contains 21 European languages, which limits its generalizability to most of the languages. The Bible, on the other hand, is also perfectly parallel and is widely available in 2,530 languages.7 Its disadvantages, however, are that the contents are very limited (mostly about family and religion), the data set size is small (1,189 chapters), and many languages do not have digital format (Christodoulopoulos and Steedman 2015). Compared with E URO PARL and the Bible, Wikipedia provides"
2020.cl-1.3,W11-2125,0,0.0607435,"Missing"
2020.cl-1.3,N16-1132,0,0.0277967,"Missing"
2020.cl-1.3,N16-1053,0,0.0403102,"Missing"
2020.cl-1.3,E14-1056,0,0.0626521,"Missing"
2020.cl-1.3,D09-1092,0,0.0498777,"tric (Wallach, Mimno, and McCallum 2009), though in this work we use symmetric priors. Figure 2 shows the plate notation of LDA. 2.2 Multilingual Topic Models We now describe a variety of multilingual topic models, organized into two families based on the type of supervision they use. Later, in Section 4, we focus on a subset of the models described here for deeper analysis using our knowledge transfer formulation, selecting the most general and representative models. 2.2.1 Document Level. The first model proposed to process multilingual corpora using LDA is the Polylingual Topic Model (PLTM; Mimno et al. 2009; Ni et al. 2009). This model extracts language-consistent topics from parallel or highly comparable multilingual corpora (for example, Wikipedia articles aligned across languages), assuming that document translations share the same topic distributions. This model has been ↵ ✓ D Nd z w k K Figure 2 Plate notation of LDA. α and β are Dirichlet hyperparameters for θ and {φ(k) }Kk=1 . Topic assignments are denoted as z, and w denotes observed tokens. 97 Computational Linguistics Volume 46, Number 1 extensively used and adapted in various ways for different crosslingual tasks (Krstovski and Smith"
2020.cl-1.3,J14-2003,0,0.0578634,"Missing"
2020.cl-1.3,P15-1165,0,0.01958,") (i1 ,sv) 00 , 00 ⌘ ⌘ ]) i⌘ Figure 3 An illustration of the tree structure used in word-level models. Hyperparameters β(r,`) and β(i,`) are both vectors, and β0 and β00 are scalars. In the figure, i1 has three translations, so the corresponding hyperparameter β1(r,EN ) = β1(r,SV ) = 3β0 . Document-topic distributions θd are generated in the same way as monolingual LDA, because no document translation is required. The use of dictionaries to model similarities across topic-word distributions has been formulated in other ways as well. P ROB B I LDA (Ma and Nasukawa 2017) uses inverted indexing (Søgaard et al. 2015) to encode assumptions that word translations are generated from same distributions. P ROB B I LDA does not use tree structures in the parameters as in VOCLINK, but the general idea of sharing distributions among word translations is similar. Guti´errez et al. (2016) use part-of-speech taggers to separate topic words (nouns) and perspective words (adjectives and verbs), developed for the application of detecting cultural differences, such as how different languages have different perspectives on the same topic. Topic words are modeled in the same way as in VOCLINK, whereas perspective words ar"
2020.cl-1.3,tiedemann-2012-parallel,0,0.0274577,"nd the paired language. This only applies to H IGH L AN, because TED do not have documents in L OW L AN. In TED+GV, we infer topics on English documents from TED, and infer topics on documents from GV in the paired language (both H IGH L AN and L OW L AN). The two types of test sets also represent different application situations. TED + TED implies that the test documents in both languages are parallel and come from the same source, whereas TED + GV represents how the topic model performs when the two languages have different data sources. Both corpora are retrieved from http://opus.nlpl.eu/ (Tiedemann 2012). The labels, however, are manually retrieved from http://ted.com/ and http://globalvoices. org/. In TED corpus, each document is a transcript of a talk and is assigned to multiple categories on the Web page, such as “technology,” “arts,” and so forth. We collect all categories for the entire TED corpus, and use the three most frequent categories— technology, culture, science—as document labels. Similarly, in GV corpus, each document is a news story, and has been labeled with multiple categories on the Web page of the story. Because in TED + GV, the two sets are from different sources, and tra"
2020.cl-1.3,P16-1157,0,0.0157532,"rly good performance on document classification as other models, but the topic qualities according to coherence-based metrics are lower. Comparing to SOFTLINK, which also requires a dictionary as resource, directly modeling word translations in VOCLINK turns out to be a less efficient way of transferring dictionary knowledge. Therefore, when using dictionary information, we recommend SOFTLINK over VOCLINK. 8.2 Crosslingual Representations As an alternative method to learning crosslingual representations, crosslingual word embeddings have been gaining attention (Ruder, Vulic, and Søgaard 2019; Upadhyay et al. 2016). Recent crosslingual embedding architectures have been applied to a wider range of applications in natural language processing, and achieve state-of-the-art performance. Similar to the topic space in multilingual topic models, crosslingual embeddings learn semantically consistent features in a shared embedding space for all languages. Both approaches—topic modeling and embedding—have advantages and limitations. Multilingual topic models still rely on supervised data to learn crosslingual representations. The choice of such supervision and model is important, which leads to our main discussion"
2020.cl-1.3,P16-1024,0,0.0433781,"Missing"
2020.cl-1.3,D14-1040,0,0.0216043,"such as word translation detection and crosslingual information retrieval, also utilize the trained distributions, but here we focus on a straightforward and representative task. 5.3.1 Intrinsic Evaluation: Topic Quality. Intrinsic evaluation refers to evaluating the learned model directly without applying it to any particular task; for topic models, this is usually based on the quality of the topics. Standard evaluation measures for monolingual models, such as perplexity (or held-out likelihood; Wallach et al. 2009) and Normalized Pointwise Mutual Information (NPMI, Lau, Newman, and Baldwin (2014)), could potentially be considered for crosslingual models. However, when evaluating multilingual topics, how words in different languages make sense together is also a critical criterion in addition to coherence within each of the languages. In monolingual studies, Chang et al. (2009) show that held-out likelihood is not always positively correlated with human judgments of topics. Held-out likelihood is additionally suboptimal for multilingual topic models, because this measure is only calculated within each language, and the important crosslingual information is ignored. Crosslingual Normali"
2020.cl-1.3,P11-1026,0,\N,Missing
2020.lrec-1.180,P04-3031,0,0.409686,"Missing"
2020.lrec-1.180,Q17-1010,0,0.0109897,"languages. We calN (King culate the weight for each document category by N l and Zeng, 2001), where N is the number of documents in each language and Nl is the number of documents labeled by the category. Particularly, for training BERT model, we append two additional tokens, “[CLS]” and “[SEP]”, at the start and end of each document respectively. For the neural models, we pad each document or drop rest of words up to 40 tokens. We use “unknown” as a replacement for unknown tokens. We initialize CNN and RNN classifiers by pre-trained word embeddings (Mikolov et al., 2013; Godin et al., 2015; Bojanowski et al., 2017; Deriu et al., 2017) and train the networks up to 10 epochs. LR. We first extract TF-IDF-weighted features of uni, bi-, and tri-grams on the corpora, using the most fre1443 Language English Language Polish Method LR CNN RNN BERT Acc .874 .878 .898 .705 F1-w .874 .877 .896 .635 F1-m .841 .845 .867 .579 AUC .920 .927 .938 .581 Language Method LR CNN RNN BERT Acc .864 .855 .857 .824 F1-w .846 .851 .854 .782 F1-m .653 .688 .696 .478 AUC .804 .813 .822 .474 Language Language Spanish Method LR CNN RNN BERT Italian Portuguese Acc .704 .650 .674 .605 F1-w .707 .654 .674 .573 Method LR CNN RNN BERT Ac"
2020.lrec-1.180,W19-3504,0,0.282638,"in derived synthetic author demographic attributes instead of the original author information. The common data sources either derive from Wikipedia toxic comments (Dixon et al., 2018; Park et al., 2018; Garg et al., 2019) or synthetic document templates (Kiritchenko and Mohammad, 2018; Park et al., 2018). The Wikipedia Talk corpus1 (Wulczyn et al., 2017) provides demographic information of annotators instead of the authors, Equity Evaluation Corpus2 (Kiritchenko and Mohammad, 2018) are created by sentence templates and combinations of racial names and gender coreferences. While existing work (Davidson et al., 2019; Diaz et al., 2018) infers user demographic information (white/black, young/old) from the text, such inference is still likely to cause confounding erThe work was partially done when the first author worked as an intern at Adobe Research. 1 https://figshare.com/articles/Wikipedia_ Detox_Data/4054689 2 http://saifmohammad.com/WebPages/ Biases-SA.html rors that impact and break the independence between demographic factors and the fairness evaluation of text classifiers. Second, existing research in the fairness evaluation mainly focus on only English resources, such as age biases in blog posts"
2020.lrec-1.180,N19-1423,0,0.0873731,"t the values of our approach that to avoid confounding errors, we obtain author demographic information independently from the user generated documents. 4. Demographic variations root in documents, especially in social media data (Volkova et al., 2013; Hovy, 2015; Johannsen et al., 2015). Such variations could further impact the performance and fairness of document classifiers. In this study, we experiment four different classification models including logistic regression (LR), recurrent neural network (RNN) (Chung et al., 2014), convolutional neural network (CNN) (Kim, 2014) and Google BERT (Devlin et al., 2019). We present the baseline results of both performance and fairness evaluations across the multilingual corpus. 4.1. word 16.1 age 16.7 11.7 country gender Demographic Factors 11.7 race Figure 1: Predictability of demographic attributes from the English data. We show the absolute percentage improvements in accuracy over majority-class baselines. The majority-class baselines of accuracy are .500 for the binary predictions. The darker color indicates higher improvements and vice versa. The improved prediction accuracy scores over majority baselines suggest that language variations across demograp"
2020.lrec-1.180,W19-3510,0,0.0900787,"educe and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classifiers. In this study, we combine previously published corpora labeled for Twitter hate speech recognition in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Polish (Ptaszynski et al., 2019), Portuguese (Fortuna et al., 2019), and Spanish (Basile et al., 2"
2020.lrec-1.180,S19-2080,0,0.0228826,"ics and useful signals. We implement a BERT-based classification model by HuggingFace’s Transformers (Wolf et al., 2019). The model encodes each document into a fixed size (768) of representation and feed to a linear prediction layer. The model is optimized by AdamW with a warmup and learning rate as .1 and 2e−5 respectively. We leave parameters as their default, conduct fine-tuning steps with 4 epochs and set batch size as 32 (Sun et al., 2019a). The classification model loads “bert-base-uncased” pre-trained BERT model for English and “bert-base-multilingual-uncased” multilingual BERT model (Gertner et al., 2019) for the other languages. The multilingual BERT model follows the same method of BERT by using Wikipedia text from the top 104 languages. Due to the label imbalance shown in Table 1, we balance training instances by randomly oversampling the minority during the training process. 4.3. Evaluation Metrics Performance Evaluation. To measure overall performance, we evaluate models by four metrics: accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m) and area under the ROC curve (AUC). The F1 score coherently comprecision∗recall bines both precision and recall by 2 ∗ precision+recall . We"
2020.lrec-1.180,W15-4322,0,0.0686378,"Missing"
2020.lrec-1.180,P15-1073,0,0.296437,"er privacy, we will not publicize the personal profile information, including user ids, photos, geocoordinates as well as other user profile information, which were used to infer the demographic attributes. We will, however, provide inferred demographic attributes in their original formats from the Face++ and Google Maps based on per request to allow wider researchers and communities to replicate the methodology and probe more depth of fairness in document classification. 3. Language Variations across Demographic Groups Demographic factors can improve the performances of document classifiers (Hovy, 2015), and demographic variations root in language, especially in social media data (Volkova et al., 2013; Hovy, 2015). For example, language styles are highly correlated with authors’ demographic attributes, such as age, race, gender and location (Coulmas, 2017; PreoţiucPietro and Ungar, 2018). Research (Bolukbasi et al., 2016; Zhao et al., 2017; Garg et al., 2018) find that biases and stereotypes exist in word embeddings, which is widely used in document classification tasks. For example, “receptionist” is closer to females while “programmer” is closer to males, and “professor” is closer to Asian"
2020.lrec-1.180,S19-1015,1,0.834911,"a_ Detox_Data/4054689 2 http://saifmohammad.com/WebPages/ Biases-SA.html rors that impact and break the independence between demographic factors and the fairness evaluation of text classifiers. Second, existing research in the fairness evaluation mainly focus on only English resources, such as age biases in blog posts (Diaz et al., 2018), gender biases in Wikipedia comments (Dixon et al., 2018) and racial biases in hate speech detection (Davidson et al., 2019). Different languages have shown different patterns of linguistic variations across the demographic attributes (Johannsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al.,"
2020.lrec-1.180,K15-1011,0,0.0372314,"re.com/articles/Wikipedia_ Detox_Data/4054689 2 http://saifmohammad.com/WebPages/ Biases-SA.html rors that impact and break the independence between demographic factors and the fairness evaluation of text classifiers. Second, existing research in the fairness evaluation mainly focus on only English resources, such as age biases in blog posts (Diaz et al., 2018), gender biases in Wikipedia comments (Dixon et al., 2018) and racial biases in hate speech detection (Davidson et al., 2019). Different languages have shown different patterns of linguistic variations across the demographic attributes (Johannsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 20"
2020.lrec-1.180,D14-1181,0,0.0200037,"the black. This can highlight the values of our approach that to avoid confounding errors, we obtain author demographic information independently from the user generated documents. 4. Demographic variations root in documents, especially in social media data (Volkova et al., 2013; Hovy, 2015; Johannsen et al., 2015). Such variations could further impact the performance and fairness of document classifiers. In this study, we experiment four different classification models including logistic regression (LR), recurrent neural network (RNN) (Chung et al., 2014), convolutional neural network (CNN) (Kim, 2014) and Google BERT (Devlin et al., 2019). We present the baseline results of both performance and fairness evaluations across the multilingual corpus. 4.1. word 16.1 age 16.7 11.7 country gender Demographic Factors 11.7 race Figure 1: Predictability of demographic attributes from the English data. We show the absolute percentage improvements in accuracy over majority-class baselines. The majority-class baselines of accuracy are .500 for the binary predictions. The darker color indicates higher improvements and vice versa. The improved prediction accuracy scores over majority baselines suggest th"
2020.lrec-1.180,S18-2005,0,0.0376384,"irness, multilingual, document classification, hate speech 1. Introduction While document classification models should be objective and independent from human biases in documents, research have shown that the models can learn human biases and therefore be discriminatory towards particular demographic groups (Dixon et al., 2018; Borkan et al., 2019; Sun et al., 2019b). The goal of fairness-aware document classifiers is to train and build non-discriminatory models towards people no matter what their demographic attributes are, such as gender and ethnicity. Existing research (Dixon et al., 2018; Kiritchenko and Mohammad, 2018; Park et al., 2018; Garg et al., 2019; Borkan et al., 2019) in evaluating fairness of document classifiers focus on the group fairness (Chouldechova and Roth, 2018), which refers to every demographic group has equal probability of being assigned to the positive predicted document category. However, the lack of original author demographic attributes and multilingual corpora bring challenges towards the fairness evaluation of document classifiers. First, the datasets commonly used to build and evaluate the fairness of document classifiers obtain derived synthetic author demographic attributes i"
2020.lrec-1.180,D14-1108,0,0.0426124,"Missing"
2020.lrec-1.180,W14-4204,0,0.0302902,"s in Wikipedia comments (Dixon et al., 2018) and racial biases in hate speech detection (Davidson et al., 2019). Different languages have shown different patterns of linguistic variations across the demographic attributes (Johannsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classif"
2020.lrec-1.180,D18-1302,0,0.187707,"lassification, hate speech 1. Introduction While document classification models should be objective and independent from human biases in documents, research have shown that the models can learn human biases and therefore be discriminatory towards particular demographic groups (Dixon et al., 2018; Borkan et al., 2019; Sun et al., 2019b). The goal of fairness-aware document classifiers is to train and build non-discriminatory models towards people no matter what their demographic attributes are, such as gender and ethnicity. Existing research (Dixon et al., 2018; Kiritchenko and Mohammad, 2018; Park et al., 2018; Garg et al., 2019; Borkan et al., 2019) in evaluating fairness of document classifiers focus on the group fairness (Chouldechova and Roth, 2018), which refers to every demographic group has equal probability of being assigned to the positive predicted document category. However, the lack of original author demographic attributes and multilingual corpora bring challenges towards the fairness evaluation of document classifiers. First, the datasets commonly used to build and evaluate the fairness of document classifiers obtain derived synthetic author demographic attributes instead of the origi"
2020.lrec-1.180,C18-1130,0,0.164262,"Missing"
2020.lrec-1.180,L18-1443,0,0.110849,"ang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classifiers. In this study, we combine previously published corpora labeled for Twitter hate speech recognition in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Polish (Ptaszynski et al., 20"
2020.lrec-1.180,P19-1163,0,0.0531452,"re not accurate enough to provide fine-grained information, our attribute categories are still too coarsegrained (binary age groups and gender, and only four race categories). Using coarse-grained attributes would hide the identities of specific demographic groups, including other racial minorities and people with non-binary gender. Broadening our analyses and evaluations to include more attribute values may require better methods of user attribute inference or different sources of data. Third, language variations across demographic groups might introduce annotation biases. Existing research (Sap et al., 2019) shows that annotators are more likely to annotate tweets containing African American English words as hate speech. Additionally, the nationality and educational level might also impact on the quality of annotations (Founta et al., 2018). Similarly, different annotation sources of our dataset (which merged two different corpora) might have variations in annotating schema. To reduce annotation biases due to the different annotating schema, we merge the annotations into the two most compatible document categories: normal and hate speech. Annotation biases might still exist, therefore, we will re"
2020.lrec-1.180,P19-1159,0,0.0527636,"Missing"
2020.lrec-1.180,D13-1187,0,0.15713,"tos, geocoordinates as well as other user profile information, which were used to infer the demographic attributes. We will, however, provide inferred demographic attributes in their original formats from the Face++ and Google Maps based on per request to allow wider researchers and communities to replicate the methodology and probe more depth of fairness in document classification. 3. Language Variations across Demographic Groups Demographic factors can improve the performances of document classifiers (Hovy, 2015), and demographic variations root in language, especially in social media data (Volkova et al., 2013; Hovy, 2015). For example, language styles are highly correlated with authors’ demographic attributes, such as age, race, gender and location (Coulmas, 2017; PreoţiucPietro and Ungar, 2018). Research (Bolukbasi et al., 2016; Zhao et al., 2017; Garg et al., 2018) find that biases and stereotypes exist in word embeddings, which is widely used in document classification tasks. For example, “receptionist” is closer to females while “programmer” is closer to males, and “professor” is closer to Asian Americans while “housekeeper” is closer to Hispanic Americans. This motivates us to explore and tes"
2020.lrec-1.180,N16-2013,0,0.196528,"annsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classifiers. In this study, we combine previously published corpora labeled for Twitter hate speech recognition in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Pol"
2020.lrec-1.180,W16-5618,0,0.416363,"hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 2019; Basile et al., 2019; Fortuna et al., 2019), there is still no integrated multilingual corpora that contain author demographic attributes which can be used to measure group fairness. The lack of author demographic attributes and multilingual datasets limits research for evaluating classifier fairness and developing unbiased classifiers. In this study, we combine previously published corpora labeled for Twitter hate speech recognition in English (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018), Italian (Sanguinetti et al., 2018), Polish (Ptaszynski et al., 2019), Portuguese (Fortuna et al., 2019), and Spanish (Basile et al., 2019), and publish this multilingual data augmented with author-level demographic information for four attributes: race, gender, age and country. The demographic factors are inferred from user profiles, which are independent from text documents, the tweets. To our best knowledge, this is the first multilingual hate speech corpus annotated with author attributes aiming for fairness evaluation. We start with presenting collection and inferen"
2020.lrec-1.180,D17-1323,0,0.270146,"aifmohammad.com/WebPages/ Biases-SA.html rors that impact and break the independence between demographic factors and the fairness evaluation of text classifiers. Second, existing research in the fairness evaluation mainly focus on only English resources, such as age biases in blog posts (Diaz et al., 2018), gender biases in Wikipedia comments (Dixon et al., 2018) and racial biases in hate speech detection (Davidson et al., 2019). Different languages have shown different patterns of linguistic variations across the demographic attributes (Johannsen et al., 2015; Huang and Paul, 2019), methods (Zhao et al., 2017; Park et al., 2018) to reduce and evaluate the demographic bias in English corpora may not apply to other languages. For example, Spanish has gender-dependent nouns, but this does not exist in English (Sun et al., 2019b); and Portuguese varies across Brazil and Portugal in both word usage and grammar (Maier and Gómez-Rodríguez, 2014). The rich variations have not been explored under the fairness evaluation due to lack of multilingual corpora. Additionally, while we have hate speech detection datasets in multiple languages (Waseem and Hovy, 2016; Sanguinetti et al., 2018; Ptaszynski et al., 20"
2020.lrec-1.180,L18-1404,0,0.0131256,"classifiers. We evaluate overall performance by four metrics including accuracy (Acc), weighted F1 score (F1-w), macro F1 score (F1-m) and area under the ROC curve (AUC). The higher score indicates better performance. We highlight models achieve the best performance in each column. quent 15K features with the minimum feature frequency as 2. We then train a LogisticRegression from scikitlearn (Pedregosa et al., 2011). We use “liblinear” as the solver function and leave the other parameters as default. CNN. We implement the Convolutional Neural Network (CNN) classifier described in (Kim, 2014; Zimmerman et al., 2018) by Keras (Chollet and others, 2015). We first apply 100 filters with three different kernel sizes, 3, 4 and 5. After the convolution operations, we feed the concatenated features to a fully connected layer and output document representations with 100 dimensions. We apply “softplus” function with a l2 regularization with .03 and a dropout rate with .3 in the dense layer. The model feeds the document representation to final prediction. We train the model with batch size 64, set model optimizer as Adam (Kingma and Ba, 2014) and calculate loss values by the cross entropy function. We keep all oth"
2020.lrec-1.180,P06-4018,0,\N,Missing
2020.lrec-1.180,Q18-1041,0,\N,Missing
2020.lrec-1.180,W02-0109,0,\N,Missing
2020.lrec-1.180,S19-2007,0,\N,Missing
2021.adaptnlp-1.18,K16-1017,0,0.017608,"representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as do"
2021.adaptnlp-1.18,P16-2003,1,0.855504,"om online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaura"
2021.adaptnlp-1.18,E17-1015,0,0.123495,"ectronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting th"
2021.adaptnlp-1.18,P04-3031,0,0.270174,"sources (He and McAuley, 2016; Yelp, 2018; IMDb, 2020). For the IMDb dataset, we included English movies produced in the US from 1960 to 2019. Each review associates with its author and the rated item, which refers to a movie in the IMDb data, a business unit in the Yelp data and a product in the Amazon data. To keep consistency in each dataset, we retain top 4 frequent genres of rated items and the review documents with no less than 10 tokens.1 We dropped non-English review documents by the language detector (Lui and Baldwin, 2012), lowercased all tokens and tokenized the corpora using NLTK (Bird and Loper, 2004). The review datasets have different score scales. We 1 The top 4 rated categories of Amazon-Health, IMDb and Yelp are [sports nutrition, sexual wellness, shaving & hair removal, vitamins & dietary supplements], [comedy, thriller, drama, action] and [restaurants, health & medical, home services, beauty & spas] respectively. normalize the scales and encode each review score into three discrete categories: positive (&gt; 3 for the Yelp and Amazon, &gt; 6 for the IMDb), negative (&lt; 3 for the Yelp and Amazon, &lt; 5 for the IMDb) and neutral. Table 1 shows a summary of the datasets. 2.1 Privacy Considerati"
2021.adaptnlp-1.18,D16-1171,0,0.109652,"ress sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentim"
2021.adaptnlp-1.18,W14-4012,0,0.0651627,"Missing"
2021.adaptnlp-1.18,D17-1241,0,0.0587459,"Missing"
2021.adaptnlp-1.18,2020.acl-main.700,0,0.0209386,"et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Personalized classification generally improves the performance of document classifiers (Flek, 2020). The multitask learning framework has been applied for personalizing document classifiers by optimizing the classifiers on multiple document levels (Benton et al., 2017) or general and individual levels (Wu and Huang, 2016). The social relation can bridge connections between users and generalize classification models across users (Wu and Huang, 2016; Yang and Eisenstein, 2017). For example, (Wu and Huang, 2016) optimizes document Amazon-Health Precision Recall F1 .834 .768 .793 .841 .777 .801 .838 .771 .796 .813 .844 .812 .836 .811 .821 .821 .832 .825 .866 .822 .840 .863 .812 .831 .873 .838 ."
2021.adaptnlp-1.18,S19-1015,1,0.873981,"19; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services domains) and propose a multitask framework to model language variations and incorporate the user factor into user embeddings. We focus on three online 172 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 172–182 April 20, 2021. ©2021 Association for Computational Linguistics review datasets from Amazon, IMDb, and Yelp containing diverse behaviors conditioned on user in"
2021.adaptnlp-1.18,C18-1079,0,0.0517273,"Missing"
2021.adaptnlp-1.18,P12-3005,0,0.0502504,"glish reviews of Amazon (health product), IMDb and Yelp from the publicly available sources (He and McAuley, 2016; Yelp, 2018; IMDb, 2020). For the IMDb dataset, we included English movies produced in the US from 1960 to 2019. Each review associates with its author and the rated item, which refers to a movie in the IMDb data, a business unit in the Yelp data and a product in the Amazon data. To keep consistency in each dataset, we retain top 4 frequent genres of rated items and the review documents with no less than 10 tokens.1 We dropped non-English review documents by the language detector (Lui and Baldwin, 2012), lowercased all tokens and tokenized the corpora using NLTK (Bird and Loper, 2004). The review datasets have different score scales. We 1 The top 4 rated categories of Amazon-Health, IMDb and Yelp are [sports nutrition, sexual wellness, shaving & hair removal, vitamins & dietary supplements], [comedy, thriller, drama, action] and [restaurants, health & medical, home services, beauty & spas] respectively. normalize the scales and encode each review score into three discrete categories: positive (&gt; 3 for the Yelp and Amazon, &gt; 6 for the IMDb), negative (&lt; 3 for the Yelp and Amazon, &lt; 5 for the"
2021.adaptnlp-1.18,2020.acl-main.472,0,0.217767,"line users can use the word “fast” to criticize battery quality of the electronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while"
2021.adaptnlp-1.18,W19-2103,0,0.0193578,"processing. Online generated user texts show demographic variations in the linguistic styles, and the linguistic style variability could be used for predicting user’s personality and demographic attributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; WoodDoughty et al., 2020; Gjurkovi´c et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Personalized classification generally improves the performance of document classifiers (Flek, 2020). The multitask learning framework has been applied for personalizing document classifiers by optimizing the classifiers on multiple document levels (Benton et al., 2017) or general and individual levels (Wu and Huang, 2016). The social relation can bridge connections between users and g"
2021.adaptnlp-1.18,D17-1119,0,0.0919775,"018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services domains) and propose a multitask framework to model language variations and incorporate the user factor into user embeddings. We focus on three online 172 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 172–182 April 20, 2021. ©2021 Association for Computational Linguistics review datasets from Amazon, IMDb, and Yelp containing diverse behaviors"
2021.adaptnlp-1.18,P17-1116,0,0.02697,"tion models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services domains) and propose a multitask framework to model language variations and incorporate the user factor into user embeddings. We focus on three online 172 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 172–182 April 2"
2021.adaptnlp-1.18,N19-1215,0,0.115669,"cross user factors including user interests, demographic attributes, personalities, and latent factors from user history. Research shows that language usage diversifies according to online user groups (Volkova et al., 2013), which women were more likely to use the word weakness in a positive way while men were the opposite. In social media, the user interests can include topics of user reviews (e.g., home vs. health services in Yelp) and categories of reviewed items (electronic vs kitchen products in Amazon). The ways that users express themselves depend on current contexts of user interests (Oba et al., 2019) that users may use the same words for opposite meanings and different words for the same meaning. For example, online users can use the word “fast” to criticize battery quality of the electronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representat"
2021.adaptnlp-1.18,P11-1077,0,0.0151672,"es of rated items. To map the 300d user embeddings, we use the TSNE algorithm from scikit-learn (Pedregosa et al., 2011) to compress the dimension into 2-d vectors. We set the n component as 2 and leave the other parameters as their defaults in the TSNE. We can observe that the MTL user embedding model shows more cluster178 6 Related Work User Profiling is a common task in natural language processing. Online generated user texts show demographic variations in the linguistic styles, and the linguistic style variability could be used for predicting user’s personality and demographic attributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; WoodDoughty et al., 2020; Gjurkovi´c et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Per"
2021.adaptnlp-1.18,D15-1036,0,0.247783,"ion 3. We then propose our user embedding model that adapts the user interests using a multitask learning framework in Section 4. Research (Pan and Ding, 2019) generally evaluates the user embedding via downstream tasks, but user annotations sometimes are hard to obtain and those evaluations are extrinsic instead of intrinsic tasks. For example, the MyPersonality (Kosinski et al., 2015) that was used in previous work (Ding et al., 2017; Farnadi et al., 2018; Pan and Ding, 2019) is no longer available, and an extrinsic task is to evaluate if user embeddings can help text classifiers. Research (Schnabel et al., 2015) suggests that the intrinsic evaluation including clustering is better than the extrinsic evaluation for controlling less hyperparameters. We propose an intrinsic evaluation for user embedding, which can provide a new perspective for testing future experiments. We show that our user-factor-adapted user embedding can generally outperform the existing methods on both intrinsic and extrinsic tasks. 2 Data We collected English reviews of Amazon (health product), IMDb and Yelp from the publicly available sources (He and McAuley, 2016; Yelp, 2018; IMDb, 2020). For the IMDb dataset, we included Engli"
2021.adaptnlp-1.18,P15-1098,0,0.0239691,"AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al."
2021.adaptnlp-1.18,D13-1187,0,0.210638,"on. While existing work mainly evaluated the user embedding by extrinsic tasks, we propose an intrinsic evaluation via clustering and evaluate user embeddings by an extrinsic task, text classification. The experiments on the three Englishlanguage social media datasets show that our proposed approach can generally outperform baselines via adapting the user factor. 1 Introduction Language varies across user factors including user interests, demographic attributes, personalities, and latent factors from user history. Research shows that language usage diversifies according to online user groups (Volkova et al., 2013), which women were more likely to use the word weakness in a positive way while men were the opposite. In social media, the user interests can include topics of user reviews (e.g., home vs. health services in Yelp) and categories of reviewed items (electronic vs kitchen products in Amazon). The ways that users express themselves depend on current contexts of user interests (Oba et al., 2019) that users may use the same words for opposite meanings and different words for the same meaning. For example, online users can use the word “fast” to criticize battery quality of the electronic domain or"
2021.adaptnlp-1.18,C18-1119,0,0.020667,"rds for the same meaning. For example, online users can use the word “fast” to criticize battery quality of the electronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extra"
2021.adaptnlp-1.18,W17-2912,1,0.698095,"Missing"
2021.adaptnlp-1.18,W17-4406,1,0.722932,"predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services"
2021.adaptnlp-1.18,Q17-1021,0,0.374267,"r embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisen"
2021.adaptnlp-1.18,P19-1270,0,0.0235045,"h representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Hu"
2021.adaptnlp-1.18,L16-1478,0,0.0153144,"300d user embeddings, we use the TSNE algorithm from scikit-learn (Pedregosa et al., 2011) to compress the dimension into 2-d vectors. We set the n component as 2 and leave the other parameters as their defaults in the TSNE. We can observe that the MTL user embedding model shows more cluster178 6 Related Work User Profiling is a common task in natural language processing. Online generated user texts show demographic variations in the linguistic styles, and the linguistic style variability could be used for predicting user’s personality and demographic attributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; WoodDoughty et al., 2020; Gjurkovi´c et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Personalized classifica"
C02-1017,2001.mtsummit-papers.53,0,\N,Missing
C02-1017,C00-2135,0,\N,Missing
C02-1017,C00-1014,0,\N,Missing
C02-1017,C96-1023,0,\N,Missing
C02-1017,W01-1401,1,\N,Missing
C02-1017,C92-2101,0,\N,Missing
C02-1017,C94-1091,0,\N,Missing
C02-1017,P91-1024,1,\N,Missing
C02-1017,P93-1004,0,\N,Missing
C02-1017,C96-1078,0,\N,Missing
C02-1017,takezawa-etal-2002-toward,1,\N,Missing
C08-3006,W05-0909,0,0.0284034,"70.70 – 89.73 67.33 72.19 69.14 68.32 64.55 62.91 70.81 77.62 languages like Danish, German, English, Spanish, etc. does not differ much. Moreover, languages with phrasal segments and/or rich morphology like Arabic, Malay, Russian or Vietnamese have a high total entropy and thus can be expected to be more difficult to translate. This is confirmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) where scores range between 0 (worst) and 1 (best). Besides Korean (single references only), all languages were evaluated using 16 reference translations. The evaluation results in Table 3 show that closely related language pairs like Japanese-Korean or Portuguese-Brazilian can be translated very accurately, whereas translations into languages with high total entropy are of lower quality. 3 Multilingual Speech Translation Service (MSTS) The speech translation service1 can be accessed via ‘http://www.atr-trek.co.jp/contents html’ or using the QR code in Figure 4 that also illustrates the graphi"
C08-3006,2007.iwslt-1.15,1,0.895955,"Missing"
C08-3006,P02-1040,0,0.0823612,"65.83 69.61 75.39 72.17 72.82 69.00 70.70 – 89.73 67.33 72.19 69.14 68.32 64.55 62.91 70.81 77.62 languages like Danish, German, English, Spanish, etc. does not differ much. Moreover, languages with phrasal segments and/or rich morphology like Arabic, Malay, Russian or Vietnamese have a high total entropy and thus can be expected to be more difficult to translate. This is confirmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) where scores range between 0 (worst) and 1 (best). Besides Korean (single references only), all languages were evaluated using 16 reference translations. The evaluation results in Table 3 show that closely related language pairs like Japanese-Korean or Portuguese-Brazilian can be translated very accurately, whereas translations into languages with high total entropy are of lower quality. 3 Multilingual Speech Translation Service (MSTS) The speech translation service1 can be accessed via ‘http://www.atr-trek.co.jp/contents html’ or using the QR code in Fig"
C08-3006,2008.iwslt-evaluation.11,1,\N,Missing
C18-1220,kamholz-etal-2014-panlex,0,0.0253597,"al., 2018; Moritz and B¨uchler, 2017). Therefore, the requirement of parallel/comparable corpora for multilingual topic models limits their usage in many situations. Another line of research focuses on using multilingual dictionaries as supervision (Ma and Nasukawa, 2017; Guti´errez et al., 2016; Liu et al., 2015; Jagarlamudi and Daum´e III, 2010; Boyd-Graber and Blei, 2009). In contrast to parallel corpora, dictionaries are widely available and often easy to obtain. PAN L EX, a free online dictionary database, for example, covers 5,700 languages and more than one billion dictionary entries (Kamholz et al., 2014; Baldwin et al., 2010).3 Thus, a multilingual topic model built on a dictionary rather than a parallel corpus is potentially applicable to more languages. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 https://meta.wikimedia.org/wiki/List_of_Wikipedias 2 Reported by United Bible Societies at https://www.unitedbiblesocieties.org/ 3 https://panlex.org/ 2595 Proceedings of the 27th International Conference on Computational Linguistics, pages 2595–2609 Santa Fe, New Mexico, USA, August 20-26, 2"
C18-1220,C12-1089,0,0.0677407,"a held-out portion of Wikipedia. CNPMI is an intrinsic evaluation, so it is only available for the training sets, WIKI - PACO and WIKI - INCO . 5.3.2 Extrinsic Evaluation: Crosslingual Classification A successful multilingual topic model should provide informative features for crosslingual tasks. To show that our model is beneficial to downstream applications, we use crosslingual document classification to evaluate topic model performance. A high classification accuracy when testing on a different language from training indicates topic consistency across languages (Hermann and Blunsom, 2014; Klementiev et al., 2012; Smet et al., 2011). As in other studies on multilingual topic models, we first train topic models on a bilingual corpus D(`1 ,`2 ) , and then use topic-word distributions φ(`1 ) and φ(`2 ) to infer document-topic distributions on unseen documents D0 (`1 ) and D0 (`2 ) . Thus, a classifier is trained on θd`1 with corresponding labels where d`1 ∈ D0 (`1 ) , and tested on θd`2 where d`2 ∈ D0 (`2 ) , and vice versa. In our experiments, we use WIKI PACO and WIKI - INCO to train topic models first, and then perform inference on either TED + TED (both English and non-English documents from TED) or"
C18-1220,2005.mtsummit-papers.11,0,0.0638929,"2015), cultural difference discovery (Shutova et al., 2017; Guti´errez et al., 2016), translation detection (Krstovski et al., 2016; Krstovski and Smith, 2016), and others (Barrett et al., 2016; Agi´c et al., 2016; Hintz and Biemann, 2016). Typical probabilistic multilingual topic models are based on Latent Dirichlet Allocation (LDA, Blei et al. (2003)), adding supervision on connections between languages. Most models achieve this by making strong assumptions on the training data—they either require a parallel corpus that has sentence-aligned documents in different languages (e.g., EuroParl, Koehn (2005)), or a comparable corpus that has documents of similar content (e.g., Wikipedia articles paired across languages). These training requirements limit the usage of such models: an adequately large parallel corpus is difficult to obtain, particularly for low-resource languages. For example, only 300 languages are available on Wikipedia,1 and only 250 languages have more than 1,000 articles. Another common choice for parallel corpus in multilingual research, the Bible, is available in 2,530 languages (Agi´c et al., 2015).2 However, studies show that its archaic themes and small corpus size (1,189"
C18-1220,N16-1132,0,0.170533,"ctively learns coherent multilingual topics from partially and fully incomparable corpora with limited amounts of dictionary resources. 1 Introduction Multilingual topic models provide an overview of document structures in multilingual corpora, by learning language-specific versions of each topic (Figure 1). Their simplicity, efficiency and interpretability make models from this family popular for various crosslingual tasks, e.g., feature extraction (Liu et al., 2015), cultural difference discovery (Shutova et al., 2017; Guti´errez et al., 2016), translation detection (Krstovski et al., 2016; Krstovski and Smith, 2016), and others (Barrett et al., 2016; Agi´c et al., 2016; Hintz and Biemann, 2016). Typical probabilistic multilingual topic models are based on Latent Dirichlet Allocation (LDA, Blei et al. (2003)), adding supervision on connections between languages. Most models achieve this by making strong assumptions on the training data—they either require a parallel corpus that has sentence-aligned documents in different languages (e.g., EuroParl, Koehn (2005)), or a comparable corpus that has documents of similar content (e.g., Wikipedia articles paired across languages). These training requirements limi"
C18-1220,N16-1053,0,0.552789,"that our new method effectively learns coherent multilingual topics from partially and fully incomparable corpora with limited amounts of dictionary resources. 1 Introduction Multilingual topic models provide an overview of document structures in multilingual corpora, by learning language-specific versions of each topic (Figure 1). Their simplicity, efficiency and interpretability make models from this family popular for various crosslingual tasks, e.g., feature extraction (Liu et al., 2015), cultural difference discovery (Shutova et al., 2017; Guti´errez et al., 2016), translation detection (Krstovski et al., 2016; Krstovski and Smith, 2016), and others (Barrett et al., 2016; Agi´c et al., 2016; Hintz and Biemann, 2016). Typical probabilistic multilingual topic models are based on Latent Dirichlet Allocation (LDA, Blei et al. (2003)), adding supervision on connections between languages. Most models achieve this by making strong assumptions on the training data—they either require a parallel corpus that has sentence-aligned documents in different languages (e.g., EuroParl, Koehn (2005)), or a comparable corpus that has documents of similar content (e.g., Wikipedia articles paired across languages). Thes"
C18-1220,E14-1056,0,0.169267,"CLINK, we set βr = 0.01 for priors from root to internal nodes, and βi = 100 from internal nodes i to leaves, following Hu et al. (2014). 5.3 Evaluation Metrics We evaluate each model in two ways. Experimental results below are averaged across all language pairs. 2600 5.3.1 Intrinsic Evaluation: Multilingual Topic Coherence Typical topic model evaluations include intrinsic and extrinsic measurements. Intrinsic evaluation focuses on topic quality or coherence of the trained topics. The most widely-used metric for measuring monolingual topic coherence is normalized pointwise mutual information (Lau et al., 2014; Newman et al., 2010). Hao et al. (2018) proposed crosslingual normalized pointwise mutual information (CNPMI) by extending this idea to multilingual settings, which correlates well with bilingual speakers’ judgments on topic quality. Given a bilingual topic k in languages `1 and `2 , and a parallel reference corpus R(`1 ,`2 ) , the CNPMI of topic k is calculated as:   (`1 ) (`2 ) C Pr w , w X i j 1 1   · log     CNPMI (`1 , `2 , k) = (6) (` ) (` ) (` ) (` ) C2 log Pr w 1 , w 2 Pr w 1 Pr w 2 i,j i j i j (`) where C is the cardinality of a topic, i.e., the C most probable words in the"
C18-1220,D09-1092,0,0.61431,"Missing"
C18-1220,W17-0505,0,0.0217971,"Missing"
C18-1220,N10-1012,0,0.0329465,"0.01 for priors from root to internal nodes, and βi = 100 from internal nodes i to leaves, following Hu et al. (2014). 5.3 Evaluation Metrics We evaluate each model in two ways. Experimental results below are averaged across all language pairs. 2600 5.3.1 Intrinsic Evaluation: Multilingual Topic Coherence Typical topic model evaluations include intrinsic and extrinsic measurements. Intrinsic evaluation focuses on topic quality or coherence of the trained topics. The most widely-used metric for measuring monolingual topic coherence is normalized pointwise mutual information (Lau et al., 2014; Newman et al., 2010). Hao et al. (2018) proposed crosslingual normalized pointwise mutual information (CNPMI) by extending this idea to multilingual settings, which correlates well with bilingual speakers’ judgments on topic quality. Given a bilingual topic k in languages `1 and `2 , and a parallel reference corpus R(`1 ,`2 ) , the CNPMI of topic k is calculated as:   (`1 ) (`2 ) C Pr w , w X i j 1 1   · log     CNPMI (`1 , `2 , k) = (6) (` ) (` ) (` ) (` ) C2 log Pr w 1 , w 2 Pr w 1 Pr w 2 i,j i j i j (`) where C is the cardinality of a topic, i.e., the C most probable words in the topic-word distributi"
C18-1220,Q15-1004,1,0.862278,"ope. 4.2.2 Dynamic Focusing: an Annealing Method Static focusing treats transfer distributions δ as fixed parameters during sampling, and it is difficult to decide how sparse a transfer distribution should be to achieve optimal performance. Therefore, we propose dynamic focusing, where we avoid choosing a specific focal threshold and selection scope. Specifically, we adjust the transfer distribution during inference dynamically, beginning with a dense transfer distribution and iteratively sharpening the distribution using deterministic annealing (Ueda and Nakano, 1994; Smith and Eisner, 2006; Paul and Dredze, 2015). (t) Assume at iteration t, the transfer distribution for a document d`2 is denoted as δd`2 . Then at iteration  0  1/τ (t ) (t) t0 , we anneal its transfer distribution by δd`2 ∝ δd`2 where τ is a fixed temperature, which d`1 d`1 we set to 0.9 in our experiments. We start with non-focused transfer distributions, and apply annealing at scheduling intervals during Gibbs sampling. Designing an effective annealing schedule is critical. We propose two schedules below. Fixed Schedule. The simplest schedule is to apply annealing for all transfer distributions every I iterations. In our experime"
C18-1220,D10-1025,0,0.0327725,"nslation in Swedish, o¨ , should be similar, i.e., Pr islandEN |φk ≈ Pr o¨SV |φk . Most multilingual topic models extend LDA with one or both of two types of “link” information: document translations and word translations. Document Links. The polylingual topic model (Mimno et al., 2009; Ni et al., 2009) assumes that during the generative process, a topic distribution θd generates a tuple of comparable documents in  (` ) (` ) 1 L different languages, i.e., d = d , . . . , d and each language ` has its own topic-word distributions, (`) φk . This model has been widely used (Vuli´c et al., 2013; Platt et al., 2010; Smet and Moens, 2009), but it requires a parallel/comparable corpus in order to link documents. Vocabulary Links. Another type of model uses word translations (Jagarlamudi and Daum´e III, 2010; Boyd-Graber and Blei, 2009) rather than linking documents. A multilingual dictionary is used to construct a tree structure where each internal node contains word translations, and applies hyper-Dirichlet type I distributions to generate words (Andrzejewski et al., 2009; Minka, 1999; Dennis III, 1991). For each topic k, a distribution from root r to all the internal nodes  iis drawn by φk,r ∼ Dir(βr"
C18-1220,P06-1072,0,0.0593046,"ners as the selection scope. 4.2.2 Dynamic Focusing: an Annealing Method Static focusing treats transfer distributions δ as fixed parameters during sampling, and it is difficult to decide how sparse a transfer distribution should be to achieve optimal performance. Therefore, we propose dynamic focusing, where we avoid choosing a specific focal threshold and selection scope. Specifically, we adjust the transfer distribution during inference dynamically, beginning with a dense transfer distribution and iteratively sharpening the distribution using deterministic annealing (Ueda and Nakano, 1994; Smith and Eisner, 2006; Paul and Dredze, 2015). (t) Assume at iteration t, the transfer distribution for a document d`2 is denoted as δd`2 . Then at iteration  0  1/τ (t ) (t) t0 , we anneal its transfer distribution by δd`2 ∝ δd`2 where τ is a fixed temperature, which d`1 d`1 we set to 0.9 in our experiments. We start with non-focused transfer distributions, and apply annealing at scheduling intervals during Gibbs sampling. Designing an effective annealing schedule is critical. We propose two schedules below. Fixed Schedule. The simplest schedule is to apply annealing for all transfer distributions every I ite"
D09-1146,D08-1038,0,0.0262081,"assentation that each topic is in some way shared across all collections. However, it does not explicitly model the similarities and differences between collections as we do in this research. In computational linguistics, topic models have been used in various applications, such as predicting response to political webposts (Yano et al., 2009), analyzing Enron and academic emails (McCallum et al., 2007a), analyzing voting records and corresponding text of resolutions from the U.S. Senate and the U.N. (McCallum et al., 2007b), as well as studying the history of ideas in various research fields (Hall et al., 2008; Paul and Girju, 2009). To our knowledge, the application of topic models to identifying cross-cultural differences is novel. 3 The Model In this section we first review the basic pLSI and LDA models. We then introduce our extension to LDA: cross-collection LDA (ccLDA). 3.1 Basic Topic Modeling The most basic generative model that assumes document topicality is the standard Na¨ıve Bayes model, where each document is assumed to belong to exactly one topic, and each topic is associated with a probability distribution over words (Mitchell, 1997). While this single-topic approach can be sufficien"
D09-1146,R09-1061,1,0.737693,"ch topic is in some way shared across all collections. However, it does not explicitly model the similarities and differences between collections as we do in this research. In computational linguistics, topic models have been used in various applications, such as predicting response to political webposts (Yano et al., 2009), analyzing Enron and academic emails (McCallum et al., 2007a), analyzing voting records and corresponding text of resolutions from the U.S. Senate and the U.N. (McCallum et al., 2007b), as well as studying the history of ideas in various research fields (Hall et al., 2008; Paul and Girju, 2009). To our knowledge, the application of topic models to identifying cross-cultural differences is novel. 3 The Model In this section we first review the basic pLSI and LDA models. We then introduce our extension to LDA: cross-collection LDA (ccLDA). 3.1 Basic Topic Modeling The most basic generative model that assumes document topicality is the standard Na¨ıve Bayes model, where each document is assumed to belong to exactly one topic, and each topic is associated with a probability distribution over words (Mitchell, 1997). While this single-topic approach can be sufficient for classification ta"
D09-1146,N09-1054,0,0.0122354,"n. Wang et al. recently introduced Markov topic models (MTM) (2009), a family of models which can simultaneously learn the topic structure of a single collection while discovering correlated topics in other collections. This is promising in that this type of model makes no assentation that each topic is in some way shared across all collections. However, it does not explicitly model the similarities and differences between collections as we do in this research. In computational linguistics, topic models have been used in various applications, such as predicting response to political webposts (Yano et al., 2009), analyzing Enron and academic emails (McCallum et al., 2007a), analyzing voting records and corresponding text of resolutions from the U.S. Senate and the U.N. (McCallum et al., 2007b), as well as studying the history of ideas in various research fields (Hall et al., 2008; Paul and Girju, 2009). To our knowledge, the application of topic models to identifying cross-cultural differences is novel. 3 The Model In this section we first review the basic pLSI and LDA models. We then introduce our extension to LDA: cross-collection LDA (ccLDA). 3.1 Basic Topic Modeling The most basic generative mode"
D10-1007,N10-1122,0,0.0398014,"LDA-style probabilistic topic models of document content (Blei et al., 2003) have been shown to offer state-of-the-art summarization quality. Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009). In our case, we want to add more structure to a model to incorporate the notion of viewpoint/perspective into our summaries. When it comes to extracting viewpoints, recent research suggests that it may be beneficial to model both topics and perspectives, as sentiment may be expressed differently depending on the issue involved (Brody and Elhadad, 2010; Paul and Girju, 2010). For example, let’s consider a set of product reviews for a home theater system. Content topics in this data might include things like sound quality, usability, etc., while the viewpoints might be the positive and negative sentiments. A word like speakers, for instance depends on the sound topic but not a viewpoint, while good would be an example of a word that depends on a viewpoint but not any particular topic. A word like loud would depend on both (since it would be considered positive sentiment only in the context of the sound quality topic), while a word like think"
D10-1007,N09-1057,0,0.0207838,"sing features returned by a dependency parser. For this, we used the Stanford parser1 , which returns dependency tuples of the form rel(a, b) where rel is some dependency relation and a and b are tokens of a sentence. We can use these specific tuples as features, referred here as the full-tuple representation. One problem with this representation is that we are using very specific information and it is harder for learning algorithms to find patterns due to the lack of redundancy. One solution is to generalize these features and rewrite a tuple rel(a, b) as two tuples: rel(a, ∗) and rel(∗, b) (Greene and Resnik, 2009; Joshi and Ros´e, 2009). We will refer to this as the split-tuple representation. 2.2.3 Negation If a word wi appears in the head of a neg relation, then we would like this to be reflected in other dependency tuples in which wi occurs. For a tuple rel(wi , wj ), if either wi or wj is negated, then we simply rewrite it as ¬rel(wi , wj ). An alternative would be to rewrite the individual word wi as ¬wi . However in our experiments this representation produced worse accuracies, perhaps because this produces less redundancy. 1 http://nlp.stanford.edu/software/ 2.2.4 Polarity We also hypothesize t"
D10-1007,N09-1041,0,0.122569,"model for more complex linguistic features extracted from text. These are 67 more discriminative than single word tokens and can improve the accuracy of extracting multiple viewpoints as we will show in the experimental results’ section. Below we first give a brief introduction to TAM and then present the proposed set of features. 2.1 Topic-Aspect Model (TAM) LDA-style probabilistic topic models of document content (Blei et al., 2003) have been shown to offer state-of-the-art summarization quality. Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009). In our case, we want to add more structure to a model to incorporate the notion of viewpoint/perspective into our summaries. When it comes to extracting viewpoints, recent research suggests that it may be beneficial to model both topics and perspectives, as sentiment may be expressed differently depending on the issue involved (Brody and Elhadad, 2010; Paul and Girju, 2010). For example, let’s consider a set of product reviews for a home theater system. Content topics in this data might include things like sound quality, usability, etc., while the viewpoints might be the positive and negativ"
D10-1007,P09-2079,0,0.0107585,"Missing"
D10-1007,N09-2029,0,0.0310303,"ontrastive macro and micro multi-view summaries in an unsupervised way, which is the goal of our work. For example, Hu and Liu (2006) rank sentences based on their dominant sentiment according to the polarity of adjectives occuring near a product feature in a sentence. A contradiction occurs when two sentences are highly unlikely to be simultaneously true (cf. (Marneffe et al., 2008)). Although little work has been done on contradiction detection, there are a few notable approaches (Harabagiu et al., 2006; Marneffe et al., 2008; Kim and Zhai, 2009). The closest work to ours is perhaps that of Lerman and McDonald (2009) who present an approach to contrastive summarization. They add an objective to their summarization model such that the summary model for one set of text is different from the model for the other set. The idea is to highlight the key differences between the sets, however this is a different type of contrast than the one we study here – our goal is instead to make the summaries similar to each other, to contrast how the same information is conveyed through different viewpoints. In this paper, we propose a two-stage approach to solving this novel summarization problem, which will be explained in"
D10-1007,W06-2915,0,0.0461805,"Missing"
D10-1007,W04-1013,0,0.0669811,"are more (rather than less) similar to each other. This contrastive version of our model-based baseline is formulated as: − k X KL(L(Sm1 )||L(Xm1 )) + m1 =1  1 k−1 P m2 ∈[1,k],m1 6=m2 KL(L(Sm1 )||L(Xm2 ))  Our summary generation algorithm is to iteratively add excerpts to the summary in a greedy fashion, selecting the excerpt with the highest score in each iteration. Note that this approach only generates macro-level summaries, leaving us with the LexRank baseline for micro-level summaries. 4.3.3 Metrics We will evaluate our summaries using a variant of the standard ROUGE evaluation metric (Lin, 2004). Recall that we have two different evaluation sets – one that contains all of the reasons for each view73 point, and one that consists only of aligned pairs of excerpts. Since the same excerpt may appear in multiple pairs, there would be significant redundancy in our reference summary if we were to include every pair. Thus, we will restrict a contrastive reference summary to exclude overlapping pairs, and we will have many reference sets for all possible combinations of pairs. There is only one reference set for the representativeness criterion. Our reference summaries have a unique property"
D10-1007,P08-1118,0,0.070202,"Missing"
D10-1007,H05-1044,0,0.00808263,"iments this representation produced worse accuracies, perhaps because this produces less redundancy. 1 http://nlp.stanford.edu/software/ 2.2.4 Polarity We also hypothesize that lexical polarity information may improve our model. If we are using the full-tuple representation, then a tuple becomes more general by replacing the specific word with a + or −. In the case that both words are polarity words, we use two tuples, replacing only one word at a time rather than replacing both words with their polarity signs. To determine the polarity of a word, we simply use the Subjectivity Clues lexicon (Wilson et al., 2005) and as polarity values, positive (+), negative (-), and neutral (*). Under our split-tuple representation, this becomes more specific by replacing the ∗ with the polarity sign. For example, the tuple amod(idea, good) would be represented as amod(idea, +) and amod(∗, good). We collapse negated features to flip the polarity sign such that ¬rel(a, +) becomes rel(a, −). 2.2.5 Generalized Relations We also experimented with backing off the relations themselves. Since the Stanford dependencies can be organized in a hierarchy2 , we will represent the relations at more generalized levels in the hiera"
D12-1009,P06-1026,0,0.0392577,"Missing"
D12-1009,N04-1015,0,0.0160934,"b and nb is the total number of tokens in block b. Standard optimization methods can be used to learn these parameters. In our experiments, we find that we obtain good results by simply performing a single iteration of gradient ascent after each sampling iteration t,3 with the following update: (t+1) λzk (t) = λzk + η(t) ∂` ∂λzk (4) where η is a step size function. 4 Related Work Hidden Markov models have a recent history as simple models of document structure. Stolcke et al. (2000) used HMMs as a general model of discourse with an application to speech acts (or dialog acts) in conversations. Barzilay and Lee (2004) applied HMMs as an unsupervised model of discourse. This work used HMMs to model the progression of sentences in articles, and was shown to be useful for ordering sentences and generating summaries of news articles. More recently, Wang et al. (2011b) experimented with similar tasks using a related HMMbased model called the Structural Topic Model. Unsupervised HMMs were applied to conversational data by Ritter et al. (2010) who experimented with Twitter conversations. The authors also experimented with incorporating a topic model on top of the HMM to distinguish speech acts from topical cluste"
D12-1009,N09-1042,0,0.0118223,"ing a topic model on top of the HMM to distinguish speech acts from topical clusters, with mixed results. Joty et al. (2011) extended this work by enriching the emission distributions and using additional features such as speaker and position information. An approach to unsupervised discourse modeling that does not use HMMs is 3 Incremental updates are justified under the generalized EM algorithm (Dempster et al., 1977). Each gradient step with respect to λ corresponds to a generalized M-step, while each sampling iteration corresponds to a stochastic E-step. 98 the latent permutation model of Chen et al. (2009). This model assumes each segment (e.g. paragraph) in a document is associated with a latent class or topic, and the ordering of topics within a document is modeled as a deviation from some canonical ordering. Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model (Hospedales et al., 2009), which allows each HMM state to be associated with its own distribution over topics in a topic model. Like the block HMM, this still assumes a relatively small number of HMM states, but with an extra layer of latent variables before"
D12-1009,W04-3240,0,0.274934,"Missing"
D12-1009,P05-1045,0,0.0288604,"Missing"
D12-1009,P07-1094,0,0.034267,"sider 36K conversation threads for a total of 100K messages with average length 13.4 tokens. Both data sets are already annotated with the reply structure, so the discourse graph is given. We preprocess the data by treating contiguous blocks of punctuation as tokens, and we remove infrequent words. The Twitter corpus has some additional preprocessing, such as converting URLs to a single word type. 5.2 Baseline Models Our work is motivated by the Bayesian HMM approach of Ritter et al. (2010) – the model we refer to as the block HMM (BHMM) – and we consider this our primary baseline. (See also (Goldwater and Griffiths, 2007) for more details on Bayesian HMMs with Dirichlet priors.) We also compare against LDA, which makes latent assignments at the token-level, but blocks of text are independent of 5 Three messages in this corpus have multiple parents. For the sake of conciseness, we simply remove these threads rather than introducing a method to model multiple parents. 99 Incorporating Background Distributions In our experiments, we find that the intrusion of common stop words can make the results difficult to interpret, but we do not want to perform simple stop word removal because common function words often pl"
D12-1009,W10-2923,0,0.058912,"rd Gibbs samplers for both baseline models, and we optimize the Dirichlet hyperparameters (for the transition and topic distributions) using Minka’s fixed-point iterations (2003). Experiments with Conversation Data 5.3 We experiment with two corpora of text-based asynchronous conversations on the Web. One of these is annotated with speech act labels, against which we compare our unsupervised clusters. We measure the predictive capabilities of the model via perplexity experiments and the task of thread reconstruction. 5.1 Data Sets First, we use a corpus of discussion threads from CNET forums (Kim et al., 2010), which are mostly technical discussion and support. This corpus includes 321 threads and a total of 1309 messages, with an average message length of 78 tokens after preprocessing.5 Second, we use the Twitter data set created by Ritter et al. (2010). We consider 36K conversation threads for a total of 100K messages with average length 13.4 tokens. Both data sets are already annotated with the reply structure, so the discourse graph is given. We preprocess the data by treating contiguous blocks of punctuation as tokens, and we remove infrequent words. The Twitter corpus has some additional prep"
D12-1009,D11-1069,0,0.266131,"Missing"
D12-1009,N10-1020,0,0.692703,"d, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics α α θ1 θ2 θ3 ... π1 π2 π3 ... ... z1 z2 z3 ... z1 z2 z3 ... ... w1 ... w1 π z1 z2 w1 z3 w2 N w3 N (a) Block HMM N λ w2 N w3 N N (b) LDA w2 N ... w3 N N (c) M4 Figure 1: The graphical models for the block HMM (left) where each block of tokens depends on exactly one latent class, LDA (center) where each token individually depends on a latent class, and M4 (right) where the class distributions are dependent across blocks. Some parameters are omitted for simplicity. This figure depicts the Bayesian variant of the block HMM (Ritter et al., 2010) where the transition distributions π depend on a Dirichlet(α) prior. govern the transitions between text blocks in a sequence. We generalize the block HMM approach so that there is no longer a one-to-one correspondence between states in the Markov chain and latent discourse classes. Instead, we allow a state in the HMM to correspond to a mixture of many classes: we refer to this family of models as mixed membership Markov models (M4 ). Instead of defining explicit transition probabilities from one class to another as in a traditional HMM, we define the distribution over classes as a function"
D12-1009,P95-1005,0,0.258741,"Missing"
D12-1009,J00-3003,0,0.736162,"Missing"
D12-1009,P11-1153,0,0.431036,"n baseline models. 1 Introduction The proliferation of social media in recent years has lead to an increased use of informal Web data in the language processing community. With this rising interest in social domains, it is natural to consider models which explicitly incorporate the conversational patterns of social text. Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (Elsas and Carbonell, 2009; Seo et al., 2009), question answering and expert finding (Xu et al., 2008; Wang et al., 2011a), and interpersonal relationship identification (Diehl et al., 2007). While conversational features may be important, Web-derived corpora are not always annotated with While block HMMs offer a concise model of inter-message structure, they have the limitation that each text block (message) belongs to exactly one class. Many modern generative models of text, in contrast, allow documents to contain many latent classes. For example, topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) assume each document has its own distribution over multiple classes (often called “topics"
D12-1009,D11-1002,0,0.244478,"n baseline models. 1 Introduction The proliferation of social media in recent years has lead to an increased use of informal Web data in the language processing community. With this rising interest in social domains, it is natural to consider models which explicitly incorporate the conversational patterns of social text. Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (Elsas and Carbonell, 2009; Seo et al., 2009), question answering and expert finding (Xu et al., 2008; Wang et al., 2011a), and interpersonal relationship identification (Diehl et al., 2007). While conversational features may be important, Web-derived corpora are not always annotated with While block HMMs offer a concise model of inter-message structure, they have the limitation that each text block (message) belongs to exactly one class. Many modern generative models of text, in contrast, allow documents to contain many latent classes. For example, topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) assume each document has its own distribution over multiple classes (often called “topics"
D19-1349,W13-0102,0,0.361098,"ign detection (Paul and Dredze, 2014) and medical issue analysis (Huang et al., 2015, 2017). To reliably utilize topic models trained for these tasks, we need to evaluate them carefully and ensure that they have as high quality as possible. When topic models are used in an extrinsic task, like text categorization, they can be assessed by measuring how effectively they contribute to that task (Chen et al., 2016; Huang et al., 2015). However, when they are generated for human consumption, their evaluation is more challenging. In such cases, interpretability is critical, and Chang et al. (2009); Aletras and Stevenson (2013) have shown that the standard way to evaluate the output of a probabilistic model, by measuring perplexity on held-out data (Wallach et al., 1 Our code and data are available here. 2009), does not imply that the inferred topics are human-interpretable. A topic inferred by LDA is typically represented by the set of words with the highest probability given the topic. With this characteristic, we can evaluate the topic quality by determining how coherent the set of topic words is. While a variety of techniques (Section 2) have been geared towards measuring the topic quality in this way, in this p"
D19-1349,D18-1098,0,0.0454009,"s the measure of Inter-Annotator Agreement (IAA) for three datasets. The average human rating score/IAA for 20NG, Wiki and NYT are 2.91/0.71, 3.23/0.82 and 3.06/0.69, respectively. 5.2 Experimental Design Topic Modeling Following the settings in Xing and Paul (2018), we ran the LDA Gibbs samplers for 2,000 iterations (Griffiths and Steyvers, 2004) for each datasets, with 1,000 burn-in iterations, collecting samples every 10 iterations for the final 1,000 iterations. The set of estimates Θ thus contains 100 samples. Estimator Training was performed following the cross-domain training strategy (Bhatia et al., 2018). With the ground truth (human judgments), we train the estimator on all topics over one dataset, and test it on another (one-to-one). To enlarge the training set, we also train the estimator on two datasets merged together and test 6 https://www.figure-eight.com/ it on the third one (two-to-one). Given the limited amount of data and the need for interpretability, we experimented only with non-neural classifiers, including linear regression, nearest neighbors regression, Bayesian regression, and Support Vector Regression (SVR) using sklearn (Pedregosa et al., 2011); we report the results with"
D19-1349,D08-1038,0,0.0607881,"e variability of the posterior distributions. Compared to several existing baselines for automatic topic evaluation, the proposed metric achieves state-of-the-art correlations with human judgments of topic quality in experiments on three corpora.1 We additionally demonstrate that topic quality estimation can be further improved using a supervised estimator that combines multiple metrics. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003) topic modeling has been widely used for NLP tasks which require the extraction of latent themes, such as scientific article topic analysis (Hall et al., 2008), news media tracking (Roberts et al., 2013), online campaign detection (Paul and Dredze, 2014) and medical issue analysis (Huang et al., 2015, 2017). To reliably utilize topic models trained for these tasks, we need to evaluate them carefully and ensure that they have as high quality as possible. When topic models are used in an extrinsic task, like text categorization, they can be assessed by measuring how effectively they contribute to that task (Chen et al., 2016; Huang et al., 2015). However, when they are generated for human consumption, their evaluation is more challenging. In such case"
D19-1349,N18-1099,1,0.901672,"e of work exploits the co-occurrence statistics indirectly. Aletras and Stevenson (2013) devised a new method by mapping the topic words into a semantic space and then computing the pairwise distributional similarity (DS) of words in that space. However, the semantic space is still built on PMI or NPMI. Roder et al. (2015) studied a unifying framework to explore a set of co-occurrence based topic quality measures and their parameters, identifying two complex combinations, (named CV and CP in that paper2 ), as the best performers on their test corpora. Posterior Based Method Recently, Xing and Paul (2018) analyzed how the posterior of LDA parameters vary during Gibbs sampling inference (Geman and Geman, 1984; Griffiths and Steyvers, 2004) and proposed a new topic quality measurement named Topic Stability. The Gibbs sampling for LDA generates estimates for two distributions: for topics given a document (θ), and for words given a topic (φ). Topic stability considers φ and is defined as: stability(Φk ) = 1 X sim(φk , φ¯k ) (1) |Φk | φk ∈Φk The stability of topic k is computed as the mean cosine similarity between the mean (φ¯k ) of all the sampled topic k’s distribution estimates (Φk ) and topic"
D19-1349,Y15-1064,0,0.0259867,"ves state-of-the-art correlations with human judgments of topic quality in experiments on three corpora.1 We additionally demonstrate that topic quality estimation can be further improved using a supervised estimator that combines multiple metrics. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003) topic modeling has been widely used for NLP tasks which require the extraction of latent themes, such as scientific article topic analysis (Hall et al., 2008), news media tracking (Roberts et al., 2013), online campaign detection (Paul and Dredze, 2014) and medical issue analysis (Huang et al., 2015, 2017). To reliably utilize topic models trained for these tasks, we need to evaluate them carefully and ensure that they have as high quality as possible. When topic models are used in an extrinsic task, like text categorization, they can be assessed by measuring how effectively they contribute to that task (Chen et al., 2016; Huang et al., 2015). However, when they are generated for human consumption, their evaluation is more challenging. In such cases, interpretability is critical, and Chang et al. (2009); Aletras and Stevenson (2013) have shown that the standard way to evaluate the output"
D19-1349,E14-1056,0,0.0390238,"to evaluate the quality of LDA topic models: Co-occurrence Based Methods and Posterior Based Methods. Co-occurrence Based Methods Most prominent topic quality evaluations use various pairwise co-occurrence statistics to estimate topic’s semantic similarity. Mimno et al. (2011) proposed the Coherence metric, which is the summation of the conditional probability of each topic word given all other words. Newman et al. (2010) showed that the summation of the pairwise pointwise mutual information (PMI) of all possible topic word pairs is also an effective metric to assess topic quality. Later, in Lau et al. (2014), PMI was replaced 3471 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3471–3477, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 1: Two example topics and their distributions of µ, σ and cv from the NYT corpus. Two topics are: Topic1 (in blue): {financial, banks, bank, money, debt, fund, loans, investors, funds, hedge}. Topic2 (in orange): {world, one, like, good, even, know, think, get, many, got}. Their human rating scores are"
D19-1349,D11-1024,0,0.0895915,"Missing"
D19-1349,N10-1012,0,0.237904,"opic word co-occurrence. Our novel estimator further improves the topic quality assessment on two out of the three corpora we have. 2 Automatic Topic Quality Evaluation There are two common ways to evaluate the quality of LDA topic models: Co-occurrence Based Methods and Posterior Based Methods. Co-occurrence Based Methods Most prominent topic quality evaluations use various pairwise co-occurrence statistics to estimate topic’s semantic similarity. Mimno et al. (2011) proposed the Coherence metric, which is the summation of the conditional probability of each topic word given all other words. Newman et al. (2010) showed that the summation of the pairwise pointwise mutual information (PMI) of all possible topic word pairs is also an effective metric to assess topic quality. Later, in Lau et al. (2014), PMI was replaced 3471 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3471–3477, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 1: Two example topics and their distributions of µ, σ and cv from the NYT corpus. Two topics are: Topic1 (in blu"
E03-1048,P02-1040,0,0.0780651,"ere an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the RED rank, and the BLEU score. While RED accords with HUMAN, BLEU fails to agree with HUMAN in the EJ evaluation. One reason for this is that the BLEU score favors SAT translations in that they are more similar to the reference translation from the viewpoint of Ngrams. Table 1 Quality Evaluation of Three MTs 5 (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations (Papineni et al., 2002). It ranges from 1.0 (best) down to 0.0 (worst). (3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC 4 . A translator compared MT translations with human ones, then, MT&apos;s proficiency is estimated by regression analysis (Sugaya et al., 2000). It ranges from 10 (lowest) to 990 points (perfect). 3.3 Results Table 1 wraps up the results. So far, SMT has been applied mainly to language pairs of similar European languages. Skeptical opinions dominate about Average is calculated: A, B, C, and D are assigned values of 4"
E03-1048,2001.mtsummit-papers.3,1,0.832851,"e and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Verbmobil experiment (Ney, 2001) where an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the"
E03-1048,shimohata-sumita-2002-automatic,1,0.89347,"Missing"
E03-1048,C02-1076,1,0.819515,"ging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the source and target correspondences from the semantic point of view are maintained in a state-of-the-art translation system. However, the second assumption does not necessarily hold. To solve this problem, Akiba et al. (2002) used not only a language model but also a translation model of SMT derived from a corpus, and Sumita et al. (2002) exploited a corpus whose sentences are converted into semantic class sequences. These two selectors outperformed conventional selectors using the target N-gram in our experiments. 5 Paraphrasing and Filtering This section introduces another feature of C3 : paraphrasing and filtering corpora. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for"
E03-1048,1995.tmi-1.17,0,0.00827994,"w the HUMAN rank, as described above. Table 2. Sample of Translation Variety [B] Is the payment cash? Or is it the credit card? [A] Would you like to pay in cash or with a credit card? [C] Could you cash or credit card? In our experiment, while D3 , HPAT, and SAT for the E-to-J direction have A-ratios of 0.62, 0.55, and 0.53, respectively, the ideal selection would have an interestingly high A-ratio of 0.79. Thus, we could obtain a large increase in accuracy if it were possible to select the best one of the three different translations for each input sentence. Unlike other approaches such as (Brown and Frederking, 1995), we do not merge multiple results into a single one but we select the best one because the large difference between multiple translations for distant language pairs such as Japanese and English makes merging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the"
E03-1048,2001.mtsummit-papers.12,0,0.0767102,"Missing"
E03-1048,W02-1611,1,0.895885,"Missing"
E03-1048,suyaga-etal-2002-proposal,0,0.0182927,"or example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the coun"
E03-1048,2002.tmi-tutorials.2,0,0.0456358,"corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation"
E03-1048,W01-1401,1,0.84948,"stical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phra"
E03-1048,2002.tmi-papers.9,1,0.841258,"slation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phrasebooks for tourists. The size is about 150 thousand sentence pairs. A quality evaluation was done using a test set consisting of 345 sentences selected randomly from the above corpus, and the remaining sentences were used f"
E03-1048,1983.tc-1.13,0,0.350529,"Missing"
E03-1048,E03-1029,1,0.834211,"g. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the count of words in the translation pair. After abandoning the non-literal parts of the corpus, the acquisition of HPAT transfer patterns is done. The effect has been confirmed by an improvement in translation quality. 6 Conclusion Our project, called C3 , places corpora at the center of speech-to-speech technology. Good performance in translation components is demonstrated in the experiment"
E03-1048,1999.mtsummit-1.34,1,0.836105,"ezawa et al., 2002). 171 We used bilingual dictionaries and thesauri of about fifty thousand words for the travel domain. 3.2 Evaluation Measures We used the measures below. The BLEU score and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Ver"
E03-1048,W01-1405,0,0.048248,"g a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3 . Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual tree"
E03-1048,C02-1050,1,0.897796,"Missing"
E03-1048,J90-2002,0,\N,Missing
E03-1048,takezawa-etal-2002-toward,1,\N,Missing
federico-etal-2012-iwslt,niessen-etal-2000-evaluation,0,\N,Missing
federico-etal-2012-iwslt,N04-4038,0,\N,Missing
federico-etal-2012-iwslt,P02-1040,0,\N,Missing
federico-etal-2012-iwslt,W07-0734,0,\N,Missing
federico-etal-2012-iwslt,2005.mtsummit-papers.11,0,\N,Missing
federico-etal-2012-iwslt,O07-5005,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.10,0,\N,Missing
federico-etal-2012-iwslt,I05-3027,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.1,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.5,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.1,1,\N,Missing
I11-1091,N09-2056,1,0.759659,"Missing"
I11-1091,N07-1061,0,0.262952,"oaches heavily depends on the amount and coverage of bilingual language resources available to train the statistical models. There exist several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large text data sets are readily available. However, for less frequently used language pairs only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on SMT focuses on the usage of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008). Instead of a direct translation between two languages where only a 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 811 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 811–818, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP translation model entropy, reordering, monotonicity, engine performance) are investigated to determine the significance of each factor in predicting translation quality using linear regression analysis. 2 Table 1: Language Resources (European Languages) Language Da"
I11-1091,2008.iwslt-papers.1,0,0.311528,"age of bilingual language resources available to train the statistical models. There exist several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large text data sets are readily available. However, for less frequently used language pairs only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on SMT focuses on the usage of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008). Instead of a direct translation between two languages where only a 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 811 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 811–818, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP translation model entropy, reordering, monotonicity, engine performance) are investigated to determine the significance of each factor in predicting translation quality using linear regression analysis. 2 Table 1: Language Resources (European Languages) Language Danish da German de English en Spanish es Fre"
I11-1091,P07-1108,0,0.147268,"he amount and coverage of bilingual language resources available to train the statistical models. There exist several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large text data sets are readily available. However, for less frequently used language pairs only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on SMT focuses on the usage of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008). Instead of a direct translation between two languages where only a 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 811 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 811–818, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP translation model entropy, reordering, monotonicity, engine performance) are investigated to determine the significance of each factor in predicting translation quality using linear regression analysis. 2 Table 1: Language Resources (European Languages) Language Danish da German de E"
I11-1091,D08-1078,0,0.0649721,"third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness towards either the source or the target language. For most recent research efforts, English is the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality for certain language pairs (Paul et al., 2009; Leusch et al., 2010). Concerning the contribution of aspects of different language pairs on the quality of machine translation, (Birch et al., 2008) identified three features (morphological complexity, amount of reordering, historical relatedness) for predicting success of MT in translations between the official languages of the European Union. Moreover, (Koehn et al., 2009) investigated an additional feature (translation model complexity) using the JRC-Aquis corpus covering not only Indo-European languages, but also one semitic and three Finno-Ugric languages. This paper differs from previous research in the following aspects: we focus on the framework of pivot translation, where a target language translation of a source language input i"
I11-1091,2009.mtsummit-papers.7,0,0.237539,"choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality for certain language pairs (Paul et al., 2009; Leusch et al., 2010). Concerning the contribution of aspects of different language pairs on the quality of machine translation, (Birch et al., 2008) identified three features (morphological complexity, amount of reordering, historical relatedness) for predicting success of MT in translations between the official languages of the European Union. Moreover, (Koehn et al., 2009) investigated an additional feature (translation model complexity) using the JRC-Aquis corpus covering not only Indo-European languages, but also one semitic and three Finno-Ugric languages. This paper differs from previous research in the following aspects: we focus on the framework of pivot translation, where a target language translation of a source language input is obtained through an intermediate (pivot) language, investigate what factors make a good pivot language and what impact these factors have on the overall translation quality of language pairs not only including Indo-Euopean lang"
I11-1091,2010.iwslt-papers.12,0,0.283095,"Missing"
I11-1091,J03-1002,0,0.00320969,"periments, single translation references were used. Table 2: Pivot Language Dependency (European Languages) ru 57.1 57.9 59.4 58.3 57.3 52.3 44.7 55.3 Table 2 summarizes the BLEU score ranges of all pivot translation experiments obtained for a given pivot language. The results show a large variation in BLEU scores for all pivot languages indicating that the best pivot choice largely depends on the respective source and target language. For European pivot languages, the best language combination scores are in general much higher than the ones obtained for Asian pivot languages. word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training ( MERT) was used to tune the decoder’s parameters, and was performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, an in-house multi-stack phrase-based deTable 3 lists the highest BLEU scores of the pivot translation experiments obtained for all language pair combinations. The pivot language achieving 813 Table 4: Changes in Pivot Selection for Non-English European and Asian Language Pairs (BLEU) TRG → ↓ SRC da de es fr hi it nl pl pt ptb ru da – de (Non-English Europe"
I11-1091,P02-1040,0,0.082109,"– 39.9 (en) 41.1 (en) 40.4 (es) 40.4 (en) 39.9 (en) 36.9 (en) 40.5 (en) 39.9 (en) 39.7 (en) 42.6 (ptb) 42.2 (pt) 37.0 (en) 38.7 (en) 41.5 (ms) 33.1 (ko) 32.9 (ja) 42.9 (id) 37.0 (en) 39.3 (en) 38.3 (en) 33.8 (ja) 35.0 – (ja) coder was used. For the evaluation of translation quality, we applied the standard automatic evaluation metric BLEU which calculates the geometric mean of n-gram precision by the system output with respect to reference translations multiplied by a brevity penalty to prevent very short candidates from receiving too high a score. Scores range between 0 (worst) and 1 (best) (Papineni et al., 2002). For our experiments, single translation references were used. Table 2: Pivot Language Dependency (European Languages) ru 57.1 57.9 59.4 58.3 57.3 52.3 44.7 55.3 Table 2 summarizes the BLEU score ranges of all pivot translation experiments obtained for a given pivot language. The results show a large variation in BLEU scores for all pivot languages indicating that the best pivot choice largely depends on the respective source and target language. For European pivot languages, the best language combination scores are in general much higher than the ones obtained for Asian pivot languages. word"
K17-1018,P11-1137,0,0.212529,"ed performance across the entire feature set, we now focus on only the most highly associated features. The top features are important because these can give insights into the classification task, revealing which features are most associated with the target classes. Having top features that are meaningful and interpretable will lead to more trust in these models (Paul, 2016), and idenGeneralizability A motivation for learning features with causal associations with document classes is to learn robust 167 2.0 5 Standardized F1 score tifying meaningful features can itself be the goal of a study (Eisenstein et al., 2011b). We experimented with a small number of features M ∈ {5, 10, 20}. Under the assumption that optimal hyperparameters may be different when using such a small number of features, we retuned the PSM parameters again for the experiments in this subsection, using M =10. Table 4 shows the five words with the lowest p-values with both methods. At a glance, the top words from PSM seem to have strong sentiment associations; for example, excellent is a top five feature in all three datasets using PSM, and none of the datasets using χ2 . Words without obvious sentiment associations seem to appear more"
K17-1018,P11-1015,0,0.0751586,"n documents that do and do not contain the word (the “after” and “before” conditions, respectively, when considering words as treatments). 4 Doctors PSM χ2 .8569 .8560 .6510 .5497 .7799 .7853 Datasets We used datasets of reviews from three domains: • Doctors: Doctor reviews from RateMDs.com (Wallace et al., 2014). Doctors are rated on a scale from 1–5 along four different dimensions (knowledgeability, staff, helpfulness, punctuality). We averaged the four ratings for each review and labeled a review positive if the average rating was ≥ 4 and negative if ≤ 2. • Movies: Movie reviews from IMDB (Maas et al., 2011). Movies are rated on a scale from 1–10. Reviews rated ≥ 7 are labeled positive and reviews rated ≤ 4 are labeled negative. • Products: Product reviews from Amazon (Jindal and Liu, 2008). Products are rated on a scale from 1–5, with reviews rated ≥ 4 labeled positive and reviews rated ≤ 2 labeled negative. Baseline We compare propensity score matching with McNemar’s test (PSM) to a standard chisquared test (χ2 ) for feature selection, one of the All datasets were sampled to have an equal class balance. We used unigram word features. For ef166 Doctors 0.90 Movies 0.90 0.85 0.80 0.80 F1 score 0."
K17-1018,P04-1035,0,0.0305746,", we used no regularization for the sentiment classifiers. Since regularization and feature selection are both used to avoid overfitting, we did not want to conflate the effects of the two, so by using unregularized classifiers we can directly assess the efficacy of our feature selection methods on held-out data. All models were implemented with scikit-learn (Pedregosa et al., 2011). Experiments with Feature Selection To evaluate the ability of propensity score matching to identify meaningful word features, we use it for feature selection (Yang and Pedersen, 1997) in sentiment classification (Pang and Lee, 2004). 4.1 Test Corpus Movies PSM χ2 .6796 .6657 .8094 .7421 .8299 .8245 Table 3: Area under the feature selection curve (see Figure 1) using F1-score as the evaluation metric. All differences between corresponding PSM and χ2 results are statistically significant with p  0.01 except for (Doctors, Doctors). number of documents that do not contain the word with a positive sentiment label). This test statistic has a chi-squared distribution with 1 degree of freedom. This test is related to a traditional chi-squared test used for feature selection (which we compare to experimentally in Section 4), exc"
K17-1018,D09-1009,0,0.0254724,"on (Smith and Eisner, 2005) is also related to matching. In contrastive estimation, negative training examples are synthesized by perturbing positive instances. This strategy essentially matches instances that have the same semantics but different syntax. Annotation Perhaps the work that most closely gets at the concept of causality in document classification is work that asks for annotators to identify which features are important. There are branches of active learning which ask annotators to label not only documents, but to label features for importances or relevance (Raghavan et al., 2006; Druck et al., 2009). Work on annotator rationales (Zaidan et al., 2007; Zaidan and Eisner, 2008) seeks to model why annotators labeled a document a certain way—in other words, what “caused” the document to have its label? These ideas could potentially be integrated with causal inference methods for document classification. 7 8 Future Work Conclusion We have introduced and experimented with the idea of using propensity score matching for document classification. This method matches documents of similar propensity to contain a word as a way to simulate the random assignment to treatment and control groups, allowin"
K17-1018,D08-1004,0,0.0628968,"Missing"
K17-1018,N07-1033,0,0.327624,"hing. In contrastive estimation, negative training examples are synthesized by perturbing positive instances. This strategy essentially matches instances that have the same semantics but different syntax. Annotation Perhaps the work that most closely gets at the concept of causality in document classification is work that asks for annotators to identify which features are important. There are branches of active learning which ask annotators to label not only documents, but to label features for importances or relevance (Raghavan et al., 2006; Druck et al., 2009). Work on annotator rationales (Zaidan et al., 2007; Zaidan and Eisner, 2008) seeks to model why annotators labeled a document a certain way—in other words, what “caused” the document to have its label? These ideas could potentially be integrated with causal inference methods for document classification. 7 8 Future Work Conclusion We have introduced and experimented with the idea of using propensity score matching for document classification. This method matches documents of similar propensity to contain a word as a way to simulate the random assignment to treatment and control groups, allowing us to more reEfficiency is a drawback of the curr"
K17-1018,P05-1044,0,0.0774773,"eral framework for improving learning of text categories. Matching There have been instances of using matching techniques to improve text training data. Tan et al. (2014) built models to estimate the number of retweets of Twitter messages and addressed confounding factors by matching tweets of the same author and topic (based on posting the same link). Zhang et al. (2016) built classifiers to predict media coverage of journal articles used matching sampling to select negative training examples, choosing articles from the same journal issue. While motivated differently, contrastive estimation (Smith and Eisner, 2005) is also related to matching. In contrastive estimation, negative training examples are synthesized by perturbing positive instances. This strategy essentially matches instances that have the same semantics but different syntax. Annotation Perhaps the work that most closely gets at the concept of causality in document classification is work that asks for annotators to identify which features are important. There are branches of active learning which ask annotators to label not only documents, but to label features for importances or relevance (Raghavan et al., 2006; Druck et al., 2009). Work o"
K17-1018,P14-1017,0,0.0282869,"classes, which has not been widely considered in text processing. Prior work that used matching and related techniques for text classification was generally motivated by specific factors that needed to be controlled for, but our study found that a general-purpose matching approach can also lead to better feature discovery. We want this work to be seen not necessarily as a specific prescription for one method of feature selection, but as a general framework for improving learning of text categories. Matching There have been instances of using matching techniques to improve text training data. Tan et al. (2014) built models to estimate the number of retweets of Twitter messages and addressed confounding factors by matching tweets of the same author and topic (based on posting the same link). Zhang et al. (2016) built classifiers to predict media coverage of journal articles used matching sampling to select negative training examples, choosing articles from the same journal issue. While motivated differently, contrastive estimation (Smith and Eisner, 2005) is also related to matching. In contrastive estimation, negative training examples are synthesized by perturbing positive instances. This strategy"
K17-1018,P14-1074,0,0.0205603,"testing. Counts are on a log scale. every word in the vocabulary. Perhaps documents could instead be matched based on another metric, like cosine similarity. This would match documents with similar context, which is what the PSM method appears to be doing based on our analysis. We emphasize that the results of the PSM statistical analysis could be used in ways other than using it to select features ahead of training, which is less common today than doing feature selection directly through the training process, for example with sparse regularization (Tibshirani, 1994; Eisenstein et al., 2011a; Yogatama and Smith, 2014). One way to integrate PSM with regularization would be to use each feature’s test statistic to weight its regularization penalty, discouraging features with high p-values from having large coefficients in a classifier. In general, we believe this work shows the utility of controlling for the context in which features appear in documents when learning associations between features and classes, which has not been widely considered in text processing. Prior work that used matching and related techniques for text classification was generally motivated by specific factors that needed to be control"
N04-4003,J93-2003,0,0.00503405,"ement in the overall system performance compared to translation selection methods based on statistical scores only. 1 Introduction The statistical machine translation framework (SMT) formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability: TM·LM = argmaxT p(S|T ) ∗ p(T ), Parallel Text Corpus (1) where p(S|T ) is called a translation model (T M ), representing the generation probability from T into S, p(T ) is called a language model (LM ) and represents the likelihood of the target language (Brown et al., 1993). The T M and LM probabilities are trained automatically from a parallel text corpus (parameter estimation). They represent the general translation knowledge used to map a sequence of words from the source language into the target language. During the translation process (decoding) a statistical score based on the probabilities of the translation and the language models is assigned to each translation candidate and the one with the highest TM·LM score is selected as the translation output. However, the system might not be able to find a good translation due to parameter estimation problems of"
N04-4003,P01-1030,0,0.0536799,"information retrieval framework by treating each translation example as a document. For each word of the input, its term frequency tfi,j is combined with its document frequency dfi into a single weight wi,j , which is used to select the most relevant ones out of N documents (= example targets). Another possibility for obtaining translation examples is simply to utilize available (off-the-shelf) MT systems by pairing the input sentence with the obtained MT output. However, the quality of those translation examples might be much lower than manually created translations. 3 Statistical Decoding (Germann et al., 2001) presents a greedy approach to search for the translation that is most likely according to previously learned statitistical models. An extension of this approach that can take advantage of translation examples provided for a given input sentence is proposed in (Watanabe and Sumita, 2003). Instead of decoding and generating an output string word-by-word as is done in the basic concept, this greedy approach slightly modifies the target part of the translation examples so that the pair becomes the actual translation. The advantage of the example-based approach is that the search for a good transl"
N04-4003,P02-1040,0,0.0833116,"s and only the statistical scores for the selection of the translation. For the MT-based retrieval method we used eight machine translation systems for Japanese-to-English. Three of them were in-house EBMT systems which differ in the translation unit (sentence-based vs. phrase-based). They were trained on the same corpus as the statistical decoder. The remaining five systems were (off-the-shelf) generalpurpose translation engines with quite different levels of performance (cf. Table 2). • BLEU: the geometric mean of n-gram precision for the translation results found in reference translations (Papineni et al., 2002) • Translation Accuracy (ACC): subjective evaluation ranks ranging from A to D (A: perfect, B: fair, C: acceptable and D: nonsense), judged blindly by a native speaker (Sumita et al., 1999) In contrast to WER, higher BLEU and ACC scores indicate better translations. For the automatic scoring measures we utilized up to 16 human reference translations. 5.1 Downgrading Effects During Decoding In order to get an idea about how much degradation is to be expected in the translation candidates modified by the statistical decoder, we conducted an experiment using the reference translations of the test"
N04-4003,C92-2067,0,0.318995,"(Takezawa et al., 2002). The Basic Travel Expression Corpus (BTEC) contains 157K sentence pairs and the average lengths in words of Japanese and English sentences are 7.7 and 5.5, respectively. The corpus was split randomly into three parts for training (155K), parameter tuning (10K), and evaluation (10K) purposes. The experiments described below were carried out on 510 sentences selected randomly as the test set. For the evaluation, we used the following automatic scoring measures and human assessment. • Word Error Rate (WER), which penalizes the edit distance against reference translations (Su et al., 1992) In the second experiment, we used two types of retrieval methods (tf·idf-based, M T -based), as introduced in Section 2, and compared the results with the baseline system TM·LM, i.e., the example-based decoding approach of (Watanabe and Sumita, 2003) using the tf·idf criteria for the retrieval of translation examples and only the statistical scores for the selection of the translation. For the MT-based retrieval method we used eight machine translation systems for Japanese-to-English. Three of them were in-house EBMT systems which differ in the translation unit (sentence-based vs. phrase-base"
N04-4003,1999.mtsummit-1.34,1,0.839395,"in-house EBMT systems which differ in the translation unit (sentence-based vs. phrase-based). They were trained on the same corpus as the statistical decoder. The remaining five systems were (off-the-shelf) generalpurpose translation engines with quite different levels of performance (cf. Table 2). • BLEU: the geometric mean of n-gram precision for the translation results found in reference translations (Papineni et al., 2002) • Translation Accuracy (ACC): subjective evaluation ranks ranging from A to D (A: perfect, B: fair, C: acceptable and D: nonsense), judged blindly by a native speaker (Sumita et al., 1999) In contrast to WER, higher BLEU and ACC scores indicate better translations. For the automatic scoring measures we utilized up to 16 human reference translations. 5.1 Downgrading Effects During Decoding In order to get an idea about how much degradation is to be expected in the translation candidates modified by the statistical decoder, we conducted an experiment using the reference translations of the test set as the input of the example-based decoder. These seed sentences are already accurate translations, thus simulating the “optimal” translation example retrieval case resulting in an uppe"
N04-4003,takezawa-etal-2002-toward,1,0.81227,"cale ∗ ED(sd ,d) ) (2) The second rescoring function assigns a probability to each decoder output that combines the exponential of the sum of log probabilities of TM and LM and the scaled negative ED scores of all translation candidates T C as follows. TM·LM·EDP (d) = P exp(log TM(d)+log LM(d)−scale ∗ ED(sd ,d)) exp(log TM(tc)+log LM(tc)−scale ∗ ED(stc ,tc)) (stc ,tc)∈T C (3) 5 Evaluation 5.2 Baseline Comparison The evaluation of our approach is carried out using a collection of Japanese sentences and their English translations that are commonly found in phrasebooks for tourists going abroad (Takezawa et al., 2002). The Basic Travel Expression Corpus (BTEC) contains 157K sentence pairs and the average lengths in words of Japanese and English sentences are 7.7 and 5.5, respectively. The corpus was split randomly into three parts for training (155K), parameter tuning (10K), and evaluation (10K) purposes. The experiments described below were carried out on 510 sentences selected randomly as the test set. For the evaluation, we used the following automatic scoring measures and human assessment. • Word Error Rate (WER), which penalizes the edit distance against reference translations (Su et al., 1992) In the"
N04-4003,2003.mtsummit-papers.54,1,0.961415,"part of the translation examples enables us to identify translation candidates that might be close to the actual translation. A common approach to measure the distance between sequences of words is the edit distance criteria (Wagner, 1974). The distance is defined as the sum of the costs of insertion (INS), deletion (DEL), and substitution (SUB) operations required to map one word sequence into the other. The edit distance can be calculated by a standard dynamic programming technique. ED(s1 ,s2 ) = |INS |+ |DEL|+ |SUB| An extension of the edit-distance-based retrieval method is presented in (Watanabe and Sumita, 2003). It incorporates the tf·idf criteria as seen in the information retrieval framework by treating each translation example as a document. For each word of the input, its term frequency tfi,j is combined with its document frequency dfi into a single weight wi,j , which is used to select the most relevant ones out of N documents (= example targets). Another possibility for obtaining translation examples is simply to utilize available (off-the-shelf) MT systems by pairing the input sentence with the obtained MT output. However, the quality of those translation examples might be much lower than man"
N06-2029,P02-1040,0,0.0901909,"Missing"
N06-2029,C02-1076,1,0.886913,"Missing"
N06-2029,2005.iwslt-1.5,1,0.884543,"Missing"
N09-2056,J03-1002,0,0.00229055,"tence pairs was used to train the source-to-pivot translation models (80k sp ) and the second subset of sentence pairs was used to train the pivot-to-target translation models (80k pt ). Table 1 summarizes the characteristics of the BTEC corpus data sets used for the training (train) of the SMT models, the tuning of model weights (dev), and the evaluation of translation quality (eval). Besides the number of sentences (sen) and the vocabulary (voc), the sentence length (len) is also given, as the average number of words per sentence. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters, and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, an in-house multi-stack phrase-based decoder comparable to MOSES was used. For the evaluation of translation quality, we applied standard automatic evaluation metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). For the experimental results in this paper, the given scores are calculated as the average of the respective BLEU and MET"
N09-2056,P02-1040,0,0.0971744,"vocabulary (voc), the sentence length (len) is also given, as the average number of words per sentence. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters, and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, an in-house multi-stack phrase-based decoder comparable to MOSES was used. For the evaluation of translation quality, we applied standard automatic evaluation metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). For the experimental results in this paper, the given scores are calculated as the average of the respective BLEU and METEOR scores obtained for each system output and are listed as percent figures. 222 Table 1: Language Resources BTEC Corpus # of sen en voc len de voc len es voc len fr voc len hi voc len id voc len ja voc len ko voc len ms voc len th voc len vi voc len zh voc len train 80ksp 80kpt 80,000 80,000 12,264 11,047 7.8 7.2 19,593 17,324 7.4 6.8 16,317 14,807 7.6 7.1 15,319 13,663 7.8 7.3 26,096 19,906 8.1 7.6 14,585 13,224 7.0 6.5 13,868 12,51"
N09-2056,N07-1061,0,\N,Missing
N09-2056,W05-0909,0,\N,Missing
N09-2056,D08-1078,0,\N,Missing
N12-1024,P03-1006,0,0.0917783,"ing the n-grams should occur. In a long string, we might want to encourage or discourage an n-gram in a certain “region” of the string. Our features can only encourage or discourage it everywhere in the string, which may lead to slow convergence. Nevertheless, in our particular experimental settings, we find that this works better than other topologies we have considered. Sparse N-Gram Encoding A full n-gram language model requires ≈ |Σ|n arcs to encode as a WFSA. This could be quite expensive. Fortunately, large n-gram models can be compacted by using failure arcs (φ-arcs) to encode backoff (Allauzen et al., 2003). These arcs act as -transitions that can be taken only when no other transition is available. They allow us to encode the sparse subset of ngrams that have nonzero Lagrangians. We encode G such that all features whose λ value is 0 will back off to the next largest n-gram having nonzero weight. 236 This form of G still accepts Σ∗ and has the same weights as a dense representation, but could require substantially fewer states. 4.2 Incrementally Expanding G As mentioned above, we may need to alter G as we go along. Intuitively, we may want to start with features that are cheap to encode, to mov"
N12-1024,N03-1003,0,0.0538035,"s for a fixed set of n-grams. timality. Even in instances where approximate algorithms perform well, it could be useful to have a true optimality guarantee. For example, our algorithm can be used to produce reference solutions, which are important to have for research purposes. Under a sum-of-pairs Levenshtein objective, the exact multi-sequence alignment can be directly obtained from the Steiner consensus string and vice versa (Gusfield, 1997). This implies that our exact algorithm could be also used to find exact multisequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al., 2006) that is almost always solved with approximate methods. We have noted that some constraints are more useful than others. Position-specific information is hard to agree on and leads to slow convergence, while pure n-gram constraints do not work as well for long strings where the position may be important. One avenue we are investigating is the use of a non-deterministic G, which would allow us to encode latent variables (Dreyer et al., 2008), such as loosely defined “regions” within a string, and to allow for the encoding of alignments between the"
N12-1024,D09-1011,1,0.89767,"Missing"
N12-1024,D08-1113,1,0.903211,"Missing"
N12-1024,knight-al-onaizan-1998-translation,0,0.0230236,"ion Many tasks in natural language processing involve functions that assign scores—such as logprobabilities—to candidate strings or sequences. Often such a function can be represented compactly as a weighted finite state automaton (WFSA). Finding the best-scoring string according to a WFSA is straightforward using standard best-path algorithms. It is common to construct a scoring WFSA by combining two or more simpler WFSAs, taking advantage of the closure properties of WFSAs. For example, consider noisy channel approaches to speech recognition (Pereira and Riley, 1997) or machine translation (Knight and Al-Onaizan, 1998). Given an input f , the score of a possible English transcription or translation e is the sum of its language model score log p(e) and its channel model score log p(f |e). If each of these functions of e is represented as a WFSA, then their sum is represented as the intersection of those two WFSAs. WFSA intersection corresponds to constraint conjunction, and hence is often a mathematically natural way to specify a solution to a problem involving ∗ The authors are grateful to Damianos Karakos for providing tools and data for the ASR experiments. This work was supported in part by an NSF Gradua"
N12-1024,D10-1125,0,0.0738401,"hip. multiple soft constraints on a desired string. Unfortunately, the intersection may be computationally inefficient in practice. The intersection of K WFSAs having n1 , n2 , . . . , nK states may have n1 ·n2 · · · nK states in the worst case.1 In this paper, we propose a more efficient method for finding the best path in an intersection without actually computing the full intersection. Our approach is based on dual decomposition, a combinatorial optimization technique that was recently introduced to the vision (Komodakis et al., 2007) and language processing communities (Rush et al., 2010; Koo et al., 2010). Our idea is to interrogate the several WFSAs separately, repeatedly visiting each WFSA to seek a high-scoring path in each WFSA that agrees with the current paths found in the other WSFAs. This iterative negotiation is reminiscent of message-passing algorithms (Sontag et al., 2008), while the queries to the WFSAs are reminiscent of loss-augmented inference (Taskar et al., 2005). We remark that a general solution whose asymptotic worst-case runtime beat that of naive intersection would have important implications for complexity theory (Karakostas et al., 2003). Our approach is not such a solu"
N12-1024,P11-1008,0,0.0157752,"be solved independently. If we can somehow combine the solutions from the subproblems into a “valid” solution to the global problem, then we can avoid optimizing the joint problem directly. A valid solution is one in which the individual solutions of each subproblem all agree on the variables which are shared in the joint problem. For example, if we are combining a parser with a part-of-speech tagger, the tag assignments from both models must agree in the final solution (Rush et al., 2010); if we are intersecting a translation model with a language model, then it is the words that must agree (Rush and Collins, 2011). More formally, suppose we want to find a global solution that is jointly PKoptimized among K subproblems: argminx k=1 fk (x). Suppose that x ranges over vectors. Introducing an auxiliary variable xk for each subproblem fk allows us to equivalently formulate this as the following constrained optimization problem: min {x1 ,...,xK } K X fk (xk ) + λk · xk (2) k=1 where the Lagrange multiplier vectors λk can be used to penalize solutions that do not satisfy the agreement constraints (∀k) xk = x. Our goal is to maximize this lower bound and hope that the result does satisfy the constraints. The g"
N12-1024,D10-1001,0,0.418378,"te Research Fellowship. multiple soft constraints on a desired string. Unfortunately, the intersection may be computationally inefficient in practice. The intersection of K WFSAs having n1 , n2 , . . . , nK states may have n1 ·n2 · · · nK states in the worst case.1 In this paper, we propose a more efficient method for finding the best path in an intersection without actually computing the full intersection. Our approach is based on dual decomposition, a combinatorial optimization technique that was recently introduced to the vision (Komodakis et al., 2007) and language processing communities (Rush et al., 2010; Koo et al., 2010). Our idea is to interrogate the several WFSAs separately, repeatedly visiting each WFSA to seek a high-scoring path in each WFSA that agrees with the current paths found in the other WSFAs. This iterative negotiation is reminiscent of message-passing algorithms (Sontag et al., 2008), while the queries to the WFSAs are reminiscent of loss-augmented inference (Taskar et al., 2005). We remark that a general solution whose asymptotic worst-case runtime beat that of naive intersection would have important implications for complexity theory (Karakostas et al., 2003). Our approach"
N13-1017,W00-0405,0,0.0102334,"ors. In this paper we consider a setting where the user has prior knowledge about the end application: mining recreational drug trends from user forums, an important clinical research problem (§2). We show how to incorporate available information from these forums into f-LDA as a novel hierarchical prior over the model parameters, guiding the model toward the desired output (§3.1). We then demonstrate the model’s utility in exploring a corpus in a targeted manner by using it to automatically extract interesting sentences from the text, a simple form of extractive multi-document summarization (Goldstein et al., 2000). In the same way that topic models can be used for aspectspecific summarization (Titov and McDonald, 2008; Haghighi and Vanderwende, 2009), we use f-LDA to extract snippets corresponding to fine-grained information patterns. Our results demonstrate that our multi-dimensional modeling approach targets more informative text than a simpler model (§4). 168 Proceedings of NAACL-HLT 2013, pages 168–178, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Analyzing Drug Trends on the Web Recreational drug use imposes a significant burden on the health infrastructure"
N13-1017,N09-1041,0,0.0456298,"nds from user forums, an important clinical research problem (§2). We show how to incorporate available information from these forums into f-LDA as a novel hierarchical prior over the model parameters, guiding the model toward the desired output (§3.1). We then demonstrate the model’s utility in exploring a corpus in a targeted manner by using it to automatically extract interesting sentences from the text, a simple form of extractive multi-document summarization (Goldstein et al., 2000). In the same way that topic models can be used for aspectspecific summarization (Titov and McDonald, 2008; Haghighi and Vanderwende, 2009), we use f-LDA to extract snippets corresponding to fine-grained information patterns. Our results demonstrate that our multi-dimensional modeling approach targets more informative text than a simpler model (§4). 168 Proceedings of NAACL-HLT 2013, pages 168–178, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Analyzing Drug Trends on the Web Recreational drug use imposes a significant burden on the health infrastructure of the United States and other countries. Accurate information on drugs, usage profiles and side effects are necessary for supporting a ran"
N13-1017,E12-1021,0,0.035425,"Missing"
N13-1017,W04-1013,0,0.0191069,"y members in psychiatry and behavioral pharmacology, who have used drug forums in the past to study emerging drugs) to rate the snippets corresponding to mephedrone/MDPV. The best f-LDA system had an average score of 2.57 compared to a baseline score of 2.45 and random score of 1.63. 4.3.2 Automatic Evaluation of Recall The human judgments effectively measured a form of precision, as the quality of snippets were judged by their correspondence to the reference text, without regard to how much of the reference text was covered by all snippets. We also used the automatic evaluation metric ROUGE (Lin, 2004) as a rough estimate of summary recall: this metric computes the percentage of n-grams in the reference text that appeared in the generated summaries. We computed ROUGE for both 1-grams and 2grams. When computing n-gram counts, we applied Porter’s stemmer to all tokens. We excluded stop 176 words from 1-gram counts but included them in 2gram counts where we care about longer phrases.2 Results are shown in Table 2. We find that f-LDA1 has the highest score for both 1- and 2-grams, suggesting that it is extracting a more diverse set of relevant snippets. When performing a paired t-test across th"
N13-1017,D09-1026,0,0.0564078,"Missing"
N13-1097,N12-1033,0,0.0434613,"Missing"
N13-1097,W10-0701,1,0.308445,"Missing"
N13-1097,W09-3012,0,0.0300103,"Missing"
N13-1097,D11-1145,0,0.678908,", and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1 , tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1 The DHHS competition relied solely on keyword counting. http://www.nowtrendingchallenge.com/ Both are related to the flu and express worry, but tell a different story. The first reports an infection of another person, while the second expresses the author’s concerned awareness. While infection tweets indicate a rise in infection rate, awareness"
N13-1097,D10-1124,0,0.0451635,"Missing"
N13-1097,P11-2008,0,0.047861,"Missing"
N13-1097,W12-3807,0,0.0145785,"Missing"
N13-1097,N12-1057,0,0.0117195,"edings of NAACL-HLT 2013, pages 789–795, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics flu but do not report an infection can corrupt infection tracking. Concerned Awareness vs. Infection (A/I) Many flu tweets express a concerned awareness as opposed to infection, including fear of getting the flu, an awareness of increased infections, beliefs related to flu infection, and preventative flu measures (e.g. flu shots.) Critically, these people do not seem to have the flu, whereas infection tweets report having the flu. This distinction is similar to modality (Prabhakaran et al., 2012a). Conflating these tweets can hurt surveillance, as around half of our annotated flu messages were awareness. Identifying awareness tweets may be of use in-and-of itself, such as for characterizing fear of illness (Epstein et al., 2008; Epstein, 2009), public perception, and discerning sentiment (e.g. flu is negative, flu shots may be positive.) We focus on surveillance improvements.2 Self vs. Other (S/O) Tweets for both awareness and infection can describe the author (self) or others. It may be that self infection reporting is more informative. We test this hypothesis by classifying tweets"
N16-1003,W10-2916,0,0.0187418,"replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences that contain phrases that are fre"
N16-1003,P10-1088,0,0.121589,"be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences that contain phrases that are frequent in monolingual data but not in"
N16-1003,P11-2071,0,0.0432275,"Missing"
N16-1003,2005.iwslt-1.7,0,0.645248,"s large corpora can be collected, for example by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a numbe"
N16-1003,2014.amta-workshop.3,0,0.0133335,"filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Select sentences randomly. 4gram-rand: Select n-gram strings of length of up to 4 in random order. sent-by-4gram-freq: Select the sentence including the most frequent uncovered phrase with length of up to 4 words (baseline 1, §3.1). 4gram-freq: Select the most frequent uncovered phrase wi"
N16-1003,E12-1025,0,0.512627,"Missing"
N16-1003,D14-1130,0,0.0163888,"many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences that contain phrases that are frequent in monolingual data but not in bilingual data (Eck et al., 2005), have low confid"
N16-1003,P09-1021,0,0.381297,"web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are methods to select sentences tha"
N16-1003,N09-1047,0,0.0215798,"xample by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators. These 20 Proceedings of NAACL-HLT 2016, pages 20–29, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics methods generally assign priority to sentences that contain data that is potentially useful to the MT system according to a number of criteria. For example, there are metho"
N16-1003,W11-2123,0,0.0105239,"after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Select sentences randomly. 4gram-rand: Select n-gram strings of length of up to 4 in random order. sent-by-4gram-freq: Select the sentence including the most frequent uncovered phrase with length of up to 4 words (baseline 1, §3.1). 4gram-freq: Select the most frequent uncovered phrase with length of up to 4 words (baseline 2, §3.2). maxsubst-freq: Select the most frequent uncovered maximal phrase (proposed, §4.1) reduced-maxsubst-freq: Select the most"
N16-1003,N03-1017,0,0.0128806,"the English-Japanese translation task, we adopted the broad-coverage example sentence corpus provided with the Eijiro dictionary4 as general domain data, and the ASPEC5 scientific paper abstract corpus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github"
N16-1003,P07-2045,0,0.0114659,"dopted the broad-coverage example sentence corpus provided with the Eijiro dictionary4 as general domain data, and the ASPEC5 scientific paper abstract corpus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Sel"
N16-1003,P11-2093,1,0.756098,". En: 46.4M Words Ja: 57.6M Words 1790 Sent. 1790 Sent. Table 1: Details of parallel data from WMT20143 as a base parallel data source and EMEA (Tiedemann, 2009), PatTR (W¨aschle and Riezler, 2012), and Wikipedia titles, used in the medical translation task, as the target domain data. For the English-Japanese translation task, we adopted the broad-coverage example sentence corpus provided with the Eijiro dictionary4 as general domain data, and the ASPEC5 scientific paper abstract corpus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Mos"
N16-1003,J03-1002,0,0.00479721,"pus as the target domain data. For preprocessing, we tokenized Japanese corpora using the KyTea word segmenter (Neubig et al., 2011) and filtered out the lines of length over 60 from all the training parallel data to ensure accuracy of parsing and alignment. We show the details of the parallel dataset after pre-processing in Table 1. For the machine translation framework, we used phrase-based SMT (Koehn et al., 2003) with the Moses toolkit (Koehn et al., 2007) as a decoder. To efficiently re-train the models with new data, we adopted inc-giza-pp,6 a specialized version of GIZA++ word aligner (Och and Ney, 2003) supporting incremental training, and the memory-mapped dynamic suffix array phrase tables (MMSAPT) feature of Moses (Germann, 2014) for on-memory construction of phrase tables. We train 5-gram models over the target side of all the general domain and target domain data using KenLM (Heafield, 2011). 3 http://statmt.org/wmt14/ http://eijiro.jp 5 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 6 https://github.com/akivajp/inc-giza-pp 4 24 sent-rand: Select sentences randomly. 4gram-rand: Select n-gram strings of length of up to 4 in random order. sent-by-4gram-freq: Select the sentence including the most"
N16-1003,P03-1021,0,0.0212038,"To simulate a realistic active learning scenario, we started from given parallel data in the general domain and sequentially added additional source language data in a specific target domain. For the English-French translation task, we adopted the Europarl corpus 2 The method does not distinguish between equivalent word sequences even if they have different tree structures Lang Pair En-Fr En-Ja Domain Dataset General (Base) Train Medical Train (Target) Test Dev General (Base) Train Scientific Train (Target) Test Dev For the tuning of decoding parameters, since it is not realistic to run MERT (Och, 2003) at each retraining step, we tuned the parameters to maximize the BLEU score (Papineni et al., 2002) for the baseline system, and re-used the parameters thereafter. We compare the following 8 segment selection methods, including 2 random selection methods, 2 conventional methods and 4 proposed methods: Amount 1.89M Sent. En: 47.6M Words Fr: 49.4M Words 15.5M Sent. En: 393M Words Fr: 418M Words 1000 Sent. 500 Sent. 414k Sent. En: 6.72M Words Ja: 9.69M Words 1.87M Sent. En: 46.4M Words Ja: 57.6M Words 1790 Sent. 1790 Sent. Table 1: Details of parallel data from WMT20143 as a base parallel data s"
N16-1003,N15-3009,1,0.751844,", §3.2). maxsubst-freq: Select the most frequent uncovered maximal phrase (proposed, §4.1) reduced-maxsubst-freq: Select the most frequent uncovered semi-maximal phrase (proposed, §4.1) struct-freq: Select the most frequent uncovered phrase extracted from the subtrees (proposed, §4.2). reduced-struct-freq: Select the most frequent uncovered semi-maximal phrase extracted from the subtrees (proposed, §4.1 and §4.2). To generate oracle translations, we used an SMT system trained on all of the data in both the general and target-domain corpora. To generate parse trees, we used the Ckylark parser (Oda et al., 2015). 5.2 Results and Discussion Comparison of efficiency: In Figure 3, we show the evaluation score results by the number of additional source words up to 100k and 1M words. We can see that in English-French translation, the accuracy of the selection methods using parse trees grows more rapidly than other methods and was significantly better even at the point of 1M additional words. In the case of English-Japanese translation, the gains over 4-gram frequency are much smaller, but the proposed methods still consistently perform as well or better than the other methods. Besides, in all the graphs w"
N16-1003,P02-1040,0,0.0981299,"the general domain and sequentially added additional source language data in a specific target domain. For the English-French translation task, we adopted the Europarl corpus 2 The method does not distinguish between equivalent word sequences even if they have different tree structures Lang Pair En-Fr En-Ja Domain Dataset General (Base) Train Medical Train (Target) Test Dev General (Base) Train Scientific Train (Target) Test Dev For the tuning of decoding parameters, since it is not realistic to run MERT (Och, 2003) at each retraining step, we tuned the parameters to maximize the BLEU score (Papineni et al., 2002) for the baseline system, and re-used the parameters thereafter. We compare the following 8 segment selection methods, including 2 random selection methods, 2 conventional methods and 4 proposed methods: Amount 1.89M Sent. En: 47.6M Words Fr: 49.4M Words 15.5M Sent. En: 393M Words Fr: 418M Words 1000 Sent. 500 Sent. 414k Sent. En: 6.72M Words Ja: 9.69M Words 1.87M Sent. En: 46.4M Words Ja: 57.6M Words 1790 Sent. 1790 Sent. Table 1: Details of parallel data from WMT20143 as a base parallel data source and EMEA (Tiedemann, 2009), PatTR (W¨aschle and Riezler, 2012), and Wikipedia titles, used in"
N16-1003,J03-3002,0,0.0863344,"Missing"
N16-1003,D08-1112,0,0.0420495,"gual data (Eck et al., 2005), have low confidence according to the MT system (Haffari et al., 2009), or are predicted to be poor translations by an MT quality estimation system (Ananthakrishnan et al., 2010). However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate. To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks (Settles and Craven, 2008; Bloodgood and Callison-Burch, 2010; Tomanek and Hahn, 2009; Sperber et al., 2014). For MT in particular, Bloodgood and Callison-Burch (2010) have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words (§3). Nevertheless, our experiments identified two major practical problems with this method. First, as shown in Figure 1 (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency. Second, it is common to see fragments o"
N16-1003,Q14-1014,1,0.851551,"et al., 2009), or are predicted to be poor translations by an MT quality estimation system (Ananthakrishnan et al., 2010). However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate. To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks (Settles and Craven, 2008; Bloodgood and Callison-Burch, 2010; Tomanek and Hahn, 2009; Sperber et al., 2014). For MT in particular, Bloodgood and Callison-Burch (2010) have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words (§3). Nevertheless, our experiments identified two major practical problems with this method. First, as shown in Figure 1 (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency. Second, it is common to see fragments of complex phrases such as “one of the preceding,” which may be difficult for worker"
N16-1003,P09-1117,0,0.0296401,"the MT system (Haffari et al., 2009), or are predicted to be poor translations by an MT quality estimation system (Ananthakrishnan et al., 2010). However, while the selected sentences may contain useful phrases, they will also generally contain many already covered phrases that nonetheless cost time and money to translate. To solve the problem of wastefulness in fullsentence annotation for active learning, there have been a number of methods proposed to perform sub-sentential annotation of short phrases for natural language tasks (Settles and Craven, 2008; Bloodgood and Callison-Burch, 2010; Tomanek and Hahn, 2009; Sperber et al., 2014). For MT in particular, Bloodgood and Callison-Burch (2010) have proposed a method that selects poorly covered ngrams to show to translators, allowing them to focus directly on poorly covered parts without including unnecessary words (§3). Nevertheless, our experiments identified two major practical problems with this method. First, as shown in Figure 1 (a), many of the selected phrases overlap with each other, causing translation of redundant phrases, damaging efficiency. Second, it is common to see fragments of complex phrases such as “one of the preceding,” which may"
N16-1003,W08-0305,0,0.0285635,"Missing"
N16-1003,P11-1122,0,0.0204067,"sed parse subtree selection method Figure 1: Conventional and proposed data selection methods Introduction In statistical machine translation (SMT) (Brown et al., 1993), large quantities of high-quality bilingual data are essential to achieve high translation accuracy. While in many cases large corpora can be collected, for example by crawling the web (Resnik and 1 Code to replicate the experiments can be found at https://github.com/akivajp/naacl2016 Smith, 2003), in many domains or language pairs it is still necessarily to create data by hand, either by hiring professionals or crowdsourcing (Zaidan and Callison-Burch, 2011). In these cases, active learning (§2), which selects which data to annotate based on their potential benefit to the translation system, has been shown to be effective for improving SMT systems while keeping the required amount of annotation to a minimum (Eck et al., 2005; Turchi et al., 2008; Haffari et al., 2009; Haffari and Sarkar, 2009; Ananthakrishnan et al., 2010; Bloodgood and Callison-Burch, 2010; Gonz´alez-Rubio et al., 2012; Green et al., 2014). Most work on active learning for SMT, and natural language tasks in general, has focused on choosing which sentences to give to annotators."
N16-1003,J07-2003,0,\N,Missing
N18-1099,P14-1110,1,0.877669,"ncreases, more low-probability and irrelevant words appear the topic, which lowers CNPMI scores. However, MTA stays stable or increases with increasing cardinality. Thus, MTA fails to fulfill a critical property of topic model evaluation. Finally, MTA requires a comprehensive multilingual dictionary, which may be unavailable for lowresource languages. Additionally, most languages often only have one dictionary, which makes it problematic to use the same resource (a language’s single multilingual dictionary) for training and evaluating models that use a dictionary to build multilingual topics (Hu et al., 2014). Given these concerns, we continue the paper’s focus on CNPMI as a data-driven alternative to MTA. However, for many applications MTA may suffice as a simple, adequate evaluation metric. 6 Model-Level Evaluation While the previous section looked at individual topics, we also care about how well CNPMI characterizes the quality of models through an average of a model’s constituent topics. 6.1 Training Knowledge Adding more knowledge to multilingual topic models improves topics (Hu et al., 2014), so an effective evaluation should reflect this improvement as knowlege is added to the model. For po"
N18-1099,C12-1089,0,0.154564,"Missing"
N18-1099,2005.mtsummit-papers.11,0,0.0393457,"Missing"
N18-1099,N16-1132,0,0.114728,"cific “views” of a document share the document-topic distribution θd . The generative story for the document-links model is: 1 2 3 4 5 6 7 8 for each topic k and each language ` do Draw a distribution over words φ`k ∼ Dirichlet(β);   Alternatively, word translations (Jagarlamudi and Daumé III, 2010), concept links (Gutiérrez et al., 2016; Yang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges. Since the polylingual topic model is the most common approach for building multilingual topic models (Vuli´c et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model. Monolingual Evaluation Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010). The most successful (Lau et al., 2014) is normalized pointwise mutual information (Bouma, 2009, NPMI). NPMI compares the joint probability of words appearing together Pr(wi , wj ) to their probability assuming independence Pr(wi ) Pr(wj ), normalized by the joint probability: Pr(w ,w ) NPMI (wi , wj )"
N18-1099,N16-1053,0,0.107928,"el/comparable language-specific documents, 1090 Proceedings of NAACL-HLT 2018, pages 1090–1100 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics d(`) , and the language-specific “views” of a document share the document-topic distribution θd . The generative story for the document-links model is: 1 2 3 4 5 6 7 8 for each topic k and each language ` do Draw a distribution over words φ`k ∼ Dirichlet(β);   Alternatively, word translations (Jagarlamudi and Daumé III, 2010), concept links (Gutiérrez et al., 2016; Yang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges. Since the polylingual topic model is the most common approach for building multilingual topic models (Vuli´c et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model. Monolingual Evaluation Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010). The most successful (Lau et al., 2014) is normalized pointwise mutual information (Boum"
N18-1099,N16-1057,0,0.0756798,"pair do not appear in the reference corpus, the cooccurrence score is zero. Similar to monolingual settings, CNPMI for a bilingual topic k is the average of the NPMI scores of all C 2 bilingual word pairs, CNPMI (`1 , `2 , k) = PC i,j NPMI (wi,`1 , wj,`2 ) . C2 (4) It is straightforward to generalize CNPMI from a language pair to multiple languages by averaging CNPMI (`i , `j , k) over all language pairs (`i , `j ). 3 Adapting to Low-Resource Languages CNPMI needs a reference corpus for co-occurrence statistics. Wikipedia, which has good coverage of topics and vocabularies is a common choice (Lau and Baldwin, 2016). Unfortunately, Wikipedia is often unavailable or not large enough for lowresource languages. It only covers 282 languages,1 and only 249 languages have more than 1,000 pages: many of pages are short or unlinked to 1 a high-resource language. Since CNPMI requires comparable documents, the usable reference corpus is defined by paired documents. Another option for a parallel reference corpus is the Bible (Resnik et al., 1999), which is available in most world languages;2 however, it is small and archaic. It is good at evaluating topics such as family and religion, but not “modern” topics like b"
N18-1099,Q16-1004,0,0.203877,"Missing"
N18-1099,E14-1056,0,0.698881,"ang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges. Since the polylingual topic model is the most common approach for building multilingual topic models (Vuli´c et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model. Monolingual Evaluation Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010). The most successful (Lau et al., 2014) is normalized pointwise mutual information (Bouma, 2009, NPMI). NPMI compares the joint probability of words appearing together Pr(wi , wj ) to their probability assuming independence Pr(wi ) Pr(wj ), normalized by the joint probability: Pr(w ,w ) NPMI (wi , wj ) = j log Pr(wi ) iPr(w j) log Pr(wi , wj ) . (1) The word probabilities are calculated from a reference corpus, R, typically a large corpus such as Wikipedia that can provide meaningful cooccurrence patterns that are independent of the target dataset. The quality of topic k is the average NPMI of all word pairs (wi , wj ) in the topic"
N18-1099,D16-1229,0,0.0191501,"ern words are. The word era features are the earliest usage year 3 for each word in a topic. We use both the mean and standard deviation as features. Meaning Drift (DRIFT). The meaning of a word can expand and drift over time. For example, in the Bible, “web” appears in Isaiah 59:5: They hatch cockatrice’ eggs, and weave the spider’s web. 3 https://oxforddictionaries.com/ The word “web” could be evaluated correctly in an animal topic. For modern topics, however, Bible fails to capture modern meanings of “web”, as in Topic 5 (Figure 1). To address this meaning drift, we use a method similar to Hamilton et al. (2016). For each English word, we calculate the context vector from Bible and from Wikipedia with a window size of five and calculate the cosine similarity between them as word similarity. Similar context vectors mean that the usage in the Bible is consistent with Wikipedia. We calculate word similarities for all the English topic words in a topic and use the average and standard deviation as features. 3.2 Example In Figure 3, Topic 1 is coherent while Topic 8 is not. From left to right, we incrementally add new feature sets, and show how the estimated topic coherence scores (dashed lines) approach"
N18-1099,D09-1092,0,0.158757,"hes to multilingual settings. 2.1 Multilingual Topic Modeling Probabilistic topic models associate each document in a corpus with a distribution over latent topics, while each topic is associated with a distribution over words in the vocabulary. The most widely used topic model, latent Dirichlet allocation (Blei et al., 2003, LDA), can be extended to connect languages. These extensions require additional knowledge to link languages together. One common encoding of multilingual knowledge is document links (indicators that documents are parallel or comparable), used in polylingual topic models (Mimno et al., 2009; Ni et al., 2009). In these models, each document d indexes a tuple of parallel/comparable language-specific documents, 1090 Proceedings of NAACL-HLT 2018, pages 1090–1100 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics d(`) , and the language-specific “views” of a document share the document-topic distribution θd . The generative story for the document-links model is: 1 2 3 4 5 6 7 8 for each topic k and each language ` do Draw a distribution over words φ`k ∼ Dirichlet(β);   Alternatively, word translations (Jagarlamudi and Daumé III, 2010), conce"
N18-1099,D11-1024,0,0.0584405,"and not adaptable to low-resource languages. The approaches here could help improve the evaluation of all multilingual representation learning algorithms (Schnabel et al., 2015). ● ● ● ● 0.015 ● ● 0.010 ● ● ● ● ● ● ● ● Bible ● ● 8 ● ● ● ● ● ● 0.4 0.6 0.8 ● ● 0.005 ● 0.2 1.0 Proportion of Reference Corpus Figure 10: CNPMI is stable once the number of reference documents is large enough (around five thousand documents). 7 Related Work Topic Coherence Many coherence metrics based on co-occurrence statistics have been proposed besides NPMI. Similar metrics—such as asymmetrical word pair metrics (Mimno et al., 2011) and combinations of existing measurements (Lau et al., 2014; Röder et al., 2015)— correlate well with human judgments. NPMI has been the current gold standard for evaluation and improvements of monolingual topic models (Pecina, 2010; Newman et al., 2011). External Tasks Another approach is to use a model for predictive tasks: the better the results are on external tasks, the better a topic model is assumed to be. A common task is held-out likelihood (Wallach et al., 2009b; Jagarlamudi and Daumé III, 2010; Fukumasu et al., 2012), but as Chang et al. (2009) show, this does not always reflect hu"
N18-1099,D15-1036,0,0.0242104,"● 0.10 ● CNPMI 0.05 ● ● ● Wikipedia ● Representation Learning Topic models are one example of a broad class of techniques of learning representations of documents (Bengio et al., 2013). Other approaches learn respresentations at the word (Klementiev et al., 2012; Vyas and Carpuat, 2016), paragraph (Mogadala and Rettinger, 2016), or corpus level (Søgaard et al., 2015). However, neural representation learning approaches are often data hungry and not adaptable to low-resource languages. The approaches here could help improve the evaluation of all multilingual representation learning algorithms (Schnabel et al., 2015). ● ● ● ● 0.015 ● ● 0.010 ● ● ● ● ● ● ● ● Bible ● ● 8 ● ● ● ● ● ● 0.4 0.6 0.8 ● ● 0.005 ● 0.2 1.0 Proportion of Reference Corpus Figure 10: CNPMI is stable once the number of reference documents is large enough (around five thousand documents). 7 Related Work Topic Coherence Many coherence metrics based on co-occurrence statistics have been proposed besides NPMI. Similar metrics—such as asymmetrical word pair metrics (Mimno et al., 2011) and combinations of existing measurements (Lau et al., 2014; Röder et al., 2015)— correlate well with human judgments. NPMI has been the current gold standard"
N18-1099,P15-1165,0,0.0941828,"Missing"
N18-1099,tiedemann-2012-parallel,0,0.060771,"Missing"
N18-1099,N16-1083,0,0.0548686,"Missing"
N18-1099,N10-1012,0,0.618154,"tion Topic models provide a high-level view of the main themes of a document collection (Boyd-Graber et al., 2017). Document collections, however, are often not in a single language, driving the development of multilingual topic models. These models discover topics that are consistent across languages, providing useful tools for multilingual text analysis (Vuli´c et al., 2015), such as detecting cultural differences (Gutiérrez et al., 2016) and bilingual dictionary extraction (Liu et al., 2015). Monolingual topic models can be evaluated through likelihood (Wallach et al., 2009b) or coherence (Newman et al., 2010), but topic model evaluation is not well understood in multilingual settings. Our contributions are two-fold. We introduce an improved intrinsic evaluation metric for multilingual topic models, called Crosslingual Normalized Pointwise Mutual Information (CNPMI, Section 2). We explore the behaviors of CNPMI at both the model and topic levels with six language pairs and varying model specifications. This metric Evaluating Multilingual Coherence A multilingual topic contains one topic for each language. For a multilingual topic to be meaningful to humans (Figure 1), the meanings should be consist"
N18-1099,D09-1026,0,0.0124036,"els are often used as a feature extraction technique for downstream machine learning Bible AM TL 0.607 0.796 RO SV 0.631 0.797 ZH TR 0.907 0.911 Train RO + SV ZH + TR RO + SV + ZH + TR 0.677 0.875 AM + TL 0.912 0.959 RO + SV 0.918 0.862 0.707 0.924 ZH + TR 0.919 0.848 AM + TL 0.951 0.898 0.694 0.918 AM + TL + ZH + TR 0.931 0.878 RO + SV + AM + TL 0.939 0.887 Table 4: At the model level, the estimator improves correlations between CNPMI and downstream classification for all languages except for Turkish. applications, and topic model evaluations should reflect whether these features are useful (Ramage et al., 2009). For each model, we apply a document classifier trained on the model parameters to test whether CNPMI is consistent with classification accuracy. Specifically, we want our classifier to transfer information from training on one language to testing on another (Smet et al., 2011; Heyman et al., 2016). We train a classifier on one language’s documents, where each document’s feature vector is the document-topic distribution θd . We apply this to TED Talks, where each document is labeled with multiple categories. We choose the most frequent seven categories across the corpus as labels,5 and only h"
N18-1099,N16-1142,0,0.0434465,"Missing"
N18-1099,D17-1203,1,0.855381,"s, each document d indexes a tuple of parallel/comparable language-specific documents, 1090 Proceedings of NAACL-HLT 2018, pages 1090–1100 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics d(`) , and the language-specific “views” of a document share the document-topic distribution θd . The generative story for the document-links model is: 1 2 3 4 5 6 7 8 for each topic k and each language ` do Draw a distribution over words φ`k ∼ Dirichlet(β);   Alternatively, word translations (Jagarlamudi and Daumé III, 2010), concept links (Gutiérrez et al., 2016; Yang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges. Since the polylingual topic model is the most common approach for building multilingual topic models (Vuli´c et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model. Monolingual Evaluation Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010). The most successful (Lau et al., 2014)"
N19-1158,C16-1126,0,0.0195465,"c formulation of knowledge transfer in multilingual topic models as a starting point of our analysis (Section 2.1). We then formulate Gibbs sampling as circular validation and quantify a loss during this phase (Section 2.2). This formulation leads us to a PAC-Bayesian bound that explicitly shows the factors that affect the crosslingual training (Section 2.3). Lastly, we look further into different transfer mechanisms in more depth (Section 2.4). Crosslingual Transfer Knowledge transfer through crosslingual representations has been studied in prior work. Smet and Moens (2009) and Heyman et al. (2016) show empirically how document classification using topic models implements the ideas of crosslingual transfer, but to date there has been no theoretical framework to analyze this transfer process in detail. In this paper, we describe two types of transfer—on-site and off-site—based on the nature of where and how the transfer takes place. We refer to transfer that happens while training topic models (i.e., during representation learning) as onsite. Once we obtain the low-dimensional representations, they can be used for downstream tasks. We refer to transfer in this phase as off-site, since th"
N19-1158,E14-1049,0,0.0639525,"et al., 2016), and various linguistic studies (Shutova et al., 2017; Barrett et al., 2016). Crosslingual learning methods generally extend monolingual algorithms by using various multilingual resources. In contrast to traditional high-dimensional vector space models, modern crosslingual models tend to rely on learning lowdimensional word representations that are more efficient and generalizable. A popular approach to representation learning comes from the word embedding community, in which words are represented as vectors in an embedding space shared by multiple languages (Ruder et al., 2018; Faruqui and Dyer, 2014; Klementiev et al., 2012). Another direction is from the topic modeling community, where words are projected into a probabilistic topic space (Ma and Nasukawa, 2017; Jagarlamudi and III, 2010). While formulated differently, 1.1 Background and Contributions Multilingual Topic Models Given a multilingual corpus D(1,...,L) in languages ℓ = 1, . . . , L as inputs, a multilingual topic model learns K topics. Each multilingual topic k (1,...,L) (k = 1, as an L-dimensional tuple ( . . . , K), is defined ) (1) (L) (ℓ) ϕk , . . . , ϕk , where ϕk is a multinomial distribution over the vocabulary V (ℓ)"
N19-1158,Q16-1004,0,0.353583,"Missing"
N19-1158,N18-1099,1,0.620572,"(Section 2). The upper bound explicitly shows the factors that can affect knowledge transfer. We then move on to off-site transfer, and focus on crosslingual document classification as a downstream task (Section 3). Finally, we show experimentally that the on-site transfer error can have impact on the performance of downstream tasks (Section 4). 2 Priors are an important component in Bayesian models like PLTM. In the original generative process of PLTM, each comparable document pair (dS , dT ) in the source and target languages (S, T ) is generated by the same multinomial θ ∼ Dir(α). Hao and Paul (2018) showed that knowledge transfer across languages happens through priors. Specifically, assume the source document is generated from θ(dS ) ∼ Dir(α), and has a sufficient statistics ndS ∈ NK where each cell nk|dS is the count of topic k in document dS . When generating the corresponding comparable document dT , the Dirichlet prior of the distribution over topics θ(dT ) , instead of a symmetric α, is parameterized by α + ndS . This formulation yields the same posterior estimation as the original joint model and is the foundation of our analysis in this section. To see this transfer process more"
N19-1158,C18-1220,1,0.613374,"theories (Section 2). The upper bound explicitly shows the factors that can affect knowledge transfer. We then move on to off-site transfer, and focus on crosslingual document classification as a downstream task (Section 3). Finally, we show experimentally that the on-site transfer error can have impact on the performance of downstream tasks (Section 4). 2 Priors are an important component in Bayesian models like PLTM. In the original generative process of PLTM, each comparable document pair (dS , dT ) in the source and target languages (S, T ) is generated by the same multinomial θ ∼ Dir(α). Hao and Paul (2018) showed that knowledge transfer across languages happens through priors. Specifically, assume the source document is generated from θ(dS ) ∼ Dir(α), and has a sufficient statistics ndS ∈ NK where each cell nk|dS is the count of topic k in document dS . When generating the corresponding comparable document dT , the Dirichlet prior of the distribution over topics θ(dT ) , instead of a symmetric α, is parameterized by α + ndS . This formulation yields the same posterior estimation as the original joint model and is the foundation of our analysis in this section. To see this transfer process more"
N19-1158,C12-1089,0,0.0376526,"us linguistic studies (Shutova et al., 2017; Barrett et al., 2016). Crosslingual learning methods generally extend monolingual algorithms by using various multilingual resources. In contrast to traditional high-dimensional vector space models, modern crosslingual models tend to rely on learning lowdimensional word representations that are more efficient and generalizable. A popular approach to representation learning comes from the word embedding community, in which words are represented as vectors in an embedding space shared by multiple languages (Ruder et al., 2018; Faruqui and Dyer, 2014; Klementiev et al., 2012). Another direction is from the topic modeling community, where words are projected into a probabilistic topic space (Ma and Nasukawa, 2017; Jagarlamudi and III, 2010). While formulated differently, 1.1 Background and Contributions Multilingual Topic Models Given a multilingual corpus D(1,...,L) in languages ℓ = 1, . . . , L as inputs, a multilingual topic model learns K topics. Each multilingual topic k (1,...,L) (k = 1, as an L-dimensional tuple ( . . . , K), is defined ) (1) (L) (ℓ) ϕk , . . . , ϕk , where ϕk is a multinomial distribution over the vocabulary V (ℓ) in language ℓ. From a huma"
N19-1158,D09-1092,0,0.213489,"each language ℓ has its own version of the topic. Multilingual topic models are generally extended from Latent Dirichlet Allocation (Blei et al., 2003, LDA). Though many variations have been proposed, the underlying structures of multilingual topic models are similar. These models require either a parallel/comparable corpus in multiple languages, or word translations from a 1551 Proceedings of NAACL-HLT 2019, pages 1551–1565 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics dictionary. One of the most popular models is the polylingual topic model (Mimno et al., 2009, PLTM ), where comparable document pairs share distributions over topics θ, while each language ℓ (ℓ) has its own distributions {ϕk }K k=1 over the vocabulary V (ℓ) . By re-marginalizing the estima(ℓ) tions {ϕbk }K k=1 , we obtain word representations (w) (w) K φ b ∈ R for each word w, where φ bk = Pr(zw = k|w), i.e., the probability of topic k given a word type w. To this end, we first describe a specific formulation of knowledge transfer in multilingual topic models as a starting point of our analysis (Section 2.1). We then formulate Gibbs sampling as circular validation and quantify a loss"
P18-2110,P07-1056,0,0.298685,"Missing"
P18-2110,P12-1011,0,0.0980017,"rs are often built to be applied to future data that doesn’t yet exist, and performance on held-out data is measured to estimate performance on future data whose distribution may have changed. Methods exist to adjust for changes in the data distribution (covariate shift) (Shimodaira, 2000; Bickel et al., 2009), but time is not typically incorporated into such methods explicitly. One line of work that explicitly studies the relationship between time and the distribution of data is work on classifying the time period in which a document was written (document dating) (Kanhabua and Nørv˚ag, 2008; Chambers, 2012; Kotsakos et al., 2014). However, this task is directed differently from our work: predicting timestamps given documents, rather than predicting information about documents given timestamps. Introduction Language, and therefore data derived from language, changes over time (Ullmann, 1962). Word senses can shift over long periods of time (Wilkins, 1993; Wijaya and Yeniterzi, 2011; Hamilton et al., 2016), and written language can change rapidly in online platforms (Eisenstein et al., 2014; Goel et al., 2016). However, little is known about how shifts in text over time affect the performance of"
P18-2110,D17-1119,0,0.0775209,"Missing"
P18-2110,P07-1033,0,0.400199,"Missing"
P18-2110,N09-1068,0,0.0593858,"Missing"
P18-2110,P16-1141,0,0.060377,"that explicitly studies the relationship between time and the distribution of data is work on classifying the time period in which a document was written (document dating) (Kanhabua and Nørv˚ag, 2008; Chambers, 2012; Kotsakos et al., 2014). However, this task is directed differently from our work: predicting timestamps given documents, rather than predicting information about documents given timestamps. Introduction Language, and therefore data derived from language, changes over time (Ullmann, 1962). Word senses can shift over long periods of time (Wilkins, 1993; Wijaya and Yeniterzi, 2011; Hamilton et al., 2016), and written language can change rapidly in online platforms (Eisenstein et al., 2014; Goel et al., 2016). However, little is known about how shifts in text over time affect the performance of language processing systems. This paper focuses on a standard text processing task, document classification, to provide insight into how classification performance varies with time. We consider both long-term variations in text over time and seasonal variations which change throughout a year but repeat across years. Our empirical study considers corpora contain694 Proceedings of the 56th Annual Meeting"
P18-2110,D13-1187,0,0.138968,"Missing"
P19-1403,D17-1118,0,0.0158493,"diachronic word embeddings to study how language changes over time (Kulkarni et al., 2015; Hamilton et al., 2016; Kutuzov et al., 2018). These studies have shown that shifts in the corpora across time cause changes in word contexts and consequently, changes in the learned representations. In our study, we further examine these shifts as they relate to important features for document classification. While other research has applied diachronic word embeddings to semantic change detection and validation (Mihalcea and Nastase, 2012; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Dubossarsky et al., 2017; Yao et al., 2018; Rudolph and Blei, 2018; Rosenfeld and Erk, 2018) and semantic relation analysis (Liao and Cheng, 2016; Szymanski, 2017; Rosin et al., 2017), these types of embeddings have not been studied particularly for the document classification task. We show that neural classifiers which use these embeddings can perform better on future data. As part of this work, we propose a new method for constructing diachronic words embeddings, which we show to be competitive with prior approaches. Second, we propose a neural classification model that adapts to changes in time using ideas from do"
P19-1403,P16-1141,0,0.163676,"ent research has shown that document classifiers can become more stable over time when trained in ways that specifically account for temporal variations (Huang and Paul, 2018; He et al., 2018). We refer to this task of accounting for such variations during training as temporality adaptation. This paper investigates temporality adaptation in two ways. First, we explore how diachronic word embeddings, which encode time-varying representations of words, can be used in this setting. Recent research has used diachronic word embeddings to study how language changes over time (Kulkarni et al., 2015; Hamilton et al., 2016; Kutuzov et al., 2018). These studies have shown that shifts in the corpora across time cause changes in word contexts and consequently, changes in the learned representations. In our study, we further examine these shifts as they relate to important features for document classification. While other research has applied diachronic word embeddings to semantic change detection and validation (Mihalcea and Nastase, 2012; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Dubossarsky et al., 2017; Yao et al., 2018; Rudolph and Blei, 2018; Rosenfeld and Erk, 2018) and semantic relatio"
P19-1403,P18-2110,1,0.664159,"fication model inspired by methods for domain adaptation. Experiments on six corpora show how these methods can make classifiers more robust over time. 1 Introduction Language changes and varies over time, which can cause a degradation of performance in natural language processing models over time. For example, document classifiers are typically trained on historical data and tested on future data, where the performance tends to be worse. Recent research has shown that document classifiers can become more stable over time when trained in ways that specifically account for temporal variations (Huang and Paul, 2018; He et al., 2018). We refer to this task of accounting for such variations during training as temporality adaptation. This paper investigates temporality adaptation in two ways. First, we explore how diachronic word embeddings, which encode time-varying representations of words, can be used in this setting. Recent research has used diachronic word embeddings to study how language changes over time (Kulkarni et al., 2015; Hamilton et al., 2016; Kutuzov et al., 2018). These studies have shown that shifts in the corpora across time cause changes in word contexts and consequently, changes in the"
P19-1403,D14-1181,0,0.0143017,"Missing"
P19-1403,W14-2517,0,0.0168916,"of words, can be used in this setting. Recent research has used diachronic word embeddings to study how language changes over time (Kulkarni et al., 2015; Hamilton et al., 2016; Kutuzov et al., 2018). These studies have shown that shifts in the corpora across time cause changes in word contexts and consequently, changes in the learned representations. In our study, we further examine these shifts as they relate to important features for document classification. While other research has applied diachronic word embeddings to semantic change detection and validation (Mihalcea and Nastase, 2012; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Dubossarsky et al., 2017; Yao et al., 2018; Rudolph and Blei, 2018; Rosenfeld and Erk, 2018) and semantic relation analysis (Liao and Cheng, 2016; Szymanski, 2017; Rosin et al., 2017), these types of embeddings have not been studied particularly for the document classification task. We show that neural classifiers which use these embeddings can perform better on future data. As part of this work, we propose a new method for constructing diachronic words embeddings, which we show to be competitive with prior approaches. Second, we propose a neural"
P19-1403,C18-1117,0,0.0433647,"that document classifiers can become more stable over time when trained in ways that specifically account for temporal variations (Huang and Paul, 2018; He et al., 2018). We refer to this task of accounting for such variations during training as temporality adaptation. This paper investigates temporality adaptation in two ways. First, we explore how diachronic word embeddings, which encode time-varying representations of words, can be used in this setting. Recent research has used diachronic word embeddings to study how language changes over time (Kulkarni et al., 2015; Hamilton et al., 2016; Kutuzov et al., 2018). These studies have shown that shifts in the corpora across time cause changes in word contexts and consequently, changes in the learned representations. In our study, we further examine these shifts as they relate to important features for document classification. While other research has applied diachronic word embeddings to semantic change detection and validation (Mihalcea and Nastase, 2012; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Dubossarsky et al., 2017; Yao et al., 2018; Rudolph and Blei, 2018; Rosenfeld and Erk, 2018) and semantic relation analysis (Liao and Ch"
P19-1403,W02-0109,0,0.0696928,"Missing"
P19-1403,P12-2051,0,0.214448,"time-varying representations of words, can be used in this setting. Recent research has used diachronic word embeddings to study how language changes over time (Kulkarni et al., 2015; Hamilton et al., 2016; Kutuzov et al., 2018). These studies have shown that shifts in the corpora across time cause changes in word contexts and consequently, changes in the learned representations. In our study, we further examine these shifts as they relate to important features for document classification. While other research has applied diachronic word embeddings to semantic change detection and validation (Mihalcea and Nastase, 2012; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Dubossarsky et al., 2017; Yao et al., 2018; Rudolph and Blei, 2018; Rosenfeld and Erk, 2018) and semantic relation analysis (Liao and Cheng, 2016; Szymanski, 2017; Rosin et al., 2017), these types of embeddings have not been studied particularly for the document classification task. We show that neural classifiers which use these embeddings can perform better on future data. As part of this work, we propose a new method for constructing diachronic words embeddings, which we show to be competitive with prior approaches. Second, w"
P19-1403,D17-1121,0,0.0324009,"Missing"
P19-1403,P17-2071,0,0.0151036,"dies have shown that shifts in the corpora across time cause changes in word contexts and consequently, changes in the learned representations. In our study, we further examine these shifts as they relate to important features for document classification. While other research has applied diachronic word embeddings to semantic change detection and validation (Mihalcea and Nastase, 2012; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Dubossarsky et al., 2017; Yao et al., 2018; Rudolph and Blei, 2018; Rosenfeld and Erk, 2018) and semantic relation analysis (Liao and Cheng, 2016; Szymanski, 2017; Rosin et al., 2017), these types of embeddings have not been studied particularly for the document classification task. We show that neural classifiers which use these embeddings can perform better on future data. As part of this work, we propose a new method for constructing diachronic words embeddings, which we show to be competitive with prior approaches. Second, we propose a neural classification model that adapts to changes in time using ideas from domain adaptation. We previously showed that out-of-the-box domain adaptation techniques can make n-gram classifiers more robust to temporal"
P19-1403,N16-1174,0,0.0940847,"Missing"
P19-1489,W04-3230,0,\N,Missing
P19-1489,W02-1001,0,\N,Missing
P19-1489,D09-1092,0,\N,Missing
P19-1489,H93-1061,0,\N,Missing
P19-1489,D11-1024,0,\N,Missing
P19-1489,N10-1012,0,\N,Missing
P19-1489,W15-1806,0,\N,Missing
P19-1489,P15-1162,1,\N,Missing
P19-1489,E14-1056,0,\N,Missing
P19-1489,goldhahn-etal-2012-building,0,\N,Missing
P19-1489,C12-1089,0,\N,Missing
P19-1489,N15-1128,0,\N,Missing
P19-1489,N15-1104,0,\N,Missing
P19-1489,Q17-1010,0,\N,Missing
P19-1489,W16-1620,0,\N,Missing
P19-1489,P16-1024,0,\N,Missing
P19-1489,D16-1250,0,\N,Missing
P19-1489,L16-1521,0,\N,Missing
P19-1489,P17-1179,0,\N,Missing
P19-1489,P17-1042,0,\N,Missing
P19-1489,D18-1043,0,\N,Missing
P19-1489,E14-1049,0,\N,Missing
P19-1489,D15-1243,0,\N,Missing
P19-1489,2016.gwc-1.30,0,\N,Missing
Q15-1004,D11-1024,0,0.680152,"l to jointly infer topic hierarchies and author perspective, which we apply to corpora of political debates and online reviews. We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks. 1 Introduction Topic models can be a powerful aid for analyzing large collections of text by uncovering latent interpretable structures without manual supervision. Yet people often have expectations about topics in a given corpus and how they should be structured for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents S PRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. S PRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and Mc"
Q15-1004,N13-1017,1,0.914432,"the desired output. This paper presents S PRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. S PRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). S PRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies and factorizations (§3), and we will show that this framework captures many existin"
Q15-1004,D10-1007,1,0.813793,"cific word distributions. This is an alternative to the more common approach to regression based topic modeling, where the variables affect the topic distributions rather than the word distributions. Our S PRITE-based model does both: the document features adjust the prior over topic distributions (through δ), but by tying together the document and topic components (with β), the document features also affect the prior over word distributions. To the best of our knowledge, this is the first topic model to condition both topic and word distributions on the same features. The topic aspect model (Paul and Girju, 2010a) is also a two-dimensional factored model that has been used to jointly model topic and perspective (Paul and Girju, 2010b). However, this model does not use structured priors over the parameters, unlike most of the models discussed in §4. An alternative approach to incorporating user preferences and expertise are interactive topic models (Hu et al., 2013), a complimentary approach to S PRITE. 9 Discussion and Conclusion We have presented S PRITE, a family of topic models that utilize structured priors to induce preferred topic structures. Specific instantiations of S PRITE are similar or eq"
Q15-1004,D09-1026,0,0.169968,"rovide a flexible framework for encoding preferences as priors for how topics should be structured. S PRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). S PRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies and factorizations (§3), and we will show that this framework captures many existing topic models (§4). After describing the general form of the model, we show h"
Q15-1004,P06-1072,0,0.0304323,"onte Carlo EM approach, using a tial derivative τt ρ−1 βkc , which adds extra weight to collapsed Gibbs sampler to sample from the posthe sparse Dirichlet prior in the objective. The terior of the topic assignments z conditioned on algorithm used in our experiments begins with the hyperparameters, then optimizing the hyperpaτ1 = 1 and optionally increases τ over time. This rameters using gradient-based optimization condiis a deterministic annealing approach, where τ tioned on the samples. corresponds to an inverse temperature (Ueda and Given the hyperparameters, the sampling equaNakano, 1998; Smith and Eisner, 2006). tions are identical to the standard LDA sampler As τ approaches infinity, the prior-annealed (Griffiths and Steyvers, 2004). The partial derivaMAP objective maxβ P (φ|β)P (β)τ approaches tive of the collapsed log likelihood L of the corpus maxβ P (φ|β) maxβ P (β). Annealing only the with respect to each hyperparameter βkc is: prior P (β) results in maximization of this term only, while the outer max chooses a good β under ∂L ∂P (β) X = + ωcv φ˜kv × (1) P (φ|β) as a tie-breaker among all β values that ∂βkc ∂βkc   v P P 0 maximize the inner max (binary-valued β).3 k k Ψ(nv + φ˜kv ) −Ψ(φ˜kv )"
Q15-1004,N12-1096,1,\N,Missing
Q15-1004,P11-1026,0,\N,Missing
R09-1061,D08-1038,0,0.118581,"l editors. Another approach to the analysis of scientific research relies on topic models which uncover structures used to explore text collections. In particular, they divide documents according to their topics and use the hidden structure to determine similarity between documents. Popular unsupervised topic models such as Latent Dirichlet Allocation (LDA) [2] and hierarchical models [7] have been successfully applied to various publications such as The American Political Science Review and Science. In Computational Linguistics, the only work of which we are aware is that of Hall et al. 2008 [4] who study the history of ideas using LDA and topic entropy. In this paper we extend over the work of Hall et al. 2008 [4] by adding two related fields (Linguistics and Education) and by employing various novel topic models for scientific research analysis. 3 Approach In this section we present the data used in this research and the topic models employed. We categorize both by topics 337 International Conference RANLP 2009 - Borovets, Bulgaria, pages 337–342 Field Venue LING LING LING LING CL CL CL CL CL EDU EDU Language Linguistics, Journal of Linguistic Inquiry Ling. & Philosophy ACL EACL NA"
S19-1015,P04-3031,0,0.687106,"Missing"
S19-1015,W06-1615,0,0.190538,"n how different groups express the concepts being classified (e.g., sentiment). The Twitter data shows the most variation while the Yelp hotel data shows the least variation. 3 Model Models for user factor adaptation generally treat this as a problem of domain adaptation (Volkova et al., 2013; Lynn et al., 2017). Domain adaptation methods are used to learn models that can be applied to data whose distributions may differ from the training data. Commonly used methods include feature augmentation (Daume III, 2007; Joshi et al., 2013; Huang and Paul, 2018) and structural correspondence learning (Blitzer et al., 2006), while recent approaches rely on domain adversarial training (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Huang et al., 2018). We borrow concepts of domain adaptation to construct a model that is robust to variations across user factors. Domain Sampling and Model Inputs. Our model requires all domains (demographic attributes) to be known during training, but not all attributes are known in our datasets. Instead of explicitly modeling the missing data, we simply sample documents where all user attributes of interest are available. At test time, this limitation does not apply becau"
S19-1015,P15-1073,0,0.0588225,"a positive way, while men were more likely to use the word in a negative expression (Volkova et al., 2013). Models for text classification, the automatic categorization of documents into categories, typically ignore attributes about the authors of the text. With the growing amount of text generated by users online, whose personal characteristics are highly variable, there has been increased attention to how user demographics are associated with the text they write. Promising recent studies have shown that incorporating demographic factors can improve text classification (Volkova et al., 2013; Hovy, 2015; Yang and Eisenstein, 2017; Li et al., 2018). Lynn et al. (2017) refer to this idea as user factor adaptation and proposed to treat this as a domain adaptation problem in which demographic attributes constitute different domains. We extend this line of work in a number of ways: In this study, we treat adapting across the demographic factors as a domain work problem, in which we consider each demographic factor as a domain. We focus on four different demographic factors (gender, age, country, region) in four English-language social media datasets (Twitter, Amazon reviews, Yelp hotel reviews, a"
S19-1015,D18-1469,0,0.0465688,"Missing"
S19-1015,D11-1120,0,0.130926,"Missing"
S19-1015,D18-1074,1,0.839455,"l data shows the least variation. 3 Model Models for user factor adaptation generally treat this as a problem of domain adaptation (Volkova et al., 2013; Lynn et al., 2017). Domain adaptation methods are used to learn models that can be applied to data whose distributions may differ from the training data. Commonly used methods include feature augmentation (Daume III, 2007; Joshi et al., 2013; Huang and Paul, 2018) and structural correspondence learning (Blitzer et al., 2006), while recent approaches rely on domain adversarial training (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Huang et al., 2018). We borrow concepts of domain adaptation to construct a model that is robust to variations across user factors. Domain Sampling and Model Inputs. Our model requires all domains (demographic attributes) to be known during training, but not all attributes are known in our datasets. Instead of explicitly modeling the missing data, we simply sample documents where all user attributes of interest are available. At test time, this limitation does not apply because only the document text is 140 Region … … … Gender + … … Gradient Reversal Layer NE Demographic Attributes Predictions MW K Domain Bi-LST"
S19-1015,P18-2110,1,0.853996,"own in Figure 2. Lower percentages indicate higher variation in how different groups express the concepts being classified (e.g., sentiment). The Twitter data shows the most variation while the Yelp hotel data shows the least variation. 3 Model Models for user factor adaptation generally treat this as a problem of domain adaptation (Volkova et al., 2013; Lynn et al., 2017). Domain adaptation methods are used to learn models that can be applied to data whose distributions may differ from the training data. Commonly used methods include feature augmentation (Daume III, 2007; Joshi et al., 2013; Huang and Paul, 2018) and structural correspondence learning (Blitzer et al., 2006), while recent approaches rely on domain adversarial training (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Huang et al., 2018). We borrow concepts of domain adaptation to construct a model that is robust to variations across user factors. Domain Sampling and Model Inputs. Our model requires all domains (demographic attributes) to be known during training, but not all attributes are known in our datasets. Instead of explicitly modeling the missing data, we simply sample documents where all user attributes of interest are"
S19-1015,P07-1033,0,0.143626,"Missing"
S19-1015,D10-1124,0,0.145814,"Missing"
S19-1015,K15-1011,0,0.067942,"ith neural domain adaptation models (Ganin et al., 2016), which may provide better performance than the simpler models used in prior work on user factor adaptation. We also propose a new model using a multitask framework with adversarial training. • Our approach requires demographic attributes at training time but not at test time: we learn a single representation to be invariant to demographic changes. This approach thus requires fewer resources than prior work. Introduction Different demographic groups can show substantial linguistic variations, especially in online data (Goel et al., 2016; Johannsen et al., 2015). These variations can affect natural language processing models such as sentiment classifiers. For example, researchers found that women were more likely to use the word weakness in a positive way, while men were more likely to use the word in a negative expression (Volkova et al., 2013). Models for text classification, the automatic categorization of documents into categories, typically ignore attributes about the authors of the text. With the growing amount of text generated by users online, whose personal characteristics are highly variable, there has been increased attention to how user d"
S19-1015,D18-1002,0,0.0415821,"Missing"
S19-1015,N13-1080,0,0.0562705,"Missing"
S19-1015,D14-1181,0,0.00569373,"metric is weighted F1 score. 4.2 Adaptation Models Baselines: No Adaptation We compare to three standard classifiers that do not perform adaptation. N-gram. We extract TF-IDF-weighted features of 1-, 2-, and 3-grams on the corpora, using the most frequent 15K features with the minimum feature frequency as 2. We trained a logistic regression classifier using the SGDClassifier implementation in scikit-learn (Pedregosa et al., 2011) using a batch size of 256 and 1,000 iterations. CNN. We used Keras (Chollet et al., 2015) to implement the Convolutional Neural Network (CNN) classifier described in Kim (2014). To keep consistent, we initialize the embedding weight with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014). We only keep the 15K most frequent words and replace the rest with an “unk” token. Each document was padded to a length of 50. We keep all parameter settings as described in the paper. We fed 50 documents to the model each batch and trained for 20 epochs. DANN. We consider the domain adversarial training network (Ganin et al., 2016) (DANN) on the user factor adaptation task. We use Keras to implement the same network and deploy the same pre-trained word emb"
S19-1015,P17-1060,0,0.0216242,"nction for the region and a binary sigmoid function for the other user demographic factors. required as input to the document classifier. Shared Embedding Space. We use a common embedding layer for both document and demographic factor predictions. The goal is that the trained embeddings will capture the language variations that are associated with the demographic groups as well as document labels. Parameters are initialized with pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014). K+2 Bi-LSTMs. We combine ideas from two previous works on domain adaptation (Liu et al., 2017; Kim et al., 2017). Kim et al. (2017) proposed K+1 Bi-LSTMs, where K is the number of domains, and Liu et al. (2017) proposed to combine shared and independent Bi-LSTMs for each prediction task. In our model, we create one independent Bi-LSTM for each demographic domain (blue), one independent Bi-LSTM for the document classifier (orange), and one shared Bi-LSTM that is used in both the demographic prediction and document classification tasks (yellow). The intuition is to transfer learned information to one and the other through this shared Bi-LSTM while leaving some free spaces for both document label and demog"
S19-1015,P18-2005,0,0.0163656,"ely to use the word in a negative expression (Volkova et al., 2013). Models for text classification, the automatic categorization of documents into categories, typically ignore attributes about the authors of the text. With the growing amount of text generated by users online, whose personal characteristics are highly variable, there has been increased attention to how user demographics are associated with the text they write. Promising recent studies have shown that incorporating demographic factors can improve text classification (Volkova et al., 2013; Hovy, 2015; Yang and Eisenstein, 2017; Li et al., 2018). Lynn et al. (2017) refer to this idea as user factor adaptation and proposed to treat this as a domain adaptation problem in which demographic attributes constitute different domains. We extend this line of work in a number of ways: In this study, we treat adapting across the demographic factors as a domain work problem, in which we consider each demographic factor as a domain. We focus on four different demographic factors (gender, age, country, region) in four English-language social media datasets (Twitter, Amazon reviews, Yelp hotel reviews, and Yelp restaurant reviews), which contain te"
S19-1015,D13-1187,0,0.454646,"Missing"
S19-1015,P17-1001,0,0.14106,"hile the Yelp hotel data shows the least variation. 3 Model Models for user factor adaptation generally treat this as a problem of domain adaptation (Volkova et al., 2013; Lynn et al., 2017). Domain adaptation methods are used to learn models that can be applied to data whose distributions may differ from the training data. Commonly used methods include feature augmentation (Daume III, 2007; Joshi et al., 2013; Huang and Paul, 2018) and structural correspondence learning (Blitzer et al., 2006), while recent approaches rely on domain adversarial training (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Huang et al., 2018). We borrow concepts of domain adaptation to construct a model that is robust to variations across user factors. Domain Sampling and Model Inputs. Our model requires all domains (demographic attributes) to be known during training, but not all attributes are known in our datasets. Instead of explicitly modeling the missing data, we simply sample documents where all user attributes of interest are available. At test time, this limitation does not apply because only the document text is 140 Region … … … Gender + … … Gradient Reversal Layer NE Demographic Attributes Predictio"
S19-1015,D17-1119,0,0.4387,"d in a negative expression (Volkova et al., 2013). Models for text classification, the automatic categorization of documents into categories, typically ignore attributes about the authors of the text. With the growing amount of text generated by users online, whose personal characteristics are highly variable, there has been increased attention to how user demographics are associated with the text they write. Promising recent studies have shown that incorporating demographic factors can improve text classification (Volkova et al., 2013; Hovy, 2015; Yang and Eisenstein, 2017; Li et al., 2018). Lynn et al. (2017) refer to this idea as user factor adaptation and proposed to treat this as a domain adaptation problem in which demographic attributes constitute different domains. We extend this line of work in a number of ways: In this study, we treat adapting across the demographic factors as a domain work problem, in which we consider each demographic factor as a domain. We focus on four different demographic factors (gender, age, country, region) in four English-language social media datasets (Twitter, Amazon reviews, Yelp hotel reviews, and Yelp restaurant reviews), which contain text authored by a div"
S19-1015,W18-5904,1,0.826035,"the most accurate (Jung et al., 2018) and least biased (Buolamwini and Gebru, 2018). We filtered out users that are inferred to be younger than 12 years old. If multiple faces are in an image, we used the first result from the API. Gender is encoded with two values, male and female. For simplicity, we also binarized the age values (≤30 and &gt;30). Data We experiment with four corpora from three social media sources: • Twitter: Tweets were labeled with whether they indicate that the user received an influenza vaccination (i.e., a flu shot) (Huang et al., 2017), used in a recent NLP shared task (Weissenbacher et al., 2018). Country and Region. We define two factors based on the location of the user. For the Twitter data, we inferred the location of each user with the Carmen geolocation system (Dredze et al., 2013), which resolves the user’s location string in their profile to a structured location. Because this comes from the user profile, it is generally taken to be the “home” location of the user. For Amazon and Yelp, we collected user locations listed in their profiles, then used pattern matching and manual whitelisting to resolve the strings to specific locations (city, state, country). To construct user fa"
S19-1015,D14-1039,0,0.0413689,"Missing"
S19-1015,P11-1096,0,0.0706978,"Missing"
S19-1015,D14-1162,0,0.081805,"ction task. The predictions use the final concatenated representations, where the prediction is modeled with a softmax function for the region and a binary sigmoid function for the other user demographic factors. required as input to the document classifier. Shared Embedding Space. We use a common embedding layer for both document and demographic factor predictions. The goal is that the trained embeddings will capture the language variations that are associated with the demographic groups as well as document labels. Parameters are initialized with pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014). K+2 Bi-LSTMs. We combine ideas from two previous works on domain adaptation (Liu et al., 2017; Kim et al., 2017). Kim et al. (2017) proposed K+1 Bi-LSTMs, where K is the number of domains, and Liu et al. (2017) proposed to combine shared and independent Bi-LSTMs for each prediction task. In our model, we create one independent Bi-LSTM for each demographic domain (blue), one independent Bi-LSTM for the document classifier (orange), and one shared Bi-LSTM that is used in both the demographic prediction and document classification tasks (yellow). The intuition is to transfer learned information"
S19-1015,Q17-1021,0,0.0696259,"ay, while men were more likely to use the word in a negative expression (Volkova et al., 2013). Models for text classification, the automatic categorization of documents into categories, typically ignore attributes about the authors of the text. With the growing amount of text generated by users online, whose personal characteristics are highly variable, there has been increased attention to how user demographics are associated with the text they write. Promising recent studies have shown that incorporating demographic factors can improve text classification (Volkova et al., 2013; Hovy, 2015; Yang and Eisenstein, 2017; Li et al., 2018). Lynn et al. (2017) refer to this idea as user factor adaptation and proposed to treat this as a domain adaptation problem in which demographic attributes constitute different domains. We extend this line of work in a number of ways: In this study, we treat adapting across the demographic factors as a domain work problem, in which we consider each demographic factor as a domain. We focus on four different demographic factors (gender, age, country, region) in four English-language social media datasets (Twitter, Amazon reviews, Yelp hotel reviews, and Yelp restaurant reviews)"
S19-1015,D17-1323,0,0.107018,"Missing"
W01-1615,W99-0207,1,\N,Missing
W01-1615,P00-1053,0,\N,Missing
W01-1615,P95-1017,0,\N,Missing
W09-0418,2007.mtsummit-papers.48,1,0.737342,"ocabulary (OOV) words, i.e., source language words that do not occur in the training corpus. For unknown words, no translation entry is available in the statistical translation model (phrase-table). As a result, these OOV words cannot be translated. Dealing with languages with a rich morphology like Spanish and having a limited amount of bilingual resources make this problem even more severe. There have been several efforts in dealing with OOV words to improve translation quality. In addition to parallel text corpora, external bilingual dictionaries can be exploited to reduce the OOV problem (Okuma et al., 2007). However, these approaches depend on the coverage of the utilized external dictionaries. Data sparseness problems due to inflectional variations were previously addressed by applying word transformations using stemming or lemmatization (Popovic and Ney, 2005; Gupta and Federico, 2006). A tight integration of morphosyntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. However, these approaches still suffer from"
W09-0418,W05-0909,0,0.0471768,"Missing"
W09-0418,P02-1040,0,0.0750922,"Missing"
W09-0418,W08-0309,0,0.0308403,"n quality of the baseline phrasebased SMT system. 1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machine Translation. We participated in the SpanishEnglish translation task under the Constrained Condition. For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the new"
W09-0418,W08-0334,1,0.83212,", we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and disProceedings of the Fourth Workshop on Statistical Machine Translation , pages 105–109, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 105 katakana1 words with phonetically"
W09-0418,I08-8003,1,0.924723,", we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and disProceedings of the Fourth Workshop on Statistical Machine Translation , pages 105–109, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 105 katakana1 words with phonetically"
W09-0418,W07-0717,0,0.0215096,"ning of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and disProceedings of the Fourth Workshop on Statistical Machine Translation , pages 105–109, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 105 katakana1"
W09-0418,P97-1017,0,0.0702819,"posed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. However, these approaches still suffer from the data sparseness problem, since lemmata and inflectional forms never seen in the training corpus cannot be translated. In order to generate translations for unknown words, previous approaches focused on transliteration methods, where a sequence of characters is mapped from one writing system into another. For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introduced a probabilistic model that replaces Japanese Abstract This paper describes the NICT statistical machine translation (SMT) system used for the WMT 2009 Shared Task (WMT09) evaluation. We participated in the Spanish-English translation task. The focus of this year’s participation was to investigate model adaptation and transliteration techniques in order to improve the translation quality of the baseline phrasebased SMT system. 1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machi"
W09-0418,D07-1091,0,0.0188625,"have been several efforts in dealing with OOV words to improve translation quality. In addition to parallel text corpora, external bilingual dictionaries can be exploited to reduce the OOV problem (Okuma et al., 2007). However, these approaches depend on the coverage of the utilized external dictionaries. Data sparseness problems due to inflectional variations were previously addressed by applying word transformations using stemming or lemmatization (Popovic and Ney, 2005; Gupta and Federico, 2006). A tight integration of morphosyntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. However, these approaches still suffer from the data sparseness problem, since lemmata and inflectional forms never seen in the training corpus cannot be translated. In order to generate translations for unknown words, previous approaches focused on transliteration methods, where a sequence of characters is mapped from one writing system into another. For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introdu"
W09-0418,2005.mtsummit-papers.11,0,0.0369022,"ation task. The focus of this year’s participation was to investigate model adaptation and transliteration techniques in order to improve the translation quality of the baseline phrasebased SMT system. 1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machine Translation. We participated in the SpanishEnglish translation task under the Constrained Condition. For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined"
W09-0418,niessen-etal-2000-evaluation,0,0.0181976,"8.36 4.3 Effects of Transliteration In order to investigate the effects of transliteration, we trained three different transliteration using the phrase-table of the baseline systems trained on (a) only the NC corpus, (b) only the EP corpus, and (c) on the merged corpus (NC+EP). The performance of these phrase-based transliteration models is evaluated for 2000 randomly selected transliteration examples. Table 5 summarizes the haracter-based automatic evaluation scores for the word error rate (WER) metrics, i.e., the edit distance between the system output and the closest reference translation (Niessen et al., 2000), as well as the BLEU and METEOR metrics. The best performance is achieved when training examples from both domains are exploit to transliterate unknown Spanish words into English. Therefore, the NC+EP transliteration model was applied to the translation outputs of all mixture models described in Section 4.2. The effects of the transliteration post-process are summarized in Table 6. Transliteration consisTable 7: Testset 2009 Performance weight NC Eval optimization BLEU tm 21.07 tm+lm 20.95 tm+dm 21.45 tm+lm+dm 21.67∗ EP Eval BLEU 20.81 20.59 21.32 21.27 5 Conclusion The work for this year’s s"
W09-0418,J03-1002,0,0.0112243,"Missing"
W09-0418,P03-1021,0,0.0130472,"Phrase penalty Language model probability Lexical reordering probability Simple distance-based distortion model Word penalty For the training of the statistical models, standard word alignment (GIZA++ (Och and Ney, 2003)) and language modeling (SRILM (Stolcke, 2002)) tools were used. We used 5-gram language models trained with modified Knesser-Ney smoothing. The language models were trained on the target side of the provided training corpora. Minimum error rate training (MERT) with respect to BLEU score was used to tune the decoder’s parameters, and performed using the technique proposed in (Och, 2003). For the translation, the inhouse multi-stack phrase-based decoder CleopATRa was used. 4.1 Baseline Our baseline system is a fairly typical phrasebased machine translation system (Finch and Sumita, 2008a) built within the framework of a feature-based exponential model containing the following features: The automatic evaluation scores of the baseline systems trained on (a) only the NC corpus and (b) only on the EP corpus are summarized in Table 3. 107 Table 3: Baseline Performance baseline NC Eval BLEU METEOR 17.56 40.52 Table 5: Transliteration Performance EP Eval BLEU METEOR 33.00 56.50 Trai"
W09-0418,N03-1017,0,\N,Missing
W09-1111,P98-1013,0,0.0186448,"Missing"
W09-1111,W04-3205,0,0.0297468,"tions, in particular on the quantifiers each other and one another (Dalrymple et al., 1998; Heim, 1991; K¨onig, 2005). Most of this work has been done by language typologists (Maslova and Nedjalkov, 2005; Haspelmath, 2007) who are interested in how reciprocal constructions of these types vary from one language to another and they do this through comparative studies of large sets of world’s languages. In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs ((Hearst, 1998; Chklovski and Pantel, 2004; Etzioni et al., 2004; Turney, 2006; Davidov and Rappoport, 2008) inter alia). We extend over these approaches in two ways: (i) our patterns indicate a new type of relation between verbs, (ii) instead of seed or hook words we use a set of simple but effective pronoun templates which ensure the validity of the patterns extracted. To the best of our knowledge, the rest of our reciprocity model is novel. In particular, we use a novel procedure which extracts pairs of reciprocal instances and present two novel unsupervised clustering methods which group the instance pairs in meaningful ways. We a"
W09-1111,P08-1079,0,0.122432,"er (Dalrymple et al., 1998; Heim, 1991; K¨onig, 2005). Most of this work has been done by language typologists (Maslova and Nedjalkov, 2005; Haspelmath, 2007) who are interested in how reciprocal constructions of these types vary from one language to another and they do this through comparative studies of large sets of world’s languages. In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs ((Hearst, 1998; Chklovski and Pantel, 2004; Etzioni et al., 2004; Turney, 2006; Davidov and Rappoport, 2008) inter alia). We extend over these approaches in two ways: (i) our patterns indicate a new type of relation between verbs, (ii) instead of seed or hook words we use a set of simple but effective pronoun templates which ensure the validity of the patterns extracted. To the best of our knowledge, the rest of our reciprocity model is novel. In particular, we use a novel procedure which extracts pairs of reciprocal instances and present two novel unsupervised clustering methods which group the instance pairs in meaningful ways. We also present some interesting observations on the data thus obtaine"
W09-1111,J06-3003,0,0.0412294,"and one another (Dalrymple et al., 1998; Heim, 1991; K¨onig, 2005). Most of this work has been done by language typologists (Maslova and Nedjalkov, 2005; Haspelmath, 2007) who are interested in how reciprocal constructions of these types vary from one language to another and they do this through comparative studies of large sets of world’s languages. In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs ((Hearst, 1998; Chklovski and Pantel, 2004; Etzioni et al., 2004; Turney, 2006; Davidov and Rappoport, 2008) inter alia). We extend over these approaches in two ways: (i) our patterns indicate a new type of relation between verbs, (ii) instead of seed or hook words we use a set of simple but effective pronoun templates which ensure the validity of the patterns extracted. To the best of our knowledge, the rest of our reciprocity model is novel. In particular, we use a novel procedure which extracts pairs of reciprocal instances and present two novel unsupervised clustering methods which group the instance pairs in meaningful ways. We also present some interesting observa"
W09-1111,H05-1044,0,0.00932439,"Missing"
W09-1111,C98-1013,0,\N,Missing
W10-1760,W07-0718,0,0.0433782,"Missing"
W10-1760,P02-1040,0,0.0790848,"th), ICTCLAS (zh), HanTagger (ko). No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step iteraExperiments The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus"
W10-1760,W08-0336,0,0.0752938,"stent with the extracted dictionary to create a word lattice for decoding. The method proposed in this papers differs from previous approaches in the following points: “consistent with SMT models” is one that identifies translation units that are small enough to be translatable, but large enough to be meaningful in the context of the given input sentence, achieving a trade-off between the coverage and the translation task complexity of the statistical models in order to improve translation quality. The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al., 2008). However, improvements have been reported for approaches taking into account not only monolingual, but also bilingual information, to derive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suita"
W10-1760,W96-0213,0,0.138718,"e shown themselves to be highly effective in a broad range of NLP tasks including sentence boundary detection or part-of-speech tagging (Berger et al., 1996). A maximum entropy classifier is an exponential model consisting of a number of binary feature functions and their weights (Pietra et al., 1997). The model is trained by adjusting the weights to maximize the entropy of the probabilistic model given constraints imposed by the training data. In our experiments, we use a conditional maximum entropy model, where the conditional probability of the outcome given the set of features is modeled (Ratnaparkhi, 1996). The model has the form: Iteration 1 segmented SRC Iteration 2 segmented SRC SMT1 … (7) extract (4) annotate (5) resegment ME1 classifier (9) resegment decode evalchr decode eval1 decode evalJ-1 better (6) train SRCtoken TRGword alignment (8) annotate better (J-1) train SMTJ-1 … extract (J) train SMTJ worse evalJ SRCtoken TRGword alignment ME2 classifier … MEJ-1 classifier Iteration J-1 segmented SRC Selected Word Segmenter … Figure 2: Iterative Bootstrap Method The feature set is given in Table 1. The lexical context features consist of target words annotated with a tag t. w0 denotes the wor"
W10-1760,D09-1075,0,0.0563363,"erive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. • it works for any language pair where the source language is unsegmented and the target language segmentation is known. • it can be applied for the translation of a source language where no linguistically motivated word segmentation tools are available. • it applies machine learning techniques to identify segmentation schemes that improve translation quality for a gi"
W10-1760,N09-2019,0,0.0478899,"Missing"
W10-1760,J01-3002,0,0.0418668,"els trained on any of the learned word segmentations and performs comparably to available state-ofthe-art monolingually-built segmentation tools. 1 (2) unknown words, i.e., existing words can be combined into new words such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models. In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in the training corpus, thus these tokens can be translated by an SMT engine. However, the contextual information p"
W10-1760,P08-1115,0,0.067533,"learned using a parallel corpus by aligning character-wise source language sentences to word units separated by a whitespace in the target language. Successive characters aligned to the same target words are merged into a larger source language unit. Therefore, the granularity of the translation unit is defined in the given bitext context. In order to minimize the side effects of alignment errors and to achieve segmentation consistency, a Maximum-Entropy (ME) algorithm is applied to learn the source language word In order to integrate multiple word segmentation schemes into the SMT decoder, (Dyer et al., 2008) proposed to generate word lattices covering all possible segmentations of the input sentence 401 dicator where only two tags are used, i.e, “E” (end-of-word character tag) and “I” (in-word character tag). The annotations are derived from the SMT training corpus as described in Figure 1. segmentation that is consistent with the translation model of an SMT system trained on the resegmented bitext. The process is iterated until no further improvement in translation quality is achieved. In order to integrate multiple word segmentation into a single SMT system, the statistical translation models t"
W10-1760,N09-1046,0,0.0169293,"ons like Machine Translation (MT). In contrast to Indo-European languages like English, many Asian languages like Chinese do not use a whitespace character to separate meaningful word units. The problems of word segmentation are:   400 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400–408, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. The method of (Ma and Way, 2009) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation schemes from the training bitext. This dictionary-based approach uses heuristics based on the maximum matching algorithm to obtain an agglomeration of segments that are covered by the dictionary. It uses all possible source segmentations that are consistent with the extracted dict"
W10-1760,zhang-etal-2004-interpreting,0,0.0314116,"Missing"
W10-1760,P06-1085,0,0.207335,"rds such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models. In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in the training corpus, thus these tokens can be translated by an SMT engine. However, the contextual information provided by these tokens might not be enough to obtain a good translation. For example, a Japanese-English SMT engine might translate the two successive characters “ ” (“white”) and “ ” (“bird”) as “white bird”, while a"
W10-1760,W08-0335,1,0.835024,"of a source language where no linguistically motivated word segmentation tools are available. • it applies machine learning techniques to identify segmentation schemes that improve translation quality for a given language pair. • it decodes directly from unsegmented text using segmentation information implicit in the phrase-table to generate the target and thus avoids issues of consistency between phrasetable and input representation. Recent research on SMT is also focusing on the usage of multiple word segmentation schemes for the source language to improve translation quality. For example, (Zhang et al., 2008) combines dictionary-based and CRF-based approaches for Chinese word segmentation in order to avoid outof-vocabulary (OOV) words. Moreover, the combination of different morphological decomposition of highly inflected languages like Arabic or Finnish is proposed in (de Gispert et al., 2009) to reduce the data sparseness problem of SMT approaches. Similarly, (Nakov et al., 2009) utilizes SMT engines trained on different word segmentation schemes and combines the translation outputs using system combination techniques as a postprocess to SMT decoding. • it uses segmentations at all iterative leve"
W10-1760,W02-0704,0,0.0334855,"of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. • it works for any language pair where the source language is unsegmented and the target language segmentation is known. • it can be applied for the translation of a source language where no linguistically motivated word segmentation tools are available. • it applies machine learning techniques to identify segmentation schemes that improve translation quality for a given language pair. • it decodes directly from unsegmented text using"
W10-1760,W07-0734,0,0.0113547,"No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step iteraExperiments The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of sent"
W10-1760,E09-1063,0,0.162335,"segmentation are:   400 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400–408, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. The method of (Ma and Way, 2009) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation schemes from the training bitext. This dictionary-based approach uses heuristics based on the maximum matching algorithm to obtain an agglomeration of segments that are covered by the dictionary. It uses all possible source segmentations that are consistent with the extracted dictionary to create a word lattice for decoding. The method proposed in this papers differs from previous approaches in the following points: “consistent with SMT models” is one that identifies translation units that are"
W10-1760,J03-1002,0,0.0117672,"s of the iterative bootstrap method (dev), and the evaluation of translation quality (test). Besides the number of sentences (sen) and the vocabulary (voc), the sentence length (len) is also given as the average number of words per sentence. The given statistics are obtained using commonly-used linguistic segmentation tools available for the respective language, i.e., CHASEN (ja), WORDCUT (th), ICTCLAS (zh), HanTagger (ko). No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with repl"
W10-1760,C08-1128,0,\N,Missing
W10-1760,J96-1002,0,\N,Missing
W11-2601,2008.iwslt-papers.1,0,0.33914,"language resources available to train the statistical models. There are several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large-sized text data sets are readily available. However, for less frequently used language pairs, only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on multilingual SMT focuses on the use of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Koehn et al., 2009). Instead of a direct translation between two languages where only a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality o"
W11-2601,J93-2003,0,0.0213426,"rce channel model, also called the n-gram transliteration model, is a joint probability model that captures information on how the source and target sentences can be generated simultaneously using transliteration pairs, i.e., the most likely sequence of source characters and target words according to a joint language model built from the co-segmentation from the Bayesian model. 4 (5) where p(src|trg) is called a translation model (T M ) and represents the generation probability from trg into src, and p(trg) is called a language model (LM ) and represents the likelihood of the target language (Brown et al., 1993). During the translation process (decoding), a score based on the statistical model probabilities is assigned to each translation hypothesis and the one that gives the highest probability is selected as the best translation. The translation quality of SMT approaches heavily depends on the amount and coverage of the bilingual language resources available to train the statistical models. In the context of dialect translation, where only few bilingual language resources (if any at all) are available for the dialect and the foreign language, only a relatively low translation quality can be obtaine"
W11-2601,P08-2006,0,0.0306173,"and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often insufficient. For example, the task of word segmentat"
W11-2601,D08-1033,0,0.0535578,"Missing"
W11-2601,2010.iwslt-papers.7,1,0.645429,"rsion of a Gibbs sampler for training, which is similar to that of (Mochihashi et al., 2009). We extended their forward filtering / backward sampling (FFBS) dynamic programing algorithm in order to deal with bilingual segmentations (see Algorithm 1). We found our sampler converged rapidly without annealing. The number of iterations was set by hand after observing the convergence behavior of the algorithm in pilot experiments. We used a value of 75 iterations through the corpus in all experiments reported in this paper. For more details on the Bayesian co-segmentation process, please refer to (Finch and Sumita, 2010). For the experiments reported in this paper, we implemented the joint-source channel model approach as a weighted finite state transducer (FST) using the OpenFst toolkit (Allauzen et al., 2007). The FST takes the sequence of dialect characters as its input and outputs the co-segmented bilingual segments from which the standard language segments are extracted. 2.3 Pivot-based SMT Recent research on speech translation focuses on corpus-based approaches, and in particular on statistical machine translation (SMT), which is a machine translation paradigm where translations are generated on the bas"
W11-2601,W05-0703,0,0.031257,"lects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often insufficient. For example, the task of word segmentation, i.e., the identification of word boundaries in continuous text, is one of the fundamental preprocessing steps of MT applications. In contrast to Indo-European languages like English, many Asian languages like Japanese do not use a whitespace character to separate meaningful word units. However, the application of a linguistically motivated standard language word segmentation tool to a dialect corpus results in a poor segmen"
W11-2601,W06-1108,0,0.0145447,"guage) is a dialect that is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect reso"
W11-2601,W10-2405,0,0.0150812,"ained on a large amount of bilingual data is applied to obtain high-quality foreign language translations as described in Section 2.3. 2.1 Bayesian Co-segmentation The method for mapping the dialect sentences into the standard language word segments is a direct character-to-character mapping between the languages. This process is known as transliteration. Many transliteration methods have previously been proposed, including methods based on stringsimilarity measures between character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric with respect to source and target language. Intuitively, the model has two basic components: a model for"
W11-2601,W03-0317,0,0.0363333,"a state-of-the-art phrase-based SMT system trained on a large amount of bilingual data is applied to obtain high-quality foreign language translations as described in Section 2.3. 2.1 Bayesian Co-segmentation The method for mapping the dialect sentences into the standard language word segments is a direct character-to-character mapping between the languages. This process is known as transliteration. Many transliteration methods have previously been proposed, including methods based on stringsimilarity measures between character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric with respect to source and target language. Intuiti"
W11-2601,P04-1021,0,0.270354,"tween character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric with respect to source and target language. Intuitively, the model has two basic components: a model for generating an outcome that has already been generated at least once before, and a second model that assigns a probability to an outcome that has not yet been produced. Ideally, to encourage the re-use of model parameters, the probability of generating a novel bilingual sequence pair should be considerably lower then the probability of generating a previously observed sequence pair. The probability distribution over these bilingual sequence pairs (including a"
W11-2601,P09-1012,0,0.0156796,"teration units. Then, an n-gram transliteration model is defined as the transliteration probability of a transliteration pair < l, s >k depending on its immediate n preceding transliteration pairs: P (σ, ω, γ) = K Y P (< l, s >k |< l, s >k−1 k−n+1 ) (4) k=1 In this equation, N is the total number of bilingual sequence pairs generated so far and N ((sk , tk )) is the number of times the sequence pair (sk , tk ) has occurred in the history. G0 and α are the base measure and concentration parameter as before. We used a blocked version of a Gibbs sampler for training, which is similar to that of (Mochihashi et al., 2009). We extended their forward filtering / backward sampling (FFBS) dynamic programing algorithm in order to deal with bilingual segmentations (see Algorithm 1). We found our sampler converged rapidly without annealing. The number of iterations was set by hand after observing the convergence behavior of the algorithm in pilot experiments. We used a value of 75 iterations through the corpus in all experiments reported in this paper. For more details on the Bayesian co-segmentation process, please refer to (Finch and Sumita, 2010). For the experiments reported in this paper, we implemented the join"
W11-2601,W97-1102,0,0.0712968,"to a foreign language. A standard dialect (or standard language) is a dialect that is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguis"
W11-2601,W10-2408,0,0.0192295,"input and the translation models. In the second step, a state-of-the-art phrase-based SMT system trained on a large amount of bilingual data is applied to obtain high-quality foreign language translations as described in Section 2.3. 2.1 Bayesian Co-segmentation The method for mapping the dialect sentences into the standard language word segments is a direct character-to-character mapping between the languages. This process is known as transliteration. Many transliteration methods have previously been proposed, including methods based on stringsimilarity measures between character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric wit"
W11-2601,J03-1002,0,0.00720165,"Missing"
W11-2601,P02-1040,0,0.0804258,"Missing"
W11-2601,N09-2056,1,0.856333,"nly a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality of certain language pairs, especially when translating from or into Asian languages (Paul et al., 2009). This paper focuses on the translation of dialects, i.e., a variety of a language that is characteristic of a particular group of the language’s speakers, into a foreign language. A standard dialect (or standard language) is a dialect that is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–"
W11-2601,2010.amta-papers.5,0,0.0321811,"distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often insufficient. For example, the task of word segmentation, i.e., the identification of word boundaries in continuous text, is one of the fundamental preprocessing steps of MT applications. In contrast to Indo-European languages like English, many Asian languages like Japanese do not use a whitespace character to separate meaningful word units. However, the application of a linguistically motivated standard language word segmentation tool to a dialect corpus results in a poor segmentation quality"
W11-2601,P07-3010,0,0.0202508,"t is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often in"
W11-2601,N07-1061,0,0.180595,"ds on the amount and coverage of the bilingual language resources available to train the statistical models. There are several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large-sized text data sets are readily available. However, for less frequently used language pairs, only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on multilingual SMT focuses on the use of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Koehn et al., 2009). Instead of a direct translation between two languages where only a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot l"
W11-2601,P07-1108,0,0.202943,"ge of the bilingual language resources available to train the statistical models. There are several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large-sized text data sets are readily available. However, for less frequently used language pairs, only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on multilingual SMT focuses on the use of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Koehn et al., 2009). Instead of a direct translation between two languages where only a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improv"
W11-2601,C08-1128,0,0.022935,"hen the sequence itself is generated given the length. Note that this model is able to assign a probability to arbitrary bilingual sequence pairs of any length in the source and target sequence, but favors shorter sequences in both. The generative model is given in Equation 3. The equation assigns a probability to the k th bilingual sequence pair (sk , tk ) in a derivation of the corpus, given all of the other sequence pairs in the history so far (s−k , t−k ). Here −k is read as: “up to but not including k”. p((sk , tk ))|(s−k , t−k )) N ((sk , tk )) + αG0 ((sk , tk )) = N +α (3) 3 Following (Xu et al., 2008), we assign the parameters λs , λt and α, the values 2, 2 and 0.3 respectively. Input: Random initial corpus segmentation Output: Unsupervised co-segmentation of the corpus according to the model foreach iter=1 to NumIterations do foreach bilingual word-pair w ∈ randperm(W) do foreach co-segmentation γi of w do Compute probability p(γi |h) where h is the set of data (excluding w) and its hidden co-segmentation end Sample a co-segmentation γi from the distribution p(γi |h) Update counts end end Algorithm 1: Blocked Gibbs Sampling Suppose that we have a dialect sentence σ = l1 l2 . . . lL and a"
W11-2601,2009.mtsummit-papers.7,0,\N,Missing
W12-0601,N10-1020,0,0.0477323,"Twitter data will not necessarily fit other social data. Introduction The explosion of social media in recent years has led to the need for NLP tools like part-of-speech (POS) taggers that are robust enough to handle data that is becoming increasingly “noisy.” Unfortunately, many NLP systems fail at out-of-domain data and struggle with the informal style of social text. With spelling errors, abbreviations, uncommon acronyms, and excessive use of slang, systems that are designed for traditional corpora such as news articles may perform poorly when given difficult input such as a Twitter feed (Ritter et al., 2010). In general, concerns about the limitations of domain-dependent models have motivated the use of sophisticated unsupervised methods. Interest in unsupervised POS induction has been revived in recent years after Bayesian HMMs are shown to increase accuracy by up to 14 percentage points over basic maximum-likelihood estimation (Goldwater and Griffiths, 2007). Despite falling well short of the accuracy obtained with supervised taggers, unsupervised approaches are preferred in situations where there is no access to 1 Proceedings of the 13th Conference of the European Chapter of the Association fo"
W12-0601,P05-1044,0,0.0893472,"Missing"
W12-0601,N10-1070,0,0.0267529,"a document. We use the same notation as LDA, where θ is a document-topic distribution and φ is a topic-word distribution. Additionally, we denote the HMM transition rows as π, which we assume is drawn from a Dirichlet with hyperparameter γ. Denote In topic models, it is generally true that common function words may overwhelm the word distributions, leading to suboptimal results that are difficult to interpret. This is usually accommodated by data pre-processing (e.g. stop word removal), by backing off to “background” word models (Chemudugunta et al., 2006), or by performing term re-weighting (Wilson and Chew, 2010). In the case of POSLDA, these common words are naturally captured by the functional classes. 3.1 Relations to Other Models The idea of having multinomials for the cross products of topics and classes is related to multifaceted topic models where word tokens are associated with multiple latent variables (Paul and Girju, 2010; Ahmed and Xing, 2010). Under such models, words can be explained by a latent topic as well as a second underlying variable such as the perspective or dialect of the author, and words may depend on both factors. In our case, the second variable is the part-of-speech – or f"
W12-0601,P11-2008,0,0.137063,"Missing"
W12-0601,P07-1094,0,0.268518,"al style of social text. With spelling errors, abbreviations, uncommon acronyms, and excessive use of slang, systems that are designed for traditional corpora such as news articles may perform poorly when given difficult input such as a Twitter feed (Ritter et al., 2010). In general, concerns about the limitations of domain-dependent models have motivated the use of sophisticated unsupervised methods. Interest in unsupervised POS induction has been revived in recent years after Bayesian HMMs are shown to increase accuracy by up to 14 percentage points over basic maximum-likelihood estimation (Goldwater and Griffiths, 2007). Despite falling well short of the accuracy obtained with supervised taggers, unsupervised approaches are preferred in situations where there is no access to 1 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1–9, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics The next section discusses related work, which is followed by a description of our model, POSLDA. We then present POS tagging results on the Twitter POS dataset (Gimpel et al., 2011). Section 5 describes further experiments on the PO"
W12-0601,D10-1111,0,\N,Missing
W12-0601,J94-2001,0,\N,Missing
W16-6201,D11-1145,0,0.0252218,"ituational awareness. 2.2 Tweet Classification Identifying relevant information in social media is challenging due to the low signal-to-noise ratio. A number of researchers have used NLP to address this challenge. There is significant work in the medi1 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 1–6, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics cal domain related to identifying health crises and events in social media data. Multiple studies have been done to analyze flu-related tweets (Culotta, 2010; Aramaki et al., 2011). Most closely related to our work (but in a different domain) is the flu classification system of Lamb et al. (2013), which first classifies tweets for relevance and then applies finergrained classifiers. Similar systems have been developed to categorize tweets in more general domains, for example by identifying tweets related to news, events, and opinions (Sankaranarayanan et al., 2009; Sriram et al., 2010). Similar classifiers have been developed for sentiment analysis (Pang and Lee, 2008) to identify and categorize sentiment-expressing tweets (Go et al., 2009; Kouloumpis et al., 2011). 3 D"
W16-6201,N13-1097,1,0.670448,"e low signal-to-noise ratio. A number of researchers have used NLP to address this challenge. There is significant work in the medi1 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 1–6, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics cal domain related to identifying health crises and events in social media data. Multiple studies have been done to analyze flu-related tweets (Culotta, 2010; Aramaki et al., 2011). Most closely related to our work (but in a different domain) is the flu classification system of Lamb et al. (2013), which first classifies tweets for relevance and then applies finergrained classifiers. Similar systems have been developed to categorize tweets in more general domains, for example by identifying tweets related to news, events, and opinions (Sankaranarayanan et al., 2009; Sriram et al., 2010). Similar classifiers have been developed for sentiment analysis (Pang and Lee, 2008) to identify and categorize sentiment-expressing tweets (Go et al., 2009; Kouloumpis et al., 2011). 3 Data 3.1 Collection In late October 2012, Hurricane Sandy generated a massive, disperse reaction in social media chann"
W16-6201,D11-1141,0,0.0113252,"seline P R .80 .56 .44 .19 .57 .24 .04 .04 .44 .23 .76 .40 .64 .26 All Features F1 P R .71 .81 .64 .39 .46 .35 .48 .57 .41 .07 .10 .07 .36 .41 .32 .73 .71 .75 .53 .58 .49 Best Features F1 P R .72 .79 .66 .41 .42 .40 .49 .50 .49 .08 .10 .07 .36 .38 .35 .75 .71 .80 .52 .52 .52 Table 2: Results for relevance and fine-grained classification. uational awareness vs not. We used these four Verma classifiers to tag our Hurricane Sandy dataset and included these tags as features. • We included n-grams augmented with their partof-speech tags, as well as named entities, using the Twitter-based tagger of Ritter et al. (2011). 4.3 • Word embeddings have been used extensively in recent NLP work, with promising results (Goldberg, 2015). A Word2Vec model (Mikolov et al., 2013) was trained on the 22.2M tweets collected from the Hurricane Sandy dataset, using ˇ uˇrek and Sojka, 2010), the Gensim package (Reh˚ using the C-BOW algorithm with negative sampling (n=5), a window of 5, and with 200 dimensions per word. For each tweet, the mean embedding of all words was used to create 200 features. Classification performance was measured using fivefold cross-validation. We conducted an ablation study (Figure 1), removing indi"
W17-4406,K16-1017,0,0.108617,"ion of network and geographic features. 1 • We describe a preprocessing step that allows the inclusion of non-textual discrete features (e.g., followers, locations) into off-the-shelf text embedding methods. Our method is easy to use, requiring no special implementation. Introduction A variety of social media tasks benefit from having dense vector representations of users. For example, “who to follow” recommendations can be done by calculating cosine similarity between user vectors. Recent work has experimented with neural embeddings of social media users, most commonly based on text content (Amir et al., 2016; Wan et al., 2016), with some work combining input features from other metadata, including social network information (Li et al., 2015; Benton et al., 2016; Yang et al., 2016). Since social media like Twitter provide different types of data (e.g., text, network, location), constructing user embeddings with appropriate features can improve the performance based on the target task’s requirements. For instance, for recommending tweets a user may be interested in, the user’s text content will be a crucial feature. For recommending users to follow, information about • We introduce a novel type of"
W17-4406,P16-2003,0,0.300525,"locations) into off-the-shelf text embedding methods. Our method is easy to use, requiring no special implementation. Introduction A variety of social media tasks benefit from having dense vector representations of users. For example, “who to follow” recommendations can be done by calculating cosine similarity between user vectors. Recent work has experimented with neural embeddings of social media users, most commonly based on text content (Amir et al., 2016; Wan et al., 2016), with some work combining input features from other metadata, including social network information (Li et al., 2015; Benton et al., 2016; Yang et al., 2016). Since social media like Twitter provide different types of data (e.g., text, network, location), constructing user embeddings with appropriate features can improve the performance based on the target task’s requirements. For instance, for recommending tweets a user may be interested in, the user’s text content will be a crucial feature. For recommending users to follow, information about • We introduce a novel type of feature for user embeddings—the geographic locations of users’ friends—and show that this improves performance over standard text and network features on a"
W17-4406,P16-2073,0,0.123879,"geographic features. 1 • We describe a preprocessing step that allows the inclusion of non-textual discrete features (e.g., followers, locations) into off-the-shelf text embedding methods. Our method is easy to use, requiring no special implementation. Introduction A variety of social media tasks benefit from having dense vector representations of users. For example, “who to follow” recommendations can be done by calculating cosine similarity between user vectors. Recent work has experimented with neural embeddings of social media users, most commonly based on text content (Amir et al., 2016; Wan et al., 2016), with some work combining input features from other metadata, including social network information (Li et al., 2015; Benton et al., 2016; Yang et al., 2016). Since social media like Twitter provide different types of data (e.g., text, network, location), constructing user embeddings with appropriate features can improve the performance based on the target task’s requirements. For instance, for recommending tweets a user may be interested in, the user’s text content will be a crucial feature. For recommending users to follow, information about • We introduce a novel type of feature for user em"
W17-4406,D16-1152,0,0.156311,"he-shelf text embedding methods. Our method is easy to use, requiring no special implementation. Introduction A variety of social media tasks benefit from having dense vector representations of users. For example, “who to follow” recommendations can be done by calculating cosine similarity between user vectors. Recent work has experimented with neural embeddings of social media users, most commonly based on text content (Amir et al., 2016; Wan et al., 2016), with some work combining input features from other metadata, including social network information (Li et al., 2015; Benton et al., 2016; Yang et al., 2016). Since social media like Twitter provide different types of data (e.g., text, network, location), constructing user embeddings with appropriate features can improve the performance based on the target task’s requirements. For instance, for recommending tweets a user may be interested in, the user’s text content will be a crucial feature. For recommending users to follow, information about • We introduce a novel type of feature for user embeddings—the geographic locations of users’ friends—and show that this improves performance over standard text and network features on a new Twitter dataset."
W99-0207,P98-1011,0,0.106169,"Missing"
W99-0207,C88-1021,0,0.529577,"Missing"
W99-0207,J95-2003,0,0.352919,"Missing"
W99-0207,C96-1021,0,0.0509278,"Missing"
W99-0207,P98-2143,0,0.108075,"Missing"
W99-0207,C96-2137,0,0.155725,"Missing"
W99-0207,A88-1003,0,0.032175,"Missing"
W99-0207,P98-2204,0,0.0979801,"Missing"
W99-0207,P95-1017,0,\N,Missing
W99-0207,C98-1011,0,\N,Missing
W99-0207,C98-2138,0,\N,Missing
W99-0207,C98-2199,0,\N,Missing
