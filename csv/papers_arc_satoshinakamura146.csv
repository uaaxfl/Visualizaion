2021.sigdial-1.9,{ARTA}: Collection and Classification of Ambiguous Requests and Thoughtful Actions,2021,-1,-1,4,1,1438,shohei tanaka,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Human-assisting systems such as dialogue systems must take thoughtful, appropriate actions not only for clear and unambiguous user requests, but also for ambiguous user requests, even if the users themselves are not aware of their potential requirements. To construct such a dialogue agent, we collected a corpus and developed a model that classifies ambiguous user requests into corresponding system actions. In order to collect a high-quality corpus, we asked workers to input antecedent user requests whose pre-defined actions could be regarded as thoughtful. Although multiple actions could be identified as thoughtful for a single user request, annotating all combinations of user requests and system actions is impractical. For this reason, we fully annotated only the test data and left the annotation of the training data incomplete. In order to train the classification model on such training data, we applied the positive/unlabeled (PU) learning method, which assumes that only a part of the data is labeled with positive examples. The experimental results show that the PU learning method achieved better performance than the general positive/negative (PN) learning method to classify thoughtful actions given an ambiguous user request."
2021.iwslt-1.1,{FINDINGS} {OF} {THE} {IWSLT} 2021 {EVALUATION} {CAMPAIGN},2021,-1,-1,8,0,832,antonios anastasopoulos,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2021) featured this year four shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Multilingual speech translation, (iv) Low-resource speech translation. A total of 22 teams participated in at least one of the tasks. This paper describes each shared task, data and evaluation metrics, and reports results of the received submissions."
2021.iwslt-1.3,{NAIST} {E}nglish-to-{J}apanese Simultaneous Translation System for {IWSLT} 2021 Simultaneous Text-to-text Task,2021,-1,-1,10,1,5723,ryo fukuda,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,This paper describes NAIST{'}s system for the English-to-Japanese Simultaneous Text-to-text Translation Task in IWSLT 2021 Evaluation Campaign. Our primary submission is based on wait-k neural machine translation with sequence-level knowledge distillation to encourage literal translation.
2021.iwslt-1.24,On Knowledge Distillation for Translating Erroneous Speech Transcriptions,2021,-1,-1,3,1,5723,ryo fukuda,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"Recent studies argue that knowledge distillation is promising for speech translation (ST) using end-to-end models. In this work, we investigate the effect of knowledge distillation with a cascade ST using automatic speech recognition (ASR) and machine translation (MT) models. We distill knowledge from a teacher model based on human transcripts to a student model based on erroneous transcriptions. Our experimental results demonstrated that knowledge distillation is beneficial for a cascade ST. Further investigation that combined knowledge distillation and fine-tuning revealed that the combination consistently improved two language pairs: English-Italian and Spanish-English."
2021.iwslt-1.27,Large-Scale {E}nglish-{J}apanese Simultaneous Interpretation Corpus: Construction and Analyses with Sentence-Aligned Data,2021,-1,-1,3,0,5729,kosuke doi,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"This paper describes the construction of a new large-scale English-Japanese Simultaneous Interpretation (SI) corpus and presents the results of its analysis. A portion of the corpus contains SI data from three interpreters with different amounts of experience. Some of the SI data were manually aligned with the source speeches at the sentence level. Their latency, quality, and word order aspects were compared among the SI data themselves as well as against offline translations. The results showed that (1) interpreters with more experience controlled the latency and quality better, and (2) large latency hurt the SI quality."
2021.humeval-1.5,Is This Translation Error Critical?: Classification-Based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors,2021,-1,-1,3,0,1440,katsuhito sudoh,Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval),0,"This paper discusses a classification-based approach to machine translation evaluation, as opposed to a common regression-based approach in the WMT Metrics task. Recent machine translation usually works well but sometimes makes critical errors due to just a few wrong word choices. Our classification-based approach focuses on such errors using several error type labels, for practical machine translation evaluation in an age of neural machine translation. We made additional annotations on the WMT 2015-2017 Metrics datasets with fluency and adequacy labels to distinguish different types of translation errors from syntactic and semantic viewpoints. We present our human evaluation criteria for the corpus development and automatic evaluation experiments using the corpus. The human evaluation corpus will be publicly available upon publication."
2020.sltu-1.18,"Cross-Lingual Machine Speech Chain for {J}avanese, {S}undanese, {B}alinese, and {B}ataks Speech Recognition and Synthesis",2020,-1,-1,4,0,14701,sashi novitasari,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),0,"Even though over seven hundred ethnic languages are spoken in Indonesia, the available technology remains limited that could support communication within indigenous communities as well as with people outside the villages. As a result, indigenous communities still face isolation due to cultural barriers; languages continue to disappear. To accelerate communication, speech-to-speech translation (S2ST) technology is one approach that can overcome language barriers. However, S2ST systems require machine translation (MT), speech recognition (ASR), and synthesis (TTS) that rely heavily on supervised training and a broad set of language resources that can be difficult to collect from ethnic communities. Recently, a machine speech chain mechanism was proposed to enable ASR and TTS to assist each other in semi-supervised learning. The framework was initially implemented only for monolingual languages. In this study, we focus on developing speech recognition and synthesis for these Indonesian ethnic languages: Javanese, Sundanese, Balinese, and Bataks. We first separately train ASR and TTS of standard Indonesian in supervised training. We then develop ASR and TTS of ethnic languages by utilizing Indonesian ASR and TTS in a cross-lingual machine speech chain framework with only text or only speech data removing the need for paired speech-text data of those ethnic languages."
2020.lrec-1.62,Emotional Speech Corpus for Persuasive Dialogue System,2020,-1,-1,5,0,16730,sara asai,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Expressing emotion is known as an efficient way to persuade one{'}s dialogue partner to accept one{'}s claim or proposal. Emotional expression in speech can express the speaker{'}s emotion more directly than using only emotion expression in the text, which will lead to a more persuasive dialogue. In this paper, we built a speech dialogue corpus in a persuasive scenario that uses emotional expressions to build a persuasive dialogue system with emotional expressions. We extended an existing text dialogue corpus by adding variations of emotional responses to cover different combinations of broad dialogue context and a variety of emotional states by crowd-sourcing. Then, we recorded emotional speech consisting of of collected emotional expressions spoken by a voice actor. The experimental results indicate that the collected emotional expressions with their speeches have higher emotional expressiveness for expressing the system{'}s emotion to users."
2020.iwslt-1.21,{NAIST}{'}s Machine Translation Systems for {IWSLT} 2020 Conversational Speech Translation Task,2020,-1,-1,3,1,5723,ryo fukuda,Proceedings of the 17th International Conference on Spoken Language Translation,0,"This paper describes NAIST{'}s NMT system submitted to the IWSLT 2020 conversational speech translation task. We focus on the translation disfluent speech transcripts that include ASR errors and non-grammatical utterances. We tried a domain adaptation method by transferring the styles of out-of-domain data (United Nations Parallel Corpus) to be like in-domain data (Fisher transcripts). Our system results showed that the NMT model with domain adaptation outperformed a baseline. In addition, slight improvement by the style transfer was observed."
2020.coling-main.234,Improving Spoken Language Understanding by Wisdom of Crowds,2020,-1,-1,4,0.972004,1439,koichiro yoshino,Proceedings of the 28th International Conference on Computational Linguistics,0,"Spoken language understanding (SLU), which converts user requests in natural language to machine-interpretable expressions, is becoming an essential task. The lack of training data is an important problem, especially for new system tasks, because existing SLU systems are based on statistical approaches. In this paper, we proposed to use two sources of the {``}wisdom of crowds,{''} crowdsourcing and knowledge community website, for improving the SLU system. We firstly collected paraphrasing variations for new system tasks through crowdsourcing as seed data, and then augmented them using similar questions from a knowledge community website. We investigated the effects of the proposed data augmentation method in SLU task, even with small seed data. In particular, the proposed architecture augmented more than 120,000 samples to improve SLU accuracies."
2020.coling-main.319,Incorporating Noisy Length Constraints into Transformer with Length-aware Positional Encodings,2020,-1,-1,4,0,5724,yui oka,Proceedings of the 28th International Conference on Computational Linguistics,0,"Neural Machine Translation often suffers from an under-translation problem due to its limited modeling of output sequence lengths. In this work, we propose a novel approach to training a Transformer model using length constraints based on length-aware positional encoding (PE). Since length constraints with exact target sentence lengths degrade translation performance, we add random noise within a certain window size to the length constraints in the PE during the training. In the inference step, we predict the output lengths using input sequences and a BERT-based length prediction model. Experimental results in an ASPEC English-to-Japanese translation showed the proposed method produced translations with lengths close to the reference ones and outperformed a vanilla Transformer (especially in short sentences) by 3.22 points in BLEU. The average translation results using our length prediction model were also better than another baseline method using input lengths for the length constraints. The proposed noise injection improved robustness for length prediction errors, especially within the window size."
2020.acl-srw.8,Reflection-based Word Attribute Transfer,2020,-1,-1,4,0,22487,yoichi ishibashi,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Word embeddings, which often represent such analogic relations as king - man + woman queen, can be used to change a word{'}s attribute, including its gender. For transferring king into queen in this analogy-based manner, we subtract a difference vector man - woman based on the knowledge that king is male. However, developing such knowledge is very costly for words and attributes. In this work, we propose a novel method for word attribute transfer based on reflection mappings without such an analogy operation. Experimental results show that our proposed method can transfer the word attributes of the given words without changing the words that do not have the target attributes."
2020.acl-main.327,Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model,2020,-1,-1,3,0,6008,kosuke takahashi,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method using Cross-lingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance."
W19-8627,Neural Conversation Model Controllable by Given Dialogue Act Based on Adversarial Learning and Label-aware Objective,2019,0,0,3,0,23329,seiya kawano,Proceedings of the 12th International Conference on Natural Language Generation,0,"Building a controllable neural conversation model (NCM) is an important task. In this paper, we focus on controlling the responses of NCMs by using dialogue act labels of responses as conditions. We introduce an adversarial learning framework for the task of generating conditional responses with a new objective to a discriminator, which explicitly distinguishes sentences by using labels. This change strongly encourages the generation of label-conditioned sentences. We compared the proposed method with some existing methods for generating conditional responses. The experimental results show that our proposed method has higher controllability for dialogue acts even though it has higher or comparable naturalness to existing methods."
W19-4106,Conversational Response Re-ranking Based on Event Causality and Role Factored Tensor Event Embedding,2019,29,0,4,1,1438,shohei tanaka,Proceedings of the First Workshop on NLP for Conversational AI,0,"We propose a novel method for selecting coherent and diverse responses for a given dialogue context. The proposed method re-ranks response candidates generated from conversational models by using event causality relations between events in a dialogue history and response candidates (e.g., {``}be stressed out{''} precedes {``}relieve stress{''}). We use distributed event representation based on the Role Factored Tensor Model for a robust matching of event causality relations due to limited event causality knowledge of the system. Experimental results showed that the proposed method improved coherency and dialogue continuity of system responses."
W18-5017,Unsupervised Counselor Dialogue Clustering for Positive Emotion Elicitation in Neural Dialogue System,2018,0,0,4,1,1578,nurul lubis,Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"Positive emotion elicitation seeks to improve user{'}s emotional state through dialogue system interaction, where a chat-based scenario is layered with an implicit goal to address user{'}s emotional needs. Standard neural dialogue system approaches still fall short in this situation as they tend to generate only short, generic responses. Learning from expert actions is critical, as these potentially differ from standard dialogue acts. In this paper, we propose using a hierarchical neural network for response generation that is conditioned on 1) expert{'}s action, 2) dialogue context, and 3) user emotion, encoded from user input. We construct a corpus of interactions between a counselor and 30 participants following a negative emotional exposure to learn expert actions and responses in a positive emotion elicitation scenario. Instead of relying on the expensive, labor intensive, and often ambiguous human annotations, we unsupervisedly cluster the expert{'}s responses and use the resulting labels to train the network. Our experiments and evaluation show that the proposed approach yields lower perplexity and generates a larger variety of responses."
W18-2711,Multi-Source Neural Machine Translation with Missing Data,2018,11,2,4,0,28399,yuta nishimura,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"Multi-source translation is an approach to exploit multiple inputs (e.g. in two different languages) to increase translation accuracy. In this paper, we examine approaches for multi-source neural machine translation (NMT) using an incomplete multilingual corpus in which some translations are missing. In practice, many multilingual corpora are not complete due to the difficulty to provide translations in all of the relevant languages (for example, in TED talks, most English talks only have subtitles for a small portion of the languages that TED supports). Existing studies on multi-source translation did not explicitly handle such situations. This study focuses on the use of incomplete multilingual corpora in multi-encoder NMT and mixture of NMT experts and examines a very simple implementation where missing source translations are replaced by a special symbol {\textless}NULL{\textgreater}. These methods allow us to use incomplete corpora both at training time and test time. In experiments with real incomplete multilingual corpora of TED Talks, the multi-source NMT with the {\textless}NULL{\textgreater} tokens achieved higher translation accuracies measured by BLEU than those by any one-to-one NMT systems."
N18-1120,Guiding Neural Machine Translation with Retrieved Translation Pieces,2018,15,4,5,1,12715,jingyi zhang,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases. In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process. Specifically, for an input sentence, we use a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect n-grams that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call {``}translation pieces{''}. We compute pseudo-probabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces. Finally, an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to accuracy, speed, and simplicity of implementation."
L18-1194,Dialogue Scenario Collection of Persuasive Dialogue with Emotional Expressions via Crowdsourcing,2018,0,2,6,1,1439,koichiro yoshino,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1462,{J}apanese Dialogue Corpus of Information Navigation and Attentive Listening Annotated with Extended {ISO}-24617-2 Dialogue Act Tags,2018,0,0,5,1,1439,koichiro yoshino,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1468,Construction of {E}nglish-{F}rench Multimodal Affective Conversational Corpus from {TV} Dramas,2018,0,1,5,0,14701,sashi novitasari,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-5712,A Simple and Strong Baseline: {NAIST}-{NICT} Neural Machine Translation System for {WAT}2017 {E}nglish-{J}apanese Translation Task,2017,0,0,3,1,296,yusuke oda,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"This paper describes the details about the NAIST-NICT machine translation system for WAT2017 English-Japanese Scientific Paper Translation Task. The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture."
W17-5542,Information Navigation System with Discovering User Interests,2017,5,1,3,1,1439,koichiro yoshino,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"We demonstrate an information navigation system for sightseeing domains that has a dialogue interface for discovering user interests for tourist activities. The system discovers interests of a user with focus detection on user utterances, and proactively presents related information to the discovered user interest. A partially observable Markov decision process (POMDP)-based dialogue manager, which is extended with user focus states, controls the behavior of the system to provide information with several dialogue acts for providing information. We transferred the belief-update function and the policy of the manager from other system trained on a different domain to show the generality of defined dialogue acts for our information navigation system."
W17-4709,Tree as a Pivot: Syntactic Matching Methods in Pivot Translation,2017,0,2,4,1,31611,akiva miura,Proceedings of the Second Conference on Machine Translation,0,None
W17-4753,{NICT}-{NAIST} System for {WMT}17 Multimodal Translation Task,2017,7,3,5,1,12715,jingyi zhang,Proceedings of the Second Conference on Machine Translation,0,None
W17-3208,An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation,2017,6,3,6,1,303,makoto morishita,Proceedings of the First Workshop on Neural Machine Translation,0,"Training of neural machine translation (NMT) models usually uses mini-batches for efficiency purposes. During the mini-batched training process, it is necessary to pad shorter sentences in a mini-batch to be equal in length to the longest sentence therein for efficient computation. Previous work has noted that sorting the corpus based on the sentence length before making mini-batches reduces the amount of padding and increases the processing speed. However, despite the fact that mini-batch creation is an essential step in NMT training, widely used NMT toolkits implement disparate strategies for doing so, which have not been empirically validated or compared. This work investigates mini-batch creation strategies with experiments over two different datasets. Our results suggest that the choice of a mini-batch creation strategy has a large effect on NMT training and some length-based sorting strategies do not always work well compared with simple shuffling."
P17-1079,Neural Machine Translation via Binary Code Prediction,2017,9,4,5,1,296,yusuke oda,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10."
I17-1016,Improving Neural Machine Translation through Phrase-based Forced Decoding,2017,26,1,5,1,12715,jingyi zhang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Compared to traditional statistical machine translation (SMT), neural machine translation (NMT) often sacrifices adequacy for the sake of fluency. We propose a method to combine the advantages of traditional SMT and NMT by exploiting an existing phrase-based SMT model to compute the phrase-based decoding cost for an NMT output and then using the phrase-based decoding cost to rerank the n-best NMT outputs. The main challenge in implementing this approach is that NMT outputs may not be in the search space of the standard phrase-based decoding algorithm, because the search space of phrase-based SMT is limited by the phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs."
I17-1044,Local Monotonic Attention Mechanism for End-to-End Speech And Language Processing,2017,20,3,3,0,14702,andros tjandra,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Recently, encoder-decoder neural networks have shown impressive performance on many sequence-related tasks. The architecture commonly uses an attentional mechanism which allows the model to learn alignments between the source and the target sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces misalignment on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in several tasks, such as automatic speech recognition (ASR), grapheme-to-phoneme (G2P), etc. In this paper, we propose a novel attention mechanism that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results on ASR, G2P and machine translation between two languages with similar sentence structures, demonstrate that the proposed encoder-decoder model with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture."
I17-1092,Acquisition and Assessment of Semantic Content for the Generation of Elaborateness and Indirectness in Spoken Dialogue Systems,2017,19,1,4,0.952381,18290,louisa pragst,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In a dialogue system, the dialogue manager selects one of several system actions and thereby determines the system{'}s behaviour. Defining all possible system actions in a dialogue system by hand is a tedious work. While efforts have been made to automatically generate such system actions, those approaches are mostly focused on providing functional system behaviour. Adapting the system behaviour to the user becomes a difficult task due to the limited amount of system actions available. We aim to increase the adaptability of a dialogue system by automatically generating variants of system actions. In this work, we introduce an approach to automatically generate action variants for elaborateness and indirectness. Our proposed algorithm extracts RDF triplets from a knowledge base and rates their relevance to the original system action to find suitable content. We show that the results of our algorithm are mostly perceived similarly to human generated elaborateness and indirectness and can be used to adapt a conversation to the current user and situation. We also discuss where the results of our algorithm are still lacking and how this could be improved: Taking into account the conversation topic as well as the culture of the user is likely to have beneficial effect on the user{'}s perception."
W16-3610,Cultural Communication Idiosyncrasies in Human-Computer Interaction,2016,7,8,5,0,16743,juliana miehle,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Comunicacio presentada a: 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue; celebrada del 13 al 15 de setembre de 2016 a Los Angeles, USA"
W16-3640,Analyzing the Effect of Entrainment on Dialogue Acts,2016,18,2,5,1,1445,masahiro mizukami,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Entrainment is a factor in dialogue that affects not only human-human but also human-machine interaction. While entrainment on the lexical level is well documented, less is known about how entrainment affects dialogue on a more abstract, structural level. In this paper, we investigate the effect of entrainment on dialogue acts and on lexical choice given dialogue acts, as well as how entrainment changes during a dialogue. We also define a novel measure of entrainment to measure these various types of entrainment. These results may serve as guidelines for dialogue systems that would like to entrain with users in a similar manner."
P16-1130,A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation,2016,23,0,5,1,12715,jingyi zhang,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1003,"Selecting Syntactic, Non-redundant Segments in Active Learning for Machine Translation",2016,16,9,4,1,31611,akiva miura,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
L16-1314,Optimizing Computer-Assisted Transcription Quality with Iterative User Interfaces,2016,23,2,3,1,10828,matthias sperber,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Computer-assisted transcription promises high-quality speech transcription at reduced costs. This is achieved by limiting human effort to transcribing parts for which automatic transcription quality is insufficient. Our goal is to improve the human transcription quality via appropriate user interface design. We focus on iterative interfaces that allow humans to solve tasks based on an initially given suggestion, in this case an automatic transcription. We conduct a user study that reveals considerable quality gains for three variations of iterative interfaces over a non-iterative from-scratch transcription interface. Our iterative interfaces included post-editing, confidence-enhanced post-editing, and a novel retyping interface. All three yielded similar quality on average, but we found that the proposed retyping interface was less sensitive to the difficulty of the segment, and superior when the automatic transcription of the segment contained relatively many errors. An analysis using mixed-effects models allows us to quantify these and other factors and draw conclusions over which interface design should be chosen in which circumstance."
L16-1346,Construction of {J}apanese Audio-Visual Emotion Database and Its Application in Emotion Recognition,2016,4,1,6,1,1578,nurul lubis,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Emotional aspects play a vital role in making human communication a rich and dynamic experience. As we introduce more automated system in our daily lives, it becomes increasingly important to incorporate emotion to provide as natural an interaction as possible. To achieve said incorporation, rich sets of labeled emotional data is prerequisite. However, in Japanese, existing emotion database is still limited to unimodal and bimodal corpora. Since emotion is not only expressed through speech, but also visually at the same time, it is essential to include multiple modalities in an observation. In this paper, we present the first audio-visual emotion corpora in Japanese, collected from 14 native speakers. The corpus contains 100 minutes of annotated and transcribed material. We performed preliminary emotion recognition experiments on the corpus and achieved an accuracy of 61.42{\%} for five classes of emotion."
D16-1162,Incorporating Discrete Translation Lexicons into Neural Machine Translation,2016,33,54,3,1,8469,philip arthur,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time."
D16-1263,Learning a Lexicon and Translation Model from Phoneme Lattices,2016,23,6,6,0,11431,oliver adams,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-5003,Neural Reranking Improves Subjective Quality of Machine Translation: {NAIST} at {WAT}2015,2015,22,25,3,0.462684,834,graham neubig,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,"This year, the Nara Institute of Science and Technology (NAIST)'s submission to the 2015 Workshop on Asian Translation was based on syntax-based statistical machine translation, with the addition of a reranking component using neural attentional machine translation models. Experiments re-confirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words."
W15-4605,Reinforcement Learning in Multi-Party Trading Dialog,2015,26,4,5,1,27167,takuya hiraoka,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"In this paper, we apply reinforcement learning (RL) to a multi-party trading scenario where the dialog system (learner) trades with one, two, or three other agents. We experiment with different RL algorithms and reward functions. The negotiation strategy of the learner is learned through simulated dialog with trader simulators. In our experiments, we evaluate how the performance of the learner varies depending on the RL algorithm used and the number of traders. Our results show that (1) even in simple multi-party trading dialog tasks, learning an effective negotiation policy is a very hard problem; and (2) the use of neural fitted Q iteration combined with an incremental reward function produces negotiation policies as effective or even better than the policies of two strong hand-crafted baselines."
W15-3057,An Investigation of Machine Translation Evaluation Metrics in Cross-lingual Question Answering,2015,21,3,7,0,30028,kyoshiro sugiyama,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accuracy. However, it is not clear whether an MT system that is better for human consumption is also better for CLQA. In this paper, we investigate the relationship between manual and automatic translation evaluation metrics and CLQA accuracy by creating a data set using both manual and machine translations and perform CLQA using this created data set. 1 As a result, we find that QA accuracy is closely related with a metric that considers frequency of words, and as a result of manual analysis, we identify 3 factors of translation results that affect CLQA accuracy."
Q15-1041,Semantic Parsing of Ambiguous Input through Paraphrasing and Verification,2015,39,1,5,1,8469,philip arthur,Transactions of the Association for Computational Linguistics,0,"We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the original input. This paraphrase can be used to disambiguate the meaning representation via verification using a language model that calculates the probability of each paraphrase."
P15-2094,Improving Pivot Translation by Remembering the Pivot,2015,13,3,5,1,31611,akiva miura,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Pivot translation allows for translation of language pairs with little or no parallel data by introducing a third language for which data exists. In particular, the triangulation method, which translates by combining source-pivot and pivot-target translation models into a source-target model, is known for its high translation accuracy. However, in the conventional triangulation method, information of pivot phrases is forgotten and not used in the translation process. In this paper, we propose a novel approach torememberthe pivot phrases in the triangulation stage, and use a pivot language model as an additional information source at translation time. Experimental results on the Europarl corpus showed gains of 0.4-1.2 BLEU points in all tested combinations of languages 1 ."
P15-1020,Syntax-based Simultaneous Translation through Prediction of Unseen Syntactic Constituents,2015,25,12,5,1,296,yusuke oda,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Simultaneous translation is a method to reduce the latency of communication through machine translation (MT) by dividing the input into short segments before performing translation. However, short segments pose problems for syntaxbased translation methods, as it is difficult to generate accurate parse trees for sub-sentential segments. In this paper, we perform the first experiments applying syntax-based SMT to simultaneous translation, and propose two methods to prevent degradations in accuracy: a method to predict unseen syntactic constituents that help generate complete parse trees, and a method that waits for more input when the current utterance is not enough to generate a fluent translation. Experiments on English-Japanese translation show that the proposed methods allow for improvements in accuracy, particularly with regards to word order of the target sentences."
N15-3009,{C}kylark: A More Robust {PCFG}-{LA} Parser,2015,7,11,5,1,296,yusuke oda,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"This paper describes Ckylark, a PCFG-LA style phrase structure parser that is more robust than other parsers in the genre. PCFG-LA parsers are known to achieve highly competitive performance, but sometimes the parsing process fails completely, and no parses can be generated. Ckylark introduces three new techniques that prevent possible causes for parsing failure: outputting intermediate results when coarse-to-fine analysis fails, smoothing lexicon probabilities, and scaling probabilities to avoid underflow. An experiment shows that this allows millions of sentences can be parsed without any failures, in contrast to other publicly available PCFG-LA parsers. Ckylark is implemented in C, and is available opensource under the LGPL license.1"
D15-1250,A Binarized Neural Network Joint Model for Machine Translation,2015,17,2,5,1,12715,jingyi zhang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks."
2015.iwslt-papers.12,Improving translation of emphasis with pause prediction in speech-to-speech translation systems,2015,-1,-1,5,1,14186,quoc do,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,None
2015.iwslt-papers.16,Parser self-training for syntax-based machine translation,2015,-1,-1,6,1,303,makoto morishita,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,None
2015.iwslt-evaluation.17,The {NAIST} {E}nglish speech recognition system for {IWSLT} 2015,2015,31,0,5,1,1582,michael heck,Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The International Workshop for Spoken Language Translation (IWSLT) is an annual evaluation campaign for core speech processing technologies. This paper presents Nara Institute of Science and Technologyxe2x80x99s (NAISTxe2x80x99s) contribution to the English automatic speech recognition (ASR) track for the 2015 evaluation campaign. The ASR systems presented in this paper make use of various frontends, varying deep neural net (DNN) acoustic models and separate language models for decoding and rescoring. Recognition is performed in three stages: Decoding, lattice rescoring and system combination via recognizer output voting error reduction (ROVER). We discuss the application of a rank-score based weighting approach for the system combination. Also, a Gaussian mixture model hidden Markov model (GMM-HMM) based speech/non-speech segmenter makes use of said combination scheme. The primary submission achieves a word error rate (WER) of 9.5% and 10.1% on the official development set, given manual and automatic segmentation respectively."
W14-4004,Rule-based Syntactic Preprocessing for Syntax-based Machine Translation,2014,28,1,5,0,38010,yuto hatakoshi,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Several preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output. On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper, we examine whether the sort of rule-based syntactic preprocessing approaches that have proved beneficial for PBMT can contribute to syntax-based SMT. Specifically, we tailor a highly successful preprocessing method for EnglishJapanese PBMT to syntax-based SMT, andfind that while the gains achievable are smaller than those for PBMT, significant improvements in accuracy can be realized."
W14-3211,Linguistic and Acoustic Features for Automatic Identification of Autism Spectrum Disorders in Children{'}s Narrative,2014,-1,-1,5,0,30027,hiroki tanaka,Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,0,None
Q14-1014,Segmentation for Efficient Supervised Language Annotation with an Explicit Cost-Utility Tradeoff,2014,30,4,4,1,10828,matthias sperber,Transactions of the Association for Computational Linguistics,0,"In this paper, we study the problem of manually correcting automatic annotations of natural language in as efficient a manner as possible. We introduce a method for automatically segmenting a corpus into chunks such that many uncertain labels are grouped into the same chunk, while human supervision can be omitted altogether for other segments. A tradeoff must be found for segment sizes. Choosing short segments allows us to reduce the number of highly confident labels that are supervised by the annotator, which is useful because these labels are often already correct and supervising correct labels is a waste of effort. In contrast, long segments reduce the cognitive effort due to context switches. Our method helps find the segmentation that optimizes supervision efficiency by defining user models to predict the cost and utility of supervising each segment and solving a constrained optimization problem balancing these contradictory objectives. A user study demonstrates noticeable gains over pre-segmented, confidence-ordered baselines on two natural language processing tasks: speech transcription and word segmentation."
P14-2090,Optimizing Segmentation Strategies for Simultaneous Speech Translation,2014,15,29,5,1,296,yusuke oda,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we propose new algorithms for learning segmentation strategies for simultaneous speech translation. In contrast to previously proposed heuristic methods, our method finds a segmentation that directly maximizes the performance of the machine translation system. We describe two methods based on greedy search and dynamic programming that search for the optimal segmentation strategy. An experimental evaluation finds that our algorithm is able to segment the input two to three times more frequently than conventional methods in terms of number of words, while maintaining the same score of automatic evaluation. 1"
shimizu-etal-2014-collection,Collection of a Simultaneous Translation Corpus for Comparative Analysis,2014,10,9,5,1,39435,hiroaki shimizu,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper describes the collection of an English-Japanese/Japanese-English simultaneous interpretation corpus. There are two main features of the corpus. The first is that professional simultaneous interpreters with different amounts of experience cooperated with the collection. By comparing data from simultaneous interpretation of each interpreter, it is possible to compare better interpretations to those that are not as good. The second is that for part of our corpus there are already translation data available. This makes it possible to compare translation data with simultaneous interpretation data. We recorded the interpretations of lectures and news, and created time-aligned transcriptions. A total of 387k words of transcribed data were collected. The corpus will be helpful to analyze differences in interpretations styles and to construct simultaneous interpretation systems."
sakti-etal-2014-towards,Towards Multilingual Conversations in the Medical Domain: Development of Multilingual Medical Data and A Network-based {ASR} System,2014,14,3,6,1,5730,sakriani sakti,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper outlines the recent development on multilingual medical data and multilingual speech recognition system for network-based speech-to-speech translation in the medical domain. The overall speech-to-speech translation (S2ST) system was designed to translate spoken utterances from a given source language into a target language in order to facilitate multilingual conversations and reduce the problems caused by language barriers in medical situations. Our final system utilizes a weighted finite-state transducers with n-gram language models. Currently, the system successfully covers three languages: Japanese, English, and Chinese. The difficulties involved in connecting Japanese, English and Chinese speech recognition systems through Web servers will be discussed, and the experimental results in simulated medical conversation will also be presented."
E14-4025,Acquiring a Dictionary of Emotion-Provoking Events,2014,14,6,5,0,30853,hoa vu,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"This paper is concerned with the discovery and aggregation of events that provoke a particular emotion in the person who experiences them, or emotion-provoking events. We first describe the creation of a small manually-constructed dictionary of events through a survey of 30 subjects. Next, we describe first attempts at automatically acquiring and aggregating these events from web data, with a baseline from previous work and some simple extensions using seed expansion and clustering. Finally, we propose several evaluation measures for evaluating the automatically acquired events, and perform an evaluation of the effectiveness of automatic event extraction."
C14-1106,Discriminative Language Models as a Tool for Machine Translation Error Analysis,2014,12,3,5,0,38009,koichi akabe,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we propose a new method for effective error analysis of machine translation (MT) systems. In previous work on error analysis of MT, error trends are often shown by frequency. However, if we attempt to perform a more detailed analysis based on frequently erroneous word strings, the word strings also often occur in correct translations, and analyzing these correct sentences decreases the overall efficiency of error analysis. In this paper, we propose the use of regularized discriminative language models (LMs) to allow for more focused MT error analysis. In experiments, we demonstrate that our method is more efficient than frequency-based analysis, and examine differences across systems, language pairs, and evaluation measures. 1"
C14-1161,Reinforcement Learning of Cooperative Persuasive Dialogue Policies using Framing,2014,20,13,5,1,27167,takuya hiraoka,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we apply reinforcement learning for automatically learning cooperative persuasive dialogue system policies using framing, the use of emotionally charged statements common in persuasive dialogue between humans. In order to apply reinforcement learning, we describe a method to construct user simulators and reward functions specifically tailored to persuasive dialogue based on a corpus of persuasive dialogues between human interlocutors. Then, we evaluate the learned policy and the effect of framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that applying reinforcement learning is effective for construction of cooperative persuasive dialogue systems which use framing."
W13-4604,Towards High-Reliability Speech Translation in the Medical Domain,2013,28,4,4,0.602837,834,graham neubig,The First Workshop on Natural Language Processing for Medical and Healthcare Fields,0,"In this paper, we describe the overall design for a speech translation system that aims to reduce the problems caused by language barriers in medical situations. As first steps to building a system according to this design, we describe a collection of a medical corpus, and some translation experiments performed on this corpus. As a result of the experiments, we find that the best of three modern translation systems is able to translate 33%-81% of the sentences in a way such that the main content is understandable."
2013.iwslt-papers.3,Constructing a speech translation system using simultaneous interpretation data,2013,-1,-1,5,1,39435,hiroaki shimizu,Proceedings of the 10th International Workshop on Spoken Language Translation: Papers,0,"There has been a fair amount of work on automatic speech translation systems that translate in real-time, serving as a computerized version of a simultaneous interpreter. It has been noticed in the field of translation studies that simultaneous interpreters perform a number of tricks to make the content easier to understand in real-time, including dividing their translations into small chunks, or summarizing less important content. However, the majority of previous work has not specifically considered this fact, simply using translation data (made by translators) for learning of the machine translation system. In this paper, we examine the possibilities of additionally incorporating simultaneous interpretation data (made by simultaneous interpreters) in the learning process. First we collect simultaneous interpretation data from professional simultaneous interpreters of three levels, and perform an analysis of the data. Next, we incorporate the simultaneous interpretation data in the learning of the machine translation system. As a result, the translation style of the system becomes more similar to that of a highly experienced simultaneous interpreter. We also find that according to automatic evaluation metrics, our system achieves performance similar to that of a simultaneous interpreter that has 1 year of experience."
2013.iwslt-papers.8,Incremental unsupervised training for university lecture recognition,2013,-1,-1,5,1,1582,michael heck,Proceedings of the 10th International Workshop on Spoken Language Translation: Papers,0,"In this paper we describe our work on unsupervised adaptation of the acoustic model of our simultaneous lecture translation system. We trained a speaker independent acoustic model, with which we produce automatic transcriptions of new lectures in order to improve the system for a specific lecturer. We compare our results against a model that was trained in a supervised way on an exact manual transcription. We examine four different ways of processing the decoder outputs of the automatic transcription with respect to the treatment of pronunciation variants and noise words. We will show that, instead of fixating the latter informations in the transcriptions, it is of advantage to let the Viterbi algorithm during training decide which pronunciations to use and where to insert which noise words. Further, we utilize word level posterior probabilities obtained during decoding by weighting and thresholding the words of a transcription."
2013.iwslt-evaluation.23,The {NAIST} {E}nglish speech recognition system for {IWSLT} 2013,2013,-1,-1,5,1,5730,sakriani sakti,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the NAIST English speech recognition system for the IWSLT 2013 Evaluation Campaign. In particular, we participated in the ASR track of the IWSLT TED task. Last year, we participated in collaboration with Karlsruhe Institute of Technology (KIT). This year is our first time to build a full-fledged ASR system for IWSLT solely developed by NAIST. Our final system utilizes weighted finitestate transducers with four-gram language models. The hypothesis selection is based on the principle of system combination. On the IWSLT official test set our system introduced in this work achieves a WER of 9.1{\%} for tst2011, 10.0{\%} for tst2012, and 16.2{\%} for the new tst2013."
2012.iwslt-papers.2,A method for translation of paralinguistic information,2012,9,13,6,0,43820,takatomo kano,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"This paper is concerned with speech-to-speech translation that is sensitive to paralinguistic information. From the many different possible paralinguistic features to handle, in this paper we chose duration and power as a first step, proposing a method that can translate these features from input speech to the output speech in continuous space. This is done in a simple and language-independent fashion by training a regression model that maps source language duration and power information into the target language. We evaluate the proposed method on a digit translation task and show that paralinguistic information in input speech appears in output speech, and that this information can be used by target language speakers to detect emphasis."
2012.iwslt-evaluation.5,The {NAIST} machine translation system for {IWSLT}2012,2012,22,2,8,0.602837,834,graham neubig,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the NAIST statistical machine translation system for the IWSLT2012 Evaluation Campaign. We participated in all TED Talk tasks, for a total of 11 language-pairs. For all tasks, we use the Moses phrase-based decoder and its experiment management system as a common base for building translation systems. The focus of our work is on performing a comprehensive comparison of a multitude of existing techniques for the TED task, exploring issues such as out-of-domain data filtering, minimum Bayes risk decoding, MERT vs. PRO tuning, word alignment combination, and morphology."
2012.iwslt-evaluation.10,The 2012 {KIT} and {KIT}-{NAIST} {E}nglish {ASR} systems for the {IWSLT} evaluation,2012,-1,-1,11,0,28037,christian saam,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes our English Speech-to-Text (STT) systems for the 2012 IWSLT TED ASR track evaluation. The systems consist of 10 subsystems that are combinations of different front-ends, e.g. MVDR based and MFCC based ones, and two different phone sets. The outputs of the subsystems are combined via confusion network combination. Decoding is done in two stages, where the systems of the second stage are adapted in an unsupervised manner on the combination of the first stage outputs using VTLN, MLLR, and cM-LLR."
2012.iwslt-evaluation.11,The {KIT}-{NAIST} (contrastive) {E}nglish {ASR} system for {IWSLT} 2012,2012,-1,-1,11,1,1582,michael heck,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the KIT-NAIST (Contrastive) English speech recognition system for the IWSLT 2012 Evaluation Campaign. In particular, we participated in the ASR track of the IWSLT TED task. The system was developed by Karlsruhe Institute of Technology (KIT) and Nara Institute of Science and Technology (NAIST) teams in collaboration within the interACT project. We employ single system decoding with fully continuous and semi-continuous models, as well as a three-stage, multipass system combination framework built with the Janus Recognition Toolkit. On the IWSLT 2010 test set our single system introduced in this work achieves a WER of 17.6{\%}, and our final combination achieves a WER of 14.4{\%}."
2012.iwslt-evaluation.15,Minimum {B}ayes-risk decoding extended with similar examples: {NAIST}-{NCT} at {IWSLT} 2012,2012,10,0,4,1,39435,hiroaki shimizu,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes our methods used in the NAIST-NICT submission to the International Workshop on Spoken Language Translation (IWSLT) 2012 evaluation campaign. In particular, we propose two extensions to minimum bayes-risk decoding which reduces a expected loss."
W11-2028,Toward Construction of Spoken Dialogue System that Evokes Users{'} Spontaneous Backchannels,2011,13,4,6,1,38434,teruhisa misu,Proceedings of the {SIGDIAL} 2011 Conference,0,This paper addresses a first step toward a spoken dialogue system that evokes user's spontaneous backchannels. We construct an HMM-based dialogue-style text-to-speech (TTS) system that generates human-like cues that evoke users' backchannels. A spoken dialogue system for information navigation was implemented and the TTS was evaluated in terms of evoked user backchannels. We conducted user experiments and demonstrated that the user backchannels evoked by our TTS are more informative for the system in detecting users' feelings than those by conventional reading-style TTS.
W10-4339,Modeling Spoken Decision Making Dialogue and Optimization of its Dialogue Strategy,2010,7,9,7,1,38434,teruhisa misu,Proceedings of the {SIGDIAL} 2010 Conference,0,"This paper presents a spoken dialogue framework that helps users in making decisions. Users often do not have a definite goal or criteria for selecting from a list of alternatives. Thus the system has to bridge this knowledge gap and also provide the users with an appropriate alternative together with the reason for this recommendation through dialogue. We present a dialogue state model for such decision making dialogue. To evaluate this model, we implement a trial sightseeing guidance system and collect dialogue data. Then, we optimize the dialogue strategy based on the state model through reinforcement learning with a natural policy gradient approach using a user simulator trained on the collected dialogue corpus."
ohtake-etal-2010-dialogue,Dialogue Acts Annotation for {NICT} {K}yoto Tour Dialogue Corpus to Construct Statistical Dialogue Systems,2010,15,6,5,1,35675,kiyonori ohtake,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper introduces a new corpus of consulting dialogues designed for training a dialogue manager that can handle consulting dialogues through spontaneous interactions from the tagged dialogue corpus. We have collected more than 150 hours of consulting dialogues in the tourist guidance domain. We are developing the corpus that consists of speech, transcripts, speech act (SA) tags, morphological analysis results, dependency analysis results, and semantic content tags. This paper outlines our taxonomy of dialogue act (DA) annotation that can describe two aspects of an utterance: the communicative function (SA), and the semantic content of the utterance. We provide an overview of the Kyoto tour dialogue corpus and a preliminary analysis using the DA tags. We also show a result of a preliminary experiment for SA tagging via Support Vector Machines (SVMs). We introduce the current states of the corpus development In addition, we mention the usage of our corpus for the spoken dialogue system that is being developed."
W09-3405,Annotating Dialogue Acts to Construct Dialogue Systems for Consulting,2009,16,11,5,1,35675,kiyonori ohtake,Proceedings of the 7th Workshop on {A}sian Language Resources ({ALR}7),0,"This paper introduces a new corpus of consulting dialogues, which is designed for training a dialogue manager that can handle consulting dialogues through spontaneous interactions from the tagged dialogue corpus. We have collected 130 h of consulting dialogues in the tourist guidance domain. This paper outlines our taxonomy of dialogue act annotation that can describe two aspects of an utterances: the communicative function (speech act), and the semantic content of the utterance. We provide an overview of the Kyoto tour guide dialogue corpus and a preliminary analysis using the dialogue act tags."
W09-3410,Construction of {C}hinese Segmented and {POS}-tagged Conversational Corpora and Their Evaluations on Spontaneous Speech Recognitions,2009,4,2,3,0,40383,xinhui hu,Proceedings of the 7th Workshop on {A}sian Language Resources ({ALR}7),0,"The performance of a corpus-based language and speech processing system depends heavily on the quantity and quality of the training corpora. Although several famous Chinese corpora have been developed, most of them are mainly written text. Even for some existing corpora that contain spoken data, the quantity is insufficient and the domain is limited. In this paper, we describe the development of Chinese conversational annotated textual corpora currently being used in the NICT/ATR speech-to-speech translation system. A total of 510K manually checked utterances provide 3.5M words of Chinese corpora. As far as we know, this is the largest conversational textual corpora in the domain of travel. A set of three parallel corpora is obtained with the corresponding pairs of Japanese and English words from which the Chinese words are translated. Evaluation experiments on these corpora were conducted by comparing the parameters of the language models, perplexities of test sets, and speech recognition performance with Japanese and English. The characteristics of the Chinese corpora, their limitations, and solutions to these limitations are analyzed and discussed."
N09-2056,On the Importance of Pivot Language Selection for Statistical Machine Translation,2009,6,26,4,0.205827,12388,michael paul,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"Recent research on multilingual statistical machine translation focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs. Due to the richness of available language resources, English is in general the pivot language of choice. In this paper, we investigate the appropriateness of languages other than English as pivot languages. Experimental results using state-of-the-art statistical machine translation techniques to translate between twelve languages revealed that the translation quality of 61 out of 110 language pairs improved when a non-English pivot language was chosen."
2009.iwslt-papers.6,Network-based speech-to-speech translation,2009,0,0,8,1,40303,chiori hori,Proceedings of the 6th International Workshop on Spoken Language Translation: Papers,0,"This demo shows the network-based speech-to-speech translation system. The system was designed to perform realtime, location-free, multi-party translation between speakers of different languages. The spoken language modules: automatic speech recognition (ASR), machine translation (MT), and text-to-speech synthesis (TTS), are connected through Web servers that can be accessed via client applications worldwide. In this demo, we will show the multiparty speech-to-speech translation of Japanese, Chinese, Indonesian, Vietnamese, and English, provided by the NICT server. These speech-to-speech modules have been developed by NICT as a part of A-STAR (Asian Speech Translation Advanced Research) consortium project1."
nishiura-etal-2008-evaluation,Evaluation Framework for Distant-talking Speech Recognition under Reverberant Environments: newest Part of the {CENSREC} Series -,2008,6,10,14,0,48254,takanobu nishiura,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Recently, speech recognition performance has been drastically improved by statistical methods and huge speech databases. Now performance improvement under such realistic environments as noisy conditions is being focused on. Since October 2001, we from the working group of the Information Processing Society in Japan have been working on evaluation methodologies and frameworks for Japanese noisy speech recognition. We have released frameworks including databases and evaluation tools called CENSREC-1 (Corpus and Environment for Noisy Speech RECognition 1; formerly AURORA-2J), CENSREC-2 (in-car connected digits recognition), CENSREC-3 (in-car isolated word recognition), and CENSREC-1-C (voice activity detection under noisy conditions). In this paper, we newly introduce a collection of databases and evaluation tools named CENSREC-4, which is an evaluation framework for distant-talking speech under hands-free conditions. Distant-talking speech recognition is crucial for a hands-free speech interface. Therefore, we measured room impulse responses to investigate reverberant speech recognition. The results of evaluation experiments proved that CENSREC-4 is an effective database suitable for evaluating the new dereverberation method because the traditional dereverberation process had difficulty sufficiently improving the recognition performance. The framework was released in March 2008, and many studies are being conducted with it in Japan."
I08-8004,Development of {I}ndonesian Large Vocabulary Continuous Speech Recognition System within A-{STAR} Project,2008,4,24,6,1,5730,sakriani sakti,Proceedings of the Workshop on Technologies and Corpora for Asia-Pacific Speech Translation ({TCAST}),0,"The paper outlines the development of a large vocabulary continuous speech recognition (LVCSR) system for the Indonesian language within the Asian speech translation (A-STAR) project. An overview of the A-STAR project and Indonesian language characteristics will be briefly described. We then focus on a discussion of the development of Indonesian LVCSR, including data resources issues, acoustic modeling, language modeling, the lexicon, and accuracy of recognition. There are three types of Indonesian data resources: daily news, telephone application, and BTEC tasks, which are used in this project. They are available in both text and speech forms. The Indonesian speech recognition engine was trained using the clean speech of both daily news and telephone application tasks. The optimum performance achieved on the BTEC task was 92.47% word accuracy. 1 A-STAR Project Overview The A-STAR project is an Asian consortium that is expected to advance the state-of-the-art in multilingual man-machine interfaces in the Asian region. This basic infrastructure will accelerate the development of large-scale spoken language corpora in Asia and also facilitate the development of related fundamental information communication technologies (ICT), such as multi-lingual speech translation, Figure 1: Outline of future speech-technology services connecting each area in the Asian region through network. multi-lingual speech transcription, and multi-lingual information retrieval. These fundamental technologies can be applied to the human-machine interfaces of various telecommunication devices and services connecting Asian countries through the network using standardized communication protocols as outlined in Fig. 1. They are expected to create digital opportunities, improve our digital capabilities, and eliminate the digital divide resulting from the differences in ICT levels in each area. The improvements to borderless communication in the Asian region are expected to result in many benefits in everyday life including tourism, business, education, and social security. The project was coordinated together by the Advanced Telecommunication Research (ATR) and the National Institute of Information and Communications Technology (NICT) Japan in cooperation with several research institutes in Asia, such as the National Laboratory of Pattern Recognition (NLPR) in China, the Electronics and Telecommunication Research Institute (ETRI) in Korea, the Agency for the Assessment and Application Technology (BPPT) in Indonesia, the National Electronics and Computer Technology Center (NECTEC) in Thailand, the Center for Development of Advanced Computing (CDAC) in India, the National Taiwan University (NTU) in Taiwan. Partners are still being sought for other languages in Asia. More details about the A-STAR project can be found in (Nakamura et al., 2007). 2 Indonesian Language Characteristic The Indonesian language, or so-called Bahasa Indonesia, is a unified language formed from hundreds of languages spoken throughout the Indonesian archipelago. Compared to other languages, which have a high density of native speakers, Indonesian is spoken as a mother tongue by only 7% of the population, and more than 195 million people speak it as a second language with varying degrees of proficiency. There are approximately 300 ethnic groups living throughout 17,508 islands, speaking 365 native languages or no less than 669 dialects (Tan, 2004). At home, people speak their own language, such as Javanese, Sundanese or Balinese, even though almost everybody has a good understanding of Indonesian as they learn it in school. Although the Indonesian language is infused with highly distinctive accents from different ethnic languages, there are many similarities in patterns across the archipelago. Modern Indonesian is derived from the literary of the Malay dialect. Thus, it is closely related to the Malay spoken in Malaysia, Singapore, Brunei, and some other areas. Unlike the Chinese language, it is not a tonal language. Compared with European languages, Indonesian has a strikingly small use of gendered words. Plurals are often expressed by means of word repetition. It is also a member of the agglutinative language family, meaning that it has a complex range of prefixes and suffixes, which are attached to base words. Consequently, a word can become very long. More details on Indonesian characteristics can be found in (Sakti et al., 2004). 3 Indonesian Phoneme Set The Indonesian phoneme set is defined based on Indonesian grammar described in (Alwi et al., 2003). A full phoneme set contains 33 phoneme symbols in total, which consists of 10 vowels (including diphthongs), 22 consonants, and one silent symbol. The vowel articulation pattern of the Indonesian language, which indicates the first two resonances of the vocal tract, F1 (height) and F2 (backness), is shown in Fig. 2."
C08-3006,Multilingual Mobile-Phone Translation Services for World Travelers,2008,5,13,7,0.205827,12388,michael paul,Coling 2008: Companion volume: Demonstrations,0,This demonstration introduces two new multilingual translation services for mobile phones. The first translation service provides state-of-the-art text-to-text translations of Japanese as well as English conversational spoken language in the travel domain into 17 languages using statistical machine translation technologies trained automatically from a large-scale multilingual corpus. The second demonstration is a speech translation service between Japanese and English for real environments. It is based on distributed speech recognition with noise suppression. Flexible interfaces between internal and external speech translation resources ease the portability of the system to other languages and enable real-time location-free communication world-wide.
P07-2007,{NICT}-{ATR} Speech-to-Speech Translation System,2007,13,6,3,0,129,eiichiro sumita,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"This paper describes the latest version of speech-to-speech translation systems developed by the team of NICT-ATR for over twenty years. The system is now ready to be deployed for the travel domain. A new noise-suppression technique notably improves speech recognition performance. Corpus-based approaches of recognition, translation, and synthesis enable coverage of a wide variety of topics and portability to other languages."
itahashi-etal-2006-oriental,"Oriental {COCOSDA}: Past, Present and Future",2006,1,1,3,0,30020,shuichi itahashi,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The purpose of Oriental COCOSDA is to exchange ideas, to share information and to discuss regional matters on creation, utilization, dissemination of spoken language corpora of oriental languages and also on the assessment methods of speech recognition/synthesis systems as well as to promote speech research on oriental languages. A series of International Workshop on East Asian Language Resources and Evaluation (EALREW) or Oriental COCOSDA Workshop has been held annually since the preparatory meeting held in 1997. After that, we have had a series of workshops every year in Japan, Taiwan, China, Korea, Thailand, Singapore, India and Indonesia. The Oriental COCOSDA is managed by a convener, three advisory members, and 21 representatives from ten regions in Oriental countries. We need much more Pan-Asia collaboration with research organizations and consortia, though there are some domestic activities in Oriental countries. We note that speech research has become popular gradually in Oriental countries including Malaysia, Vietnam, Xinjang Uygur Autonomous Region of China, etc. We plan to hold future Oriental COCOSDA meetings in these places in order to promote speech research there."
2006.iwslt-papers.8,Development of client-server speech translation system on a multi-lingual speech communication platform,2006,11,7,5,0,48707,tohru shimizu,Proceedings of the Third International Workshop on Spoken Language Translation: Papers,0,"This paper describes a client-server speech-to-speech translation system developed on a multi-lingual speech communication platform. This platform enables easy assembly of speech communication system from the corresponding software modules (e.g. speech recognition, spoken language machine-translation, speech synthesis). This client-server speech translation system is designed for use at mobile terminals. Terminals and servers are connected via a 3G public mobile phone networks, and speech translation services are available at various places with thin client. This system realizes hands-free communication and robustness for real use of speech translation in noisy environments. A microphone array and new noise suppression technique improves speech recognition performance, and a corpus-based approach enables wide coverage, robustness and portability to new languages and domains. Recent evaluation of the overall system showed that the utterance correctness of speech recognition output achieved 83%, and more than 88% of the utterances are correctly translated for Japanse-English and JapaneseChinese."
2004.iwslt-papers.8,Multi-lingual speech recognition system for speech-to-speech translation,2004,13,2,1,1,1441,satoshi nakamura,Proceedings of the First International Workshop on Spoken Language Translation: Papers,0,"This paper describes the speech recognition module of the speech-to-speech translation system being currently developed at ATR. It is a multi-lingual large vocabulary continuous speech recognition system supporting Japanese, English and Chinese languages. A corpusbased statistical approach was adopted for the system design. The database we collected consists of more than 600 000 sentences covering broad range of travel related conversations in each of the three languages. The recognition system is based on language-dependent acoustic and language models, and pronunciation dictionaries. The models are built using the latest training methods developed at ATR as the Minimum Description Length Successive State Splitting (MDL-SSS) and Multi-dimensional Composite N-gram techniques. The specifics of each language are taken into account in order to achieve high recognition performance. The speech recognition system is under constant improvement and enhancement, and although the models for the different languages are at different development stages, the recent evaluation experiments showed that the recognition performance is above 92% for every language."
kuwabara-etal-2002-present,"The Present Status of Speech Database in {J}apan: Development, Management, and Application to Speech Research",2002,2,2,5,0,53304,hisao kuwabara,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The present status of Japanese speech database has been described. The database project in Japan started in early 1980s. The first one was a committee of Japan Electronic Industry Development Association, abbreviated as JEIDA, which aimed at creating a speech database that can commonly evaluate performance of the then existing speech input/output machines and systems. Several database projects have been undertaken since then including the one initiated by the Advanced Telecommunication Research Institute (ATR) and now it has come to the point where an enormous amount of spontaneous speech data is available. A survey has been conducted recently about the usage of the presently existing speech databases among industry and university institutions in Japan where speech research is now actively going on. It has been revealed that the ATRxe2x80x99s continuous speech database is the most frequently used followed by the equivalent version of the Acoustical Society of Japan."
nakamura-etal-2000-acoustical,Acoustical Sound Database in Real Environments for Sound Scene Understanding and Hands-Free Speech Recognition,2000,5,159,1,1,1441,satoshi nakamura,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"LREC2000: the 2nd International Conference on Language Resources and Evaluation, May 31 - June 2, 2000, Athens, Greece."
