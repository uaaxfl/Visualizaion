2021.nlp4posimpact-1.2,The Climate Change Debate and Natural Language Processing,2021,-1,-1,1,1,2824,manfred stede,Proceedings of the 1st Workshop on NLP for Positive Impact,0,"The debate around climate change (CC){---}its extent, its causes, and the necessary responses{---}is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the {''}text-as-data{''} paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change."
2021.konvens-1.23,Automatically evaluating the conceptual complexity of {G}erman texts,2021,-1,-1,2,0,5581,freya hewett,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2021.germeval-1.2,{UPA}pplied{CL} at {G}erm{E}val 2021: Identifying Fact-Claiming and Engaging {F}acebook Comments Using Transformers,2021,-1,-1,2,1,6201,robin schaefer,"Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments",0,"In this paper we present UPAppliedCL{'}s contribution to the GermEval 2021 Shared Task. In particular, we participated in Subtasks 2 (Engaging Comment Classification) and 3 (Fact-Claiming Comment Classification). While acceptable results can be obtained by using unigrams or linguistic features in combination with traditional machine learning models, we show that for both tasks transformer models trained on fine-tuned BERT embeddings yield best results."
2020.lrec-1.131,Shallow Discourse Parsing for Under-Resourced Languages: Combining Machine Translation and Annotation Projection,2020,-1,-1,3,0,16881,henny sluytergathje,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Shallow Discourse Parsing (SDP), the identification of coherence relations between text spans, relies on large amounts of training data, which so far exists only for English - any other language is in this respect an under-resourced one. For those languages where machine translation from English is available with reasonable quality, MT in conjunction with annotation projection can be an option for producing an SDP resource. In our study, we translate the English Penn Discourse TreeBank into German and experiment with various methods of annotation projection to arrive at the German counterpart of the PDTB. We describe the key characteristics of the corpus as well as some typical sources of errors encountered during its creation. Then we evaluate the GermanPDTB by training components for selected sub-tasks of discourse parsing on this silver data and compare performance to the same components when trained on the gold, original PDTB corpus."
2020.lrec-1.133,The {P}otsdam Commentary Corpus 2.2: Extending Annotations for Shallow Discourse Parsing,2020,-1,-1,2,0.912698,56,peter bourgonje,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present the Potsdam Commentary Corpus 2.2, a German corpus of news editorials annotated on several different levels. New in the 2.2 version of the corpus are two additional annotation layers for coherence relations following the Penn Discourse TreeBank framework. Specifically, we add relation senses to an already existing layer of discourse connectives and their arguments, and we introduce a new layer with additional coherence relation types, resulting in a German corpus that mirrors the PDTB. The aim of this is to increase usability of the corpus for the task of shallow discourse parsing. In this paper, we provide inter-annotator agreement figures for the new annotations and compare corpus statistics based on the new annotations to the equivalent statistics extracted from the PDTB."
2020.lrec-1.138,{D}i{ML}ex-{B}angla: A Lexicon of {B}angla Discourse Connectives,2020,-1,-1,2,1,16890,debopam das,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present DiMLex-Bangla, a newly developed lexicon of discourse connectives in Bangla. The lexicon, upon completion of its first version, contains 123 Bangla connective entries, which are primarily compiled from the linguistic literature and translation of English discourse connectives. The lexicon compilation is later augmented by adding more connectives from a currently developed corpus, called the Bangla RST Discourse Treebank (Das and Stede, 2018). DiMLex-Bangla provides information on syntactic categories of Bangla connectives, their discourse semantics and non-connective uses (if any). It uses the format of the German connective lexicon DiMLex (Stede and Umbach, 1998), which provides a cross-linguistically applicable XML schema. The resource is the first of its kind in Bangla, and is freely available for use in studies on discourse structure and computational applications."
2020.lrec-1.139,Semi-Supervised Tri-Training for Explicit Discourse Argument Expansion,2020,-1,-1,2,1,11480,rene knaebel,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper describes a novel application of semi-supervision for shallow discourse parsing. We use a neural approach for sequence tagging and focus on the extraction of explicit discourse arguments. First, additional unlabeled data is prepared for semi-supervised learning. From this data, weak annotations are generated in a first setting and later used in another setting to study performance differences. In our studies, we show an increase in the performance of our models that ranges between 2-10{\%} F1 score. Further, we give some insights to the generated discourse annotations and compare the developed additional relations with the training relations. We release this new dataset of explicit discourse arguments to enable the training of large statistical models."
2020.findings-emnlp.222,Adapting Coreference Resolution to {T}witter Conversations,2020,-1,-1,4,1,19705,berfin aktacs,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter conversation data. Further experiments by combining different portions of OntoNotes with Twitter data show that selecting text genres for the training data can beat the mere maximization of training data amount. In addition, we inspect several phenomena such as the role of deictic pronouns in conversational data, and present additional results for variant settings. Our best configuration improves the performance of the{''}out of the box{''} system by 21.6{\%}."
2020.coling-main.505,Exploiting a lexical resource for discourse connective disambiguation in {G}erman,2020,-1,-1,2,0.912698,56,peter bourgonje,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper we focus on connective identification and sense classification for explicit discourse relations in German, as two individual sub-tasks of the overarching Shallow Discourse Parsing task. We successively augment a purely-empirical approach based on contextualised embeddings with linguistic knowledge encoded in a connective lexicon. In this way, we improve over published results for connective identification, achieving a final F1-score of 87.93; and we introduce, to the best of our knowledge, first results for German sense classification, achieving an F1-score of 87.13. Our approach demonstrates that a connective lexicon can be a valuable resource for those languages that do not have a large PDTB-style-annotated coprus available."
2020.coling-main.508,Variation in Coreference Strategies across Genres and Production Media,2020,-1,-1,2,1,19705,berfin aktacs,Proceedings of the 28th International Conference on Computational Linguistics,0,"In response to (i) inconclusive results in the literature as to the properties of coreference chains in written versus spoken language, and (ii) a general lack of work on automatic coreference resolution on both spoken language and social media, we undertake a corpus study involving the various genre sections of Ontonotes, the Switchboard corpus, and a corpus of Twitter conversations. Using a set of measures that previously have been applied individually to different data sets, we find fairly clear patterns of {``}behavior{''} for the different genres/media. Besides their role for psycholinguistic investigation (why do we employ different coreference strategies when we write or speak) and for the placement of Twitter in the spoken{--}written continuum, we see our results as a contribution to approaching genre-/media-specific coreference resolution."
2020.codi-1.7,Contextualized Embeddings for Connective Disambiguation in Shallow Discourse Parsing,2020,-1,-1,2,1,11480,rene knaebel,Proceedings of the First Workshop on Computational Approaches to Discourse,0,"This paper studies a novel model that simplifies the disambiguation of connectives for explicit discourse relations. We use a neural approach that integrates contextualized word embeddings and predicts whether a connective candidate is part of a discourse relation or not. We study the influence of those context-specific embeddings. Further, we show the benefit of training the tasks of connective disambiguation and sense classification together at the same time. The success of our approach is supported by state-of-the-art results."
2020.argmining-1.6,Annotation and Detection of Arguments in Tweets,2020,-1,-1,2,1,6201,robin schaefer,Proceedings of the 7th Workshop on Argument Mining,0,"Notwithstanding the increasing role Twitter plays in modern political and social discourse, resources built for conducting argument mining on tweets remain limited. In this paper, we present a new corpus of German tweets annotated for argument components. To the best of our knowledge, this is the first corpus containing not only annotated full tweets but also argumentative spans within tweets. We further report first promising results using supervised classification (F1: 0.82) and sequence labeling (F1: 0.72) approaches."
W19-8607,Computational Argumentation Synthesis as a Language Modeling Task,2019,0,1,4,0,15731,roxanne baff,Proceedings of the 12th International Conference on Natural Language Generation,0,"Synthesis approaches in computational argumentation so far are restricted to generating claim-like argument units or short summaries of debates. Ultimately, however, we expect computers to generate whole new arguments for a given stance towards some topic, backing up claims following argumentative and rhetorical considerations. In this paper, we approach such an argumentation synthesis as a language modeling task. In our language model, argumentative discourse units are the {``}words{''}, and arguments represent the {``}sentences{''}. Given a pool of units for any unseen topic-stance pair, the model selects a set of unit types according to a basic rhetorical strategy (logos vs. pathos), arranges the structure of the types based on the units{'} argumentative roles, and finally {``}phrases{''} an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments."
W19-4512,The Utility of Discourse Parsing Features for Predicting Argumentation Structure,2019,0,0,4,0,5581,freya hewett,Proceedings of the 6th Workshop on Argument Mining,0,"Research on argumentation mining from text has frequently discussed relationships to discourse parsing, but few empirical results are available so far. One corpus that has been annotated in parallel for argumentation structure and for discourse structure (RST, SDRT) are the {`}argumentative microtexts{'} (Peldszus and Stede, 2016a). While results on perusing the gold RST annotations for predicting argumentation have been published (Peldszus and Stede, 2016b), the step to automatic discourse parsing has not yet been taken. In this paper, we run various discourse parsers (RST, PDTB) on the corpus, compare their results to the gold annotations (for RST) and then assess the contribution of automatically-derived discourse features for argumentation parsing. After reproducing the state-of-the-art Evidence Graph model from Afantenos et al. (2018) for the microtexts, we find that PDTB features can indeed improve its performance."
W19-3015,Coherence models in schizophrenia,2019,-1,-1,8,0,24569,sandra just,Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology,0,"Incoherent discourse in schizophrenia has long been recognized as a dominant symptom of the mental disorder (Bleuler, 1911/1950). Recent studies have used modern sentence and word embeddings to compute coherence metrics for spontaneous speech in schizophrenia. While clinical ratings always have a subjective element, computational linguistic methodology allows quantification of speech abnormalities. Clinical and empirical knowledge from psychiatry provide the theoretical and conceptual basis for modelling. Our study is an interdisciplinary attempt at improving coherence models in schizophrenia. Speech samples were obtained from healthy controls and patients with a diagnosis of schizophrenia or schizoaffective disorder and different severity of positive formal thought disorder. Interviews were transcribed and coherence metrics derived from different embeddings. One model found higher coherence metrics for controls than patients. All other models remained non-significant. More detailed analysis of the data motivates different approaches to improving coherence models in schizophrenia, e.g. by assessing referential abnormalities."
W19-2707,Annotating Shallow Discourse Relations in {T}witter Conversations,2019,-1,-1,4,0.950688,23351,tatjana scheffler,Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019,0,"We introduce our pilot study applying PDTB-style annotation to Twitter conversations. Lexically grounded coherence annotation for Twitter threads will enable detailed investigations of the discourse structure of conversations on social media. Here, we present our corpus of 185 threads and annotation, including an inter-annotator agreement study. We discuss our observations as to how Twitter discourses differ from written news text wrt. discourse connectives and relations. We confirm our hypothesis that discourse relations in written social media conversations are expressed differently than in (news) text. We find that in Twitter, connective arguments frequently are not full syntactic clauses, and that a few general connectives expressing EXPANSION and CONTINGENCY make up the majority of the explicit relations in our data."
W19-2712,{RST}-Tace A tool for automatic comparison and evaluation of {RST} trees,2019,-1,-1,4,0,24634,shujun wan,Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019,0,"This paper presents RST-Tace, a tool for automatic comparison and evaluation of RST trees. RST-Tace serves as an implementation of Iruskieta{'}s comparison method, which allows trees to be compared and evaluated without the influence of decisions at lower levels in a tree in terms of four factors: constituent, attachment point, nuclearity as well as relation. RST-Tace can be used regardless of the language or the size of rhetorical trees. This tool aims to measure the agreement between two annotators. The result is reflected by F-measure and inter-annotator agreement. Both the comparison table and the result of the evaluation can be obtained automatically."
P19-2010,Automated Cross-language Intelligibility Analysis of {P}arkinson{'}s Disease Patients Using Speech Recognition Technologies,2019,0,2,3,0,25489,nina hosseinikivanani,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Speech deficits are common symptoms amongParkinson{'}s Disease (PD) patients. The automatic assessment of speech signals is promising for the evaluation of the neurological state and the speech quality of the patients. Recently, progress has been made in applying machine learning and computational methods to automatically evaluate the speech of PD patients. In the present study, we plan to analyze the speech signals of PD patients and healthy control (HC) subjects in three different languages: German, Spanish, and Czech, with the aim to identify biomarkers to discriminate between PD patients and HC subjects and to evaluate the neurological state of the patients. Therefore, the main contribution of this study is the automatic classification of PD patients and HC subjects in different languages with focusing on phonation, articulation, and prosody. We will focus on an intelligibility analysis based on automatic speech recognition systems trained on these three languages. This is one of the first studies done that considers the evaluation of the speech of PD patients in different languages. The purpose of this research proposal is to build a model that can discriminate PD and HC subjects even when the language used for train and test is different."
K19-1072,Window-Based Neural Tagging for Shallow Discourse Argument Labeling,2019,0,0,2,1,11480,rene knaebel,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"This paper describes a novel approach for the task of end-to-end argument labeling in shallow discourse parsing. Our method describes a decomposition of the overall labeling task into subtasks and a general distance-based aggregation procedure. For learning these subtasks, we train a recurrent neural network and gradually replace existing components of our baseline by our model. The model is trained and evaluated on the Penn Discourse Treebank 2 corpus. While it is not as good as knowledge-intense approaches, it clearly outperforms other models that are also trained without additional linguistic features."
W18-5902,Stance-Taking in Topics Extracted from Vaccine-Related Tweets and Discussion Forum Posts,2018,0,0,2,0.905534,22194,maria skeppstedt,Proceedings of the 2018 {EMNLP} Workshop {SMM}4{H}: The 3rd Social Media Mining for Health Applications Workshop {\\&} Shared Task,0,"The occurrence of stance-taking towards vaccination was measured in documents extracted by topic modelling from two different corpora, one discussion forum corpus and one tweet corpus. For some of the topics extracted, their most closely associated documents contained a proportion of vaccine stance-taking texts that exceeded the corpus average by a large margin. These extracted document sets would, therefore, form a useful resource in a process for computer-assisted analysis of argumentation on the subject of vaccination."
W18-5218,More or less controlled elicitation of argumentative text: Enlarging a microtext corpus via crowdsourcing,2018,0,5,3,0.905534,22194,maria skeppstedt,Proceedings of the 5th Workshop on Argument Mining,0,"We present an extension of an annotated corpus of short argumentative texts that had originally been built in a controlled text production experiment. Our extension more than doubles the size of the corpus by means of crowdsourcing. We report on the setup of this experiment and on the consequences that crowdsourcing had for assembling the data, and in particular for annotation. We labeled the argumentative structure by marking claims, premises, and relations between them, following the scheme used in the original corpus, but had to make a few modifications in response to interesting phenomena in the data. Finally, we report on an experiment with the automatic prediction of this argumentation structure: We first replicated the approach of an earlier study on the original corpus, and compare the performance to various settings involving the extension."
W18-5037,Identifying Explicit Discourse Connectives in {G}erman,2018,0,1,2,0.912698,56,peter bourgonje,Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"We are working on an end-to-end Shallow Discourse Parsing system for German and in this paper focus on the first subtask: the identification of explicit connectives. Starting with the feature set from an English system and a Random Forest classifier, we evaluate our approach on a (relatively small) German annotated corpus, the Potsdam Commentary Corpus. We introduce new features and experiment with including additional training data obtained through annotation projection and achieve an f-score of 83.89."
W18-5042,Constructing a Lexicon of {E}nglish Discourse Connectives,2018,0,1,4,1,16890,debopam das,Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"We present a new lexicon of English discourse connectives called DiMLex-Eng, built by merging information from two annotated corpora and an additional list of relation signals from the literature. The format follows the German connective lexicon DiMLex, which provides a cross-linguistically applicable XML schema. DiMLex-Eng contains 149 English connectives, and gives information on syntactic categories, discourse semantics and non-connective uses (if any). We report on the development steps and discuss design decisions encountered in the lexicon expansion phase. The resource is freely available for use in studies of discourse structure and computational applications."
W18-0701,Anaphora Resolution for {T}witter Conversations: An Exploratory Study,2018,0,2,3,1,19705,berfin aktacs,"Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference",0,"We present a corpus study of pronominal anaphora on Twitter conversations. After outlining the specific features of this genre, with respect to reference resolution, we explain the construction of our corpus and the annotation steps. From this we derive a list of phenomena that need to be considered when performing anaphora resolution on this type of data. Finally, we test the performance of an off-the-shelf resolution system, and provide some qualitative error analysis."
L18-1258,A Multi-layer Annotated Corpus of Argumentative Text: From Argument Schemes to Discourse Relations,2018,0,1,2,0,15510,elena musi,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1288,Developing the {B}angla {RST} {D}iscourse {T}reebank,2018,0,0,2,1,16890,debopam das,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1693,A Lexicon of Discourse Markers for {P}ortuguese {--} {LDM}-{PT},2018,0,2,3,0,18863,amalia mendes,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1318,Argumentation Synthesis following Rhetorical Strategies,2018,0,3,2,0.164474,6983,henning wachsmuth,Proceedings of the 27th International Conference on Computational Linguistics,0,"Persuasion is rarely achieved through a loose set of arguments alone. Rather, an effective delivery of arguments follows a rhetorical strategy, combining logical reasoning with appeals to ethics and emotion. We argue that such a strategy means to select, arrange, and phrase a set of argumentative discourse units. In this paper, we model rhetorical strategies for the computational synthesis of effective argumentation. In a study, we let 26 experts synthesize argumentative texts with different strategies for 10 topics. We find that the experts agree in the selection significantly more when following the same strategy. While the texts notably vary for different strategies, especially their arrangement remains stable. The results suggest that our model enables a strategical synthesis."
W17-6802,Extracting word lists for domain-specific implicit opinions from corpora,2017,0,0,2,0,31335,nuria castello,{IWCS} 2017 - 12th International Conference on Computational Semantics - Long papers,0,None
W17-5801,Automatic detection of stance towards vaccination in online discussion forums,2017,16,1,3,0.905534,22194,maria skeppstedt,Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 ({DDDSM}-2017),0,"A classifier for automatic detection of stance towards vaccination in online forums was trained and evaluated. Debate posts from six discussion threads on the British parental website Mumsnet were manually annotated for stance {`}against{'} or {`}for{'} vaccination, or as {`}undecided{'}. A support vector machine, trained to detect the three classes, achieved a macro F-score of 0.44, while a macro F-score of 0.62 was obtained by the same type of classifier on the binary classification task of distinguishing stance {`}against{'} vaccination from stance {`}for{'} vaccination. These results show that vaccine stance detection in online forums is a difficult task, at least for the type of model investigated and for the relatively small training corpus that was used. Future work will therefore include an expansion of the training data and an evaluation of other types of classifiers and features."
W17-3602,"The Good, the Bad, and the Disagreement: Complex ground truth in rhetorical structure analysis",2017,14,0,2,1,16890,debopam das,Proceedings of the 6th Workshop on Recent Advances in {RST} and Related Formalisms,0,None
W17-1506,Multi-source annotation projection of coreference chains: assessing strategies and testing opportunities,2017,0,1,2,0.666667,13685,yulia grishina,Proceedings of the 2nd Workshop on Coreference Resolution Beyond {O}nto{N}otes ({CORBON} 2017),0,"In this paper, we examine the possibility of using annotation projection from multiple sources for automatically obtaining coreference annotations in the target language. We implement a multi-source annotation projection algorithm and apply it on an English-German-Russian parallel corpus in order to transfer coreference chains from two sources to the target side. Operating in two settings {--} a low-resource and a more linguistically-informed one {--} we show that automatic coreference transfer could benefit from combining information from multiple languages, and assess the quality of both the extraction and the linking of target coreference mentions."
W16-4309,Generating Sentiment Lexicons for {G}erman {T}witter,2016,29,0,2,0,33592,uladzimir sidarenka,"Proceedings of the Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media ({PEOPLES})",0,"Despite a substantial progress made in developing new sentiment lexicon generation (SLG) methods for English, the task of transferring these approaches to other languages and domains in a sound way still remains open. In this paper, we contribute to the solution of this problem by systematically comparing semi-automatic translations of common English polarity lists with the results of the original automatic SLG algorithms, which were applied directly to German data. We evaluate these lexicons on a corpus of 7,992 manually annotated tweets. In addition to that, we also collate the results of dictionary- and corpus-based SLG methods in order to find out which of these paradigms is better suited for the inherently noisy domain of social media. Our experiments show that semi-automatic translations notably outperform automatic systems (reaching a macro-averaged F1-score of 0.589), and that dictionary-based techniques produce much better polarity lists as compared to corpus-based approaches (whose best F1-scores run up to 0.479 and 0.419 respectively) even for the non-standard Twitter genre."
W16-2812,Rhetorical structure and argumentation structure in monologue text,2016,13,11,2,1,28019,andreas peldszus,Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016),0,None
W16-0706,Anaphoricity in Connectives: A Case Study on {G}erman,2016,14,2,1,1,2824,manfred stede,Proceedings of the Workshop on Coreference Resolution Beyond {O}nto{N}otes ({CORBON} 2016),0,"Anaphoric connectives are event anaphors (or abstract anaphors) that in addition convey a coherence relation holding between the an- tecedent and the host clause of the connective. Some of them carry an explicitly-anaphoric morpheme, others do not. We analysed the set of German connectives for this property and found that many have an additional non- connective reading, where they serve as nomi- nal anaphors. Furthermore, many connectives can have multiple senses, so altogether the processing of these words can involve substan- tial disambiguation. We study the problem for one specific German word, demzufolge, which can be taken as representative for a large group of similar words."
L16-1160,Adding Semantic Relations to a Large-Coverage Connective Lexicon of {G}erman,2016,0,8,2,1,23351,tatjana scheffler,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"DiMLex is a lexicon of German connectives that can be used for various language understanding purposes. We enhanced the coverage to 275 connectives, which we regard as covering all known German discourse connectives in current use. In this paper, we consider the task of adding the semantic relations that can be expressed by each connective. After discussing different approaches to retrieving semantic information, we settle on annotating each connective with senses from the new PDTB 3.0 sense hierarchy. We describe our new implementation in the extended DiMLex, which will be available for research purposes."
L16-1167,Parallel Discourse Annotations on a Corpus of Short Texts,2016,17,10,1,1,2824,manfred stede,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present the first corpus of texts annotated with two alternative approaches to discourse structure, Rhetorical Structure Theory (Mann and Thompson, 1988) and Segmented Discourse Representation Theory (Asher and Lascarides, 2003). 112 short argumentative texts have been analyzed according to these two theories. Furthermore, in previous work, the same texts have already been annotated for their argumentation structure, according to the scheme of Peldszus and Stede (2013). This corpus therefore enables studies of correlations between the two accounts of discourse structure, and between discourse and argumentation. We converted the three annotation formats to a common dependency tree format that enables to compare the structures, and we describe some initial findings."
L16-1271,Information structure in the {P}otsdam Commentary Corpus: Topics,2016,19,0,1,1,2824,manfred stede,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The Potsdam Commentary Corpus is a collection of 175 German newspaper commentaries annotated on a variety of different layers. This paper introduces a new layer that covers the linguistic notion of information-structural topic (not to be confused with {`}topic{'} as applied to documents in information retrieval). To our knowledge, this is the first larger topic-annotated resource for German (and one of the first for any language). We describe the annotation guidelines and the annotation process, and the results of an inter-annotator agreement study, which compare favourably to the related work. The annotated corpus is freely available for research."
K16-2002,"{OPT}: {O}slo{--}{P}otsdam{--}{T}eesside. Pipelining Rules, Rankers, and Classifier Ensembles for Shallow Discourse Parsing",2016,15,2,5,0,2623,stephan oepen,Proceedings of the {C}o{NLL}-16 shared task,0,"The OPT submission to the Shared Task of the 2016 Conference on Natural Language Learning (CoNLL) implements a xe2x80x98classicxe2x80x99 pipeline architecture, combining binary classification of (candidate) explicit connectives, heuristic rules for non-explicit discourse relations, ranking and xe2x80x98editingxe2x80x99 of syntactic constituents for argument identification, and an ensemble of classifiers to assign discourse senses. With an end-toend performance of 27.77 F1 on the English xe2x80x98blindxe2x80x99 test data, our system advances the previous state of the art (Wang & Lan, 2015) by close to four F1 points, with particularly good results for the argument identification sub-tasks. OPT system results appear more competitive on the new, xe2x80x98blindxe2x80x99 test data than on the xe2x80x98testxe2x80x99 and xe2x80x98developmentxe2x80x99 sections of the Penn Discourse Treebank (PDTB; Prasad et al., 2008), which may indicate reduced over-fitting to specific properties of the venerable Wall Street Journal (WSJ) text underlying the PDTB."
C16-1312,Towards assessing depth of argumentation,2016,11,0,1,1,2824,manfred stede,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"For analyzing argumentative text, we propose to study the {`}depth{'} of argumentation as one important component, which we distinguish from argument quality. In a pilot study with German newspaper commentary texts, we asked students to rate the degree of argumentativeness, and then looked for correlations with features of the annotated argumentation structure and the rhetorical structure (in terms of RST). The results indicate that the human judgements correlate with our operationalization of depth and with certain structural features of RST trees."
W15-3403,Knowledge-lean projection of coreference chains across languages,2015,25,1,2,0.784314,13685,yulia grishina,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"Common technologies for automatic coreference resolution require either a language-specific rule set or large collections of manually annotated data, which is typically limited to newswire texts in major languages. This makes it difficult to develop coreference resolvers for a large number of the so-called low-resourced languages. We apply a direct projection algorithm on a multi-genre and multilingual corpus (English, German, Russian) to automatically produce coreference annotations for two target languages without exploiting any linguistic knowledge of the languages. Our evaluation of the projected annotations shows promising results, and the error analysis reveals structural differences of referring expressions and coreference chains for the three languages, which can now be targeted with more linguistically-informed projection algorithms."
W15-0513,Towards Detecting Counter-considerations in Text,2015,14,8,2,1,28019,andreas peldszus,Proceedings of the 2nd Workshop on Argumentation Mining,0,"Argumentation mining obviously involves finding support relations between statements, but many interesting instances of argumentation also contain counter-considerations, which the author mentions in order to preempt possible objections by the readers. A counterconsideration in monologue text thus involves a switch of perspective toward an imaginary opponent. We present a classification approach to classifying counter-considerations and apply it to two different corpora: a selection of very short argumentative texts produced in a text generation experiment, and a set of newspaper commentaries. As expected, the latter pose more difficulties, which we investigate in a brief error anaylsis."
D15-1110,Joint prediction in {MST}-style discourse parsing for argumentation mining,2015,21,51,2,1,28019,andreas peldszus,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We introduce a new approach to argumentation mining that we applied to a parallel German/English corpus of short texts annotated with argumentation structure. We focus on structure prediction, which we break into a number of subtasks: relation identification, central claim identification, role classification, and function classification. Our new model jointly predicts different aspects of the structure by combining the different subtask predictions in the edge weights of an evidence graph; we then apply a standard MST decoding algorithm. This model not only outperforms two reasonable baselines and two datadriven models of global argument structure for the difficult subtask of relation identification, but also improves the results for central claim identification and function classification and it compares favorably to a complex mstparser pipeline."
W14-2906,Conceptual and Practical Steps in Event Coreference Analysis of Large-scale Data,2014,18,4,4,0,27926,fatemeh asr,"Proceedings of the Second Workshop on {EVENTS}: Definition, Detection, Coreference, and Representation",0,"A simple conceptual model is employed to investigate events, and break the task of coreference resolution into two steps: semantic class detection and similaritybased matching. With this perspective an algorithm is implemented to cluster event mentions in a large-scale corpus. Results on test data from AQUAINT TimeML, which we annotated manually with coreference links, reveal how semantic conventions vs. information available in the context of event mentions affect decisions in coreference analysis."
stede-neumann-2014-potsdam,{P}otsdam Commentary Corpus 2.0: Annotation for Discourse Research,2014,13,27,1,1,2824,manfred stede,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a revised and extended version of the Potsdam Commentary Corpus, a collection of 175 German newspaper commentaries (op-ed pieces) that has been annotated with syntax trees and three layers of discourse-level information: nominal coreference,connectives and their arguments (similar to the PDTB, Prasad et al. 2008), and trees reflecting discourse structure according to Rhetorical Structure Theory (Mann/Thompson 1988). Connectives have been annotated with the help of a semi-automatic tool, Conano (Stede/Heintze 2004), which identifies most connectives and suggests arguments based on their syntactic category. The other layers have been created manually with dedicated annotation tools. The corpus is made available on the one hand as a set of original XML files produced with the annotation tools, based on identical tokenization. On the other hand, it is distributed together with the open-source linguistic database ANNIS3 (Chiarcos et al. 2008; Zeldes et al. 2009), which provides multi-layer search functionality and layer-specific visualization modules. This allows for comfortable qualitative evaluation of the correlations between annotation layers."
budzynska-etal-2014-model,A Model for Processing Illocutionary Structures and Argumentation in Debates,2014,21,16,5,0,39854,kasia budzynska,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we briefly present the objectives of Inference Anchoring Theory (IAT) and the formal structure which is proposed for dialogues. Then, we introduce our development corpus, and a computational model designed for the identification of discourse minimal units in the context of argumentation and the illocutionary force associated with each unit. We show the categories of resources which are needed and how they can be reused in different contexts."
sonntag-stede-2014-grapat,{G}ra{PAT}: a Tool for Graph Annotations,2014,13,5,2,1,38618,jonathan sonntag,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We introduce GraPAT, a web-based annotation tool for building graph structures over text. Graphs have been demonstrated to be relevant in a variety of quite diverse annotation efforts and in different NLP applications, and they serve to model annotatorsÂ intuitions quite closely. In particular, in this paper we discuss the implementation of graph annotations for sentiment analysis, argumentation structure, and rhetorical text structures. All of these scenarios can create certain problems for existing annotation tools, and we show how GraPAT can help to overcome such difficulties."
W13-2708,Towards a Tool for Interactive Concept Building for Large Scale Analysis in the Humanities,2013,22,7,6,0,1033,andre blessing,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"We develop a pipeline consisting of various text processing tools which is designed to assist political scientists in finding specific, complex concepts within large amounts of text. Our main focus is the interaction between the political scientists and the natural language processing groups to ensure a beneficial assistance for the political scientists and new application challenges for NLP. It is of particular importance to find a xe2x80x9ccommon languagexe2x80x9d between the different disciplines. Therefore, we use an interactive web-interface which is easily usable by non-experts. It interfaces an active learning algorithm which is complemented by the NLP pipeline to provide a rich feature selection. Political scientists are thus enabled to use their own intuitions to find custom concepts."
W13-2312,Importing {MASC} into the {ANNIS} linguistic database: A case study of mapping {G}r{AF},2013,8,5,3,0,11002,arne neumann,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"This paper describes the importation of Manually Annotated Sub-Corpus (MASC) data and annotations into the linguistic database ANNIS, which allows users to visualize and query linguistically-annotated corpora. We outline the process of mapping MASCxe2x80x99s GrAF representation to ANNISxe2x80x99s internal format relANNIS and demonstrate how the system provides access to multiple annotation layers in the corpus. This access provides information about inter-layer relations and dependencies that have been previously difficult to explore, and which are highly valuable for continued development of language processing applications."
W13-2324,Ranking the annotators: An agreement study on argumentation structure,2013,15,22,2,1,28019,andreas peldszus,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"We investigate methods for evaluating agreement among a relatively large group of annotators who have not received extensive training and differ in terms of ability and motivation. We show that it is possible to isolate a reliable subgroup of annotators, so that aspects of the difficulty of the underlying task can be studied. Our task is to annotate the argumentative structure of short texts."
W13-1611,From newspaper to microblogging: What does it take to find opinions?,2013,11,2,5,0,41066,wladimir sidorenko,"Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"We compare the performance of two lexiconbased sentiment systems xe2x80x90 SentiStrength (Thelwall et al., 2012) and SO-CAL (Taboada et al., 2011) xe2x80x90 on the two genres of newspaper text and tweets. While SentiStrength has been geared specifically toward short social-media text, SO-CAL was built for general, longer text. After the initial comparison, we successively enrich the SO-CAL-based analysis with tweet-specific mechanisms and observe that in some cases, this improves the performance. A qualitative error analysis then identifies classes of typical problems the two systems have with tweets."
N13-4002,Discourse Processing,2013,-1,-1,1,1,2824,manfred stede,NAACL HLT 2013 Tutorial Abstracts,0,None
varges-etal-2012-semscribe,{S}em{S}cribe: Natural Language Generation for Medical Reports,2012,19,3,3,0,42857,sebastian varges,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Natural language generation in the medical domain is heavily influenced by domain knowledge and genre-specific text characteristics. We present SemScribe, an implemented natural language generation system that produces doctor's letters, in particular descriptions of cardiological findings. Texts in this domain are characterized by a high density of information and a relatively telegraphic style. Domain knowledge is encoded in a medical ontology of about 80,000 concepts. The ontology is used in particular for concept generalizations during referring expression generation. Architecturally, the system is a generation pipeline that uses a corpus-informed syntactic frame approach for realizing sentences appropriate to the domain. The system reads XML documents conforming to the HL7 Clinical Document Architecture (CDA) Standard and enhances them with generated text and references to the used data elements. We conducted a first clinical trial evaluation with medical staff and report on the findings."
J11-2001,Lexicon-Based Methods for Sentiment Analysis,2011,117,1529,5,1,27927,maite taboada,Computational Linguistics,0,"We present a lexicon-based approach to extracting sentiment from text. The Semantic Orientation CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensification and negation. SO-CAL is applied to the polarity classification task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and in completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability."
W09-3909,Genre-Based Paragraph Classification for Sentiment Analysis,2009,29,47,3,1,27927,maite taboada,Proceedings of the {SIGDIAL} 2009 Conference,0,"We present a taxonomy and classification system for distinguishing between different types of paragraphs in movie reviews: formal vs. functional paragraphs and, within the latter, between description and comment. The classification is used for sentiment extraction, achieving improvement over a baseline without paragraph classification."
W09-3005,By all these lovely tokens... Merging Conflicting Tokenizations,2009,35,18,3,0,2108,christian chiarcos,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"Given the contemporary trend to modular NLP architectures and multiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everyday's NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences for the handling of queries on annotated corpora."
W08-2218,Connective-based Local Coherence Analysis: A Lexicon for Recognizing Causal Relationships,2008,22,3,1,1,2824,manfred stede,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"Local coherence analysis is the task of deriving the (most likely) coherence relation holding between two elementary discourse units or, recursively, larger spans of text. The primary source of information for this step is the connectives provided by a language for, more or less explicitly, signaling the relations. Focusing here on causal coherence relations, we propose a lexical resource that holds both lexicographic and corpus-statistic information on German connectives. It can serve as the central repository of information needed for identifying and disambiguating connectives in text, including determining the coherence relations being signaled. We sketch a procedure performing this task, and describe a manually-annotated corpus of causal relations (also in German), which serves as reference data."
W07-1530,Discourse Annotation Working Group Report,2007,0,2,1,1,2824,manfred stede,Proceedings of the Linguistic Annotation Workshop,0,None
2007.sigdial-1.16,Identifying Formal and Functional Zones in Film Reviews,2007,14,10,3,1,42858,heike bieler,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"We describe our system for breaking a film review (as an instance of a semi-structured document) into its formal and functional constituents. Based on a corpus study, we devised a set of 25 zone labels indicating the role that a unit can play within the review. We identify formal zones with a set of symbolic rules, while the distinction between descriptive and evaluative paragraphs is drawn with a statistical classifier. The approach achieves between 70 and 79% precision in recognizing the zones in our corpus."
W04-0607,Feeding {OWL}: Extracting and Representing the Content of Pathology Reports,2004,12,17,2,0,4242,david schlangen,Proceeedings of the Workshop on {NLP} and {XML} ({NLPXML}-2004): {RDF}/{RDFS} and {OWL} in Language Technology,0,"This paper reports on an ongoing project that combines NLP with semantic web technologies to support a content-based storage and retrieval of medical pathology reports. We describe the NLP component of the project (a robust parser) and the background knowledge component (a domain ontology represented in OWL), and how they work together during extraction of domain specific information from natural language reports. The system provides a good example of how NLP techniques can be used to populate the Semantic Web."
W04-0213,The {P}otsdam Commentary Corpus,2004,13,93,1,1,2824,manfred stede,Proceedings of the Workshop on Discourse Annotation,0,"A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation."
C04-1061,Machine-Assisted Rhetorical Structure Annotation,2004,11,12,1,1,2824,manfred stede,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Manually annotating the rhetorical structure of texts is very labour-intensive. At the same time, high-quality automatic analysis is currently out of reach. We thus propose to split the manual annotation in two phases: the simpler marking of lexical connectives and their relations, and the more difficult decisions on overall tree structure. To this end, we developed an environment of two analysis tools and XML-based declarative resources. Our ConAno tool allows for efficient, interactive annotation of connectives, scopes and relations. This intermediate result is exported to O'Donnell's 'RST Tool', which facilitates completing the tree structure."
W03-2411,Step by step: underspecified markup in incremental rhetorical analysis,2003,8,28,2,1,5874,david reitter,Proceedings of 4th International Workshop on Linguistically Interpreted Corpora ({LINC}-03) at {EACL} 2003,0,"While quite a few linguistic corpora with syntactic annotations are available today, resources are scarce on the level of discourse annotation. A flexible, extendible annotation format speeds up development. We therefore propose an XML format for annotating rhetorical structure trees. In human and automatic analysis, rhetorical structure is often difficult and assigned incrementally. Thus, the format allows for underspecification. The paper discusses the various design decisions involved, illustrates the format with an example, and sketches some applications."
W03-0909,Surfaces and depths in text understanding: The case of newspaper commentary,2003,14,2,1,1,2824,manfred stede,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Text Meaning,0,"Using a specific example of a newspaper commentary, the paper explores the relationship between 'surface-oriented' and 'deep' analysis for purposes such as text summarization. The discussion is followed by a description of our ongoing work on automatic commentary understanding and the current state of the implementation."
N03-2011,Rhetorical Parsing with Underspecification and Forests,2003,3,8,3,0,41033,thomas hanneforth,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,We combine a surface based approach to discourse parsing with an explicit rhetorical grammar in order to efficiently construct an underspecified representation of possible discourse structures.
W02-1704,{XML}/{XSL} in the Dictionary: The Case of Discourse Markers,2002,10,5,3,0,53167,daniela berger,COLING-02: The 2nd Workshop on NLP and XML (NLPXML-2002),0,"We describe our ongoing work on an application of XML/XSL technology to a dictionary, from whose source representation various views for the human reader as well as for automatic text generation and understanding are derived. Our case study is a dictionary of discourse markers, the words (often, but not always, conjunctions) that signal the presence of a disocurse relation between adjacent spans of text."
C02-2027,"{P}olibox: Generating Descriptions, Comparisons, and Recommendations from a Database",2002,10,4,1,1,2824,manfred stede,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"We describe our ongoing work on Polibox, a web-based text generator producing adaptive hypertext from a product database, currently one of computational linguistics text-books. When given a specification of a desired ideal book, Polibox selects suitable candidates from the database, and presents them one-by-one to the user. Books are described, compared to one another, and, under the right circumstances, actively recommended. This project note concentrates on the stages of content selection, text planning and sentence planning."
W00-1413,The hyperonym problem revisited: Conceptual and lexical hierarchies in language generation,2000,9,5,1,1,2824,manfred stede,{INLG}{'}2000 Proceedings of the First International Conference on Natural Language Generation,0,"When a lexical item is selected in the language production process, it needs to be explained why none of its superordinates gets selected instead, since their applicability conditions are fulfilled all the same. This question has received much attention in cognitive modelling and not as much in other branches of NLG. This paper describes the various approaches taken, discusses the reasons why they are so different, and argues that production models using symbolic representations should make a distinction between conceptual and lexical hierarchies, which can be organized along fixed levels as studied in (some branches of) lexical semantics."
J00-2008,Book Reviews: Predicative Forms in Natural Language and in Lexical Knowledge Bases,2000,-1,-1,1,1,2824,manfred stede,Computational Linguistics,0,None
W98-1414,Discourse Marker Choice in Sentence Planning,1998,20,24,2,0,55110,brigitte grote,Natural Language Generation,0,None
P98-2202,{D}i{ML}ex: A Lexicon of Discourse Markers for Text Generation and Understanding,1998,15,34,1,1,2824,manfred stede,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"Discourse markers ('cue words') are lexical items that signal the kind of coherence relation holding between adjacent text spans; for example, because, since, and for this reason are different markers for causal relations. Discourse markers are a syntactically quite heterogeneous group of words, many of which are traditionally treated as function words belonging to the realm of grammar rather than to the lexicon. But for a single discourse relation there is often a set of similar markers, allowing for a range of paraphrases for expressing the relation. To capture the similarities and differences between these, and to represent them adequately, we are developing DiMLex, a lexicon of discourse markers. After describing our methodology and the kind of information to be represented in DiMLex, we briefly discuss its potential applications in both text generation and understanding."
J98-3003,A Generative Perspective on Verb Alternations,1998,34,19,1,1,2824,manfred stede,Computational Linguistics,0,"Verb alternations have been researched extensively in linguistics, but they have not yet received a systematic treatment in natural language generation systems; consequently, generators cannot make informed choices among alternatives. As a step towards overcoming this discrepancy, we review some linguistic work on several prominent alternations, revise and extend it, and suggest a set of rules that allow the series of alternated forms to be produced from a single base form of the verb, the lexical entry. The framework has been implemented in the MOOSE sentence generator, which can thus choose a particular verb alternation in order to accomplish generation goals such as placing emphasis on the most important element of the sentence."
C98-2197,{D}i{ML}ex: A lexicon of discourse markers for text generation and understanding,1998,15,34,1,1,2824,manfred stede,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"Discourse markers ('cue words') are lexical items that signal the kind of coherence relation holding between adjacent text spans; for example, because, since, and for this reason are different markers for causal relations. Discourse markers are a syntactically quite heterogeneous group of words, many of which are traditionally treated as function words belonging to the realm of grammar rather than to the lexicon. But for a single discourse relation there is often a set of similar markers, allowing for a range of paraphrases for expressing the relation. To capture the similarities and differences between these, and to represent them adequately, we are developing DiMLex, a lexicon of discourse markers. After describing our methodology and the kind of information to be represented in DiMLex, we briefly discuss its potential applications in both text generation and understanding."
W97-0401,Discourse particles and routine formulas in spoken language translation,1997,-1,-1,1,1,2824,manfred stede,Spoken Language Translation,0,None
W96-0415,A generative perspective on verbs and their readings,1996,10,0,1,1,2824,manfred stede,Eighth International Natural Language Generation Workshop,0,None
C94-1055,Generating Multilingual Documents from a Knowledge Base The {TECHDOC} Project,1994,0,34,2,0,39549,dietmar rosner,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"TECHDOC is an implemented system demonstrating the feasibility of generating multilingual technical documents on the basis of a language-independent knowledge base. Its application domain is user and maintenance instructions, which are produced from underlying plan structures representing the activities, the participating objects with their properties, relations, and so on. This paper gives a brief outline of the system architecture and discusses some recent developments in the project: the addition of actual event simulation in the KB, steps towards a document authoring tool, and a multimodal user interface."
A94-1044,{TECHDOC}: Multilingual generation of online and offline instructional text,1994,3,2,2,0,39549,dietmar rosner,Fourth Conference on Applied Natural Language Processing,0,None
E93-1055,Lexical Choice Criteria in Language Generation,1993,16,21,1,1,2824,manfred stede,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
