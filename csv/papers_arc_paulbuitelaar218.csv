2021.gem-1.13,{NUIG}-{DSI}{'}s submission to The {GEM} Benchmark 2021,2021,-1,-1,3,1,6274,nivranshu pasricha,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",0,"This paper describes the submission by NUIG-DSI to the GEM benchmark 2021. We participate in the modeling shared task where we submit outputs on four datasets for data-to-text generation, namely, DART, WebNLG (en), E2E and CommonGen. We follow an approach similar to the one described in the GEM benchmark paper where we use the pre-trained T5-base model for our submission. We train this model on additional monolingual data where we experiment with different masking strategies specifically focused on masking entities, predicates and concepts as well as a random masking strategy for pre-training. In our results we find that random masking performs the best in terms of automatic evaluation metrics, though the results are not statistically significantly different compared to other masking strategies."
2021.deelio-1.8,Enhancing Multiple-Choice Question Answering with Causal Knowledge,2021,-1,-1,3,0,11251,dhairya dalal,Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,0,"The task of causal question answering aims to reason about causes and effects over a provided real or hypothetical premise. Recent approaches have converged on using transformer-based language models to solve question answering tasks. However, pretrained language models often struggle when external knowledge is not present in the premise or when additional context is required to answer the question. To the best of our knowledge, no prior work has explored the efficacy of augmenting pretrained language models with external causal knowledge for multiple-choice causal question answering. In this paper, we present novel strategies for the representation of causal knowledge. Our empirical results demonstrate the efficacy of augmenting pretrained models with external causal knowledge. We show improved performance on the COPA (Choice of Plausible Alternatives) and WIQA (What If Reasoning Over Procedural Text) benchmark tasks. On the WIQA benchmark, our approach is competitive with the state-of-the-art and exceeds it within the evaluation subcategories of In-Paragraph and Out-of-Paragraph perturbations."
2020.wildre-1.2,A Dataset for Troll Classification of {T}amil{M}emes,2020,-1,-1,6,0,11157,shardul suryawanshi,Proceedings of the WILDRE5{--} 5th Workshop on Indian Language Data: Resources and Evaluation,0,"Social media are interactive platforms that facilitate the creation or sharing of information, ideas or other forms of expression among people. This exchange is not free from offensive, trolling or malicious contents targeting users or communities. One way of trolling is by making memes, which in most cases combines an image with a concept or catchphrase. The challenge of dealing with memes is that they are region-specific and their meaning is often obscured in humour or sarcasm. To facilitate the computational modelling of trolling in the memes for Indian languages, we created a meme dataset for Tamil (TamilMemes). We annotated and released the dataset containing suspected trolls and not-troll memes. In this paper, we use the a image classification to address the difficulties involved in the classification of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score."
2020.webnlg-1.6,Utilising Knowledge Graph Embeddings for Data-to-Text Generation,2020,-1,-1,3,1,6274,nivranshu pasricha,Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+),0,"Data-to-text generation has recently seen a move away from modular and pipeline architectures towards end-to-end architectures based on neural networks. In this work, we employ knowledge graph embeddings and explore their utility for end-to-end approaches in a data-to-text generation task. Our experiments show that using knowledge graph embeddings can yield an improvement of up to 2 {--} 3 BLEU points for seen categories on the WebNLG corpus without modifying the underlying neural network architecture."
2020.webnlg-1.15,{NUIG}-{DSI} at the {W}eb{NLG}+ challenge: Leveraging Transfer Learning for {RDF}-to-text generation,2020,-1,-1,3,1,6274,nivranshu pasricha,Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+),0,"This paper describes the system submitted by NUIG-DSI to the WebNLG+ challenge 2020 in the RDF-to-text generation task for the English language. For this challenge, we leverage transfer learning by adopting the T5 model architecture for our submission and fine-tune the model on the WebNLG+ corpus. Our submission ranks among the top five systems for most of the automatic evaluation metrics achieving a BLEU score of 51.74 over all categories with scores of 58.23 and 45.57 across seen and unseen categories respectively."
2020.trac-1.6,Multimodal Meme Dataset ({M}ulti{OFF}) for Identifying Offensive Content in Image and Text,2020,-1,-1,4,0,11157,shardul suryawanshi,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying",0,"A meme is a form of media that spreads an idea or emotion across the internet. As posting meme has become a new form of communication of the web, due to the multimodal nature of memes, postings of hateful memes or related events like trolling, cyberbullying are increasing day by day. Hate speech, offensive content and aggression content detection have been extensively explored in a single modality such as text or image. However, combining two modalities to detect offensive content is still a developing area. Memes make it even more challenging since they express humour and sarcasm in an implicit way, because of which the meme may not be offensive if we only consider the text or the image. Therefore, it is necessary to combine both modalities to identify whether a given meme is offensive or not. Since there was no publicly available dataset for multimodal offensive meme content detection, we leveraged the memes related to the 2016 U.S. presidential election and created the MultiOFF multimodal meme dataset for offensive content detection dataset. We subsequently developed a classifier for this task using the MultiOFF dataset. We use an early fusion technique to combine the image and text modality and compare it with a text- and an image-only baseline to investigate its effectiveness. Our results show improvements in terms of Precision, Recall, and F-Score. The code and dataset for this paper is published in \textit{ \url{https://github.com/bharathichezhiyan/Multimodal-Meme-Classification-Identifying-Offensive-Content-in-Image-and-Text} }"
2020.semeval-1.208,{NUIG} at {S}em{E}val-2020 Task 12: Pseudo Labelling for Offensive Content Classification,2020,-1,-1,3,0,11157,shardul suryawanshi,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This work addresses the classification problem defined by sub-task A (English only) of the OffensEval 2020 challenge. We used a semi-supervised approach to classify given tweets into an offensive (OFF) or not-offensive (NOT) class. As the OffensEval 2020 dataset is loosely labelled with confidence scores given by unsupervised models, we used last year{'}s offensive language identification dataset (OLID) to label the OffensEval 2020 dataset. Our approach uses a pseudo-labelling method to annotate the current dataset. We trained four text classifiers on the OLID dataset and the classifier with the highest macro-averaged F1-score has been used to pseudo label the OffensEval 2020 dataset. The same model which performed best amongst four text classifiers on OLID dataset has been trained on the combined dataset of OLID and pseudo labelled OffensEval 2020. We evaluated the classifiers with precision, recall and macro-averaged F1-score as the primary evaluation metric on the OLID and OffensEval 2020 datasets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: \url{http://creativecommons.org/licenses/by/4.0/}."
2020.lrec-1.254,A Term Extraction Approach to Survey Analysis in Health Care,2020,-1,-1,5,0,17157,cecile robin,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The voice of the customer has for a long time been a key focus of businesses in all domains. It has received a lot of attention from the research community in Natural Language Processing (NLP) resulting in many approaches to analyzing customers feedback ((aspect-based) sentiment analysis, topic modeling, etc.). In the health domain, public and private bodies are increasingly prioritizing patient engagement for assessing the quality of the service given at each stage of the care. Patient and customer satisfaction analysis relate in many ways. In the domain of health particularly, a more precise and insightful analysis is needed to help practitioners locate potential issues and plan actions accordingly. We introduce here an approach to patient experience with the analysis of free text questions from the 2017 Irish National Inpatient Survey campaign using term extraction as a means to highlight important and insightful subject matters raised by patients. We evaluate the results by mapping them to a manually constructed framework following the Activity, Resource, Context (ARC) methodology (Ordenes, 2014) and specific to the health care environment, and compare our results against manual annotations done on the full 2017 dataset based on those categories."
2020.lrec-1.285,Evaluation Dataset and Methodology for Extracting Application-Specific Taxonomies from the {W}ikipedia Knowledge Graph,2020,-1,-1,4,1,17236,georgeta bordea,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this work, we address the task of extracting application-specific taxonomies from the category hierarchy of Wikipedia. Previous work on pruning the Wikipedia knowledge graph relied on silver standard taxonomies which can only be automatically extracted for a small subset of domains rooted in relatively focused nodes, placed at an intermediate level in the knowledge graphs. In this work, we propose an iterative methodology to extract an application-specific gold standard dataset from a knowledge graph and an evaluation framework to comparatively assess the quality of noisy automatically extracted taxonomies. We employ an existing state of the art algorithm in an iterative manner and we propose several sampling strategies to reduce the amount of manual work needed for evaluation. A first gold standard dataset is released to the research community for this task along with a companion evaluation framework. This dataset addresses a real-world application from the medical domain, namely the extraction of food-drug and herb-drug interactions."
2020.lrec-1.712,Figure Me Out: A Gold Standard Dataset for Metaphor Interpretation,2020,-1,-1,3,1,18055,omnia zayed,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Metaphor comprehension and understanding is a complex cognitive task that requires interpreting metaphors by grasping the interaction between the meaning of their target and source concepts. This is very challenging for humans, let alone computers. Thus, automatic metaphor interpretation is understudied in part due to the lack of publicly available datasets. The creation and manual annotation of such datasets is a demanding task which requires huge cognitive effort and time. Moreover, there will always be a question of accuracy and consistency of the annotated data due to the subjective nature of the problem. This work addresses these issues by presenting an annotation scheme to interpret verb-noun metaphoric expressions in text. The proposed approach is designed with the goal of reducing the workload on annotators and maintain consistency. Our methodology employs an automatic retrieval approach which utilises external lexical resources, word embeddings and semantic similarity to generate possible interpretations of identified metaphors in order to enable quick and accurate annotation. We validate our proposed approach by annotating around 1,500 metaphors in tweets which were annotated by six native English speakers. As a result of this work, we publish as linked data the first gold standard dataset for metaphor interpretation which will facilitate research in this area."
2020.findings-emnlp.36,Contextual Modulation for Relation-Level Metaphor Identification,2020,-1,-1,3,1,18055,omnia zayed,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Identifying metaphors in text is very challenging and requires comprehending the underlying comparison. The automation of this cognitive process has gained wide attention lately. However, the majority of existing approaches concentrate on word-level identification by treating the task as either single-word classification or sequential labelling without explicitly modelling the interaction between the metaphor components. On the other hand, while existing relation-level approaches implicitly model this interaction, they ignore the context where the metaphor occurs. In this work, we address these limitations by introducing a novel architecture for identifying relation-level metaphoric expressions of certain grammatical relations based on contextual modulation. In a methodology inspired by works in visual reasoning, our approach is based on conditioning the neural network computation on the deep contextualised features of the candidate expressions using feature-wise linear modulation. We demonstrate that the proposed architecture achieves state-of-the-art results on benchmark datasets. The proposed methodology is generic and could be applied to other textual classification problems that benefit from contextual interaction."
2020.figlang-1.22,Adaptation of Word-Level Benchmark Datasets for Relation-Level Metaphor Identification,2020,-1,-1,3,1,18055,omnia zayed,Proceedings of the Second Workshop on Figurative Language Processing,0,"Metaphor processing and understanding has attracted the attention of many researchers recently with an increasing number of computational approaches. A common factor among these approaches is utilising existing benchmark datasets for evaluation and comparisons. The availability, quality and size of the annotated data are among the main difficulties facing the growing research area of metaphor processing. The majority of current approaches pertaining to metaphor processing concentrate on word-level processing due to data availability. On the other hand, approaches that process metaphors on the relation-level ignore the context where the metaphoric expression. This is due to the nature and format of the available data. Word-level annotation is poorly grounded theoretically and is harder to use in downstream tasks such as metaphor interpretation. The conversion from word-level to relation-level annotation is non-trivial. In this work, we attempt to fill this research gap by adapting three benchmark datasets, namely the VU Amsterdam metaphor corpus, the TroFi dataset and the TSV dataset, to suit relation-level metaphor identification. We publish the adapted datasets to facilitate future research in relation-level metaphor processing."
S19-2151,{S}em{E}val-2019 Task 9: Suggestion Mining from Online Reviews and Forums,2019,0,6,3,1,25119,sapna negi,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"We present the pilot SemEval task on Suggestion Mining. The task consists of subtasks A and B, where we created labeled data from feedback forum and hotel reviews respectively. Subtask A provides training and test data from the same domain, while Subtask B evaluates the system on a test dataset from a different domain than the available training data. 33 teams participated in the shared task, with a total of 50 members. We summarize the problem definition, benchmark dataset preparation, and methods used by the participating teams, providing details of the methods used by the top ranked systems. The dataset is made freely available to help advance the research in suggestion mining, and reproduce the systems submitted under this task"
W18-6216,Linking News Sentiment to Microblogs: A Distributional Semantics Approach to Enhance Microblog Sentiment Classification,2018,0,1,2,0.757576,18333,tobias daudert,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Social media{'}s popularity in society and research is gaining momentum and simultaneously increasing the importance of short textual content such as microblogs. Microblogs are affected by many factors including the news media, therefore, we exploit sentiments conveyed from news to detect and classify sentiment in microblogs. Given that texts can deal with the same entity but might not be vastly related when it comes to sentiment, it becomes necessary to introduce further measures ensuring the relatedness of texts while leveraging the contained sentiments. This paper describes ongoing research introducing distributional semantics to improve the exploitation of news-contained sentiment to enhance microblog sentiment classification."
W18-3107,Leveraging News Sentiment to Improve Microblog Sentiment Classification in the Financial Domain,2018,0,1,2,0.757576,18333,tobias daudert,Proceedings of the First Workshop on Economics and Natural Language Processing,0,"With the rising popularity of social media in the society and in research, analysing texts short in length, such as microblogs, becomes an increasingly important task. As a medium of communication, microblogs carry peoples sentiments and express them to the public. Given that sentiments are driven by multiple factors including the news media, the question arises if the sentiment expressed in news and the news article themselves can be leveraged to detect and classify sentiment in microblogs. Prior research has highlighted the impact of sentiments and opinions on the market dynamics, making the financial domain a prime case study for this approach. Therefore, this paper describes ongoing research dealing with the exploitation of news contained sentiment to improve microblog sentiment classification in a financial context."
W18-0910,Phrase-Level Metaphor Identification Using Distributed Representations of Word Meaning,2018,0,1,3,1,18055,omnia zayed,Proceedings of the Workshop on Figurative Language Processing,0,"Metaphor is an essential element of human cognition which is often used to express ideas and emotions that might be difficult to express using literal language. Processing metaphoric language is a challenging task for a wide range of applications ranging from text simplification to psychotherapy. Despite the variety of approaches that are trying to process metaphor, there is still a need for better models that mimic the human cognition while exploiting fewer resources. In this paper, we present an approach based on distributional semantics to identify metaphors on the phrase-level. We investigated the use of different word embeddings models to identify verb-noun pairs where the verb is used metaphorically. Several experiments are conducted to show the performance of the proposed approach on benchmark datasets."
L18-1149,Automatic Enrichment of Terminological Resources: the {IATE} {RDF} Example,2018,0,0,4,1,6275,mihael arcan,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This publication is supported by a research grant from Science Foundation Ireland, SFI/12/RC/2289 (Insight), a research visit grant from Universidad Politecnica de Madrid, xc2xb4n by the Spanish Datos4.0 project (TIN2016-78011-C4-4-R)n and by the EUxe2x80x99s Lynx project (H2020 Research and Innovation Programme under GA num 780602)."
L18-1192,A Comparison Of Emotion Annotation Schemes And A New Annotated Data Set,2018,0,4,4,0,4046,ian wood,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,We would like to thank volunteers from the Insight Centren for Data Analytics for their efforts in pilot study annotations.n This work was supported in part by the Science Foundationn Ireland under Grant Number 16/IFB/4336 and Grantn Number SFI/12/RC/2289 (Insight). The research leadingn to these results has received funding from the Europeann Unionxe2x80x99s Horizon 2020 research and innovation programmen under grant agreements No. 644632 (MixedEmotions).
L18-1324,A supervised approach to taxonomy extraction using word embeddings,2018,0,1,3,0,3060,rajdeep sarkar,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1383,{T}eanga: A Linked Data based platform for Natural Language Processing,2018,0,3,3,0,29939,housam ziad,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
S16-2022,A Study of Suggestions in Opinionated Texts and their Automatic Detection,2016,11,6,4,1,25119,sapna negi,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,None
S16-1110,{NUIG}-{UNLP} at {S}em{E}val-2016 Task 1: Soft Alignment and Deep Learning for Semantic Textual Similarity,2016,10,0,4,0.78234,1255,john mccrae,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
S16-1168,{S}em{E}val-2016 Task 13: Taxonomy Extraction Evaluation ({TE}x{E}val-2),2016,24,33,3,1,17236,georgeta bordea,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes the second edition of the shared task on Taxonomy Extraction Evaluation organised as part of SemEval 2016. This task aims to extract hypernym-hyponym relations between a given list of domain-specific terms and then to construct a domain taxonomy based on them. TExEval-2 introduced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science. A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by comparison with gold standard taxonomies and by manual quality assessment of novel relations."
L16-1066,Forecasting Emerging Trends from Scientific Literature,2016,0,3,4,1,34186,kartik asooja,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Text analysis methods for the automatic identification of emerging technologies by analyzing the scientific publications, are gaining attention because of their socio-economic impact. The approaches so far have been mainly focused on retrospective analysis by mapping scientific topic evolution over time. We propose regression based approaches to predict future keyword distribution. The prediction is based on historical data of the keywords, which in our case, are LREC conference proceedings. Considering the insufficient number of data points available from LREC proceedings, we do not employ standard time series forecasting methods. We form a dataset by extracting the keywords from previous year proceedings and quantify their yearly relevance using tf-idf scores. This dataset additionally contains ranked lists of related keywords and experts for each keyword."
L16-1090,{IRIS}: {E}nglish-{I}rish Machine Translation System,2016,19,0,4,1,6275,mihael arcan,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We describe IRIS, a statistical machine translation (SMT) system for translating from English into Irish and vice versa. Since Irish is considered an under-resourced language with a limited amount of machine-readable text, building a machine translation system that produces reasonable translations is rather challenging. As translation is a difficult task, current research in SMT focuses on obtaining statistics either from a large amount of parallel, monolingual or other multilingual resources. Nevertheless, we collected available English-Irish data and developed an SMT system aimed at supporting human translators and enabling cross-lingual language technology tasks."
L16-1385,Generating a Large-Scale Entity Linking Dictionary from {W}ikipedia Link Structure and Article Text,2016,6,0,2,0,35114,ravindra harige,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Wikipedia has been increasingly used as a knowledge base for open-domain Named Entity Linking and Disambiguation. In this task, a dictionary with entity surface forms plays an important role in finding a set of candidate entities for the mentions in text. Existing dictionaries mostly rely on the Wikipedia link structure, like anchor texts, redirect links and disambiguation links. In this paper, we introduce a dictionary for Entity Linking that includes name variations extracted from Wikipedia article text, in addition to name variations derived from the Wikipedia link structure. With this approach, we show an increase in the coverage of entities and their mentions in the dictionary in comparison to other Wikipedia based dictionaries."
C16-1010,Expanding wordnets to new languages with multilingual sense disambiguation,2016,33,5,3,1,6275,mihael arcan,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Princeton WordNet is one of the most important resources for natural language processing, but is only available for English. While it has been translated using the expand approach to many other languages, this is an expensive manual process. Therefore it would be beneficial to have a high-quality automatic translation approach that would support NLP techniques, which rely on WordNet in new languages. The translation of wordnets is fundamentally complex because of the need to translate all senses of a word including low frequency senses, which is very challenging for current machine translation approaches. For this reason we leverage existing translations of WordNet in other languages to identify contextual information for wordnet senses from a large set of generic parallel corpora. We evaluate our approach using 10 translated wordnets for European languages. Our experiment shows a significant improvement over translation without any contextual information. Furthermore, we evaluate how the choice of pivot languages affects performance of multilingual word sense disambiguation."
W15-4929,{M}ixed{E}motions: Social Semantic Emotion Analysis for Innovative Multilingual Big Data Analytics Markets,2015,-1,-1,2,1,6275,mihael arcan,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W15-4205,Reconciling Heterogeneous Descriptions of Language Resources,2015,19,3,10,0.78234,1255,john mccrae,Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications,0,"Language resources are a cornerstone of linguistic research and for the development of natural language processing tools, but the discovery of relevant resources remains a challenging task. This is due to the fact that relevant metadata records are spread among different repositories and it is currently impossible to query all these repositories in an integrated fashion, as they use different data models and vocabularies. In this paper we present a first attempt to collect and harmonize the metadata of different repositories, thus making them queriable and browsable in an integrated way. We make use of RDF and linked data technologies for this and provide a first level of harmonization of the vocabularies used in the different resources by mapping them to standard RDF vocabularies including Dublin Core and DCAT. Further, we present an approach that relies on NLP and in particular word sense disambiguation techniques to harmonize resources by mapping values of attributes xe2x80x90 such as the type, license or intended use of a resource xe2x80x90 into normalized values. Finally, as there are duplicate entries within the same repository as well as across different repositories, we also report results of detection of these duplicates."
W15-0115,Curse or Boon? Presence of Subjunctive Mood in Opinionated Text,2015,-1,-1,2,1,25119,sapna negi,Proceedings of the 11th International Conference on Computational Semantics,0,None
S15-2151,{S}em{E}val-2015 Task 17: Taxonomy Extraction Evaluation ({TE}x{E}val),2015,20,43,2,1,17236,georgeta bordea,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the first shared task on Taxonomy Extraction Evaluation organised as part of SemEval-2015. Participants were asked to find hypernym-hyponym relations between given terms. For each of the four selected target domains the participants were provided with two lists of domainspecific terms: a WordNet collection of terms and a well-known terminology extracted from an online publicly available taxonomy. A total of 45 taxonomies submitted by 6 participating teams were evaluated using standard structural measures, the structural similarity with a gold standard taxonomy, and through manual quality assessment of sampled novel relations."
S15-1010,Non-Orthogonal Explicit Semantic Analysis,2015,27,6,4,1,34283,nitish aggarwal,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"Explicit Semantic Analysis (ESA) utilizes the Wikipedia knowledge base to represent the semantics of a word by a vector where every dimension refers to an explicitly defined concept like a Wikipedia article. ESA inherently assumes that Wikipedia concepts are orthogonal to each other, therefore, it considers that two words are related only if they co-occur in the same articles. However, two words can be related to each other even if they appear separately in related articles rather than cooccurring in the same articles. This leads to a need for extending the ESA model to consider the relatedness between the explicit concepts (i.e. Wikipedia articles in Wikipedia based implementation) for computing textual relatedness. In this paper, we present NonOrthogonal ESA (NESA) which represents more fine grained semantics of a word as a vector of explicit concept dimensions, where every such concept dimension further constitutes a semantic vector built in another vector space. Thus, NESA considers the concept correlations in computing the relatedness between two words. We explore different approaches to compute the concept correlation weights, and compare these approaches with other existing methods. Furthermore, we evaluate our model NESA on several word relatedness benchmarks showing that it outperforms the state of the art methods."
P15-1069,Knowledge Portability with Semantic Expansion of Ontology Labels,2015,44,13,3,1,6275,mihael arcan,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,This publication has emanated from research supported in part by a research grant from Sciencen Foundation Ireland (SFI) under Grant Numbern SFI/12/RC/2289 (Insight) and the European Unionn supported projects LIDER (ICT-2013.4.1-610782)n and MixedEmotions (H2020-644632).
D15-1258,Towards the Extraction of Customer-to-Customer Suggestions from Reviews,2015,16,8,2,1,25119,sapna negi,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"State of the art in opinion mining mainly focuses on positive and negative sentiment summarisation of online customer reviews. We observe that reviewers tend to provide advice, recommendations and tips to the fellow customers on a variety of points of interest. In this work, we target the automatic detection of suggestion expressing sentences in customer reviews. This is a novel problem, and therefore to begin with, requires a well formed problem definition and benchmark dataset. This work provides a 3- fold contribution, namely, problem definition, benchmark dataset, and an approach for detection of suggestions for the customers. The problem is framed as a sentence classification problem, and a set of linguistically motivated features are proposed. Analysis of the nature of suggestions, and classification errors, highlight challenges and research opportunities associated with this problem."
2015.eamt-1.30,{M}ixed{E}motions: Social Semantic Emotion Analysis for Innovative Multilingual Big Data Analytics Markets,2015,-1,-1,2,1,6275,mihael arcan,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
W14-4803,Identification of Bilingual Terms from Monolingual Documents for Statistical Machine Translation,2014,29,10,4,1,6275,mihael arcan,Proceedings of the 4th International Workshop on Computational Terminology (Computerm),0,"This publication has emanated from research supported in part by a research grant from Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289 and by the European Union supported projectsn EuroSentiment (Grant No. 296277), LIDER (Grant No. 610782) and MateCat (ICT-2011.4.2-287688)."
W14-4508,Using Distributional Semantics to Trace Influence and Imitation in Romantic Orientalist Poetry,2014,-1,-1,3,1,34283,nitish aggarwal,Proceedings of the First {AHA}!-Workshop on Information Discovery in Text,0,None
S14-2058,{INSIGHT} {G}alway: Syntactic and Lexical Features for Aspect Based Sentiment Analysis,2014,9,2,2,1,25119,sapna negi,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This work analyses various syntactic and lexical features for sentence level aspect based sentiment analysis. The task focuses on detection of a writerxe2x80x99s sentiment towards an aspect which is explicitly mentioned in a sentence. The target sentiment polarities are positive, negative, conflict and neutral. We use a supervised learning approach, evaluate various features and report accuracies which are much higher than the provided baselines. Best features include unigrams, clauses, dependency relations and SentiWordNet polarity scores."
S14-1006,Exploring {ESA} to Improve Word Relatedness,2014,14,4,3,1,34283,nitish aggarwal,Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014),0,"Explicit Semantic Analysis (ESA) is an approach to calculate the semantic relatedness between two words or natural language texts with the help of concepts grounded in human cognition. ESA usage has received much attention in the field of natural language processing, information retrieval and text analysis, however, performance of the approach depends on several parameters that are included in the model, and also on the text data type used for evaluation. In this paper, we investigate the behavior of using different number of Wikipedia articles in building ESA model, for calculating the semantic relatedness for different types of text pairs: word-word, phrasephrase and document-document. With our findings, we further propose an approach to improve the ESA semantic relatedness scores for words by enriching the words with their explicit context such as synonyms, glosses and Wikipedia definitions."
wolff-etal-2014-missed,Missed opportunities in translation memory matching,2014,9,1,3,0,39307,friedel wolff,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"A translation memory system stores a data set of source-target pairs of translations. It attempts to respond to a query in the source language with a useful target text from the data set to assist a human translator. Such systems estimate the usefulness of a target text suggestion according to the similarity of its associated source text to the source text query. This study analyses two data sets in two language pairs each to find highly similar target texts, which would be useful mutual suggestions. We further investigate which of these useful suggestions can not be selected through source text similarity, and we do a thorough analysis of these cases to categorise and quantify them. This analysis provides insight into areas where the recall of translation memory systems can be improved. Specifically, source texts with an omission, and semantically very similar source texts are some of the more frequent cases with useful target text suggestions that are not selected with the baseline approach of simple edit distance between the source texts."
buitelaar-etal-2014-hot,Hot Topics and Schisms in {NLP}: Community and Trend Analysis with Saffron on {ACL} and {LREC} Proceedings,2014,15,2,1,1,6276,paul buitelaar,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we present a comparative analysis of two series of conferences in the field of Computational Linguistics, the LREC conference and the ACL conference. Conference proceedings were analysed using Saffron by performing term extraction and topical hierarchy construction with the goal of analysing topic trends and research communities. The system aims to provide insight into a research community and to guide publication and participation strategies, especially of novice researchers."
2014.amta-researchers.5,Enhancing statistical machine translation with bilingual terminology in a {CAT} environment,2014,40,14,4,1,6275,mihael arcan,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"In this paper, we address the problem of extracting and integrating bilingual terminology into a Statistical Machine Translation (SMT) system for a Computer Aided Translation (CAT) tool scenario. We develop a framework that, taking as input a small amount of parallel in-domain data, gathers domain-specific bilingual terms and injects them in an SMT system to enhance the translation productivity. Therefore, we investigate several strategies to extract and align bilingual terminology, and to embed it into the SMT. We compare two embedding methods that can be easily used at run-time without altering the normal activity of an SMT system: XML markup and the cache-based model. We tested our framework on two different domains showing improvements up to 15{\%} BLEU score points."
W13-5502,Linguistic Linked Data for Sentiment Analysis,2013,11,10,1,1,6276,paul buitelaar,"Proceedings of the 2nd Workshop on Linked Data in Linguistics ({LDL}-2013): Representing and linking lexicons, terminologies and other language data",0,"In this paper we describe the specification of amodel for the semantically interoperable representation of language resources for sentiment analysis. The model integrates lemon, an RDF-based model for the specification of ontology-lexica (Buitelaar et al. 2009), which is used increasinglyfor the representation of language resources asLinked Data, with Marl, an RDF-based model for the representation of sentiment annotations (West-erski et al., 2011; Sanchez-Rada et al., 2013)"
N13-2006,Ontology Label Translation,2013,20,4,2,1,6275,mihael arcan,Proceedings of the 2013 {NAACL} {HLT} Student Research Workshop,0,"Our research investigates the translation of ontology labels, which has applications in multilingual knowledge access. Ontologies are often defined only in one language, mostly English. To enable knowledge access across languages, such monolingual ontologies need to be translated into other languages. The primary challenge in ontology label translation is the lack of context, which makes this task rather different than document translation. The core objective therefore, is to provide statistical machine translation (SMT) systems with additional context information. In our approach, we first extend standard SMT by enhancing a translation model with context information that keeps track of surrounding words for each translation. We compute a semantic similarity between the phrase pair context vector from the parallel corpus and a vector of noun phrases that occur in surrounding ontology labels. We applied our approach to the translation of a financial ontology, translating from English to German, using Europarl as parallel corpus. This experiment showed that our approach can provide a slight improvement over standard SMT for this task, without exploiting any additional domain-specific resources."
2013.mtsummit-posters.1,Translating the {FINREP} Taxonomy using a Domain-specific Corpus,2013,-1,-1,4,1,6275,mihael arcan,Proceedings of Machine Translation Summit XIV: Posters,0,None
2013.mtsummit-european.13,{MONNET}: Multilingual Ontologies for Networked Knowledge,2013,-1,-1,2,1,6275,mihael arcan,Proceedings of Machine Translation Summit XIV: European projects,0,None
W12-4210,Using Domain-specific and Collaborative Resources for Term Translation,2012,23,2,3,1,6275,mihael arcan,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In this article we investigate the translation of terms from English into German and vice versa in the isolation of an ontology vocabulary. For this study we built new domain-specific resources from the translation search engine Linguee and from the online encyclopedia Wikipedia. We learned that a domain-specific resource produces better results than a bigger, but more general one. The first finding of our research is that the vocabulary and the structure of the parallel corpus are important. By integrating the multilingual knowledge base Wikipedia, we further improved the translation wrt. the domain-specific resources, whereby some translation evaluation metrics outperformed the results of Google Translate. This finding leads us to the conclusion that a hybrid translation system, a combination of bilingual terminological resources and statistical machine translation can help to improve translation of domain-specific terms."
S12-1095,{DERI}{\\&}{UPM}: Pushing Corpus Based Relatedness to Similarity: Shared Task System Description,2012,9,16,3,1,34283,nitish aggarwal,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"In this paper, we describe our system submitted for the semantic textual similarity (STS) task at SemEval 2012. We implemented two approaches to calculate the degree of similarity between two sentences. First approach combines corpus-based semantic relatedness measure over the whole sentence with the knowledge-based semantic similarity scores obtained for the words falling under the same syntactic roles in both the sentences. We fed all these scores as features to machine learning models to obtain a single score giving the degree of similarity of the sentences. Linear Regression and Bagging models were used for this purpose. We used Explicit Semantic Analysis (ESA) as the corpus-based semantic relatedness measure. For the knowledge-based semantic similarity between words, a modified WordNet based Lin measure was used. Second approach uses a bipartite based method over the WordNet based Lin measure, without any modification. This paper shows a significant improvement in calculating the semantic similarity between sentences by the fusion of the knowledge-based similarity measure and the corpus-based relatedness measure against corpus based measure taken alone."
qasemizadeh-etal-2012-semi,Semi-Supervised Technical Term Tagging With Minimal User Feedback,2012,20,3,2,0,18354,behrang qasemizadeh,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we address the problem of extracting technical terms automatically from an unannotated corpus. We introduce a technology term tagger that is based on Liblinear Support Vector Machines and employs linguistic features including Part of Speech tags and Dependency Structures, in addition to user feedback to perform the task of identification of technology related terms. Our experiments show the applicability of our approach as witnessed by acceptable results on precision and recall."
bordea-etal-2012-expertise,Expertise Mining for Enterprise Content Management,2012,13,6,3,1,17236,georgeta bordea,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Enterprise content analysis and platform configuration for enterprise content management is often carried out by external consultants that are not necessarily domain experts. In this paper, we propose a set of methods for automatic content analysis that allow users to gain a high level view of the enterprise content. Here, a main concern is the automatic identification of key stakeholders that should ideally be involved in analysis interviews. The proposed approach employs recent advances in term extraction, semantic term grounding, expert profiling and expert finding in an enterprise content management setting. Extracted terms are evaluated using human judges, while term grounding is evaluated using a manually created gold standard for the DBpedia datasource."
C12-1005,Experiments with Term Translation,2012,24,4,3,1,6275,mihael arcan,Proceedings of {COLING} 2012,0,"In this article we investigate the translation of financial terms from English into German in the isolation of an ontology vocabulary. For this study we automatically built new domain-specific resources from the translation search engine Linguee and from the online encyclopaedia Wikipedia. Due to the fact that we performed the translation approach on a monolingual ontology, we ran several sub-experiments to find the most appropriate model to translate the financial vocabulary. The findings from these experiments lead to the conclusion that a hybrid translation system, a combination of bilingual terminological resources and statistical machine translation, can help to improve translation of domain-specific terms. Finally we undertook a manual cross-lingual evaluation on the monolingual ontology to get a better understanding on this specific short text translation task."
S10-1030,{DERIUNLP}: A Context Based Approach to Automatic Keyphrase Extraction,2010,10,12,2,1,17236,georgeta bordea,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"The DERI UNLP team participated in the SemEval 2010 Task #5 with an unsupervised system that automatically extracts keyphrases from scientific articles. Our approach does not only consider a general description of a term to select keyphrase candidates but also context information in the form of skill types. Even though our system analyses only a limited set of candidates, it is still able to outperform baseline unsupervised and supervised approaches."
W09-4508,Deriving Clinical Query Patterns from Medical Corpora Using Domain Ontologies,2009,13,1,2,0,46777,pinar wennerberg,Proceedings of the Workshop on Biomedical Information Extraction,0,"For an effective search and management of large amounts of medical image and patient data, it is relevant to know the kind of information the clinicians and radiologists seek for. This information is typically represented in their queries when searching for text and medical images about patients. Statistical clinical query pattern derivation described in this paper is an approach to obtain this information semi-automatically. It is based on predicting clinical query patterns given medical ontologies, domain corpora and statistical analysis. The patterns identified in this way are then compared to a corpus of clinical questions to identify possible overlaps between them and the actual questions. Additionally, they are discussed with the clinical experts. We describe our ontology driven clinical query pattern derivation approach, the comparison results with the clinical questions corpus and the evaluation by the radiology experts."
W09-3742,Identifying the Epistemic Value of Discourse Segments in Biology Texts (project abstract),2009,3,14,2,0,2136,anita waard,Proceedings of the Eight International Conference on Computational Semantics,0,"To manage the flood of information that threatens to engulf (life-)scientists, an abundance of computer-aided tools are being developed. These tools aim to provide access to the knowledge conveyed within a collection of research papers, without actually having to read the papers. Many of these tools focus on text mining, by looking for specific named-entities that have scientific meaning, and relationships between these. An overview of the current state of the art is given in Rebholz-Schuhmann et al. (2005) and Couto et al. (2003). Typically, these tools identify a list of sentences containing relationships between two specific named-entities that can be found using rules or a thesaurus of synonyms. These sentences represent an overview of the interactions that are known with a specific entity, thus precluding the need for an exhaustive literature study. For example, the following are a few sentences that have been found using a typical text mining tool for the relationship 'p53 activates*':n n 1. The p53 tumor suppressor protein exerts most of its anti-tumorigenic activity by transcriptionally activating several pro-apoptotic genes.n n 2. We found that p53 ... activates[,] the promoter of the myosin VI gene."
W08-0625,Statistical Term Profiling for Query Pattern Mining,2008,-1,-1,1,1,6276,paul buitelaar,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,None
buitelaar-eigner-2008-ontology,Ontology Search with the {O}nto{S}elect Ontology Library,2008,5,3,1,1,6276,paul buitelaar,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"OntoSelect is a dynamic web-based ontology library that harvests, analyzes and organizes ontologies published on the Semantic Web. OntoSelect allows searching as well as browsing of ontologies according to size (number of classes, properties), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies) and human languages used for class- and object property-labels. Ontology search in OntoSelect is based on a combined measure of coverage, structure and connectedness. Further, and in contrast to other ontology search engines, OntoSelect provides ontology search based on a complete web document instead of one or more keywords only."
miguel-buitelaar-2008-domain,Domain-Specific {E}nglish-To-{S}panish Translation of {F}rame{N}et,2008,3,8,2,0,48492,mario miguel,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper is motivated by the demand for more linguistic resources for the study of languages and the improvement of those already existing. The first step in our work is the selection of the most significant frames in the English FrameNet according to a representative medical corpus. These frames were subsequently attached to different EuroWordNet synsets and translated into Spanish. Results show how the translation was made with high accuracy (95.9 {\%} of correct words). In addition to that, the original English lexical units were augmented with new units by 120{\%}"
buitelaar-etal-2006-ontology,Ontology-based Information Extraction with {SOBA},2006,11,72,1,1,6276,paul buitelaar,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper we describe SOBA, a sub-component of the SmartWeb multi-modal dialog system. SOBA is a component for ontologybased information extraction from soccer web pages for automatic population of a knowledge base that can be used for domainspecific question answering. SOBA realizes a tight connection between the ontology, knowledge base and the information extraction component. The originality of SOBA is in the fact that it extracts information from heterogeneous sources such as tabular structures, text and image captions in a semantically integrated way. In particular, it stores extracted information in a knowledge base, and in turn uses the knowledge base to interpret and link newly extracted information with respect to already existing entities."
E06-2010,Generating and Visualizing a Soccer Knowledge Base,2006,10,13,1,1,6276,paul buitelaar,Demonstrations,0,"This demo abstract describes the SmartWeb Ontology-based Annotation system (SOBA). A key feature of SOBA is that all information is extracted and stored with respect to the SmartWeb Integrated Ontology (SWIntO). In this way, other components of the systems, which use the same ontology, can access this information in a straightforward way. We will show how information extracted by SOBA is visualized within its original context, thus enhancing the browsing experience of the end user."
W04-0602,Towards Metadata Interoperability,2004,2,4,3,0,39140,peter wittenburg,Proceeedings of the Workshop on {NLP} and {XML} ({NLPXML}-2004): {RDF}/{RDFS} and {OWL} in Language Technology,0,"Within two European projects metadata interoperability is one of the central topics. While the INTERA project has as one of its goals to achieve an interoperability between two widely used metadata sets for the domain of language resources, the ECHO project created an integrated metadata domain of in total nine data providers from five different disciplines from the humanities. In both projects ad hoc techniques are used to achieve results. In the INTERA project, however, machine readable and ISO compliant concept definitions are created as a first step towards the Semantic Web. In the ECHO project a complex ontology was realized purely relying on XML. It is argued that concept definitions should be registered in open Data Category Repositories and that relations between them should be described as RDF assertions. Yet we are missing standards that would allow us to overcome the ad hoc solutions."
kasper-etal-2004-integrated,Integrated Language Technologies for Multilingual Information Services in the {MEMPHIS} Project,2004,8,7,4,0,36666,walter kasper,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The MEMPHIS project integrates a large set of NLP technologies. An overview of components, their underlying technologies and resources will be presented: language identification, document classification, linguistic analysis, summarization, information extraction, machine translation, knowledge management and crosslingual retrieval."
declerck-etal-2004-towards,Towards a Language Infrastructure for the Semantic Web,2004,8,8,2,0,2109,thierry declerck,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In recent years, the Internet evolved from a global medium for information exchange (directed mainly towards human users) into a xe2x80x9dglobal, virtual work environmentxe2x80x9d (for both human users and machines). Building on the world-wide-web, developments such as grid technology, web services and the semantic web contributed to this transformation, the implications of which are now slowly but clearly being integrated into all areas of the new digital society (e-business, e-government, e-science, etc.) In this conctext the semantic web allows for increasingly intelligent and therefore autonomous processing. This development brings new challenges for Human Language Technology (HLT), which require not only some adaptation of processes within the state of the art processing chain of HLT, but also changes at the infrastructure level of HLT resources."
buitelaar-etal-2004-evaluation,Evaluation Resources for Concept-based Cross-Lingual Information Retrieval in the Medical Domain,2004,7,9,1,1,6276,paul buitelaar,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The paper describes evaluation resources for concept-based, cross-lingual information retrieval in the medical domain. All resources were constructed in the context of the MuchMore project and are freely available through the project website. Available resources include: a bilingual, parallel document collection of German and English medical scientific abstracts, a set of queries and corresponding relevance assessments, two manually disambiguated test sets for semantic annotation (sense disambiguation), two evaluation lists for German morphological decomposition of medical terms."
buitelaar-etal-2004-towards,Towards Ontology Engineering Based on Linguistic Analysis,2004,11,11,1,1,6276,paul buitelaar,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we describe OntoLT, a plug -in for the widely used Protege ontology development tool that supports the interactive extraction and/or extension of ontologies from text. The OntoLT approach aims at providing an environment for the integration of linguistic analysis in ontology development. OntoLT enables the definition of mapping rules with which concepts and attributes can be extracted automatically from linguistically annotated text collections. Mapping rules are defined by use of a constraint language. Constraints are implemented as XPATH expressions over the XML-based linguistic annotation. If all constraints are satisfied, the mapping rule activates one or more operators that describe in which way the ontology should be extended if a candidate is found."
W03-1302,Unsupervised Monolingual and Bilingual Word-Sense Disambiguation of Medical Documents using {UMLS},2003,9,26,6,0,30375,dominic widdows,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"This paper describes techniques for unsupervised word sense disambiguation of English and German medical documents using UMLS. We present both monolingual techniques which rely only on the structure of UMLS, and bilingual techniques which also rely on the availability of parallel corpora. The best results are obtained using relations between terms given by UMLS, a method which achieves 74% precision, 66% coverage for English and 79% precision, 73% coverage for German on evaluation corpora and over 83% coverage over the whole corpus. The success of this technique for German shows that a lexical resource giving relations between concepts used to index an English document collection can be used for high quality disambiguation in another language."
E03-2012,A Cross Language Document Retrieval System Based on Semantic Annotation,2003,3,8,2,0,42996,bogdan sacaleanu,Demonstrations,0,"The paper describes a cross-lingual document retrieval system in the medical domain that employs a controlled vocabulary (UMLS1) in constructing an XML-based intermediary representation into which queries as well as documents are mapped. The system assists in the retrieval of English and German medical scientific abstracts relevant to a German query document (electronic patient record). The modularity of the system allows for deployment in other domains, given appropriate linguistic and semantic resources."
raileanu-etal-2002-evaluation,Evaluation Corpora for Sense Disambiguation in the Medical Domain,2002,6,9,2,0,53420,diana raileanu,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"An important aspect of word sense disambiguation is the evaluation of different methods and parameters. Unfortunately, there is a lack of test sets for evaluation, specifically for languages other than English and even more so for specific domains like medicine. Given that our work focuses on English as well as German text in the medical domain, we had to develop our own evaluation corpora in order to test our disambiguation methods. In this paper we describe the work on developing these corpora, using GermaNet and UMLS as (lexical) semantic resources, next to a description of the annotation tool KiC that we developed for support of the annotation task."
vintar-etal-2002-efficient,An Efficient and Flexible Format for Linguistic and Semantic Annotation,2002,8,11,2,0,21021,vspela vintar,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The paper describes an XML annotation format and tool developed within the MUCHMORE project. The annotation scheme was designed specifically for the purposes of Cross-Lingual Information Retrieval in the medical domain so as to allow both efficient and flexible access to layers of information. We use a parallel English-German corpus of medical abstracts and annotate it with linguistic information (tokenisation, part-of-speech tagging, lemmatisation and decomposition, phrase recognition, grammatical functions) as well as semantic information from various sources. The annotation of medical terms/concepts, semantic types and semantic relations is based on the Unified Medical Language System (UMLS). Additionally, we use EuroWordNet as a general-language resource in annotating word senses and to compare domain-specific and general language use. A major aim of the project is also to complement existing ontological resources by extracting new terms and new semantic relations. We present the annotation scheme, which is conceptually related to stand-off annotation, and describe our tool for automatic semantic annotation."
S01-1012,"The {SENSEVAL}-2 Panel on Domains, Topics and Senses",2001,13,0,1,1,6276,paul buitelaar,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"An important aspect of sense disambiguation is the wider semantic space (domain, topic) in which the ambiguous word occurs. This may be most clearly illustrated by some cross-lingual examples, as they would appear in (machine) translation. Consider for instance the English word housing. In a more general sense, this translates in German into Wohnung. In an engineering setting however it translates into Gehause. Also verbs may be translated differently (i.e. have a different sense) according to the semantic space in which they occur. For instance, English warming up translates into erhitzen in a more general sense, but into aufwarmen in the sports domain."
W00-0103,Reducing Lexical Semantic Complexity with Systematic Polysemous Classes and Underspecification,2000,12,19,1,1,6276,paul buitelaar,{NAACL}-{ANLP} 2000 Workshop: Syntactic and Semantic Complexity in Natural Language Processing Systems,0,"This paper presents an algorithm for finding systematic polysemous classes in WordNet and similar semantic databases, based on a definition in (Apresjan 1973). The introduction of systematic polysemous classes can reduce the amount of lexical semantic processing, because the number of disambiguation decisions can be restricted more clearly to those cases that involve real ambiguity (homonymy). In many applications, for instance in document categorization, information retrieval, and information extraction, it may be sufficient to know if a given word belongs to a certain class (underspecified sense) rather than to know which of its (related) senses exactly to pick. The approach for finding systematic polysemous classes is based on that of (Buitelaar 1998a, Buitelaar 1998b), while addressing some previous shortcomings."
W97-0205,A Lexicon for Underspecified Semantic Tagging,1997,11,17,1,1,6276,paul buitelaar,"Tagging Text with Lexical Semantics: Why, What, and How?",0,"The paper defends the notion that semantic tagging should be viewed as more than disambiguation between senses. Instead, semantic tagging should be a first step in the interpretation process by assigning each lexical item a representation of all of its systematically related senses, from which further semantic processing steps can derive discourse dependent interpretations. This leads to a new type of semantic lexicon (CoreLex) that supports underspecified semantic tagging through a design based on systematic polysemous classes and a class-based acquisition of lexical knowledge for specific domains."
