2006.eamt-1.28,P05-1048,0,0.075225,"Missing"
2006.eamt-1.28,J94-4003,0,0.0751692,"a sentence or text. For MT purposes, however, the context may also include the translation in the target language, i.e., words in the text which have already been translated. Although intuitively plausible, this strategy has not been explored specifically for WSD. On the other hand, some related approaches have exploited similar strategies for other purposes. For example, some approaches for MT, especially the statistics-based approaches, make use of the words which have already been translated as context, implicitly accomplishing basic WSD during the translation process (Egedi et al., 1994; Dagan and Itai, 1994). Some approaches for monolingual WSD use techniques which are similar to ours to gather cooccurrence evidence from bilingual corpus in order either to carry out WSD (Mihalcea and Moldovan, 1999), or to create monolingual sense tagged corpora (Fernández et al., 2004). Other monolingual related approaches somehow explore the already disambiguated or unambiguous words by taking into account the senses of the other words in the sentence in order to disambiguate a given word (Lesk, 1986; Hirst, 1987; Cowie et al., 1992). In this paper we investigate the use of the translation context, that is, the"
2006.eamt-1.28,1994.amta-1.7,0,0.0356531,"surrounding words in a sentence or text. For MT purposes, however, the context may also include the translation in the target language, i.e., words in the text which have already been translated. Although intuitively plausible, this strategy has not been explored specifically for WSD. On the other hand, some related approaches have exploited similar strategies for other purposes. For example, some approaches for MT, especially the statistics-based approaches, make use of the words which have already been translated as context, implicitly accomplishing basic WSD during the translation process (Egedi et al., 1994; Dagan and Itai, 1994). Some approaches for monolingual WSD use techniques which are similar to ours to gather cooccurrence evidence from bilingual corpus in order either to carry out WSD (Mihalcea and Moldovan, 1999), or to create monolingual sense tagged corpora (Fernández et al., 2004). Other monolingual related approaches somehow explore the already disambiguated or unambiguous words by taking into account the senses of the other words in the sentence in order to disambiguate a given word (Lesk, 1986; Hirst, 1987; Cowie et al., 1992). In this paper we investigate the use of the translatio"
2006.eamt-1.28,fernandez-etal-2004-automatic,0,0.0191785,"On the other hand, some related approaches have exploited similar strategies for other purposes. For example, some approaches for MT, especially the statistics-based approaches, make use of the words which have already been translated as context, implicitly accomplishing basic WSD during the translation process (Egedi et al., 1994; Dagan and Itai, 1994). Some approaches for monolingual WSD use techniques which are similar to ours to gather cooccurrence evidence from bilingual corpus in order either to carry out WSD (Mihalcea and Moldovan, 1999), or to create monolingual sense tagged corpora (Fernández et al., 2004). Other monolingual related approaches somehow explore the already disambiguated or unambiguous words by taking into account the senses of the other words in the sentence in order to disambiguate a given word (Lesk, 1986; Hirst, 1987; Cowie et al., 1992). In this paper we investigate the use of the translation context, that is, the surrounding words which have already been translated, as knowledge source for multilingual WSD. We present experiments on the disambiguation of 10 ambiguous verbs in English-Portuguese translation. The target language contextual information is applied by analysing t"
2006.eamt-1.28,P99-1020,0,0.0311478,"uitively plausible, this strategy has not been explored specifically for WSD. On the other hand, some related approaches have exploited similar strategies for other purposes. For example, some approaches for MT, especially the statistics-based approaches, make use of the words which have already been translated as context, implicitly accomplishing basic WSD during the translation process (Egedi et al., 1994; Dagan and Itai, 1994). Some approaches for monolingual WSD use techniques which are similar to ours to gather cooccurrence evidence from bilingual corpus in order either to carry out WSD (Mihalcea and Moldovan, 1999), or to create monolingual sense tagged corpora (Fernández et al., 2004). Other monolingual related approaches somehow explore the already disambiguated or unambiguous words by taking into account the senses of the other words in the sentence in order to disambiguate a given word (Lesk, 1986; Hirst, 1987; Cowie et al., 1992). In this paper we investigate the use of the translation context, that is, the surrounding words which have already been translated, as knowledge source for multilingual WSD. We present experiments on the disambiguation of 10 ambiguous verbs in English-Portuguese translati"
2006.eamt-1.28,W06-2505,1,0.900824,"WordNet (Miller, 1990), to identify the monolingual senses, which are then mapped into the target language translations. However, mapping senses between languages is a very complex issue. One of the reasons for this complexity is the difference in the sense inventories of the languages, as already discussed in (Hutchins and Somers, 1992) and recently evidenced by studies with certain pairs of languages. For example, Bentivogli et al. (2004) investigate the sense inventory discrepancies for English-Italian, Miháltz (2005), for English-Hungarian, Chatterjee et al. (2005), for English-Hindi, and Specia et al. (2006), for English-Portuguese. They show that there is not a one-to-one relation between the number of senses in the source language and their translations into another language. More specifically, they show that many source language senses are translated into a unique target language word, while some senses need to be split into different translations, conveying sense distinctions that only exist in the target language. In addition to the differences in the sense inventory, the disambiguation process can vary according to the application. For instance, in monolingual WSD the main information is th"
2006.eamt-1.28,H05-1097,0,0.0734031,"Missing"
2006.eamt-1.28,C04-1053,0,\N,Missing
2020.aacl-main.76,N19-1423,0,0.00791772,"obust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy “man is to computer-programmer as woman is to homemaker” is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings. 1 Introduction Word embeddings, distributed representations of words in a low-dimensional vector space, are used in many downstream NLP tasks (Mikolov et al., 2013a,b; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019). Recent work has shown they can contain harmful bias and proposed techniques to quantify it (Bolukbasi et al., 2016; Caliskan et al., 2017; Ethayarajh et al., 2019; Gonen and Goldberg, 2019). These techniques leverage cosine similarity to a base pair of gender words, such as (man, woman). They include bias measures, which return a magnitude of bias for a given word, and analogies. A well-known example of the latter is “Man is to computer programmer as woman is to homemaker” (Bolukbasi et al., 2016), which has been widely interpreted as demonstrating bias. There have also been related attempts"
2020.aacl-main.76,C16-1332,0,0.0361415,"e analogical relationship can be identified using vector arithmetic (Mikolov et al., 2013a; Levy and Goldberg, 2014; Ethayarajh et al., 2018). A no−−→ −→ table example of this phenomena is king - − man + − − − − − → − − − → woman ≈ queen (Mikolov et al., 2013c). This relationship is frequently attributed to a gender −→ and − −−−−→ and difference vector between − man woman, −−→ −−→ (Mikolov et al., 2013c; between king and − queen Ethayarajh et al., 2018). Analogies are considered a benchmark method of measuring the quality of embeddings, though their suitability has been debated (Linzen, 2016; Drozd et al., 2016; Gladkova et al., 2016). The standard approach to solving ‘a is to b as c is to ?,” is to return: → −∗ → − − → − d = argmax CosSim(→ w, b − → a + −c ), Our aim is to examine the extent to which bias identifying techniques are reliabily capturing societal gender bias. Bias is a highly complex concept, and although the four bias measures (DB, WA, NBM and RIPA) may detect certain kinds of bias, there is no theoretical guarantee they will detect all forms, that the “bias” they find will be accurate or that different choices of base pair will behave similarly. We therefore explore whether the bias"
2020.aacl-main.76,P19-1166,0,0.69115,"gy “man is to computer-programmer as woman is to homemaker” is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings. 1 Introduction Word embeddings, distributed representations of words in a low-dimensional vector space, are used in many downstream NLP tasks (Mikolov et al., 2013a,b; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019). Recent work has shown they can contain harmful bias and proposed techniques to quantify it (Bolukbasi et al., 2016; Caliskan et al., 2017; Ethayarajh et al., 2019; Gonen and Goldberg, 2019). These techniques leverage cosine similarity to a base pair of gender words, such as (man, woman). They include bias measures, which return a magnitude of bias for a given word, and analogies. A well-known example of the latter is “Man is to computer programmer as woman is to homemaker” (Bolukbasi et al., 2016), which has been widely interpreted as demonstrating bias. There have also been related attempts to debias * denotes equal contribution. embeddings (Bolukbasi et al., 2016; Zhao et al., 2018; Dev and Phillips, 2019; Kaneko and Bollegala, 2019; Manzini et al.,"
2020.aacl-main.76,N16-2002,0,0.0180663,"nship can be identified using vector arithmetic (Mikolov et al., 2013a; Levy and Goldberg, 2014; Ethayarajh et al., 2018). A no−−→ −→ table example of this phenomena is king - − man + − − − − − → − − − → woman ≈ queen (Mikolov et al., 2013c). This relationship is frequently attributed to a gender −→ and − −−−−→ and difference vector between − man woman, −−→ −−→ (Mikolov et al., 2013c; between king and − queen Ethayarajh et al., 2018). Analogies are considered a benchmark method of measuring the quality of embeddings, though their suitability has been debated (Linzen, 2016; Drozd et al., 2016; Gladkova et al., 2016). The standard approach to solving ‘a is to b as c is to ?,” is to return: → −∗ → − − → − d = argmax CosSim(→ w, b − → a + −c ), Our aim is to examine the extent to which bias identifying techniques are reliabily capturing societal gender bias. Bias is a highly complex concept, and although the four bias measures (DB, WA, NBM and RIPA) may detect certain kinds of bias, there is no theoretical guarantee they will detect all forms, that the “bias” they find will be accurate or that different choices of base pair will behave similarly. We therefore explore whether the bias measures are robust in"
2020.aacl-main.76,W19-3621,0,0.315635,"ogrammer as woman is to homemaker” is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings. 1 Introduction Word embeddings, distributed representations of words in a low-dimensional vector space, are used in many downstream NLP tasks (Mikolov et al., 2013a,b; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019). Recent work has shown they can contain harmful bias and proposed techniques to quantify it (Bolukbasi et al., 2016; Caliskan et al., 2017; Ethayarajh et al., 2019; Gonen and Goldberg, 2019). These techniques leverage cosine similarity to a base pair of gender words, such as (man, woman). They include bias measures, which return a magnitude of bias for a given word, and analogies. A well-known example of the latter is “Man is to computer programmer as woman is to homemaker” (Bolukbasi et al., 2016), which has been widely interpreted as demonstrating bias. There have also been related attempts to debias * denotes equal contribution. embeddings (Bolukbasi et al., 2016; Zhao et al., 2018; Dev and Phillips, 2019; Kaneko and Bollegala, 2019; Manzini et al., 2019). However, to remove b"
2020.aacl-main.76,P19-1160,0,0.0211823,"Missing"
2020.aacl-main.76,W14-1618,0,0.265398,"− ||→ x −→ y || Analogies An alternative approach to identifying gender bias in embeddings is via word analogies. Unlike the gender bias measures dicussed in Section 2.1, analogies do not measure the bias of a particular word. Instead, they identify pairs of words which are assumed to have a gendered relationship. Analogies in word embeddings are important because it has been observed that embedding vectors seem to possess unexpected linear properties: vectors associated with word pairs sharing the same analogical relationship can be identified using vector arithmetic (Mikolov et al., 2013a; Levy and Goldberg, 2014; Ethayarajh et al., 2018). A no−−→ −→ table example of this phenomena is king - − man + − − − − − → − − − → woman ≈ queen (Mikolov et al., 2013c). This relationship is frequently attributed to a gender −→ and − −−−−→ and difference vector between − man woman, −−→ −−→ (Mikolov et al., 2013c; between king and − queen Ethayarajh et al., 2018). Analogies are considered a benchmark method of measuring the quality of embeddings, though their suitability has been debated (Linzen, 2016; Drozd et al., 2016; Gladkova et al., 2016). The standard approach to solving ‘a is to b as c is to ?,” is to return"
2020.aacl-main.76,W16-2503,0,0.228775,"haring the same analogical relationship can be identified using vector arithmetic (Mikolov et al., 2013a; Levy and Goldberg, 2014; Ethayarajh et al., 2018). A no−−→ −→ table example of this phenomena is king - − man + − − − − − → − − − → woman ≈ queen (Mikolov et al., 2013c). This relationship is frequently attributed to a gender −→ and − −−−−→ and difference vector between − man woman, −−→ −−→ (Mikolov et al., 2013c; between king and − queen Ethayarajh et al., 2018). Analogies are considered a benchmark method of measuring the quality of embeddings, though their suitability has been debated (Linzen, 2016; Drozd et al., 2016; Gladkova et al., 2016). The standard approach to solving ‘a is to b as c is to ?,” is to return: → −∗ → − − → − d = argmax CosSim(→ w, b − → a + −c ), Our aim is to examine the extent to which bias identifying techniques are reliabily capturing societal gender bias. Bias is a highly complex concept, and although the four bias measures (DB, WA, NBM and RIPA) may detect certain kinds of bias, there is no theoretical guarantee they will detect all forms, that the “bias” they find will be accurate or that different choices of base pair will behave similarly. We therefore expl"
2020.aacl-main.76,N19-1062,0,0.0922354,"ajh et al., 2019; Gonen and Goldberg, 2019). These techniques leverage cosine similarity to a base pair of gender words, such as (man, woman). They include bias measures, which return a magnitude of bias for a given word, and analogies. A well-known example of the latter is “Man is to computer programmer as woman is to homemaker” (Bolukbasi et al., 2016), which has been widely interpreted as demonstrating bias. There have also been related attempts to debias * denotes equal contribution. embeddings (Bolukbasi et al., 2016; Zhao et al., 2018; Dev and Phillips, 2019; Kaneko and Bollegala, 2019; Manzini et al., 2019). However, to remove bias effectively, an accurate method of identifying it is first required. This is a complex task, not least because the concept of “bias” has multiple interpretations: Mehrabi et al. (2019) identify 23 types of bias that can occur in machine learning applications, including historic (pre-existing in society), algorithmic (introduced by the algorithm) and evaluation (occurs during model evaluation). In the case of word embeddings, it remains an open question if bias identifying techniques reflect social stereotypes in the training data, an artifact of the embedding process"
2020.aacl-main.76,N13-1090,0,0.744896,"pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy “man is to computer-programmer as woman is to homemaker” is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings. 1 Introduction Word embeddings, distributed representations of words in a low-dimensional vector space, are used in many downstream NLP tasks (Mikolov et al., 2013a,b; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019). Recent work has shown they can contain harmful bias and proposed techniques to quantify it (Bolukbasi et al., 2016; Caliskan et al., 2017; Ethayarajh et al., 2019; Gonen and Goldberg, 2019). These techniques leverage cosine similarity to a base pair of gender words, such as (man, woman). They include bias measures, which return a magnitude of bias for a given word, and analogies. A well-known example of the latter is “Man is to computer programmer as woman is to homemaker” (Bolukbasi et al., 2016), which has been widely i"
2020.aacl-main.76,D14-1162,0,0.0825709,"Missing"
2020.aacl-main.76,N18-1202,0,0.0323292,"off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy “man is to computer-programmer as woman is to homemaker” is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings. 1 Introduction Word embeddings, distributed representations of words in a low-dimensional vector space, are used in many downstream NLP tasks (Mikolov et al., 2013a,b; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019). Recent work has shown they can contain harmful bias and proposed techniques to quantify it (Bolukbasi et al., 2016; Caliskan et al., 2017; Ethayarajh et al., 2019; Gonen and Goldberg, 2019). These techniques leverage cosine similarity to a base pair of gender words, such as (man, woman). They include bias measures, which return a magnitude of bias for a given word, and analogies. A well-known example of the latter is “Man is to computer programmer as woman is to homemaker” (Bolukbasi et al., 2016), which has been widely interpreted as demonstrating bias. There have also"
2020.aacl-main.76,D18-1521,0,0.0378843,"to quantify it (Bolukbasi et al., 2016; Caliskan et al., 2017; Ethayarajh et al., 2019; Gonen and Goldberg, 2019). These techniques leverage cosine similarity to a base pair of gender words, such as (man, woman). They include bias measures, which return a magnitude of bias for a given word, and analogies. A well-known example of the latter is “Man is to computer programmer as woman is to homemaker” (Bolukbasi et al., 2016), which has been widely interpreted as demonstrating bias. There have also been related attempts to debias * denotes equal contribution. embeddings (Bolukbasi et al., 2016; Zhao et al., 2018; Dev and Phillips, 2019; Kaneko and Bollegala, 2019; Manzini et al., 2019). However, to remove bias effectively, an accurate method of identifying it is first required. This is a complex task, not least because the concept of “bias” has multiple interpretations: Mehrabi et al. (2019) identify 23 types of bias that can occur in machine learning applications, including historic (pre-existing in society), algorithmic (introduced by the algorithm) and evaluation (occurs during model evaluation). In the case of word embeddings, it remains an open question if bias identifying techniques reflect soc"
2020.lrec-1.465,P19-1310,0,0.0388915,"Missing"
2020.lrec-1.465,W91-0109,0,0.371684,"her studies, such as Europarl(Koehn, 2005), with Spanish being the easiest language to be translated into and also achieving one of the best scores, just behind English/French. As for Asian languages, Korean/English presented the best scores, while English/Chinese language pair achieved discouraging results. We suspect this is due to possible misalignment and/or translation divergences. 4.2.1. Note on Asian Languages BLEU scores between European and Asian languages are lower than between European languages. Hsu (2014) pointed out that translation divergences can affect MT output. According to Barnett et al. (1991), even though a set of legal translations can be valid, there is the notion of preferred translation that is not easily defined, but can be seen 3771 the gain factor of the E-DPDCH in the compressed mode is determined accurately, furthermore the transmission power of the EDPDCH is determined according to the gain factor, the waste of the transmission power of the E-DPDCH is reduced [...] Natural Chinese: 本发明实施例根据数据 初次传输时所需的码道数，确定压缩模 式下E-DPDCH 的增益因子，实现了准确 确定压缩模式下E-DPDCH 的增益因子， 进而根据该增益因子确定E-DPDCH 的发 射功率，减少了E-DPDCH 的发射功率浪 费.[...] Literal English: In the embodiment of the present invention, the ga"
2020.lrec-1.465,2005.mtsummit-papers.11,0,0.340097,"d Neural Machine Translation systems (SMT and NMT). Acquiring a high-quality parallel corpus that is large enough to train MT systems, particularly NMT ones, is not a trivial task due to the need for correct alignment and, in many cases, human curation. In this context, the automated creation of parallel corpora from freely available resources is extremely important in Natural Language Processing (NLP). Many parallel corpora are already available, some bilingually aligned (Tiedemann, 2012) and others multilingually aligned, with 3 or more languages (e.g. Europarl from the European Parliament (Koehn, 2005), JRCAcquis from the European Commission (Steinberger et al., 2006) and OpenSubtitles consisting of film subtitles (Zhang et al., 2014)). The extraction of parallel sentences from patents and scientific texts can be a valuable language resource for MT and other NLP tasks. The problem has been researched by several authors to support, for example, translation of biomedical articles (Wu et al., 2011; Neves et al., 2016) and named entity recognition of biomedical concepts (Kors et al., 2015). The most comprehensive dataset released to date for multilingual patent analytics and machine translation"
2020.lrec-1.465,O05-1029,0,0.0506602,"of EDPDCH in the compression mode is accurately determined, and then the transmission power of the E-DPDCH is determined according to the gain factor, reducing the emission power waste of E-DPDCH[...] Table 3: BLEU scores for translation using FairSeq in the test sets. Language Pair Test ES→EN 57.71 FR→EN 49.60 EN→RU 49.10 EN→FR 48.39 DE→EN 48.35 EN→DE 46.77 RU→EN 46.73 EN→ES 44.98 DE→FR 42.13 FR→DE 36.56 KO→EN 23.23 JA→EN 17.78 EN→JA 13.15 ZH→EN 13.01 EN→KO 11.93 EN→ZH 10.29 JA→FR 9.52 FR→JA 7.10 when the natural translation differs from the source in a significant way. Based on the works of Lin et al. (2005) and Barnett et al. (1991), we identified the main translation divergences in patents between English and Asian languages (i.e. Korean, Japanese, and Chinese) as thematic and discourse. Below we give examples of both in the Chinese language. • Thematic: when the arguments appears in different thematic roles, such as changing the focus of the sentence or changing from passive to active voice. Our linguists at TransPerfect Translations also identified that some patents that were presumably written first in Chinese (i.e. were initially submitted to the Chinese Patent Office) are not written in fl"
2020.lrec-1.465,L16-1470,0,0.558968,"Missing"
2020.lrec-1.465,N19-4009,0,0.0495817,"Missing"
2020.lrec-1.465,P02-1040,0,0.107036,"Missing"
2020.lrec-1.465,W18-6319,0,0.0229217,"Missing"
2020.lrec-1.465,2009.mtsummit-posters.15,0,0.198374,"Missing"
2020.lrec-1.465,steinberger-etal-2006-jrc,0,0.273479,"Missing"
2020.lrec-1.465,tiedemann-2012-parallel,0,0.758518,"nts, Machine Translation Models 1. Introduction The availability of parallel corpora is required by current Statistical and Neural Machine Translation systems (SMT and NMT). Acquiring a high-quality parallel corpus that is large enough to train MT systems, particularly NMT ones, is not a trivial task due to the need for correct alignment and, in many cases, human curation. In this context, the automated creation of parallel corpora from freely available resources is extremely important in Natural Language Processing (NLP). Many parallel corpora are already available, some bilingually aligned (Tiedemann, 2012) and others multilingually aligned, with 3 or more languages (e.g. Europarl from the European Parliament (Koehn, 2005), JRCAcquis from the European Commission (Steinberger et al., 2006) and OpenSubtitles consisting of film subtitles (Zhang et al., 2014)). The extraction of parallel sentences from patents and scientific texts can be a valuable language resource for MT and other NLP tasks. The problem has been researched by several authors to support, for example, translation of biomedical articles (Wu et al., 2011; Neves et al., 2016) and named entity recognition of biomedical concepts (Kors et"
2020.lrec-1.465,zhang-etal-2014-dual,0,0.0523176,"Missing"
2021.naacl-main.187,D19-1522,0,0.0208138,"2 .544 .382 .550 .535 .380 .535 .328 .480 .337 .483 .310 .433 .333 .465 .354 .492 .379 .541 96 91 121 152 1007 229 1128 184 180 267 207 343 320 9 37 56 85 370 350 534 598 4053 751 4589 704 945 1173 1120 1006 1144 42 159 243 285 Table 2: Model effectiveness and efficiency on link prediction benchmarks. : training time (minutes); : carbon dioxide production (grams). NS: negative sampling; TB: traditional batch. The performance results of baselines are coloured heavily and lightly if they are below those of P ROCRUST E S and “w/ NS+TB”, respectively. Stateof-the-art scores are in bold. Following Balazevic et al. (2019) and Zhang et al. (2019), for fair comparison, both RotatE and OTE results are reported with conventional negative sampling rather than the self-adversarial one. et al., 2018), A2N (Bansal et al., 2019), RotatE (Sun et al., 2019), SACN (Shang et al., 2019), TuckER (Balazevic et al., 2019), QuatE (Zhang et al., 2019), InteractE (Vashishth et al., 2020), OTE (Tang et al., 2020), and RotH (Chami et al., 2020). For all these baselines, we use the official code and published hyper-parameters to facilitate reproducibility. 3.2 Main Results Tab. 2 reports the results of both our P ROCRUST E S and all"
2021.naacl-main.187,P19-1431,0,0.0245499,"06 1144 42 159 243 285 Table 2: Model effectiveness and efficiency on link prediction benchmarks. : training time (minutes); : carbon dioxide production (grams). NS: negative sampling; TB: traditional batch. The performance results of baselines are coloured heavily and lightly if they are below those of P ROCRUST E S and “w/ NS+TB”, respectively. Stateof-the-art scores are in bold. Following Balazevic et al. (2019) and Zhang et al. (2019), for fair comparison, both RotatE and OTE results are reported with conventional negative sampling rather than the self-adversarial one. et al., 2018), A2N (Bansal et al., 2019), RotatE (Sun et al., 2019), SACN (Shang et al., 2019), TuckER (Balazevic et al., 2019), QuatE (Zhang et al., 2019), InteractE (Vashishth et al., 2020), OTE (Tang et al., 2020), and RotH (Chami et al., 2020). For all these baselines, we use the official code and published hyper-parameters to facilitate reproducibility. 3.2 Main Results Tab. 2 reports the results of both our P ROCRUST E S and all other 13 baselines on both WN18RR and FB15k-237 datasets. We analyse these results from two dimensions: (1) Effectiveness: the model performance on link prediction task (MRR is our main indicator); (2)"
2021.naacl-main.187,P17-1162,0,0.0690268,"Missing"
2021.naacl-main.187,N19-1423,0,0.0221222,"Missing"
2021.naacl-main.187,D18-2024,0,0.0176384,"erent methodologies). In response to this issue, some state-of-the-art KGE algorithms attempted to accelerate their inference speed either through making use of the high-speed of the convolutional neural networks (Dettmers et al., 2018) or through reducing the scale of parameters of the model (Zhang et al., 2019; Zhu et al., 2020). As for the acceleration of model training, a number of attempts have been conducted in a mostly engineering way. These well-engineered systems adopt linear KGE methods to multi-thread versions in other to make full use of the hardware capacity (Joulin et al., 2017; Han et al., 2018), which accelerates training time of, for example, TransE, from more than an hour to only a couple of minutes. Nonetheless, this line of work has two major issues: one is that training models faster in this way does not necessarily mean they also emit less, as process scheduling of a multi-thread system can be energy-consuming. The other is that they are all extensions of linear KGE models only (also noting that linear models are naturally much faster than other non-linear models) without any algorithmic contribution, which leading to the performance of the resulting models limited by the uppe"
2021.naacl-main.187,P15-1067,0,0.0835756,"Missing"
2021.naacl-main.187,W17-1601,0,0.0199119,"Missing"
2021.naacl-main.187,2020.acl-main.412,0,0.0349132,"the parameters used for training KGEs with the help of the quaternion. Knowledge Graph Embeddings (KGEs) have In contrast with previous work, this paper explores been intensively explored in recent years due algorithmic approaches to the development of effito their promise for a wide range of applicacient KGE techniques. tions. However, existing studies focus on improving the final model performance without Knowledge Graphs are core to many NLP tasks acknowledging the computational cost of the and downstream applications, such as question anproposed approaches, in terms of execution swering (Saxena et al., 2020), dialogue agents (He time and environmental impact. This paper et al., 2017), search engines (Dong et al., 2014) and proposes a simple yet effective KGE framerecommendation systems (Guo et al., 2020). Facts work which can reduce the training time and stored in a knowledge graph are always in the forcarbon footprint by orders of magnitudes commat of tuples consisting of one head entity, one tail pared with state-of-the-art approaches, while entity (both are nodes in knowledge graphs) and a producing competitive performance. We highlight three technical innovations: full batch relation (an edge"
2021.naacl-main.187,P19-1355,0,0.0184323,"ensive with associated envieffectiveness and efficiency of our algorithm. ronmental costs. For example, training the SACN model (Shang et al., 2019) can lead to emissions 1 Introduction of more than 5.3kg CO2 (for more data of other The recent growth in energy requirements for Nat- algorithms, see Tab. 2). ural Language Processing (NLP) algorithms has To alleviate the computational cost we introduce led to the recognition of the importance of com- P ROCRUST E S, a lightweight, fast, and eco-friendly putationally cheap and eco-friendly approaches KGE training technique. P ROCRUST E S is built (Strubell et al., 2019). The increase in computa- upon three novel techniques. First, to reduce the tional requirements can, to a large extent, be at- batch-wise computational overhead, we propose to tributed to the popularity of massive pre-trained parallelise batches by grouping tuples according to models, such as Language Models (e.g., BERT (De- their relations, which ultimately enables efficient vlin et al., 2019) and GPT-3 (Brown et al., 2020)) full batch learning. Second, we turn to a closedand Knowledge Graph Embeddings (KGEs, e.g., form solution for Orthogonal Procrustes Problem to SACN (Shang et al., 2019))"
2021.naacl-main.187,2020.acl-main.241,0,0.160067,"th are nodes in knowledge graphs) and a producing competitive performance. We highlight three technical innovations: full batch relation (an edge in knowledge graphs) between learning via relational matrices, closed-form them. KGEs learn representations of relations Orthogonal Procrustes Analysis for KGEs, and and entities in a knowledge graph, which are then non-negative-sampling training. In addition, utilised in downstream tasks like predicting missas the first KGE method whose entity embeding relations (Bordes et al., 2013; Sun et al., 2019; dings also store full relation information, our Tang et al., 2020). The application of deep learntrained models encode rich semantics and are ing has led to significant advances in KGE (Rossi highly interpretable. Comprehensive experiments and ablation studies involving 13 strong et al., 2021). Nonetheless, such approaches are baselines and two standard datasets verify the computationally expensive with associated envieffectiveness and efficiency of our algorithm. ronmental costs. For example, training the SACN model (Shang et al., 2019) can lead to emissions 1 Introduction of more than 5.3kg CO2 (for more data of other The recent growth in energy requiremen"
2021.naacl-main.187,W15-4007,0,0.0269056,"20), existing KGE We assess the performance of P ROCRUST E S on the methods employ negative sampling as a standard task of multi-relational link prediction, which is the technique for reducing training time, where update de facto standard of KGE evaluation. is performed only on a subset of parameters by Datasets. In this study, following previous calculating loss based on the generated negative works (e.g., baselines in Tab. 2), we employ two samples. With our proposed closed-form solution (i.e., Eq. (3)), computing gradients to update em- benchmark datasets for link prediction: (1) FB15K237 (Toutanova and Chen, 2015), which consists of beddings is no longer an efficiency bottleneck for sub-graphs extracted from Freebase, and contains P ROCRUST E S. Instead, the speed bottleneck turns no inverse relations; and (2) WN18RR (Dettmers out to be the extra bandwidth being occupied due to the added negative samples. Therefore, for P RO - et al., 2018), which is extracted from WordNet. Tab. 1 shows descriptive statistics for these two CRUST E S, we do not employ negative sampling but datasets, indicating that FB15K-237 is larger in rather update all embeddings during each round of size and has more types relations"
2021.naacl-main.187,2020.acl-main.358,0,0.0210289,"t time, thereby enriching the expressiveness of entity embeddings and producing new insights into interpretability. by orders of magnitude while retaining strong performance. This is achieved by introducing three novel optimisation strategies, namely, relational mini-batch, closed-form Orthogonal Procrustes Analysis, and non-negative sampling training. 2.1 Preliminaries: Segmented Embeddings Our proposed P ROCRUST E S model is built upon segmented embeddings, a technique which has been leveraged by a number of promising recent approaches to KGE learning (e.g., RotatE (Sun et al., 2019), SEEK (Xu et al., 2020), and OTE (Tang et al., 2020)). In contrast to conventional methods for KGEs where each entity only corresponds to one single vector, algorithms adopting segmented embeddings explicitly divide the entity representation space into multiple independent sub-spaces. During training each entity is encoded as a concatenation of decoupled sub-vectors (i.e., different segments, and hence the name). For example, as shown in Fig. 1, to encode a graph with 7 entities, the embedding of the tth entity is the rowwise concatenation of its d/ds sub-vectors (i.e., et,1 _ et,2 _ . . ._ et,d/ds ), where d and ds"
2021.naacl-main.187,2020.emnlp-main.595,0,0.0609747,"Missing"
2021.naacl-main.214,D16-1250,0,0.123292,"antages. First, adding the orthogonalvectors representing words with similar meanity constraint to the mapping function has been ing (regardless of language) are closely lodemonstrated to significantly enhance the quality of cated. Existing methods for building highquality CLWEs learn mappings that minimise CLWEs (Xing et al., 2015). Second, the existence the `2 norm loss function. However, this opof a closed-form solution to the `2 optima (Schönetimisation objective has been demonstrated to mann, 1966) greatly simplifies the computation be sensitive to outliers. Based on the more rorequired (Artetxe et al., 2016; Smith et al., 2017). bust Manhattan norm (aka. `1 norm) goodnessDespite its popularity, work in various appliof-fit criterion, this paper proposes a simple cation domains has noted that `2 loss is not ropost-processing step to improve CLWEs. An advantage of this approach is that it is fully agbust to noise and outliers. It is widely known in nostic to the training process of the original computer vision that `2 -loss-based solutions can CLWEs and can therefore be applied widely. severely exaggerate noise, leading to inaccurate Extensive experiments are performed involvestimates (Aanæs et al."
2021.naacl-main.214,P18-1073,0,0.342465,"s than guages, e.g., machine translation (Artetxe et al., closer pairs. This raises the question of the appro2018b) and transfer learning (Peng et al., 2021). priateness of `2 loss functions for CLWEs. The most successful CLWE models are the soCompared to the conventional `2 loss, `1 loss called projection-based methods, which learn map- (aka. Manhattan distance) has been mathematpings between monolingual word vectors with very ically demonstrated to be less affected by outlittle, or even zero, cross-lingual supervision (Lam- liers (Rousseeuw and Leroy, 1987) and empirically ple et al., 2018; Artetxe et al., 2018a; Glavaš proven useful in computer vision and data minet al., 2019). Mainstream projection-based CLWE ing (Aanæs et al., 2002; De La Torre and Black, models typically identify orthogonal mappings by 2003; Kwak, 2008). Motivated by this insight, ∗ Chenghua Lin is the corresponding author. our paper proposes a simple yet effective post2690 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2690–2701 June 6–11, 2021. ©2021 Association for Computational Linguistics processing technique to improve th"
2021.naacl-main.214,D18-1549,0,0.150157,"s than guages, e.g., machine translation (Artetxe et al., closer pairs. This raises the question of the appro2018b) and transfer learning (Peng et al., 2021). priateness of `2 loss functions for CLWEs. The most successful CLWE models are the soCompared to the conventional `2 loss, `1 loss called projection-based methods, which learn map- (aka. Manhattan distance) has been mathematpings between monolingual word vectors with very ically demonstrated to be less affected by outlittle, or even zero, cross-lingual supervision (Lam- liers (Rousseeuw and Leroy, 1987) and empirically ple et al., 2018; Artetxe et al., 2018a; Glavaš proven useful in computer vision and data minet al., 2019). Mainstream projection-based CLWE ing (Aanæs et al., 2002; De La Torre and Black, models typically identify orthogonal mappings by 2003; Kwak, 2008). Motivated by this insight, ∗ Chenghua Lin is the corresponding author. our paper proposes a simple yet effective post2690 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2690–2701 June 6–11, 2021. ©2021 Association for Computational Linguistics processing technique to improve th"
2021.naacl-main.214,L18-1550,0,0.0262577,"I|) &gt; , (8) where  is a sufficiently small threshold. The resulting M? can be used to adjust the word vectors of LA and output refined CLWEs. A significant advantage of our algorithm is its generality: it is fully independent of the method used for creating the original CLWEs and can therefore be used to enhance a wide range of models, both in supervised and unsupervised settings. 4 4.1 Experimental Setup Datasets In order to demonstrate the generality of our proposed method, we conduct experiments using two groups of monolingual word embeddings trained on very different corpora: Wiki-Embs (Grave et al., 2018): embeddings developed using Wikipedia dumps for a range of ten diverse languages: two Germanic (English|EN, German|DE), two Slavic (Croatian|HR, Russian|RU), three Romance (French|FR, Italian|IT, Spanish|ES) and three non-Indo-European (Finnish|FI from the 1 See Chu and Trendafilov (2001) for derivation details. 2 Uralic family, Turkish|TR from the Turkic family It takes averagely 3 hours and up to 12 hours to perform Eq. (7) on an Intel Core i9-9900K CPU. In comparison, the and Chinese|ZH from the Sino-Tibetan family). time required to solve Eq. (2) in each training loop is less than News-Em"
2021.naacl-main.214,N19-1188,0,0.0471488,"Missing"
2021.naacl-main.214,P17-1152,0,0.0430405,"Missing"
2021.naacl-main.214,D18-1269,0,0.0211115,"le based on the popular Inter- 2017) based on the large-scale English MultiNLI corpus (Williams et al., 2018) using vectors of lanQuartile Range (Hoaglin et al., 1986). We found guage LA (EN) from an aligned bilingual embedding that many of the outliers correspond to polysemous space (e.g., EN–DE). Next, we replace the LA vecentries, such as {state (2× noun meanings and 1× verb meaning), состояние (only means status)}, tors with the vectors of language LB (e.g., DE), and {type (2× nominal meanings and 1× verb mean- directly test the trained model on the language LB portion of the XNLI corpus (Conneau et al., 2018). ing), тип (only means kind)}, and {film (5× noun meanings), фильм (only means movie)}. We then Results in Tab. 4 show that the CLWEs refined 2697 by our algorithm yield the highest ACC for all language pairs in both supervised and unsupervised settings. The `2 refinement, on the contrary, is not beneficial overall. Improvements in cross-lingual transfer for NLI exhibit similar trends to those in the BLI experiments, i.e. greater performance gain for unsupervised methods and more distant language pairs, consistent with previous observations (Glavaš et al., 2019). For instance, M USE-`1 JAM US"
2021.naacl-main.214,P18-1128,0,0.0248764,"Missing"
2021.naacl-main.214,D18-1330,0,0.018105,"rices composed using vectors from source and target embedding spaces. While Xing et al. (2015) exploited an approximate and relatively slow gradient-based solver, more recent approaches such as Artetxe et al. (2016) and Smith et al. (2017) introduced an exact closed-form solution for Eq. (1). Originally proposed by Schönemann (1966), it utilises Singular Value Decomposition (SVD): M? = UV |, with UΣV |= SVD(A |B), (2) where M? denotes the `2 -optimal mapping matrix. The efficiency and effectiveness of Eq. (2) have led to its application within many other approaches, e.g., Ruder et al. (2018), Joulin et al. (2018) and Glavaš et al. (2019). In particular, P ROC -B (Glavaš et al., 2019), a supervised CLWE framework that simply applies multiple iterations of `2 OPA, has been demonstrated to produce very competitive performance on various benchmark tasks including BLI as well as cross-lingual transfer for NLI and information retrieval. 2 Related Work While the aforementioned approaches still reCLWE methods. One approach to generating quire some weak supervision (i.e., seed dictionarCLWEs is to train shared semantic representations ies), there have also been some successful attempts using multilingual texts"
2021.naacl-main.214,2021.eacl-main.273,1,0.686372,"and structural 1 Introduction variance (Ruder et al., 2019)), making the presCross-Lingual Word Embedding (CLWE) tech- ence of outliers more likely. Empirical analysis of niques have recently received significant atten- CLWEs also demonstrates that more distant word tion as an effective means to support Natural Lan- pairs (which are more likely to be outliers) have guage Processing applications for low-resource lan- more influence on the behaviour of `2 loss than guages, e.g., machine translation (Artetxe et al., closer pairs. This raises the question of the appro2018b) and transfer learning (Peng et al., 2021). priateness of `2 loss functions for CLWEs. The most successful CLWE models are the soCompared to the conventional `2 loss, `1 loss called projection-based methods, which learn map- (aka. Manhattan distance) has been mathematpings between monolingual word vectors with very ically demonstrated to be less affected by outlittle, or even zero, cross-lingual supervision (Lam- liers (Rousseeuw and Leroy, 1987) and empirically ple et al., 2018; Artetxe et al., 2018a; Glavaš proven useful in computer vision and data minet al., 2019). Mainstream projection-based CLWE ing (Aanæs et al., 2002; De La Tor"
2021.naacl-main.214,D18-1042,0,0.0177707,", and A and B are matrices composed using vectors from source and target embedding spaces. While Xing et al. (2015) exploited an approximate and relatively slow gradient-based solver, more recent approaches such as Artetxe et al. (2016) and Smith et al. (2017) introduced an exact closed-form solution for Eq. (1). Originally proposed by Schönemann (1966), it utilises Singular Value Decomposition (SVD): M? = UV |, with UΣV |= SVD(A |B), (2) where M? denotes the `2 -optimal mapping matrix. The efficiency and effectiveness of Eq. (2) have led to its application within many other approaches, e.g., Ruder et al. (2018), Joulin et al. (2018) and Glavaš et al. (2019). In particular, P ROC -B (Glavaš et al., 2019), a supervised CLWE framework that simply applies multiple iterations of `2 OPA, has been demonstrated to produce very competitive performance on various benchmark tasks including BLI as well as cross-lingual transfer for NLI and information retrieval. 2 Related Work While the aforementioned approaches still reCLWE methods. One approach to generating quire some weak supervision (i.e., seed dictionarCLWEs is to train shared semantic representations ies), there have also been some successful attempts us"
2021.naacl-main.214,N18-1101,0,0.0148795,"tance between vectors corresponding to the ground- those trained with Wiki-Embs for BLI. For M USE, truth dictionary of Lample et al. (2018) (cf. Fig. 1a). JA-M USE and V EC M AP, we also obtain CLWEs for EN–TR pair with the same configuration. We then detected large outliers by finding vector Following Glavaš et al. (2019), we first train the pairs whose distance falls above Q3 + 1.5 · (Q3 − Enhanced Sequential Inference Model (Chen et al., Q1), where Q1 and Q3 respectively denote the lower and upper quartile based on the popular Inter- 2017) based on the large-scale English MultiNLI corpus (Williams et al., 2018) using vectors of lanQuartile Range (Hoaglin et al., 1986). We found guage LA (EN) from an aligned bilingual embedding that many of the outliers correspond to polysemous space (e.g., EN–DE). Next, we replace the LA vecentries, such as {state (2× noun meanings and 1× verb meaning), состояние (only means status)}, tors with the vectors of language LB (e.g., DE), and {type (2× nominal meanings and 1× verb mean- directly test the trained model on the language LB portion of the XNLI corpus (Conneau et al., 2018). ing), тип (only means kind)}, and {film (5× noun meanings), фильм (only means movie)}."
2021.naacl-main.214,N15-1104,0,0.0601929,"Missing"
2021.naacl-main.214,N19-1161,0,0.0331679,"Missing"
2021.naacl-main.214,P16-1157,0,0.0745876,"Missing"
2021.naacl-main.214,P16-1024,0,0.0649496,"Missing"
2021.naacl-main.214,D19-1450,0,0.0415003,"Missing"
2021.nlp4if-1.1,2020.acl-main.164,0,0.0801319,"Missing"
2021.nlp4if-1.1,2020.coling-main.208,0,0.0625523,"Missing"
2021.nlp4if-1.1,N19-1423,0,0.0612986,"Missing"
2021.nlp4if-1.1,P11-1032,0,0.0972169,"Missing"
2021.nlp4if-1.1,2020.emnlp-demos.25,0,0.0696521,"Missing"
2021.nlp4if-1.1,C18-1287,0,0.0479867,"Missing"
2021.nlp4if-1.1,P19-3019,0,0.0927491,"are indeed able to capture patterns in the original data. Even though the number of words in the generated headlines is bound by the maximum number of words learned in the corresponding language model, the distribution of words is similar across real and generated headlines. In Figures 1 and 2 we indicatively show the 15 most frequent words in the real and generated headlines respectively. POS tag frequencies are shown in Table 1 for the top tags in each set. In real headlines, nouns are used more often, whereas in generated headlines the distribution is smoother, consistent with findings in Gehrmann et al. (2019). Furthermore, in generated headlines verbs appear more often in their base (VB) and third-person singular (VBZ) form while in real headlines verb tags are more uniformly distributed. Overall, GPT-2 has accurately learned the real distribution, with similarities across the board. Dataset 3.1 Dataset Development The dataset was created using Australian Broadcasting Corporation headlines and headlines generated from a model. A pretrained5 GPT-2 model (Radford et al., 2019) was finetuned on the headlines data. Text was generated using sampling with tem5 Figure 1: Top 15 Words for real headlines L"
A00-1012,H92-1022,0,0.0136449,"Missing"
A00-1012,J96-2004,0,0.017698,"xed Mixed Mixed Upper Mixed 84 93 90 97 96 97 5 5 68 78 76 90 89 67 5 5 76 85 82 94 92 79 5 5 Table 1: Results from Human Annotation Experiment The performance of the human annotators on the upper case text is quite significantly lower than the reported performance of the algorithms which performed punctuation disambiguation on standard text as described in Section 2. This suggests that the performance which may be obtained for this task may be lower than has been achieved for standard text. ~Sarther insight into the task can be gained from determining the degree to which the subjects agreed. Carletta (1996) argues that the kappa statistic (a) should be adopted to judge annotator consistency for classification tasks in the area of discourse and dialogue analysis. It is worth noting that the problem of sentence boundary detection presented so far in this paper has been formulated as a classification task in which each token boundary has to be classifted as either being a sentence boundary or not. Carletta argues that several incompatible measures of annotator agreement have been used in discourse analysis, making comparison impossible. Her solution is to look to the field of content analysis, whic"
A00-1012,P98-2140,0,0.023444,"Missing"
A00-1012,A97-1001,0,0.0225575,"generally in single case (usually upper), unpunctuated and may contain transcription errors. 1 Figure 1 compares a short text in the format which would be produced by an ASR system with a fully punctuated version which includes case information. For the remainder of this paper errorfree texts such as newspaper articles or novels shall be referred to as &quot;standard text&quot; and the output from a speech recognition system as &quot;ASR text&quot;. There are many possible situations in which an NLP system may be required to process ASR text. The most obvious examples are NLP systems which take speech input (eg. Moore et al. (1997)). Also, dictation software programs do not punctuate or capitalise their output but, if this information could be added to ASR text, the results would be far more usable. One of the most important pieces of informlike an execution. Schools inspections are going to be tougher to force bad teachers out. And the four thousand couples who shared the Queen&apos;s golden day. Figure 1: Example text shown in standard and ASR format ation which is not available in ASR output is sentence boundary information. However, knowledge of sentence boundaries is required by many NLP technologies. Part of speech tag"
A00-1012,A94-1013,0,0.196013,"Missing"
A00-1012,A97-1004,0,0.137396,"g indicating whether following word is a stop word Flag indicating whether following word is capitalised word sentence_boundary or no_boundary Table 2: Features used in Timbl representation Case information [I P I R I F Applied I 78 [ 75 [ 76 Not applied 36 35 35 Table 3: Results of the sentence boundary detection program suming a zero word error rate. This result is in agreement with the results from the human annotation experiments described in Section 3. However, there is a far greater difference between the automatic system&apos;s performance on standard and ASR text than the human annotators. Reynar and Ratnaparkhi (1997) (Section 2) argued that a context of one word either side is sufficient for the punctuation disambiguation problem. However, the results of our system suggest that this may be insufficient for the sentence boundary detection problem even assuming reliable part of speech tags (cf note 5). These experiments do not make use of prosodic information which may be included as part of the ASR output. Such information includes pause length, pre-pausal lengthening and pitch declination. If this information was made available in the form of extra features to a machine learning algorithm then it is possi"
A00-1012,C98-2135,0,\N,Missing
A00-1040,H92-1022,0,0.0427965,"Missing"
A00-1040,M98-1028,0,0.0177751,"ts are applied and their performance compared against hand-crafted lists. In the next section the NE task is described in further detail. 2 NE background 2.1 NE Recognition of Broadcast News The NE task itself was first introduced as part of the MUC6 (MUC, 1995) evaluation exercise and was continued in MUC7 (MUC, 1998). This formulation of the NE task defines seven types of NE: PERSON, ORGANIZATION, LOCATION, DATE, TIME, MONEY and PERCENT. Figure 1 shows a short text marked up in S G M L with NEs in the M U C style. The task was duplicated for the D A R P A / N I S T HUB4 evaluation exercise (Chinchor et al., 1998) but this time the corpus to be processed consisted of single case transcribed speech, rather than mixed case newswire text. Participants were asked to carry out NE recognition on North American broadcast news stories recorded from radio and television and processed by automatic speech recognition (ASR) software. The participants were provided with a training corpus consisting of around 32,000 words of transcribed broadcast news stories from 1997 annotated with NEs. Participants used these text to 290 ""It's a chance to think about first-level questions,"" said Ms. &lt;enamex type=""PERS0N"">Cohn&lt;ena"
A00-1040,M98-1015,0,0.0419794,"Missing"
A00-1040,E99-1001,0,0.149493,"Missing"
A00-1040,C96-1071,1,0.886753,"Missing"
A00-1040,M98-1004,0,\N,Missing
agirre-etal-2012-matching,D11-1072,0,\N,Missing
agirre-etal-2012-matching,E06-1002,0,\N,Missing
agirre-etal-2012-matching,D11-1074,0,\N,Missing
agirre-etal-2012-matching,P11-1115,0,\N,Missing
agirre-etal-2012-matching,P11-1095,0,\N,Missing
agirre-etal-2012-matching,D07-1074,0,\N,Missing
C02-1038,C96-1005,0,\N,Missing
C02-1038,J01-3001,1,\N,Missing
C02-1038,P01-1032,0,\N,Missing
C02-1038,J92-1001,0,\N,Missing
C02-1038,H92-1045,0,\N,Missing
C04-1126,M95-1011,0,0.0359036,"h information to be listed as an event under Soderland’s representation, consequently the MUC events generated from these sentences fall into the nomatch category. It was found that there were eighteen events in the Soderland data set which were not included in the MUC version. This is unexpected since the events in the Soderland corpus should be a subset of those in the MUC corpus. Analysis showed that half of these corresponded to spurious events in the Soderland set which could not be matched onto events in the text. Many of these were caused by problems with the BADGER syntactic analyser (Fisher et al., 1995) used to pre-process the texts before manual analysis stage in which the events were identified. Mistakes in this pre-processing sometimes caused the texts to read as though the sentence contained an event when it did not. We examined the MUC texts themselves to determine whether there was an event rather than relying on the pre-processed output. Of the remaining nine events it was found that the majority (eight) of these corresponded to events in the text which were not listed in the MUC data set. These were not identified as events in the MUC data because of the the strict guidelines, for ex"
C04-1126,C96-1079,0,0.0333617,"s are described within a single sentence. The remainder of this paper is organised as follows. Section 2 describes the formats for representing events used in the MUC and Soderland data sets. Section 3 introduces a common representation scheme which allows events to be compared, a method for classifying types of event matches and a procedure for comparing the two data sets. The results and implications of this experiment are presented in Section 4. Some related work is discussed in Section 5. 2 Event Scope and Representation The topic of the sixth MUC (MUC-6) was management succession events (Grishman and Sundheim, 1996). The MUC-6 data has been commonly used to evaluate IE systems. The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995). The format used to represent events in the MUC-6 corpus is now described. 2.1 MUC Representation Events in the MUC-6 evaluation data are recorded in a nested template structure. This format is useful for representing complex events which have more than one participant, for example, when one executive leaves a post to be replaced by another. Figure 2 is a simpl"
C04-1126,M95-1002,0,0.0407308,"allows events to be compared, a method for classifying types of event matches and a procedure for comparing the two data sets. The results and implications of this experiment are presented in Section 4. Some related work is discussed in Section 5. 2 Event Scope and Representation The topic of the sixth MUC (MUC-6) was management succession events (Grishman and Sundheim, 1996). The MUC-6 data has been commonly used to evaluate IE systems. The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995). The format used to represent events in the MUC-6 corpus is now described. 2.1 MUC Representation Events in the MUC-6 evaluation data are recorded in a nested template structure. This format is useful for representing complex events which have more than one participant, for example, when one executive leaves a post to be replaced by another. Figure 2 is a simplified event from the the MUC-6 evaluation similar to one described by Grishman and Sundheim (1996). This template describes an event in which “John J. Dooner Jr.” becomes chairman of the company “McCann-Erickson”. The MUC templates are"
C04-1126,W02-1010,0,0.0557778,"Missing"
C04-1126,C02-1165,0,\N,Missing
C04-1126,M95-1006,0,\N,Missing
C08-1102,W04-0807,0,0.0652784,"el 3 query and 83 for the more relaxed level 2 query. For this sense, abstracts returned by the level 2 query would be used if 83 or fewer examples were required, otherwise abstracts returned by the level 1 query would be used. Note that the queries submitted to Entrez are restricted so the terms only match against the title and abstract of the PubMed articles. This avoids spurious matches against other parts of the records including metadata and authors’ names. 4 WSD System The basis of our WSD system was developed by Agirre and Mart´ınez (2004a) and participated in the Senseval-3 challenge (Mihalcea et al., 2004) with a performance which was close to the best system for the English and Basque lexical sample tasks. The system has been adapted to the biomedical domain (Stevenson et al., 2008) and has the best reported results over the NLM-WSD corpus (Weeber et al., 2001), a standard data set for evaluation of WSD algorithms in this domain. The system uses a wide range of features which The process of relaxing queries is carried out as are commonly employed for WSD: Local collocations: A total of 41 features which follows. Assume we have an ambiguous term, a, and a set of terms T identified using the pro"
C08-1102,P03-1058,0,0.0241659,"D in the biomedical domain using information derived from UMLS (Humphreys et al., 1998). However, results from SemEval (Agirre et al., 2007) and its predecessors have shown that supervised approaches to WSD generally outperform unsupervised ones. It has also been shown that results obtained from supervised methods improve with access to additional labeled data for training (Ng, 1997). Consequently various techniques for automatically generating training data have been developed. One approach makes use of the fact that different senses of ambiguous words often have different translations (e.g. Ng et al. (2003)). Parallel text is used as training data with the alternative translations serving as sense labels. However, disadvantages of this approach are that the alternative translations do not always correspond to the sense distinctions in the original language and parallel text is not always available. Another approach, developed by Leacock et al. (1998) and extended by Agirre and Mart´ınez (2004b), is to examine a lexical resource, WordNet in both cases, to identify unambiguous terms which are closely related to each of the senses of an ambiguous term. These “monosemous relatives” are used to as qu"
C08-1102,W97-0201,0,0.413193,"icantly improve the performance of a state-of-the-art WSD system. 1 Introduction The resolution of lexical ambiguities has long been considered an important part of the process of understanding natural language. Supervised approaches to Word Sense Disambiguation (WSD) have been shown to perform better than unsupervised ones (Agirre and Edmonds, 2007) but require examples of ambiguous words used in context annotated with the appropriate sense (labeled examples). However these often prove difficult to obtain since manual sense annotation of text is a complex and time consuming process. In fact, Ng (1997) estimated that 16 person years of manual effort would be required to create enough labeled examples to train a wide-coverage WSD system. This c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. limitation is commonly referred to as the data acquisition bottleneck. It is particularly acute in specific domains, such as biomedicine, where terms may have technical usages which only domain experts are likely to be aware of. For example, possible meanings of the term “gangli"
C08-1102,N01-1011,0,0.0339728,"))”. Similarly, level noun-modifier, preposition and sibling. These are |T |− 2 queries return documents containing the identified using heuristic patterns and regular exambiguous term and all but two of the terms pressions applied to PoS tag sequences around the in T . Level 1 queries, the most relaxed, return ambiguous word (Agirre and Mart´ınez, 2004a). Salient bigrams: Salient bigrams within the abdocuments containing the ambiguous term and one of the terms in T . We do not use just the stract with high log-likelihood scores, as described ambiguous term as the query since this does not by Pedersen (2001). Unigrams: Lemmas of all content words contain any information which could discriminate between the possible meanings. Figure 1 shows (nouns, verbs, adjectives, adverbs) in the target the queries which are formed for the ambigu- word’s sentence and, as a separate feature, lemous term “culture” and the three most salient mas of all content words within a 4-word window terms identified for the ‘anthropological culture’ around the target word, excluding those in a list sense. The “matches” column lists the number of corpus-specific stopwords (e.g. “ABSTRACT”, of PubMed abstracts the query matche"
C08-1102,W08-0611,1,0.774651,"ts returned by the level 1 query would be used. Note that the queries submitted to Entrez are restricted so the terms only match against the title and abstract of the PubMed articles. This avoids spurious matches against other parts of the records including metadata and authors’ names. 4 WSD System The basis of our WSD system was developed by Agirre and Mart´ınez (2004a) and participated in the Senseval-3 challenge (Mihalcea et al., 2004) with a performance which was close to the best system for the English and Basque lexical sample tasks. The system has been adapted to the biomedical domain (Stevenson et al., 2008) and has the best reported results over the NLM-WSD corpus (Weeber et al., 2001), a standard data set for evaluation of WSD algorithms in this domain. The system uses a wide range of features which The process of relaxing queries is carried out as are commonly employed for WSD: Local collocations: A total of 41 features which follows. Assume we have an ambiguous term, a, and a set of terms T identified using the process extensively describe the context of the ambiguous in Section 3.1. The first, most specific query, word and fall into two main types: (1) bigrams is formed from the conjunction"
C08-1102,W03-1302,0,0.0296664,"o each of the senses of an ambiguous term. These “monosemous relatives” are used to as query terms for a search engine and the examples returned used as additional training data. In the biomedical domain, Humphrey et al. (2006) use journal descriptors to train models based on the terms which are likely to co-occur with each sense. Liu et al. (2002) used information in UMLS to disambiguate automatically retrieved examples which were then used as labeled training data. The meanings of 35 ambiguous abbreviations were identified by examining the closeness of concepts in the same abstract in UMLS. Widdows et al. (2003) employ a similar approach, although their method also makes use of parallel corpora when available. All of these approaches rely on the existence of an external resource (e.g. parallel text or a domain ontology). In this paper we present a novel approach, inspired by the relevance feedback technique used in IR, which automatically identifes additional training examples using existing labeled data. 3 Generating Examples using Relevance Feedback The aim of relevance feedback is to generate improved search queries based on manual analysis of a set of retrieved documents which has been shown to i"
C08-1102,J98-1006,0,\N,Missing
C08-1102,W04-0813,0,\N,Missing
C08-1102,P04-1036,0,\N,Missing
C08-1102,W04-3204,0,\N,Missing
C12-1054,W04-2214,0,0.0316181,"subject/ 880 a controlled vocabulary of keywords (or subject headings), which are widely used in libraries to catalogue materials and facilitate information access. Similarly, in the medical domain MeSH4 , created and maintained by the National Library of Medicine, provides a controlled vocabulary of medical subject headings. In computational linguistics, WordNet (Fellbaum, 1998) is a commonly used lexical knowledge base that links concepts in various ways. WordNet has been expanded with WordNet domain labels which group together words from different syntactic categories and different senses (Bentivogli et al., 2004). These domain labels are organised into a hierarchical structure. There is a body of previous work on automatically deriving taxonomies and relations from free text. Hearst (1992) was perhaps the earliest significant effort to derive hyponym-hypernym relations from free text using the now eponymous Hearst patterns which code common syntactic forms of the hyponymy pattern (e.g. ‘Vehicles such as Cars’). These patterns were hand-crafted. More recently Snow et al. (2004) developed on this work by using an existing knowledge base to automatically derive lexico-syntactic patterns containing the hy"
C12-1054,C92-2082,0,0.28413,"domain MeSH4 , created and maintained by the National Library of Medicine, provides a controlled vocabulary of medical subject headings. In computational linguistics, WordNet (Fellbaum, 1998) is a commonly used lexical knowledge base that links concepts in various ways. WordNet has been expanded with WordNet domain labels which group together words from different syntactic categories and different senses (Bentivogli et al., 2004). These domain labels are organised into a hierarchical structure. There is a body of previous work on automatically deriving taxonomies and relations from free text. Hearst (1992) was perhaps the earliest significant effort to derive hyponym-hypernym relations from free text using the now eponymous Hearst patterns which code common syntactic forms of the hyponymy pattern (e.g. ‘Vehicles such as Cars’). These patterns were hand-crafted. More recently Snow et al. (2004) developed on this work by using an existing knowledge base to automatically derive lexico-syntactic patterns containing the hyponym-hypernym pairs. An alternative to creating hierarchies of concepts from the pattern-based methods is to use statistical methods. Sanderson and Croft (1999) used an approach t"
C18-1029,P11-1030,0,0.0824852,"Missing"
C18-1029,E17-2068,0,0.0326572,"nd could. Figure 7: Explanation of individual predictions of Logistic Regression classifier on an IMDb62 document using LIME. The bar chart represent the weight given to the most relevant words which are also highlighted in the text. 6 Extending a Neural Model These findings are further validated by applying them to a continuous n-grams-based authorship attribution model recently proposed by Sari et al. (2017). They represented a document as a bag of n-grams features and learned the continuous representation of each feature jointly with the classifier in a shallow feed-forward neural network (Joulin et al., 2017). Sari et al. conducted experiments with three different feature choices: characters, words and their combination. The character-based model outperformed the state-of-the-art on the CCAT50 and IMDb62 datasets, while producing comparable results on the remaining two. We extend their character-based model by incorporating each feature type (style, content and hybrid) as an auxiliary feature represented in discrete form. Auxiliary features provide additional information related to the dataset characteristics. Given xaux as a normalized auxiliary features frequency vector, V is the weight applied"
C18-1029,N16-3020,0,0.0316082,"Missing"
C18-1029,rose-etal-2002-reuters,1,0.440355,"cteristics in terms of the number of authors, topic/genre and document length (see Table 1). Judgment genre # authors # total documents avg characters per document avg words per document legal judgments 3 1,342 11,957 2367 CCAT10 CCAT50 newswire 10 50 1,000 5,000 3,089 3,058 580 584 IMDb62 movie reviews 62 79,550 1,401 288 Table 1: Dataset statistics. 1 Code to reproduce the experiments is available from https://github.com/yunitata/coling2018 344 Judgment consists of legal judgments from three Australian High Court judges while both CCAT datasets are subsets of Reuters Corpus Volume 1 (RCV1) (Rose et al., 2002). The IMDb62 dataset was collected from movie reviews and message board posts of the Internet Movie database. Train/test partitions are provided for both CCAT datasets by the respective authors. For Judgment and IMDb62 we follow previous work (Seroussi et al., 2013) by using 10-fold cross validation in our experiments. We do not make use of datasets from recent authorship attribution shared task events, e.g. PAN (Juola, 2012), due to their relatively small size and fact that they provide a very small number of documents per author. 4 Dataset Analysis The aim of this analysis is to quantify the"
C18-1029,N15-1010,0,0.561458,"gorization. In some cases, where there is a clear topical distinction between the documents written by different authors, content-related features such as those used in text categorization may be effective. However, style-based features are more likely to be effective for datasets containing a more homogeneous set of topics. Previous work on feature exploration for authorship attribution, focused on the overall effectiveness of features without considering the characteristics of the datasets to which they were applied, e.g. (Grieve, 2007; Guthrie, 2008; Stamatatos, 2009; Brennan et al., 2012; Sapkota et al., 2015). A wide range of features have been applied to the authorship attribution problem and many previous studies concluded that using character n-grams is often effective, e.g. (Peng et al., 2003; Koppel et al., 2011; Schwartz et al., 2013; Sapkota et al., 2015; Sari et al., 2017; Shrestha et al., 2017). Thus, character n-grams have become the go-to features for this task to capture both an author’s topical preferences and writing style. This study explores how the characteristics of a dataset affect the usefulness of different types of features for the authorship attribution task. Experiments are"
C18-1029,E17-2043,1,0.800402,"s containing a more homogeneous set of topics. Previous work on feature exploration for authorship attribution, focused on the overall effectiveness of features without considering the characteristics of the datasets to which they were applied, e.g. (Grieve, 2007; Guthrie, 2008; Stamatatos, 2009; Brennan et al., 2012; Sapkota et al., 2015). A wide range of features have been applied to the authorship attribution problem and many previous studies concluded that using character n-grams is often effective, e.g. (Peng et al., 2003; Koppel et al., 2011; Schwartz et al., 2013; Sapkota et al., 2015; Sari et al., 2017; Shrestha et al., 2017). Thus, character n-grams have become the go-to features for this task to capture both an author’s topical preferences and writing style. This study explores how the characteristics of a dataset affect the usefulness of different types of features for the authorship attribution task. Experiments are carried out using four datasets that have previously been widely used for this task. Three types of features are considered: style, content and hybrid (a mixture of the previous two types). In contrast to previous work, this study finds that character n-grams do not perform"
C18-1029,D13-1193,0,0.609556,"tions. We apply the conclusions from our analysis to an extension of an existing approach to authorship attribution and outperform the prior state-of-the-art on two out of the four datasets used. 1 Introduction Authorship attribution plays an important role in many applications, including plagiarism detection and forensic investigation. Approaches to this problem attempt to identify a document’s author through analysis of individual’s writing style and/or topics they tend to write about. The problem has been extensively studied and a wide range of features has been explored (Stamatatos, 2013; Schwartz et al., 2013; Seroussi et al., 2013; H¨urlimann et al., 2015). However, there has been a lack of analysis of the behavior of features across multiple datasets or using a range of classifiers. Consequently, it is difficult to determine which types of features will be most useful for a particular authorship attribution dataset. Authorship attribution is a unique task which is closely related to both the representation of individuals’ writing style and text categorization. In some cases, where there is a clear topical distinction between the documents written by different authors, content-related features su"
C18-1029,E17-2106,0,0.123968,"homogeneous set of topics. Previous work on feature exploration for authorship attribution, focused on the overall effectiveness of features without considering the characteristics of the datasets to which they were applied, e.g. (Grieve, 2007; Guthrie, 2008; Stamatatos, 2009; Brennan et al., 2012; Sapkota et al., 2015). A wide range of features have been applied to the authorship attribution problem and many previous studies concluded that using character n-grams is often effective, e.g. (Peng et al., 2003; Koppel et al., 2011; Schwartz et al., 2013; Sapkota et al., 2015; Sari et al., 2017; Shrestha et al., 2017). Thus, character n-grams have become the go-to features for this task to capture both an author’s topical preferences and writing style. This study explores how the characteristics of a dataset affect the usefulness of different types of features for the authorship attribution task. Experiments are carried out using four datasets that have previously been widely used for this task. Three types of features are considered: style, content and hybrid (a mixture of the previous two types). In contrast to previous work, this study finds that character n-grams do not perform equally well in all data"
C18-2008,W14-3417,1,0.704761,"he paths to yield new connections. Both LBD systems are applied to all PubMed abstracts published up to 30 April 2016: the linguistically motivated subject-relation-object triples (such as X-treats-Y or X-affects-Y) are extracted from a SemRep (Rindflesch and Fiszman, 2003) annotated 2016 version of PubMed (available as semmedVER26 download created using regular SemRep version 1.7 and UMLS 2016AA1 ) and used for the A-B-C model. UMLS 2016AA was used to obtain the manually created triples for the graph model. A range of filtering approaches are applied to reduce the volume of hidden knowledge (Preiss, 2014). Individually, the A-B-C model generated a total of 2,947,874,564 pairs of hidden knowledge, while the graph model yielded 198,295,133 pairs. The intersection of hidden knowledge pairs, 6,471,922 pairs, is presented within the interface, and the hidden knowledge pairs are ranked by the weights output by the graph model. 3 Online System The approach described in Section 2 is implemented as a publicly available tool, HiDE (Hidden Discovery Explorer), which allows a user to interactively explore the hidden knowledge generated by an LBD system. Interaction with HiDE begins with the user specifyin"
C98-2223,C92-4189,0,0.185678,"senses for adjectives list the type which they expect for the noun they modify, senses for adverbs the type they expect of their modifier and verbs list between one and three types (depending on their transitivity) which are the expected semantic types of the verb's subject, direct object and indirect object. G r a m m a t i c a l links between verbs, adjectives and adverbs and the head noun of their arguments arer identified using a specially constructed shallow syntactic analyser (Stevenson, 1998). The semantic classes in L D O C E are not provided with a hierarchy, but, Bruce and Guthrie (Bruce and Guthrie, 1992) manually identified hierarchical relations between tile semantic classes, constructing them into a hierarchy which we use to resolve the restrictions. We resolve the restrictions by returning, for each word, the set of sense which do not break them (that is, those whose semantic category is at the same, or a lower, level in the hierarchy). 4 Combining Knowledge Sources Since each of our partial taggers suggests only possible senses for each word it is necessary to have some method to combine their results. We trained decision lists (Clark and Niblett, 1989) using a supervised learning approac"
C98-2223,H92-1045,0,0.0134226,"d that the new evaluation function led to an improvement in the algorithm's effectiveness. 3.4 Pragmatic Codes Our next partial tagger makes use of the hierarchy of L D O C E pragmatic codes which indicate the likely subject area for a sense. Disambiguation is carried out using a modified version of the simulated annealing algorithm, and a t t e m p t s to optimise the number of pragmatic codes of the same type in the sen~ tence. Rather than processing over single sentences we optimise over entire paragraphs and only for the sense of nouns. We chose this strategy since there is good evidence (Gale et al., 1992) that nouns are best disambiguated by broad contextual considerations, while other parts of speech are resolved by more local factors. 3.5 Seleetional Restrictions L D O C E senses contain simple selectional restrictions for each content word in the dictionary. A set of 35 semantic classes are used, such as H = Hu~ man, M = H u m a n male, P = Plant, S = Solid and so on. Each word sense for a noun is given one of these semantic types, senses for adjectives list the type which they expect for the noun they modify, senses for adverbs the type they expect of their modifier and verbs list between"
C98-2223,W97-0212,0,0.0361733,"p e d onto two or three L D O C E senses when the WordNet sense does not distinguish between them. Tile mapping also contained significant gaps (words and senses not in the translation). S E M C O R contains 91,808 words tagged with WordNet synsets, 6,071 of which are proper nmnes which we ignore, leaving 85,737 words which could potentially be translated. The translation contains only 36,869 words tagged with L D O C E senses, although this is a reasonable size for an evaluation corpus given this type of task (it is several orders of magnitude larger than those used by (Cowie et al., 1992) (Harley and Glennon, 1997) (Mahesh et al., 1997)). This corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any inconsistencies which may occur with a poorly controlled tagging strategy. 6 Results 2['0 date we have tested our system on only a portion of the text we derived from SEMCOR, which consisted of 2021 words tagged with L D O C E senses (and 12,208 words in total). The 2021 word occuranees are made up from 1068 different types, with an average t)olyselny of 7.65. As a baseline against which to compare results we computed the t)ereentage of words which are corr"
C98-2223,1997.tmi-1.18,0,0.0202849,"D O C E senses when the WordNet sense does not distinguish between them. Tile mapping also contained significant gaps (words and senses not in the translation). S E M C O R contains 91,808 words tagged with WordNet synsets, 6,071 of which are proper nmnes which we ignore, leaving 85,737 words which could potentially be translated. The translation contains only 36,869 words tagged with L D O C E senses, although this is a reasonable size for an evaluation corpus given this type of task (it is several orders of magnitude larger than those used by (Cowie et al., 1992) (Harley and Glennon, 1997) (Mahesh et al., 1997)). This corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any inconsistencies which may occur with a poorly controlled tagging strategy. 6 Results 2['0 date we have tested our system on only a portion of the text we derived from SEMCOR, which consisted of 2021 words tagged with L D O C E senses (and 12,208 words in total). The 2021 word occuranees are made up from 1068 different types, with an average t)olyselny of 7.65. As a baseline against which to compare results we computed the t)ereentage of words which are correctly tagged if we cho"
C98-2223,J92-1001,0,0.0242673,"ues to a single phenomenon, WSD? This is a situation quite unlike syntactic parsing or part-of-speech tagging: in the latter case, for example, one can write a Cherry-style rule tagger or an HMM learning model, but there is no reason the believe these represent different types of information, just different ways of conceptualising and coding it. T h a t seems not to tie the case, at first sight, with the many forms of information for WSD. It ix odd that this has not been much discussed in the field. In this work, we shall adopt the methodology first explicitly noted in connection with WSD by (McRoy, 1992), and more recently (Ng and Lee, 1996), namely t h a t of bringing together a number of partial sources of infbrmation about a phenomenon and combining them in a tn'inciI)led manner. Tiffs ix in the AI tradition of combining ""weak"" me.thods for strong results (usually ascribed to Newell (Newell, 1973)) and used in the CRL-NMSU lexical work on the Eighties (Wilks et al., 1990). We shall, in this paper, offer a system that combines the three types of ilffornlation listed above (plus part-of-speech illtering) and, more importantly, applies a learning algorithm to determine the optimal combination"
C98-2223,P96-1006,0,0.13304,"This is a situation quite unlike syntactic parsing or part-of-speech tagging: in the latter case, for example, one can write a Cherry-style rule tagger or an HMM learning model, but there is no reason the believe these represent different types of information, just different ways of conceptualising and coding it. T h a t seems not to tie the case, at first sight, with the many forms of information for WSD. It ix odd that this has not been much discussed in the field. In this work, we shall adopt the methodology first explicitly noted in connection with WSD by (McRoy, 1992), and more recently (Ng and Lee, 1996), namely t h a t of bringing together a number of partial sources of infbrmation about a phenomenon and combining them in a tn'inciI)led manner. Tiffs ix in the AI tradition of combining ""weak"" me.thods for strong results (usually ascribed to Newell (Newell, 1973)) and used in the CRL-NMSU lexical work on the Eighties (Wilks et al., 1990). We shall, in this paper, offer a system that combines the three types of ilffornlation listed above (plus part-of-speech illtering) and, more importantly, applies a learning algorithm to determine the optimal combination of such modules for a given word dist"
C98-2223,C90-2067,0,0.0198715,"Missing"
C98-2223,P95-1026,0,0.0495222,"into a hierarchy which we use to resolve the restrictions. We resolve the restrictions by returning, for each word, the set of sense which do not break them (that is, those whose semantic category is at the same, or a lower, level in the hierarchy). 4 Combining Knowledge Sources Since each of our partial taggers suggests only possible senses for each word it is necessary to have some method to combine their results. We trained decision lists (Clark and Niblett, 1989) using a supervised learning approach. Decision lists have already been successfully applied to lexical ambiguity resolution by (Yarowsky, 1995) where they perfi'omed well. We present the decision list system with a number of training words for which the correct sense 1400 is known. For each of the words we supply each of its possible senses (apart fi'om those removed from consideration by the part-of-speech filter (Section 3.2)) within a context consisting of the results from each of the partial taggers, frequency information and 10 simple collocations (first n o u n / v e r b / p r e p o s i t i o n to the left/right and first/second word to the left/right). Each sense is marked as either a p p r o p r i a t e (if it is tile correct"
C98-2223,H93-1052,0,\N,Missing
C98-2223,W97-0208,1,\N,Missing
E14-4005,N09-1003,0,0.128596,"Missing"
E14-4005,W13-0102,1,0.87551,"equencies. The matrix Z consists of V rows and columns representing the V vocabulary words. Element zij is the log of the number of documents that contains the words i and j normalised by the document frequency, DF, of the word j. Mimno et al. (2011) introduced that metric to measure topic coherence. We adapted it to estimate topic similarity by aggregating the co-document frequency of the words between two topics (Doc-Co-occ). Reference Corpus Semantic Space Topic words can also be represented as vectors in a semantic space constructed from an external source. We adapt the method proposed by Aletras and Stevenson (2013) for measuring topic coherence using distributional semantics1 . 2.5 Top-N Features A semantic space is constructed considering only the top n most frequent words in Wikipedia (excluding stop words) as context features. Each topic word is represented as a vector of n features weighted by computing the Pointwise Mutual Information (PMI) (Church and Hanks, 1989) between the topic word and each context feature, PMI(wi , wj )γ . γ is a variable for assigning more importance to higher PMI values. In our experiments, we set γ = 3 and found that the best performance is obtained for n = 5000. Similari"
E14-4005,D11-1024,0,0.262278,"Missing"
E14-4005,N10-1012,0,0.0267255,"titles weighted by their relevance to the keyword. For each topic, the centroid is computed from the keyword vectors. Similarity between topics is computed as the cosine similarity of the ESA centroid vectors. Topic Word Space Alternatively, we consider only the top-10 topic words from the two topics as context features to generate topic word vectors. Then, topic similarity is computed as the pairwise cosine similarity of the topic word vectors (RCSCos-TWS). 2.6 Feature Combination Using SVR Word Association Topic similarity can also be computed by applying word association measures directly. Newman et al. (2010) measure topic coherence as the average PMI between the topic words. This approach can be adapted to measure We also evaluate the performance of a support vector regression system (SVR) (Vapnik, 1998) with a linear kernel using a combination of approaches described above as features2 . The system is trained and tested using 10-fold cross validation. 1 Wikipedia is used as a reference corpus to count word co-occurrences and frequencies using a context window of ±10 words centred on a topic word. 2 With the exception of JSD, features based on the topics’ word probability distributions were not u"
E14-4005,P89-1010,0,0.568287,"document frequency of the words between two topics (Doc-Co-occ). Reference Corpus Semantic Space Topic words can also be represented as vectors in a semantic space constructed from an external source. We adapt the method proposed by Aletras and Stevenson (2013) for measuring topic coherence using distributional semantics1 . 2.5 Top-N Features A semantic space is constructed considering only the top n most frequent words in Wikipedia (excluding stop words) as context features. Each topic word is represented as a vector of n features weighted by computing the Pointwise Mutual Information (PMI) (Church and Hanks, 1989) between the topic word and each context feature, PMI(wi , wj )γ . γ is a variable for assigning more importance to higher PMI values. In our experiments, we set γ = 3 and found that the best performance is obtained for n = 5000. Similarity between two topics is defined as the average cosine similarity of the topic word vectors (RCS-Cos-N). Knowledge-based Methods UKB (Agirre et al., 2009) is used to generate a probability distribution over WordNet synsets for each word in the vocabulary V of the topic model using the Personalized PageRank algorithm. The similarity between two topic words is c"
E14-4005,D09-1026,0,0.0174585,"hef.ac.uk Abstract topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures such as KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). Kim and Oh (2011) also applied the cosine measure and KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount Cumulative Gain and Jensen Shannon Divergence (JSD). This paper compares a wider range of approaches to measuring topic similarity than previous work. In addition these measures are evaluated directly by comparing them against human judgements. Previous approaches to the problem of measuring similarity between automatically generated topics have been based on comparison"
E14-4005,D12-1087,0,0.0886818,"0.47 0.58 0.61 0.51 0.49 0.51 0.42 0.42 0.47 0.59 0.70 0.61 0.62 0.64 0.57 0.60 0.58 0.42 0.55 0.62 0.41 0.54 0.64 0.54 0.36 0.58 0.57 0.49 0.31 0.43 0.26 0.43 0.34 0.41 0.67 0.40 0.64 0.41 0.70 0.43 0.62 0.42 0.61 0.71 0.62 0.60 0.65 0.66 0.68 0.64 0.67 0.63 0.64 Table 1: Results for various approaches to topic similarity. All correlations are significant p < 0.001. Underlined scores denote best performance of a single feature. Bold denotes best overall performance. rics with human judgements increase with the number of topics for both corpora. This result is consistent with the findings of Stevens et al. (2012) that topic model coherence increases with the number of topics. Fewer topics makes the task of identifying similar topics more difficult because it is likely that they will contain some terms that do not relate to the topic’s main subject. Correlations in CTM are more stable for different number of topics because of the nature of the model, the pairs have been generated using the topic graph which by definition contains correlated topics. ate reliable estimates of tf.idf or co-document frequency. ESA, one of the knowledge-based methods (Section 2.5), performs well and is comparable to (or in"
E14-4005,D08-1038,0,0.0174424,"ons. It seems intuitively plausible that some automatically generated topics will be similar while others are dis-similar. For example, a topic about basketball (team game james season player nba play knicks coach league) is more similar to a topic about football (world cup team soccer africa player south game match goal) than one about the global finance (fed financial banks federal reserve bank bernanke rule crisis credit). Methods for automatically determining the similarity between topics have several potential applications, such as analysis of corpora to determine topics being discussed (Hall et al., 2008) or within topic browsers to decide which topics should be shown together (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a popular type of topic model but cannot capture such correlations unless the semantic similarity between topics is measured. Other 2 Measuring Topic Similarity We compare measures based on word probability distributions (Section 2.1), distributional semantic methods (Sections 2.2-2.4), knowledge-based approaches (Section 2.5) and their combination (Section 2.6). 2.1 Topic Word Probability Di"
E14-4005,J90-1003,0,\N,Missing
E17-2043,bogdanova-lazaridou-2014-cross,0,0.0377882,"Missing"
E17-2043,P11-1030,0,0.335794,"Missing"
E17-2043,D14-1181,0,0.00396687,"ains. 3 p(y|x) = sof tmax(BAx) (1) where x is the frequency vector of features for the document, the weight matrix A is a dictionary containing the embeddings learned for each feature, and B is a weight matrix that is learned to predict the label correctly using the learned representations (essentially averaged feature embeddings). Since the documents in this model are represented as bags of discrete features, sequence information is lost. To recover some of this information we will consider feature n-grams, similar to the way convolutional neural network architectures incorporate word order (Kim, 2014) but with a simpler architecture. The proposed model ignores long-range dependencies that could conceivably be captured using alternative architectures, such as recurrent neural networks (RNN) (Mikolov et al., 2010; Luong et al., 2013). However, topical and stylistic information is contained in shorter word and character sequences for which the shallow neural network architectures with n-gram feature representations are likely to be sufficient, while having the advantage of being much faster to run. This is particularly important for authorship attribution tasks which normally involves documen"
E17-2043,rose-etal-2002-reuters,1,0.427592,"Missing"
E17-2043,W13-3512,0,0.020701,"to predict the label correctly using the learned representations (essentially averaged feature embeddings). Since the documents in this model are represented as bags of discrete features, sequence information is lost. To recover some of this information we will consider feature n-grams, similar to the way convolutional neural network architectures incorporate word order (Kim, 2014) but with a simpler architecture. The proposed model ignores long-range dependencies that could conceivably be captured using alternative architectures, such as recurrent neural networks (RNN) (Mikolov et al., 2010; Luong et al., 2013). However, topical and stylistic information is contained in shorter word and character sequences for which the shallow neural network architectures with n-gram feature representations are likely to be sufficient, while having the advantage of being much faster to run. This is particularly important for authorship attribution tasks which normally involves documents that are much longer than the single sentences which RNNs typically model. Continuous n-grams Representations 4 This work focuses on learning continuous n-gram representations for authorship attribution tasks. Continuous representat"
E17-2043,W06-1657,0,0.0337857,"Missing"
E17-2043,N15-1010,0,0.15788,"chos and Mark Stevenson Department of Computer Science, University of Sheffield, UK {y.sari, a.vlachos, mark.stevenson}@sheffield.ac.uk Abstract Previous studies have found that word and character-level n-grams are the most effective features for identifying authors (Peng et al., 2003; Stamatatos, 2013; Schwartz et al., 2013). Word n-grams can represent local structure of texts and document topic (Coyotl-Morales et al., 2006; Wang and Manning, 2012). On the other hand, character n-grams have been shown to be effective for capturing stylistic and morphological information (Koppel et al., 2011; Sapkota et al., 2015). However, previous work relied on discrete feature representations which suffer from data sparsity and do not consider the semantic relatedness between features. To address this problem we propose the use of continuous n-gram representations learned jointly with the classifier as a feedforward neural network. Continuous n-grams representations combine the advantages of n-grams features and continuous representations. The proposed method outperforms the prior state-of-theart approaches on two out of four datasets while producing comparable results for the remaining two. This paper presents wor"
E17-2043,D13-1193,0,0.0301519,"rd-based approaches on both datasets. In addition, this results also support the superiority of character n-grams that have been reported in the previous work (Peng et al., 2003; Stamatatos, Figure 1: Accuracy on IMDb62 data subset with varying number of authors 5.1 Domain Influence Word vs Character Table 2 demonstrates that the character models are superior to the word models. In particular, we found that models which employ character level n-grams appear to be more suitable for datasets with a large number of authors, i.e. CCAT50 and IMDb62. To explore this further, we ran an addi270 2013; Schwartz et al., 2013). Acknowledgments 5.3 We thank the anonymous reviewers for their comments. The first author would like to acknowledge Indonesia Endowment Fund for Education (LPDP) for support in the form of a doctoral studentship. Feature Contributions An ablation study was performed to further explore the influence of different types of features by removing a single class of n-grams. For this experiment the character model was used on the two CCAT datasets. Three feature types are defined including: References Shlomo Argamon, Casey Whitelaw, Paul Chase, Sobhan Raj Hota, Navendu Garg, and Shlomo Levitan. 2007"
E17-2043,J14-2003,0,\N,Missing
E99-1050,J93-2004,0,0.0226188,"Our proposal is, briefly, to use a pair of taggers with each assigning annotations from the lexical tag-sets we are interested in mapping. These taggers can then be applied to, the same, large body of text and a mapping derived from the distributions of the pair of tag-sets in the corpus. 2 Case Study In order to test this approach we attempted to map together two part of speech tag-sets. We chose this form of linguistic annotation because it is commonly used in NLP systems and reliable taggers are readily available. The tags sets we shall examine are the set used in the Penn Tree Bank (PTB) (Marcus et al., 1993) and the C5 tag-set used by the CLAWS part-of-speech tagger (Garside, 1996). The PTB set consists of 48 annotations while the C5 uses a larger set of 73 tags. A portion of the British National Corpus (BNC), consisting of nearly 9 million words, was used to derive a mapping. One advantage of using the BNC is that it has already been tagged with C5 tags. The first stage was to re-tag our corpus using the Brill tagger (Brill, 1994). This produces a bi-tagged corpus in which each token has two annotations. For example ponders/VBZ/VVZ, which represents the token is ponders assigned the Brill tag VB"
fernando-stevenson-2012-mapping,S07-1006,0,\N,Missing
fernando-stevenson-2012-mapping,S07-1068,0,\N,Missing
fernando-stevenson-2012-mapping,S07-1054,0,\N,Missing
fernando-stevenson-2012-mapping,E09-1005,0,\N,Missing
fernando-stevenson-2012-mapping,P10-1154,0,\N,Missing
fernando-stevenson-2012-mapping,P08-1041,0,\N,Missing
J01-3001,J95-4004,0,0.015163,"Missing"
J01-3001,P91-1034,0,0.178797,"iguate all content words. 1 In addition to avoiding the problems inherent in restricted vocabulary systems, wide coverage systems are more likely to be useful for NLP applications, as discussed by Wilks et al. (1990). A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). Like Cowie, Guthrie, and Guthrie (1992), we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., Brown et al. 1991; Gale, Church, and Yarowsky 1992c; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions. In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge (see Section 2). 1.1 Lexical Knowledge and WSD Syntactic, semantic, and pragmatic information are all potentially useful for WSD"
J01-3001,P94-1020,0,0.0231778,"the best approach to WSD. Currently, machine learning methods (Yarowsky 1995; Rigau, Atserias, and Agirre 1997) and combinations of classifiers (McRoy 1992) have been popular. This paper reports a WSD system employing elements of both approaches. Another source of difference in approach is the proportion of the vocabulary disambiguated. Some researchers have concentrated on producing WSD systems that base results on a limited number of words, for example Yarowsky (1995) and Schtitze (1992) who quoted results for 12 words, and a second group, including Leacock, Towell, and Voorhees (1993) and Bruce and Wiebe (1994), who gave results for just one, namely interest. But limiting the vocabulary on which a system is evaluated can have two serious drawbacks. First, the words used were not chosen by frequency-based sampling techniques and so we have no way of knowing whether or not they are special cases, a point emphasised by Kilgarriff (1997). Secondly, there is no guarantee • Department of Computer Science, 211 Regent Court, Portobello Street, Sheffield $1 4DP, UK (~) 2001 Association for Computational Linguistics Computational Linguistics Volume 27, Number 3 that the techniques employed will be applicable"
J01-3001,H92-1046,0,0.048092,"Missing"
J01-3001,W96-0102,0,0.0347084,"ns and surface form combine to represent the context of each ambiguous word. 4.7 Combining Disambiguation Modules The results from the disambiguation modules (filter, partial taggers, and feature extractor) are then presented to a machine learning algorithm to combine their results. The algorithm we chose was the TIMBLmemory-based learning algorithm (Daelemans et al. 1999). Memory-based learning is another name for exemplar-based learning, as employed by Ng and Lee (Section 2.3). The TiMBLalgorithm has already been used for various NLP tasks including part-of-speech tagging and PP-attachment (Daelemans et al. 1996; Zavrel, Daelemans, and Veenstra 1997). 337 Computational Linguistics Volume 27, Number 3 Like PEBLS, which formed the core of Ng and Lee&apos;s LEXAS system, TiMBLclassifies new examples by comparing them against previously seen cases. The class of the most similar example is assigned. At the heart of this approach is the distance metric A(X, Y) which computes the similarity between instances X and Y. This measure is calculated using the weighted overlap metric shown in (8), which calculates the total distance by computing the sum of the distance between each position in the feature vector. n A(X"
J01-3001,P92-1032,0,0.0950738,"Missing"
J01-3001,H92-1045,0,0.101688,"Missing"
J01-3001,W97-0212,0,0.225792,"s likely that mark-up for a restricted vocabulary can be carried out more rapidly since the subject has to learn the possible senses of fewer words. Among the researchers mentioned above, one must distinguish between, on the one hand, supervised approaches that are inherently limited in performance to the words over which they evaluate because of limited training data and, on the other hand, approaches whose unsupervised learning methodology is applied to only small numbers of words for evaluation, but which could in principle have been used to tag all content words in a text. Others, such as Harley and Glennon (1997) and ourselves Wilks and Stevenson (1998a, 1998b; Stevenson and Wilks 1999), have concentrated on approaches that disambiguate all content words. 1 In addition to avoiding the problems inherent in restricted vocabulary systems, wide coverage systems are more likely to be useful for NLP applications, as discussed by Wilks et al. (1990). A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). Like Cowie, Guthrie, and Guthrie (1992), we shall give r"
J01-3001,J98-1001,0,0.0270454,"Missing"
J01-3001,H93-1051,0,0.192722,"Missing"
J01-3001,1997.tmi-1.18,0,0.0474687,"Missing"
J01-3001,J93-2004,0,0.0487215,"Missing"
J01-3001,J92-1001,0,0.786067,"e are the choice of computational paradigm, the proportion of text words disambiguated, the granularity of the meanings assigned to them, and the knowledge sources used. We will discuss each in turn. Resnik and Yarowsky (1997) noted that, for the most part, part-of-speech tagging is tackled using the noisy channel model, although transformation rules and grammaticostatistical methods have also had some success. There has been far less consensus as to the best approach to WSD. Currently, machine learning methods (Yarowsky 1995; Rigau, Atserias, and Agirre 1997) and combinations of classifiers (McRoy 1992) have been popular. This paper reports a WSD system employing elements of both approaches. Another source of difference in approach is the proportion of the vocabulary disambiguated. Some researchers have concentrated on producing WSD systems that base results on a limited number of words, for example Yarowsky (1995) and Schtitze (1992) who quoted results for 12 words, and a second group, including Leacock, Towell, and Voorhees (1993) and Bruce and Wiebe (1994), who gave results for just one, namely interest. But limiting the vocabulary on which a system is evaluated can have two serious drawb"
J01-3001,P96-1006,0,0.140732,"extremely brief and has not covered large areas of research into WSD. For example, we have not discussed connectionist approaches, as used by Waltz and Pollack (1985), V6ronis and Ide (1990), Hirst (1987), and Cottrell (1984), However, we have attempted to discuss some of the approaches to combining diverse types of linguistic knowledge for WSD and have concentrated on those which are related to the techniques used in our own disambiguation system. Of central interest to our research is the relative contribution of the various knowledge sources which have been applied to the WSD problem. Both Ng and Lee (1996) and Yarowsky (1993) reported some results in the area. However, Ng and Lee reported results for only a single word and Yarowsky considers only words with two possible senses. This paper is an attempt to increase the scope of this research by discussing a disambiguation algorithm which operates over all content words and combines a varied set of linguistic knowledge sources. In addition, we examine the relative effect of each knowledge source to gauge which are the most important, and under what circumstances. We first report an in-depth study of a particular knowledge source, namely partof-sp"
J01-3001,W97-0808,0,0.165911,"Missing"
J01-3001,P97-1007,0,0.0862017,"Missing"
J01-3001,C90-2067,0,0.245305,"Missing"
J01-3001,J96-3009,0,0.0150266,"Missing"
J01-3001,C92-2070,0,0.710194,"on to avoiding the problems inherent in restricted vocabulary systems, wide coverage systems are more likely to be useful for NLP applications, as discussed by Wilks et al. (1990). A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). Like Cowie, Guthrie, and Guthrie (1992), we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., Brown et al. 1991; Gale, Church, and Yarowsky 1992c; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions. In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge (see Section 2). 1.1 Lexical Knowledge and WSD Syntactic, semantic, and pragmatic information are all potentially useful for WSD, as can be demonstrated by consi"
J01-3001,H93-1052,0,0.108222,"Missing"
J01-3001,P95-1026,0,0.281949,"although it has yet to achieve that aim completely. The main sources of divergence are the choice of computational paradigm, the proportion of text words disambiguated, the granularity of the meanings assigned to them, and the knowledge sources used. We will discuss each in turn. Resnik and Yarowsky (1997) noted that, for the most part, part-of-speech tagging is tackled using the noisy channel model, although transformation rules and grammaticostatistical methods have also had some success. There has been far less consensus as to the best approach to WSD. Currently, machine learning methods (Yarowsky 1995; Rigau, Atserias, and Agirre 1997) and combinations of classifiers (McRoy 1992) have been popular. This paper reports a WSD system employing elements of both approaches. Another source of difference in approach is the proportion of the vocabulary disambiguated. Some researchers have concentrated on producing WSD systems that base results on a limited number of words, for example Yarowsky (1995) and Schtitze (1992) who quoted results for 12 words, and a second group, including Leacock, Towell, and Voorhees (1993) and Bruce and Wiebe (1994), who gave results for just one, namely interest. But l"
J01-3001,W97-1016,0,0.0544813,"Missing"
J01-3001,C92-4189,0,\N,Missing
J01-3001,M95-1017,0,\N,Missing
N10-1053,P04-1036,0,0.0316197,"biguity which effect suitability of the approach. 3 Analysis of Ambiguities The three data sets are compared using measures designed to determine the level of ambiguity they contain. Section 3.1 reports results using various widely used measures based on the distribution of senses. Section 3.2 introduces a measure based on the semantic similarity between the possible senses of ambiguous terms. 3.1 Sense Distributions Three measures for characterising the difficulty of WSD data sets based on their sense distribution were used. The first is the widely applied most frequent sense (MFS) baseline (McCarthy et al., 2004), i.e. the proportion of examples for an ambiguous term that are labeled with the commonest sense. The second is number of senses per ambiguous term. The final measure, the entropy of the sense distribution, has been shown to be a good indication of disambiguation difficulty (Kilgarriff and Rosenzweig, 2000). For two of these measures (number of senses and entropy) a higher figure indicates greater ambiguity while for the MFS measure a lower figure indicates a more difficult data set. Table 2 shows the results of computing these measures averaged across all terms in the corpus. For two measure"
N10-1053,W04-0807,0,0.0360596,"examples that can be used as training data for WSD systems have been proposed, including a semisupervised approach based on relevance feedback (Stevenson et al., 2008a). This approach was shown to generate examples that improved the performance of a WSD system for a set of ambiguous terms from the biomedical domain. However, we find that this approach does not perform as well on other data sets. The levels of ambiguity in these data sets are analysed and we suggest this is the reason for this negative result. 1 Application to a Range of Data Sets 2.1 Introduction Several studies, for example (Mihalcea et al., 2004; Pradhan et al., 2007), have shown that supervised approaches to Word Sense Disambiguation (WSD) outperform unsupervised ones. But these rely on labeled training data which is difficult to create and not always available (e.g. (Weeber et al., 2001)). Various techniques for creating labeled training data automatically have been suggested in the literature. Stevenson et al. (2008a) describe a semi-supervised approach that used relevance feedback (Rocchio, 1971) to analyse existing labeled examples and use the information produced to generate further ones. The approach was tested on the biomedic"
N10-1053,W09-2402,0,0.0668605,"Missing"
N10-1053,N04-3012,0,0.0632717,"Missing"
N10-1053,C08-1102,1,0.833551,"Missing"
N10-1053,J98-1006,0,\N,Missing
N10-1053,S07-1016,0,\N,Missing
N10-1053,P00-1064,0,\N,Missing
N10-1053,O97-1002,0,\N,Missing
N13-1016,E09-1005,0,0.0161532,"s, i.e. SIFT features 2 https://developers.google.com/ apis-explorer/#s/customsearch/v1 3 http://en.wikipedia.org Ranking Candidate Images We rank images in the candidates set using graphbased algorithms. The graph is created by treating images as nodes and using similarity scores (textual or visual) between images to weight the edges. 3.3.1 PageRank PageRank (Page et al., 1999) is a graph-based algorithm for identifying important nodes in a graph that was originally developed for assigning importance to web pages. It has been used for a range of NLP tasks including word sense disambiguation (Agirre and Soroa, 2009) and keyword extraction (Mihalcea and Tarau, 2004). Let G = (V, E) be a graph with a set of vertices, V , denoting image candidates and a set of edges, E, denoting similarity scores between two images. For example, sim(Vi , Vj ) indicates the similarity between images Vi and Vj . The PageRank score (P r) over G for an image (Vi ) can be computed by the following equation: P r(Vi ) = d · X Vj ∈C(Vi ) sim(Vi , Vj ) P P r(Vj ) + (1 − d)v sim(Vj , Vk ) Vk ∈C(Vj ) (1) where C(Vi ) denotes the set of vertices which are connected to the vertex Vi . d is the damping factor which is set to the default"
N13-1016,W13-0102,1,0.780223,"t average human annotations). categories (e.g. SPORTS, POLITICS, COMPUTING ). Categories that have more that 80 articles associated with them are considered. These articles are collected to produce a corpus of approximately 60,000 articles generated from 1,461 categories. Documents in the two collections are tokenised and stop words removed. LDA was applied to learn 200 topics from NYT and 400 topics from WIKI. The gensim package6 was used to implement and compute LDA. The hyperparameters (α, β) were set to num of1 topics . Incoherent topics are filtered out by applying the method proposed by Aletras and Stevenson (2013). We randomly selected 100 topics from NYT and 200 topics from WIKI resulting in a data set of 300 topics. Candidate images for these topics were generated using the approach described in Section 3.1, producing a total of 6,000 candidate images (20 for 6 each topic). 4.2 Human judgements of the suitability of each image were obtained using an online crowdsourcing platform, Crowdflower7 . Annotators were provided with a topic (represented as a set of 10 keywords) and a candidate image. They were asked to judge how appropriate the image was as a representation of the main subject of the topic an"
N13-1016,D07-1109,0,0.0489585,"Missing"
N13-1016,W11-2503,0,0.0286949,"entified by applying graph centrality measures which assume that words that cooccurring in text are likely to refer to concepts that are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta et al., 2008; Szeliski, 2010) and these have been combined with NLP (Feng and Lapata, 2010a; Feng and Lapata, 2010b; Agrawal et al., 2011; Bruni et al., 2011). The closest work to our own is the text illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different from, the one explored in our paper. Those approaches used queries that were much smaller (e.g. between one and three words) and more focussed than"
N13-1016,P10-1126,0,0.0423325,"e statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). An alternative approach is to represent a topic using an illustrative image (or set of images). Images have the advantage that they can be understood quickly and are language independent. This is particularly important for applications in which the topics are used to provide an overview of a collection with many topics being shown simultaneously (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). This paper explores the problem of selecting images to illustrate automatically generated topics. Our approa"
N13-1016,N10-1125,0,0.0411802,"e statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). An alternative approach is to represent a topic using an illustrative image (or set of images). Images have the advantage that they can be understood quickly and are language independent. This is particularly important for applications in which the topics are used to provide an overview of a collection with many topics being shown simultaneously (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). This paper explores the problem of selecting images to illustrate automatically generated topics. Our approa"
N13-1016,N09-1041,0,0.0470478,"nes and can provide images that are useful to represent a topic. 1 Introduction Topic models are statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). An alternative approach is to represent a topic using an illustrative image (or set of images). Images have the advantage that they can be understood quickly and are language independent. This is particularly important for applications in which the topics are used to provide an overview of a collection with many topics being shown simultaneously (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). This paper explores the"
N13-1016,C10-2069,0,0.684296,"4DP, UK {n.aletras, m.stevenson}@dcs.shef.ac.uk Abstract Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). Topics generated automatically, e.g. using LDA, are now widely used in Computational Linguistics. Topics are normally represented as a set of keywords, often the n terms in a topic with the highest marginal probabilities. We introduce an alternative approach in which topics are represented using images. Candidate images for each topic are retrieved from the web by querying a search engine using the top n terms. The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image"
N13-1016,P11-1154,0,0.805906,", m.stevenson}@dcs.shef.ac.uk Abstract Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). Topics generated automatically, e.g. using LDA, are now widely used in Computational Linguistics. Topics are normally represented as a set of keywords, often the n terms in a topic with the highest marginal probabilities. We introduce an alternative approach in which topics are represented using images. Candidate images for each topic are retrieved from the web by querying a search engine using the top n terms. The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image and features extrac"
N13-1016,W04-3252,0,\N,Missing
N13-1079,E09-1005,0,0.07218,"isambiguation Only the topic model documents need to be automatically annotated with the chosen WSD system, after this, the WSD system is never applied again (an LDA classification determines the sense distribution) – this is particularly useful for supervised system which frequently have a long execution time. We explore three different types of WSD system: two versions of a knowledge base based system (Section 4.1), an unsupervised system (Section 4.2) and a supervised system (Section 4.3). 4.1 Personalized PageRank (ppr and w2w) We use the freely available3 Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet 3.0. In Section 5 we present results from two options of the Personalized PageRank algorithm: ppr, which performs one PageRank calculation for a whole content, and w2w, which performs one PageRank calculation for every word in the context to be disambiguated. 3 WordNet similarity (sim) We also evaluated another unsupervised approach, the Perl package WordNet::SenseRelate::AllWords (Pedersen and Kolhatkar, 2009), which finds senses of each word in a text based on senses of the surrounding words. The algorithm is invoked with Lesk similarity (Banerjee and Pedersen, 2002). Topic wor"
N13-1079,D07-1109,0,0.120769,"using a small number of hand-tagged examples, but further examples would be required for each new domain. Agirre et al. (2009) automatically construct a thesaurus from texts in a domain which they use for WSD. Unfortunately, performance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing the context with domain information. Cai et al. (2007) use LDA to create an additional feature for a supervised WSD algorithm, by inferring topics for labeled training data. Boyd-Graber et al. (2007) integrate a topic model with WordNet and use it to carry out disambiguation and learn topics simultaneously. Li et al. (2010) use sense paraphrases to estimate probabilities of senses and carry out WSD. Koeling et al. (2005) showed that automatically acquiring the predominant sense of a word from a corpus from the same domain increases performance (over using a predominant sense acquired from a balanced corpus), but their work requires a separate thesaurus to be built for each domain under investigation. Navigli et al. (2011) extracted relevant terms from texts in a domain and used them to in"
N13-1079,D07-1108,0,0.131132,"nt has been shown to be useful information for WSD. For example, Khapra et al. (2010) improve the performance of a graph-based WSD system using a small number of hand-tagged examples, but further examples would be required for each new domain. Agirre et al. (2009) automatically construct a thesaurus from texts in a domain which they use for WSD. Unfortunately, performance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing the context with domain information. Cai et al. (2007) use LDA to create an additional feature for a supervised WSD algorithm, by inferring topics for labeled training data. Boyd-Graber et al. (2007) integrate a topic model with WordNet and use it to carry out disambiguation and learn topics simultaneously. Li et al. (2010) use sense paraphrases to estimate probabilities of senses and carry out WSD. Koeling et al. (2005) showed that automatically acquiring the predominant sense of a word from a corpus from the same domain increases performance (over using a predominant sense acquired from a balanced corpus), but their work requires a separate the"
N13-1079,H92-1045,0,0.5849,"usly. Li et al. (2010) use sense paraphrases to estimate probabilities of senses and carry out WSD. Koeling et al. (2005) showed that automatically acquiring the predominant sense of a word from a corpus from the same domain increases performance (over using a predominant sense acquired from a balanced corpus), but their work requires a separate thesaurus to be built for each domain under investigation. Navigli et al. (2011) extracted relevant terms from texts in a domain and used them to initialize a random walk over the WordNet graph. Our approaches rely on a one sense per topic hypothesis (Gale et al., 1992), making use of topics induced using LDA – we present three novel techniques for exploiting domain information that are employable with any WSD algorithm (unsupervised or supervised). Using any WSD algorithm, we create a sense per topic distribution for each LDA topic, and the classification of a new document into a topic determines the sense distribution of the words within. Once a sense per topic distribution is obtained, no further WSD annotation of new texts is required. Instead of fixing domains, our technique 680 Proceedings of NAACL-HLT 2013, pages 680–684, c Atlanta, Georgia, 9–14 June"
N13-1079,P10-1155,0,0.017832,"roduction Assigning each word its most frequent sense (MFS) is commonly used as a baseline in Word Sense Disambiguation (WSD). This baseline can be difficult to beat, particularly for unsupervised systems which do not have access to the annotated training data used to determine the MFS. However, it has also been shown that unsupervised methods can be used to identify the most likely sense for each ambiguous word type and this approach can be effective for disambiguation (McCarthy et al., 2004). Knowledge of the domain of a document has been shown to be useful information for WSD. For example, Khapra et al. (2010) improve the performance of a graph-based WSD system using a small number of hand-tagged examples, but further examples would be required for each new domain. Agirre et al. (2009) automatically construct a thesaurus from texts in a domain which they use for WSD. Unfortunately, performance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing the context with domain information. Cai et al. (2007) use LDA to create an additional feature for a supervised WSD algori"
N13-1079,H05-1053,0,0.449378,"formance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing the context with domain information. Cai et al. (2007) use LDA to create an additional feature for a supervised WSD algorithm, by inferring topics for labeled training data. Boyd-Graber et al. (2007) integrate a topic model with WordNet and use it to carry out disambiguation and learn topics simultaneously. Li et al. (2010) use sense paraphrases to estimate probabilities of senses and carry out WSD. Koeling et al. (2005) showed that automatically acquiring the predominant sense of a word from a corpus from the same domain increases performance (over using a predominant sense acquired from a balanced corpus), but their work requires a separate thesaurus to be built for each domain under investigation. Navigli et al. (2011) extracted relevant terms from texts in a domain and used them to initialize a random walk over the WordNet graph. Our approaches rely on a one sense per topic hypothesis (Gale et al., 1992), making use of topics induced using LDA – we present three novel techniques for exploiting domain info"
N13-1079,P10-1116,0,0.202081,"ically construct a thesaurus from texts in a domain which they use for WSD. Unfortunately, performance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing the context with domain information. Cai et al. (2007) use LDA to create an additional feature for a supervised WSD algorithm, by inferring topics for labeled training data. Boyd-Graber et al. (2007) integrate a topic model with WordNet and use it to carry out disambiguation and learn topics simultaneously. Li et al. (2010) use sense paraphrases to estimate probabilities of senses and carry out WSD. Koeling et al. (2005) showed that automatically acquiring the predominant sense of a word from a corpus from the same domain increases performance (over using a predominant sense acquired from a balanced corpus), but their work requires a separate thesaurus to be built for each domain under investigation. Navigli et al. (2011) extracted relevant terms from texts in a domain and used them to initialize a random walk over the WordNet graph. Our approaches rely on a one sense per topic hypothesis (Gale et al., 1992), ma"
N13-1079,P04-1036,0,0.220662,"ncrease each time, suggesting that the technique may be useful to improve the performance of any available WSD system. 1 Introduction Assigning each word its most frequent sense (MFS) is commonly used as a baseline in Word Sense Disambiguation (WSD). This baseline can be difficult to beat, particularly for unsupervised systems which do not have access to the annotated training data used to determine the MFS. However, it has also been shown that unsupervised methods can be used to identify the most likely sense for each ambiguous word type and this approach can be effective for disambiguation (McCarthy et al., 2004). Knowledge of the domain of a document has been shown to be useful information for WSD. For example, Khapra et al. (2010) improve the performance of a graph-based WSD system using a small number of hand-tagged examples, but further examples would be required for each new domain. Agirre et al. (2009) automatically construct a thesaurus from texts in a domain which they use for WSD. Unfortunately, performance drops when the thesaurus is combined with information from local context. Stevenson et al. (2011) showed that performance of an unsupervised WSD algorithm can be improved by supplementing"
N13-1079,H93-1061,0,0.152775,"Missing"
N13-1079,rose-etal-2002-reuters,1,0.692026,"tain it). This technique is not applicable to non bag-of-words WSD algorithms, as structure is absent. 4 4.2 Available from http://ixa2.si.ehu.es/ukb/ 682 4.3 Vector space model (vsm) An existing vector space model (VSM) based stateof-the-art supervised WSD system with features derived from the text surrounding the ambiguous word (Stevenson et al., 2008) is trained on Semcor (Miller et al., 1993).4 5 5.1 Experiments Data The approach is evaluated using a domain-specific WSD corpus (Koeling et al., 2005) which includes articles from the FINANCE and SPORTS domains taken from the Reuters corpus (Rose et al., 2002). This corpus contains 100 manually annotated instances (from each domain) for 41 words.5 The word-sense LDA topic models are created from 80,128 documents randomly selected from the Reuters corpus (this corresponds to a tenth of the entire Reuters corpus). LDA can abstract a model from a relatively small corpus and a tenth of the Reuters corpus is much more manageable in terms of memory and time requirements, particularly given the need to word sense disambiguate (some part of) each document in this dataset.6 4 A version of Semcor automatically transformed to WordNet 3.0 available from http:/"
N13-1079,W08-0611,1,0.899659,"Missing"
N13-1079,N09-5005,0,\N,Missing
N13-1079,S07-1053,0,\N,Missing
N13-3001,W12-2429,1,0.786773,"Missing"
N13-3001,M98-1007,0,0.332621,"tions (Schuemie et al., 2005). Identifying the correct interpretation of ambiguous terms is important to ensure that the text can be processed appropriately. Many WSD systems developed for biomedical documents are based on supervised learning, for example (McInnes et al., 2007; Martinez and Baldwin, An alternative approach is to use automatically labeled examples which can be generated without manual annotation (Leacock et al., 1998). These have been used to generate an all-words WSD system that assigns senses from WordNet (Zhong and Ng, 2010). For biomedical documents the UMLS Metathesaurus (Humphreys et al., 1998b) is a more suitable lexical resource than WordNet and techniques have been developed to create automatically labeled examples for this resource (Stevenson and Guo, 2010). However, to date, automatically labeled examples have only been used as substitutes for ambiguous terms for which manually labeled examples are not available, rather than using them to create a WSD system that can resolve a wider range of ambiguities in biomedical documents. DALE (Disambiguation using Automatically Labeled Examples) is an online WSD system for biomedical documents that was developed by creating automaticall"
N13-3001,J98-1006,0,0.131614,"rocessing system since language contains ambiguous terms which can be difficult to interpret. Ambiguous terms that are found in biomedical documents include words, phrases and abbreviations (Schuemie et al., 2005). Identifying the correct interpretation of ambiguous terms is important to ensure that the text can be processed appropriately. Many WSD systems developed for biomedical documents are based on supervised learning, for example (McInnes et al., 2007; Martinez and Baldwin, An alternative approach is to use automatically labeled examples which can be generated without manual annotation (Leacock et al., 1998). These have been used to generate an all-words WSD system that assigns senses from WordNet (Zhong and Ng, 2010). For biomedical documents the UMLS Metathesaurus (Humphreys et al., 1998b) is a more suitable lexical resource than WordNet and techniques have been developed to create automatically labeled examples for this resource (Stevenson and Guo, 2010). However, to date, automatically labeled examples have only been used as substitutes for ambiguous terms for which manually labeled examples are not available, rather than using them to create a WSD system that can resolve a wider range of amb"
N13-3001,P08-3009,0,0.0244724,"medical documents that was developed by creating automatically labeled examples for all ambiguous terms in the UMLS Metathesaurus. DALE is 1 Proceedings of the NAACL HLT 2013 Demonstration Session, pages 1–4, c Atlanta, Georgia, 10-12 June 2013. 2013 Association for Computational Linguistics able to identify a meaning for any term that is ambiguous in the Metathesaurus and therefore has far greater coverage of ambiguous terms than other supervised WSD systems. Other all-words WSD systems for biomedical documents are unsupervised and do not have as high accuracy as supervised approaches, e.g. (McInnes, 2008; Agirre et al., 2010). An unsupervised WSD algorithm (Humphreys et al., 1998a) is included in MetaMap (Aronson and Lang, 2010) but is unable to resolve all types of sense distinction. 2 2.1 The DALE System Automatically Labeling Examples DALE assigns Concept Unique Identifiers (CUIs) from the UMLS Metathesaurus. The WSD algorithm in DALE is based around a supervised algorithm (Stevenson et al., 2008) trained using automatically labeled examples. The examples are generated using two methods: Monosemous relatives and Co-occurring concepts (Stevenson and Guo, 2010). Both approaches take a single"
N13-3001,P10-4014,0,0.0356474,"are found in biomedical documents include words, phrases and abbreviations (Schuemie et al., 2005). Identifying the correct interpretation of ambiguous terms is important to ensure that the text can be processed appropriately. Many WSD systems developed for biomedical documents are based on supervised learning, for example (McInnes et al., 2007; Martinez and Baldwin, An alternative approach is to use automatically labeled examples which can be generated without manual annotation (Leacock et al., 1998). These have been used to generate an all-words WSD system that assigns senses from WordNet (Zhong and Ng, 2010). For biomedical documents the UMLS Metathesaurus (Humphreys et al., 1998b) is a more suitable lexical resource than WordNet and techniques have been developed to create automatically labeled examples for this resource (Stevenson and Guo, 2010). However, to date, automatically labeled examples have only been used as substitutes for ambiguous terms for which manually labeled examples are not available, rather than using them to create a WSD system that can resolve a wider range of ambiguities in biomedical documents. DALE (Disambiguation using Automatically Labeled Examples) is an online WSD sy"
P05-1047,P02-1022,0,0.0163776,"er of documents in which a pattern can occur since these very frequent patterns are often too general to be useful for IE. Patterns which occur in more than β × C, where C is the number of documents in the collection, are not learned. For the experiments in this paper we set α to 2 and β to 0.3. 4 Implementation A number of pre-processing stages have to be applied to documents in order for the set of patterns to be extracted before learning can take place. Firstly, items belonging to semantic categories are identified by running the text through the named entity identifier in the GATE system (Cunningham et al., 2002). The corpus is then parsed, using a version of MINIPAR (Lin, 1999) adapted to process text marked with named entities, to produce dependency trees from which SVO-patterns are extracted. Active and passive voice is taken into account in MINIPAR’s output so the sentences “COMPANY fired their C.E.O.” and “The C.E.O. was fired by COMPANY” would yield the same triple, COMPANY+fire+ceo. The indirect object of ditransitive verbs is not extracted; these verbs are treated like transitive verbs for the purposes of this analysis. An implementation of the algorithm described in Section 3 was completed in"
P05-1047,O97-1002,0,0.101703,"similarity between elements of a vector and would assign equal similarity to each pair of patterns in the example shown in Figure 1.1 The semantic similarity matrix in Equation 1 provides a mechanism to capture semantic similarity between lexical items which allows us to identify chairman+resign and ceo+quit as the most similar pair of patterns. 3.2 Populating the Matrix It is important to choose appropriate values for the elements of W . We chose to make use of the research that has concentrated on computing similarity between pairs of lexical items using the WordNet hierarchy (Resnik, 1995; Jiang and Conrath, 1997; Patwardhan et al., 2003). We experimented with several of the measures which have been reported in the literature and found that the one proposed by Jiang and Conrath (1997) to be the most effective. The similarity measure proposed by Jiang and Conrath (1997) relies on a technique developed by Resnik (1995) which assigns numerical values to each sense in the WordNet hierarchy based upon the amount of information it represents. These values are derived from corpus counts of the words in the synset, either directly or via the hyponym relation and are used to derive the Information Content (IC)"
P05-1047,P03-1017,0,0.00939802,"ame, country, currency values, numerical expressions etc. In this paper lexical items are represented in lower case and semantic categories are capitalised. For example, in the pattern COMPANY+fired+ceo, fired and ceo are lexical items and COMPANY a semantic category which could match any lexical item belonging to that type. The algorithm described here relies on identifying patterns with similar meanings. The approach we have developed to do this is inspired by the vector space model which is commonly used in Information Retrieval (Salton and McGill, 1983) and language processing in general (Pado and Lapata, 2003). Each pattern can be represented as a set of pattern element-filler pairs. For example, the pattern COMPANY+fired+ceo consists of three pairs: subject COMPANY, verb fired and object ceo. Each pair consists of either a lexical item or semantic category, and pattern element. Once an appropriate set of pairs has been established a pattern can be represented as a binary vector in which an element with value 1 denotes that the pattern contains a particular pair and 0 that it does not. 3.1 Pattern Similarity The similarity of two pattern vectors can be compared using the measure shown in Equation 1"
P05-1047,rose-etal-2002-reuters,1,0.362741,"ted with it. 590 documents from a version of the MUC-6 evaluation corpus described by Soderland (1999) were used. After the pre-processing stages described in Section 4, the MUC-6 corpus produced 15,407 pattern tokens from 11,294 different types. 10,512 patterns appeared just once and these were effectively discarded since our learning algorithm only considers patterns which occur at least twice (see Section 3.3). The document-centric approach benefits from a large corpus containing a mixture of relevant and irrelevant documents. We provided this using a subset of the Reuters Corpus Volume I (Rose et al., 2002) which, like the MUC-6 corpus, consists of newswire COMPANY+appoint+PERSON COMPANY+elect+PERSON COMPANY+promote+PERSON COMPANY+name+PERSON PERSON+resign PERSON+depart PERSON+quit 0.80 0.75 F-measure 0.70 Table 1: Seed patterns for extraction task 0.65 0.60 0.55 texts. 3000 documents relevant to the management succession task (identified using document metadata) and 3000 irrelevant documents were used to produce the supplementary corpus. This supplementary corpus yielded 126,942 pattern tokens and 79,473 types with 14,576 of these appearing more than once. Adding the supplementary corpus to the"
P05-1047,P03-1029,0,0.356858,"algorithm which could be used to complement manual approaches. These results could be improved by manual filtering the patterns identified by the algorithm. The learning algorithm presented in Section 3 includes a mechanism for comparing two extraction patterns using information about lexical similarity derived from WordNet. This approach is not restricted to this application and could be applied to other language processing tasks such as question answering, paraphrase identification and generation or as a variant of the vector space model commonly used in Information Retrieval. In addition, Sudo et al. (2003) proposed representations for IE patterns which extends the SVO representation used here and, while they did not appear to significantly improve IE, it is expected that it will be straightforward to extend the vector space model to those pattern representations. One of the reasons for the success of the approach described here is the appropriateness of WordNet which is constructed on paradigmatic principles, listing the words which may be substituted for one another, and is consequently an excellent resource for this application. WordNet is also a generic resource not associated with a particu"
P05-1047,C00-2136,0,0.877923,"pproach. 1 Introduction Developing systems which can be easily adapted to new domains with the minimum of human intervention is a major challenge in Information Extraction (IE). Early IE systems were based on knowledge engineering approaches but suffered from a knowledge acquisition bottleneck. For example, Lehnert et al. (1992) reported that their system required around 1,500 person-hours of expert labour to modify for a new extraction task. One approach to this problem is to use machine learning to automatically learn the domain-specific information required to port a system (Riloff, 1996). Yangarber et al. (2000) proposed an algorithm for learning extraction patterns for a small number of examples which greatly reduced the burden on the application developer and reduced the knowledge acquisition bottleneck. Extraction patterns are potentially useful for many language processing tasks, including question answering and the identification of lexical relations (such as meronomy and hyponymy). In addition, IE patterns encode the different ways in which a piece of information can be expressed in text. For example, “Acme Inc. fired Jones”, “Acme Inc. let Jones go”, and “Jones was given notice by his employer"
P05-1047,P03-1044,0,0.706122,"ribe an implementation (Section 4). Two evaluation regimes are described; one based on the identification of relevant documents and another which aims to identify sentences in a corpus which 379 Proceedings of the 43rd Annual Meeting of the ACL, pages 379–386, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics are relevant for a particular IE task (Section 5). Results on each of these evaluation regimes are then presented (Sections 6 and 7). 2 Extraction Pattern Learning We begin by outlining the general process of learning extraction patterns, similar to one presented by (Yangarber, 2003). 1. For a given IE scenario we assume the existence of a set of documents against which the system can be trained. The documents are unannotated and may be either relevant (contain the description of an event relevant to the scenario) or irrelevant although the algorithm has no access to this information. 2. This corpus is pre-processed to generate the set of all patterns which could be used to represent sentences contained in the corpus, call this set S. The aim of the learning process is to identify the subset of S representing patterns which are relevant to the IE scenario. 3. The user pro"
P05-1047,M91-1033,0,\N,Missing
P05-1047,M92-1038,0,\N,Missing
P07-1006,2006.iwslt-evaluation.5,0,0.0240444,"translation for an ambiguous source word. There is not always a direct relation between the possible senses for a word in a (monolingual) lexicon and its translations to a particular language, so this represents a different task to WSD against a (monolingual) lexicon (Hutchins and Somers, 1992). Although it has been argued that WSD does not yield better translation quality than a machine translation system alone, it has been recently shown that a WSD module that is developed following specific multilingual requirements can significantly improve the performance of a machine translation system (Carpuat et al., 2006). This paper focuses on the application of our approach to the translation of verbs in English to Portuguese translation, specifically for a set of 10 mainly light and highly ambiguous verbs. We also experiment with a monolingual task by using the verbs from Senseval-3 lexical sample task. We explore knowledge from 12 syntactic, semantic and pragmatic sources. In principle, the proposed approach could also be applied to any lexical disambiguation task by customizing the sense reposi42 tory and knowledge sources. In the remainder of this paper we first present related approaches to WSD and disc"
P07-1006,W04-0824,0,0.025698,"olingual task (Senseval-3 verbs with fine-grained sense distinctions and using the evaluation system provided by Senseval). It also shows the average accuracy of the most frequent sense and accuracies reported on the same set of verbs by the best systems submitted by the sites which participated in this task. Syntalex-3 (Mohammad and Pedersen, 2004) is based on an ensemble of bagged decision trees with narrow context part-of-speech features and bigrams. CLaC1 (Lamjiri et al., 2004) uses a Naive Bayes algorithm with a dynamically adjusted context window around the target word. Finally, MC-WSD (Ciaramita and Johnson, 2004) is a multi-class averaged perceptron classifier using syntactic and narrow context features, with one component trained on the data provided by Senseval and other trained on WordNet glosses. System Majority sense Syntalex-3 CLaC1 MC-WSD Aleph a small number of rules (from 6, for verbs with a few examples, to 88) and all knowledge sources are used across different rules and verbs. In general, results from both multilingual and monolingual tasks demonstrate that the hypothesis put forward in Section 1, that ILP’s ability to generate expressive rules which combine and integrate a wide range of k"
P07-1006,P96-1006,0,0.287988,"2 Related Work WSD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schütze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001). Hybrid approaches can combine advantages from both strategies, potentially yielding accurate and comprehensive systems, particularly when deep knowledge is explored. Linguistic knowledge is available in electronic resources suitable for practical use, such as WordNet (Fellbaum, 1998), dictionaries and parsers. However, the use of this information has been hampered by the limitations of the modeling techniques that have been explored so far: using deep sources of domain knowledge is beyond the capabilities of such techniques, which are in general based on attribute"
P07-1006,P05-1006,0,0.0608579,"Missing"
P07-1006,W04-0833,0,0.0295984,"Missing"
P07-1006,P93-1016,0,0.0121142,"igram(snt1, back, as). has_bigram(snt1, such, a). … KS3. Narrow context containing 5 content words to the right and left of the verb, identified using POS tags, represented by has_narrow(snt, word_position, word): has_narrow(snt1, 1st_word_left, mind). has_narrow(snt1, 1st_word_right, back). … KS4. POS tags of 5 words to the right and left of the verb, represented by has_pos(snt, word_position, pos): has pos(snt1, 1st_word_left, nn). has pos(snt1, 1st_word_right, rb). … Table 1. Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech (POS) tagged using Minipar (Lin, 1993) and 44 KS5. 11 collocations of the verb: 1st preposition to the right, 1st and 2nd words to the left and right, 1st noun, 1st adjective, and 1st verb to the left and right. These are represented using definitions of the form has_collocation(snt, type, collocation): has_collocation(snt1, 1st_prep_right, back). has_collocation(snt1, 1st_noun_left, mind).… KS6. Subject and object of the verb obtained using Minipar and represented by has_rel(snt, type, word): has_rel(snt1, subject, i). has_rel(snt1, object, nil). … KS7. Grammatical relations not including the target verb also identified using Min"
P07-1006,W04-0807,0,0.181853,"lation. Sense ambiguity has been recognized as one of the most important obstacles Maria das Graças V. Nunes NILC/ICMC University of São Paulo Caixa Postal 668, 13560-970 São Carlos, SP, Brazil gracan@icmc.usp.br to successful language understanding since the early 1960’s and many techniques have been proposed to solve the problem. Recent approaches focus on the use of various lexical resources and corpus-based techniques in order to avoid the substantial effort required to codify linguistic knowledge. These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al., 2004 for an overview of state-ofthe-art systems). However, current approaches rely on limited knowledge representation and modeling techniques: traditional machine learning algorithms and attribute-value vectors to represent disambiguation instances. This has made it difficult to exploit deep knowledge sources in the generation of the disambiguation models, that is, knowledge that goes beyond simple features extracted directly from the corpus, like bags-of-words and collocations, or provided by shallow natural language tools like part-of-speech taggers. In this paper we present a novel approach fo"
P07-1006,W04-0839,0,0.0150978,"s for both Aleph and the other learning algorithms. Values that yielded the best average predictive accuracy in the training sets were assumed to be optimal and used to evaluate the test sets. 4.2 Monolingual task Table 3 shows the average accuracy obtained by Aleph in the monolingual task (Senseval-3 verbs with fine-grained sense distinctions and using the evaluation system provided by Senseval). It also shows the average accuracy of the most frequent sense and accuracies reported on the same set of verbs by the best systems submitted by the sites which participated in this task. Syntalex-3 (Mohammad and Pedersen, 2004) is based on an ensemble of bagged decision trees with narrow context part-of-speech features and bigrams. CLaC1 (Lamjiri et al., 2004) uses a Naive Bayes algorithm with a dynamically adjusted context window around the target word. Finally, MC-WSD (Ciaramita and Johnson, 2004) is a multi-class averaged perceptron classifier using syntactic and narrow context features, with one component trained on the data provided by Senseval and other trained on WordNet glosses. System Majority sense Syntalex-3 CLaC1 MC-WSD Aleph a small number of rules (from 6, for verbs with a few examples, to 88) and all"
P07-1006,W97-0213,0,0.0198365,"efits from more specific knowledge sources, such as the verb’s relation to other items in the sentence (for example, by analysing the semantic type of its subject and object). Consequently, we believe that the disambiguation of verbs is task to which ILP is particularly wellsuited. Therefore, this paper focuses on the disambiguation of verbs, which is an interesting task since much of the previous work on WSD has concentrated on the disambiguation of nouns. WSD is usually approached as an independent task, however, it has been argued that different applications may have specific requirements (Resnik and Yarowsky, 1997). For example, in machine translation, WSD, or translation disambiguation, is responsible for identifying the correct translation for an ambiguous source word. There is not always a direct relation between the possible senses for a word in a (monolingual) lexicon and its translations to a particular language, so this represents a different task to WSD against a (monolingual) lexicon (Hutchins and Somers, 1992). Although it has been argued that WSD does not yield better translation quality than a machine translation system alone, it has been recently shown that a WSD module that is developed fo"
P07-1006,J98-1004,0,0.029006,"itations (Section 2). We then describe some basic concepts on ILP and our application of this technique to WSD (Section 3). Finally, we described our experiments and their results (Section 4). 2 Related Work WSD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schütze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001). Hybrid approaches can combine advantages from both strategies, potentially yielding accurate and comprehensive systems, particularly when deep knowledge is explored. Linguistic knowledge is available in electronic resources suitable for practical use, such as WordNet (Fellbaum, 1998), dictionaries and parsers. However, the use of this information has been hampered by the li"
P07-1006,P06-3010,1,0.851114,"s a subject B that is a proper noun (nnp) or a personal pronoun (prp). 4 Experiments and results To assess the performance of the approach the model produced for each verb was tested on the corresponding set of test cases by applying the rules in a decision-list like approach, i.e., retaining the order in which they were produced and backing off to the most frequent sense in the training set to classify cases that were not covered by any of the rules. All the knowledge sources were made available to be used by the inference engine, since previous experiments showed that they are all relevant (Specia, 2006). In what follows we present the results and discuss each task. 4.1 with three learning algorithms frequently used for WSD, which rely on knowledge represented as attribute-value vectors: C4.5 (decision-trees), Naive Bayes and Support Vector Machine (SVM)1. In order to represent all knowledge sources in attribute-value vectors, KS2, KS7, KS9 and KS10 had to be pre-processed to be transformed into binary attributes. For example, in the case of selectional restrictions (KS9), one attribute was created for each possible sense of the verb and a true/false value was assigned to it depending on whet"
P07-1006,J01-3001,1,0.881631,"SD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schütze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001). Hybrid approaches can combine advantages from both strategies, potentially yielding accurate and comprehensive systems, particularly when deep knowledge is explored. Linguistic knowledge is available in electronic resources suitable for practical use, such as WordNet (Fellbaum, 1998), dictionaries and parsers. However, the use of this information has been hampered by the limitations of the modeling techniques that have been explored so far: using deep sources of domain knowledge is beyond the capabilities of such techniques, which are in general based on attribute-value vector representation"
P07-1006,C96-1005,0,\N,Missing
P07-1006,P95-1026,0,\N,Missing
P13-4026,W12-1014,1,0.797453,"ed. Consequently links to relevant articles in Wikipedia were added to each the meta-data of each artefact using Wikipedia Miner (Milne and Witten, 2008) to provide background information. In addition to the link, Wikipedia Miner returns a confidence value between 0 and 1 for each link based on the context of the item. The accuracy of the links added by Wikipedia Miner were evaluated using the meta-data associated with 21 randomly selected artefacts. Three annotators analysed the links added and found that a confidence value of 0.5 represented a good balance between accuracy and coverage. See Fernando and Stevenson (2012) for further details. Filtered 466,958 1,219,731 14,983 1,701,672 Table 1: Number of artefacts in Europeana collections before and after filtering 4 Data Processing A range of pre-preprocessing steps were carried out on these collections to provide additional information to support navigation in the PATHS system. 4.1 Artefact Similarity 4.3 We begin by computing the similarity between the various artefacts in the Europeana collections. This information is useful for navigation and recommendation but is not available in the Europeana collections since they are drawn from a diverse range of sour"
P13-4026,C12-1054,1,0.879666,"Missing"
P13-4026,W07-0907,0,0.022716,"this information is used within the system. 1 2 Related Work Heitzman et al. (1997) describe the ILEX system which acts as a guide through the jewellery collection of the National Museum of Scotland. The user explores the collection through a set of web pages which provide descriptions of each artefact that are personalised for each user. The system makes use of information about the artefacts the user has viewed to build up a model of their interests and uses this to customise the descriptions of each artefact and provide recommendations for further artefacts in which they may be interested. Grieser et al. (2007) also explore providing recommendations based on the artefacts a user has viewed so far. They make use of a range of techniques including language modelling, geospatial modelling and analysis of previous visitors’ behaviour to provide recommendations to visitors to the Melbourne Museum. Grieser et al. (2011) explore methods for determining the similarity between museum artefacts, commenting that this is useful for navigation through these collections and important for personalisation (Bowen and Filippini-Fantoni, 2004; O’Donnell et al., 2001), recommendation (Bohnert et al., 2009; Trant, 2009)"
P14-2103,N13-1016,1,0.775279,"Missing"
P14-2103,C10-2069,0,0.216961,"ent Court, 211 Portobello Sheffield, S1 4DP United Kingdom {n.aletras, m.stevenson}@dcs.shef.ac.uk Abstract could be labelled as E DUCATION and a suitable label for the topic shown above would be G LOBAL F INANCIAL C RISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al. (2010) proposed selecting the most representative word from a topic as its label. Hulpus et al. (2013) make use of structured data from DBpedia to label topics. Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. Additional labels are then generated by chunk parsing the article titles to identify n-grams that represent Wikipedia articles as well. Outlier labels (less relevant to the topic) are identified and removed. Finally, the top-5 topic terms a"
P14-2103,P11-1154,0,0.217497,"ic shown above would be G LOBAL F INANCIAL C RISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al. (2010) proposed selecting the most representative word from a topic as its label. Hulpus et al. (2013) make use of structured data from DBpedia to label topics. Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. Additional labels are then generated by chunk parsing the article titles to identify n-grams that represent Wikipedia articles as well. Outlier labels (less relevant to the topic) are identified and removed. Finally, the top-5 topic terms are added to the candidate set. The labels are ranked using Support Vector Regression (SVR) (Vapnik, 1998) and features extracted using word association measures (i.e. PMI,"
P14-2103,N13-3002,0,0.0235252,"d generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods. 1 Introduction Topic models (Hofmann, 1999; Blei et al., 2003) have proved to be a useful way to represent the content of document collections, e.g. (Chaney and Blei, 2012; Ganguly et al., 2013; Gretarsson et al., 2012; Hinneburg et al., 2012; Snyder et al., 2013). In these interfaces, topics need to be presented to users in an easily interpretable way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL , BANK , MARKET, GOVERNMENT, MORTGAGE , BAILOUT, BILLION , STREET, WALL , CRISIS . But interpreting such lists is not always straightforward, particularly since background knowledge may be required (Chang et al., 2009). Textual labels could assist with the interpr"
P14-2103,W04-3252,0,\N,Missing
P15-2045,D11-1049,0,0.085884,"Missing"
P15-2045,N13-1095,0,0.0510502,"ned using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same"
P15-2045,P05-1022,0,0.0134751,"stant supervision is carried out for a target UMLS relation by identifying instance pairs and using them to create a set of positive instance pairs. Any pairs which also occur as an instance pair of another UMLS relation are removed from this set. A set of negative instance pairs is then created by forming new combinations that do not occur within the positive instance pairs. Sentences containing a positive or negative instance pair are then extracted to generate positive and negative training examples for the relation. These candidate sentences are then stemmed (Porter, 1997) and PoS tagged (Charniak and Johnson, 2005). The sets of positive and negative training examples are then filtered to remove sentences that meet any of the following criteria: contain the same positive pair more than once; contain both a positive and negative pair; more than 5 words between the two elements of the instance pair; contain very common instance pairs. 3.2 weight 10.53 6.17 2.80 -0.06 Table 1: Example PRA-induced paths and weights for the NCI relation biological-process-involvesgene-product. The paths induced by PRA are used to identify potential false negatives in the negative training examples (Section 3.1). Each negative"
P15-2045,P09-1113,0,0.161407,"inimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take in"
P15-2045,Q13-1030,0,0.0488415,"supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not nece"
P15-2045,W11-0902,0,0.391962,"Missing"
P15-2045,D12-1042,0,0.190986,"sed as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled training set. This work proposes a novel approach to the problem by applying an in"
P15-2045,P12-1076,0,0.159924,"On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled training set. This work proposes a novel approach to the problem by applying an inference learning method to identify potential false negatives in distantly labelled data. Our method makes use of a modified version of PRA to learn relation paths from a knowledge base and uses this information to identify false negatives. Introduction Dista"
P15-2045,P13-2141,0,0.168429,"quently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different rel"
P15-2045,P10-1030,0,\N,Missing
P15-2045,P11-2048,0,\N,Missing
P98-2228,H92-1022,0,0.0104855,"filters and partial taggers. A filter removes senses from consideration, thereby reducing the complexity of the disambiguation task. Each partial tagger makes use of a different knowledge source from the lexicon and uses it to suggest a set of possible senses for each ambiguous word in context. None of these modules performs the disambiguation alone but they are combined to make use of all of their results. 3.1 Preprocessing Before the filters or partial taggers are applied the text is tokenised, lemmatised, split into sentences and part-of-speech tagged using the Brill part-ofspeech tagger (Brill, 1992). Our system disambiguates only the content words in the text 1 (the part-of-speech tags assigned by 1We define content words as nouns, verbs, adjectives and adverbs, prepositions are not included in this class. 1399 Brill&apos;s tagger are used to decide which are content words). 3.2 Part-of-speech Previous work (Wilks and Stevenson, 1998) showed that part-of-speech tags can play an important role in the disambiguation of word senses. A small experimentwas carried out on a 1700 word corpus taken from the Wall Street Journal and, using only part-ofspeech tags, an attempt was made to find the correc"
P98-2228,C92-4189,0,0.173559,"emantic types, senses for adjectives list the type which they expect for the noun they modify, senses for adverbs the type they expect of their modifier and verbs list between one and three types (depending on their transitivity) which are the expected semantic types of the verb&apos;s subject, direct object and indirect object. Grammatical links between verbs, adjectives and adverbs and the head noun of their arguments arer identified using a specially constructed shallow syntactic analyser (Stevenson, 1998). The semantic classes in LDOCE are not provided with a hierarchy, but, Bruce and Guthrie (Bruce and Guthrie, 1992) manually identified hierarchical relations between the semantic classes, constructing them into a hierarchy which we use to resolve the restrictions. We resolve the restrictions by returning, for each word, the set of sense which do not break them (that is, those whose semantic category is at the same, or a lower, level in the hierarchy). 4 Combining Knowledge Sources Since each of our partial taggers suggests only possible senses for each word it is necessary to have some method to combine their results. We trained decision lists (Clark and Niblett, 1989) using a supervised learning approach"
P98-2228,H92-1046,0,0.0457272,"e, and some WordNet senses are mapped onto two or three LDOCE senses when the WordNet sense does not distinguish between them. The mapping also contained significant gaps (words and senses not in the translation). SEMCOR contains 91,808 words tagged with WordNet synsets, 6,071 of which are proper names which we ignore, leaving 85,737 words which could potentially be translated. The translation contains only 36,869 words tagged with L D O C E senses, although this is a reasonable size for an evaluation corpus given this type of task (it is several orders of magnitude larger than those used by (Cowie et al., 1992) (Harley and Glennon, 1997) (Mahesh et al., 1997)). This corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any inconsistencies which may occur with a poorly controlled tagging strategy. 6 Results To date we have tested our system on only a portion of the text we derived from SEMCOR, which consisted of 2021 words tagged with LDOCE senses (and 12,208 words in total). The 2021 word occurances are made up from 1068 different types, with an average polysemy of 7.65. As a baseline against which to compare results we computed the percentage of wo"
P98-2228,H92-1045,0,0.0134167,"nd that the new evaluation function led to an improvement in the algorithm&apos;s effectiveness. 3.4 P r a g m a t i c C o d e s Our next partial tagger makes use of the hierarchy of LDOCE pragmatic codes which indicate the likely subject area for a sense. Disambiguation is carried out using a modified version of the simulated annealing algorithm, and attempts to optimise the number of pragmatic codes of the same type in the sentence. Rather than processing over single sentences we optimise over entire paragraphs and only for the sense of nouns. We chose this strategy since there is good evidence (Gale et al., 1992) that nouns are best disambiguated by broad contextual considerations, while other parts of speech are resolved by more local factors. 3.5 S e l e c t i o n a l R e s t r i c t i o n s LDOCE senses contain simple selectional restrictions for each content word in the dictionary. A set of 35 semantic classes are used, such as S = Human, M = Human male, P = Plant, S -- Solid and so on. Each word sense for a noun is given one of these semantic types, senses for adjectives list the type which they expect for the noun they modify, senses for adverbs the type they expect of their modifier and verbs l"
P98-2228,W97-0212,0,0.0356721,"enses are mapped onto two or three LDOCE senses when the WordNet sense does not distinguish between them. The mapping also contained significant gaps (words and senses not in the translation). SEMCOR contains 91,808 words tagged with WordNet synsets, 6,071 of which are proper names which we ignore, leaving 85,737 words which could potentially be translated. The translation contains only 36,869 words tagged with L D O C E senses, although this is a reasonable size for an evaluation corpus given this type of task (it is several orders of magnitude larger than those used by (Cowie et al., 1992) (Harley and Glennon, 1997) (Mahesh et al., 1997)). This corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any inconsistencies which may occur with a poorly controlled tagging strategy. 6 Results To date we have tested our system on only a portion of the text we derived from SEMCOR, which consisted of 2021 words tagged with LDOCE senses (and 12,208 words in total). The 2021 word occurances are made up from 1068 different types, with an average polysemy of 7.65. As a baseline against which to compare results we computed the percentage of words which are correctly tag"
P98-2228,1997.tmi-1.18,0,0.0614336,"r three LDOCE senses when the WordNet sense does not distinguish between them. The mapping also contained significant gaps (words and senses not in the translation). SEMCOR contains 91,808 words tagged with WordNet synsets, 6,071 of which are proper names which we ignore, leaving 85,737 words which could potentially be translated. The translation contains only 36,869 words tagged with L D O C E senses, although this is a reasonable size for an evaluation corpus given this type of task (it is several orders of magnitude larger than those used by (Cowie et al., 1992) (Harley and Glennon, 1997) (Mahesh et al., 1997)). This corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any inconsistencies which may occur with a poorly controlled tagging strategy. 6 Results To date we have tested our system on only a portion of the text we derived from SEMCOR, which consisted of 2021 words tagged with LDOCE senses (and 12,208 words in total). The 2021 word occurances are made up from 1068 different types, with an average polysemy of 7.65. As a baseline against which to compare results we computed the percentage of words which are correctly tagged if we chose the fi"
P98-2228,J92-1001,0,0.0656518,"lues to a single phenomenon, WSD? This is a situation quite unlike syntactic parsing or part-of-speech tagging: in the latter case, for example, one can write a Cherry-style rule tagger or an HMM learning model, but there is no reason the believe these represent different types of information, just different ways of conceptualising and coding it. T h a t seems not to be the case, at first sight, with the many forms of information for WSD. It is odd that this has not been much discussed in the field. In this work, we shall adopt the methodology first explicitly noted in connection with WSD by (McRoy, 1992), and more recently (Ng and Lee, 1996), namely that of bringing together a number of partial sources of information about a phenomenon and combining them in a principled manner. This is in the AI tradition of combining ""weak"" methods for strong results (usually ascribed to Newell (Newell, 1973)) and used in the CRL-NMSU lexical work on the Eighties (Wilks et al., 1990). We shall, in this paper, offer a system that combines the three types of information listed above (plus part-of-speech filtering) and, more importantly, applies a learning algorithm to determine the optimal combination of such"
P98-2228,P96-1006,0,0.331348,"This is a situation quite unlike syntactic parsing or part-of-speech tagging: in the latter case, for example, one can write a Cherry-style rule tagger or an HMM learning model, but there is no reason the believe these represent different types of information, just different ways of conceptualising and coding it. T h a t seems not to be the case, at first sight, with the many forms of information for WSD. It is odd that this has not been much discussed in the field. In this work, we shall adopt the methodology first explicitly noted in connection with WSD by (McRoy, 1992), and more recently (Ng and Lee, 1996), namely that of bringing together a number of partial sources of information about a phenomenon and combining them in a principled manner. This is in the AI tradition of combining ""weak"" methods for strong results (usually ascribed to Newell (Newell, 1973)) and used in the CRL-NMSU lexical work on the Eighties (Wilks et al., 1990). We shall, in this paper, offer a system that combines the three types of information listed above (plus part-of-speech filtering) and, more importantly, applies a learning algorithm to determine the optimal combination of such modules for a given word distribution;"
P98-2228,C90-2067,0,0.0168567,"Missing"
P98-2228,P95-1026,0,0.0429719,"into a hierarchy which we use to resolve the restrictions. We resolve the restrictions by returning, for each word, the set of sense which do not break them (that is, those whose semantic category is at the same, or a lower, level in the hierarchy). 4 Combining Knowledge Sources Since each of our partial taggers suggests only possible senses for each word it is necessary to have some method to combine their results. We trained decision lists (Clark and Niblett, 1989) using a supervised learning approach. Decision lists have already been successfully applied to lexical ambiguity resolution by (Yarowsky, 1995) where they perfromed well. We present the decision list system with a number of training words for which the correct sense 1400 is known. For each of the words we supply each of its possible senses (apart from those removed from consideration by the part-of-speech filter (Section 3.2)) within a context consisting of the results from each of the partial taggers, frequency information and 10 simple collocations (first noun/verb/preposition to the left/right and first/second word to the left/right). Each sense is marked as either a p p r o p r i a t e (if it is the correct sense given the contex"
P98-2228,H93-1052,0,\N,Missing
P98-2228,W97-0208,1,\N,Missing
R11-1004,C10-2047,0,0.0356919,"Missing"
R11-1004,P10-1135,0,0.0561046,"Missing"
R11-1004,P05-1061,0,0.0353433,"hes to intra-sentential relation extraction can be effectively adapted for inter-sentential relation extraction. The results demonstrate that a composite kernel approach to inter-sentential relation extraction can achieve comparable results with intra-sentential relation extraction. We have also shown that the structured features used for intra-sentential relation extraction can be easily adapted for the inter-sentential case. The performance of structured features has been found to be superior to flat features which have previously been used for the inter-sentential relation extraction task (McDonald et al., 2005; Roberts et al., 2008). Overall, composite kernels, that combine a larger context window with a SPT tree, were found to give better performance than either flat or structured features alone. Inter-sentential PerPost relations could not effectively be extracted using this approach, most likely due to the bias in the PerPost data set. Threshold adaption, which was optimised using nested cross-validation, significantly improved the performance of SVM models for inter-sentential relation extraction. Average f-measure improved from 0.295 to 0.605, a significant improvement in performance over all"
R11-1004,W07-1023,0,0.0647968,"Missing"
R11-1004,P09-1113,0,0.433925,"Missing"
R11-1004,P08-1004,0,0.0711792,"Missing"
R11-1004,P04-1056,0,0.0606836,"Missing"
R11-1004,E06-1015,0,0.0324616,"Missing"
R11-1004,W08-0602,0,0.0363591,"Missing"
R11-1004,P06-2094,0,0.0526561,"Missing"
R11-1004,swampillai-stevenson-2010-inter,1,0.910547,"Missing"
R11-1004,P06-1104,0,0.0913403,"n to the representation. 3.2 Structured Features Structured features used for intra-sentential relation extraction are based on parse trees. As only entities occurring in the same sentence can be part of a intra-sentential relation, it can be assumed that related entities always appear in a single parse tree. However, this assumption does not hold for inter-sentential relations. We overcame this problem by joining parse trees for pairs of entities by adding a new node (ROOT) that connects the parses. Two new features were developed using this approach based on the shortest path-enclosed tree (Zhang et al., 2006): Figure 1: Examples of (a) shortest path-enclosed tree and (b) shortest path tree adapted for intersentential relation extraction. three relation types, PerOrg, PerPost and PostOrg, have been manually identified annotated. For example, the following sentences include one intra-sentential relation, PerPost(Vern Raburn, president), and two inter-sentential relations, PerOrg(Vern Raburn, Paul Allen Group) and PostOrg(president, Paul Allen Group). • The shortest path tree (SPT) structure which only contains the shortest path between the two entities, that is the conjunction of the path from e1 to"
R11-1004,D07-1076,0,0.0608198,"ature representing the entity that occurs first in the sentence; the sequence of lexical tokens and part-of-speech (POS) tags between the two entities, in the sentence; a sequence of lexical tokens and their POS tags on the left hand side of the first entity and on the right hand side of the second entity; a dependency path between the two entities and the verbs that occur between the entities. Composite kernels using flat and structured features have been successfully applied for intra-sentential relation extraction (Zelenko et al., 2003; Bunescu and Mooney, 2004; Culotta and Sorensen, 2004; Zhou et al., 2007). Culotta and Sorensen (2004) and Zhou et al. (2007) have shown that tree kernels combined with flat kernels are more effective for intra-sentential relation extraction than either kernel used alone. In experiments on the ACE corpus, Zhou et al. (2007) achieved f-measures of 0.741 using syntactic parse tree features which outperforms dependency trees. Zhang et al. (2006) further explored which portion of parse trees are most informative for intrasentential relation extraction by testing seven dif3.1 Flat Features The entities participating in an inter-sentential relation can occur in any two s"
R11-1004,P04-1054,0,\N,Missing
rose-etal-2002-reuters,W98-1507,0,\N,Missing
rose-etal-2002-reuters,brants-2000-inter,0,\N,Missing
rose-etal-2002-reuters,J97-1002,0,\N,Missing
rose-etal-2002-reuters,J96-2004,0,\N,Missing
rose-etal-2002-reuters,wayne-2000-multilingual,0,\N,Missing
S10-1087,H93-1061,0,0.352349,"echniques using the background documents released in the task were used to assign ranking scores to the words and their senses. The test data was disambiguated using the Personalized PageRank algorithm which was applied to a graph constructed from the whole of WordNet in which nodes are initialized with ranking scores of words and their senses. In the competition, our systems achieved comparable accuracy of 53.4 and 52.2, which outperforms the most frequent sense baseline (50.5). 1 Introduction The senses in WordNet are ordered according to their frequency in a manually tagged corpus, SemCor (Miller et al., 1993). Senses that do not occur in SemCor are ordered arbitrarily after those senses of the word that have occurred. It is known from the results of SENSEVAL2 (Cotton et al., 2001) and SENSEVAL3 (Mihalcea and Edmonds, 2004) that first sense heuristic outperforms many WSD systems (see McCarthy et al. (2007)). The first sense baseline’s strong performance is due to the skewed frequency distribution of word senses. WordNet sense distributions based on SemCor are clearly useful, however in a given domain these distributions may not hold true. For example, the first sense for “bank” in WordNet refers to"
S10-1087,E09-1005,0,0.435374,", our approach aims to use these sense distributions collected from domain specific corpora as a knowledge source and combine this with information from the context. Our approach focuses on the strong influence of domain for WSD (Buitelaar et al., 2006) and the benefits of focusing on words salient to the domain (Koeling et al., 2005). Words are assigned a ranking score based on its keyness (salience) in the given domain. We use these word scores as another knowledge source. Graph based methods have been shown to produce state-of-the-art performance for unsupervised word sense disambiguation (Agirre and Soroa, 2009; Sinha and Mihalcea, 2007). These approaches use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular lexical knowledge base (LKB), such as WordNet. These graphbased algorithms are appealing because they take into account information drawn from the entire graph as well as from the given context, making them superior to other approaches that rely only on local information individually derived for each word. Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph We describe two systems that part"
S10-1087,N04-3012,0,0.0579884,"e of sense wsi is Domain Sense Ranking McCarthy et al. (2004) propose a method for finding predominant senses from raw text. The method uses a thesaurus acquired from automatically parsed text based on the method described by Lin (1998). This provides the top k nearest neighbours for each target word w, along with the distributional similarity score between the target word and each neighbour. The senses of a word w are each assigned a score by summing over the distributional similarity scores of its neighbours. These are weighted by a semantic similarity score (using WordNet Similarity score (Pedersen et al., 2004) between the sense of w and the sense of the neighbour that maximizes the semantic similarity score. More formally, let Nw = {n1 , n2 , . . . nk } be the ordered set of the top k scoring neighbours of w from the thesaurus with associated distributional similarity scores {dss(w, n1 ), dss(w, n2 ), . . . dss(w, nk )}. Let senses(w) be the set of senses of w. For each sense of w (wsi ∈ senses(w)) a ranking score is obtained by summing over the dss(w, nj ) of each neighbour (nj ∈ Nw ) multiplied by a weight. This weight is the WordNet similarity score (wnss) between the target sense (wsi ) and the"
S10-1087,W00-0901,0,0.0576778,"the domain specific corpus. In the next section we describe the way in which we compute krs(nj ). WordNet::Similarity::lesk (Pedersen et al., 2004) was used to compute word similarity wnss. IIITH1 and IIITH2 systems differ in the way senses are ranked. IIITH1 uses srs(wsj ) whereas IIITH2 system uses msrs(wsj ) for computing sense ranking scores in the given domain. 3 Domain Keyword Ranking We extracted keywords in the domain by comparing the frequency lists of domain corpora (background documents) and a very large general corpus, ukWaC (Ferraresi et al., 2008), using the method described by Rayson and Garside (2000). For each word in the frequency list of the domain corpora, words(domain), we calculated the loglikelihood (LL) statistic as described in Rayson and Garside (2000). We then normalized LL to compute keyword ranking score krs(w) of word w words(domain) using wnss(wsi , nj ) X wnss(wsi , nj ) X wsi senses(w) srs(wsi ) = X dss(w, nj )× wnss(wsi , nj ) wsi senses(w) 388 krs(w) = XLL(w) Keyword Ranking scores with PPR (KRS + PPR): This is same as PPR except that context words are initialized with krs. Sense Ranking scores with PPR (SRS + PPR): Edges connecting words and their synsets are assigned"
S10-1087,kilgarriff-etal-2010-corpus,1,0.850105,"Missing"
S10-1087,P03-1054,0,0.00449573,"ver the SemEval data are provided in Section 5. 2 where wnss(wsi , nj ) = maxnsx ∈senses(nj ) (wnss(wsi , nsx )) Since this approach requires only raw text, sense rankings for a particular domain can be generated by simply training the algorithm using a corpus representing that domain. We used the background documents provided to the participants in this task as a domain specific corpus. In general, a domain specific corpus can be obtained using domain-specific keywords (Kilgarriff et al., 2010). A thesaurus is acquired from automatically parsed background documents using the Stanford Parser (Klein and Manning, 2003). We used k = 5 to built the thesaurus. As we increased k we found the number of non-domain specific words occurring in the thesaurus increased and negatively affected the sense distributions. To counter this, one of our systems IIITH2 used a slightly modified ranking score by multiplying the effect of each neighbour with its domain keyword ranking score. The modified sense ranking msrs(wsj ) score of sense wsi is Domain Sense Ranking McCarthy et al. (2004) propose a method for finding predominant senses from raw text. The method uses a thesaurus acquired from automatically parsed text based o"
S10-1087,H05-1053,1,\N,Missing
S10-1087,J07-4005,1,\N,Missing
S10-1087,P04-1036,1,\N,Missing
S10-1087,P98-2127,0,\N,Missing
S10-1087,C98-2122,0,\N,Missing
S12-1008,W04-1013,0,0.0131164,"s are applied in the text reuse score by generating modified n-grams for the document that is suspected to contain reused text. These n-grams are then compared with the original document to determine the overlap. However, the techniques in Section 3.2 generate a large number of modified n-grams which means that the number of n-grams that overlap with document A can be greater than the total number of n-grams in B, leading to similarity scores greater than 1. To avoid this the n-gram overlap counts are constrained in a similar way that they are clipped in BLEU and ROUGE (Papineni et al., 2002; Lin, 2004). For each n-gram in B, a set of modified n-grams, mod(ngram), is created.1 The count for an individual n-gram in B, exp count(ngram, B), can be computed as the number of times any n-gram in mod(ngram) occurs in A, see equation 2. X count(ngram0 , A) (2) ngram0 ∈mod(ngram) However, the contribution of this count to the text reuse score has to be bounded to ensure that the combined count of the modified n-grams appearing in A does not exceed the number of times the original n-gram occurs in B. Consequently the text reuse score, scoren (A, B), is computed using equation 3. P min(exp count(ngram,"
S12-1008,D08-1021,0,0.0221423,"Missing"
S12-1008,P02-1040,0,0.0868661,"ams The modified n-grams are applied in the text reuse score by generating modified n-grams for the document that is suspected to contain reused text. These n-grams are then compared with the original document to determine the overlap. However, the techniques in Section 3.2 generate a large number of modified n-grams which means that the number of n-grams that overlap with document A can be greater than the total number of n-grams in B, leading to similarity scores greater than 1. To avoid this the n-gram overlap counts are constrained in a similar way that they are clipped in BLEU and ROUGE (Papineni et al., 2002; Lin, 2004). For each n-gram in B, a set of modified n-grams, mod(ngram), is created.1 The count for an individual n-gram in B, exp count(ngram, B), can be computed as the number of times any n-gram in mod(ngram) occurs in A, see equation 2. X count(ngram0 , A) (2) ngram0 ∈mod(ngram) However, the contribution of this count to the text reuse score has to be bounded to ensure that the combined count of the modified n-grams appearing in A does not exceed the number of times the original n-gram occurs in B. Consequently the text reuse score, scoren (A, B), is computed using equation 3. P min(exp"
S12-1008,P02-1020,1,0.731809,"fies these (by substituting words with synonyms and deleting words) to identify when text has been altered. The approach is applied to a corpus of newspaper stories and found to outperform a previously reported method. 1 2 Introduction Text reuse is the process of creating new document(s) using text from existing document(s). Text reuse is standard practice in some situations, such as journalism. Applications of automatic detection of text reuse include the removal of (near-)duplicates from search results (Hoad and Zobel, 2003; Seo and Croft, 2008), identification of text reuse in journalism (Clough et al., 2002) and identification of plagiarism (Potthast et al., 2011). Text reuse is more difficult to detect when the original text has been altered. We propose an approach to the identification of text reuse which is intended to identify reuse in such cases. The approach is based on comparison of word n-grams, a popular approach to detecting text reuse. However, we also account for synonym replacement and word deletion, two common text editing operations (Bell, Related Work Approaches for identifying text reuse based on word-level comparison (such as the SCAM copy detection system (Shivakumar and Molina"
S12-1008,rose-etal-2002-reuters,1,0.634959,"the set of n-grams that could have been created by modifing an n-gram in B and includes the original n-gram itself. rare n-grams and decrease the contribution of common ones. N-gram probabilities are computed using the SRILM language modelling toolkit (Stolcke, 2002). The score for each n-gram is computed as its Information Content (Cover and Thomas, 1991), ie. −log(P ). When the language model (LM) is applied the scores associated with each n-gram are used instead of counts in equations 2 and 3. The language model (Section 3.4) was trained using 806,791 news articles from the Reuters Corpus (Rose et al., 2002). A high proportion of the news stories selected were related to the topics of entertainment and legal reports to reflect the subjects of the new articles in the METER corpus. 4 Tables 1 and 2 show the results of the binary and ternary classification experiments respectively. “NG” refers to the comparison of n-grams in each document (Section 3.1), while “Del”, “WN” and “Para” refer to the modified n-grams created using deletions, WordNet and paraphrases respectively (Section 3.2). The prefix “LM” (e.g. “LM-NG”) indicates that the n-grams are weighted using the language model probability scores"
S12-1097,S12-1051,0,0.0422744,"004). In these approaches the similarity of texts is computed as a function of the number of matching tokens, or sequences of tokens, they contain. However, this approach fails to identify similarities when the same meaning is conveyed using synonymous terms or phrases (for example, “The dog sat on the mat” and “The hound sat on the mat”) or when the meanings of the texts are similar but not identical (for example, “The cat sat on the mat” and “A dog sat on the chair”). Introduction This paper describes the University of Sheffield’s submission to SemEval-2012 Task 6: Semantic Text Similarity (Agirre et al., 2012). The task is concerned with determining the degree of semantic equivalence between a pair of sentences. Measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (Seo and Croft, 2008; Bendersky and Croft, 2009), textual entailment (Szpektor et al., 2004; Zanzotto et al., 2009), paraphrase detection (Barzilay and Lee, 2003; Dolan et al., 2004), Information Extraction/Question Answering (Lin and Pantel, 2001; Stevenson and Greenwood, 2005), Information Retrieval (Baeza-Yates and Ribeiro-Ne"
S12-1097,N03-1003,0,0.045456,"at on the mat” and “A dog sat on the chair”). Introduction This paper describes the University of Sheffield’s submission to SemEval-2012 Task 6: Semantic Text Similarity (Agirre et al., 2012). The task is concerned with determining the degree of semantic equivalence between a pair of sentences. Measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (Seo and Croft, 2008; Bendersky and Croft, 2009), textual entailment (Szpektor et al., 2004; Zanzotto et al., 2009), paraphrase detection (Barzilay and Lee, 2003; Dolan et al., 2004), Information Extraction/Question Answering (Lin and Pantel, 2001; Stevenson and Greenwood, 2005), Information Retrieval (Baeza-Yates and Ribeiro-Neto, Significant amounts of previous work on text similarity have focussed on comparing the meanings of texts longer than a single sentence, such as paragraphs or documents (Baeza-Yates and RibeiroNeto, 1999; Seo and Croft, 2008; Bendersky and Croft, 2009). The size of these texts means that there is a reasonable amount of lexical items in each document that can be used to determine similarity and failing to identify connections"
S12-1097,C04-1051,0,0.0556026,"g sat on the chair”). Introduction This paper describes the University of Sheffield’s submission to SemEval-2012 Task 6: Semantic Text Similarity (Agirre et al., 2012). The task is concerned with determining the degree of semantic equivalence between a pair of sentences. Measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (Seo and Croft, 2008; Bendersky and Croft, 2009), textual entailment (Szpektor et al., 2004; Zanzotto et al., 2009), paraphrase detection (Barzilay and Lee, 2003; Dolan et al., 2004), Information Extraction/Question Answering (Lin and Pantel, 2001; Stevenson and Greenwood, 2005), Information Retrieval (Baeza-Yates and Ribeiro-Neto, Significant amounts of previous work on text similarity have focussed on comparing the meanings of texts longer than a single sentence, such as paragraphs or documents (Baeza-Yates and RibeiroNeto, 1999; Seo and Croft, 2008; Bendersky and Croft, 2009). The size of these texts means that there is a reasonable amount of lexical items in each document that can be used to determine similarity and failing to identify connections between related term"
S12-1097,O97-1002,0,0.223289,"milarity measures (Mihalcea and Corley, 2006) and several of these were compared. Implementations of these measures from the NLTK (Bird et al., 2009) were used. Path Distance uses the length of the shortest path between two senses to determine the similarity between them. Leacock and Chodorow (1998) expand upon the path distance similarity measure by scaling the path length by the maximum depth of the WordNet taxonomy. Resnik (1995) makes use of techniques from Information Theory. The measure of relatedness between two concepts is based on the Information Content of the Least Common Subsumer. Jiang and Conrath (1997) also uses the Information Content of the two input synsets. 657 Lin (1998) uses the same values as Jiang and Conrath (1997) but takes the ratio of the shared information content to that of the individual concepts. Results produced by the various combinations of word sense disambiguation strategy and similarity measures are shown in Table 1. This table shows the Pearson correlation of the system output with the gold standard over all of the SemEval training data. The row labelled ‘Binary’ shows the results using binary vectors which are not augmented with any similarity values. The remainder o"
S12-1097,W04-1013,0,0.00730519,"represents each sentence as a set of n-grams. This approach also makes use of information from WordNet. Results from the formal evaluation show that both approaches are useful for determining the similarity in meaning between pairs of sentences with the best performance being obtained by the supervised approach. Incorporating information from WordNet also improves performance for both approaches. 1 Many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (Baeza-Yates and Ribeiro-Neto, 1999; Papineni et al., 2002; Lin, 2004). In these approaches the similarity of texts is computed as a function of the number of matching tokens, or sequences of tokens, they contain. However, this approach fails to identify similarities when the same meaning is conveyed using synonymous terms or phrases (for example, “The dog sat on the mat” and “The hound sat on the mat”) or when the meanings of the texts are similar but not identical (for example, “The cat sat on the mat” and “A dog sat on the chair”). Introduction This paper describes the University of Sheffield’s submission to SemEval-2012 Task 6: Semantic Text Similarity (Agir"
S12-1097,P04-1036,0,0.106294,"Missing"
S12-1097,E09-1065,0,0.0213374,"ort (single sentences). There are fewer lexical items to match in this case, making it more important that connections between related terms are identified. One way in which this information has been incorporated in NLP systems has 655 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 655–661, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics been to make use of WordNet to provide information about similarity between word meanings, and this approach has been shown to be useful for computing text similarity (Mihalcea and Corley, 2006; Mohler and Mihalcea, 2009). This paper describes two approaches to the semantic text similarity problem that use WordNet (Miller et al., 1990) to provide information about relations between word meanings. The two approaches are based on commonly used techniques for computing semantic similarity based on lexical matching. The first is unsupervised while the other requires annotated data to train a learning algorithm. Results of the SemEval evaluation show that the supervised approach produces the best overall results and that using the information provided by WordNet leads to an improvement in performance. The remainder"
S12-1097,P02-1040,0,0.0833954,"ed machine learning and represents each sentence as a set of n-grams. This approach also makes use of information from WordNet. Results from the formal evaluation show that both approaches are useful for determining the similarity in meaning between pairs of sentences with the best performance being obtained by the supervised approach. Incorporating information from WordNet also improves performance for both approaches. 1 Many of the previous approaches to measuring the similarity between texts have relied purely on lexical matching techniques, for example (Baeza-Yates and Ribeiro-Neto, 1999; Papineni et al., 2002; Lin, 2004). In these approaches the similarity of texts is computed as a function of the number of matching tokens, or sequences of tokens, they contain. However, this approach fails to identify similarities when the same meaning is conveyed using synonymous terms or phrases (for example, “The dog sat on the mat” and “The hound sat on the mat”) or when the meanings of the texts are similar but not identical (for example, “The cat sat on the mat” and “A dog sat on the chair”). Introduction This paper describes the University of Sheffield’s submission to SemEval-2012 Task 6: Semantic Text Simi"
S12-1097,W06-2501,0,0.0305874,"2.1.1 Creating vectors Each sentence is tokenised, stop words removed and the remaining words lemmatised using NLTK (Bird et al., 2009). (The WordPunctTokenizer and WordNetLemmatizer are applied.) Binary vectors are then created for each sentence. The similarity between sentences can be computed by comparing these vectors using the cosine metric. However, this does not take account of words with similar meanings, such as “dog” and “hound” in the sentences “The dog sat on the mat” and “The hound sat on the mat”. To take account of these similarities WordNet-based similarity measures are used (Patwardhan and Pedersen, 2006). Any terms that occur in only one of the sentences do not contribute to the similarity score since they will have a 0 value in the binary vector. Any words with a 0 value in one of the binary vectors are compared with all of the words in the other sentence and the similarity values computed. The highest similarity value is selected and use to replace the 0 value in that vector, see Figure 1. (If the similarity score is below the set threshold of 0.5 then the similarity value is not used and in these cases the 0 value remains unaltered.) This substitution of 0 values in the vectors ensures tha"
S12-1097,W05-0202,0,0.0344182,"Missing"
S12-1097,P05-1047,1,0.80838,"submission to SemEval-2012 Task 6: Semantic Text Similarity (Agirre et al., 2012). The task is concerned with determining the degree of semantic equivalence between a pair of sentences. Measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (Seo and Croft, 2008; Bendersky and Croft, 2009), textual entailment (Szpektor et al., 2004; Zanzotto et al., 2009), paraphrase detection (Barzilay and Lee, 2003; Dolan et al., 2004), Information Extraction/Question Answering (Lin and Pantel, 2001; Stevenson and Greenwood, 2005), Information Retrieval (Baeza-Yates and Ribeiro-Neto, Significant amounts of previous work on text similarity have focussed on comparing the meanings of texts longer than a single sentence, such as paragraphs or documents (Baeza-Yates and RibeiroNeto, 1999; Seo and Croft, 2008; Bendersky and Croft, 2009). The size of these texts means that there is a reasonable amount of lexical items in each document that can be used to determine similarity and failing to identify connections between related terms may not be problematic. The situation is different for the problem of semantic text similarity"
S12-1097,W04-3206,0,0.0199371,"s of the texts are similar but not identical (for example, “The cat sat on the mat” and “A dog sat on the chair”). Introduction This paper describes the University of Sheffield’s submission to SemEval-2012 Task 6: Semantic Text Similarity (Agirre et al., 2012). The task is concerned with determining the degree of semantic equivalence between a pair of sentences. Measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including the identification of text reuse (Seo and Croft, 2008; Bendersky and Croft, 2009), textual entailment (Szpektor et al., 2004; Zanzotto et al., 2009), paraphrase detection (Barzilay and Lee, 2003; Dolan et al., 2004), Information Extraction/Question Answering (Lin and Pantel, 2001; Stevenson and Greenwood, 2005), Information Retrieval (Baeza-Yates and Ribeiro-Neto, Significant amounts of previous work on text similarity have focussed on comparing the meanings of texts longer than a single sentence, such as paragraphs or documents (Baeza-Yates and RibeiroNeto, 1999; Seo and Croft, 2008; Bendersky and Croft, 2009). The size of these texts means that there is a reasonable amount of lexical items in each document that c"
S13-1010,W04-0813,0,0.0231175,"corpus are matched against a test corpus sentence, and results of each match are summed to yield a preferred use in the given context with a higher weight (experimentally determined) being assigned to longer n-grams. The system backs off to the most frequent usage (as derived from the training data). 3.3 Vector Space Model (VSM) Distinguishing between common and proper nouns can be viewed as a classification problem. Treating the problem in this manner is reminiscent of techniques commonly employed in Word Sense Disambiguation (WSD). Our supervised approach is based on an existing WSD system (Agirre and Martinez, 2004) that uses a wide range of features: • Word form, lemma or PoS bigrams and trigrams containing the target word. • Preceding or following lemma (or word form) content word appearing in the same sentence as the target word. • High-likelihood, salient, bigrams. • Lemmas of all content words in the same sentence as the target word. • Lemmas of all content words within a ±4 word window of the target word. The approaches described in the previous section are evaluated on two data sets extracted automatically from the BNC. The BNC-PoS data set is created using the output from the CLAWS tagger. Nouns"
S13-1010,P06-4020,0,0.111845,"Missing"
S13-1010,P98-1036,0,0.0243766,"n start with a capital letter in English, capitalization can be inconsistent, incorrect or omitted, and the presence or absence of an article cannot be relied on. The problem of distinguishing between common and proper usages of nouns has not received much attention within language processing, despite being an important component for many tasks including machine translation (Lopez, 2008; Hermjakob et al., 2008), sentiment analysis (Pang and Lee, 2008; Wilson et al., 2009) and topic tracking (Petrovi´c et al., 2010). Approaches to the problem also have applications to tasks such as web search (Chen et al., 1998; Baeza-Yates and Ribeiro-Neto, 2011), and case restoration (e.g., in automatic speech recognition output) (Baldwin et al., 2009), but frequently involve the manual creation of a gazeteer (a list of proper nouns), which suffer not only from omissions but also often do not allow the listed words to assume their common role in text. This paper presents methods for generating lists of nouns that have both common and proper usages (Section 2) and methods for identifying the type of usage (Section 3) which are evaluated using data derived automatically from the BNC (Section 4) and on two applicatio"
S13-1010,P05-1045,0,0.0170269,"Capitalization Nouns appearing intrasententially with both lower and upper case first letters are assumed to be ambiguous. This technique is applied to the 5-grams from the Google corpus (Brants and Franz, 2006) and the BNC (creating the lists 5-grams and BNCcaps). Wikipedia includes disambiguation pages for ambiguous words which provide information about their potential usage. Wikipedia pages for nouns with senses (according to the disambiguation page) in a set of predefined categories were identified to form the list Wikipedia. Named entity recognition The Stanford Named Entity Recogniser (Finkel et al., 2005) was run over the BNC and any nouns that occur in the corpus with both named entity and non-named entity tags are extracted to form the list Stanford. WordNet The final heuristic makes use of WordNet (Fellbaum, 1998) which lists nouns that are often used as proper nouns with capitalisation. Nouns which appeared in both a capitalized and lowercased form were extracted to create the list WordNet. Table 1 shows the number of nouns identified by each technique in the column labeled words which demonstrates that the number of nouns identified varies significantly depending upon which heuristic is u"
S13-1010,P08-1045,0,0.0377033,"Missing"
S13-1010,N10-1021,0,0.112629,"Missing"
S13-1010,J09-3003,0,0.0472439,"; for example, a specific instance of a common noun, e.g., District Court turns court into a proper noun. While heuristically, proper nouns often start with a capital letter in English, capitalization can be inconsistent, incorrect or omitted, and the presence or absence of an article cannot be relied on. The problem of distinguishing between common and proper usages of nouns has not received much attention within language processing, despite being an important component for many tasks including machine translation (Lopez, 2008; Hermjakob et al., 2008), sentiment analysis (Pang and Lee, 2008; Wilson et al., 2009) and topic tracking (Petrovi´c et al., 2010). Approaches to the problem also have applications to tasks such as web search (Chen et al., 1998; Baeza-Yates and Ribeiro-Neto, 2011), and case restoration (e.g., in automatic speech recognition output) (Baldwin et al., 2009), but frequently involve the manual creation of a gazeteer (a list of proper nouns), which suffer not only from omissions but also often do not allow the listed words to assume their common role in text. This paper presents methods for generating lists of nouns that have both common and proper usages (Section 2) and methods for"
S13-1010,C98-1036,0,\N,Missing
S13-1018,E09-1005,1,0.849802,"se2 . • Subject and description: cosine similarity of TF.IDF vectors of respective fields. IDF values were calculated using a subset of Europeana items (the Culture Grid collection), available internally. These preliminary scores were im2 urlhttp://wordnetcode.princeton.edu/standofffiles/morphosemantic-links.xls 132 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 132–137, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics proved using TF.IDF based on Wikipedia, UKB (Agirre and Soroa, 2009) and a more informed time similarity measure. We describe each of these processes in turn. 2.1 TF.IDF A common approach to computing document similarity is to represent documents as Bag-Of-Words (BOW). Each BOW is a vector consisting of the words contained in the document, where each dimension corresponds to a word, and the weight is the frequency in the corresponding document. The similarity between two documents can be computed as the cosine of the angle between their vectors. This is the approached use above. This approach can be improved giving more weight to words which occur in only a fe"
S13-1018,agirre-etal-2010-exploring,1,0.860547,"idf2w qP 2 2 w∈a (tfw,a × idfw ) × w∈b (tfw,b × idfw ) P qP w∈a,b where tfw,x is the frequency of the term w in x ∈ {a, b} and idfw is the inverted document frequency of the word w measured in Wikipedia. We substituted the preliminary general similarity score by the obtained using the TF.IDF presented in this section. 2.2 UKB The semantic disambiguation UKB3 algorithm (Agirre and Soroa, 2009) applies personalized PageRank on a graph generated from the English WordNet (Fellbaum, 1998), or alternatively, from Wikipedia. This algorithm has proven to be very competitive in word similarity tasks (Agirre et al., 2010). To compute similarity using UKB we represent WordNet as a graph G = (V, E) as follows: graph nodes represent WordNet concepts (synsets) and 3 http://ixa2.si.ehu.es/ukb/ 133 dictionary words; relations among synsets are represented by undirected edges; and dictionary words are linked to the synsets associated to them by directed edges. Our method is provided with a pair of vectors of words and a graph-based representation of WordNet. We first compute the personalized PageRank over WordNet separately for each of the vector of words, producing a probability distribution over WordNet synsets. We"
S13-1018,P05-1045,0,0.00503141,"y, using the training data provided by the organisers with the previously defined similarity measures as features. We begin by describing our basic system in Section 2, followed by the machine learning system in 1 http://www.europeana.eu/ Basic system The items in this task are taken from Europeana. They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.IDF vectors of tokens, taken from all fields. • Author: cosine similarity of TF.IDF vectors of dc:Creator field. • People involved, time period and location: cosine similarity of TF.IDF vectors of location/date/people entities recognized by NERC in all fields. • Events: cosine similarity of TF.IDF vectors of event verbs and nouns. A list of verbs and nouns possibly denoting events was derived using the WordNet Morphosemantic Database2 . • Subject and descri"
S13-1018,N03-1033,0,0.0199478,"data provided by the organisers with the previously defined similarity measures as features. We begin by describing our basic system in Section 2, followed by the machine learning system in 1 http://www.europeana.eu/ Basic system The items in this task are taken from Europeana. They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.IDF vectors of tokens, taken from all fields. • Author: cosine similarity of TF.IDF vectors of dc:Creator field. • People involved, time period and location: cosine similarity of TF.IDF vectors of location/date/people entities recognized by NERC in all fields. • Events: cosine similarity of TF.IDF vectors of event verbs and nouns. A list of verbs and nouns possibly denoting events was derived using the WordNet Morphosemantic Database2 . • Subject and description: cosine similarity"
S13-1018,L10-1000,0,\N,Missing
S13-1018,W12-1012,1,\N,Missing
S15-1003,E09-1005,0,0.0330812,"mber) will not be. The graph-based algorithm increases the weight of context features car.n.01 automobile ~ automobile auto ~ auto car ~ car machine ~ machine motorcar ~ motorcar + ~ car.n.01 Figure 1: In the Synset Distributional Model the vector representing a synset (white box) is computed as the centroid of its lemma vectors (grey boxes) that synsets share with neighbours and reduces those that are not shared. PageRank (Page et al., 1999) is a graph-based algorithm for identifying important nodes in a graph that has been applied to a range of NLP tasks including word sense disambiguation (Agirre and Soroa, 2009) and keyword extraction (Mihalcea and Tarau, 2004). Let G = (V, E) be a graph with a set of vertices, V , denoting synsets and a set of edges, E, denoting links between synsets in the WordNet hierarchy. The PageRank score (P r) over G for a synset (Vi ) can be computed by the following equation: P r(Vi ) = d · X Vj ∈I(Vi ) 1 P r(Vj ) + (1 − d)v O(Vj ) (1) where I(Vi ) denotes the in-degree of the vertex Vi and O(Vj ) is the out-degree of vertex Vj . d is the damping factor which is set to the default value of d = 0.85 (Page et al., 1999). In standard PageRank all elements of the vector v are t"
S15-1003,N09-1003,0,0.833161,"equation 1 to prefer certain nodes. The values in v effectively initialises the graph and assigning high values to nodes in v makes them more likely to be assigned a high PPR score. For each context feature c in C if c ∈ LM where LM contains all the lemma names of synsets in S, we apply PPR to assign importance to synsets. The score of each synset Sc in the personalisation vector 22 v, is set to |S1c |where |Sc |is the number of synsets that context feature i belongs. The personalisation value of all the other sysnets is set to 0. We apply PPR over WordNet for each context feature using UKB (Agirre et al., 2009) and obtain weights for each synset-context feature pair resulting to a new semantic space Hp , S × C, where vector elements are weighted by PageRank values. Figure 2 shows how the synset scores are computed by applying PPR over WordNet given the context feature car. Note that we use the context features of the distributional model D. 2.3 Latent Semantic Analysis Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dumais, 1997) has been used to reduce the dimensionality of semantic spaces leading to improved performance. LSA applies Singular Value Decomposition (SVD) to a mat"
S15-1003,S12-1051,0,0.0273137,"the value that ˜ and maximises performance which is k = 700 for H ˜ p . For the joint spaces learned using k = 650 for H CCA, we also tune the number of the top l correlated features in RG. We set l ∈ {10, 20, ..., 650} and select the value that maximises performance which 24 is l = 250 for H ∗ and l = 40 for Hp∗ . 3.4 Evaluation Metric Performance is measured as the correlation between the similarity scores returned by each proposed method and the human judgements. This is the standard approach to evaluate word and text similarity tasks, e.g. (Budanitsky and Hirst, 2001; Agirre et al., 2009; Agirre et al., 2012). Our experiments use Spearman’s correlation coefficient. 3.5 Results Table 1 shows the Spearman’s correlation of similarity scores generated by each model and human judgements of similarity across various data sets by taking the maximum pairwise similarity score of two words’ synsets. The first row of the table shows the results obtained by the word distributional model of Baroni et al. (2014). The full hybrid models H and Hp perform consistently worse than the original distributional model D across data sets. The main reason is that a large number of synsets contain only one lemma name which"
S15-1003,P14-1023,0,0.328415,"ic space to represent words as vectors (Section 2.1). Then, we make use of the WordNet’s clusters of synonyms and hierarchy in combination with the standard distributional space to build hybrid models (Section 2.2) which are augmented using Latent Semantic Analysis (Section 2.3) and Canonical Correlation Analysis (Section 2.4). 2.1 Distributional Model We consider a semantic space D, as a word by context feature matrix, L × C. Vector representations consist of context features C in a reference corpus. We made use of pre-computed publicly available vectors1 optimised for word similarity tasks (Baroni et al., 2014). Word co-occurrence counts are extracted using a symmetric window of two words over a corpus of 2.8 billion tokens obtained by concatenating 1 http://clic.cimec.unitn.it/composes/ semantic-vectors.html 21 ukWaC, the English Wikipedia and the British National Corpus. Vectors are weighted using positive Pointwise Mutual Information and the set of context features consists of the top 300K most frequent words in the corpus. 2.2 Hybrid Models 2.2.1 Synset Distributional Model We assume that making use of information about the structure of WordNet can reduce noise introduced in vectors of D due to"
S15-1003,W11-2503,0,0.025799,"9; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a similar direction, Faruqui and 27 Dyer (2014) found that learning joint spaces from multilingual vector spaces using CCA improves the performance of standard monolingual vector spaces on semantic similarity. Fyshe et al. (2014) showed that integrating textual vector space models with brain activation data when people are reading words achieves better correlation to behavioural data than models of one modality. Our hybrid models are also closely related to a supervised method proposed by Faruqui et al. (2015). Their method refines di"
S15-1003,P12-1015,0,0.0266854,"0.86 0.80 0.56 Table 1: Spearman’s correlation on various data sets. Maximum similarity between pairs of synsets. edness. First, we make use of WS-353 (Finkelstein et al., 2001) which contains 353 pairs of words annotated by humans. Furthermore, we make use of the similarity (WS-Sim) and relatedness (WS-Rel) pairs of words created by Agirre et al. (2009) from the original WS-353 data set. We also made use of the RG (Rubenstein and Goodenough, 1965) and MC (Miller and Charles, 1991) data sets which contain 65 and 30 pairs of nouns respectively. Finally, we make use of the larger MEN data set (Bruni et al., 2012) which contains 3,000 pairs of words that has been used as image tags. Annotations are obtained using croudsourcing. 3.3 Model Parameters The parameters we need to tune are the number of ˜ and H ˜ p , and the top components in LSA spaces, H ∗ ∗ CCA spaces, H and Hp . For the LSA spaces, we tune the number of the top k components in RG. We set k ∈ {50, 100, ..., 1000} and select the value that ˜ and maximises performance which is k = 700 for H ˜ p . For the joint spaces learned using k = 650 for H CCA, we also tune the number of the top l correlated features in RG. We set l ∈ {10, 20, ..., 650}"
S15-1003,D09-1003,0,0.0250988,"s such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (B"
S15-1003,D10-1113,0,0.0215044,"vious work tackled the problem through vector adaptation, clustering and language models (Erk, 2012). Vector adaptation methods modify a traditional (i.e. polysemous) target word vector by applying pointwise operations such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distribution"
S15-1003,D08-1094,0,0.0776876,"Missing"
S15-1003,P10-2017,0,0.0171648,"problem through vector adaptation, clustering and language models (Erk, 2012). Vector adaptation methods modify a traditional (i.e. polysemous) target word vector by applying pointwise operations such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representa"
S15-1003,E14-1049,0,0.0557514,"mensional variables x and y, CCA finds two projection vectors by maximising the correlations of the variables onto these projections. The function to be maximised is: ρ= p E[x2 ]E[y 2 ] 3 3.1 Joint Representation using CCA E[xy] The dimensionality of the projection vectors is lower or equal to the dimensionality of the original variables. The computation of CCA directly over H and Hp is computationally infeasible because of their high dimensionality (300K). We apply CCA over the re˜ and H ˜ p to obduced spaces learned using LSA, H tain two joint semantic spaces following a similar approach to Faruqui and Dyer (2014). These are the spaces H ∗ , resulting from the projection of the ˜ and Hp∗ , resulting Synset Distributional Model H, ˜ p. from the projection of the Synset Rank Model H (4) 23 Word Similarity Computing Similarity Since hybrid models represent words as synset vectors, similarity between two words can be computed following two ways. First, we compute similarity between two words as the maximum of their pairwise synset similarity. On the other hand, similarity can be computed as the average pairwise synset similarity using the synsets that the two words belong. Similarity is computed as the cos"
S15-1003,N15-1184,0,0.0378744,"on to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a similar direction, Faruqui and 27 Dyer (2014) found that learning joint spaces from multilingual vector spaces using CCA improves the performance of standard monolingual vector spaces on semantic similarity. Fyshe et al. (2014) showed that integrating textual vector space models with brain activation data when people are reading words achieves better correlation to behavioural data than models of one modality. Our hybrid models are also closely related to a supervised method proposed by Faruqui et al. (2015). Their method refines distributional semantic models using relational information from various semantic lexicons, including WordNet, by making linked words in these lexicons to have similar vector representations. While our models are also based on using information from WordNet for refining vector representations, they are fundamentally different. They create synset vectors in an unsupervised fashion and more importantly can be used for sense tagging. 6 Conclusions This paper proposed hybrid models of lexical semantics that combine distributional and knowledge-based approaches and offer adva"
S15-1003,P14-1046,0,0.0205097,"to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a similar direction, Faruqui and 27 Dyer (2014) found that learning joint spaces from multilingual vector spaces using CCA improves the performance of standard monolingual vector spaces on semantic similarity. Fyshe et al. (2014) showed that integrating textual vector space models with brain activation data when people are reading words achieves better correlation to behavioural data than models of one modality. Our hybrid models are also closely related to a supervised method proposed by Faruqui et al. (2015). Their method refines distributional semantic models using relational information from various semantic lexicons, including WordNet, by making linked words in these lexicons to have similar vector representations. While our models are also based on using information from WordNet for refining vector representatio"
S15-1003,J98-1001,0,0.396835,"Missing"
S15-1003,P14-1132,0,0.0604464,"Missing"
S15-1003,P08-1028,0,0.0509565,"eral features from them. This is not a problem for word similarity since there is no need to model the senses found in the lexicon. 5 Related Work Dealing with polysemy in distributional semantics is a fundamental issue since the various senses of a word type are conflated in a single vector. Previous work tackled the problem through vector adaptation, clustering and language models (Erk, 2012). Vector adaptation methods modify a traditional (i.e. polysemous) target word vector by applying pointwise operations such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applie"
S15-1003,S01-1005,0,0.109327,"Missing"
S15-1003,E14-1025,0,0.0258188,"tence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a"
S15-1003,N10-1013,0,0.0317742,"or adaptation, clustering and language models (Erk, 2012). Vector adaptation methods modify a traditional (i.e. polysemous) target word vector by applying pointwise operations such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporati"
S15-1003,P13-1056,0,0.0195276,"oon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a similar direction, Faruqui and 27 Dyer (2014) found that learning joint spaces from multilingual vector spaces using CCA improves the performance of standard monolingual vector spaces on semantic similarity. Fyshe et al. (2014) showed that integrating textual vector space models with brain activation data when people are reading words achieves better correlation to behavioural data than models of one modality. Our hybrid models are also closely related to a supervised method proposed by Faruqui et al. (2015). Their method refines distributional semantic m"
S15-1003,W04-0811,0,0.0637368,"Missing"
S15-1003,I11-1127,0,0.0376576,"Missing"
S15-1003,D11-1094,0,0.0481879,"Missing"
S15-1003,W10-2807,0,0.0206293,"plication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 201"
S15-1003,W04-3252,0,\N,Missing
stevenson-clough-2004-eurowordnet,A92-1018,0,\N,Missing
stevenson-clough-2004-eurowordnet,P99-1029,0,\N,Missing
swampillai-stevenson-2010-inter,W08-0602,0,\N,Missing
swampillai-stevenson-2010-inter,P04-1054,0,\N,Missing
swampillai-stevenson-2010-inter,P05-1052,0,\N,Missing
swampillai-stevenson-2010-inter,C96-1079,0,\N,Missing
swampillai-stevenson-2010-inter,X96-1047,0,\N,Missing
swampillai-stevenson-2010-inter,D07-1076,0,\N,Missing
W06-0202,H05-1091,0,0.029602,"Missing"
W06-0202,P05-1047,1,0.87074,"tween a word (the head) and one of its modifiers. These links may be labelled to indicate the grammatical relation between the head and modifier (e.g. subject, object). In general cyclical paths are disallowed so that the analysis forms a tree structure. An example dependency analysis for the sentence “Acme Inc. hired Mr Smith as their new CEO, replacing Mr Bloggs.” is shown Figure 1. The remainder of this section outlines four models for representing extraction patterns which can be derived from dependency trees. Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verbobject tuples from the dependency parse as extraction patterns. These consist of a verb and its subject and/or direct object1 . An SVO pattern is extracted for each verb in a sentence. Figure 2 shows the two SVO patterns2 which are produced for the dependency tree shown in Figure 1. This model may be motivated by the assumption that many IE scenarios involve the extraction Chains provide a mechanism for encoding information beyond the direct arguments of predicates and includes areas of the dependency tree ignored by the SVO model. For example, they can represent inform"
W06-0202,H01-1009,0,0.226912,"annot represent information described using other linguistic constructions such as nominalisations or prepositional phrases. For example, in the MUC6 texts it is common for job titles to be mentioned within prepositional phrases, e.g. “Smith joined Acme Inc. as CEO”. Figure 1: An example dependency tree. tern models are introduced (Sections 2 and 3). Section 4 describes experiments comparing each model and the results are discussed in Section 5. Chains: A pattern is defined as a path between a verb node and any other node in the dependency tree passing through zero or more intermediate nodes (Sudo et al., 2001). Figure 2 shows the eight chains which can be extracted from the tree in Figure 1. 2 Pattern Models In dependency analysis (Mel’ˇcuk, 1987) the syntax of a sentence is represented by a set of directed binary links between a word (the head) and one of its modifiers. These links may be labelled to indicate the grammatical relation between the head and modifier (e.g. subject, object). In general cyclical paths are disallowed so that the analysis forms a tree structure. An example dependency analysis for the sentence “Acme Inc. hired Mr Smith as their new CEO, replacing Mr Bloggs.” is shown Figur"
W06-0202,P03-1029,0,0.246464,"on about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used patterns based on a dependency analysis of the input text (Yangarber, 2003; Sudo et al., 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2005). These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency tree). For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al. (2003) allow any subpart of the tree to act as an extraction pattern. The set of patterns allowed by the first model is a proper subset of the second and therefore captures less of the information contained in the dependency tree. Little analysis has been carried out into the appropriateness of each model. Sudo et al. (2003) compared three models in terms of their ability to identify event participants. The choice of pattern model has an effect on the number of potential patterns. This has implications on the practical application for each approach, particularly when used for automatic acquisition o"
W06-0202,A97-1011,0,0.0554514,"Missing"
W06-0202,C00-2136,0,0.784478,"unwieldy number of possible patterns. 1 Introduction A common approach to Information Extraction (IE) is to use patterns which match against text and identify items of interest. Patterns are applied to text which has undergone various levels of linguistic analysis, such as phrase chunking (Soderland, 1999) and full syntactic parsing (Gaizauskas et al., 1996). The approaches use different definitions of what constitutes a valid pattern. For example, the AutoSlog system (Riloff, 1993) uses patterns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al. (2000) use subject-verbobject tuples derived from a dependency parse. An appropriate pattern language must encode enough 12 Proceedings of the Workshop on Information Extraction Beyond The Document, pages 12–19, c Sydney, July 2006. 2006 Association for Computational Linguistics of participants in specific events. For example, the MUC-6 (MUC, 1995) management succession scenario concerns the identification of individuals who are changing job. These events are often described using a simple predicate argument structure, e.g. “Acme Inc. fired Smith”. However, the SVO model cannot represent information"
W06-0202,P03-1044,0,0.391759,"DP, UK {marks,m.greenwood}@dcs.shef.ac.uk Abstract information about the text to be able to accurately identify the items of interest. However, it should not contain so much information as to be complex and impractical to apply. Several recent approaches to IE have used patterns based on a dependency analysis of the input text (Yangarber, 2003; Sudo et al., 2001; Sudo et al., 2003; Bunescu and Mooney, 2005; Stevenson and Greenwood, 2005). These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency tree). For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al. (2003) allow any subpart of the tree to act as an extraction pattern. The set of patterns allowed by the first model is a proper subset of the second and therefore captures less of the information contained in the dependency tree. Little analysis has been carried out into the appropriateness of each model. Sudo et al. (2003) compared three models in terms of their ability to identify event participants. The choice of pattern model has an effect on the number of potential patterns. This has implications on the practical application for eac"
W06-0202,P03-1054,0,0.015706,"taken from the biomedical domain, specifically the training corpus used in the LLL-05 challenge task (N´edellec, 2005), and a pair of corpora (Craven and Kumlien, 1999) which were derived from the Yeast Proteome Database (YPD) (Hodges et al., 1999) and the Online Mendelian Inheritance in Man database (OMIM) (Hamosh et al., 2002). Each of these corpora are annotated with binary 4.2 Generating Dependency Patterns Three dependency parsers were used for these experiments: MINIPAR3 (Lin, 1999), the Machinese Syntax4 parser from Connexor Oy (Tapanainen and J¨arvinen, 1997) and the Stanford5 parser (Klein and Manning, 2003). These three parsers represent a cross-section of approaches to producing dependency analyses: MINIPAR uses a constituency grammar internally before converting the result to a dependency tree, Machinese Syntax uses a functional dependency grammar, and the Stanford Parser is a lexicalized probabilistic parser. Before these parsers were applied to the various corpora the named entities participating in relations are replaced by a token indicating their class. For example, in the MUC6 corpus “Acme hired Smith” would become “Organisation hired PersonIn”. Each parser was adapted to deal with these"
W06-0202,M95-1017,0,\N,Missing
W06-0204,H05-1091,0,0.620848,"se patterns could identify the relation between Jones and Smith in the sentence “Jones replaced Smith”. However, no pattern consisting of a verb and its arguments could be constructed which could identify the same relation in “Jones was named as Smith’s successor.” Others have suggested alternative approaches for generating extraction patterns from dependency trees, each of which allows a particular part of the dependency analysis to act as an extraction pattern. For example, Sudo et al. (2003) used patterns consisting of a path from a verb to any of its descendents (direct or indirect) while Bunescu and Mooney (2005) suggest the shortest path between the items being related. However, iterative learning algorithms, such as the ones used by Yangarber et al. (2000) and Stevenson and Greenwood (2005), have not made use of these more complex extraction patterns. Part of the reason for this is that these algorithms require a way of determining the similarity between patterns (in order to compare candidate patterns with the seeds). This process is straightforward for simple patterns, based on SVO tuples, but less so for more complex exThis paper presents a novel approach to the semi-supervised learning of Inform"
W06-0204,C00-2136,0,0.278325,"ts arguments could be constructed which could identify the same relation in “Jones was named as Smith’s successor.” Others have suggested alternative approaches for generating extraction patterns from dependency trees, each of which allows a particular part of the dependency analysis to act as an extraction pattern. For example, Sudo et al. (2003) used patterns consisting of a path from a verb to any of its descendents (direct or indirect) while Bunescu and Mooney (2005) suggest the shortest path between the items being related. However, iterative learning algorithms, such as the ones used by Yangarber et al. (2000) and Stevenson and Greenwood (2005), have not made use of these more complex extraction patterns. Part of the reason for this is that these algorithms require a way of determining the similarity between patterns (in order to compare candidate patterns with the seeds). This process is straightforward for simple patterns, based on SVO tuples, but less so for more complex exThis paper presents a novel approach to the semi-supervised learning of Information Extraction patterns. The method makes use of more complex patterns than previous approaches and determines their similarity using a measure in"
W06-0204,P03-1044,0,0.268688,"each pattern as a vector consisting of the lexical items which formed the pattern and used a version of the cosine metric to determine the similarity between pairs of patterns, consequently this approach is referred to as “cosine similarity”. The metric used by this approach incorporated information from WordNet and assigned high similarity scores to patterns with similar meanings expressed in different ways. 2 Semi-Supervised Learning of Extraction Patterns We begin by outlining the general process of learning extraction patterns using a semi-supervised algorithm, similar to one presented by Yangarber (2003). 1. For a given IE scenario we assume the existence of a set of documents against which the system can be trained. The documents are unannotated and may be either relevant (contain the description of an event relevant to the scenario) or irrelevant. 2. This corpus is pre-processed to generate a set of all patterns which could be used to represent sentences contained in the corpus, call this set P . The aim of the learning process is to identify the subset of P representing patterns which are relevant to the IE scenario. 3. The user provides a small set of seed patterns, Pseed , which are rele"
W06-0204,P05-1047,1,0.270002,"ucted which could identify the same relation in “Jones was named as Smith’s successor.” Others have suggested alternative approaches for generating extraction patterns from dependency trees, each of which allows a particular part of the dependency analysis to act as an extraction pattern. For example, Sudo et al. (2003) used patterns consisting of a path from a verb to any of its descendents (direct or indirect) while Bunescu and Mooney (2005) suggest the shortest path between the items being related. However, iterative learning algorithms, such as the ones used by Yangarber et al. (2000) and Stevenson and Greenwood (2005), have not made use of these more complex extraction patterns. Part of the reason for this is that these algorithms require a way of determining the similarity between patterns (in order to compare candidate patterns with the seeds). This process is straightforward for simple patterns, based on SVO tuples, but less so for more complex exThis paper presents a novel approach to the semi-supervised learning of Information Extraction patterns. The method makes use of more complex patterns than previous approaches and determines their similarity using a measure inspired by recent work using kernel"
W06-0204,W06-0202,1,0.8401,"Missing"
W06-0204,H01-1009,0,0.0498343,"nnot be used to identify a number of relations including the connection between Mr. Smith and CEO or between Mr. Smith and Mr. Bloggs. A number of alternative approaches to constructing extraction patterns from dependency trees have been proposed (e.g. (Sudo et al., 2003; Bunescu and Mooney, 2005)). Previous analysis (Stevenson and Greenwood, 2006a) suggests that the most useful of these is one based on pairs of linked chains from the dependency tree. A chain can be defined as a path between a verb node and any other node in the dependency tree passing through zero or more intermediate nodes (Sudo et al., 2001). The linked chains model (Greenwood et al., 2005) represents extraction patterns as a pair of chains which share the same verb but no direct descendants. It can be shown that linked 4 Pattern Similarity Patterns such as linked chains have not been used by semi-supervised approaches to pattern learning. These algorithms require a method of determining the similarity of patterns. Simple patterns, such as SVO tuples, have a fixed structure containing few items and tend to occur relatively frequently in corpora. However, more complex patterns, such as linked chains, have a less fixed structure an"
W06-0204,P03-1029,0,0.620796,"d then only extract a limited set of relations; those expressed using a verb and its direct arguments. For example, these patterns could identify the relation between Jones and Smith in the sentence “Jones replaced Smith”. However, no pattern consisting of a verb and its arguments could be constructed which could identify the same relation in “Jones was named as Smith’s successor.” Others have suggested alternative approaches for generating extraction patterns from dependency trees, each of which allows a particular part of the dependency analysis to act as an extraction pattern. For example, Sudo et al. (2003) used patterns consisting of a path from a verb to any of its descendents (direct or indirect) while Bunescu and Mooney (2005) suggest the shortest path between the items being related. However, iterative learning algorithms, such as the ones used by Yangarber et al. (2000) and Stevenson and Greenwood (2005), have not made use of these more complex extraction patterns. Part of the reason for this is that these algorithms require a way of determining the similarity between patterns (in order to compare candidate patterns with the seeds). This process is straightforward for simple patterns, base"
W06-2505,J96-2004,0,0.0638774,"Missing"
W06-2505,P05-1048,0,0.0333307,"articular applications. 1 Introduction Word Sense Disambiguation (WSD) is concerned with the choice of the most appropriate sense of an ambiguous word given its context. The applications for which WSD has been thought to be helpful include Information Retrieval, Information Extraction, and Machine Translation (MT) (Ide and Verónis, 1998). The usefulness of WSD for MT, particularly, has been recently subject of debate, with conflicting results. Vickrey et al. (2005), e.g., show that the inclusion of a WSD module significantly improves the performance of their statistical MT system. Conversely, Carpuat and Wu (2005) found that WSD does not yield significantly better translation quality than a statistical MT system alone. In this latter work, however, the WSD module was not specifically designed for MT: it is based on the use of monolingual methods to identify the source language senses, which are then mapped into the target language translations. In fact, although it has been agreed that WSD is more useful when it is meant for a specific application (Wilks and Stevenson, 1998; Kilgarriff, 1997; Resnik and Yarowsky, 1997), little has been done on the development of WSD modules specifically for particular"
W06-2505,P02-1033,0,0.0353418,"similar senses. Ide (1999), for example, analyzes translations of English words into four different languages, in order to check if the different senses of an English word are lexicalized by different words in all the other languages. A parallel aligned corpus is used and the translated senses are mapped into WordNet senses. She uses this information to determine a set of monolingual sense distinctions that is potentially useful for NLP applications. In subsequent work (Ide et al., 2002), seven languages and clustering techniques are employed to create sense groups based on the translations. Diab and Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. An English sense inventory and a parallel corpus automatically produced by an MT system are employed. Sentence and word alignment systems are used to assign the word correspondences between the two languages. After grouping all the words that correspond to translations of a single word in the target language, all their possible senses are considered as candidates. The sense that maximizes the semantic similarity of the word with the others in the group is chosen. Similarly, Ng et al. (20"
W06-2505,W99-0508,0,0.0249968,"They are motivated by the argument that the senses of a word should be determined based on the distinctions that are lexicalized in a second language (Resnik and Yarowsky, 1997). In general, the assumptions behind these approaches are the following: (1) If a source language word is translated differently into a second language, it might be ambiguous and the different translations can indicate the senses in the source language. 34 (2) If two distinct source language words are translated as the same word into a second language, it often indicates that the two are being used with similar senses. Ide (1999), for example, analyzes translations of English words into four different languages, in order to check if the different senses of an English word are lexicalized by different words in all the other languages. A parallel aligned corpus is used and the translated senses are mapped into WordNet senses. She uses this information to determine a set of monolingual sense distinctions that is potentially useful for NLP applications. In subsequent work (Ide et al., 2002), seven languages and clustering techniques are employed to create sense groups based on the translations. Diab and Resnik (2002) use"
W06-2505,H05-1097,0,0.108904,"at the traditional monolingual WSD strategies are not suitable for multilingual applications, we intend to motivate the development of WSD methods for particular applications. 1 Introduction Word Sense Disambiguation (WSD) is concerned with the choice of the most appropriate sense of an ambiguous word given its context. The applications for which WSD has been thought to be helpful include Information Retrieval, Information Extraction, and Machine Translation (MT) (Ide and Verónis, 1998). The usefulness of WSD for MT, particularly, has been recently subject of debate, with conflicting results. Vickrey et al. (2005), e.g., show that the inclusion of a WSD module significantly improves the performance of their statistical MT system. Conversely, Carpuat and Wu (2005) found that WSD does not yield significantly better translation quality than a statistical MT system alone. In this latter work, however, the WSD module was not specifically designed for MT: it is based on the use of monolingual methods to identify the source language senses, which are then mapped into the target language translations. In fact, although it has been agreed that WSD is more useful when it is meant for a specific application (Wilk"
W06-2505,H94-1046,0,0.051065,"tigated the ambiguity in the translation of the English verb “to have” into Hindi. 11 translation patterns were identified for the 19 senses of the verb, according to the various target syntactic structures and/or target words for the verb. They argued that differences in both these aspects do not depend only on the sense of the verb. Out of the 14 senses analyzed, six had 2-5 different translations each. Bentivogli et al. (2004) proposed an approach to create an Italian sense tagged corpus (MultiSemCor) based on the transference of the annotations from the English sense tagged corpus SemCor (Miller et al., 1994), by means of wordalignment methods. A gold standard corpus was created by manually transferring senses in SemCor to the Italian words in a translated version of that corpus. From a total of 1,054 English words, 155 annotations were considered nontransferable to their corresponding Italian words, mainly due to the lack of synonymy at the lexical level. Miháltz (2005) manually mapped senses from the English in a sense tagged corpus to Hungarian translations, in order to carry out WSD between these languages. Out of 43 ambiguous nouns, 38 had all or most of their English senses mapped into the s"
W06-2505,P03-1058,0,0.0893779,"Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. An English sense inventory and a parallel corpus automatically produced by an MT system are employed. Sentence and word alignment systems are used to assign the word correspondences between the two languages. After grouping all the words that correspond to translations of a single word in the target language, all their possible senses are considered as candidates. The sense that maximizes the semantic similarity of the word with the others in the group is chosen. Similarly, Ng et al. (2003) employ EnglishChinese parallel word aligned corpora to identify a repository of senses for English. The English word senses are manually defined, based on the WordNet senses, and then revised in the light of the Chinese translations. For example, if two occurrences of a word with two different senses in WordNet are translated into the same Chinese word, they will be considered to have the same English sense. In general, these approaches rely on the two previously mentioned assumptions about the interaction between translations and word senses. Although these assumptions can be useful when usi"
W06-2505,C04-1053,0,\N,Missing
W06-2505,W02-0808,0,\N,Missing
W07-1211,P05-1024,0,0.0260717,"ection 3) suggested that the number of subtrees which would be generated from a corpus could be difficult to process computationally and this is supported by our findings here. The rightmost extension algorithm is most suited to finding subtrees which occur multiple times and, even using this efficient approach, we were unable to generate subtrees which occurred fewer than four times in the MUC-6 texts in a reasonable time. Similar restrictions have been encountered within other approaches which have relied on the generation of a comprehensive set of subtrees from a parse forest. For example, Kudo et al. (2005) used subtrees for parse ranking but could only generate subtrees which appear at least ten times in a 40,000 sentence corpus. They comment that the size of their data set meant that it would have been difficult to complete the experiments with less restrictive parameters. In addition, Sudo et al. (2003) only generated subtrees which appeared in at least three documents. Kudo et al. (2005) and Sudo et al. (2003) both used the rightmost extension algorithm to generate subtrees. 4.4 To provide a direct comparison of the pattern models we also produced versions of the sets of patterns extracted f"
W07-1211,rose-etal-2002-reuters,1,0.649331,"our models when the patterns are both filtered and unfiltered. (Although the set of unfiltered subtree patterns were not generated it is possible to determine the number of patterns which would be generated using a process described by Stevenson and Greenwood (2006).) Model SVO Chains Linked chains Subtrees Filtered 9,189 16,563 23,452 369,453 Unfiltered 23,128 142,019 493,463 1.69 ×1012 The value of β in equation 1 was set using a separate corpus from which the patterns were generated, a methodology suggested by Sudo et al. (2003). To generate this additional text we used the Reuters Corpus (Rose et al., 2002) which consists of a year’s worth of newswire output. Each document in the Reuters corpus has been manually annotated with topic codes indicating its general subject area(s). One of these topic codes (C411) refers to management succession events and was used to identify documents which are relevant to the MUC6 IE scenario. A corpus consisting of 348 documents annotated with code C411 and 250 documents without that code, representing irrelevant documents, were taken from the Reuters corpus to create a corpus with the same distribution of relevant and irrelevant documents as found in the MUC-6 c"
W07-1211,P05-1047,1,0.907888,"ually or automatically) create a set of patterns which match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of"
W07-1211,W06-0202,1,0.818187,"ree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while Sudo et al. (2003) allow any subtree within the dependency parse to act as an extraction pattern. Stevenson and Greenwood (2006) showed that the choice of pattern model has important implications for IE algorithms including significant differences between the various models in terms of their ability to identify information of interest in text. However, there has been little comparison between the various pattern models. Those which have been carried out have been limited by the fact that they used indirect tasks to evaluate the various models and did not compare them in an IE scenario. We address this limitation here by presenting a direct comparison of four previously described pattern models using an unsupervised lea"
W07-1211,H01-1009,0,0.127728,"a set of patterns which match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency pars"
W07-1211,P03-1029,0,0.13391,"hich match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) whi"
W07-1211,A00-1039,0,0.469395,"traction (IE) is to (manually or automatically) create a set of patterns which match against text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dep"
W07-1211,P03-1044,0,0.366212,"text to identify information of interest. Muslea (1999) reviewed the approaches which were used at the time and found that the most common techniques relied on lexicosyntactic patterns being applied to text which has undergone relatively shallow linguistic processing. For example, the extraction rules used by Soderland (1999) and Riloff (1996) match text in which syntactic chunks have been identified. More recently researchers have begun to employ deeper syntactic analysis, such as dependency parsing (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Sudo et al., 2001; Sudo et al., 2003; Yangarber, 2003). In these approaches extraction patterns are essentially parts of the dependency tree. To perform extraction they are compared against the dependency analysis of a sentence to determine whether it contains the pattern. Each of these approaches relies on a pattern model to define which parts of the dependency tree can be used to form the extraction patterns. A variety of pattern models have been proposed. For example the patterns used by Yangarber et al. (2000) are the subject-verb-object tuples from the dependency tree (the remainder of the dependency parse is discarded) while Sudo et al. (20"
W08-0611,W04-0813,1,0.901904,"Missing"
W08-0611,P04-1036,0,0.341367,"Missing"
W08-0611,W04-0807,0,0.200016,"Missing"
W08-0611,N01-1011,0,0.180154,"ncies of the ambiguous words than can be represented by the local collocations. Five relations are extracted: object, subject, noun-modifier, preposition and sibling. These are identified using heuristic patterns and regular expressions applied to PoS tag sequences around the ambiguous word. In the above example, “heparin” is noun-modifier feature of “adjustment”. 2 A maximum-entropy-based part of speech tagger was used (Ratnaparkhi, 1996) without the adaptation to the biomedical domain. 83 • Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). • Unigrams: Lemmas of unigrams which appear more frequently than a predefined threshold in the entire corpus, excluding those in a list of stopwords. We empirically set the threshold to 1. This feature was not used by Agirre and Mart´ınez (2004), but Joshi et al. (2005) found them to be useful for this task. Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al. (2007) to generate features based on UMLS Concept Unique Identifiers (CUIs). The MetaMap program (Aronson, 2001) identifies all words and terms in a text which could be mapped onto a UMLS CUI. MetaMap d"
W08-0611,W96-0213,0,0.0227158,"t-POS “NNS IN”, leftcontent-word-form “area adjustments”, rightfunction-word-form “adjustment of ”, etc. • Syntactic Dependencies: These features model longer-distance dependencies of the ambiguous words than can be represented by the local collocations. Five relations are extracted: object, subject, noun-modifier, preposition and sibling. These are identified using heuristic patterns and regular expressions applied to PoS tag sequences around the ambiguous word. In the above example, “heparin” is noun-modifier feature of “adjustment”. 2 A maximum-entropy-based part of speech tagger was used (Ratnaparkhi, 1996) without the adaptation to the biomedical domain. 83 • Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). • Unigrams: Lemmas of unigrams which appear more frequently than a predefined threshold in the entire corpus, excluding those in a list of stopwords. We empirically set the threshold to 1. This feature was not used by Agirre and Mart´ınez (2004), but Joshi et al. (2005) found them to be useful for this task. Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al. (2007) to generate features ba"
W08-0611,J01-3001,1,\N,Missing
W08-0611,J06-1003,0,\N,Missing
W09-1309,J08-4004,0,0.0645861,"een widely used in classification tasks. SVMs map feature vectors onto a high dimensional space and construct a classifier by searching for the hyperplane that gives the greatest separation between the classes. We used our own implementation of the Vector Space Model and Weka implementations (Witten and Frank, 2005) of the other two algorithms. 4 Evaluation Corpus The most common method for generating corpora to train and test WSD systems is to manually annotate instances of ambiguous terms found in text with the appropriate meaning. However, this process is both time-consuming and difficult (Artstein and Poesio, 2008). An alternative to manual tagging is to find a way of automatically creating sense tagged corpora. For the translation of ambiguous English words Ng et al. (2003) made use of the fact that the various senses are often translated differently. For example when “bank” is used in the ‘financial institution’ sense it is translated to French as “banque” and “bord” when it is used to mean ‘edge of river’. However, a disadvantage of this approach is that it relies on the existence of parallel text which may not be available. In the biomedical domain Liu et al. (2001)(2002) created a corpus using unam"
W09-1309,W04-0807,0,0.0216742,"scribe an approach which starts by identifying the set of candidate expansions in the same sentence as an abbreviation. The most likely one is identified by searching for the shortest candidate which contains all the characters in the abbreviation in the correct order. 3 Abbreviation Disambiguation System Our abbreviation disambiguation system is based on a state-of-the-art WSD system that has been adapted to the biomedical domain by augmenting it with additional knowledge sources. The system on which our approach is based (Agirre and Mart´ınez, 2004) participated in the Senseval-3 challenge (Mihalcea et al., 2004) with a performance close to the best system for the lexical sample tasks in two languages while the version adapted to the biomedical domain has achieved the best recorded results (Stevenson et al., 2008) on a standard test set consisting of ambiguous terms (Weeber et al., 2001). This system is based on a supervised learning approach with features derived from text around the ambiguous word that are domain independent. We refer to these as general features. This feature set has been adapted for the disambiguation of biomedical text by adding further linguistic features and two different types"
W09-1309,P03-1058,0,0.0376679,"atest separation between the classes. We used our own implementation of the Vector Space Model and Weka implementations (Witten and Frank, 2005) of the other two algorithms. 4 Evaluation Corpus The most common method for generating corpora to train and test WSD systems is to manually annotate instances of ambiguous terms found in text with the appropriate meaning. However, this process is both time-consuming and difficult (Artstein and Poesio, 2008). An alternative to manual tagging is to find a way of automatically creating sense tagged corpora. For the translation of ambiguous English words Ng et al. (2003) made use of the fact that the various senses are often translated differently. For example when “bank” is used in the ‘financial institution’ sense it is translated to French as “banque” and “bord” when it is used to mean ‘edge of river’. However, a disadvantage of this approach is that it relies on the existence of parallel text which may not be available. In the biomedical domain Liu et al. (2001)(2002) created a corpus using unambiguous related terms (see Section 2) although they found that it was not always possible to identify suitable related terms. 4.1 Corpus Creation Liu et al. (2001)"
W09-1309,C08-1083,0,0.142956,"Missing"
W09-1309,P02-1021,0,0.495723,"Missing"
W09-1309,N01-1011,0,0.150567,"following lemma/word-form of the content words (adjective, adverb, noun and verb) in the same sentence as the ambiguous abbreviation. For example, consider the sentence below with the target abreviation BSA. “Lean BSA was obtained from height and lean body weight ...” The features would include the following: left-content-word-lemma “lean BSA”, rightfunction-word-lemma “BSA be”, left-POS “JJ NNP”, right-POS “NNP VBD”, left-contentword-form “Lean BSA”, right-function-wordform “BSA was”, etc. • Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). • Unigrams: Lemmas of all content words in the abstract and words within a ±4-word window around the target word, excluding those in a list of stopwords. In addition, the lemmas of any unigrams appearing at least twice in the entire corpus and which are found in the abstract are also included as features. Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al. (2007) to generate features based on UMLS Concept Unique Identifiers (CUIs). The MetaMap program (Aronson, 2001) identifies all words and terms in a text which could be mapped onto a UMLS CUI. MetaMap does"
W09-1309,W02-0312,0,0.272251,"is often the case in biomedical documents, in this domain ubiquitous abbreviations (such as DNA and mRNA) often appear without an expansion. It has been reported that misinterpretation of abbreviations in biomedical documents has lead to medical practitioners making fatal errors (Fred and Cheng, 1999). However, identifying the correct expansion is not a straightforward task since an abbreviation may have several possible expansions. Chang et al. (2002) reported that abbreviations in biomedical journal articles consisting of six characters or less have an average of 4.61 possible meanings and Pustejovsky et al. (2002) mention that the simple abbreviation “AC” is associated with at least 10 strings in different biomedical documents including “atrioventricular connection”, “anterior colporrhaphy procedure”, “auditory cortex” and “atypical carcinoid”. The problem of identifying the correct expansion of an ambiguous abbreviation can be viewed as a Word Sense Disambiguation (WSD) task where the various expansions are the “senses” of the abbreviation. In this paper we approach the problem in this way by applying a WSD system which has previously been applied to biomedical text (Stevenson et al., 2008). The WSD s"
W09-1309,W04-0813,0,\N,Missing
W10-1907,W04-1013,0,0.0267144,"higher scores as the proportion of common concepts between the title and the target sentence is increased. Despite their simplicity, these are well accepted summarization heuristics that are commonly used (Bawakid and Oussalah, 2008; Bossard et al., 2008). The final selection of the sentences for the summary is based on the weighted sum of these feature values, as stated in (4). The values for the parameters λ, θ and χ have been empirically set to 0.8, 0.1, and 0.1 respectively. 6 Score(Sj ) = λ × SemGr(Sj ) + θ × Location(Sj ) + χ × T itle(Sj ) 5 6.1 (4) Experiments Setup The ROUGE metrics (Lin, 2004) are used to evaluate the system. ROUGE compares automatically generated summaries (called peers) against human-created summaries (called models), and calculates a set of measures to estimate the content quality of the summaries. Results are reported for the ROUGE-1 (R-1), ROUGE-2 (R2), ROUGE-SU4 (R-SU) and ROUGE-W (R-W) metrics. ROUGE-N (e.g. ROUGE-1 and ROUGE2) evaluates n-gram co-occurrences among the peer and models summaries, where N stands for the length of the n-grams. ROUGE-SU4 allows bi-gram to have intervening word gaps no longer than four words. Finally, ROUGE-W computes the union o"
W10-1907,W01-0100,0,0.177952,"suggests future lines of work. 2 Word sense disambiguation attempts to solve lexical ambiguities by identifying the correct meaning of a word based on its context. Supervised approaches have been shown to perform better than unsupervised ones (Agirre and Edmonds, 2006) but need large amounts of manually-tagged data, which are often unavailable or impractical to create. Knowledge-based approaches are a good alternative that do not require manually-tagged data. Related work Summarization has been an active area within NLP research since the 1950s and a variety of approaches have been proposed (Mani, 2001; Afantenos et al., 2005). Our focus is on graph-based summarization methods. Graph-based approaches typically represent the document as a graph, where the nodes represent text units (i.e. words, sentences or paragraphs), and the links represent cohesion relations or similarity measures between these units. The best-known work in the area is LexRank (Erkan and Radev, 2004). It assumes a fully connected and undirected graph, where each node corresponds to a sentence, represented by its TF-IDF vector, and the edges are labeled with the cosine similarity between the sentences. Mihalcea and Tarau"
W10-1907,W04-3252,0,0.371956,"mation regarding the main topic covered in the text. Automatic summarization of biomedical texts may benefit both health-care services and biomedical research (Reeve et al., 2007; Hunter and Cohen, 2006). Providing physicians with summaries of their patient records can help to reduce the diagnosis time. Researchers can use summaries to quickly determine whether a document is of interest without having to read it all. Summarization systems usually work with a representation of the document consisting of information that can be directly extracted from the document itself (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). However, recent studies have demonstrated the benefit of summarization based on richer representations that make use of external knowledge sources (Plaza et al., 2008; Fiszman et The purpose of this paper is to study the effect of lexical ambiguity in the knowledge source on semantic approaches to biomedical summarization. To this end, the paper describes a conceptbased summarization system for biomedical documents that uses the UMLS as an external knowledge source. To address the word ambiguity problem, we have adapted an existing WSD system (Agirre and Soroa, 2009) to assign concepts from"
W10-1907,E09-1005,0,0.470753,"Erkan and Radev, 2004; Mihalcea and Tarau, 2004). However, recent studies have demonstrated the benefit of summarization based on richer representations that make use of external knowledge sources (Plaza et al., 2008; Fiszman et The purpose of this paper is to study the effect of lexical ambiguity in the knowledge source on semantic approaches to biomedical summarization. To this end, the paper describes a conceptbased summarization system for biomedical documents that uses the UMLS as an external knowledge source. To address the word ambiguity problem, we have adapted an existing WSD system (Agirre and Soroa, 2009) to assign concepts from the UMLS. The system is applied to the summarization of 150 biomedical scientific articles from the BioMed Central corpus and it is found that 55 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 55–63, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics to represent the document at a conceptual level. In particular, in the biomedical domain Reeve et al. (2007) adapt the lexical chaining approach (Barzilay and Elhadad, 1997) to work with UMLS concepts, using the MetaMap Transfer Tool to annotate thes"
W10-1907,W08-2008,1,0.86921,"Missing"
W10-1907,W97-0703,0,0.0993949,". To address the word ambiguity problem, we have adapted an existing WSD system (Agirre and Soroa, 2009) to assign concepts from the UMLS. The system is applied to the summarization of 150 biomedical scientific articles from the BioMed Central corpus and it is found that 55 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 55–63, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics to represent the document at a conceptual level. In particular, in the biomedical domain Reeve et al. (2007) adapt the lexical chaining approach (Barzilay and Elhadad, 1997) to work with UMLS concepts, using the MetaMap Transfer Tool to annotate these concepts. Yoo et al. (2007) represent a corpus of documents as a graph, where the nodes are the MeSH descriptors found in the corpus, and the edges represent hypernymy and co-occurrence relations between them. They cluster the MeSH concepts in the corpus to identify sets of documents dealing with the same topic and then generate a summary from each document cluster. WSD improves the quality of the summaries. This paper is, to our knowledge, the first to apply WSD to the summarization of biomedical documents and also"
W10-1907,W04-2611,0,0.0344272,"cs.shef.ac.uk albertodiaz@fdi.ucm.es Universidad Complutense de Madrid, C/Prof. Jos´e Garc´ıa Santesmases, 28040 Madrid, Spain ∗ University of Sheffield, Regent Court, 211 Portobello St., Sheffield, S1 4DP, UK Abstract al., 2004). These approaches can represent semantic associations between the words and terms in the document (i.e. synonymy, hypernymy, homonymy or co-occurrence) and use this information to improve the quality of the summaries. In the biomedical domain the Unified Medical Language System (UMLS) (Nelson et al., 2002) has proved to be a useful knowledge source for summarization (Fiszman et al., 2004; Reeve et al., 2007; Plaza et al., 2008). In order to access the information contained in the UMLS, the vocabulary of the document being summarized has to be mapped onto it. However, ambiguity is common in biomedical documents (Weeber et al., 2001). For example, the string “cold” is associated with seven possible meanings in the UMLS Metathesuarus including “common cold”, “cold sensation” , “cold temperature” and “Chronic Obstructive Airway Disease”. The majority of summarization systems in the biomedical domain rely on MetaMap (Aronson, 2001) to map the text onto concepts from the UMLS Metat"
W12-1012,N09-1003,0,0.0900202,"Missing"
W12-1012,N10-1125,0,0.0994089,"ld help to identify them automatically. However, previous work on computing similarity in the CH domain has been limited and, in particular, has not made use of information from multiple types of media. For example, Grieser et al. (2011) computed similarity between exhibits in Melbourne Museum by applying a range of text similarity measures but did not make use of other media. Techniques for exploiting information from multimedia collections have been developed and are commonly applied to a wide range of problems such as Content-based Image Retrieval (Datta et al., 2008) and image annotation (Feng and Lapata, 2010). Introduction and Motivation In recent years a vast amount of Cultural Heritage (CH) artefacts have been digitised and made available on-line. For example, the Louvre and the British Museum provide information about exhibits on their web pages1 . In addition, information is also available via sites that aggregate CH information from multiple resources. A typical example is Europeana2 , a web-portal to collections from several European institutions that provides access to over 20 million items including paintings, films, books, archives and museum exhibits. However, online information about CH"
W12-1012,P06-2071,0,0.0617018,"Missing"
W12-1014,E06-1002,0,0.127821,"of times it is used a destination from some anchor text e.g. the anchor text ‘Tree’ links to the article about the plant more often than the mathematical concept and is thus more common. Relatedness gives a measure of the similarity of two articles by comparing their incoming and outgoing links. The performance achieved using their approach is currently state of the art for this task. The WikiMiner software is freely available2 , and has been used as the basis for the approaches presented here. Recent work on named entity linking and wikification makes use of categories and link information (Bunescu and Pasca, 2006; Dakka and Cucerzan, 2008; Kulkarni et al., 2009). Wikification has also been applied to the medical domain (He et al., 2011). Wikipedia categories and links have been used previously to find the similarity between CH items (Grieser et al., 2011). The category retraining approach presented here differs in that it only makes use of the top-level categories. 3 Methods Three approaches to improving the quality of Wikipedia links added by WikiMiner were developed. The first two make use of Wikipedia’s category structure while the third uses the links between Wikipedia articles. 3.1 Wikipedia Cate"
W12-1014,I08-1071,0,0.0213452,"stination from some anchor text e.g. the anchor text ‘Tree’ links to the article about the plant more often than the mathematical concept and is thus more common. Relatedness gives a measure of the similarity of two articles by comparing their incoming and outgoing links. The performance achieved using their approach is currently state of the art for this task. The WikiMiner software is freely available2 , and has been used as the basis for the approaches presented here. Recent work on named entity linking and wikification makes use of categories and link information (Bunescu and Pasca, 2006; Dakka and Cucerzan, 2008; Kulkarni et al., 2009). Wikification has also been applied to the medical domain (He et al., 2011). Wikipedia categories and links have been used previously to find the similarity between CH items (Grieser et al., 2011). The category retraining approach presented here differs in that it only makes use of the top-level categories. 3 Methods Three approaches to improving the quality of Wikipedia links added by WikiMiner were developed. The first two make use of Wikipedia’s category structure while the third uses the links between Wikipedia articles. 3.1 Wikipedia Categories Almost all articles"
W12-2429,W04-0813,0,0.0400896,"Missing"
W12-2429,W04-3204,0,0.033142,"Missing"
W12-2429,J08-4004,0,0.0431352,"upervised learning (McInnes et al., 2007; Xu et al., 2007; Stevenson et al., 2008; Yepes and Aronson, 2011). These systems require labeled training data, examples of an ambiguous term labeled with the correct meaning. Some sets of labeled data have been developed for the biomedical domain (Weeber et al., 2001; Savova et al., 2008; JimenoYepes et al., 2011). However, these data sets only contain examples for a few hundred terms and can only be used to develop WSD systems to identify the meanings of those terms. The process of creating labeled examples is extremely time-consuming and difficult (Artstein and Poesio, 2008), making it impractical to create labeled examples of all possible ambiguous terms found in biomedical documents. Two alternative approaches have been explored to develop systems which are able to disambiguate all ambiguous terms in biomedical documents. The first makes use of unsupervised WSD algorithms (see Section 2.1) and the second creates labeled data automatically and uses it to train a supervised WSD system (see Section 2.2). 232 2.1 Unsupervised WSD Unsupervised WSD algorithms make use of information from some knowledge source, rather than relying on training data. Humphrey et al. (20"
W12-2429,J98-1006,0,0.444044,"ated. This information is then combined with the graph and a ranked list of the possible senses for each ambiguous word generated. Unsupervised systems have the advantage of being able to disambiguate all ambiguous terms. However, the performance of unsupervised systems that have been developed for biomedical documents is lower than that of supervised ones. 2.2 Automatic Generation of Labeled Data Automatic generation of labeled data for WSD combines the accuracy of supervised approaches with the ability of unsupervised approaches to disambiguate all ambiguous terms. It was first suggested by Leacock et al. (1998). Their approach is based on the observation that some terms in a lexicon occur only once and, consequently, there is no doubt about their meaning. These are referred to as being monosemous. Examples for each possible meaning of an ambiguous term are generated by identifying the closest monosemous term (the monosemous relative) in the lexicon and using examples of that term. Variants of the approach have been applied to the biomedical domain using the UMLS Metathesaurus as the sense inventory. Liu et al. (2002) were the first to apply the monosemous relatives approach to biomedical WSD and use"
W12-2429,P04-1036,0,0.04322,"r the NLM-WSD corpus manual annotation is used to decide the number of instances that are annotated with each sense of an ambiguous term. However, the NLM-MSH corpus was constructed automatically and each ambiguous term has roughly the same number of examples of each possible sense. 4.2 Experiments The WSD system described in Section 3 was tested using each of the three techniques for determining the bias, i.e. number of examples generated for each CUI. Performance is compared against various alternative approaches. Two supervised approaches are included. The first, most frequent sense (MFS) (McCarthy et al., 2004), is widely used baseline for supervised WSD systems. It consists of assigning each ambiguous term the meaning that is more frequently observed in the training data. The second supervised approach 7 http://wsd.nlm.nih.gov is to train the WSD system using manually labeled examples from the NLM-WSD and MSH-WSD corpora. 10-fold cross validation is applied to evaluate this approach. Performance of the Personalised Page Rank approach described in Section 2.1 is also provided to allow comparison with an unsupervised algorithm. Both Personalised Page Rank and the techniques we employ to generate labe"
W12-2429,W96-0208,0,0.156098,"omatically create the labeled data that is used to train supervised WSD systems. Several approaches (Liu et al., 2002; Stevenson and Guo, 2010; JimenoYepes and Aronson, 2010) have used information from the UMLS Metathesaurus1 to create labeled training data that have successfully been used to create WSD systems. A key decision for any system that automatically generates labeled examples is the number of examples of each sense to create, known as the bias of the data set. It has been shown that the bias of a set of labeled examples affects the performance of the WSD system it is used to train (Mooney, 1996; Agirre and Mart´ınez, 2004b). Some of the previous approaches to generating labeled data relied on manually annotated examples to determine the bias of the data sets and were therefore not completely unsupervised. This paper describes the development of a large scale WSD system that is able to disambiguate all 1 http://www.nlm.nih.gov/research/umls/ 231 Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 231–239, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics terms that are ambiguous in the UMLS Metathesaurus. The"
W12-2429,W11-0223,0,0.0275205,"lly labeled examples and that their performance is better than a state-of-the-art unsupervised approach. The remainder of this paper is organized as follows. Previous approaches to WSD in biomedical documents are described in the next Section. Section 3 presents the methods used to identify bias in the labeled examples and WSD system. Experiments in which these approaches are compared are described in Section 4 and their results in Section 5. 2 Background Many WSD systems for the biomedical domain are based on supervised learning (McInnes et al., 2007; Xu et al., 2007; Stevenson et al., 2008; Yepes and Aronson, 2011). These systems require labeled training data, examples of an ambiguous term labeled with the correct meaning. Some sets of labeled data have been developed for the biomedical domain (Weeber et al., 2001; Savova et al., 2008; JimenoYepes et al., 2011). However, these data sets only contain examples for a few hundred terms and can only be used to develop WSD systems to identify the meanings of those terms. The process of creating labeled examples is extremely time-consuming and difficult (Artstein and Poesio, 2008), making it impractical to create labeled examples of all possible ambiguous term"
W13-0102,D07-1109,0,0.0121182,"l Information (PMI). Topic coherence is determined by measuring the distance between these vectors computed using a variety of metrics. Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task. 1 Introduction Topic modelling is a popular statistical method for (soft) clustering documents (Blei et al., 2003; Deerwester et al., 1990; Hofmann, 1999). Latent Dirichlet Allocation (LDA) (Blei et al., 2003), one type of topic model, has been widely used in NLP and applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009) and generation of comparable corpora (Preiss, 2012). A variety of approaches has been proposed to evaluate the topics generated by these models. The first to be explored were extrinsic methods, measuring the performance achieved by a model in a specific task or using statistical methods. For example, topic models have been evaluated by measuring their accuracy for information retrieval (Wei and Croft, 2006). Statistical methods have also been applied to measure the predictive likelihood of a topic model in held-out documents by co"
W13-0102,N09-1041,0,0.0412623,"suring the distance between these vectors computed using a variety of metrics. Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task. 1 Introduction Topic modelling is a popular statistical method for (soft) clustering documents (Blei et al., 2003; Deerwester et al., 1990; Hofmann, 1999). Latent Dirichlet Allocation (LDA) (Blei et al., 2003), one type of topic model, has been widely used in NLP and applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009) and generation of comparable corpora (Preiss, 2012). A variety of approaches has been proposed to evaluate the topics generated by these models. The first to be explored were extrinsic methods, measuring the performance achieved by a model in a specific task or using statistical methods. For example, topic models have been evaluated by measuring their accuracy for information retrieval (Wei and Croft, 2006). Statistical methods have also been applied to measure the predictive likelihood of a topic model in held-out documents by computing their perplexity. Wallach et al. (2009) gives a detaile"
W13-0102,islam-inkpen-2006-second,0,0.00952619,"wj )) (7) Finally, we introduce γ which is a parameter to assign more emphasis on context features with high PMI (or NPMI) values with a topic word. Vectors are weighted using P M I(wi , fj )γ or N P M I(wi , fj )γ where wi is a topic word and fj is a context feature. For all of our experiments we set γ = 2 which was found to produce the best results. 3.2.2 Reducing the Basis Including all co-occurring terms in the vectors leads to a high dimensional space. We also experimented with two approaches to reducing the number of terms to form a semantic space with smaller basis. Firstly, following Islam and Inkpen (2006), a Reduced Semantic Space is created by choosing the βwi most related context features for each topic word wi : 2 (log2 (m)) βwi = log(c(wi )) (8) δ where δ is a parameter for adjusting the number of features for each word and m is the size of the corpus. Varying the value of δ did not effect performance for values above 1. This parameter was set of 3 for the results reported here. In addition a frequency cut-off of 20 was also applied. In addition, a smaller semantic space was created by considering only topic words as context features, leading to n features for each topic word. This is ref"
W13-0102,P11-1154,0,0.230023,"Missing"
W13-0102,D11-1024,0,0.835817,"on of document collections (Chaney and Blei, 2012; Newman et al., 2010a), where automatically generated topics are used to provide an overview of the collection and the top-n words in each topic used to represent it. Chang et al. (2009) showed that humans find topics generated by models with high predictive likelihood to be less coherent than topics generated from others with lower predictive likelihood. Following Chang’s findings, recent work on evaluation of topic models has been focused on automatically measuring the coherence of generated topics by comparing them against human judgements (Mimno et al., 2011; Newman et al., 2010b). Newman et al. (2010b) define topic coherence as the average semantic relatedness between topic words and report the best correlation with humans using the Pointwise Mutual Information (PMI) between topic words in Wikipedia. 1: oil, louisiana, coast, gulf, orleans, spill, state, fisherman, fishing, seafood 2: north, kim, korea, korean, jong, south, il, official, party, son 3: model, wheel, engine, system, drive, front, vehicle, rear, speed, power 4: drink, alcohol, indonesia, drinking, indonesian, four, nokia, beverage, mcdonald, caffeine 5: privacy, andrews, elli, alex"
W13-0102,N10-1012,0,0.316027,"rics. However, these approaches do not provide any information about how interpretable the topics are to humans. Figure 1 shows some example topics generated by a topic model. The first three topics appear quite coherent, all the terms in each topic are associated with a common theme. On the other hand, it is difficult to identify a coherent theme connecting all of the words in topics 4 and 5. These topics are difficult to interpret and could be considered as “junk” topics. Interpretable topics are important in applications such as visualisation of document collections (Chaney and Blei, 2012; Newman et al., 2010a), where automatically generated topics are used to provide an overview of the collection and the top-n words in each topic used to represent it. Chang et al. (2009) showed that humans find topics generated by models with high predictive likelihood to be less coherent than topics generated from others with lower predictive likelihood. Following Chang’s findings, recent work on evaluation of topic models has been focused on automatically measuring the coherence of generated topics by comparing them against human judgements (Mimno et al., 2011; Newman et al., 2010b). Newman et al. (2010b) defin"
W13-0102,P03-1017,0,0.0616069,"Missing"
W13-0102,N12-1065,0,0.0214637,"metrics. Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task. 1 Introduction Topic modelling is a popular statistical method for (soft) clustering documents (Blei et al., 2003; Deerwester et al., 1990; Hofmann, 1999). Latent Dirichlet Allocation (LDA) (Blei et al., 2003), one type of topic model, has been widely used in NLP and applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009) and generation of comparable corpora (Preiss, 2012). A variety of approaches has been proposed to evaluate the topics generated by these models. The first to be explored were extrinsic methods, measuring the performance achieved by a model in a specific task or using statistical methods. For example, topic models have been evaluated by measuring their accuracy for information retrieval (Wei and Croft, 2006). Statistical methods have also been applied to measure the predictive likelihood of a topic model in held-out documents by computing their perplexity. Wallach et al. (2009) gives a detailed description of such statistical metrics. However,"
W13-2018,W11-1828,0,0.0452773,"Missing"
W13-2018,W09-1402,0,0.0597877,"Missing"
W13-2018,W11-1820,0,0.0193823,"n stages: trigger detection, argument detection and post-processing. Bj¨orne and Salakoski (2011) improved the performance of this system for BioNLP 2011, but was outperformed by FAUST. Our approach to the BioNLP Shared Task relies on separating the process of event classification into multiple stages and creates separate classifiers for each event type. Our system begins by pre-processing the input text, followed by multiple classification stages and a post-processing stage. The pre-processing applies tokenization, sentence splitting and dictionary-based trigger detection, similar to Bui and Sloot (2011). Classification is based on a Support Vector Machine (SVM) and uses three main stages: trigger-protein detection, trigger-event detection and event-cause detection. Post-processing is a combination of classification and rule-based approaches. We train a separate classifier for each event type, rather that relying on a single classifier to recognise trigger-theme relaIntroduction The BioNLP 2013 Shared Task focuses on information extraction in the biomedical domain and comprises of a range of extraction tasks. Our system was developed to participate within the Genia Event Extraction task (GE),"
W13-2018,W09-1401,0,0.0431989,"s. All events consist of a core event, which contains a trigger word and a theme. With the exception of regulation events, the theme always refer to a protein. A regulation event theme can either refer to a protein or to another event. Binding events can include up to two proteins as themes. In addition to the core event, events may include additional arguments such as ‘cause’ or ‘to location’. Figure 1 shows examples of events from the BioNLP 2013 corpus. More details about the Genia Event task can be found in Kim et al. (2011). Previous editions of the BioNLP Shared Task took place in 2009 (Kim et al., 2009) and 2011 125 Proceedings of the BioNLP Shared Task 2013 Workshop, pages 125–129, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics tionships for all event types. In addition, we also optimise the SVM’s parameters and apply feature selection for each event type. Our system participated in subtask 1 of the GE task, which involves the recognition of core events, including identification of their ‘cause’. The remainder of this paper describes our system in detail (Section 2), presents results from the Genia Event Extraction task (Section 3) and draws the conclusions"
W13-2018,W11-1802,0,0.107241,"arates the process of event classification into multiple stages. For each event type the SVM parameters are adjusted and feature selection carried out. We find that this optimisation improves the performance of our approach. Overall our system achieved the highest precision score of all systems and was ranked 6th of 10 participating systems on F-measure (strict matching). 1 Figure 1: Two events from the BioNLP 2013 GE task: a phosphorylation event consisting of a trigger and a protein and a positive-regulation event consisting of a trigger, a theme referring to an event and a cause argument. (Kim et al., 2011). Promising approaches in the most recent competition were event parsing (McClosky et al., 2011) and dual decomposition models (Riedel and McCallum, 2011). The winner of the GE task 2011, FAUST (Riedel et al., 2011), combined these two approaches by using result from the event parser as an additional input feature for the dual decomposition. The UTurku system of Bj¨orne et al. (2009) was the winner of the GE task in 2009. The system was based on a pipeline containing three main stages: trigger detection, argument detection and post-processing. Bj¨orne and Salakoski (2011) improved the performa"
W13-2018,W11-1806,0,0.0123866,"M parameters are adjusted and feature selection carried out. We find that this optimisation improves the performance of our approach. Overall our system achieved the highest precision score of all systems and was ranked 6th of 10 participating systems on F-measure (strict matching). 1 Figure 1: Two events from the BioNLP 2013 GE task: a phosphorylation event consisting of a trigger and a protein and a positive-regulation event consisting of a trigger, a theme referring to an event and a cause argument. (Kim et al., 2011). Promising approaches in the most recent competition were event parsing (McClosky et al., 2011) and dual decomposition models (Riedel and McCallum, 2011). The winner of the GE task 2011, FAUST (Riedel et al., 2011), combined these two approaches by using result from the event parser as an additional input feature for the dual decomposition. The UTurku system of Bj¨orne et al. (2009) was the winner of the GE task in 2009. The system was based on a pipeline containing three main stages: trigger detection, argument detection and post-processing. Bj¨orne and Salakoski (2011) improved the performance of this system for BioNLP 2011, but was outperformed by FAUST. Our approach to the BioNLP Sh"
W13-2018,W11-1807,0,0.016081,"d out. We find that this optimisation improves the performance of our approach. Overall our system achieved the highest precision score of all systems and was ranked 6th of 10 participating systems on F-measure (strict matching). 1 Figure 1: Two events from the BioNLP 2013 GE task: a phosphorylation event consisting of a trigger and a protein and a positive-regulation event consisting of a trigger, a theme referring to an event and a cause argument. (Kim et al., 2011). Promising approaches in the most recent competition were event parsing (McClosky et al., 2011) and dual decomposition models (Riedel and McCallum, 2011). The winner of the GE task 2011, FAUST (Riedel et al., 2011), combined these two approaches by using result from the event parser as an additional input feature for the dual decomposition. The UTurku system of Bj¨orne et al. (2009) was the winner of the GE task in 2009. The system was based on a pipeline containing three main stages: trigger detection, argument detection and post-processing. Bj¨orne and Salakoski (2011) improved the performance of this system for BioNLP 2011, but was outperformed by FAUST. Our approach to the BioNLP Shared Task relies on separating the process of event classi"
W13-2018,W11-1808,0,0.0221817,"ur approach. Overall our system achieved the highest precision score of all systems and was ranked 6th of 10 participating systems on F-measure (strict matching). 1 Figure 1: Two events from the BioNLP 2013 GE task: a phosphorylation event consisting of a trigger and a protein and a positive-regulation event consisting of a trigger, a theme referring to an event and a cause argument. (Kim et al., 2011). Promising approaches in the most recent competition were event parsing (McClosky et al., 2011) and dual decomposition models (Riedel and McCallum, 2011). The winner of the GE task 2011, FAUST (Riedel et al., 2011), combined these two approaches by using result from the event parser as an additional input feature for the dual decomposition. The UTurku system of Bj¨orne et al. (2009) was the winner of the GE task in 2009. The system was based on a pipeline containing three main stages: trigger detection, argument detection and post-processing. Bj¨orne and Salakoski (2011) improved the performance of this system for BioNLP 2011, but was outperformed by FAUST. Our approach to the BioNLP Shared Task relies on separating the process of event classification into multiple stages and creates separate classifier"
W13-2701,P10-1158,0,0.0225347,"e digital content in the form of exhibitions, tours and trails. M¨akel¨a et al. (2007) describe a system which utilises semantically annotated content to generate personalised ‘exhibitions’ from a structured narrative-based search query. Similarly, Zdrahal et al. (2008) demonstrate how pathways can be generated through a collection of semantically related documents to provide a means of exploration, using non-NLP clustering and path creation techniques. Sophisticated approaches such as linear programming and evolutionary algorithms have also been proposed for generating summaries and stories (McIntyre and Lapata, 2010; Woodsend and Lapata, 2010). In contrast, Wang et al. (2007) use a recommender system approach to generate museum tours on the basis of ratings stored within a dynamic user model, and Pechenizkiy and Calders (2007) propose the additional use of data mining techniques on log data to improve this type of tour personalisation. Initial user requirements interviews with 22 expert users in the heritage, education and professional domains found a strong affinity with the path metaphor, revealing a range of different interpretations of what it means in the CH context and how they could be employed in"
W13-2701,P10-1058,0,0.0293729,"rm of exhibitions, tours and trails. M¨akel¨a et al. (2007) describe a system which utilises semantically annotated content to generate personalised ‘exhibitions’ from a structured narrative-based search query. Similarly, Zdrahal et al. (2008) demonstrate how pathways can be generated through a collection of semantically related documents to provide a means of exploration, using non-NLP clustering and path creation techniques. Sophisticated approaches such as linear programming and evolutionary algorithms have also been proposed for generating summaries and stories (McIntyre and Lapata, 2010; Woodsend and Lapata, 2010). In contrast, Wang et al. (2007) use a recommender system approach to generate museum tours on the basis of ratings stored within a dynamic user model, and Pechenizkiy and Calders (2007) propose the additional use of data mining techniques on log data to improve this type of tour personalisation. Initial user requirements interviews with 22 expert users in the heritage, education and professional domains found a strong affinity with the path metaphor, revealing a range of different interpretations of what it means in the CH context and how they could be employed in an online environment to en"
W13-2701,agirre-etal-2012-matching,1,\N,Missing
W13-2701,L12-1000,0,\N,Missing
W14-1112,P11-2048,0,0.0180468,"rst results using UMLS for distant supervision. This paper describes first results using the Unified Medical Language System (UMLS) for distantly supervised relation extraction. UMLS is a large knowledge base which contains information about millions of medical concepts and relations between them. Our approach is evaluated using existing relation extraction data sets that contain relations that are similar to some of those in UMLS. 1 Introduction Distant supervision has proved to be a popular approach to relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009; Hoffmann et al., 2010; Nguyen and Moschitti, 2011). It has the advantage that it does not require manually annotated training data. Distant supervision avoids this by using information from a knowledge base to automatically identify instances of a relation from text and use them in order to generate training data for a relation extraction system. Distant supervision has already been applied to the biomedical domain (Craven and Kumlien, 1999; Thomas et al., 2011). Craven and Kumlien (1999) were the first to apply distant supervision and used the Yeast Protein Database (YPD) to detect sentences containing subcellar-localization relations. Thoma"
W14-1112,S13-2057,0,0.0212924,"otated with UMLS relations were available. We showed that using a distantly supervised classifier trained on MRREL relations similar to those found in the evaluation data set provides promising results. Overall, our system works with some components which should be improved to achieve better results. First, we rely on a cheap and fast annotation using MetaMap, which might produce annotation errors. In addition, the use of noisy distant supervised training data decreases the classification quality. An improvement of the selection process and an improvement of the classification method, such as Chowdhury and Lavelli (2013), could lead to better classification results. In future we would also like to make further use of existing data sets with similar relations to those of interest to evaluate distant supervision approaches. Prec. / Recall / F1 0.874 / 1.000 / 0.933 0.944 / 0.923 / 0.934 0.059 / 1.000 / 0.112 0.909 / 0.667 / 0.769 Table 4: Rosario & Hearst baseline results Table 5 shows the results for the various distant supervision approaches. Again, 1000 positive training examples were used to train the classifier. Since the F-Score of the naive and the supervised approaches of TREAT are very high, it is diff"
W14-1112,Q13-1030,0,0.293741,"S for Distantly Supervised Relation Detection Roland Roller and Mark Stevenson University of Sheffield Regent Court, 211 Portobello S1 4DP Sheffield, UK {R.Roller,M.Stevenson}@dcs.shef.ac.uk Abstract quality of the automatically identified training instances identified by the self-annotation. The use of instances that have been incorrectly labelled as positive can lower performance (Takamatsu et al., 2012). Another problem arises when positive examples are included in the set of negative training instances, which can occur when information is missing from the knowledge base (Min et al., 2013; Ritter et al., 2013; Xu et al., 2013). Evaluation of relation extraction systems that use distant supervision represents a further challenge. In the ideal case an annotated evaluation set is available. Others, such as Ritter et al. (2013) and Hoffmann et al. (2011), use Freebase as knowledge base and evaluate their classifier on an annotated New York Times corpus. However, if no evaluation set is available leave-out can be used where the data identified using distant supervision used for both training and testing (Hoffmann et al., 2010). This paper makes use of the Unified Medical Language System (UMLS) as a kno"
W14-1112,W13-2018,1,0.834222,"sing UMLS. The first experiment will be evaluated on a subset of the DDI 2011 training data set using the MRREL relation has contraindicated drug and has contraindication. The second experiment uses the MRREL relations may be treated by and may be prevented by and are evaluated on the Rosario & Hearst data set. We use 7,500,000 Medline abstracts annotated with CUIs using MetaMap (choosing the best mapping as annotation) as a corpus for distant supervision. Our information extraction platform based on a system developed for the BioNLP https://www.nlm.nih.gov/research/umls/ 81 Shared Task 2013 (Roller and Stevenson, 2013). In contrast to our previous work, our classification process relies on the Shallow Linguistic Kernel (Giuliano et al., 2006) in combination with LibSVM (Chang and Lin, 2011) taking the kernel as input. 4.1 we also apply a combination of both techniques (5w+com). 1000 positive examples were generated using each approach and used for training. Although it would be possible to generate more examples for some approaches, for example basic, applying the combination of techniques (5w+com) significantly reduces the number of instances available. Experiment 1: DDI 2011 The DDI 2011 data set was spli"
W14-1112,P04-1055,0,0.260216,"n. The use of Freebase to train a classifier, e.g. (Mintz et al., 2009; Riedel et al., 2010), has proved popular. Other, such as Hoffmann et al. (2010), use Wikipedia info-boxes as the knowledge base. Applications of distant supervision face several challenges. The main problem is ensuring the 80 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 80–84, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Unified Medical Language System match for the DDI relation. The data set described by Rosario and Hearst (2004) focuses on different relationships between treatments and diseases. The two most common relations TREAT FOR DIS (TREAT), denoting the treatment for a particular disease, and PREVENT (PREV), which indicates that a treatment can be used to prevent a disease. The MRREL isa relationship also matches many of these relations, again due to its prevalence in MRREL. Other MRREL relations (may be prevented by and may be treated by) match fewer CUI pairs but seem to be better matches for the TREAT and PREV relations. The Unified Medical Language System1 is a set of files and software which combines diff"
W14-1112,E06-1051,0,0.184661,"indicated drug and has contraindication. The second experiment uses the MRREL relations may be treated by and may be prevented by and are evaluated on the Rosario & Hearst data set. We use 7,500,000 Medline abstracts annotated with CUIs using MetaMap (choosing the best mapping as annotation) as a corpus for distant supervision. Our information extraction platform based on a system developed for the BioNLP https://www.nlm.nih.gov/research/umls/ 81 Shared Task 2013 (Roller and Stevenson, 2013). In contrast to our previous work, our classification process relies on the Shallow Linguistic Kernel (Giuliano et al., 2006) in combination with LibSVM (Chang and Lin, 2011) taking the kernel as input. 4.1 we also apply a combination of both techniques (5w+com). 1000 positive examples were generated using each approach and used for training. Although it would be possible to generate more examples for some approaches, for example basic, applying the combination of techniques (5w+com) significantly reduces the number of instances available. Experiment 1: DDI 2011 The DDI 2011 data set was split into training and test sets for the experiments. Table 2 presents results that place the distant supervision performance in"
W14-1112,P10-1030,0,0.452315,"Missing"
W14-1112,P11-1055,0,0.0592177,"ining instances identified by the self-annotation. The use of instances that have been incorrectly labelled as positive can lower performance (Takamatsu et al., 2012). Another problem arises when positive examples are included in the set of negative training instances, which can occur when information is missing from the knowledge base (Min et al., 2013; Ritter et al., 2013; Xu et al., 2013). Evaluation of relation extraction systems that use distant supervision represents a further challenge. In the ideal case an annotated evaluation set is available. Others, such as Ritter et al. (2013) and Hoffmann et al. (2011), use Freebase as knowledge base and evaluate their classifier on an annotated New York Times corpus. However, if no evaluation set is available leave-out can be used where the data identified using distant supervision used for both training and testing (Hoffmann et al., 2010). This paper makes use of the Unified Medical Language System (UMLS) as a knowledge source for distant supervision. It is widely used for biomedical language processing and readily available. The advantage of UMLS is that it contains information about a wide range of different types of relations and therefore has the pote"
W14-1112,P12-1076,0,0.0548639,"by selecting pairs of CUIs that are occur in any other MRREL relation. Sentences containing these CUI pairs are identified in the subset of the MetaMapped Medline. In the basic setup (basic), sentences containing a positive pair will be considered as a positive training example. There are many cases where just the occurrence of a positive MRREL pair does not express the target relation. In an effort to remove this noisy data we apply some simple heuristics. The first discards all training instances with more than five words (5w) between the two entities, an approach similar to one applied by Takamatsu et al. (2012). The second discards positive sentences containing a comma between the related entities (com). We found that commas often indicate a sentence containing a list of items (e.g. genes or diseases) and that these sentences do not form good training examples due to the multiple relations that are possible when there are several items. Finally 4.2 Experiment 2: Rosario & Hearst The second experiment addresses the problem of detecting the MRREL relations may be prevented by and may be treated by. Parts of the Rosario & Hearst data set are used to evaluate this relation. This data set differs in stru"
W14-1112,N13-1095,0,0.103231,"Missing"
W14-1112,W11-3904,0,0.483102,"ose in UMLS. 1 Introduction Distant supervision has proved to be a popular approach to relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009; Hoffmann et al., 2010; Nguyen and Moschitti, 2011). It has the advantage that it does not require manually annotated training data. Distant supervision avoids this by using information from a knowledge base to automatically identify instances of a relation from text and use them in order to generate training data for a relation extraction system. Distant supervision has already been applied to the biomedical domain (Craven and Kumlien, 1999; Thomas et al., 2011). Craven and Kumlien (1999) were the first to apply distant supervision and used the Yeast Protein Database (YPD) to detect sentences containing subcellar-localization relations. Thomas et al. (2011) trained a classifier for protein-protein interactions (PPI) using the knowledge base IntAct and evaluated their approach on different PPI corpora. There have also been recent applications of distant supervision outside the biomedical domain. The use of Freebase to train a classifier, e.g. (Mintz et al., 2009; Riedel et al., 2010), has proved popular. Other, such as Hoffmann et al. (2010), use Wiki"
W14-1112,P13-2117,0,0.546355,"Missing"
W14-1112,P09-1113,0,0.169339,"n relations. In section 4 we present our first results using UMLS for distant supervision. This paper describes first results using the Unified Medical Language System (UMLS) for distantly supervised relation extraction. UMLS is a large knowledge base which contains information about millions of medical concepts and relations between them. Our approach is evaluated using existing relation extraction data sets that contain relations that are similar to some of those in UMLS. 1 Introduction Distant supervision has proved to be a popular approach to relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009; Hoffmann et al., 2010; Nguyen and Moschitti, 2011). It has the advantage that it does not require manually annotated training data. Distant supervision avoids this by using information from a knowledge base to automatically identify instances of a relation from text and use them in order to generate training data for a relation extraction system. Distant supervision has already been applied to the biomedical domain (Craven and Kumlien, 1999; Thomas et al., 2011). Craven and Kumlien (1999) were the first to apply distant supervision and used the Yeast Protein Database (YPD) to detect sentence"
W15-2612,P10-1030,0,0.0322696,"Missing"
W15-2612,N13-1095,0,0.0135627,"mance obtained. The paper concludes with section 5. quality as manually labelled data. Despite this distantly supervised relation extraction provides reasonable results compared to those based on supervised learning (see e.g. in (Thomas et al., 2011)). Distant supervision allows relation extraction systems to be created without manually labelled data. However, this raises the issue of how such a system can be evaluated. Previous approaches have carried out evaluation using existing data sets labelled with examples of the target relation (Bellare and Mccallum, 2007; Nguyen and Moschitti, 2011; Min et al., 2013) or a similar relation (Thomas et al., 2011; Roller and Stevenson, 2014). However, in the majority of scenarios the best use for any labeled data available is as training data. Others, such as Craven and Kumlien (1999), generated their own gold standard to annotate relevant relations of their knowledge base. But the effort required to generate manually labelled evaluation data somewhat negates the benefit of reduced development time provided by distant supervision. An alternative approach, which does not require any labelled data, is held-out evaluation. This approach splits facts from the kno"
W15-2612,W13-2001,0,0.0269119,"Missing"
W15-2612,D12-1042,0,0.0232622,"its facts from the knowledge base into two parts: one to generate distantly supervised training data and the other to generate distantly supervised evaluation data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2010; Roller et al., 2015). This approach is often combined with a manual evaluation in which a subset of the predictions is selected to be examined in more detail. For example, Riedel et al. (2010) supplemented the held-out evaluation of their distant supervision approach for Freebase by selecting the top 1000 facts it predicted and evaluating them manually. Others such as Surdeanu et al. (2012) and Intxaurrondo et al. (2013) work with the same knowledge base and are able to re-use the manually labelled data generated by Riedel et al. (2010). However, this data is only available for some Freebase relations and evaluation data has to be generated for each new relation. Approaches such as Takamatsu et al. (2012), Zhang et al. (2013) and Augenstein et al. (2014) combine a held-out evaluation with a manual evaluation of a randomly chosen subset or the top-k predictions. This technique is a more reliable evaluation method but requires more effort including (potentially) domain knowledge a"
W15-2612,I11-1082,0,0.0219942,"sets and compares the performance obtained. The paper concludes with section 5. quality as manually labelled data. Despite this distantly supervised relation extraction provides reasonable results compared to those based on supervised learning (see e.g. in (Thomas et al., 2011)). Distant supervision allows relation extraction systems to be created without manually labelled data. However, this raises the issue of how such a system can be evaluated. Previous approaches have carried out evaluation using existing data sets labelled with examples of the target relation (Bellare and Mccallum, 2007; Nguyen and Moschitti, 2011; Min et al., 2013) or a similar relation (Thomas et al., 2011; Roller and Stevenson, 2014). However, in the majority of scenarios the best use for any labeled data available is as training data. Others, such as Craven and Kumlien (1999), generated their own gold standard to annotate relevant relations of their knowledge base. But the effort required to generate manually labelled evaluation data somewhat negates the benefit of reduced development time provided by distant supervision. An alternative approach, which does not require any labelled data, is held-out evaluation. This approach splits"
W15-2612,P12-1076,0,0.0197726,"ubset of the predictions is selected to be examined in more detail. For example, Riedel et al. (2010) supplemented the held-out evaluation of their distant supervision approach for Freebase by selecting the top 1000 facts it predicted and evaluating them manually. Others such as Surdeanu et al. (2012) and Intxaurrondo et al. (2013) work with the same knowledge base and are able to re-use the manually labelled data generated by Riedel et al. (2010). However, this data is only available for some Freebase relations and evaluation data has to be generated for each new relation. Approaches such as Takamatsu et al. (2012), Zhang et al. (2013) and Augenstein et al. (2014) combine a held-out evaluation with a manual evaluation of a randomly chosen subset or the top-k predictions. This technique is a more reliable evaluation method but requires more effort including (potentially) domain knowledge and needs to be repeated for each version of the classifier. 2 Data Generation A large set of distantly labelled examples was generated (Section 2.1). A small portion of these were used as held-out test data. This data set was also manually annotated (Section 2.2). 2.1 Distant labelling Distantly labelled examples are ge"
W15-2612,W11-3904,0,0.0397895,"redicted labels. The remainder of this paper is structured as follows. The next section 2 describes the creation of the distantly supervised data and a manually labelled subset. A comparison of the automatically and manually generated labels is carried out in Section 3. Sections 4 evaluates a relation extraction system using different data sets and compares the performance obtained. The paper concludes with section 5. quality as manually labelled data. Despite this distantly supervised relation extraction provides reasonable results compared to those based on supervised learning (see e.g. in (Thomas et al., 2011)). Distant supervision allows relation extraction systems to be created without manually labelled data. However, this raises the issue of how such a system can be evaluated. Previous approaches have carried out evaluation using existing data sets labelled with examples of the target relation (Bellare and Mccallum, 2007; Nguyen and Moschitti, 2011; Min et al., 2013) or a similar relation (Thomas et al., 2011; Roller and Stevenson, 2014). However, in the majority of scenarios the best use for any labeled data available is as training data. Others, such as Craven and Kumlien (1999), generated the"
W15-2612,Q13-1030,0,0.0282,"Missing"
W15-2612,W14-4501,0,0.0387154,"Missing"
W15-2612,W14-1112,1,0.762284,"manually labelled data. Despite this distantly supervised relation extraction provides reasonable results compared to those based on supervised learning (see e.g. in (Thomas et al., 2011)). Distant supervision allows relation extraction systems to be created without manually labelled data. However, this raises the issue of how such a system can be evaluated. Previous approaches have carried out evaluation using existing data sets labelled with examples of the target relation (Bellare and Mccallum, 2007; Nguyen and Moschitti, 2011; Min et al., 2013) or a similar relation (Thomas et al., 2011; Roller and Stevenson, 2014). However, in the majority of scenarios the best use for any labeled data available is as training data. Others, such as Craven and Kumlien (1999), generated their own gold standard to annotate relevant relations of their knowledge base. But the effort required to generate manually labelled evaluation data somewhat negates the benefit of reduced development time provided by distant supervision. An alternative approach, which does not require any labelled data, is held-out evaluation. This approach splits facts from the knowledge base into two parts: one to generate distantly supervised trainin"
W15-2612,P15-2045,1,0.730045,"lien (1999), generated their own gold standard to annotate relevant relations of their knowledge base. But the effort required to generate manually labelled evaluation data somewhat negates the benefit of reduced development time provided by distant supervision. An alternative approach, which does not require any labelled data, is held-out evaluation. This approach splits facts from the knowledge base into two parts: one to generate distantly supervised training data and the other to generate distantly supervised evaluation data (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2010; Roller et al., 2015). This approach is often combined with a manual evaluation in which a subset of the predictions is selected to be examined in more detail. For example, Riedel et al. (2010) supplemented the held-out evaluation of their distant supervision approach for Freebase by selecting the top 1000 facts it predicted and evaluating them manually. Others such as Surdeanu et al. (2012) and Intxaurrondo et al. (2013) work with the same knowledge base and are able to re-use the manually labelled data generated by Riedel et al. (2010). However, this data is only available for some Freebase relations and evaluat"
W15-2612,P13-2117,0,0.0297889,"Missing"
W15-2612,D13-1003,0,0.02448,"Missing"
W15-2612,P13-2141,0,0.0124377,"is selected to be examined in more detail. For example, Riedel et al. (2010) supplemented the held-out evaluation of their distant supervision approach for Freebase by selecting the top 1000 facts it predicted and evaluating them manually. Others such as Surdeanu et al. (2012) and Intxaurrondo et al. (2013) work with the same knowledge base and are able to re-use the manually labelled data generated by Riedel et al. (2010). However, this data is only available for some Freebase relations and evaluation data has to be generated for each new relation. Approaches such as Takamatsu et al. (2012), Zhang et al. (2013) and Augenstein et al. (2014) combine a held-out evaluation with a manual evaluation of a randomly chosen subset or the top-k predictions. This technique is a more reliable evaluation method but requires more effort including (potentially) domain knowledge and needs to be repeated for each version of the classifier. 2 Data Generation A large set of distantly labelled examples was generated (Section 2.1). A small portion of these were used as held-out test data. This data set was also manually annotated (Section 2.2). 2.1 Distant labelling Distantly labelled examples are generated using the Uni"
W15-2612,S13-2056,0,0.0575079,"Missing"
W15-2612,W11-0902,0,0.0294557,"Missing"
W15-2612,P09-1113,0,\N,Missing
W15-3802,ellendorff-etal-2014-using,0,0.0220383,"al., 2013) focus on approaches using Freebase as knowledge source to generate automatically labelled data. In recent years distant supervision has also become more popular in the biomedical domain beeing used to detect protein-protein interactions using IntAct (Thomas et al., 2011), protein-residue associations with PDB (Ravikumar et al., 2012) or relationships of the National Drug File-Reference Terminology (NDF-RT) using the UMLS Metathesaurus (Roller and Stevenson, 2014). Liu et al. (2014) focus on the detection of genes in brain regions from literature using the UMLS Semantic Network and Ellendorff et al. (2014) uses the Comparative Toxicogenomics Database (CTD) to detect interactions between genes and chemicals. The distantly supervised methods of Nguyen and Moschitti (2011) and Pershina et al. (2014) differ slightly from many other approaches. Both combine supervised and distantly supervised models. Nguyen and Moschitti (2011) use a support vector machine and combine the supervised and the distantly supervised classifier with a linear combination. Pershina et al. (2014) instead integrate the manually labelled data directly within their distantly supervised multi-learning approach. There are a few c"
W15-3802,P10-1030,0,0.0721584,"Missing"
W15-3802,P11-1055,0,0.120638,"her than using manually annotated data, distant supervision uses knowledge about which entity pairs are instances of the target relation to generate automatically labelled data which is used to train a relational classifier. Craven and Kumlien (1999) introduced distant supervision for relation extraction. The authors used the Yeast Protein Database (YPD) as source of knowledge and mapped this information to PubMed articles to generate training examples. The technique has been widely applied particularly outside the medical domain. Many approaches such as (Mintz et al., 2009; Sun et al., 2011; Hoffmann et al., 2011; Krause et al., 2012; Xu et al., 2013) focus on approaches using Freebase as knowledge source to generate automatically labelled data. In recent years distant supervision has also become more popular in the biomedical domain beeing used to detect protein-protein interactions using IntAct (Thomas et al., 2011), protein-residue associations with PDB (Ravikumar et al., 2012) or relationships of the National Drug File-Reference Terminology (NDF-RT) using the UMLS Metathesaurus (Roller and Stevenson, 2014). Liu et al. (2014) focus on the detection of genes in brain regions from literature using th"
W15-3802,P05-1022,0,0.0810642,"Missing"
W15-3802,W11-1801,0,0.0824265,"Missing"
W15-3802,P09-1113,0,0.683273,"interest and are also difficult and time-consuming to create. Other approaches may be more appropriate in situations where training data is limited or unavailable. Minimally supervised approaches, such as seed and bootstrapping techniques (Brin, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000), are provided with a small set of seed instances (examples of related information) or patterns and acquire further examples from a large corpus by applying an iterative process. While these approaches do not require labelled training data they often suffer from low precision or semantic drift (Mintz et al., 2009). Distant supervision combines the advantages of minimally supervised and supervised approaches to relation extraction. Distant supervision makes use of an external knowledge source that provides information about pairs of entities which are related. Sentences containing both entities in a pair are identified from a corpus and used in place of labeled training examples. For example, knowledge that hair loss is a drug-related adverse effect of paroxetine would allow further positive examples to be identified by searching for other sentences containing the same drug and side-effect. Many knowled"
W15-3802,Q13-1030,0,0.0189539,"hina et al. (2014) instead integrate the manually labelled data directly within their distantly supervised multi-learning approach. There are a few case reports on [CONDITION:hair loss] associated with tricyclic antidepressants and serotonin selective reuptake inhibitors (SSRIs), but none deal specifically with [DRUG:paroxetine]. Figure 1: Generation of false positives by using automatically labelled data, PMID=10442258 The majority of distant supervision approaches use structured knowledge sources such as Wikipedia (Hoffmann et al., 2010) or Freebase (Mintz et al., 2009; Riedel et al., 2010; Ritter et al., 2013; Augenstein et al., 2014). However there may not be a suitable knowledge base available for a particular relation of interest. This paper addresses the problem of developing relation extraction systems in situations where only a small amount of training data is available. We introduce a method for relation extraction that can be used when only limited amounts of training data are available. The approach is based on distant supervision but, rather than relying on a knowledge base, seed pairs are extracted from Medline articles. Sentences from the Medline Baseline Repository containing these se"
W15-3802,W14-1112,1,0.774728,"cularly outside the medical domain. Many approaches such as (Mintz et al., 2009; Sun et al., 2011; Hoffmann et al., 2011; Krause et al., 2012; Xu et al., 2013) focus on approaches using Freebase as knowledge source to generate automatically labelled data. In recent years distant supervision has also become more popular in the biomedical domain beeing used to detect protein-protein interactions using IntAct (Thomas et al., 2011), protein-residue associations with PDB (Ravikumar et al., 2012) or relationships of the National Drug File-Reference Terminology (NDF-RT) using the UMLS Metathesaurus (Roller and Stevenson, 2014). Liu et al. (2014) focus on the detection of genes in brain regions from literature using the UMLS Semantic Network and Ellendorff et al. (2014) uses the Comparative Toxicogenomics Database (CTD) to detect interactions between genes and chemicals. The distantly supervised methods of Nguyen and Moschitti (2011) and Pershina et al. (2014) differ slightly from many other approaches. Both combine supervised and distantly supervised models. Nguyen and Moschitti (2011) use a support vector machine and combine the supervised and the distantly supervised classifier with a linear combination. Pershina"
W15-3802,W13-2001,0,0.028288,"Missing"
W15-3802,S13-2056,0,0.160468,"Missing"
W15-3802,P11-1053,0,0.0322528,"very popular. Rather than using manually annotated data, distant supervision uses knowledge about which entity pairs are instances of the target relation to generate automatically labelled data which is used to train a relational classifier. Craven and Kumlien (1999) introduced distant supervision for relation extraction. The authors used the Yeast Protein Database (YPD) as source of knowledge and mapped this information to PubMed articles to generate training examples. The technique has been widely applied particularly outside the medical domain. Many approaches such as (Mintz et al., 2009; Sun et al., 2011; Hoffmann et al., 2011; Krause et al., 2012; Xu et al., 2013) focus on approaches using Freebase as knowledge source to generate automatically labelled data. In recent years distant supervision has also become more popular in the biomedical domain beeing used to detect protein-protein interactions using IntAct (Thomas et al., 2011), protein-residue associations with PDB (Ravikumar et al., 2012) or relationships of the National Drug File-Reference Terminology (NDF-RT) using the UMLS Metathesaurus (Roller and Stevenson, 2014). Liu et al. (2014) focus on the detection of genes in brain regions f"
W15-3802,I11-1082,0,0.0209027,"popular in the biomedical domain beeing used to detect protein-protein interactions using IntAct (Thomas et al., 2011), protein-residue associations with PDB (Ravikumar et al., 2012) or relationships of the National Drug File-Reference Terminology (NDF-RT) using the UMLS Metathesaurus (Roller and Stevenson, 2014). Liu et al. (2014) focus on the detection of genes in brain regions from literature using the UMLS Semantic Network and Ellendorff et al. (2014) uses the Comparative Toxicogenomics Database (CTD) to detect interactions between genes and chemicals. The distantly supervised methods of Nguyen and Moschitti (2011) and Pershina et al. (2014) differ slightly from many other approaches. Both combine supervised and distantly supervised models. Nguyen and Moschitti (2011) use a support vector machine and combine the supervised and the distantly supervised classifier with a linear combination. Pershina et al. (2014) instead integrate the manually labelled data directly within their distantly supervised multi-learning approach. There are a few case reports on [CONDITION:hair loss] associated with tricyclic antidepressants and serotonin selective reuptake inhibitors (SSRIs), but none deal specifically with [DR"
W15-3802,W11-3904,0,0.0347028,"uthors used the Yeast Protein Database (YPD) as source of knowledge and mapped this information to PubMed articles to generate training examples. The technique has been widely applied particularly outside the medical domain. Many approaches such as (Mintz et al., 2009; Sun et al., 2011; Hoffmann et al., 2011; Krause et al., 2012; Xu et al., 2013) focus on approaches using Freebase as knowledge source to generate automatically labelled data. In recent years distant supervision has also become more popular in the biomedical domain beeing used to detect protein-protein interactions using IntAct (Thomas et al., 2011), protein-residue associations with PDB (Ravikumar et al., 2012) or relationships of the National Drug File-Reference Terminology (NDF-RT) using the UMLS Metathesaurus (Roller and Stevenson, 2014). Liu et al. (2014) focus on the detection of genes in brain regions from literature using the UMLS Semantic Network and Ellendorff et al. (2014) uses the Comparative Toxicogenomics Database (CTD) to detect interactions between genes and chemicals. The distantly supervised methods of Nguyen and Moschitti (2011) and Pershina et al. (2014) differ slightly from many other approaches. Both combine supervi"
W15-3802,P14-2119,0,0.0160293,"n beeing used to detect protein-protein interactions using IntAct (Thomas et al., 2011), protein-residue associations with PDB (Ravikumar et al., 2012) or relationships of the National Drug File-Reference Terminology (NDF-RT) using the UMLS Metathesaurus (Roller and Stevenson, 2014). Liu et al. (2014) focus on the detection of genes in brain regions from literature using the UMLS Semantic Network and Ellendorff et al. (2014) uses the Comparative Toxicogenomics Database (CTD) to detect interactions between genes and chemicals. The distantly supervised methods of Nguyen and Moschitti (2011) and Pershina et al. (2014) differ slightly from many other approaches. Both combine supervised and distantly supervised models. Nguyen and Moschitti (2011) use a support vector machine and combine the supervised and the distantly supervised classifier with a linear combination. Pershina et al. (2014) instead integrate the manually labelled data directly within their distantly supervised multi-learning approach. There are a few case reports on [CONDITION:hair loss] associated with tricyclic antidepressants and serotonin selective reuptake inhibitors (SSRIs), but none deal specifically with [DRUG:paroxetine]. Figure 1: G"
W15-3802,P07-1074,0,0.016551,"be applied when only limited training data is available. The approach uses a form of distant supervision but does not require an external knowledge base. Instead, it uses information from the training set to acquire new labelled data and combines it with manually labelled data. The approach was tested on an adverse drug data set using a limited amount of manually labelled training data and shown to outperform a supervised approach. 1 Introduction Relation extraction is a widely explored problem that has been applied to a range of domains (Craven and Kumlien, 1999; Agichtein and Gravano, 2000; Xu et al., 2007) using a variety of techniques (Yangarber, 2003; Bunescu and Mooney, 2006; Neumann and Schmeier, 2012). In the biomedical domain relation extraction has been used to identify a wide range of types of relation, including adverse drug effects (ADE), gene regulations and drug-drug interactions. Community evaluation exercises, such as the BioNLP Shared Task (Kim et al., 2011; N´edellec et al., 2013) or the Drug-Drug Interaction (DDI) challenge (Segura-Bedmar et al., 2013), have shown that supervised learning techniques normally produce better results than other approaches. 12 Proceedings of the 20"
W15-3802,P13-2117,0,0.278222,"Missing"
W15-3802,P03-1044,0,0.0432909,"ailable. The approach uses a form of distant supervision but does not require an external knowledge base. Instead, it uses information from the training set to acquire new labelled data and combines it with manually labelled data. The approach was tested on an adverse drug data set using a limited amount of manually labelled training data and shown to outperform a supervised approach. 1 Introduction Relation extraction is a widely explored problem that has been applied to a range of domains (Craven and Kumlien, 1999; Agichtein and Gravano, 2000; Xu et al., 2007) using a variety of techniques (Yangarber, 2003; Bunescu and Mooney, 2006; Neumann and Schmeier, 2012). In the biomedical domain relation extraction has been used to identify a wide range of types of relation, including adverse drug effects (ADE), gene regulations and drug-drug interactions. Community evaluation exercises, such as the BioNLP Shared Task (Kim et al., 2011; N´edellec et al., 2013) or the Drug-Drug Interaction (DDI) challenge (Segura-Bedmar et al., 2013), have shown that supervised learning techniques normally produce better results than other approaches. 12 Proceedings of the 2015 Workshop on Biomedical Natural Language Proc"
W15-3802,E06-1051,0,\N,Missing
W15-3817,W08-1301,0,0.142825,"Missing"
W15-3817,J02-4002,0,0.67913,"nts a system that aids those searching for studies that discuss a particular research question. The system acts as a mediator between the search engine and the user. It interprets the search engine results and returns the most informative sentence(s) from the claim zone of each abstract that are potential answers to the research question. The system reduces the cognitive loads on the user by assisting their identification of relevant claims within abstracts The system comprises two components. The first component identifies the claim zone in each abstract using the rhetorical moves principle (Teufel and Moens, 2002), and the second component uses the sentences in the claim zone to predict the most informative sentence(s) from each abstract to the given query. This paper makes three contributions: presenting a new set of features to build a classifier to identify the structure role of sentences in an abstract that is at least shows similar performance to the current systems; building a classifier to detect the best sentence(s) (lexically) that can be an answer to a given query; and introducing a new feature (Z-score) for this task. Given a set of abstracts retrieved from a search engine such as Pubmed, we"
W15-3817,P02-1053,0,0.00488538,"larity score with the query is considered an informative sentence with respect to the query. The component classifier is built using a decision tree algorithm (Quinlan, 1993). The decision tree algorithm builds a tree-like model that can be converted into rules which can be easily interpreted and analysed by human. We use the open source implementation of decision tree (J48) in Weka (Hall et al., 2009) to build the model. Features The claim zoning component employs various features: N-grams: N-grams are lexical features that have been reported as useful to capture the general context of text (Turney, 2002; Yu and Hatzivassiloglou, 2003). For every sentence, uni-grams and bi-grams are extracted from the abstract’s title, the current sentence Sn , the previous sentence Sn−1 , and the next sentence Sn+1 . Sentence-Title similarity (st-sim): This feature is the cosine similarity score sim(s, T ) between each sentence in an abstract and its title. This feature has been previously found useful for summarization tasks (Teufel and Moens, 2002). Achieving an accurate similarity score between the sentences and the title in an abstract is not a straightforward task. Many abstracts in the medical domain u"
W15-3817,I08-1050,0,0.120408,"re141 Proceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP 2015), pages 141–146, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics National Library of Medicine (NLM) have reported that 2,779 headings have been used to label abstracts sections in Medline (Ripple et al., 2012). garded such information as key information to determine the research topic. Our research is similar to that work since we use the conclusion section to identify the key information in an abstract with respect to a query, but we also include the result sections. Hirohata et al. (2008) showed a similar system using CRFs to classify the abstract sentences into four categories: objective, methods, results, and conclusions. That classifier takes into account the neighbouring features in sentence Sn such as the n-grams of the previous sentence Sn−1 and the next sentence Sn+1 . Agarwal et al. (2009) described a system that automatically classifies sentences appear in full biomedical articles into one of four rhetorical categories: introduction, methods, results and discussions. The best system was achieved using Multinominal Naive Bayes. They reported that their system outperfor"
W15-3817,W03-1017,0,0.0580436,"ith the query is considered an informative sentence with respect to the query. The component classifier is built using a decision tree algorithm (Quinlan, 1993). The decision tree algorithm builds a tree-like model that can be converted into rules which can be easily interpreted and analysed by human. We use the open source implementation of decision tree (J48) in Weka (Hall et al., 2009) to build the model. Features The claim zoning component employs various features: N-grams: N-grams are lexical features that have been reported as useful to capture the general context of text (Turney, 2002; Yu and Hatzivassiloglou, 2003). For every sentence, uni-grams and bi-grams are extracted from the abstract’s title, the current sentence Sn , the previous sentence Sn−1 , and the next sentence Sn+1 . Sentence-Title similarity (st-sim): This feature is the cosine similarity score sim(s, T ) between each sentence in an abstract and its title. This feature has been previously found useful for summarization tasks (Teufel and Moens, 2002). Achieving an accurate similarity score between the sentences and the title in an abstract is not a straightforward task. Many abstracts in the medical domain use multiple forms (i.e abbreviat"
W16-5606,P05-1054,0,0.0487202,"C-P from the eight possible classifications in each data set. A classification pipeline was created, taking each user’s corpus of tweets as input, tokenized using a 5 http://openstreetmap.org/ Twitter aware tokenizer (Gimpel et al., 2011). TFIDF transformed word n-grams (1- and 2-grams) were used as features for a multi-class Support Vector Machine (SVM) with a linear kernel. n-grams and SVMs were chosen as they have been shown to consistently perform well at user profiling tasks, both for social media (Rao and Yarowsky, 2010; Rout et al., 2013; Schwartz et al., 2013) and other types of text (Boulis and Ostendorf, 2005; Garera and Yarowsky, 2009), and are as such a useful tool to establish baseline performance. Balanced sets were extracted from the OAC-P and LAC-P datasets with 2000 members per label in both cases. 10-fold crossvalidation was used for all experiments. 5.1 Results The results of the SVM classifier are presented in Table 1, compared with results from a random baseline. Prediction of both OAC and LAC outperform the random baseline, indicating that the training dataset described in this article can be used to create valuable user profiling systems. Results for LAC are encouraging and indicate t"
W16-5606,D11-1120,0,0.0343199,"nalysis indicates that the nature of the demographic information is an important factor in performance of this approach. 1 Introduction Previous research has shown that the language a person uses on-line can be indicative of a wide range of personal characteristics, including gender, age (Schler et al., 2006), personality (Schwartz et al., 2013), political ideology (Sylwester and Purver, 2015) and occupational class (Preot¸iuc-Pietro et al., 2015a). Several user profiling models that predict these characteristics have been developed, some of which have accuracy that exceeds human performance (Burger et al., 2011; Youyou et al., 2015). User profiling models have applications such as gendered behaviour analysis (Purohit et al., 2015) and bias reduction in predictive models (Culotta, 2014). Previous work on user profiling has traditionally relied on profiles annotated with self-reported characteristics for training data. This can be difficult to acquire in large quantities and forms a bottleneck in the development of user profiling systems. Recently, approaches have attempted to build user profiling datasets through other means. Preot¸iucPietro et al. (2015a)(2015b) extracted known job titles from Twitt"
W16-5606,P09-1080,0,0.0213685,"classifications in each data set. A classification pipeline was created, taking each user’s corpus of tweets as input, tokenized using a 5 http://openstreetmap.org/ Twitter aware tokenizer (Gimpel et al., 2011). TFIDF transformed word n-grams (1- and 2-grams) were used as features for a multi-class Support Vector Machine (SVM) with a linear kernel. n-grams and SVMs were chosen as they have been shown to consistently perform well at user profiling tasks, both for social media (Rao and Yarowsky, 2010; Rout et al., 2013; Schwartz et al., 2013) and other types of text (Boulis and Ostendorf, 2005; Garera and Yarowsky, 2009), and are as such a useful tool to establish baseline performance. Balanced sets were extracted from the OAC-P and LAC-P datasets with 2000 members per label in both cases. 10-fold crossvalidation was used for all experiments. 5.1 Results The results of the SVM classifier are presented in Table 1, compared with results from a random baseline. Prediction of both OAC and LAC outperform the random baseline, indicating that the training dataset described in this article can be used to create valuable user profiling systems. Results for LAC are encouraging and indicate that it is possible to achiev"
W16-5606,P11-2008,0,0.109487,"Missing"
W16-5606,P15-1169,0,0.1581,"Missing"
W19-0404,W13-0102,1,0.86446,"Missing"
W19-0404,N13-1016,1,0.867714,"Missing"
W19-0404,P14-2103,1,0.792236,"ble words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible. Table 1 shows topics represented by the 30 most probable words. Words displayed in b"
W19-0404,N16-1057,0,0.0184346,"raditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible."
W19-0404,P11-1154,0,0.0285062,"pic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible. Table 1 shows topics represented by the 30 most probabl"
W19-0404,E14-1056,0,0.0236024,"ial distributions over a predefined vocabulary of words. Traditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank t"
W19-0404,D11-1024,0,0.0604857,"atent variables called topics. Topics are multinomial distributions over a predefined vocabulary of words. Traditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus."
W19-0404,N10-1012,0,0.198147,"nts as a mixture of latent variables called topics. Topics are multinomial distributions over a predefined vocabulary of words. Traditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in t"
W19-0404,P02-1040,0,0.10711,"ons are statistically significant for all three datasets (Pearson’s r varies between 0.81 and 0.90, p < 0.05). This suggests that the automated evaluation approach presented in Section 5 is a useful tool for assessing the effectiveness of methods for word re-ranking with the advantage that results can be obtained more rapidly than methods that require human judgments. However, human judgments are recommended when performance is similar and automated evaluation should not be relied upon to make fine-grained distinctions between approaches, as is common for some tasks (e.g. Machine Translation (Papineni et al., 2002)). 7 Conclusion We presented a study on word re-ranking methods designed to improve topic interpretability. Four methods were presented and assessed through two experiments. In the first experiment, participants on a crowdsourcing platform were asked to associate documents with related topics. In the second experiment, automated evaluation was based on a document retrieval task. Re-ranking the topic words was found to improve the interpretability of topics and therefore should be used as a post-processing step to improve topic representation. The most effective re-ranking schemes were those wh"
W19-0404,E17-2069,0,0.0136829,"rious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible. Table 1 shows topics represented by the 30 most probable words. Words displayed in bold font are more general (less informative, e.g. with high document frequency) while the remaining words a"
W19-0404,W14-3110,0,0.180598,"pirical evaluation of the effectiveness of their approach. Other word re-ranking methods have also combined information about the overall probability of a word and its relative probability in one topic compared to others. Chuang et al. (2012) describe a word reranking method applied within a topic model visualisation system. Their approach combines information about the word’s overall probability within the corpus and its distinctiveness for a particular topic which is computed as the Kullback-Leibler divergence between the distribution of topics given the word and the distribution of topics. Sievert and Shirley (2014) also combine both types of information within a topic visualisation system. Bischof and Airoldi (2012) developed an approach for hierarchical topic models which balances information about the word frequency in a topic and the exclusivity of that word to that topic relative to a set of similar topics within the hierarchy. Others have proposed approaches that only take into account the relative probability of each word in a topic compared to the others. Song et al. (2009) introduced a word ranking method based on normalising the probability of a word in a topic with the sum of the probabilities"
W19-0404,Q17-1001,0,0.0214953,"combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements. 1 Introduction Probabilistic topic modelling (Blei, 2012) is a widely used approach in Natural Language Processing (Boyd-Graber et al., 2017) with applications to areas such as enhancing exploratory search interfaces (Chaney and Blei, 2012; Aletras et al., 2014; Smith et al., 2017; Aletras et al., 2017) and developing interpretable machine learning models (Paul, 2016). A topic model, e.g. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) learns a low-dimensional representation of documents as a mixture of latent variables called topics. Topics are multinomial distributions over a predefined vocabulary of words. Traditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words"
W19-0404,E17-2111,1,0.783238,"et them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible. Table 1 shows topics represented by the 30 most probable words. Words displayed in bold font are more general (less informative, e.g. with h"
W97-0208,H92-1022,0,0.184011,"e chosen to use the machine readable version of LDOCE as our lexicon. This has been 49 used extensively in NLP research and provides a broad set of senses for sense tagging. The text is initially stemmed, leaving only morphological roots, and split into sentences. Then words belonging to a list of stop words (prepositions, pronouns etc.) are removed. For each of the remaining words, e ~ of its senses are extracted from LDOCE and stored with that word. The textual definitions in each sense is processed to remove stop words and stem remaining words. 2. The text is tagged using the Brill tagger (Brill, 1992) and a translation is carried out using a manually defined mapping from the syntactic tags assigned by Briil (Penn Tree Bank tags (Marcus, Santorini, and Marcinkiewicz, 1993)) onto the simpler part-of-speech categories associated with LDOCE senses. We then remove all senses whose part-of-speech is not consistent with the one assigned by the tagger, if none of the senses are consistent with the part-of-speech we assume the tagger has made an error and do not remove any senses. 3. The final stage is to use the simulated annealing algorithm to optimise the dictionary deftnition overlap for the re"
W97-0208,C92-4189,0,0.312739,"ord sense disambiguation (WSD) algorithms. By WSD algorithm we mean any procedure which carries out semantic disambignation on words, these may not necessarily be tagging algorithms, in that they do 2 Recent Word Sense Disambiguation algorithms Recent word sense disambignation (WSD) algorithms can be categorised into two broad types: 1. WSD using information in an explicit lexicon. This is usually a Machine Readable Dictionary (MRD) such as the Longman Dictionary o] Contemporary English (LDOCE) (Procter, 1978), WordNet (Miller (Ed.), 1990) or handcrafted. Recent examples of this work include (Bruce and Guthrie, 1992), (Bruce and Wiebe, 1994), (McRoy, 1992). 2. WSD using information gained from training on some corpus. This approach can be further subIOften loosened to each content word in a text. 47 m m classified: (a) Supervised training, where information is gathered from corpora which have already been semantically disambiguated. As such corpora are hard to obtain, usually requiring expensive hand-tagging, researchin this area has concentrated on other forms of lexical ambiguities, eg. (Gale, Church, and Yarowsky, 1992). (b) Unsupervised training, where information is gathered from raw corpora which ha"
W97-0208,P94-1020,0,0.0475901,"SD) algorithms. By WSD algorithm we mean any procedure which carries out semantic disambignation on words, these may not necessarily be tagging algorithms, in that they do 2 Recent Word Sense Disambiguation algorithms Recent word sense disambignation (WSD) algorithms can be categorised into two broad types: 1. WSD using information in an explicit lexicon. This is usually a Machine Readable Dictionary (MRD) such as the Longman Dictionary o] Contemporary English (LDOCE) (Procter, 1978), WordNet (Miller (Ed.), 1990) or handcrafted. Recent examples of this work include (Bruce and Guthrie, 1992), (Bruce and Wiebe, 1994), (McRoy, 1992). 2. WSD using information gained from training on some corpus. This approach can be further subIOften loosened to each content word in a text. 47 m m classified: (a) Supervised training, where information is gathered from corpora which have already been semantically disambiguated. As such corpora are hard to obtain, usually requiring expensive hand-tagging, researchin this area has concentrated on other forms of lexical ambiguities, eg. (Gale, Church, and Yarowsky, 1992). (b) Unsupervised training, where information is gathered from raw corpora which has not been semantically d"
W97-0208,H92-1046,0,0.146275,"Missing"
W97-0208,C96-1055,0,0.0132282,"can be used to disambiguate (usually nominal) senses, as was shown by (Bruce and Guthrie, 1992) and (Yarowsky, 1992). Our intuition here is that disambiguation evidence can be gained by choosing senses which are closest in a thesanral hierarchy. Closeness in such a hierarchy can be effectively expressed as the number of nodes between concepts. We are implementing a simple algorithm which prefers close senses in our domain hierarchy which was derived from LDOCE (Bruce and Guthrie, 1992). 5.3 Collocates Recent work has been done using collocations as semantic disambiguators, (Yarowsky, 1993), (Dorr, 1996), particularly for verbs. We are attempting to derive disambiguation information by examining the prepositions as given in the subcategorization frames of verbs, and in the example sentences in LDOCE. 5.4 6 A Basic Tagger We have recently implemented a basic version of this tagger, initially incorporating only the part-ofspeech (5.1) and dictionary definition (5.5) stages in the process, with further stages to be added later. Our tagger currently consists of three modules: Selectionai Preferences • Dictionary look-up module There has been a long tradition in NLP of using selectional preference"
W97-0208,H92-1045,0,0.212931,"r, 1978), WordNet (Miller (Ed.), 1990) or handcrafted. Recent examples of this work include (Bruce and Guthrie, 1992), (Bruce and Wiebe, 1994), (McRoy, 1992). 2. WSD using information gained from training on some corpus. This approach can be further subIOften loosened to each content word in a text. 47 m m classified: (a) Supervised training, where information is gathered from corpora which have already been semantically disambiguated. As such corpora are hard to obtain, usually requiring expensive hand-tagging, researchin this area has concentrated on other forms of lexical ambiguities, eg. (Gale, Church, and Yarowsky, 1992). (b) Unsupervised training, where information is gathered from raw corpora which has not been semantically disambiguated. The best examples of this approach has been the resent work of Yarowsky - (Yarowsky, 1992), (Yarowsky, 1993), (Yarowsky, 1995). These approaches are not mutually exclusive and there are, of course, some hybrid cases, for example Luk (Luk, 1995) uses information in MRD definitions (approach 1) and statistical information from untagged corpora (approach 2b). 3 Comparing Different Approaches Approach 2a is the least promising since text tagged with word senses is practically"
W97-0208,P95-1025,0,0.0128418,"ich have already been semantically disambiguated. As such corpora are hard to obtain, usually requiring expensive hand-tagging, researchin this area has concentrated on other forms of lexical ambiguities, eg. (Gale, Church, and Yarowsky, 1992). (b) Unsupervised training, where information is gathered from raw corpora which has not been semantically disambiguated. The best examples of this approach has been the resent work of Yarowsky - (Yarowsky, 1992), (Yarowsky, 1993), (Yarowsky, 1995). These approaches are not mutually exclusive and there are, of course, some hybrid cases, for example Luk (Luk, 1995) uses information in MRD definitions (approach 1) and statistical information from untagged corpora (approach 2b). 3 Comparing Different Approaches Approach 2a is the least promising since text tagged with word senses is practically non-existent and is both time consuming and difficnlt to produce manually. Much of the research in this area has been compromised by the fact that researchers have focussed on lexical ambiguities that are not true word sense distinctions, such as words translated differently across two languages (Gale, Church, and Yarowsky, 1992) or homophones ~ (Yarowsky, 1993). E"
W97-0208,J93-2004,0,0.0269029,"t of senses for sense tagging. The text is initially stemmed, leaving only morphological roots, and split into sentences. Then words belonging to a list of stop words (prepositions, pronouns etc.) are removed. For each of the remaining words, e ~ of its senses are extracted from LDOCE and stored with that word. The textual definitions in each sense is processed to remove stop words and stem remaining words. 2. The text is tagged using the Brill tagger (Brill, 1992) and a translation is carried out using a manually defined mapping from the syntactic tags assigned by Briil (Penn Tree Bank tags (Marcus, Santorini, and Marcinkiewicz, 1993)) onto the simpler part-of-speech categories associated with LDOCE senses. We then remove all senses whose part-of-speech is not consistent with the one assigned by the tagger, if none of the senses are consistent with the part-of-speech we assume the tagger has made an error and do not remove any senses. 3. The final stage is to use the simulated annealing algorithm to optimise the dictionary deftnition overlap for the remaining senses. This algorithm assigns a single sense to each token which is the tag assodated with that token. 7 Example Output Below is an example of the senses assigned b"
W97-0208,J92-1001,0,0.754777,"gorithm we mean any procedure which carries out semantic disambignation on words, these may not necessarily be tagging algorithms, in that they do 2 Recent Word Sense Disambiguation algorithms Recent word sense disambignation (WSD) algorithms can be categorised into two broad types: 1. WSD using information in an explicit lexicon. This is usually a Machine Readable Dictionary (MRD) such as the Longman Dictionary o] Contemporary English (LDOCE) (Procter, 1978), WordNet (Miller (Ed.), 1990) or handcrafted. Recent examples of this work include (Bruce and Guthrie, 1992), (Bruce and Wiebe, 1994), (McRoy, 1992). 2. WSD using information gained from training on some corpus. This approach can be further subIOften loosened to each content word in a text. 47 m m classified: (a) Supervised training, where information is gathered from corpora which have already been semantically disambiguated. As such corpora are hard to obtain, usually requiring expensive hand-tagging, researchin this area has concentrated on other forms of lexical ambiguities, eg. (Gale, Church, and Yarowsky, 1992). (b) Unsupervised training, where information is gathered from raw corpora which has not been semantically disambiguated. T"
W97-0208,C92-2070,0,0.0639846,"(Miller (Ed.), 1990) or handcrafted. Recent examples of this work include (Bruce and Guthrie, 1992), (Bruce and Wiebe, 1994), (McRoy, 1992). 2. WSD using information gained from training on some corpus. This approach can be further subIOften loosened to each content word in a text. 47 m m classified: (a) Supervised training, where information is gathered from corpora which have already been semantically disambiguated. As such corpora are hard to obtain, usually requiring expensive hand-tagging, researchin this area has concentrated on other forms of lexical ambiguities, eg. (Gale, Church, and Yarowsky, 1992). (b) Unsupervised training, where information is gathered from raw corpora which has not been semantically disambiguated. The best examples of this approach has been the resent work of Yarowsky - (Yarowsky, 1992), (Yarowsky, 1993), (Yarowsky, 1995). These approaches are not mutually exclusive and there are, of course, some hybrid cases, for example Luk (Luk, 1995) uses information in MRD definitions (approach 1) and statistical information from untagged corpora (approach 2b). 3 Comparing Different Approaches Approach 2a is the least promising since text tagged with word senses is practically"
W97-0208,H93-1052,0,0.688258,"subIOften loosened to each content word in a text. 47 m m classified: (a) Supervised training, where information is gathered from corpora which have already been semantically disambiguated. As such corpora are hard to obtain, usually requiring expensive hand-tagging, researchin this area has concentrated on other forms of lexical ambiguities, eg. (Gale, Church, and Yarowsky, 1992). (b) Unsupervised training, where information is gathered from raw corpora which has not been semantically disambiguated. The best examples of this approach has been the resent work of Yarowsky - (Yarowsky, 1992), (Yarowsky, 1993), (Yarowsky, 1995). These approaches are not mutually exclusive and there are, of course, some hybrid cases, for example Luk (Luk, 1995) uses information in MRD definitions (approach 1) and statistical information from untagged corpora (approach 2b). 3 Comparing Different Approaches Approach 2a is the least promising since text tagged with word senses is practically non-existent and is both time consuming and difficnlt to produce manually. Much of the research in this area has been compromised by the fact that researchers have focussed on lexical ambiguities that are not true word sense distin"
W97-0208,P95-1026,0,0.166945,"d to each content word in a text. 47 m m classified: (a) Supervised training, where information is gathered from corpora which have already been semantically disambiguated. As such corpora are hard to obtain, usually requiring expensive hand-tagging, researchin this area has concentrated on other forms of lexical ambiguities, eg. (Gale, Church, and Yarowsky, 1992). (b) Unsupervised training, where information is gathered from raw corpora which has not been semantically disambiguated. The best examples of this approach has been the resent work of Yarowsky - (Yarowsky, 1992), (Yarowsky, 1993), (Yarowsky, 1995). These approaches are not mutually exclusive and there are, of course, some hybrid cases, for example Luk (Luk, 1995) uses information in MRD definitions (approach 1) and statistical information from untagged corpora (approach 2b). 3 Comparing Different Approaches Approach 2a is the least promising since text tagged with word senses is practically non-existent and is both time consuming and difficnlt to produce manually. Much of the research in this area has been compromised by the fact that researchers have focussed on lexical ambiguities that are not true word sense distinctions, such as wo"
W98-1208,W97-0211,0,0.040889,"Missing"
W98-1208,H92-1022,0,0.0159555,"en words belonging to a list of stop words s are removed. The words which have not been identified as part of a named entity or removed because it is a stop word are considered by the system to be ambiguous words and those are the words which are disambignated. For each of the ambiguous words, its set of possible senses is extracted from LDOCE and stored. Each sense in LDOCE contains a short textual definition (such as those shown in figure 5) which, when extracted from the dictionary, is processed to remove stop words and stem the remaining words. . The text is tagged using the Brill tagger (Brill, 1992) and a translation is carried out using a manually defined mapping from the syntactic tags assigned by Brill (Penn Tree Bank tags (Marcus, Santorini, and Marcinkiewicz, 1993)) onto the simpler part-of-speech categories associated with LDOCE senses 6. We then remove from consideration any of the senses whose partof-speech is not consistent with the one assigned by the tagger, if none of the senses are consistent with the part-of-speech we assume the tagger has made an error and leave the set of senses for that word unaltered. Preprocessing • Named-entity identification • Dictionary look-up Disa"
W98-1208,C92-4189,0,0.0422652,"Missing"
W98-1208,1997.tmi-1.18,0,0.0520242,"Missing"
W98-1208,J93-2004,0,0.0296192,"it is a stop word are considered by the system to be ambiguous words and those are the words which are disambignated. For each of the ambiguous words, its set of possible senses is extracted from LDOCE and stored. Each sense in LDOCE contains a short textual definition (such as those shown in figure 5) which, when extracted from the dictionary, is processed to remove stop words and stem the remaining words. . The text is tagged using the Brill tagger (Brill, 1992) and a translation is carried out using a manually defined mapping from the syntactic tags assigned by Brill (Penn Tree Bank tags (Marcus, Santorini, and Marcinkiewicz, 1993)) onto the simpler part-of-speech categories associated with LDOCE senses 6. We then remove from consideration any of the senses whose partof-speech is not consistent with the one assigned by the tagger, if none of the senses are consistent with the part-of-speech we assume the tagger has made an error and leave the set of senses for that word unaltered. Preprocessing • Named-entity identification • Dictionary look-up Disarnbiguation • Part-of-speech filtering • Dictionary definition overlap 4. Our next module is based on a proposal by Lesk (Lesk, 1986) that words in a sentence could be disam"
W98-1208,J92-1001,0,0.0322514,"case means 4LDOCE senses have additional information such as subject categories, subcategorisation information and selectional restrictions which we do not show here. Implementing a Sense Tagger m m m m m m m m m II senses in texts. A natural extension to this observation is to create a disambiguation system which makes use of several of these independent knowledge sources and combines their results in an intelligent way. Our system is based on a set of partial taggers, each of which uses a different knowledge source, with their results being combined. Our system is in the tradition of McRoy (McRoy, 1992), who also made use of several knowledge sources for word sense disambiguation, although the information sources she used were not independent, making it difficult to evaluate the contribution of each component. Our system makes use of strictly independent knowledge sources and is implemented within GATE whose plug-and-play architecture makes the evaluation of individual components more straightforward. At the moment the sense tagger consists of six stages (shown in figure 6), the first two preprocess the text which is to be disambiguated while the remaining four carry out the disambiguation."
W98-1208,P96-1006,0,0.0345721,"Missing"
W98-1208,W97-0322,0,0.0781595,"Missing"
W98-1208,C96-1071,1,0.873626,"Missing"
W98-1208,W97-0208,1,0.780338,"Missing"
W98-1208,H93-1052,0,0.0544604,"Missing"
W98-1208,P95-1026,0,0.0366988,"Missing"
W98-1208,A97-1036,0,0.0262743,"for the task graphs, and the diffculty of managing these graphs as the number of modules in the system grows. The graphs currently make two main contributions to the system: they give a graphical representation of control flow, and allow the user to manipulate execution of modules; they give a graphical entry point to results visualisation. These benefits will have to be balanced against their disadvantages in future versions of the Cunningham, Stevenson and Wilks II 69 system. Another problem may arise when the architecture includes facilities for distributed processing (Zajac et al., 1997; Zajac, 1997), as it is not obvious how the linear model currently embodied in the graphs could be extended to support non-linear control strucures. 8 Conclusion The previous section indicates that GATE version 1 goes a long way to meeting it's design goals (noted in section 2). The reuse of components we have experienced in the sense tagging project and a number of other local and collaborative projects is in itself justification of the development effort spent on the system, and, hopefully, these savings will be multiplied accross other users of the system. Future versions will address the problems we un"
W98-1208,A97-1035,1,\N,Missing
W98-1208,P94-1020,0,\N,Missing
W98-1208,H92-1045,0,\N,Missing
W98-1208,C96-1079,0,\N,Missing
W98-1208,A97-2017,1,\N,Missing
