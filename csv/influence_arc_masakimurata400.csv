2002.tmi-papers.14,W99-0606,0,0.0689983,"Missing"
2002.tmi-papers.14,A00-2020,0,0.0829102,"achine translation. We describe the modality corpus in Section 2, the method of corpus correction in Section 3, and our experiments on corpus correction in Section 4. 2 Modality Corpus for Machine Translation In this section, we describe the modality corpus. A part of the modality corpus is shown in Figure 1. It is composed of a Japanese-English bilingual corpus; each English sentence can include two types of tags: 1 There is no previous paper on error correction in corpora. In terms of error detection in corpora, there has been research using boosting or anomaly detection (Abney et al. 1999; Eskin 2000). , kono kodomo wa aa ieba kou iu kara koniku-rashii This child always talks back to me, and this &lt;v>is&lt;/v> why I &lt;vj>hate&lt;/vj> him. d kare ga aa okubyou da to wa omowanakatta I &lt;v>did not think&lt;/v> he was so timid. c aa isogashikute wa yasumu hima mo nai hazu da Such a busy man as he &lt;v>cannot have&lt;/v> any spare time. Figure 1: Part of the modality corpus • The English main verb phrase is tagged with &lt;v>. • The English verb phrase corresponding to the Japanese main verb phrase is tagged with &lt;vj>. The symbols at the beginning of each Japanese sentence, such as “c” and “d”, indicate a category"
2002.tmi-papers.14,1999.tmi-1.7,1,0.874862,"Missing"
2002.tmi-papers.14,P94-1013,0,0.00968044,"f each category/feature pair as calculated from p˜(a, b) are the same as those from p(a, b) (this corresponds to Equation (1).) These estimated values are not so sparse. We can thus use the above assumption for calculating p(a, b). Furthermore, we maximize the entropy of the distribution of p˜(a, b) to obtain one solution of p˜(a, b), because using only Equation 1 produces several solutions for p˜(a, b). Maximizing the entropy has the effect of making the distribution more uniform and is considered to be a good solution for data sparseness problems. • Method based on the decision-list method (Yarowsky 1994) In this method, the probability of each category is calculated using one of the features, f j (∈ F,  ≤ j ≤ k). The probability that produces category a in context b is given by the following equation: p(a|b) = p(a|f max ), (3) such that f max is defined by f max = argmaxf j ∈F maxai ∈A p˜(ai |f j ), (4) where p˜(ai |f j ) is the occurrence rate of category a i when the context has feature f j. In this paper, we used the following items as features, which are the context when the probabilities are calculated; 26 (= 5 + 10 + 10 + 1) features appear in each English sentence: • the strings of 1-"
2020.lrec-1.87,W17-5516,0,0.0261156,"tives for the purpose of showing an attitude of attentive listening, is simply called attentive listening response. Attentive listening responses show empathy to narrative speech and enhance speaker’s motivation to speak. A representative attentive listening response is a back-channel, whose generation methods have already been proposed (Kamiya et al., 2010; Yamaguchi et al., 2016). Besides the back-channel, there are various types of attentive listening responses such as admiration response or evaluation response, and there are some works on generating such responses (Kobayashi et al., 2010; Lala et al., 2017; Meguro et al., 2011; Shitaoka et al., 2017). The degree of empathy shown by attentive listening responses are thought to depend on their type. Empathy to narratives encourages a speaker to speak more only when the degree of the empathy is appropriate. On the other hand, when the degree of the empathy is not appropriate for the narrative, such empathy discourages the speaker. In order to enhance the speaker’s motivation to speak, it is necessary to utter an attentive listening response which can show the appropriate degree of empathy. Nevertheless, the relation between types of attentive list"
C00-1074,W96-0102,0,0.0512099,"Missing"
C00-1074,C94-1027,0,0.0855273,"Missing"
C00-1074,J94-2001,0,\N,Missing
C00-1074,J95-4004,0,\N,Missing
C00-1082,J96-1002,0,0.00692912,"of information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combinati"
C00-1082,W98-1118,0,0.0131898,"tic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combination of features. Unlike the decision-tree method, as a result w"
C00-1082,W95-0107,0,0.0509382,"hashi, for example, made 146 rules for bunsetsu identi cation (Kurohashi, 1998). In an attempt to reduce the number of manhours, we used machine-learning methods for bunsetsu identi cation. Because it was not clear which machine-learning method would be the one most appropriate for bunsetsu identi cation, so we tried a variety of them. In this paper we report experiments comparing four machine-learning methods (decision tree, maximum entropy, example-based, and decision list methods) and our new methods using category-exclusive rules. 1 Bunsetsu identi cation is a problem similar to chunking (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) in other languages. 2 Bunsetsu identi cation problem We conducted experiments on the following supervised learning methods for identifying bunsetsu:  Decision tree method  Maximum entropy method  Example-based method (use of similarity)  Decision list (use of probability and frequency)  Method 1 (use of exclusive rules)  Method 2 (use of exclusive rules with the highest similarity). In general, bunsetsu identi cation is done after morphological and before syntactic analysis. Morphological analysis corresponds to part-of-speech tagging in English. Japanese synta"
C00-1082,W96-0213,0,0.0136307,"major POS, (ii) minor POS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combination of features. Unl"
C00-1082,W97-0301,0,0.0125915,"or POS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combination of features. Unlike the decision-tr"
C00-1082,E99-1023,0,0.0134246,"46 rules for bunsetsu identi cation (Kurohashi, 1998). In an attempt to reduce the number of manhours, we used machine-learning methods for bunsetsu identi cation. Because it was not clear which machine-learning method would be the one most appropriate for bunsetsu identi cation, so we tried a variety of them. In this paper we report experiments comparing four machine-learning methods (decision tree, maximum entropy, example-based, and decision list methods) and our new methods using category-exclusive rules. 1 Bunsetsu identi cation is a problem similar to chunking (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) in other languages. 2 Bunsetsu identi cation problem We conducted experiments on the following supervised learning methods for identifying bunsetsu:  Decision tree method  Maximum entropy method  Example-based method (use of similarity)  Decision list (use of probability and frequency)  Method 1 (use of exclusive rules)  Method 2 (use of exclusive rules with the highest similarity). In general, bunsetsu identi cation is done after morphological and before syntactic analysis. Morphological analysis corresponds to part-of-speech tagging in English. Japanese syntactic structures are usuall"
C00-1082,E99-1026,1,0.925638,"t for analyzing Japanese sentences. In experiments comparing the four previously available machinelearning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best. 1 Introduction This paper is about machine learning methods for identifying bunsetsus, which correspond to English phrasal units such as noun phrases and prepositional phrases. Since Japanese syntactic analysis is usually done after bunsetsu identi cation (Uchimoto et al., 1999), identifying bunsetsu is important for analyzing Japanese sentences. The conventional studies on bunsetsu identi cation1 have used hand-made rules (Kameda, 1995; Kurohashi, 1998), but bunsetsu identi cation is not an easy task. Conventional studies used many hand-made rules developed at the cost of many man-hours. Kurohashi, for example, made 146 rules for bunsetsu identi cation (Kurohashi, 1998). In an attempt to reduce the number of manhours, we used machine-learning methods for bunsetsu identi cation. Because it was not clear which machine-learning method would be the one most appropriate"
C00-1082,P94-1013,0,0.0437241,"| Symbol Punctuation 2 2 Figure 5: Example of levels of similarity but are expanded by combining all the features, and are stored in a one-dimensional list. A priority order is de ned in a certain way and all of the rules are arranged in this order. The decision-list method searches for rules from the top of the list and analyzes a particular problem by using only the rst applicable rule. In this study we used in the decision-list method the same 152 types of patterns that were used in the maximum-entropy method. To determine the priority order of the rules, we referred to Yarowsky&apos;s method (Yarowsky, 1994) and Nishiokayama&apos;s method (Nishiokayama et al., 1998) and used the probability and frequency of each rule as measures of this priority order. When multiple rules had the same probability, the rules were arranged in order of their frequency. Suppose, for example, that Pattern A Noun: Normal Noun; Particle: Case-Particle: none: wo; Verb: Normal Form: 217; Symbol: Punctuation&quot; occurs 13 times in a learning set and that ten of the occurrences include the inserted partition mark. Suppose also that Pattern B Noun; Particle; Verb; Symbol&quot; occurs 123 times in a learning set and that 90 of the occur"
C00-2126,J96-1002,0,\N,Missing
C00-2126,P99-1018,0,\N,Missing
C00-2128,P99-1008,0,0.0150643,"in corpora, i.e. noun phrases of the form A no B can be found in corpora but their semantic relations are not. If we need such semantic relations, we must semantically analyze the noun phrases (Kurohashi and Sakai, 1999). Applicability to other languages Japanese noun phrases of the form A no B are specific to Japanese. The proposed method, however, could easily be extended to other languages. For example, in English, noun phrases B of A could be used to extract semantically related nouns. Nouns related by is-a relations or part-of relations could also be extracted from corpora (Hearst, 1992; Berland and Charniak, 1999). If such semantically related nouns are extracted, Candidates beer, cola, mizu (water) yu (hot water), oyu (hot water), nettˆo (boiling water) zyˆoyˆ osya (car), best seller, kuruma (vehicle) e (painting), image,aizin (lover) gensaku (original work), meisaku (famous story), daihyˆosaku (important work) menuetto (minuet), kyoku (music), piano si (poem), tyosyo (writings), tyosaku (writings) care,kyˆ usoku (rest), kaigo (nursing) hito (person),tomodati (friend), byˆonin (sick person) Nihon (Japan),ziko (accident), kigyˆo (company) zikanho (assistant vice-minister), seikai (political world), gik"
C00-2128,C96-1025,0,0.271611,"rocessing applications, especially for machine translation (Kamei and Wakao, 1992; Fass, 1997). A metonymy may be acceptable in a source language but unacceptable in a target language. For example, a direct translation of ‘he read Mao’, which is acceptable in English and Japanese, is completely unacceptable in Chinese (Kamei and Wakao, 1992). In such cases, the machine translation system has to interpret metonymies to generate acceptable translations. Previous approaches to processing metonymy have used hand-constructed ontologies or semantic networks (Fass, 1988; Iverson and Helmreich, 1992; Bouaud et al., 1996; Fass, 1997).1 1 As for metaphor processing, Ferrari (1996) used texSuch approaches are restricted by the knowledge bases they use, and may only be applicable to domain-specific tasks because the construction of large knowledge bases could be very difficult. The method outlined in this paper, on the other hand, uses corpus statistics to interpret metonymy, so that a variety of metonymies can be handled without using hand-constructed knowledge bases. The method is quite promising as shown by the experimental results given in section 5. 2 Recognition and Interpretation Two main steps, recogniti"
C00-2128,J92-4003,0,0.0133448,"below).3 Examples of these and similar types of metonymic concepts (Lakoff and Johnson, 1980; Fass, 1997) are given below. Container for contents • glass no mizu (water) • nabe (pot) no ryˆ ori (food) Artist for artform • Beethoven no kyoku (music) • Picasso no e (painting) Object for user • ham sandwich no kyaku (customer) • sax no sˆosya (performer) Whole for part • kuruma (car) no tire Information Source We use a large corpus to extract nouns which can be syntactically related to the explicit term of a metonymy. A large corpus is valuable as a source of such nouns (Church and Hanks, 1990; Brown et al., 1992). We used Japanese noun phrases of the form A no B to extract nouns that were syntactically related to A. Nouns in such a syntactic relation are usually close semantic relatives of each other (Murata et al., 1999), and occur relatively infrequently. We thus also used an A near B relation, i.e. identifying the other nouns within the target sentence, to extract nouns that may be more loosely related to A, but occur more frequently. These two types of syntactic relation are treated differently by the statistical measure which we will discuss in section 4. The Japanese noun phrase A no B roughly c"
C00-2128,J90-1003,0,0.0136441,"s and artist for artform below).3 Examples of these and similar types of metonymic concepts (Lakoff and Johnson, 1980; Fass, 1997) are given below. Container for contents • glass no mizu (water) • nabe (pot) no ryˆ ori (food) Artist for artform • Beethoven no kyoku (music) • Picasso no e (painting) Object for user • ham sandwich no kyaku (customer) • sax no sˆosya (performer) Whole for part • kuruma (car) no tire Information Source We use a large corpus to extract nouns which can be syntactically related to the explicit term of a metonymy. A large corpus is valuable as a source of such nouns (Church and Hanks, 1990; Brown et al., 1992). We used Japanese noun phrases of the form A no B to extract nouns that were syntactically related to A. Nouns in such a syntactic relation are usually close semantic relatives of each other (Murata et al., 1999), and occur relatively infrequently. We thus also used an A near B relation, i.e. identifying the other nouns within the target sentence, to extract nouns that may be more loosely related to A, but occur more frequently. These two types of syntactic relation are treated differently by the statistical measure which we will discuss in section 4. The Japanese noun ph"
C00-2128,C88-1036,0,0.0563287,"metonymy is vital for natural language processing applications, especially for machine translation (Kamei and Wakao, 1992; Fass, 1997). A metonymy may be acceptable in a source language but unacceptable in a target language. For example, a direct translation of ‘he read Mao’, which is acceptable in English and Japanese, is completely unacceptable in Chinese (Kamei and Wakao, 1992). In such cases, the machine translation system has to interpret metonymies to generate acceptable translations. Previous approaches to processing metonymy have used hand-constructed ontologies or semantic networks (Fass, 1988; Iverson and Helmreich, 1992; Bouaud et al., 1996; Fass, 1997).1 1 As for metaphor processing, Ferrari (1996) used texSuch approaches are restricted by the knowledge bases they use, and may only be applicable to domain-specific tasks because the construction of large knowledge bases could be very difficult. The method outlined in this paper, on the other hand, uses corpus statistics to interpret metonymy, so that a variety of metonymies can be handled without using hand-constructed knowledge bases. The method is quite promising as shown by the experimental results given in section 5. 2 Recogn"
C00-2128,P96-1048,0,0.0173931,"and Wakao, 1992; Fass, 1997). A metonymy may be acceptable in a source language but unacceptable in a target language. For example, a direct translation of ‘he read Mao’, which is acceptable in English and Japanese, is completely unacceptable in Chinese (Kamei and Wakao, 1992). In such cases, the machine translation system has to interpret metonymies to generate acceptable translations. Previous approaches to processing metonymy have used hand-constructed ontologies or semantic networks (Fass, 1988; Iverson and Helmreich, 1992; Bouaud et al., 1996; Fass, 1997).1 1 As for metaphor processing, Ferrari (1996) used texSuch approaches are restricted by the knowledge bases they use, and may only be applicable to domain-specific tasks because the construction of large knowledge bases could be very difficult. The method outlined in this paper, on the other hand, uses corpus statistics to interpret metonymy, so that a variety of metonymies can be handled without using hand-constructed knowledge bases. The method is quite promising as shown by the experimental results given in section 5. 2 Recognition and Interpretation Two main steps, recognition and interpretation, are involved in the processing of met"
C00-2128,C92-2082,0,0.00414663,"tly expressed in corpora, i.e. noun phrases of the form A no B can be found in corpora but their semantic relations are not. If we need such semantic relations, we must semantically analyze the noun phrases (Kurohashi and Sakai, 1999). Applicability to other languages Japanese noun phrases of the form A no B are specific to Japanese. The proposed method, however, could easily be extended to other languages. For example, in English, noun phrases B of A could be used to extract semantically related nouns. Nouns related by is-a relations or part-of relations could also be extracted from corpora (Hearst, 1992; Berland and Charniak, 1999). If such semantically related nouns are extracted, Candidates beer, cola, mizu (water) yu (hot water), oyu (hot water), nettˆo (boiling water) zyˆoyˆ osya (car), best seller, kuruma (vehicle) e (painting), image,aizin (lover) gensaku (original work), meisaku (famous story), daihyˆosaku (important work) menuetto (minuet), kyoku (music), piano si (poem), tyosyo (writings), tyosaku (writings) care,kyˆ usoku (rest), kaigo (nursing) hito (person),tomodati (friend), byˆonin (sick person) Nihon (Japan),ziko (accident), kigyˆo (company) zikanho (assistant vice-minister),"
C00-2128,P92-1047,0,0.423418,"on Metonymy is a figure of speech in which the name of one thing is substituted for that of something to which it is related. The explicit term is ‘the name of one thing’ and the implicit term is ’the name of something to which it is related’. A typical example of metonymy is He read Shakespeare. (1) ‘Shakespeare’ is substituted for ‘the works of Shakespeare’. ‘Shakespeare’ is the explicit term and ‘works’ is the implicit term. Metonymy is pervasive in natural language. The correct treatment of metonymy is vital for natural language processing applications, especially for machine translation (Kamei and Wakao, 1992; Fass, 1997). A metonymy may be acceptable in a source language but unacceptable in a target language. For example, a direct translation of ‘he read Mao’, which is acceptable in English and Japanese, is completely unacceptable in Chinese (Kamei and Wakao, 1992). In such cases, the machine translation system has to interpret metonymies to generate acceptable translations. Previous approaches to processing metonymy have used hand-constructed ontologies or semantic networks (Fass, 1988; Iverson and Helmreich, 1992; Bouaud et al., 1996; Fass, 1997).1 1 As for metaphor processing, Ferrari (1996) u"
C00-2128,P99-1062,0,0.0621513,"lly related to A. Nouns in such a syntactic relation are usually close semantic relatives of each other (Murata et al., 1999), and occur relatively infrequently. We thus also used an A near B relation, i.e. identifying the other nouns within the target sentence, to extract nouns that may be more loosely related to A, but occur more frequently. These two types of syntactic relation are treated differently by the statistical measure which we will discuss in section 4. The Japanese noun phrase A no B roughly corresponds to the English noun phrase B of A, but it has a much broader range of usage (Kurohashi and Sakai, 1999). In fact, A no B can express most of the possible types of semantic relation between two nouns including metonymic • door no knob These examples suggest that we can extract semantically related nouns by using the A no B relation. 4 Statistical Measure A metonymy ‘Noun A Case-Marker R Predicate V ’ can be regarded as a contraction of ‘Noun A Syntactic-Relation Q Noun B CaseMarker R Predicate V ’, where A has relation Q to B (Yamamoto et al., 1998). For example, Shakespeare wo yomu (read) (read Shakespeare) is regarded as a contraction of Shakespeare no sakuhin (works) wo yomu (read the works o"
C00-2128,W99-0205,1,0.768329,"artform • Beethoven no kyoku (music) • Picasso no e (painting) Object for user • ham sandwich no kyaku (customer) • sax no sˆosya (performer) Whole for part • kuruma (car) no tire Information Source We use a large corpus to extract nouns which can be syntactically related to the explicit term of a metonymy. A large corpus is valuable as a source of such nouns (Church and Hanks, 1990; Brown et al., 1992). We used Japanese noun phrases of the form A no B to extract nouns that were syntactically related to A. Nouns in such a syntactic relation are usually close semantic relatives of each other (Murata et al., 1999), and occur relatively infrequently. We thus also used an A near B relation, i.e. identifying the other nouns within the target sentence, to extract nouns that may be more loosely related to A, but occur more frequently. These two types of syntactic relation are treated differently by the statistical measure which we will discuss in section 4. The Japanese noun phrase A no B roughly corresponds to the English noun phrase B of A, but it has a much broader range of usage (Kurohashi and Sakai, 1999). In fact, A no B can express most of the possible types of semantic relation between two nouns inc"
C02-1060,P94-1038,0,\N,Missing
C02-1060,P93-1022,0,\N,Missing
C02-1060,P90-1034,0,\N,Missing
C02-1060,P97-1009,0,\N,Missing
C02-1060,P98-2148,0,\N,Missing
C02-1060,C98-2143,0,\N,Missing
C96-2134,C94-2172,0,\N,Missing
C98-2145,1993.tmi-1.18,1,0.825132,"o &quot;OJIISAN (old man)&quot; in the following sentences have the salne referent, the second &quot;OJIISAN (old man)&quot; should be prononfinalized in the translation into English. For languages that have articles, like English, we can use articles (&quot;the&quot;, &quot;a&quot;, and so on) to decide whether a noun phrase has an antecedent or not. In contrast, for languages that have no articles, like Japanese, it is dimcult to decide whether a noun phrase has all autecedent. We previously estimated tile referential properties of noun phrases that correspond to articles for the translation of Japanese noun phrases into English (Murata and Nagao 1993). By using these referential properties, our system determines the referents of noun p h r ~ e s in Japanese sentences. Noun phrases are classified by referential property into generic noun phrases, definite noun phrases, and indefinite noun phrases. When tile referential property of a noun phrase is a definite noun phrase, the noun phrase can refer to the entity denoted by a noun phrase that has already appeared. When the referential property of a noun phrase is an indefinite noun phrase or a generic noun phrase, the uoun phrase cannot refer to the entity denoted by a noun phrase that has alr"
D09-1097,bond-etal-2008-boot,1,0.581211,"Missing"
D09-1097,P99-1016,0,0.0133585,"ctic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities or hierarchical structures in HTML documents. Etzioni et Documents hypernym ..… word The same hypernym is selected for all words in a class. Pattern-based method (Hearst 1992, Pantel et al. 2004a, Ando et al. 2003, Snow et al. 2005, Snow et al. 2006, and Etzioni et al. 2005) word word word word word Word Class Clustering-based method (Caraballo 1999, Pantel et al. 2004b, Shinzato et al. 2004, and Etzioni e"
D09-1097,C92-2082,0,0.762187,"pproach. duce some related works in Section 2. Section 3 describes the Wikipedia relation database. Section 4 describes the distributional similarity calculated by the two methods. In Section 5, we describe a method to discover an appropriate hypernym for each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Corpus/documents hypernym such as word Co-occurrences in a pattern are needed Related Works Most previous researchers have relied on lexico-syntactic patterns for hyponymy acquisition. Lexico-syntactic patterns were first used by Hearst (1992). The patterns used by her included “NP0 such as NP1,” in which NP0 is a hypernym of NP1. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method wa"
D09-1097,P08-1047,1,0.864076,"Missing"
D09-1097,P99-1004,0,0.0542805,"etween v and n. In Japanese, a relation rel is represented by postpositions attached to n and the phrase composed of n and rel modifies v. Each triple is divided into two parts. The first is &lt;v, rel&gt; and the second is n. Then, we consider the conditional probability of occurrence of the pair &lt;v, rel&gt;: P(&lt;v, rel&gt;|n). P(&lt;v, rel&gt;|n) can be regarded as the distribution of the grammatical contexts of the noun phrase n. The distributional similarity can be defined as the distance between these distributions. There are several kinds of functions for evaluating the distance between two distributions (Lee 1999). Our method uses the Jensen-Shannon divergence. The Jensen-Shannon divergence between two probability distributions, P (⋅ |n1 ) and P (⋅ |n2 ) , can be calculated as follows: DJS ( P (⋅ |n1 ) ||P (⋅ |n2 )) P (⋅ |n1 ) + P (⋅ |n2 ) 1 ) ( DKL ( P (⋅ |n1 ) || 2 2 P (⋅ |n1 ) + P (⋅ |n2 ) + DKL ( P (⋅ |n2 ) || )), 2 = &lt;v ,rel &gt;∈D if f (&lt; v, rel , n &gt;) &gt; 0, where f(&lt;v, rel, n&gt;) is the frequency of a triple &lt;v, rel, n&gt; and D is the set defined as { &lt;v, rel &gt; | f(&lt;v, rel, n&gt;) &gt; 0 }. In the case of f(&lt;v, rel, n&gt;) = 0, P(&lt;v, rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the"
D09-1097,P06-1101,0,0.0472608,"P1,” in which NP0 is a hypernym of NP1. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities o"
D09-1097,sumida-etal-2008-boosting,1,0.937771,"aper, we use the term “word” for both “a single-word word” and “a multi-word word.” 929 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929–937, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP A hypernym is selected for each word independently. Wikipedia relation database Wikipedia Hyponymy relations are extracted using the layout information of Wikipedia. hypernym : Selected from hypernyms in the Wikipedia relation database. No direct co-occurrences of hypernym and hyponym in corpora are needed. Wikipedia-based approach (Ponzetto et al. 2007 and Sumida et al. 2008) Target word: Selected from the Web : word k similar words Figure 1: Overview of the proposed approach. duce some related works in Section 2. Section 3 describes the Wikipedia relation database. Section 4 describes the distributional similarity calculated by the two methods. In Section 5, we describe a method to discover an appropriate hypernym for each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Corpus/documents hypernym such as word Co-occurrences in a pattern are needed Related Works Most previous researchers have relied on le"
D09-1097,N04-1041,0,0.0180963,"Missing"
D09-1097,P99-1014,0,0.0117575,"rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the usual maximum likelihood estimation, we modified it as above. Although this might seems strange, this kind of modification is common in information retrieval as a term weighing method (Manning et al. 1999) and it is also applied in some studies to yield better word similarities (Terada et al. 2006, Kazama et al. 2009). We also adopted this idea in this study. 4.2 P(⋅ |n1 ) . P(⋅ |n2 ) Finally, the distributional similarity between two words, n1 and n2, is defined as follows: Distributional Similarity Based on CVD Rooth et al. (1999) and Torisawa (2001) showed that EM-based clustering using verb-noun dependencies can produce semantically clean noun clusters. We exploit these EM-based clustering results as the smoothed contexts for noun n. In Torisawa’s model (2001), the probability of occurrence of the triple &lt;v, rel, n&gt; is defined as follows: P(&lt; v, rel, n &gt;) where DKL indicates the Kullback-Leibler divergence and is defined as follows: DKL ( P(⋅ |n1 ) ||P(⋅ |n2 ))= ∑ P(⋅ |n1 ) log log( f (&lt; v, rel , n &gt;)) + 1 ∑ log( f (&lt; v, rel , n &gt;) + 1 =def ∑a∈A P(&lt; v, rel &gt; |a) P(n |a) P(a), where a denotes a hidden class of &lt;v,rel&gt;"
D09-1097,N04-1010,1,0.869582,"Missing"
D09-1097,shinzato-etal-2008-large,0,\N,Missing
D09-1122,D07-1017,0,\N,Missing
D09-1122,W03-1608,0,\N,Missing
D09-1122,C08-1107,0,\N,Missing
D09-1122,N06-1007,0,\N,Missing
D09-1122,N06-1008,1,\N,Missing
D09-1122,N06-1023,0,\N,Missing
D09-1122,W03-1011,0,\N,Missing
D09-1122,C08-1029,0,\N,Missing
D09-1122,N03-1003,0,\N,Missing
D09-1122,P05-1014,0,\N,Missing
D09-1122,P05-1074,0,\N,Missing
D09-1122,P98-2127,0,\N,Missing
D09-1122,C98-2122,0,\N,Missing
D09-1122,P06-1107,0,\N,Missing
D09-1122,kawahara-kurohashi-2006-case,0,\N,Missing
D09-1122,W04-3206,0,\N,Missing
D10-1087,C92-2100,0,0.0542418,"stics on comma insertion into non-Japanese written texts (White and Rajkumar, 2008; Guo et al., 2010). In Japanese, there are several usages of commas, and some usages are specific to Japanese due to its linguistic nature. Therefore, just adopting the above mentioned methods, which have been developed to process non-Japanese texts, is not sufficient to enable high-quality comma insertion into Japanese sentences. Development of a method based on the detailed analysis of Japanese commas is required. Furthermore, there have been some investigations on comma insertion into Japanese written texts (Hayashi, 1992; Suzuki et al., 1995). These investigations have adopted rule-based methods. However, the number of their rules is not necessarily sufficient, and no quantitative evaluation has been performed. 3 Table 1: Categorization of usages of commas # 1 2 3 usage of comma commas between clauses commas indicating clear dependency relations commas for avoiding reading mistakes and reading difficulty commas indicating the subject commas inserted after a conjunction or adverb at the beginning of a sentence commas inserted between parallel words or phrases commas inserted after an adverbial phrase to indica"
D10-1087,W08-1703,0,0.0213384,"since pause information cannot be obtained from texts, we cannot use this approach because our targets are written texts. In addition, there have been some investigations 1 Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and zero or more ancillary words. 892 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 892–901, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics on comma insertion into non-Japanese written texts (White and Rajkumar, 2008; Guo et al., 2010). In Japanese, there are several usages of commas, and some usages are specific to Japanese due to its linguistic nature. Therefore, just adopting the above mentioned methods, which have been developed to process non-Japanese texts, is not sufficient to enable high-quality comma insertion into Japanese sentences. Development of a method based on the detailed analysis of Japanese commas is required. Furthermore, there have been some investigations on comma insertion into Japanese written texts (Hayashi, 1992; Suzuki et al., 1995). These investigations have adopted rule-based"
I05-2024,P96-1003,0,0.0357172,"osed method was much higher than that of the conventional TFIDF method when the process was focused on retrieving highly relevant documents, suggesting that the proposed method might be especially suited to information retrieval tasks in which precision is more critical than recall. 1 Introduction Information retrieval (IR) has been studied since an earlier stage [e.g., (Menzel, 1966)] and several kinds of basic retrieval models have been proposed (Salton and Buckley, 1988) and a number of improved IR systems based on these models have been developed by adopting various NLP techniques [e.g., (Evans and Zhai, 1996; Mitra et al., 1997; Mandara, et al., 1998; Murata, et al., 2000)]. However, an epoch-making technique 138 1 There have been a number of studies of SOM on data mining and visualization [e.g., (Kohonen, et al., 2000)] since the WEBSOM was developed in 1996. To our knowledge, however, these works mainly focused on confirming the capabilities of SOM in the self-organization and/or in the visualization. In this study, we slot the SOM-based processing into a practical IR system that enables visualization of the IR while at the same time improving its precision. The another feature of our study dif"
I05-2024,W98-0704,0,0.0308187,"he conventional TFIDF method when the process was focused on retrieving highly relevant documents, suggesting that the proposed method might be especially suited to information retrieval tasks in which precision is more critical than recall. 1 Introduction Information retrieval (IR) has been studied since an earlier stage [e.g., (Menzel, 1966)] and several kinds of basic retrieval models have been proposed (Salton and Buckley, 1988) and a number of improved IR systems based on these models have been developed by adopting various NLP techniques [e.g., (Evans and Zhai, 1996; Mitra et al., 1997; Mandara, et al., 1998; Murata, et al., 2000)]. However, an epoch-making technique 138 1 There have been a number of studies of SOM on data mining and visualization [e.g., (Kohonen, et al., 2000)] since the WEBSOM was developed in 1996. To our knowledge, however, these works mainly focused on confirming the capabilities of SOM in the self-organization and/or in the visualization. In this study, we slot the SOM-based processing into a practical IR system that enables visualization of the IR while at the same time improving its precision. The another feature of our study differing from others is that we performed com"
I05-6002,P98-1013,0,0.182561,"Missing"
I05-6002,C04-1134,0,0.0162456,"hat are compatible with the BFN, but for Japanese at least, no data has been released in a usable form, except for a few annotation examples for verbs of motion. In sum, no useful resource exists for framebased description/analysis of Japanese. This is one of the reasons that we attempted the task in this paper, along with our efforts to assess the usefulness of the database provided by BFN. The anonymous reviewers of our paper pointed out that there have been some similar projects and other methodologies that have tried to translate BFN into other languages automatically, such as BiFrameNet (Chen and Fung, 2004) and Romance FrameNet2 , and that it would have been better to include the comparison against them. BiFrameNet presented an automatic approach to constructing a bilingual semantic network using the Chinese HowNet, which is a Chinese ontology. While it is an interesting approach, we have not compared their results with ours, mainly because they seem to have used different resources and had somewhat different goals, along with the space consideration. No papers are released, let alone being available to us, related to the Romance FrameNet project for the time being. We couldn’t help putting a co"
I05-6002,P03-1068,0,0.384504,"Missing"
I05-6002,P03-1010,1,0.825151,"comparison against them. BiFrameNet presented an automatic approach to constructing a bilingual semantic network using the Chinese HowNet, which is a Chinese ontology. While it is an interesting approach, we have not compared their results with ours, mainly because they seem to have used different resources and had somewhat different goals, along with the space consideration. No papers are released, let alone being available to us, related to the Romance FrameNet project for the time being. We couldn’t help putting a comparison with it on hold.3 Proposed Procedure We used a bilingual corpus (Utiyama and Isahara, 2003) to examine which semantic frames of BFN contained LUs relevant to the Japanese verb osou. JFN, for example, used a mono-lingual corpus to construct the semantic frames. In cases like this, the construction might be inefficient because they have to construct all semantic frames by themselves. But this affects on the reliability of the frames identified and described. This risk of arbitrary description can be reduced by using a bilingual corpus, if it is of high-quality. 2.1 Identifying English equivalents of ”osou” We chose Japanese-English alignments from the bilingual corpus in which the Jap"
I05-6002,kingsbury-palmer-2002-treebank,0,0.0288436,"y situation-based, or “case-based” in the sense of Case-based Reasoning (Kolodner, 1993), and difficult to specify in terms of the lexical semantic descriptions available in resources such as WordNet (Fellbaum, 1998) which don’t specify associative relationships among concepts, including the relationships between ROBBER (e.g., a man) and WAREHOUSE OF VALUABLES (e.g., a bank, museum, jewelry shop), and the one between a PREDATOR (e.g., a wolf) and its PREY (e.g., sheep, rabbit). Thus, the NLP community has a critical need for resources that encode this kind of information. Along with PropBank (Kingsbury and Palmer, 2002; Ellsworth et al., 2004), Berkeley FrameNet An attempt was made to semi-automatically obtain “lexical units” (LUs) for Japanese from the English LUs defined in the semantic frame database provided by Berkeley FrameNet (BFN) using an English-Japanese bilingual corpus. This task was a prerequisite to building a complete database of semantic frames for Japanese. In the task, a Japanese word is first translated into an English word or phrase, E. E is one of the lexical units that evoked a particular semantic frame, F, in the BFN database. When other lexical units of F are translated back into Jap"
I05-6002,C98-1013,0,\N,Missing
I08-2100,P02-1054,0,0.036911,"and “Tokyo is also one of Japan’s 47 prefectures”, from Websites, newspaper articles, or encyclopedias. The system then outputs “Tokyo” as the correct answer. We believe question-answering systems will become a more convenient alternative to other systems designed for information retrieval and a basic component of future artificial intelligence systems. Numerous researchers have recently been attracted to this important topic. These researchers have produced many interesting studies on question-answering systems (Kupiec, 1993; Ittycheriah et al., 2001; Clarke et al., 2001; Dumis et al., 2002; Magnini et al., 2002; Moldovan et al., 2003). Evaluation conferences and contests on question-answering systems have also been held. In particular, the U.S.A. has held the Text REtrieval Conferences (TREC) (TREC-10 committee, 2001), and Japan has hosted the QuestionAnswering Challenges (QAC) (National Institute of Informatics, 2002) at NTCIR (NII Test Collection for IR Systems ) 3. These conferences and contests have aimed at improving question-answering systems. The researchers who participate in these create question-answering systems that they then use to answer the same questions, and each system’s performanc"
I08-2100,N04-1008,0,0.0336121,"systems that they then use to answer the same questions, and each system’s performance is then evaluated to yield possible improvements. We addressed non-factoid question answering in NTCIR-6 QAC-4. For example, when the question was “Why are people opposed to the Private Information Protection Law?” the system retrieved sentences based on terms appearing in the question and output an answer using the retrieved sentences. Numerous studies have addressed issues that are involved in the answering of non-factoid questions (Berger et al., 2000; Blair-Goldensohn et al., 2003; 727 Xu et al., 2003; Soricut and Brill, 2004; Han et al., 2005; Morooka and Fukumoto, 2006; Maehara et al., 2006; Asada, 2006). We constructed a system for answering nonfactoid Japanese questions for QAC-4. We used methods of passage retrieval for the system. We extracted paragraphs based on terms from an input question and output them as the preferred answers. We classified the non-factoid questions into six categories. We used a particular method for each category. For example, we increased the scores of paragraphs including the word “reason” for questions including the word “why.” We performed experiments using the NTCIR-6 QAC-4 data"
ma-etal-2008-selection,H05-1059,0,\N,Missing
ma-etal-2008-selection,W05-1514,0,\N,Missing
ma-etal-2008-selection,P98-1069,0,\N,Missing
ma-etal-2008-selection,C98-1066,0,\N,Missing
ma-etal-2008-selection,P03-1010,1,\N,Missing
murata-etal-2010-construction,ono-etal-2008-construction,1,\N,Missing
murata-etal-2010-construction,P09-1060,1,\N,Missing
murata-etal-2010-construction,W06-3711,0,\N,Missing
murata-etal-2010-construction,P06-2088,1,\N,Missing
murata-etal-2010-construction,matsubara-etal-2002-bilingual,1,\N,Missing
murata-etal-2010-construction,frederking-etal-2002-field,0,\N,Missing
P00-1042,W98-1120,0,\N,Missing
P00-1042,M98-1018,0,\N,Missing
P00-1042,W98-1118,0,\N,Missing
P00-1042,W96-0213,0,\N,Missing
P00-1042,M95-1012,0,\N,Missing
P00-1042,E99-1026,1,\N,Missing
P00-1042,J96-1002,0,\N,Missing
P00-1042,J95-4004,0,\N,Missing
P00-1042,M98-1006,0,\N,Missing
P00-1042,M95-1013,0,\N,Missing
P00-1042,A97-1029,0,\N,Missing
P06-2076,W00-0730,0,0.0252752,"ginal case particles in passivevoice sentences source case particles. We created tags for target case particles in the corpus. If we can determine the target case particles in a given sentence, we can transform the case particles in passive-voice sentences into case particles for the active voice. Therefore, our goal was to determine the target case particles. 3 Machine learning method (support vector machine) We used a support vector machine as the basis of our machine-learning method. This is because support vector machines are comparatively better than other methods in many research areas (Kudoh and Matsumoto, 2000; Taira and Haruno, 2001; 588 Large Margin Figure 4: Maximizing margin Murata et al., 2002). Data consisting of two categories were classified by using a hyperplane to divide a space with the support vector machine. When these two categories were, positive and negative, for example, enlarging the margin between them in the training data (see Figure 42 ), reduced the possibility of incorrectly choosing categories in blind data (test data). A hyperplane that maximized the margin was thus determined, and classification was done using that hyperplane. Although the basics of this method are as desc"
P09-1060,W02-2016,0,0.339023,"Missing"
P09-1060,matsubara-etal-2002-bilingual,1,0.526722,"accordance with only the width of a screen without considering the proper points of linefeeds, the caption is not easy to read. Especially, since the audience is forced to read the caption in synchronization with the speaker’s utterance speed, it is important that linefeeds are properly inserted into the displayed text in consideration of the readability as shown in Figure 3. To investigate whether the line insertion facilitates the readability of the displayed texts, we conducted an experiment using the transcribed text of lecture speeches in the Simultaneous Interpretation Database (SIDB) (Matsubara et al., 2002). We randomly selected 50 sentences from the data, and then created the following two texts for each sentence based on two different concepts about linefeed insertion. (For example, environmental problem) (population problem) (AIDS problem and so on) a lot of global-scale problems have occurred (and unfortunately, these problems) (to continue during also 21st century) (or if we look through blue glasses) (seems to become worse) 49 1 50 2 49 3 37 40 4 5 36 6 43 7 49 48 34 8 9 10 subject ID Figure 4: Result of investigation of effect of linefeed insertion into transcription （2）Text into which li"
P10-1026,P94-1038,0,0.168634,"roblem and tackled in the context of language modeling and supervised machine learning. However, to our knowledge, there Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the ﬁeld. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗ (1) The work was done while the author was at NICT. 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the Bayesian estimation and the Bhattacharyya coefﬁcient. Section 3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some implementation issues and the solutions. T"
P10-1026,I08-1025,0,0.0193862,"and set the output variable to the default value. Then, we iterate over the sparse vectors c(w1 , fk ) and c(w2 , fk ). If We used the GNU (www.gnu.org/software/gsl/), function. = T 1 X δ(wi ∈ ans), T i=1 AP = N 1 X δ(wi ∈ ans)P@i. R i=1 δ(wi ∈ ans) returns 1 if the output word wi is in the answers, and 0 otherwise. N is the number of outputs and R is the number of the answers. MP@T and MAP are the averages of these values over all input words. For each k: (D): exp(2(lnΓ(αk + 12 )). 2 P@T 5.2 Collecting context proﬁles Dependency relations are used as context proﬁles as in Kazama and Torisawa (2008) and Kazama et al. (2009). From a large corpus of Japanese Web documents (Shinzato et al., 2008) (100 million Scientiﬁc Library (GSL) which implements this 250 “B” is for the validation of the parameter tuning. documents), where each sentence has a dependency parse, we extracted noun-verb and nounnoun dependencies with relation types and then calculated their frequencies in the corpus. If a noun, n, depends on a word, w, with a relation, r, we collect a dependency pair, (n, 〈w, r〉). That is, a context fk , is 〈w, r〉 here. For noun-verb dependencies, postpositions in Japanese represent relation"
P10-1026,P97-1008,0,0.0366308,"known that p(φ|D) is also a Dirichlet distribution for this simplest case, and it can be analytically calculated as follows. p(φ|D) = Dir(φ|{αk + c(k)}), (6) where c(k) is the frequency of choice k in data D. For example, c(k) = c(wi , fk ) in the estimation of p(fk |wi ). This is very simple: we just need to add the observed counts to the hyperparameters. 248 2.2 Bhattacharyya coefﬁcient When the context proﬁles are probability distributions, we usually utilize the measures on probability distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al., 1994; Dagan et al., 1997). The JS divergence is deﬁned as follows. JS(p1 ||p2 ) = = 2 where pavg = p1 +p is a point-wise average of p1 2 and p2 and KL(.) is the Kullback-Leibler divergence. Although we found that the JS divergence is a good measure, it is difﬁcult to derive an efﬁcient calculation of Eq. 2, even in the Dirichlet prior case.1 In this study, we employ the Bhattacharyya coefﬁcient (Bhattacharyya, 1943) (BC for short), which is deﬁned as follows: K X √ BCb (w1 , w2 ) = Γ(α0 + a0 )Γ(β0 + b0 ) × Γ(α0 + a0 + 12 )Γ(β0 + b0 + 12 ) (8) K X Γ(αk + c(w1 , fk ) + 21 )Γ(βk + c(w2 , fk ) + 12 ) , Γ(αk + c(w1 , fk ))"
P10-1026,P06-1124,0,0.012983,"3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some implementation issues and the solutions. Then, Section 5 reports the experimental results. has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation. The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; Cortes and Vapnik, 1995). Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; Mochihashi et al., 2009). In this paper, we apply the Bayesian framework to the calculation of distributional similarity. The method is straightforward: Instead of using the point estimation of v(wi ), we ﬁrst estimate the distribution of the context proﬁle, p(v(wi )), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows: simb (w1 , w2 ) 2 Background 2.1 Bayesian estimation with Dirichlet prior Assume that we estimate a probabilistic model for the observed data D, p(D|φ), which is parameterized with parameters φ. In the maximum li"
P10-1026,P90-1034,0,0.43827,"has been recognized as a serious problem and tackled in the context of language modeling and supervised machine learning. However, to our knowledge, there Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the ﬁeld. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗ (1) The work was done while the author was at NICT. 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the Bayesian estimation and the Bhattacharyya coefﬁcient. Section 3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some im"
P10-1026,P08-1047,1,0.897456,"all c(wi , fk ) = 0 and set the output variable to the default value. Then, we iterate over the sparse vectors c(w1 , fk ) and c(w2 , fk ). If We used the GNU (www.gnu.org/software/gsl/), function. = T 1 X δ(wi ∈ ans), T i=1 AP = N 1 X δ(wi ∈ ans)P@i. R i=1 δ(wi ∈ ans) returns 1 if the output word wi is in the answers, and 0 otherwise. N is the number of outputs and R is the number of the answers. MP@T and MAP are the averages of these values over all input words. For each k: (D): exp(2(lnΓ(αk + 12 )). 2 P@T 5.2 Collecting context proﬁles Dependency relations are used as context proﬁles as in Kazama and Torisawa (2008) and Kazama et al. (2009). From a large corpus of Japanese Web documents (Shinzato et al., 2008) (100 million Scientiﬁc Library (GSL) which implements this 250 “B” is for the validation of the parameter tuning. documents), where each sentence has a dependency parse, we extracted noun-verb and nounnoun dependencies with relation types and then calculated their frequencies in the corpus. If a noun, n, depends on a word, w, with a relation, r, we collect a dependency pair, (n, 〈w, r〉). That is, a context fk , is 〈w, r〉 here. For noun-verb dependencies, postpositions in Japanese represent relation"
P10-1026,P98-2127,0,0.309119,"guage modeling and supervised machine learning. However, to our knowledge, there Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the ﬁeld. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗ (1) The work was done while the author was at NICT. 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the Bayesian estimation and the Bhattacharyya coefﬁcient. Section 3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some implementation issues and the solutions. Then, Section 5 reports the expe"
P10-1026,D09-1098,0,\N,Missing
P10-1026,P09-1012,0,\N,Missing
P10-1026,C98-2122,0,\N,Missing
P10-1026,P08-1000,0,\N,Missing
P98-2150,1993.tmi-1.18,1,0.694109,"e referential property of a noun phrase here means how the noun phrase denotes the referent. If the system can recognize that the second &quot;OJIISAN (old man)&quot; has the referential property of the definite noun phrase, indicating that the noun phrase refers to the contextually non-ambiguous entity, it will be able to judge that the second &quot;OJIISAN (old man)&quot; refers to the entity denoted by the first &quot;OJIISAN (old man). The referential property plays an important role in clarifying the anaphoric relation. We previously classified noun phrases by referential property into the following three types (Murata and Nagao 1993). NP g e n e r i c NP { n o n g e n e r i c NP d e f i n i t e NP i n d e f i n i t e NP G e n e r i c n o u n p h r a s e A noun phrase is classified as generic when it denotes all members of the class described by the noun phrase or the class itself of the noun phrase. For example, &quot;INU(dog)&quot; in the following sentence is a generic noun phrase. INU-WA YAKUNI-TATSU. (dog) (useful) (Dogs are useful.) (3) A generic noun phrase cannot refer to the entity denoted by an indefinite or definite noun phrase. Two generic noun phrases can have the same referent. D e f i n i t e n o u n p h r a s e A nou"
S01-1033,W00-0730,0,0.0142784,"kernel functions. We have used the following polynomial function exclusively. K(x,y) =(x·y+l)d (9) C and d are constants set by experimentation. For all of the experiments reported in this paper, C was fixed as 1 and d was fixed as 2. A set of Xi that satisfies O:i &gt; 0 is called a support vector (SVs) 4 . The summation portion of Equation (4) was calculated using only the examples that were support vectors. Support vector machine methods are capable of handling data consisting of two categories. In general, data consisting of more than two categories is handled by using the pair-wise method (Kudoh and Matsumoto, 2000). In this method, for data consisting of N categories, pairs of two different categories (N (N1)/2 pairs) are constructed. The better cate1rn Figure 1, the circles in the broken lines indicate support vectors. gory is determined by using a 2-category classifier (in this paper, a support vector machine 5 was used as the 2-category classifier), and the correct category is finally determined by ""voting"" on the N(N-1)/2 pairs that result from analysis using the 2-category classifier. The support vector machine method is, in fact, performed by combining the support vector machine and pair-wise meth"
S01-1038,W01-1415,1,0.879344,"ources of information related to the input sentence and examples. Since we want to avoid making complicated rules, we use machine learning models to calculate the similarity. Instead of all examples in the TM, English headwords are used as classes in machine learning models. Therefore, examples having the same English headword are put into the same class and are considered to have the same similarity. 1 A description on how to use ""diff"" can be found in (Murata and Isahara, 2001). 2 Work on using machine learning methods for the tra!lslation of tenses, aspects, and modalities can be found'in {Murata et al., 2001a). 156 Classes identified by machine learning models are basically English headwords in TM, and they are detected manually. For example, English headwords of the examples in Figure 1 are ""feel constrained"", ""constraint"", and ""refrain"", respectively. When English headwords are verbs, they are represented by their basic forms. English words obtained when a Japanese headword is looked up in a Japanese-English dictionary are also used as classes. For the training data, we use not only examples in the TM but also other data collected from bilingual dictionaries or a parallel corpus. The collected"
W01-1415,1999.tmi-1.7,1,\N,Missing
W01-1415,A97-1015,0,\N,Missing
W01-1415,C00-1082,1,\N,Missing
W01-1415,W00-0730,0,\N,Missing
W02-1107,P93-1023,0,0.0854817,"Missing"
W02-1107,W00-0110,1,\N,Missing
W02-1107,P90-1034,0,\N,Missing
W02-1107,P99-1063,1,\N,Missing
W03-1714,J93-2003,0,\N,Missing
W03-1714,C02-1060,1,\N,Missing
W03-1714,C88-1016,0,\N,Missing
W03-1714,C92-2101,0,\N,Missing
W03-1714,P95-1033,0,\N,Missing
W03-1714,P93-1004,0,\N,Missing
W03-1714,C02-1032,0,\N,Missing
W04-2208,C00-1007,0,0.0337696,"h expressions corresponding to a Japanese expression is 1.3 as shown in Table 2. Even when there are two or more possible English expressions, an appropriate English expression can be chosen by selecting a Japanese expression by referring to dependencies in extracted translation pairs. Therefore, in many cases, English sentences can be generated just by reordering the selected expressions. The English word order was estimated manually in this experiment. However, we can automatically estimate English word order by using a language model or an English surface sentence generator such as FERGUS (Bangalore and Rambow, 2000). Unnatural or ungrammatical parallel translations are sometimes generated in the above steps. However, comprehensible translations can be generated as shown in Figure 4. The biggest advantage of this framework is that comprehensible target sentences can be generated basically by referring only to source sentences. Although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing a human interface. For example, when we use a Japanese text generation system from keywords (Uchimoto et al., 2002), users should only select appropriat"
W04-2208,P01-1030,0,0.0114299,"o a given sentence can be semiautomatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus. 1 ‡ New York University 715 Broadway, 7th floor 3-5 Hikari-dai, Seika-cho, Soraku-gun, New York, NY 10003, USA Kyoto 619-0289, Japan {sudo,sekine}@cs.nyu.edu {uchimoto,yujie,murata,isahara}@nict.go.jp Abstract pora and do not have bilingual or multilinNational Institute of Information and Communications Technology Introduction Recently, accurate machine translation systems can be constructed by using parallel corpora (Och and Ney, 2000; Germann et al., 2001). However, almost all existing machine translation systems do not consider the problem of translating a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually"
W04-2208,2002.tmi-papers.9,0,0.015857,"and Japanese-English machine translation. We can directly compare various methods of machine translation by using this corpus. It can be summarized as follows in terms of the characteristics of the corpus. One-sentence to one-sentence translation can be simply used for the evaluation of various methods of machine translation. Morphological and syntactic information can be used for the evaluation of methods that actively use morphological and syntactic information, such as methods for examplebased machine translation (Nagao, 1981; Watanabe et al., 2003), or transfer-based machine translation (Imamura, 2002). Phrasal alignment is used for the evaluation of automatically acquired translation knowledge (Yamamoto and Matsumoto, 2003). An actual comparison and evaluation is our future work. 3.2 Analysis of Translation One-sentence to one-sentence translation reﬂects contextual information. Therefore, it is suitable to investigate the inﬂuence of the context on the translation. For example, we can investigate the diﬀerence in the use of demonstratives and pronouns between English and Japanese. We can also investigate the diﬀerence in the use of anaphora. Morphological and syntactic information and phr"
W04-2208,J93-2004,0,0.0235999,"ng a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually one-to-many or many-to-many. On the other hand, high-quality treebanks such as the Penn Treebank (Marcus et al., 1993) and the Kyoto University text corpus (Kurohashi and Nagao, 1997) have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structure analysis. However, almost all of these highquality treebanks are based on monolingual corgual information. There are few high-quality bilingual or multilingual treebank corpora because parallel corpora have mainly been actively used for machine translation between related languages such as English and French, therefore their syntactic structures are not required so much for"
W04-2208,P00-1056,0,0.126608,"ntence is similar to a given sentence can be semiautomatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus. 1 ‡ New York University 715 Broadway, 7th floor 3-5 Hikari-dai, Seika-cho, Soraku-gun, New York, NY 10003, USA Kyoto 619-0289, Japan {sudo,sekine}@cs.nyu.edu {uchimoto,yujie,murata,isahara}@nict.go.jp Abstract pora and do not have bilingual or multilinNational Institute of Information and Communications Technology Introduction Recently, accurate machine translation systems can be constructed by using parallel corpora (Och and Ney, 2000; Germann et al., 2001). However, almost all existing machine translation systems do not consider the problem of translating a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-"
W04-2208,C02-1064,1,0.816552,"tence generator such as FERGUS (Bangalore and Rambow, 2000). Unnatural or ungrammatical parallel translations are sometimes generated in the above steps. However, comprehensible translations can be generated as shown in Figure 4. The biggest advantage of this framework is that comprehensible target sentences can be generated basically by referring only to source sentences. Although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing a human interface. For example, when we use a Japanese text generation system from keywords (Uchimoto et al., 2002), users should only select appropriate keywords. We are investigating whether or not we can generate similar parallel translations to all of the Japanese sentences appearing on January 17, 1995. So far, we found that we can generate similar parallel translations to 691 out of 840 sentences (the average number of bunsetsus is about 10.3) including the 102 sentences described in Section 3.3. We found that we could not generate similar parallel translations to 149 out of 840 sentences. In the proposed framework of similar parallel translation generation, the language appearing in a corpus corresp"
W04-2208,P01-1067,0,0.102382,"erefore their syntactic structures are not required so much for aligning words or phrases. However, syntactic structures are necessary for machine translation between languages whose syntactic structures are diﬀerent from each other, such as in Japanese-English, Japanese-Chinese, and Chinese-English machine translations, because it is more diﬃcult to automatically align words or phrases between two unrelated languages than between two related languages. Actually, it has been reported that syntactic structures contribute to improving the accuracy of word alignment between Japanese and English (Yamada and Knight, 2001). Therefore, if we had a high-quality parallel treebank corpus, the accuracies of machine translation between languages whose syntactic structures are diﬀerent from each other would improve. Furthermore, if the parallel treebank corpus had word or phrase alignment, the accuracy of automatic word or phrase alignment would increase by using the parallel treebank corpus as training data. However, so far, there is no aligned parallel treebank corpus whose domain is not restricted. For example, the Japanese Electronics Industry Development Association’s (JEIDA’s) bilingual corpus (Isahara and Harun"
W04-2208,A00-2018,0,\N,Missing
W05-0629,W05-0620,0,0.106919,"Missing"
W05-0629,W04-2416,0,0.217012,"s are one of the binary classifiers based on the maximum margin strategy introduced by Vapnik (Vapnik, 1995). This algorithm has achieved good performance in many classification tasks, e.g. named entity recognition and document classification. There are some advantages to SVMs in that (i) they have high generalization performance independent of the dimensions of the feature vectors and (ii) learning with a combination of multiple features is possible by using the polynomial kernel function (Yamada and Matsumoto, 2003). SVMs were used in the CoNLL-2004 shred task and achieved good performance (Hacioglu et al., 2004) (KyungMi Park and Rim, 2004). We used YamCha (Yet Another Multipurpose Chunk Annotator) 1 (Kudo and Matsumoto, 2001), which is a general purpose SVM-based chunker. We also used TinySVM 2 as a package for SVMs. 3 System Description 3.1 Data Representation We changed the representation of original data according to Hacioglu et al. (Hacioglu et al., 2004) in our system. 1 2 http://chasen.org/˜ taku/software/yamcha/ http://chasen.org/˜ taku/software/TinySVM/ 197 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 197–200, Ann Arbor, June 2005. 2005 Associ"
W05-0629,N01-1025,0,0.0265473,"algorithm has achieved good performance in many classification tasks, e.g. named entity recognition and document classification. There are some advantages to SVMs in that (i) they have high generalization performance independent of the dimensions of the feature vectors and (ii) learning with a combination of multiple features is possible by using the polynomial kernel function (Yamada and Matsumoto, 2003). SVMs were used in the CoNLL-2004 shred task and achieved good performance (Hacioglu et al., 2004) (KyungMi Park and Rim, 2004). We used YamCha (Yet Another Multipurpose Chunk Annotator) 1 (Kudo and Matsumoto, 2001), which is a general purpose SVM-based chunker. We also used TinySVM 2 as a package for SVMs. 3 System Description 3.1 Data Representation We changed the representation of original data according to Hacioglu et al. (Hacioglu et al., 2004) in our system. 1 2 http://chasen.org/˜ taku/software/yamcha/ http://chasen.org/˜ taku/software/TinySVM/ 197 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 197–200, Ann Arbor, June 2005. 2005 Association for Computational Linguistics Bracketed representation of roles was converted into IOB2 representation (Ramhsaw"
W05-0629,W95-0107,0,0.0945868,", 2001), which is a general purpose SVM-based chunker. We also used TinySVM 2 as a package for SVMs. 3 System Description 3.1 Data Representation We changed the representation of original data according to Hacioglu et al. (Hacioglu et al., 2004) in our system. 1 2 http://chasen.org/˜ taku/software/yamcha/ http://chasen.org/˜ taku/software/TinySVM/ 197 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 197–200, Ann Arbor, June 2005. 2005 Association for Computational Linguistics Bracketed representation of roles was converted into IOB2 representation (Ramhsaw and Marcus, 1995) (Sang and Veenstra, 1999). Word-by-word was changed to the phrase-byphrase method (Hacioglu et al., 2004). Word tokens were collapsed into base phrase (BP) tokens. The BP headwords were rightmost words. Verb phrases were not collapsed because some included more the one predicate. 3.2 Feature Coding We prepared the training and development set by using files corresponding to: words, predicated partial parsing (part-of-speech, base chunks), predicate full parsing trees (Charniak models), and named entities. We will describe feature extraction according to Fig. 1. Figure 1 shows an example of an"
W05-0629,E99-1023,0,0.0223699,"purpose SVM-based chunker. We also used TinySVM 2 as a package for SVMs. 3 System Description 3.1 Data Representation We changed the representation of original data according to Hacioglu et al. (Hacioglu et al., 2004) in our system. 1 2 http://chasen.org/˜ taku/software/yamcha/ http://chasen.org/˜ taku/software/TinySVM/ 197 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 197–200, Ann Arbor, June 2005. 2005 Association for Computational Linguistics Bracketed representation of roles was converted into IOB2 representation (Ramhsaw and Marcus, 1995) (Sang and Veenstra, 1999). Word-by-word was changed to the phrase-byphrase method (Hacioglu et al., 2004). Word tokens were collapsed into base phrase (BP) tokens. The BP headwords were rightmost words. Verb phrases were not collapsed because some included more the one predicate. 3.2 Feature Coding We prepared the training and development set by using files corresponding to: words, predicated partial parsing (part-of-speech, base chunks), predicate full parsing trees (Charniak models), and named entities. We will describe feature extraction according to Fig. 1. Figure 1 shows an example of an annotated sentence. 1st W"
W05-0629,W03-3023,0,0.0467046,"rranged as follows. Section 2 describes the SVMs. Our system is described SecSupport Vector Machines SVMs are one of the binary classifiers based on the maximum margin strategy introduced by Vapnik (Vapnik, 1995). This algorithm has achieved good performance in many classification tasks, e.g. named entity recognition and document classification. There are some advantages to SVMs in that (i) they have high generalization performance independent of the dimensions of the feature vectors and (ii) learning with a combination of multiple features is possible by using the polynomial kernel function (Yamada and Matsumoto, 2003). SVMs were used in the CoNLL-2004 shred task and achieved good performance (Hacioglu et al., 2004) (KyungMi Park and Rim, 2004). We used YamCha (Yet Another Multipurpose Chunk Annotator) 1 (Kudo and Matsumoto, 2001), which is a general purpose SVM-based chunker. We also used TinySVM 2 as a package for SVMs. 3 System Description 3.1 Data Representation We changed the representation of original data according to Hacioglu et al. (Hacioglu et al., 2004) in our system. 1 2 http://chasen.org/˜ taku/software/yamcha/ http://chasen.org/˜ taku/software/TinySVM/ 197 Proceedings of the 9th Conference on"
W06-0201,I05-2043,1,0.873246,"Missing"
W14-4506,H05-1073,0,0.010247,"is precision rate is much higher than the 0.07 rate we obtain with the 34 baseline method. Some may think that the 0.40 precision rate obtained with machine learning is low. However, since the task of extracting impressive sentences is a very difficult one, and since the rate is much higher than the baseline method rate, we can say that the machine learning results are at least adequate. 5 Related studies Many methods have been reported that estimated the orientation (positive or negative contents) or the emotion of a sentence (Turney and Littman, 2003; Pang and Lee, 2008; Kim and Hovy, 2004; Alm et al., 2005; Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2008; Inkpen et al., 2009; Neviarouskaya et al., 2009). However, the studies did not address the task of collecting and analyzing impressive sentences to support the generation of such sentences. There have been studies that addressed the task of automatically evaluating sentences to support sentence generation (Bangalore and Whittaker, 2000; Mutton and Dale, 2007). However, the studies did not address the task of generating impressive sentences. In our study, we used machine learning to extract impressive sentences. There have been other"
W14-4506,W00-1401,0,0.0490155,"Missing"
W14-4506,C02-1054,0,0.0167653,"ive and 406 negative examples obtained as described in Section 2.1 are used as supervised data. 2. We use the supervised data to conduct machine learning. The machine learning is used to judge whether 10,000 sentences newly obtained from Web documents are positive or negative. We manually check sentences judged to be positive and construct new positive and negative examples. We add the new examples to the supervised data. 3. We repeat the above step 2 procedure ten times. We use a support vector machine (SVM) for machine learning (Cristianini and Shawe-Taylor, 2000; Kudoh and Matsumoto, 2000; Isozaki and Kazawa, 2002; Murata et al., 2002; Takeuchi and Collier, 2003; Mitsumori et al., 2005; Chen and Wen, 2006; Murata et al., 2011).3 We use unigram words whose parts of speech (POSs) are nouns, verbs, adjectives, adjectival verbs, adnominals, and interjections as features used in machine learning. The judgment criteria for positive and negative examples in this section are as follows: Sentences for which a judge can spontaneously produce certain comments are judged to be positive examples. Sentences that describe objective facts only are judged to be negative examples. We repeated the procedure ten times. In"
W14-4506,kawahara-kurohashi-2006-case,0,0.0341153,"ive. We performed the above procedure and obtained 1,018 positive examples and 406 negative examples. 2.2 Using supervised machine learning to collect impressive sentences We conduct machine learning using the positive and negative examples obtained as described in Section 2.1 as supervised data. We use sentences in Web documents as inputs for machine learning. Machine learning is used to judge whether the sentences are impressive. In this way we collect impressive sentences from Web documents. The specific procedure is as follows: 1 2 We used the Web documents that Kawahara et al. collected (Kawahara and Kurohashi, 2006). Some famous sentences are obtained from http://www.meigensyu.com/. 32 Table 1: Words with high appearance probabilities in positive examples Word koufuku (happiness) yujou (friendliness) seishun (youth) kanashimi (sadness) sonzai (existence) ... wareware (we) fukou (unhappiness) aisa (love) ren’ai (love) koi (love) kodoku (loneliness) konoyo (this world) aishi (love) Ratio of positive 1.00 1.00 1.00 1.00 1.00 ... 0.97 0.97 0.96 0.96 0.95 0.94 0.94 0.94 Freq. of positive 83 29 18 12 10 ... 37 32 23 44 122 32 16 31 Freq. of negative 0 0 0 0 0 ... 1 1 1 2 7 2 1 2 Word aisuru (love) arayuru (eve"
W14-4506,C04-1200,0,0.0338148,"supervised data. This precision rate is much higher than the 0.07 rate we obtain with the 34 baseline method. Some may think that the 0.40 precision rate obtained with machine learning is low. However, since the task of extracting impressive sentences is a very difficult one, and since the rate is much higher than the baseline method rate, we can say that the machine learning results are at least adequate. 5 Related studies Many methods have been reported that estimated the orientation (positive or negative contents) or the emotion of a sentence (Turney and Littman, 2003; Pang and Lee, 2008; Kim and Hovy, 2004; Alm et al., 2005; Aman and Szpakowicz, 2007; Strapparava and Mihalcea, 2008; Inkpen et al., 2009; Neviarouskaya et al., 2009). However, the studies did not address the task of collecting and analyzing impressive sentences to support the generation of such sentences. There have been studies that addressed the task of automatically evaluating sentences to support sentence generation (Bangalore and Whittaker, 2000; Mutton and Dale, 2007). However, the studies did not address the task of generating impressive sentences. In our study, we used machine learning to extract impressive sentences. Ther"
W14-4506,W00-0730,0,0.104675,"... 4 2 1. The 1,018 positive and 406 negative examples obtained as described in Section 2.1 are used as supervised data. 2. We use the supervised data to conduct machine learning. The machine learning is used to judge whether 10,000 sentences newly obtained from Web documents are positive or negative. We manually check sentences judged to be positive and construct new positive and negative examples. We add the new examples to the supervised data. 3. We repeat the above step 2 procedure ten times. We use a support vector machine (SVM) for machine learning (Cristianini and Shawe-Taylor, 2000; Kudoh and Matsumoto, 2000; Isozaki and Kazawa, 2002; Murata et al., 2002; Takeuchi and Collier, 2003; Mitsumori et al., 2005; Chen and Wen, 2006; Murata et al., 2011).3 We use unigram words whose parts of speech (POSs) are nouns, verbs, adjectives, adjectival verbs, adnominals, and interjections as features used in machine learning. The judgment criteria for positive and negative examples in this section are as follows: Sentences for which a judge can spontaneously produce certain comments are judged to be positive examples. Sentences that describe objective facts only are judged to be negative examples. We repeated t"
W14-4506,P07-1044,0,0.0211423,"Missing"
W14-4506,W03-1308,0,0.0251515,"ibed in Section 2.1 are used as supervised data. 2. We use the supervised data to conduct machine learning. The machine learning is used to judge whether 10,000 sentences newly obtained from Web documents are positive or negative. We manually check sentences judged to be positive and construct new positive and negative examples. We add the new examples to the supervised data. 3. We repeat the above step 2 procedure ten times. We use a support vector machine (SVM) for machine learning (Cristianini and Shawe-Taylor, 2000; Kudoh and Matsumoto, 2000; Isozaki and Kazawa, 2002; Murata et al., 2002; Takeuchi and Collier, 2003; Mitsumori et al., 2005; Chen and Wen, 2006; Murata et al., 2011).3 We use unigram words whose parts of speech (POSs) are nouns, verbs, adjectives, adjectival verbs, adnominals, and interjections as features used in machine learning. The judgment criteria for positive and negative examples in this section are as follows: Sentences for which a judge can spontaneously produce certain comments are judged to be positive examples. Sentences that describe objective facts only are judged to be negative examples. We repeated the procedure ten times. In total, 275 positive and 3,006 negative examples"
W98-0605,W97-0109,1,\N,Missing
W99-0205,1993.tmi-1.18,1,0.887395,"an example of the form &quot;Noun X no Noun Y&quot; (Y of X), when noun Y is a 33 Table 5: Case frame of verb &quot;mukad&apos; (go to) Surface case ga-case (subject) n/-case (object) Semantic constraint concrete place a focus is defined as a word which is stressed by the speaker (or the writer). But we cannot detect topics and foci correctly. Therefore we approximated them as shown in Table 3 and Table 4. The distance D is the number of the topics (foci) between the anaphor and a possible antecedent which is a topic (focus). The value P is given by the score of the definiteness in referential property analysis (Murata and Nagao, 1993). This is because it is easier for a definite noun phrase to have an antecedent than for an indefinite noun phrase to have one. The value S is the semantic similarity between a possible antecedent and Noun X of &quot;Noun X no Noun Y.&quot; Semantic similarity is shown by level in Bunrui Goi Hyou (NLRI, 1964). in meaning. ojiisan-wa ooyorokobi-wo-shite ie-ni kaerimashita. (the old man) (in great joy) (house) (returned) [The old man returned home (house) in great joy,] okotta koto-wo hitobito-ni hanashimashita (happened to him) (all things) (everybody) (told) (and told everybody all that had happened to"
W99-0205,J94-2003,0,\N,Missing
W99-0206,P86-1031,0,0.018776,"Missing"
W99-0206,J94-2003,0,\N,Missing
W99-0206,C94-2188,0,\N,Missing
W99-0206,C96-2134,1,\N,Missing
Y04-1032,1993.tmi-1.18,1,0.653491,"freading. (For a detailed description of the method used to extract diﬀerences, see (Murata and Isahara, 2002a; Murata and Isahara, 2002c; Murata, 2002; Murata and Isahara, 2002b).) We then counted the frequencies of the extracted English error patterns. The results are shown in Table 1. φ indicates a void: “φ ⇒” and “⇒ φ” indicate insertion and deletion, respectively. The results showed that most of the errors made by the ﬁrst author related to usage of “the”, with “a” being the next most frequent source of error. Correct usage of articles (“a” and “the”) is very diﬃcult for Japanese people (Murata and Nagao, 1993) and in this case, the author was Japanese. The next most frequent source of error was tenses such as “is” ⇒ “was” and “are” ⇒ “were”. Interestingly, the errors relating to articles were symmetrical, while those relating to tenses were not. The frequency of “is” ⇒ “was” was high, but that of “was” ⇒ “is” was low. The reason for this lack of symmetry may be that “is” and “are” are default forms and they are often used. The next most common errors related to prepositions such as φ ⇒ “of”, “of” ⇒ “for”, and “in” ⇒ “of”. We also noticed errors relating to “which” ⇒ “that” and “having” ⇒ “with”. We"
Y04-1032,P98-2174,0,0.0121267,"“recorededed”, which is a typo, is also given “[Caution!]”. In future work, we would like to develop a system that stores all the sentences that a user has written and read and show him/her which word he/she ﬁrst encountered when a new sentence is presented. (We have already investigated a system for highlighting expressions that appear ﬁrst in documents (Murata and Isahara, 2002c).) We also consider that it may be interesting to highlight the words that appear ﬁrst in English language school textbooks. Although there are systems that provide translations or meaning glosses to assist readers (Poznanski et al., 1998), the ideas of user-dependent processing and highlighting expressions that appear ﬁrst are novel and in future, we would like to further examine these concepts. 5 Conclusion We used automatic paraphrasing techniques based on natural language processing to develop three systems for helping English learners and beginners. They included a system for extracting personal error patterns in the user’s English usage; a system for transforming English sentences containing the letters “l” and “r”, which Japanese people have trouble pronouncing, into sentences containing fewer instances of these letters;"
Y04-1032,C98-2169,0,\N,Missing
Y04-1032,murata-isahara-2002-automatic,1,\N,Missing
Y05-1014,W00-1303,0,0.012695,"nai (do not), shinakatta (did not). • Feature Set 2 This set consisted of all of the morphemes in each of the input sentences, e.g., kyou (today), watashi (I), wa (topic-marker particle), hashiru (run). 1 This corpus was made in our previous studies (Murata et al., 2002b; Murata et al., 2005). We found that support vector machines were more accurate than other kinds of machine learning methods such as the decision-list method and maximum entropy method (Murata et al., 2001). In addition, the use of support vector machines has been found to be effective in many studies (Taira and Haruno, 2001; Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Murata et al., 2002a). Therefore, we used support vector machines in our translation systems. The detailed parameter settings we used are described in our previous paper (Murata et al., 2001). 2 Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. Table 1: Occurrence rates of correct categories for tense, aspect, and modality. Category Occurrence rate present 0.65 (516/800) past 0.45 (356/800) prefect 0.32 (259/800) “can” 0.11 (90/800) “will” 0.11 (87/800) progressive 0.10 (82/800) imperative 0.09 (74/800) “should” 0.07 ("
Y05-1014,W01-1415,1,0.931958,"(one category) 5. participial constructions (one category) 6. verb ellipses (one category) 7. interjections or greeting sentences (one category) We used 800 sentences extracted from a corpus1 containing 40,198 sentences for the evaluation. We calculated the accuracy rates of six translation systems on the market and our new translation systems and examined the error patterns in the results. The six translation systems were the latest of leading translation system companies as of October 2003. Our systems for translating tense, aspect, and modality are based on support vector machines (SVMs) (Murata et al., 2001).2 They translate Japanese tense, aspect, and modality expressions into English. They detect categories of tense, aspect, and modality previously defined from English expressions. The categories are detected as a categorization problem by SVMs (Cristianini and Shawe-Taylor, 2000; Kudoh, 2000). However, an SVM can handle only two categories at a time. Therefore, we used a pairwise method in addition to the SVM to handle more than two categories (Moreira and Mayoraz, 1998). As training sentences, we used the sentences remaining after eliminating the 800 evaluation sentences from the 40,198-sente"
Y05-1014,2002.tmi-papers.14,1,0.747648,"Missing"
Y06-1039,P97-1023,0,0.237241,"Missing"
Y06-1039,C00-1044,0,0.0919864,"Missing"
Y10-1073,ishikawa-etal-2010-detection,1,0.684361,"However, most of previous studies selected their learning examples carefully and, consequently, limited the scope of target users. For example, if target users are set to submitters in a certain category of a community site, learning examples are generally selected from their messages submitted to the category, not other categories. Actually, in ? This research has been supported partly by the Grant-in-Aid for Scientific Research under Grant No.20500106 and 22650021. Copyright 2010 by Naoki Ishikawa, Ryo Nishimura, Yasuhiko Watanabe, Masaki Murata, and Yoshihiro Okada 637 638 Student Papers (Ishikawa et al., 2010), we developed learning examples by using target user’s messages submitted to the target category. However, the scope of target users was limited because there is a limited number of users who submitted more messages to the target category than the minimum needed to develop their learning examples. In order to extend the scope of target users, it is necessary to relax the criteria for selecting learning examples. The point is extension’s effects on the accuracy of user identification. In other words, when we use learning examples which were not used in cases of previous studies and extend the"
Y10-1075,I08-1007,1,0.872999,"Missing"
Y10-1075,P94-1038,0,0.256331,"Missing"
Y10-1075,P08-1047,1,0.887762,"Missing"
Y10-1075,D08-1047,0,0.0534269,"Missing"
Y10-1075,I08-1025,0,0.0329099,"Missing"
Y10-1075,C02-1119,0,0.053663,"Missing"
Y10-1075,J01-2002,0,0.052682,"Missing"
Y10-1075,P97-1017,0,\N,Missing
Y10-1080,P00-1041,0,0.0423245,"ncluding plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech other than verbs by using co-occurring words. We can also perform paraphrasing of various kinds of information by using co-occurring words. Banko et al. generated newspaper headlines by using statistical machine translation (Banko et al., 2000). In this study, we handled only the case in which the output summary is one word for simplicity; however, we aim to generate a summary comprising sentences as the output in the future. The case where the output summary comprises two words or sentences can be handled by using an extended version of this method. In this paper, we have only described our idea for handling twoword summaries in Section 4. The summarization process that outputs one-word summaries can also be used to categorize documents, because that one word is representative of the main content and theme of the document. We handl"
Y10-1080,W97-0703,0,0.097702,"of elements in our method was beneficial. Our method obtained 0.75 as the ratio where the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system jud"
Y10-1080,N04-1015,0,0.0356035,"ument include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as much as possible is a suitable summary: (i) Th"
Y10-1080,1993.mtsummit-1.10,0,0.63602,"rds, “strict,” and “lenient.” In “strict,” we evaluated only correct candidates as the correct summary. In “lenient” evaluation, we evaluated candidates similar to a correct candidate as the correct summary. We obtained a high Kappa value of 0.78 and 0.75 for “strict” and “lenient,” respectively, when we evaluated the results using two test subjects in a preliminary experiment (Landis and Koch, 1977). 3.2 Results Evaluated results are shown in Tables 1 and 2. Methods 1 to 4 are described in Section 2.6. Method 5 is a comparison method that uses definition sentences in the EDR word dictionary (EDR, 1993). In Method 5, we selected a candidate word whose definition sentence contained a noun that was also present in the input document; we used these words in the definition sentence of a word x as the related words of the word x. The subsequent working is the same as that of our proposed method. Since Method 5 used a definition sentence, it is related to Kondo et al.’s studies (Kondo and Okumura, 1997). However, Kondo et al. did not propose the method of selecting a candidate from plural candidates. We used our candidate scores that were obtained on the basis of criteria (i) and (ii) for selectin"
Y10-1080,A00-1043,0,0.0246578,"e the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as mu"
Y10-1080,P08-4007,0,0.023681,"by the candidate. In this study, we use co-occurring words as word-association knowledge. By using cooccurring words, a summary can be generated containing words that do not appear in the original document (summarization by paraphrasing). In this aspect, our method is completely different Copyright 2010 by Kazuki Takigawa, Masaki Murata, Masaaki Tsuchida, Stijn De Saeger, Kazuhide Yamamoto, and Kentaro Torisawa 693 694 Student Papers from the existing methods that extract some sentences or words from the original document for generating its summary (Knight and Marcu, 2002; Lin and Hovy, 2003; Kang et al., 2008). In terms of related studies pertaining to summarization by paraphrasing, Kondo et al. proposed a method of paraphrasing plural verbs in the original document into a verb having superordinate concepts including plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech other than verbs by us"
Y10-1080,W00-1100,0,0.390754,"Missing"
Y10-1080,W03-0510,0,0.0335233,"ent is not conveyed by the candidate. In this study, we use co-occurring words as word-association knowledge. By using cooccurring words, a summary can be generated containing words that do not appear in the original document (summarization by paraphrasing). In this aspect, our method is completely different Copyright 2010 by Kazuki Takigawa, Masaki Murata, Masaaki Tsuchida, Stijn De Saeger, Kazuhide Yamamoto, and Kentaro Torisawa 693 694 Student Papers from the existing methods that extract some sentences or words from the original document for generating its summary (Knight and Marcu, 2002; Lin and Hovy, 2003; Kang et al., 2008). In terms of related studies pertaining to summarization by paraphrasing, Kondo et al. proposed a method of paraphrasing plural verbs in the original document into a verb having superordinate concepts including plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech ot"
Y10-1080,H01-1056,0,0.0238079,"mmaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as much as possible is a"
Y10-1080,W00-1108,0,\N,Missing
Y10-1080,I05-2047,0,\N,Missing
Y11-1062,J93-1006,0,0.383731,"Missing"
Y11-1062,W96-0107,0,0.101135,"s in phrase-level support is more complicated. See Ma et al. (2009) for details. The non-English examples are also rendered in (1) alphabetized form, (2) English glosses, and (3) English translation (or grammar explanations if there are no proper translations) complying with the PACLIC 25 Paper Submission Guidelines. The English translations, however, will be skipped if they appear in the pairs of the parallel translation expressions. 578 3 Extraction of Parallel Translation Expressions 3.1 Related work The earlier studies most closely related to our work were done by Sainoo et al. (2003) and Kitamura and Matsumoto (1996; 2005). The principal techniques used in those studies were used as our baseline in this study and are described in Sec. 3.2.1. In the study of Sainoo et al., the research object was limited to a small corpus of 8,500 Japanese-English parallel sentences, which were examples from a number of bilingual dictionaries, and only a couple dozen parallel translation expressions were extracted. The first study of Kitamura and Matsumoto (1996), on the other hand, used Japanese-English parallel corpora of three distinct domains: a computer manual, a scientific journal, and business correspondence letter"
Y11-1062,ma-etal-2008-selection,1,0.912621,"Keywords: parallel translation expression, extraction, rule, lexical information, Englishwriting support 1 Introduction Non-native speakers often have problems explaining ideas or presenting achievements in written English, in part because of the large amount of time needed to determine which possible translation of an expression most suits the context. Trying to develop English-writing support tools that will enable non-native speakers to produce nearly perfect English sentences for mixed English-Japanese sentences–in which expressions without know translations are simply written in Japanese–Ma et al. (2008; 2009) have developed systems that can provide support at the word and phrase levels. That is, the given Japanese parts in the mixed English-Japanese sentences can be words or phrases. For phrase-level support in those systems, the Japanese parts are extracted from the mixed sentences and segmented into words, the candidate English equivalents of the segmented Japanese words are identified by searching through a Japanese-English dictionary, and the best equivalents of the Japanese phrases are selected from the combinations of the candidate translations of the single words. This kind of suppor"
Y11-1062,C02-1020,0,0.125433,"ore not easy to scale up. In Kimura and Matsumoto’s second study (2005) they confirmed the interim results manually, used more linguistic resources than they did in their earlier study, and introduced a method for halving the extraction time by dividing a corpus into quarters. Their method is still hard to scale up, however, because the repetition processing is costly and the results can be manually confirmed only in small-scale extraction. Among other interesting studies on the extraction of Japanese-English parallel translation expressions are those done by Yamamoto and Matsumoto (2000) and Sato and Saito (2002-1; 20022). The methods proposed in those studies, however, like those proposed in the earlier studies mentioned above, have low precisions and recalls and also cannot be scaled up. These methods are thus not practical for use in the extraction of large-scale parallel translation expressions. In a study by Sato and Saito (2002-1), for example, even though the test corpus used for extraction consisted of only about 3,000 parallel sentences, whereas the training corpus consisted of 30,287 parallel sentences, more than ten times the number in the test corpus, the extraction precision was still le"
Y11-1062,W04-2208,1,0.800665,"atching list”. In this way, “matching list” has six words and and the “nonmatching list” has none. Since the “matching list” has more words than the “non-matching list”, the parallel translation expression passes. Furthermore, if this parallel translation expression and the other passing parallel translation expressions are in many-to-many relationships, the one with the largest number of words in the “matching list” is selected. 4 Experiments 4.1 Experimental setup 4.1.1 Data and tools The aligned Japanese-English parallel corpora that we used in the experiments consisted of the NICT corpus (Uchimoto et al., 2004), the JENNAD corpus, and the aligned Reuters corpus (Utiyama and Isahara, 2003). The NICT corpus is composed of sentence-aligned Mainichi newspaper articles (Japanese) and their translations done by professional translators and has about 40,000 pairs of parallel sentences. The JENNAD corpus is composed of automatically sentence-aligned Yomiuri newspaper articles (Japanese) and Daily Yomiuri newspaper articles (English) and has about 180,000 pairs of parallel sentences. The aligned Reuters corpus is composed of automatically sentence-aligned Reuters Japanese and English news articles and has ab"
Y11-1062,P03-1010,0,0.0246159,"atching list” has none. Since the “matching list” has more words than the “non-matching list”, the parallel translation expression passes. Furthermore, if this parallel translation expression and the other passing parallel translation expressions are in many-to-many relationships, the one with the largest number of words in the “matching list” is selected. 4 Experiments 4.1 Experimental setup 4.1.1 Data and tools The aligned Japanese-English parallel corpora that we used in the experiments consisted of the NICT corpus (Uchimoto et al., 2004), the JENNAD corpus, and the aligned Reuters corpus (Utiyama and Isahara, 2003). The NICT corpus is composed of sentence-aligned Mainichi newspaper articles (Japanese) and their translations done by professional translators and has about 40,000 pairs of parallel sentences. The JENNAD corpus is composed of automatically sentence-aligned Yomiuri newspaper articles (Japanese) and Daily Yomiuri newspaper articles (English) and has about 180,000 pairs of parallel sentences. The aligned Reuters corpus is composed of automatically sentence-aligned Reuters Japanese and English news articles and has about 70,000 pairs of parallel sentences. After the overlapping sentences were ex"
Y11-1062,C00-2135,0,0.221094,"uming and the extraction is therefore not easy to scale up. In Kimura and Matsumoto’s second study (2005) they confirmed the interim results manually, used more linguistic resources than they did in their earlier study, and introduced a method for halving the extraction time by dividing a corpus into quarters. Their method is still hard to scale up, however, because the repetition processing is costly and the results can be manually confirmed only in small-scale extraction. Among other interesting studies on the extraction of Japanese-English parallel translation expressions are those done by Yamamoto and Matsumoto (2000) and Sato and Saito (2002-1; 20022). The methods proposed in those studies, however, like those proposed in the earlier studies mentioned above, have low precisions and recalls and also cannot be scaled up. These methods are thus not practical for use in the extraction of large-scale parallel translation expressions. In a study by Sato and Saito (2002-1), for example, even though the test corpus used for extraction consisted of only about 3,000 parallel sentences, whereas the training corpus consisted of 30,287 parallel sentences, more than ten times the number in the test corpus, the extracti"
Y14-1040,C12-1018,0,0.138398,"Missing"
Y14-1040,D13-1137,0,0.171736,"Missing"
Y14-1040,P13-1088,0,0.0589727,"Missing"
Y14-1040,D13-1176,0,0.0863825,"Missing"
Y14-1040,W13-3512,0,0.0678898,"Missing"
Y14-1040,D13-1170,0,0.01092,"Missing"
Y14-1040,D13-1144,0,0.154897,"Missing"
Y14-1040,D13-1141,0,0.0508547,"Missing"
Y14-1040,D13-1106,0,\N,Missing
Y14-1040,P13-1078,0,\N,Missing
Y16-3001,C12-1018,0,0.0443621,"Missing"
Y16-3001,D13-1137,0,0.0258921,"Missing"
Y16-3001,P13-1088,0,0.0497777,"Missing"
Y16-3001,D13-1176,0,0.0360953,"s.mynavi.jp/news/2010/07/05/028/) In recent years, on the other hand, deep learning/neural network techniques have attracted a great deal of attention in various ﬁelds and have been successfully applied not only in speech recognition (Li et al., 2013) and image recognition (Krizhevsky et al., 2012) tasks but also in NLP tasks including morphology & syntax (Billingsley and Curran, 2012; Hermann and Blunsom, 2013; Luong et al., 2013; Socher et al., 2013a), semantics (Hashimoto et al., 2013; Srivastava et al., 2013; Tsubaki et al., 2013), machine translation (Auli et al., 2013; Liu et al., 2013; Kalchbrenner and Blunsom, 2013; Zou et al., 2013), text classiﬁcation (Glorot et al., 2011), information retrieval (Huang et al., 2013; Salakhutdinov and Hinton, 2009), and others (Seide et al., 2011; Socher et al., 2011; Socher et al., 2013b). Moreover, a uniﬁed neural network architecture and learning algorithm has also been proposed that can be applied to various NLP tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Collobert et al., 2011). How30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 20"
Y16-3001,W13-3512,0,0.0603939,"Missing"
Y16-3001,Y14-1040,1,0.0925179,"October 28-30, 2016 309 ever, there have been no studies on applying deep learning to information retrieval support tasks. It is therefore necessary to conﬁrm whether deep learning is more effective than other conventional machine learning methods in this task. Two objectives were cited above. One was to develop an effective method for predicting suitable retrieval terms and the other was to determine whether deep learning is more effective than other conventional machine learning methods, i.e., multi-layer perceptron (MLP) and support vector machines (SVM), in such NLP tasks. On this basis, Ma et al. (2014) proposed a method to predict retrieval terms in computer-related ﬁelds using machine learning methods with deep belief networks (DBN) (Hinton et al., 2006; Lee et al., 2009; Bengio et al., 2007; Bengio, 2009; Bengio et al., 2013). In small-scale experiments they showed that using DBN resulted in higher prediction precision than using either a multi-layer perceptron (MLP) or support vector machines (SVM). To evaluate their proposed method more reliably, the ﬁrst thing we must do is scale up the experiments. In general, it is not easy to obtain large training data, particularly labeled data for"
Y16-3001,D13-1170,0,0.0054021,"Missing"
Y16-3001,D13-1144,0,0.0213488,"lty deciding on the proper retrieval terms. (http://www.garbagenews.net/archives/1466626.html) (http://news.mynavi.jp/news/2010/07/05/028/) In recent years, on the other hand, deep learning/neural network techniques have attracted a great deal of attention in various ﬁelds and have been successfully applied not only in speech recognition (Li et al., 2013) and image recognition (Krizhevsky et al., 2012) tasks but also in NLP tasks including morphology & syntax (Billingsley and Curran, 2012; Hermann and Blunsom, 2013; Luong et al., 2013; Socher et al., 2013a), semantics (Hashimoto et al., 2013; Srivastava et al., 2013; Tsubaki et al., 2013), machine translation (Auli et al., 2013; Liu et al., 2013; Kalchbrenner and Blunsom, 2013; Zou et al., 2013), text classiﬁcation (Glorot et al., 2011), information retrieval (Huang et al., 2013; Salakhutdinov and Hinton, 2009), and others (Seide et al., 2011; Socher et al., 2011; Socher et al., 2013b). Moreover, a uniﬁed neural network architecture and learning algorithm has also been proposed that can be applied to various NLP tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Collobert et al., 2011). How30th Pacific"
Y16-3001,D13-1141,0,0.0176806,") In recent years, on the other hand, deep learning/neural network techniques have attracted a great deal of attention in various ﬁelds and have been successfully applied not only in speech recognition (Li et al., 2013) and image recognition (Krizhevsky et al., 2012) tasks but also in NLP tasks including morphology & syntax (Billingsley and Curran, 2012; Hermann and Blunsom, 2013; Luong et al., 2013; Socher et al., 2013a), semantics (Hashimoto et al., 2013; Srivastava et al., 2013; Tsubaki et al., 2013), machine translation (Auli et al., 2013; Liu et al., 2013; Kalchbrenner and Blunsom, 2013; Zou et al., 2013), text classiﬁcation (Glorot et al., 2011), information retrieval (Huang et al., 2013; Salakhutdinov and Hinton, 2009), and others (Seide et al., 2011; Socher et al., 2011; Socher et al., 2013b). Moreover, a uniﬁed neural network architecture and learning algorithm has also been proposed that can be applied to various NLP tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Collobert et al., 2011). How30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 309 ever, there"
Y16-3001,D13-1106,0,\N,Missing
Y16-3001,P13-1078,0,\N,Missing
