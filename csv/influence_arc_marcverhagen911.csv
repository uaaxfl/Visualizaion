2020.lrec-1.893,P10-4005,0,0.0417135,"chitecture for Text Engineering (GATE) (Cunningham et al., 2013) have been served as well-established and popular tool-chaining platforms for researchers and NLP developers. Although GATE focuses primarily on textual data, UIMA provides an extremely general model of type systems and annotations that can be applied to multimedia source data. However, there is a steep learning curve supporting UIMA’s generality, due in large part to its tight binding to XML syntax and the Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) have been developed that provide user-friendly web interfaces for chaining NLP tools. These platforms not only offer tool repositories containing state-of-the-art NLP tools for annotating textual data at a variety of linguistic levels (e.g., CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), UDPipe (Straka and Strakov´a, 2017)), but also provide open source software development kits (SDKs) for tool developers in order to promote adoption. The LAPPS Grid and WebLicht both provide for chaining tools from different developers, which use a variety of I/O formats, by virtue of underlying dat"
2020.lrec-1.893,ide-etal-2014-language,1,0.813,"et al., 2009) and the General Architecture for Text Engineering (GATE) (Cunningham et al., 2013) have been served as well-established and popular tool-chaining platforms for researchers and NLP developers. Although GATE focuses primarily on textual data, UIMA provides an extremely general model of type systems and annotations that can be applied to multimedia source data. However, there is a steep learning curve supporting UIMA’s generality, due in large part to its tight binding to XML syntax and the Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) have been developed that provide user-friendly web interfaces for chaining NLP tools. These platforms not only offer tool repositories containing state-of-the-art NLP tools for annotating textual data at a variety of linguistic levels (e.g., CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), UDPipe (Straka and Strakov´a, 2017)), but also provide open source software development kits (SDKs) for tool developers in order to promote adoption. The LAPPS Grid and WebLicht both provide for chaining tools from different developers, which use a variety of I/O"
2020.lrec-1.893,W12-2425,0,0.0198493,"Brat. among those tools. The LAPPS Grid uses LIF (Verhagen et al., 2015), a JSON-LD serialization, as its interchange format; while WebLicht uses its XML-based Text Corpus Format (TCF) (Heid et al., 2010). Additionally, the LAPPS Grid defines a linked data vocabulary that ensures semantic interoperability (Ide et al., 2015). Beyond in-platform interoperability, the LAPPS Grid has established multi-platform interoperability between LAPPS Grid and two CLARIN platforms (Hinrichs et al., 2018) as well as several other platforms (e.g., DKPro (Eckart de Castilho and Gurevych, 2014), PubAnnotation (Kim and Wang, 2012), and INCEpTION (Klie et al., 2018)). Figure 1 shows a visualization of named entity annotations in the LAPPS Grid, using Brat (Stenetorp et al., 2012). All annotations are represented in the LIF format and linked either to offsets within read-only primary data or to other annotation layers. Within the LIF document containing the annotations, each annotation references a name (e.g., PERSON), possibly coupled with additional attributes, that links to a full definition in the LAPPS Grid Web Service Exchange Vocabulary (WSEV)5 . Alternative names used within specific tools are mapped to the WSEV"
2020.lrec-1.893,C18-2002,0,0.0631695,"Missing"
2020.lrec-1.893,P14-5010,0,0.00570971,"stems and annotations that can be applied to multimedia source data. However, there is a steep learning curve supporting UIMA’s generality, due in large part to its tight binding to XML syntax and the Java programming language. More recently, web-based workflow engines such as the LAPPS Grid (Ide et al., 2014) and WebLicht (Hinrichs et al., 2010) have been developed that provide user-friendly web interfaces for chaining NLP tools. These platforms not only offer tool repositories containing state-of-the-art NLP tools for annotating textual data at a variety of linguistic levels (e.g., CoreNLP (Manning et al., 2014), OpenNLP (OpenNLP, 2017), UDPipe (Straka and Strakov´a, 2017)), but also provide open source software development kits (SDKs) for tool developers in order to promote adoption. The LAPPS Grid and WebLicht both provide for chaining tools from different developers, which use a variety of I/O formats, by virtue of underlying data interchange formats that impose a common I/O format 7230 Figure 1: Named entity annotations in the LAPPS Grid, generated by LINDAT/CLARIN’s NameTag 4 tool and visualized with Brat. among those tools. The LAPPS Grid uses LIF (Verhagen et al., 2015), a JSON-LD serializatio"
2020.lrec-1.893,W19-2512,1,0.777297,"resent the collections in their client software (e.g., in what order, in which orientation, on what zoom level, etc). Unfortunately, however, the IIIF does not provide detailed specifications for the semantics of the content of the textual annotations. In principle, one could design an independent, adequate data model for textual annotation that can be carried out on IIIF, since the IIF specification is built upon the Open Annotation model and linked-data conformity, but this would involve significant additional effort. MMIF is an interoperable representation format that is used in the CLAMS (Rim et al., 2019) project. CLAMS is a platform of computational analysis tools designed for digital libraries and archives who have to deal with not just textual data, but also audiovisual time-based data. To handle the complexity of multimodal content and semantics of the audiovisual data sources, MMIF is specifically designed to enable alignment of annotations on different modes of the primary data sources. 6 7 https://iiif.io/ https://iiif.io/api/presentation/3.0/ Specifically, multimodal annotations in MMIF are first categorized by the anchor type on which the annotation is placed. That is, an annotation c"
2020.lrec-1.893,schmidt-etal-2008-exchange,0,0.0890206,"not only NLP, but also computer vision (CV), and speech technologies processing audio and video data. The machine learning approach to solving problems is data-driven, and most state-of-the-art applications are based on supervised algorithms which rely on large sets of training data. To ensure the high quality of datasets containing rich multimodal annotations, the CL community has had to move beyond text-only annotation practices, and has tried to establish a common format for multimodal annotations, particularly with regard to annotating speech in audio and gestures in video. For example, (Schmidt et al., 2008) outline a diversity of annotation applications and formats, as well as the community effort to develop an interoperable format that carries complicated, layered multimodal annotation. As a result, 7231 Figure 2: A primary data text collection can be created from an image and can then be input to downstream NLP components. UIMA and Component MetaData Infrastructure (CMDI) (Broeder et al., 2012) have been widely adopted frameworks that provide interoperable multimodal information exchange between computational analysis tools, and for metadata repositories for discoverability, respectively. More"
2020.lrec-1.893,E12-2021,0,0.107914,"Missing"
2020.lrec-1.893,K17-3009,0,0.0570048,"Missing"
2021.naacl-srw.11,P17-4009,0,0.0217461,"g system for identifying relationships between biomedical entities. It supports template-based queries for structured search and also provides key sentences as the provenance of identified relations. Fabregat et al. (2018) proposed a knowledge base of human pathways and reactions. It supports visualization of event hierarchy and pathway networks. Linguistic visualization research in general is an emerging field of visual analytics for linguistics (Butt et al., 2020). Previous research in this field covers thematic text cluster analysis (Gold et al., 2015), NER-based document content analysis (El-Assady et al., 2017b), multi-party discourse analysis (El-Assady et al., 2017a) and topic modeling visualization (El-Assady et al., 2018). Butt et al. (2020) propose a web framework that consists of various linguistic visualization techniques. However, existing work in this field focuses on the analysis of corpora of conversational text and transcripts, and does not include approaches for analyzing and visualizing semantics of relations. 5 Conclusion We have proposed semantic visualization, a linguistic visual analytic method of multiple steps involving data extraction, parameter reduction, hierarchical structur"
2021.naacl-srw.11,2020.nlpcovid19-acl.1,0,0.0541219,"Missing"
C08-3012,S07-1025,0,0.0126557,". Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of visualizing the results was used In addition, some components were updated and test suites with unit tests and regression tests were added. In this paper, we focus on the merging of temporal links and the visualization of temporal relations. There has been a fair amount of recent research on extraction of temporal relations, including (Chambers et al., 2007; Lapata and Lascarides, 2006; Bramsen et al., 2006; Bethard and Martin, 2007; Min et al., 2007; Pus¸cas¸u, 2007). However, we are not aware of approaches that integrate temporal relations from various sources in one consistent whole. All TTK components use the TimeML annotation language (Pustejovsky et al., 2003; Pustejovsky et al., 2005). TimeML is an annotation scheme for markup of events, times, and their temporal relations in news articles. The TimeML scheme flags tensed verbs, adjectives, and nominals with EVENT tags with various attributes, including the class of event, tense, grammatical aspect, polarity (negative or positive), and any modal operators which gov"
C08-3012,P07-2044,0,0.270689,"stem described in (Verhagen et al., 2005) in several major aspects: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of visualizing the results was used In addition, some components were updated and test suites with unit tests and regression tests were added. In this paper, we focus on the merging of temporal links and the visualization of temporal relations. There has been a fair amount of recent research on extraction of temporal relations, including (Chambers et al., 2007; Lapata and Lascarides, 2006; Bramsen et al., 2006; Bethard and Martin, 2007; Min et al., 2007; Pus¸cas¸u, 2007). However, we are not aware of approaches that integrate temporal relations from various sources in one consistent whole. All TTK components use the TimeML annotation language (Pustejovsky et al., 2003; Pustejovsky et al., 2005). TimeML is an annotation scheme for markup of events, times, and their temporal relations in news articles. The TimeML scheme flags tensed verbs, adjectives, and nominals with EVENT tags with various attributes, including the class of event, tense, grammatic"
C08-3012,P06-1095,1,0.551204,"agen et al., 2005) in several major aspects: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of visualizing the results was used In addition, some components were updated and test suites with unit tests and regression tests were added. In this paper, we focus on the merging of temporal links and the visualization of temporal relations. There has been a fair amount of recent research on extraction of temporal relations, including (Chambers et al., 2007; Lapata and Lascarides, 2006; Bramsen et al., 2006; Bethard and Martin, 2007; Min et al., 2007; Pus¸cas¸u, 2007). However, we are not aware of approaches that integrate temporal relations from various sources in one consistent whole. All TTK components use the TimeML annotation language (Pustejovsky et al., 2003; Pustejovsky et al., 2005). TimeML is an annotation scheme for markup of events, times, and their temporal relations in news articles. The TimeML scheme flags tensed verbs, adjectives, and nominals with EVENT tags with various attributes, including the class of event, tense, grammatical aspect, polarity (negative"
C08-3012,H05-1088,1,0.79094,"Missing"
C08-3012,sauri-etal-2006-slinket,1,0.913365,"Missing"
C08-3012,P05-3021,1,0.204565,"n 1970?” since a boolean keyword search cannot distinguish between those documents where the event win is actually anchored to the year 1970 versus those that are not. The TARSQI Project (Temporal Awareness and Reasoning Systems for Question Interpretation) focused on enhancing natural language question answering systems so that temporallybased questions about the events and entities in news articles can be addressed. To explicitly mark the needed temporal relations the project delivered a series of tools for extracting time expressions, events, subordination relations and temporal relations (Verhagen et al., 2005; Mani et al., 2006; Saur´ı et al., 2005; Saur´ı et al., 2006a). But although those tools performed reasonably well, they were not integrated in a principled way. This paper describes the TARSQI Toolkit (TTK), which takes the TARSQI components and integrates them into a temporal parsing framework. The toolkit is different from the system described in (Verhagen et al., 2005) in several major aspects: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of vis"
C08-3012,S07-1046,0,0.0166616,"ive Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 3. a new way of visualizing the results was used In addition, some components were updated and test suites with unit tests and regression tests were added. In this paper, we focus on the merging of temporal links and the visualization of temporal relations. There has been a fair amount of recent research on extraction of temporal relations, including (Chambers et al., 2007; Lapata and Lascarides, 2006; Bramsen et al., 2006; Bethard and Martin, 2007; Min et al., 2007; Pus¸cas¸u, 2007). However, we are not aware of approaches that integrate temporal relations from various sources in one consistent whole. All TTK components use the TimeML annotation language (Pustejovsky et al., 2003; Pustejovsky et al., 2005). TimeML is an annotation scheme for markup of events, times, and their temporal relations in news articles. The TimeML scheme flags tensed verbs, adjectives, and nominals with EVENT tags with various attributes, including the class of event, tense, grammatical aspect, polarity (negative or positive), and any modal operators which govern the event bein"
C08-3012,S07-1108,0,0.057593,"Missing"
C94-2202,E89-1001,0,0.0725601,"Missing"
C94-2202,C90-2002,0,0.0695891,"Missing"
C94-2202,P89-1010,0,0.0843844,"Missing"
C94-2202,E89-1018,0,0.0492126,"Missing"
C94-2202,W91-0200,0,0.284956,"Missing"
C94-2202,J92-1001,0,0.0335149,"Missing"
C94-2202,1988.tmi-1.4,0,0.0931122,"Missing"
C94-2202,J96-1005,0,0.0572596,"Missing"
C94-2202,J91-4003,0,0.125435,"Missing"
C94-2202,P90-1032,0,0.0580417,"Missing"
C94-2202,J93-1007,0,0.0611889,"Missing"
C94-2202,J93-2005,0,\N,Missing
E93-1063,E93-1063,1,0.0513221,"Missing"
H05-1088,N04-1008,0,\N,Missing
H05-1088,W03-1206,0,\N,Missing
H05-1088,C92-1027,0,\N,Missing
H05-1088,A97-1051,0,\N,Missing
H05-1088,W02-1033,0,\N,Missing
H05-1088,P05-3021,1,\N,Missing
H05-1088,P02-1006,0,\N,Missing
H05-1088,N04-1020,0,\N,Missing
havasi-etal-2006-bulb,kingsbury-palmer-2002-treebank,0,\N,Missing
havasi-etal-2006-bulb,W04-1908,1,\N,Missing
havasi-etal-2006-bulb,C04-1133,1,\N,Missing
havasi-etal-2006-bulb,meyers-etal-2004-annotating,0,\N,Missing
havasi-etal-2006-bulb,pustejovsky-etal-2006-towards,1,\N,Missing
ide-etal-2014-language,windhouwer-2012-relcat,0,\N,Missing
ide-etal-2014-language,cieri-etal-2014-new,1,\N,Missing
ide-etal-2014-language,piperidis-2012-meta,0,\N,Missing
ide-etal-2014-language,cassidy-etal-2014-alveo,0,\N,Missing
ide-etal-2014-language,J08-3010,0,\N,Missing
ide-etal-2014-language,W14-5211,1,\N,Missing
ide-etal-2014-language,P13-1166,0,\N,Missing
ide-etal-2014-language,W14-5204,1,\N,Missing
L16-1073,ide-etal-2014-language,1,0.876291,"erability, Workflow management 1. 2 Overview 1 The NSF-SI -funded LAPPS Grid project is a collaborative effort among Brandeis University, Vassar College, Carnegie-Mellon University (CMU), and the Linguistic Data Consortium (LDC) at the University of Pennsylvania, which has developed an open, web-based infrastructure through which massive and distributed resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, evaluated, disseminated and consumed by researchers, developers, and students across a wide variety of disciplines (Ide et al., 2014). The LAPPS Grid is part of a larger multiway international collaboration including key individuals and projects from the U.S., Europe, Australia, and Asia involved with language resource development and distribution and standards-making, who are creating the “The Federated Grid of Language Services” (FGLS) federation (Ishida et al., 2014), a multi-lingual, international network of web service grids and providers. We have also recently entered into a formal partnership with WebLicht/T¨ubingen and LINDAT/CLARIN (Prague) to create a “trust network” among our sites in order to provide mutual acce"
L16-1073,P13-1166,0,0.0680149,"Missing"
L16-1073,W09-3034,1,0.6995,"ebLicht/T¨ubingen and LINDAT/CLARIN (Prague) to create a “trust network” among our sites in order to provide mutual access to all from any one of the three portals. The key to the success of these partnerships is the interoperability among tools and services that is accomplished via the service-oriented architecture and the development of common vocabularies and multi-way mappings that has involved key researchers from around the world for over a decade2 . 1 http://www.lappsgrid.org E.g., the NSF-funded Sustainable Interoperability for Language Technology (SILT) project (NSF-INTEROP 0753069) (Ide et al., 2009), the EU-funded Fostering Language Resources Network (FLaReNet) project (Calzolari et al., 2009), the International Standards Organization (ISO) committee for Language Resource Management (ISO TC37 SC4), and parallel efforts in Asia and Australia, together with the LAPPS project and international col2 The LAPPS Grid currently includes a wide range of NLP component web services and provides facilities for service discovery, service composition (including automatic format conversion between tools where necessary), performance evaluation (via provision of component-level measures for standard eva"
L18-1206,W06-2920,0,0.0607984,"Missing"
L18-1206,W14-5211,0,0.072335,"Missing"
L18-1206,hinrichs-krauwer-2014-clarin,1,0.875661,"Missing"
L18-1206,L16-1262,1,0.886086,"Missing"
L18-1206,L16-1680,1,0.905458,"Missing"
P05-3021,W97-0810,0,0.041464,"Missing"
P05-3021,W01-1315,0,0.0329764,"(Sang and D´ej´ean, 2001); they are currently being implemented as sequences of finite-state transducers along the lines of (A¨ıtMokhtar and Chanod, 1997). Evaluation results are not yet available. 6 SputLink SputLink is a temporal closure component that takes known temporal relations in a text and derives new 83 implied relations from them, in effect making explicit what was implicit. A temporal closure component helps to find those global links that are not necessarily derived by other means. SputLink is based on James Allen’s interval algebra (1983) and was inspired by (Setzer, 2001) and (Katz and Arosio, 2001) who both added a closure component to an annotation environment. Allen reduces all events and time expressions to intervals and identifies 13 basic relations between the intervals. The temporal information in a document is represented as a graph where events and time expressions form the nodes and temporal relations label the edges. The SputLink algorithm, like Allen’s, is basically a constraint propagation algorithm that uses a transitivity table to model the compositional behavior of all pairs of relations. For example, if A precedes B and B precedes C, then we can compose the two relations"
P05-3021,N03-2019,1,0.852311,"each event. For example, a past tense non-stative verb followed by a past perfect non-stative verb, with grammatical aspect maintained, suggests that the second event precedes the first. GUTenLINK uses default rules for ordering events; its handling of successive past tense nonstative verbs in case (iii) will not correctly order sequences like Max fell. John pushed him. GUTenLINK is intended as one component in a larger machine-learning based framework for ordering events. Another component which will be developed will leverage document-level inference, as in the machine learning approach of (Mani et al., 2003), which required annotation of a reference time (Reichenbach, 1947; Kamp and Reyle, 1993) for the event in each finite clause. 1 TimeBank is a 200-document news corpus manually annotated with TimeML tags. It contains about 8000 events, 2100 time expressions, 5700 TLINKs and 2600 SLINKs. See (Day et al., 2003) and www.timeml.org for more details. An early version of GUTenLINK was scored at .75 precision on 10 documents. More formal Precision and Recall scoring is underway, but it compares favorably with an earlier approach developed at Georgetown. That approach converted eventevent TLINKs from"
P05-3021,W01-0708,0,0.0278935,"Missing"
P05-3021,P00-1010,1,\N,Missing
P06-1095,P92-1030,0,0.0591899,"of rules derived from human intuitions. 1 Introduction The growing interest in practical NLP applications such as question-answering and text summarization places increasing demands on the processing of temporal information. In multidocument summarization of news articles, it can be useful to know the relative order of events so as to merge and present information from multiple news sources correctly. In questionanswering, one would like to be able to ask when an event occurs, or what events occurred prior to a particular event. A wealth of prior research by (Passoneau 1988), (Webber 1988), (Hwang and Schubert 1992), (Kamp and Reyle 1993), (Lascarides and Asher 1993), (Hitzeman et al. 1995), (Kehler 2000) and others, has explored the different knowledge sources used in inferring the temporal ordering of events, including temporal adverbials, tense, aspect, rhetorical relations, pragmatic conventions, and background knowledge. For example, the narrative convention of events being described in the order in which they occur is followed in (1), but overridden by means of a discourse relation, Explanation in (2). (1) Max stood up. John greeted him. (2) Max fell. John pushed him. In addition to discourse relat"
P06-1095,N04-1020,0,0.0106106,"Missing"
P06-1095,P04-1074,0,0.124056,"Missing"
P06-1095,N03-2019,1,0.411076,"Missing"
P06-1095,P00-1010,1,0.81515,"Missing"
P06-1095,J88-2005,0,0.0930769,"Missing"
P06-1095,H05-1088,1,0.378139,"Missing"
P06-1095,setzer-gaizauskas-2000-annotating,0,0.0424773,"54 33.33 25.00 84.29 66.37 60.86 82.54 94.20 38.60 88.23 97.26 68.29 83.87 90.90 47.72 89.30 96.09 Event-Time 88.25 (62.3) Prec Rec F 0 0 0 76.47 79.31 73.68 86.07 90.16 74.28 77.97 56.00 80.78 93.56 75.36 78.62 63.63 83.34 91.83 Table 2. Machine learning results using unclosed and closed data 3.2 Expanding Training Data using Temporal Reasoning To expand our training set, we use a temporal closure component SputLink (Verhagen 2004), that takes known temporal relations in a text and derives new implied relations from them, in effect making explicit what was implicit. SputLink was inspired by (Setzer and Gaizauskas 2000) and is based on Allen’s interval algebra, taking into account the limitations on that algebra that were pointed out by (Vilain et al. 1990). It is basically a constraint propagation algorithm that uses a transitivity table to model the compositional behavior of all pairs of relations in a document. SputLink’s transitivity table is represented by 745 axioms. An example axiom: If relation(A, B) = BEFORE && relation(B, C) = INCLUDES then infer relation(A, C) = BEFORE Once the TLINKs in each document in the corpus are closed using SputLink, the same vector generation procedure and feature represe"
P06-1095,E06-1049,0,\N,Missing
P06-1095,J88-2006,0,\N,Missing
P06-1095,E95-1035,0,\N,Missing
P06-1095,W04-3205,0,\N,Missing
P06-1095,W01-1309,0,\N,Missing
pustejovsky-etal-2006-towards,W91-0217,0,\N,Missing
pustejovsky-etal-2006-towards,bel-etal-2000-simple,0,\N,Missing
pustejovsky-etal-2006-towards,havasi-etal-2006-bulb,1,\N,Missing
S07-1014,P06-1095,1,0.245989,"ovsky et al., 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them. TimeBank (Pustejovsky et al., 2003b; Boguraev et al., forthcoming) was originally conceived of as a proof of concept that illustrates the TimeML language, but has since gone through several rounds of revisions and can now be considered a gold standard for temporal information. TimeML and TimeBank have already been used as the basis for automatic time, event and temporal relation annotation tasks in a number of research projects in recent years (Mani et al., 2006; Boguraev et al., forthcoming). An open evaluation challenge in the area of temporal annotation should serve to drive research forward, as it has in other areas of NLP. The automatic identification of all temporal referring expressions, events and temporal relations within a text is the ultimate aim of research in this area. However, addressing this aim in a first evaluation challenge was judged to be too difficult, both for organizers and participants, and a staged approach was deemed more effective. Thus we here present an initial evaluation exercise based on three limited tasks that we bel"
S10-1010,verhagen-2010-brandeis,1,0.752179,"B), precision, recall and the f1-measure are used as evaluation metrics, using the following formulas: X precision recall f -measure Table 1: Corpus size and relation tasks All corpora include event and timex annotation. The French corpus contained a subcorpus with temporal relations but these relations were not split into the four tasks C through F. Annotation proceeded in two phases: a dual annotation phase where two annotators annotate each document and an adjudication phase where a judge resolves disagreements between the annotators. Most languages used BAT, the Brandeis Annotation Tool (Verhagen, 2010), a generic webbased annotation tool that is centered around the notion of annotation tasks. With the task decomposition allowed by BAT, it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful. As 3 Evaluation Metrics = = = tp/(tp + f p) tp/(tp + f n) 2 ∗ (P ∗ R)/(P + R) Where tp is the number of tokens that are part of an extent in both key and response, fp is the number of tokens that are part of an extent in the response but not in the key, and fn is the number of tokens that are part of an extent in the key but not in the"
S10-1010,taule-etal-2008-ancora,0,0.0880496,"Missing"
S10-1010,S07-1014,1,0.681823,"ons in the text and is identical to the TIMEX3 tag in TimeML. Times can be expressed syntactically by adverbial or prepositional phrases, as shown in the following example. (1) a. on Thursday b. November 15, 2004 c. Thursday evening d. in the late 80’s e. later this afternoon Introduction The ultimate aim of temporal processing is the automatic identification of all temporal referring expressions, events and temporal relations within a text. However, addressing this aim is beyond the scope of an evaluation challenge and a more modest approach is appropriate. The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three. In the rest of this paper, we first introduce the data that we are dealing with. Which gets us in a position to present the lis"
S13-2001,derczynski-gaizauskas-2010-analysing,1,0.706855,"ems (silver). The TempEval-3 platinum evaluation corpus was annotated/reviewed by the organizers, who are experts in the area. This process used the TimeML Annotation Guidelines v1.2.1 (Saur´ı et al., 2006). Every file was annotated independently by at least two expert annotators, and a third was dedicated to adjudicating between annotations and merging the final result. Some annotators based their work on TIPSem annotation suggestions (Llorens et al., 2012b). The GATE Annotation Diff tool was used for merging (Cunningham et al., 2013), a custom TimeML validator ensured integrity,3 and CAVaT (Derczynski and Gaizauskas, 2010) was used to determine various modes of TimeML mis-annotation and inconsistency that are inexpressable via XML schema. Post-exercise, that corpus (TempEval-3 Platinum with around 6K tokens, on completely new text) is released for the community to review 3 See and improve.4 Inter-annotator agreement (measured with F1, as per Hripcsak and Rothschild (2005)) and the number of annotation passes per document were higher than in existing TimeML corpora, hence the name. Details are given in Table 1. Attribute value scores are given based on the agreed entity set. These are for exact matches. The Temp"
S13-2001,S10-1063,1,0.726158,"13 shows the results from event extraction. In this case, the previous state-of-the-art is not improved. Table 14 only shows the results obtained in temporal awareness by the state-of-the-art system since there were not participants on this task. We observe that TIPSemB-F approach offers competitive results, which is comparable to results obtained in TE3 English test set. 6.1 Comparison with TempEval-2 TempEval-2 Spanish test set is included as a subset of this TempEval-3 test set. We can therefore compare the performance across editions. Furthermore, we can include the full-featured TIPSem (Llorens et al., 2010), which unlike TIPSemB-F used the AnCora (Taul´e et al., 2008) corpus annotations as features including semantic roles. For timexes, as can be seen in Table 15, the original TIPSem obtains better results for timex extraction, which favours the hypothesis that machine learning systems are very well suited for this task (if the training data is sufficiently representative). However, for normalization (value F1), HeidelTime – a rule-engineered system – obtains better results. This indicates that rule-based approaches have the upper hand in this task. TIPSem uses FSS-TimEx TIPSemB-F TIPSem F1 59.0"
S13-2001,padro-stanilovsky-2012-freeling,0,0.0212522,"Missing"
S13-2001,S10-1062,1,0.680883,"ompletely new text) is released for the community to review 3 See and improve.4 Inter-annotator agreement (measured with F1, as per Hripcsak and Rothschild (2005)) and the number of annotation passes per document were higher than in existing TimeML corpora, hence the name. Details are given in Table 1. Attribute value scores are given based on the agreed entity set. These are for exact matches. The TempEval-3 silver evaluation corpus is a 600K word corpus collected from Gigaword (Parker et al., 2011). We automatically annotated this corpus by TIPSem, TIPSem-B (Llorens et al., 2013) and TRIOS (UzZaman and Allen, 2010). These systems were retrained on the corrected TimeBank and AQUAINT corpus to generate the original TimeML temporal relation set. We then merged these three state-of-the-art system outputs using our merging algorithm (Llorens et al., 2012a). In our selected merged configuration all entities and relations suggested by the best system (TIPSem) are added in the merged output. Suggestions from other systems (TRIOS and TIPSem-B) are added in the merged output, only if they are also supported by another system. The weights considered in our configuration are: TIPSem 0.36, TIPSemB 0.32, TRIOS 0.32."
S13-2001,P11-2061,1,0.562387,"the system identified both the entity and attribute (attr) together. Attribute Recall = |{∀x |x∈(Sysentity ∩Refentity )∧Sysattr (x)==Refattr (x)}| |Refentity | Attribute Precision = |{∀x |x∈(Sysentity ∩Refentity )∧Sysattr (x)==Refattr (x)}| |Sysentity | Attribute F1-score = 2∗p∗r p+r Attribute (Attr) accuracy, precision and recall can be calculated as well from the above information. Attr Accuracy = Attr F1 / Entity Extraction F1 Attr R = Attr Accuracy * Entity R Attr P = Attr Accuracy * Entity P 4.2 Temporal Relation Processing To evaluate relations, we use the evaluation metric presented by UzZaman and Allen (2011).5 This metric captures the temporal awareness of an annotation in terms of precision, recall and F1 score. Temporal awareness is defined as the performance of an annotation as identifying and categorizing temporal relations, which implies the correct recognition and classification of the temporal entities involved in the relations. Unlike TempEval2 relation score, where only categorization is evaluated for relations, this metric evaluates how well pairs of entities are identified, how well the relations are categorized, and how well the events and temporal expressions are extracted. + |Sys− ∩"
S13-2001,S10-1010,1,\N,Missing
S13-2001,taule-etal-2008-ancora,0,\N,Missing
S15-2136,N13-3004,0,0.0310777,"RE P OST E XP or S ET 4. Adjudicators revised and finalized the temporal relations More details on the corpus annotation process are documented in a separate article (Styler et al., 2014a). Because the data contained incompletely deidentified clinical data (the time expressions were retained), participants were required to sign a data use agreement with the Mayo Clinic to obtain the raw text of the clinical notes and pathology reports.1 The event, time and temporal relation annotations were distributed separately from the text, in an open source repository2 using the Anafora standoff format (Chen and Styler, 2013). • Identifying event expressions (EVENT annotations in the THYME corpus) consisting of the following components: – The spans (character offsets) of the expression in the text – Contextual Modality: ACTUAL, H YPO THETICAL, H EDGED or G ENERIC – Degree: M OST, L ITTLE or N/A – Polarity: P OS or N EG – Type: A SPECTUAL, E VIDENTIAL or N/A 1 The details of this process are described at http://thyme. healthnlp.org/ 2 https://github.com/stylerw/thymedata 807 Train Dev 293 147 38890 20974 3833 2078 11176 6173 3 Normalized time values (e.g. 2015-02-05) were originally planned, but annotation was not"
S15-2136,W11-0419,1,0.496993,"AL, E VIDENTIAL or N/A 1 The details of this process are described at http://thyme. healthnlp.org/ 2 https://github.com/stylerw/thymedata 807 Train Dev 293 147 38890 20974 3833 2078 11176 6173 3 Normalized time values (e.g. 2015-02-05) were originally planned, but annotation was not completed in time. • Identifying temporal relations between events and times, focusing on the following types: – Relations between events and the document creation time (B EFORE, OVER LAP , B EFORE -OVERLAP or A FTER ), represented by D OC T IME R EL annotations in the THYME corpus – Narrative container relations (Pustejovsky and Stubbs, 2011) between events and/or times, represented by T LINK annotations with T YPE =C ONTAINS in the THYME corpus The evaluation was run in two phases: 1. Systems were given access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2. Systems were given access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F1 : P = |S ∩ H| |S| R= |S ∩ H| |H| F1 = 2·P ·R P +R where S is th"
S15-2136,Q14-1012,1,0.497138,"Missing"
S15-2136,P11-2061,0,0.0272505,"ated by dividing the F1 on the attribute by the F1 on identifying the spans: A= attribute F1 span F1 For the narrative container relations, additional metrics were included that took into account temporal closure, where additional relations can be deterministically inferred from other relations (e.g., A C ON - TAINS B and B C ONTAINS C, so A C ONTAINS C): Pclosure = Rclosure |S ∩ closure(H)| |S| 6 |closure(S) ∩ H| = |H| Fclosure = Participating Systems Three research teams submitted a total of 13 runs: 2 · Pclosure · Rclosure Pclosure + Rclosure These measures take the approach of prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of humanannotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 was predicted to be a narrative container, containing only the closest event expression to it in the text. Ba"
S15-2136,S13-2001,1,0.954536,"l of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations. 1 April 23, 2014: The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea. And output annotations over the text that capture the following kinds of information: Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic. This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understan"
S15-2136,S07-1014,1,0.916093,"Missing"
S15-2136,S10-1010,1,\N,Missing
S16-1165,S16-1195,0,0.0501484,"Missing"
S16-1165,S16-1196,0,0.0406396,"Missing"
S16-1165,S15-2136,1,0.542156,"Missing"
S16-1165,S16-1193,0,0.0677741,"Missing"
S16-1165,N13-3004,1,0.0547857,"Missing"
S16-1165,S16-1192,0,0.064286,"Missing"
S16-1165,S16-1198,0,0.0757468,"Missing"
S16-1165,S16-1190,0,0.0552623,"Missing"
S16-1165,S16-1200,0,0.0379618,"Missing"
S16-1165,S16-1201,0,0.0940692,"Missing"
S16-1165,S16-1199,0,0.0614168,"Missing"
S16-1165,P14-5010,1,0.0104397,"Missing"
S16-1165,W11-0419,1,0.613193,"event expressions (EVENT annotations in the THYME corpus) consisting of the following components: – The span (character offsets) of the expression in the text – Contextual Modality: ACTUAL, H YPO THETICAL, H EDGED or G ENERIC – Degree: M OST, L ITTLE or N/A – Polarity: P OS or N EG – Type: A SPECTUAL, E VIDENTIAL or N/A • Identifying temporal relations between events and times, focusing on the following types: – Relations between events and the document creation time (B EFORE, OVER LAP , B EFORE -OVERLAP or A FTER ), represented by D OC T IME R EL annotations. – Narrative container relations (Pustejovsky and Stubbs, 2011), which indicate that an event or time is temporally contained in (i.e., occurred during) another event or time, represented by T LINK annotations with T YPE =C ONTAINS. The evaluation was run in two phases: 1. Systems were provided access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2. Systems were provided access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R)"
S16-1165,Q14-1012,1,0.456145,"Missing"
S16-1165,P11-2061,0,0.0570732,"w a comparison across systems for assigning attribute values, even when different systems produce different numbers of events and times. This metric is calculated by dividing the F1 on the attribute by the F1 on identifying the spans: A= attribute F1 span F1 For narrative container relations, the P and R definitions were modified to take into account temporal closure, where additional relations are deterministically inferred from other relations (e.g., A C ONTAINS B and B C ONTAINS C, so A C ONTAINS C): P = |S ∩ closure(H)| |S| R= |closure(S) ∩ H| |H| Similar measures were used in prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of human-annotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 Baseline Systems Two rule-based systems were used as baselines to compare the participating systems against."
S16-1165,S13-2001,1,0.208832,"on a corpus of clinical and pathology notes from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. 14 teams submitted a total of 40 system runs, with the best systems achieving near-human performance on identifying events and times. On identifying temporal relations, there was a gap between the best systems and human performance, but the gap was less than half the gap of Clinical TempEval 2015. 1 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. In recent years, the community has moved toward testing such information extraction systems on clinical data (Sun et al., 2013; Bethard et al., 2015) to broaden our understanding of the language of time beyond newswire expressions and structure. Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, rel"
S16-1165,S07-1014,1,0.223174,"Missing"
sauri-etal-2006-slinket,H05-1088,1,\N,Missing
sauri-etal-2006-slinket,W02-1033,0,\N,Missing
sauri-etal-2006-slinket,P05-3021,1,\N,Missing
sauri-etal-2006-slinket,W04-3103,0,\N,Missing
sauri-etal-2006-slinket,C96-1079,0,\N,Missing
verhagen-2010-brandeis,S07-1014,1,\N,Missing
verhagen-2010-brandeis,W07-1501,0,\N,Missing
verhagen-2010-brandeis,ide-romary-2006-representing,0,\N,Missing
verhagen-2010-brandeis,verhagen-etal-2006-annotation,1,\N,Missing
verhagen-etal-2006-annotation,A97-1051,0,\N,Missing
verhagen-etal-2006-annotation,P05-3021,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,C08-3012,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1062,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1075,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1071,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,P00-1010,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,H05-1088,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S07-1025,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S07-1108,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S07-1098,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,P06-1095,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,P05-3021,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,pustejovsky-etal-2010-iso,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,P09-1068,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1063,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,R11-1084,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,ide-romary-2006-representing,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,S10-1070,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,verhagen-etal-2006-annotation,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,sauri-etal-2006-slinket,1,\N,Missing
verhagen-pustejovsky-2012-tarsqi,W10-1920,0,\N,Missing
verhagen-pustejovsky-2012-tarsqi,W11-0419,1,\N,Missing
vogel-etal-2012-atlis,S07-1005,0,\N,Missing
vogel-etal-2012-atlis,W03-0907,0,\N,Missing
W07-1517,miltsakaki-etal-2004-penn,0,0.0398621,"Missing"
W07-1517,J05-1004,0,0.0425206,"a use case. With MAIS, we adopt the following requirements for the interoperability of syntactic and semantic annotations: 1. Each annotation scheme has its own philosophy and is independent from the other annotations. Simple and generally available interfaces provide access to the content of each annotation scheme. 2. Interactions between annotations are not defined a priori, but based on use cases. 3. Simple tree-based and one-directional merging of annotations is useful for visualization of overlap between schemes. The annotation schemes currently embedded in MAIS are the Proposition Bank (Palmer et al., 2005), NomBank (Meyers et al., 2004) and TimeBank (Pustejovsky et al., 2003). Other linguistics annotation schemes like the opinion annotation 2 Interoperability of Annotations Our goal is not to define a static merger of all annotation schemes. Rather, we avoid defining a potentially complex interlingua and instead focus on how information from different sources can be combined pragmatically. A high-level schematic representation of the system architecture is given in figure 1. PropBank NomBank annotation TimeBank initializers PropBank NomBank TimeBank interface interface interface case-based inte"
W07-1517,W04-2705,0,\N,Missing
W07-1517,J93-2004,0,\N,Missing
W09-2418,S07-1014,1,0.624748,"es. The data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a handbuilt gold standard of annotated texts using the TimeML markup scheme.1 The data sets included sentence boundaries, TIMEX 3 tags (including the special document creation time tag), and EVENT tags. For tasks A and B, a restricted set of events was used, namely those events that occur more than 5 times in TimeBank. For all three tasks, the relation labels used were BEFORE, AFTER, OVER LAP , BEFORE - OR - OVERLAP , OVERLAP - OR - AFTER and VAGUE.2 For a more elaborate description of TempEval-1, see (Verhagen et al., 2007; Verhagen et al., 2009). 1 See www.timeml.org for details on TimeML, TimeBank is distributed free of charge by the Linguistic Data Consortium (www.ldc.upenn.edu), catalog number LDC2006T08. 2 Which is different from the set of 13 labels from TimeML. The set of labels for TempEval-1 was simplified to aid data preparation and to reduce the complexity of the task. Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112–116, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics There were six systems competing in"
W11-0224,ide-romary-2006-representing,0,0.030159,"aintained in any serious fashion after 2004. Developers of the system dispersed over the world and the Medstract server fatally crashed in 2007. Here, we describe the resurrection of Medstract. One goal was that code should be open source and that installation should not depend on idiosyncraJames Pustejovsky Computer Science Department Brandeis University, Waltham, USA jamesp@cs.brandeis.edu cies of the developer’s machine, which was a problem with the inherited code base. Reusability and extendability are ensured by following the principles embodied in the Linguistic Annotation Format (LAF) (Ide and Romary, 2006). In LAF, source data are untouched, annotations are grouped in layers that can refer to each other and to the source, and each layer is required to be mappable to a graph-like pivot format. For MedstractPlus, each component is set up to be independent from other layers, although of course each layer may need access to certain types of information in order to create non-trivial output. This allows us to swap in alternative modules, making it easier to experiment with different versions of the tagger and chunker for example. We now proceed to describe the system in section 2 and finish with the"
W11-0224,W02-0312,1,0.740297,"Missing"
W14-5204,cassidy-etal-2014-alveo,0,0.05905,"se grids by users of any one of them and, perhaps most importantly, facilitate adding additional grids and service platforms to the federation. Currently, the European META-NET initiative is committed to joining the federation in the near future. In addition to the projects listed above, we are also collaborating with several groups on technical solutions to achieve interoperability and in particular, on development of the WS-EV, the JSON-LD format, and a corollary development of an ontology of web service types. These collaborators include the Alveo Project (Macquarie University, Australia) (Cassidy et al., 2014), the Language Grid project, and the Lider project29 . We actively seek collaboration with others in order to move closer to achieving a “global laboratory” for language applications. 6 Conclusion In this paper, we have given a brief overview of the LAPPS Web Service Exchange Vocabulary (WSEV), which provides a terminology for a core of linguistic objects and features exchanged among NLP tools that consume and produce linguistically annotated data. The goal is to bring the field closer to achieving semantic interoperability among NLP data, tools, and services. We are actively working to both e"
W14-5204,P13-1166,0,0.106613,"Missing"
W14-5204,W09-3034,1,0.689856,"Grid provides a critical missing layer of functionality for NLP: although existing frameworks such as UIMA and GATE provide the capability to wrap, integrate, and deploy language services, they do not provide general support for service discovery, composition, and reuse. The LAPPS Grid is a collaborative effort among US partners Brandeis University, Vassar College, Carnegie-Mellon University, and the Linguistic Data Consortium at the University of Pennsylvania, and is funded by the US National Science Foundation (NSF). The project builds on the foundation laid in the NSF-funded project SILT (Ide et al., 2009), which established a set of needs for interoperability and developed standards and best practice guidelines to implement them. LAPPS is similar in its scope and goals to ongoing projects such as The Language Grid15 , PANACEA/MetaNET16 , LinguaGrid17 , and CLARIN18 , which also provide web service access to basic NLP processing tools and resources and enable pipelining these tools to create custom NLP applications and composite services such as question answering and machine translation, as well as access to language resources such as mono- and multilingual corpora and lexicons that support NL"
W14-5204,ide-etal-2014-language,1,0.862088,"Missing"
W14-5204,windhouwer-2012-relcat,0,\N,Missing
W14-5204,W14-5211,0,\N,Missing
W14-5206,declerck-2006-synaf,0,0.0328252,"igure 3), it is generally referred to as the model of annotation. For example, GATE has its own single unified model of annotation, which is organized in annotation graphs. The arcs in the graph have a start node and an end node, an identifier, a type and a set of features (Bontcheva et al., 2004). One standardization effort (Ide and Romary, 2004), the Linguistic Annotation Framework (LAF) architecture is designed so that a pivot format, such as GrAF (Ide and Suderman, 2007), can bridge various annotation collections. Another standardization effort, the Syntactic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold’s XSLT-based mechanism, the conversion tools that brid"
W14-5206,declerck-2008-framework,0,0.0223904,"has its own single unified model of annotation, which is organized in annotation graphs. The arcs in the graph have a start node and an end node, an identifier, a type and a set of features (Bontcheva et al., 2004). One standardization effort (Ide and Romary, 2004), the Linguistic Annotation Framework (LAF) architecture is designed so that a pivot format, such as GrAF (Ide and Suderman, 2007), can bridge various annotation collections. Another standardization effort, the Syntactic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold’s XSLT-based mechanism, the conversion tools that bridge GATE annotation and UIMA annotation use GrAF as a pivot and are provided as GATE"
W14-5206,W07-1505,0,0.0205734,"collections. Another standardization effort, the Syntactic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold’s XSLT-based mechanism, the conversion tools that bridge GATE annotation and UIMA annotation use GrAF as a pivot and are provided as GATE plugins and UIMA modules (Ide and Suderman, 2009). Thus, while a pivot standard annotation model like GrAF seems very promising, popular annotation models like those provided by GATE annotations (see Figure 4) or UIMA annotations (see Figure 4) will continue to exist and evolve for a long time. As a result, more bridge strategies, like the conversion plugin (module) of GATE (UIMA) and the XSLT-based middleware mechanism, will"
W14-5206,W07-1501,0,0.224868,"dard for text encoding and interchange, which also enables meta-information description. Concerning the main part (see dashdotted-line part of Figure 3), it is generally referred to as the model of annotation. For example, GATE has its own single unified model of annotation, which is organized in annotation graphs. The arcs in the graph have a start node and an end node, an identifier, a type and a set of features (Bontcheva et al., 2004). One standardization effort (Ide and Romary, 2004), the Linguistic Annotation Framework (LAF) architecture is designed so that a pivot format, such as GrAF (Ide and Suderman, 2007), can bridge various annotation collections. Another standardization effort, the Syntactic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also ha"
W14-5206,W09-3004,0,0.0211468,"and designed as the XML serialization for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML standoff annotations for natively XML support, and provides XSLT-based online integration mechanism of various annotation collections (Sch¨afer, 2006). The UIMA specifies a UML-based data model of annotation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold’s XSLT-based mechanism, the conversion tools that bridge GATE annotation and UIMA annotation use GrAF as a pivot and are provided as GATE plugins and UIMA modules (Ide and Suderman, 2009). Thus, while a pivot standard annotation model like GrAF seems very promising, popular annotation models like those provided by GATE annotations (see Figure 4) or UIMA annotations (see Figure 4) will continue to exist and evolve for a long time. As a result, more bridge strategies, like the conversion plugin (module) of GATE (UIMA) and the XSLT-based middleware mechanism, will continue to be necessary. In the following sections, we consider the issue of the continuing availability of such conversion functions, and whether the current realization of those two conversion strategies is sufficien"
W14-5206,W02-0109,0,0.0209259,"data management, a unified interface for data exchange, and a light-weight serialization for data visualization. In addition, we propose a semantic mapping-based pipeline composition, which allows experts to interactively exchange data between heterogeneous components. 1 Introduction The recent work on open infrastructures for human language technology (HLT) research and development has stressed the important role that interoperability should play in developing Natural Language Processing (NLP) pipelines. For example, GATE (Cunningham et al., 2002), UIMA (Ferrucci and Lally, 2004), and NLTK (Loper and Bird, 2002) all allow integrating components from different categories based on common XML, or object-based (e.g., Java or Python) data presentation. The major categories of components included in these capabilities include: Sentence Splitter, Phrase Chunker, Tokenizer, Part-of-Speech (POS) Tagger, Shallow Parser, Name Entity Recognizer (NER), Coreference Solution, etc. Pipelined NLP applications can be built by composing several components; for example, a text analysis application such as “relationship analysis from medical records” can be composed by Sentence Splitter, Tokenizer, POS Tagger, NER, and C"
W14-5206,W06-2714,0,0.083658,"Missing"
W14-5206,W12-3610,0,0.0169187,"the Stanford NLP NER. This particular semantic mapping between JSON serialization of a NLTK POS Tagger and the standoff ontology of annotation of POS Tagger, and between the standoff ontology of annotation of POS Tagger and the UIMA annotation of Stanford NLP NER will be reused in the NLP application created by other end-users. Our conceptual framework does not exclusively rely on the above interoperability design. Our conceptual framework (see Figure 5) should integrate existing knowledge of various annotation frameworks, for example, the alignment knowledge from the Open Annotation models (Verspoor and Livingston, 2012) and the pivot bridge knowledge from the GrAF (Ide and Suderman, 2007) under the Linguistic Annotation Framework (LAF). Thus, existing pivot conversion solutions and XSLT-based middleware solutions can also be applied. Our interactive ontology mapping design provides a more flexible choice for linguistic experts to build up NLP pipeline applications on top of heterogeneous components, without online help from engineers. Below we present varying levels of online NLP applications, according to what kind of extra support would be needed for composing different NLP components: • Components are int"
W14-5206,wright-2014-restful,0,0.0286024,"are wrapped into Restful services so that they are operated through HTTP GET protocol, and the XML serialization of UIMA annotation is applied for input and output, each NLP components will have the same interface and data structure. Once an internationalized resource identifier (IRI) is given, all the input and output of tools can be distributed and ubiquitously identified. Moreover, a PUT/GET/POST/DELETE protocol of restful data management is equivalent to an SQL-like CRUD data management interface. For example, an IRI can be defined by a location identifier and the URL of the data service (Wright, 2014). In addition, a lightweight serialization of stand-off annotation can benefit the online visualization of data, which will be easy for experts to read, judge, or edit. For example, the XML serialization of UIMA annotation can be transferred into JSON serialization, which is preferred for online reading or editing. NLP tool services will be available by applying restful wrapping (see Figure 5). However, structural interoperability based on the restful wrapping is not enough for conceptual interoperability. For example, if an OpenNLP tokenizer is wrapped using HTTP GET protocol and GATE annotat"
W14-5206,P06-4018,0,\N,Missing
W14-5206,W03-0804,0,\N,Missing
W14-6004,D07-1114,0,0.0682028,"Missing"
W14-6004,W97-0313,0,0.276147,"Missing"
W14-6004,2009.mtsummit-wpt.4,0,0.0257841,"discover relationships, and facilitate patent searches. One of the indicators of new technology emergence is the coinage, adoption and spread of new terms; hence the identification and tracking of technical terminology over time is of particular interest to researchers designing tools to support analysts engaged in technology forecasting (e.g., Woon, 2009; deMiranda, 2006) For the most part, research into terminology extraction has either (1) focused on the identification of keywords within individual patents or corpora without regard to the roles played by the keywords within the text (e.g., Sheremetyeva, 2009) or, (2) engaged in fine-grained analysis of the semantics of narrow domains (e.g., Yang, 2008). In this paper we strive towards a middle ground, using a highlevel classification suitable for all domains, inspired in part by recent work on sentiment analysis (Liu, 2012). In aspect-based sentiment analysis, natural language reviews of specific target entities, such as restaurants or cameras, are analyzed to extract aspects, i.e., features of the target entities, along with the sentiment expressed toward those features. In the restaurant domain, for example, aspects might include the breadth of"
W14-6004,W02-1028,0,0.185093,"Missing"
W14-6004,N03-1033,0,0.0164202,"Missing"
W14-6004,H05-1044,0,0.0343327,"proach, using statistics to determine domain relevance and consensus is very similar to that adopted here. We have also drawn inspiration from sentiment analysis, proposing an ontology for patents that reflects their review-like qualities (Liu, 2012). Most relevant is the work on discovering aspects and opinions relating to a particular subject such as a camera or restaurant (Kobayashi, 2007). There are many subtleties that have been studied in opinion mining research that we have finessed in our research here, such as detecting implicit sentiment and attributes not expressed as noun phrases. Wilson et al (2005, 2009) addressed the larger problem of determining contextual polarity for subjective expressions in general, putting considerable effort into the compilation of subjectivity clues and annotations. In contrast, our aim was to test whether we could substantially reduce the annotation effort when the task is focused on polarity labeling of attributes within patents. We hypothesized that the specialized role of patents might permit a more lightweight approach amenable to bootstrapping from a very small set of annotations and feature types. 37 Bootstrapping has been successfully applied to develo"
W14-6004,J09-3003,0,0.10052,"Missing"
W14-6004,I13-1188,0,0.046619,"Missing"
W16-5202,P13-1166,0,0.0336972,"Missing"
W16-5202,gilmanov-etal-2014-swift,0,0.0168661,"allowing for their immediate inclusion in workflows supporting sophisticated applications as well as evaluation of their performance side-by-side with comparable components. Although many contributors host their own contributed services (which are called from within the LAPPS Grid), where necessary the LAPPS Grid provides hosting to ensure that software remains available to the community. Recently contributed tools include all core tools from University of Darmstadt’s DKPro3 , the AIFdb services for Argumentation analysis4 (Lawrence et al., 2012), the SWIFT Aligner for cross-lingual transfer (Gilmanov et al., 2014), the EDISON feature extraction framework5 (Sammons et al., 2016) and other tools available from the University of Illinois (e.g., semantic role labelers, entity extractors), among others. In addition, several of the basic components produced by the ARIEL team working within DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) program have been integrated into the LAPPS Grid, which include tools and data to support a wide array of under-resourced languages. The LAPPS Grid has been adopted by a Mellon-funded project at the University of Illinois, which is utilizing the platform to ap"
W16-5202,W14-5204,1,0.848346,"s in views, where each view contains metadata that spells out the information contained in that view, including information necessary to determine compatibility with other tools and data. Semantic interoperability is achieved via references to definitions in the Web Services Exchange Vocabulary (WSEV). The WSEV has been built bottom up, driven by the needs of components in the LAPPS Grid and closely following standard practice in the field as well as adopting, where possible, existing terminology and type systems. Both LIF and the WSEV are described in detail elsewhere (Verhagen et al., 2016; Ide et al., 2014; Ide et al., 2016). Another distinctive feature of the LAPPS Grid is its Open Advancement (OA) Evaluation system, a sophisticated environment that was used to develop IBM’s Jeopardy-winning Watson. OA can be simultaneously applied to multiple variant workflows involving alternative tools for a given task, and the results are evaluated and displayed so that the best possible configuration is readily apparent. Similarly, the weak links in a chain are easily detected, which can lead to module-specific improvements that affect the entire process. The inputs, tools, parameters and settings used fo"
W16-5202,N16-3019,0,0.0252091,"considerable programming effort to add or modify components. Similarly, the Natural Language Toolkit (NLTK) requires Python programming and effectively limits the user to the tools that are built-in to the system. In contrast, modules can be easily added to the LAPPS Grid by wrapping them as a service, using provided templates; and, more importantly, no programming experience or technical expertise is required, since workflows are constructed using the Galaxy project’s workflow management framework. This makes the LAPPS Grid ideal for instructional use. The recently introduced Kathaa system (Mohanty et al., 2016) provides functionality similar to the LAPPS Grid, but allows modules to be interfaced only if compatible with one another–i.e., there is no attempt to standardize inputs and outputs among modules, so that mixing and matching of different tools that perform the same function is limited. The LAPPS Grid’s Open Advancement evaluation modules, which exploit the ability to construct alternative pipelines in order produce statistics identifying the most effective tool sequence and/or components accounting for the largest proportion of error, are also unique; Kathaa in contrast has only basic evaluat"
W16-5202,L16-1645,0,0.0188181,"histicated applications as well as evaluation of their performance side-by-side with comparable components. Although many contributors host their own contributed services (which are called from within the LAPPS Grid), where necessary the LAPPS Grid provides hosting to ensure that software remains available to the community. Recently contributed tools include all core tools from University of Darmstadt’s DKPro3 , the AIFdb services for Argumentation analysis4 (Lawrence et al., 2012), the SWIFT Aligner for cross-lingual transfer (Gilmanov et al., 2014), the EDISON feature extraction framework5 (Sammons et al., 2016) and other tools available from the University of Illinois (e.g., semantic role labelers, entity extractors), among others. In addition, several of the basic components produced by the ARIEL team working within DARPA’s Low Resource Languages for Emergent Incidents (LORELEI) program have been integrated into the LAPPS Grid, which include tools and data to support a wide array of under-resourced languages. The LAPPS Grid has been adopted by a Mellon-funded project at the University of Illinois, which is utilizing the platform to apply sophisticated HLT text mining methods to the HathiTrust Resea"
W17-0808,W14-5204,1,0.928633,"er Science ♠ Vassar College, Department of Computer Science ♥ University of Oslo, Department of Informatics ♦ Brandeis University, Linguistics and Computational Linguistics Abstract representation—a uniform framework-internal convention—with mappings from tool-specific input and output formats. Specifically, we will take an in-depth look at how the results of morphosyntactic analysis are represented in (a) the DKPro Core component collection1 (Eckart de Castilho and Gurevych, 2014), (b) the Language Analysis Portal2 (LAP; Lapponi et al. (2014)), and (c) the Language Application (LAPPS) Grid3 (Ide et al., 2014a). These three systems all share the common goal of facilitating the creation of complex NLP workflows, allowing users to combine tools that would otherwise need input and output format conversion in order to be made compatible. While the programmatic interface of DKPro Core targets more technically inclined users, LAP and LAPPS are realized as web applications with a point-andclick graphical interface. All three have been under active development for the past several years and have—in contemporaneous, parallel work— designed and implemented framework-specific representations. These designs a"
W17-0808,lapponi-etal-2014-road,1,0.860158,"Marc Verhagen♦ ♣ Technische Universität Darmstadt, Department of Computer Science ♠ Vassar College, Department of Computer Science ♥ University of Oslo, Department of Informatics ♦ Brandeis University, Linguistics and Computational Linguistics Abstract representation—a uniform framework-internal convention—with mappings from tool-specific input and output formats. Specifically, we will take an in-depth look at how the results of morphosyntactic analysis are represented in (a) the DKPro Core component collection1 (Eckart de Castilho and Gurevych, 2014), (b) the Language Analysis Portal2 (LAP; Lapponi et al. (2014)), and (c) the Language Application (LAPPS) Grid3 (Ide et al., 2014a). These three systems all share the common goal of facilitating the creation of complex NLP workflows, allowing users to combine tools that would otherwise need input and output format conversion in order to be made compatible. While the programmatic interface of DKPro Core targets more technically inclined users, LAP and LAPPS are realized as web applications with a point-andclick graphical interface. All three have been under active development for the past several years and have—in contemporaneous, parallel work— designed"
W17-0808,P12-2074,1,0.837847,"pendencyStructure which can bind multiple dependency relations together and thus supports multiple parallel dependency structures even within a single LIF view. Media–Tokenization Mismatches Tokenizers may apply transformations to the original input text that introduce character offset mismatches with the normalized output. For example, some Penn Treebank–compliant tokenizers normalize different conventions for quotation marks (which may be rendered as straight ‘typewriter’ quotes or in multicharacter LATEX-style encodings, e.g. &quot; or ``) into opening (left) and closing (right) Unicode glyphs (Dridan and Oepen, 2012). To make such normalization accessible to downstream processing, it is insufficient to represent tokens as only a region (sub-string) of the underlying linguistic signal. In LXF, the string output of tokenizers is recorded in the annotations encapsulated with each token node, which is in turn linked to a region recording its character offsets in the original media. LIF (which is largely inspired by ISO LAF, much like LXF) also records the token string and its character offsets in the original medium. LIF supports this via the word property on tokens. DKPro Core has also recently started intro"
W17-0808,W14-5201,1,0.897235,"Missing"
W17-0808,heid-etal-2010-corpus,0,0.0249296,"Lacking interface standardization, thus, severely limits interoperability. The frameworks surveyed in this work address interoperability by means of a common 2 Terminological Definitions A number of closely interrelated concepts apply to the discussion of design choices in the repre1 https://dkpro.github.io/dkpro-core https://lap.clarino.uio.no 3 https://www.lappsgrid.org 4 There are, of course, additional designs and workflow frameworks that we would ultimately hope to include in this comparison, as for example the representations used by CONCRETE, WebLicht, and FoLiA (Ferraro et al., 2014; Heid et al., 2010; van Gompel and Reynaert, 2013), to name just a few. However, some of these frameworks are at least abstractly very similar to representatives in our current sample, and also for reasons of space we need to restrict this in-depth comparison to a relatively small selection. 2 67 Proceedings of the 11th Linguistic Annotation Workshop, pages 67–75, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics root nsubj det aux nn DT The the NNP Olympic Olympic neg NNP Committee Committee VBZ does do RB n’t not xcomp VB regret regret VBG choosing choose ORGANIZATION NNP China"
W17-0808,ide-etal-2014-language,1,0.90873,"Missing"
