2021.naacl-main.233,Outside Computation with Superior Functions,2021,-1,-1,2,0.925926,3944,parker riley,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We show that a general algorithm for efficient computation of outside values under the minimum of superior functions framework proposed by Knuth (1977) would yield a sub-exponential time algorithm for SAT, violating the Strong Exponential Time Hypothesis (SETH)."
2020.iwpt-1.8,Tensors over Semirings for Latent-Variable Weighted Logic Programs,2020,-1,-1,2,0,18859,esma balkir,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,0,"Semiring parsing is an elegant framework for describing parsers by using semiring weighted logic programs. In this paper we present a generalization of this concept: latent-variable semiring parsing. With our framework, any semiring weighted logic program can be latentified by transforming weights from scalar values of a semiring to rank-n arrays, or tensors, of semiring values, allowing the modelling of latent-variable models within the semiring parsing framework. Semiring is too strong a notion when dealing with tensors, and we have to resort to a weaker structure: a partial semiring. We prove that this generalization preserves all the desired properties of the original semiring framework while strictly increasing its expressiveness."
2020.coling-main.181,Generalized Shortest-Paths Encoders for {AMR}-to-Text Generation,2020,-1,-1,2,0,21272,lisa jin,Proceedings of the 28th International Conference on Computational Linguistics,0,"For text generation from semantic graphs, past neural models encoded input structure via gated convolutions along graph edges. Although these operations provide local context, the distance messages can travel is bounded by the number of encoder propagation steps. We adopt recent efforts of applying Transformer self-attention to graphs to allow global feature propagation. Instead of feeding shortest paths to the vertex self-attention module, we train a model to learn them using generalized shortest-paths algorithms. This approach widens the receptive field of a graph encoder by exposing it to all possible graph paths. We explore how this path diversity affects performance across levels of AMR connectivity, demonstrating gains on AMRs of higher reentrancy counts and diameters. Analysis of generated sentences also supports high semantic coherence of our models for reentrant AMRs. Our best model achieves a 1.4 BLEU and 1.8 chrF++ margin over a baseline that encodes only pairwise-unique shortest paths."
2020.cl-4.2,Efficient Outside Computation,2020,-1,-1,1,1,3945,daniel gildea,Computational Linguistics,0,"Weighted deduction systems provide a framework for describing parsing algorithms that can be used with a variety of operations for combining the values of partial derivations. For some operations, inside values can be computed efficiently, but outside values cannot. We view out-side values as functions from inside values to the total value of all derivations, and we analyze outside computation in terms of function composition. This viewpoint helps explain why efficient outside computation is possible in many settings, despite the lack of a general outside algorithm for semiring operations."
Q19-1002,Semantic Neural Machine Translation Using {AMR},2019,41,15,2,1,3601,linfeng song,Transactions of the Association for Computational Linguistics,0,"It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequence-to-sequence neural translation model."
P19-1446,{S}em{B}leu: A Robust Metric for {AMR} Parsing Evaluation,2019,19,1,2,1,3601,linfeng song,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs. The major evaluation metric, SMATCH (Cai and Knight, 2013), searches for one-to-one mappings between the nodes of two AMRs with a greedy hill-climbing algorithm, which leads to search errors. We propose SEMBLEU, a robust metric that extends BLEU (Papineni et al., 2002) to AMRs. It does not suffer from search errors and considers non-local correspondences in addition to local ones. SEMBLEU is fully content-driven and punishes situations where a system{'}s output does not preserve most information from the input. Preliminary experiments on both sentence and corpus levels show that SEMBLEU has slightly higher consistency with human judgments than SMATCH. Our code is available at http://github.com/ freesunshine0316/sembleu."
J19-2005,Ordered Tree Decomposition for {HRG} Rule Extraction,2019,25,0,1,1,3945,daniel gildea,Computational Linguistics,0,"We present algorithms for extracting Hyperedge Replacement Grammar (HRG) rules from a graph along with a vertex order. Our algorithms are based on finding a tree decomposition of smallest width, relative to the vertex order, and then extracting one rule for each node in this structure. The assumption of a fixed order for the vertices of the input graph makes it possible to solve the problem in polynomial time, in contrast to the fact that the problem of finding optimal tree decompositions for a graph is NP-hard. We also present polynomial-time algorithms for parsing based on our HRGs, where the input is a vertex sequence and the output is a graph structure. The intended application of our algorithms is grammar extraction and parsing for semantic representation of natural language. We apply our algorithms to data annotated with Abstract Meaning Representations and report on the characteristics of the resulting grammars."
D19-1020,Leveraging Dependency Forest for Neural Medical Relation Extraction,2019,0,2,3,1,3601,linfeng song,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Medical relation extraction discovers relations between entity mentions in text, such as research articles. For this task, dependency syntax has been recognized as a crucial source of features. Yet in the medical domain, 1-best parse trees suffer from relatively low accuracies, diminishing their usefulness. We investigate a method to alleviate this problem by utilizing dependency forests. Forests contain more than one possible decisions and therefore have higher recall but more noise compared with 1-best outputs. A graph neural network is used to represent the forests, automatically distinguishing the useful syntactic information from parsing noise. Results on two benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature."
W18-6553,Neural Transition-based Syntactic Linearization,2018,28,0,3,1,3601,linfeng song,Proceedings of the 11th International Conference on Natural Language Generation,0,"The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multilayer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task."
W18-2504,The {ACL} {A}nthology: Current State and Future Directions,2018,-1,-1,1,1,3945,daniel gildea,Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS}),0,"The Association of Computational Linguistic{'}s Anthology is the open source archive, and the main source for computational linguistics and natural language processing{'}s scientific literature. The ACL Anthology is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of Docker images will improve the Anthology{'}s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the Anthology. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards."
P18-2062,Orthographic Features for Bilingual Lexicon Induction,2018,0,4,2,0.925926,3944,parker riley,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as edit distance, which can be helpful for pairs of related languages. This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages."
P18-1150,A Graph-to-Sequence Model for {AMR}-to-Text Generation,2018,30,17,4,1,3601,linfeng song,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature."
P18-1171,Sequence-to-sequence Models for Cache Transition Systems,2018,0,6,3,1,24422,xiaochang peng,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we present a sequence-to-sequence based approach for mapping natural language sentences to AMR semantic graphs. We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system. To address the sparsity issue of neural AMR parsing, we feed feature embeddings from the transition state to provide relevant local information for each decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models."
N18-2090,Leveraging Context Information for Natural Question Generation,2018,0,37,5,1,3601,linfeng song,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"The task of natural question generation is to generate a corresponding question given the input passage (fact) and answer. It is useful for enlarging the training set of QA systems. Previous work has adopted sequence-to-sequence models that take a passage with an additional bit to indicate answer position as input. However, they do not explicitly model the information between answer and other context within the passage. We propose a model that matches the answer with the passage before generating the question. Experiments show that our model outperforms the existing state of the art using rich features."
J18-3006,Feature-Based Decipherment for Machine Translation,2018,15,3,3,1,30387,iftekhar naim,Computational Linguistics,0,"Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction (decipherment) for closely related language pairs. The existing decipherment models, however, are not well suited for exploiting these orthographic similarities. We propose a log-linear model with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed log-linear model. To address this challenge, we perform approximate inference via Markov chain Monte Carlo sampling and contrastive divergence. Our results show that the proposed log-linear model with contrastive divergence outperforms the existing generative decipherment models by exploiting the orthographic features. The model both scales to large vocabularies and preserves accuracy in low- and no-resource contexts."
J18-1003,A Notion of Semantic Coherence for Underspecified Semantic Representation,2018,-1,-1,2,1,30397,mehdi manshadi,Computational Linguistics,0,"The general problem of finding satisfying solutions to constraint-based underspecified representations of quantifier scope is NP-complete. Existing frameworks, including Dominance Graphs, Minimal Recursion Semantics, and Hole Semantics, have struggled to balance expressivity and tractability in order to cover real natural language sentences with efficient algorithms. We address this trade-off with a general principle of coherence, which requires that every variable introduced in the domain of discourse must contribute to the overall semantics of the sentence. We show that every underspecified representation meeting this criterion can be efficiently processed, and that our set of representations subsumes all previously identified tractable sets."
J18-1004,Cache Transition Systems for Graph Parsing,2018,-1,-1,1,1,3945,daniel gildea,Computational Linguistics,0,"Motivated by the task of semantic parsing, we describe a transition system that generalizes standard transition-based dependency parsing techniques to generate a graph rather than a tree. Our system includes a cache with fixed size m, and we characterize the relationship between the parameter m and the class of graphs that can be produced through the graph-theoretic concept of tree decomposition. We find empirically that small cache sizes cover a high percentage of sentences in existing semantic corpora."
J18-1005,Weighted {DAG} Automata for Semantic Graphs,2018,-1,-1,3,0,3180,david chiang,Computational Linguistics,0,"Graphs have a variety of uses in natural language processing, particularly as representations of linguistic meaning. A deficit in this area of research is a formal framework for creating, combining, and using models involving graphs that parallels the frameworks of finite automata for strings and finite tree automata for trees. A possible starting point for such a framework is the formalism of directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki and extended by Quernheim and Knight. In this article, we study the latter in depth, demonstrating several new results, including a practical recognition algorithm that can be used for inference and learning with models defined on DAG automata. We also propose an extension to graphs with unbounded node degree and show that our results carry over to the extended formalism."
D18-1246,N-ary Relation Extraction using Graph-State {LSTM},2018,0,14,4,1,3601,linfeng song,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Cross-sentence $n$-ary relation extraction detects relations among $n$ entities across multiple sentences. Typical methods formulate an input as a \textit{document graph}, integrating various intra-sentential and inter-sentential dependencies. The current state-of-the-art method splits the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though being able to model rich linguistic knowledge by leveraging graph edges, important information can be lost in the splitting procedure. We propose a graph-state LSTM model, which uses a parallel state to model each word, recurrently enriching state values via message passing. Compared with DAG LSTMs, our graph LSTM keeps the original graph structure, and speeds up computation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature."
W17-4729,{U}niversity of {R}ochester {WMT} 2017 {NMT} System Submission,2017,-1,-1,3,0,31620,chester holtz,Proceedings of the Second Conference on Machine Translation,0,None
P17-2002,{AMR}-to-text Generation with Synchronous Node Replacement Grammar,2017,34,10,5,1,3601,linfeng song,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result."
E17-1035,Addressing the Data Sparsity Issue in Neural {AMR} Parsing,2017,17,20,3,1,24422,xiaochang peng,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Neural attention models have achieved great success in different NLP tasks. However, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources."
S16-2009,Sense Embedding Learning for Word Sense Induction,2016,24,2,4,1,3601,linfeng song,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,"Conventional word sense induction (WSI) methods usually represent each instance with discrete linguistic features or cooccurrence features, and train a model for each polysemous word individually. In this work, we propose to learn sense embeddings for the WSI task. In the training stage, our method induces several sense centroids (embedding) for each polysemous word. In the testing stage, our method represents each instance as a contextual vector, and induces its sense by finding the nearest sense centroid in the embedding space. The advantages of our method are (1) distributed sense vectors are taken as the knowledge representations which are trained discriminatively, and usually have better performance than traditional count-based distributional models, and (2) a general model for the whole vocabulary is jointly trained to induce sense centroids under the mutlitask learning framework. Evaluated on SemEval-2010 WSI dataset, our method outperforms all participants and most of the recent state-of-the-art methods. We further verify the two advantages by comparing with carefully designed baselines."
S16-1183,{U}of{R} at {S}em{E}val-2016 Task 8: Learning Synchronous Hyperedge Replacement Grammar for {AMR} Parsing,2016,11,7,2,1,24422,xiaochang peng,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
J16-3003,Parsing Linear Context-Free Rewriting Systems with Fast Matrix Multiplication,2016,34,0,2,0,3318,shay cohen,Computational Linguistics,0,"We describe a recognition algorithm for a subset of binary linear context-free rewriting systems LCFRS with running time Onxcfx89d where Mm = Omxcfx89 is the running time for m xc3x97 m matrix multiplication and d is the contact rank of the LCFRS-the maximal number of combination and non-combination points that appear in the grammar rules. We also show that this algorithm can be used as a subroutine to obtain a recognition algorithm for general binary LCFRS with running time Onxcfx89d1. The currently best known xcfx89 is smaller than 2.38. Our result provides another proof for the best known result for parsing mildly context-sensitive formalisms such as combinatory categorial grammars, head grammars, linear indexed grammars, and tree-adjoining grammars, which can be parsed in time On4.76. It also shows that inversion transduction grammars can be parsed in time On5.76. In addition, binary LCFRS subsumes many other formalisms and types of grammars, for some of which we also improve the asymptotic complexity of parsing."
J16-2002,Synchronous Context-Free Grammars and Optimal Parsing Strategies,2016,37,0,1,1,3945,daniel gildea,Computational Linguistics,0,"The complexity of parsing with synchronous context-free grammars is polynomial in the sentence length for a fixed grammar, but the degree of the polynomial depends on the grammar. Specifically, the degree depends on the length of rules, the permutations represented by the rules, and the parsing strategy adopted to decompose the recognition of a rule into smaller steps. We address the problem of finding the best parsing strategy for a rule, in terms of space and time complexity. We show that it is NP-hard to find the binary strategy with the lowest space complexity. We also show that any algorithm for finding the strategy with the lowest time complexity would imply improved approximation algorithms for finding the treewidth of general graphs."
D16-1224,{AMR}-to-text generation as a Traveling Salesman Problem,2016,17,7,5,1,3601,linfeng song,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
N15-1017,Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments,2015,27,22,7,1,30387,iftekhar naim,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We address the problem of automatically aligning natural language sentences with corresponding video segments without any direct supervision. Most existing algorithms for integrating language with videos rely on handaligned parallel data, where each natural language sentence is manually aligned with its corresponding image or video segment. Recently, fully unsupervised alignment of text with video has been shown to be feasible using hierarchical generative models. In contrast to the previous generative models, we propose three latent-variable discriminative models for the unsupervised alignment task. The proposed discriminative models are capable of incorporating domain knowledge, by adding diverse and overlapping features. The results show that discriminative models outperform the generative models in terms of alignment accuracy."
K15-1004,A Synchronous Hyperedge Replacement Grammar based approach for {AMR} parsing,2015,15,42,3,1,24422,xiaochang peng,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,This paper presents a synchronous-graphgrammar-based approach for string-toAMR parsing. We apply Markov Chain Monte Carlo (MCMC) algorithms to learn Synchronous Hyperedge Replacement Grammar (SHRG) rules from a forest that represents likely derivations consistent with a fixed string-to-graph alignment. We make an analogy of string-toAMR parsing to the task of phrase-based machine translation and come up with an efficient algorithm to learn graph grammars from string-graph pairs. We propose an effective approximation strategy to resolve the complexity issue of graph compositions. We also show some useful strategies to overcome existing problems in an SHRG-based parser and present preliminary results of a graph-grammar-based approach.
P14-2039,Sliding Alignment Windows for Real-Time Crowd Captioning,2014,10,0,4,0,39133,mohammad kazemi,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The primary way of providing real-time speech to text captioning for hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom is able to type only part of what they hear. In this paper, we extend the state of the art fixed-window alignment algorithm (Naim et al., 2013) for combining the individual captions into a final output sequence. Our method performs alignment on a sliding window of the input sequences, drastically reducing both the number of errors and the latency of the system to the end user over the previously published approaches."
J14-1007,Sampling Tree Fragments from Forests,2014,36,7,3,1,2983,tagyoung chung,Computational Linguistics,0,"We study the problem of sampling trees from forests, in the setting where probabilities for each tree may be a function of arbitrarily large tree fragments. This setting extends recent work for sampling to learn Tree Substitution Grammars to the case where the tree structure TSG derived tree is not fixed. We develop a Markov chain Monte Carlo algorithm which corrects for the bias introduced by unbalanced forests, and we present experiments using the algorithm to learn Synchronous Context-Free Grammar rules for machine translation. In this application, the forests being sampled represent the set of Hiero-style rules that are consistent with fixed input word-level alignments. We demonstrate equivalent machine translation performance to standard techniques but with much smaller grammars."
D14-1180,Type-based {MCMC} for Sampling Tree Fragments from Forests,2014,19,5,2,1,24422,xiaochang peng,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"This paper applies type-based Markov Chain Monte Carlo (MCMC) algorithms to the problem of learning Synchronous Context-Free Grammar (SCFG) rules from a forest that represents all possible rules consistent with a fixed word alignment. While type-based MCMC has been shown to be effective in a number of NLP applications, our setting, where the tree structure of the sentence is itself a hidden variable, presents a number of challenges to type-based inference. We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges. These methods lead to improvements in both log likelihood and BLEU score in our experiments."
D14-1188,Comparing Representations of Semantic Roles for String-To-Tree Decoding,2014,19,2,2,1,40156,marzieh bazrafshan,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We introduce new features for incorporating semantic predicate-argument structures in machine translation (MT). The methods focus on the completeness of the semantic structures of the translations, as well as the order of the translated semantic roles. We experiment with translation rules which contain the core arguments for the predicates in the source side of a MT system, and observe that using these rules significantly improves the translation quality. We also present a new semantic feature that resembles a language model. Our results show that the language model feature can also significantly improve MT results."
W13-2262,{M}ulti-{R}ate {HMM}s for Word Alignment,2013,28,1,2,0,40960,elif eyigoz,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We apply multi-rate HMMs, a tree structured HMM model, to the word-alignment problem. Multi-rate HMMs allow us to model reordering at both the morpheme level and the word level in a hierarchical fashion. This approach leads to better machine translation results than a morphemeaware model that does not explicitly model morpheme reordering."
P13-2074,Semantic Roles for String to Tree Machine Translation,2013,21,17,2,1,40156,marzieh bazrafshan,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al. (2004). We compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure. Our results demonstrate that the second approach is effective in increasing the quality of translations."
P13-1007,"Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation",2013,26,4,2,1,30397,mehdi manshadi,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary number and type of noun phrases. No corpusbased method, however, has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation. In this paper we report early, though promising, results for automatic QSD when handling both phenomena. We also present a general model for learning to build partial orders from a set of pairwise preferences. We give ann logn algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice. Finally, we significantly improve the performance of the previous model using a rich set of automatically generated features."
N13-1004,Simultaneous Word-Morpheme Alignment for Statistical Machine Translation,2013,18,6,2,0,40960,elif eyigoz,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Current word alignment models for statistical machine translation do not address morphology beyond merely splitting words. We present a two-level alignment model that distinguishes between words and morphemes, in which we embed an IBM Model 1 inside an HMM based word alignment model. The model jointly induces word and morpheme alignments using an EM algorithm. We evaluated our model on Turkish-English parallel data. We obtained significant improvement of BLEU scores over IBM Model 4. Our results indicate that utilizing information from morphology improves the quality of word alignments."
N13-1020,Text Alignment for Real-Time Crowd Captioning,2013,26,18,2,1,30387,iftekhar naim,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear. In this paper, we describe an improved method for combining partial captions into a final output based on weighted A search and multiple sequence alignment (MSA). In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds. Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%). The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem."
P12-2060,Improving the {IBM} Alignment Models Using Variational {B}ayes,2012,13,18,2,0,42667,darcey riley,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score."
N12-1062,Tuning as Linear Regression,2012,10,10,3,1,40156,marzieh bazrafshan,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a tuning method for statistical machine translation, based on the pairwise ranking approach. Hopkins and May (2011) presented a method that uses a binary classifier. In this work, we use linear regression and show that our approach is as effective as using a binary classifier and converges faster."
J12-3008,On the String Translations Produced by Multi Bottom{--}Up Tree Transducers,2012,26,4,1,1,3945,daniel gildea,Computational Linguistics,0,"Tree transducers are defined as relations between trees, but in syntax-based machine translation, we are ultimately concerned with the relations between the strings at the yields of the input and output trees. We examine the formal power of Multi Bottom-Up Tree Transducers from this point of view."
P11-2070,Terminal-Aware Synchronous Binarization,2011,15,2,3,1,39982,licheng fang,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,We present an SCFG binarization algorithm that combines the strengths of early terminal matching on the source language side and early language model integration on the target language side. We also examine how different strategies of target-side terminal attachment during binarization can significantly affect translation quality.
P11-2072,Issues Concerning Decoding with Synchronous Context-free Grammar,2011,15,13,3,1,2983,tagyoung chung,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,We discuss some of the practical issues that arise from decoding with general synchronous context-free grammars. We examine problems caused by unary rules and we also examine how virtual nonterminals resulting from binarization can best be handled. We also investigate adding more flexibility to synchronous context-free grammars by adding glue rules and phrases.
P11-1046,Optimal Head-Driven Parsing Complexity for Linear Context-Free Rewriting Systems,2011,25,6,2,0,44662,pierluigi crescenzi,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,We study the problem of finding the best head-driven parsing strategy for Linear Context-Free Rewriting System productions. A head-driven strategy must begin with a specified righthand-side nonterminal (the head) and add the remaining nonterminals one at a time in any order. We show that it is NP-hard to find the best head-driven strategy in terms of either the time or space complexity of parsing.
J11-1008,Grammar Factorization by Tree Decomposition,2011,38,13,1,1,3945,daniel gildea,Computational Linguistics,0,"We describe the application of the graph-theoretic property known as treewidth to the problem of finding efficient parsing algorithms. This method, similar to the junction tree algorithm used in graphical models for machine learning, allows automatic discovery of efficient algorithms such as the O(n4) algorithm for bilexical grammars of Eisner and Satta. We examine the complexity of applying this method to parsing algorithms for general Linear Context-Free Rewriting Systems. We show that any polynomial-time algorithm for this problem would imply an improved approximation algorithm for the well-studied treewidth problem on general graphs."
2011.iwslt-evaluation.20,{SCFG} latent annotation for machine translation,2011,20,1,3,1,2983,tagyoung chung,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We discuss learning latent annotations for synchronous context-free grammars (SCFG) for the purpose of improving machine translation. We show that learning annotations for nonterminals results in not only more accurate translation, but also faster SCFG decoding."
W10-1406,Factors Affecting the Accuracy of {K}orean Parsing,2010,27,14,3,1,2983,tagyoung chung,Proceedings of the {NAACL} {HLT} 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"We investigate parsing accuracy on the Korean Treebank 2.0 with a number of different grammars. Comparisons among these grammars and to their English counterparts suggest different aspects of Korean that contribute to parsing difficulty. Our results indicate that the coarseness of the Treebank's nonterminal set is a even greater problem than in the English Treebank. We also find that Korean's relatively free word order does not impact parsing results as much as one might expect, but in fact the prevalence of zero pronouns accounts for a large portion of the difference between Korean and English parsing scores."
N10-1118,Optimal Parsing Strategies for Linear Context-Free Rewriting Systems,2010,12,17,1,1,3945,daniel gildea,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Factorization is the operation of transforming a production in a Linear Context-Free Rewriting System (LCFRS) into two simpler productions by factoring out a subset of the nonterminals on the production's righthand side. Factorization lowers the rank of a production but may increase its fan-out. We show how to apply factorization in order to minimize the parsing complexity of the resulting grammar, and study the relationship between rank, fan-out, and parsing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out. Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees."
D10-1058,A Fast Fertility Hidden {M}arkov Model for Word Alignment Using {MCMC},2010,14,12,2,0,46399,shaojun zhao,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4."
D10-1062,Effects of Empty Categories on Machine Translation,2010,18,38,2,1,2983,tagyoung chung,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We examine effects that empty categories have on machine translation. Empty categories are elements in parse trees that lack corresponding overt surface forms (words) such as dropped pronouns and markers for control constructions. We start by training machine translation systems with manually inserted empty elements. We find that inclusion of some empty categories in training data improves the translation result. We expand the experiment by automatically inserting these elements into a larger data set using various methods and training on the modified corpus. We show that even when automatic prediction of null elements is not highly accurate, it nevertheless improves the end translation result."
C10-1081,Semantic Role Features for Machine Translation,2010,28,67,2,1,41434,ding liu,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We propose semantic role features for a Tree-to-String transducer to model the reordering/deletion of source-side semantic roles. These semantic features, as well as the Tree-to-String templates, are trained based on a conditional log-linear model and are shown to significantly outperform systems trained based on Max-Likelihood and EM. We also show significant improvement in sentence fluency by using the semantic role features in the log-linear model, based on manual evaluation."
W09-3815,Weight Pushing and Binarization for Fixed-Grammar Parsing,2009,14,8,2,1,9757,matt post,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"We apply the idea of weight pushing (Mohri, 1997) to CKY parsing with fixed context-free grammars. Applied after rule binarization, weight pushing takes the weight from the original grammar rule and pushes it down across its binarized pieces, allowing the parser to make better pruning decisions earlier in the parsing process. This process can be viewed as generalizing weight pushing from transducers to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars."
P09-2012,{B}ayesian Learning of a Tree Substitution Grammar,2009,9,61,2,1,9757,matt post,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn. Past approaches have resorted to heuristics. In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size. The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy."
J09-4009,Binarization of Synchronous Context-Free Grammars,2009,32,54,3,0.248248,8438,liang huang,Computational Linguistics,0,"Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages. We develop a theory of binarization for synchronous context-free grammars and present a linear-time algorithm for binarizing synchronous rules when possible. In our large-scale experiments, we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. We also discuss the more general, and computationally more difficult, problem of finding good parsing strategies for non-binarizable rules, and present an approximate polynomial-time algorithm for this problem."
D09-1075,Unsupervised Tokenization for Machine Translation,2009,19,42,2,1,2983,tagyoung chung,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Training a statistical machine translation starts with tokenizing a parallel corpus. Some languages such as Chinese do not incorporate spacing in their writing system, which creates a challenge for tokenization. Moreover, morphologically rich languages such as Korean present an even bigger challenge, since optimal token boundaries for machine translation in these languages are often unclear. Both rule-based solutions and statistical solutions are currently used. In this paper, we present unsupervised methods to solve tokenization problem. Our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation."
D09-1136,{B}ayesian Learning of Phrasal Tree-to-String Templates,2009,23,7,2,1,41434,ding liu,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"We examine the problem of overcoming noisy word-level alignments when learning tree-to-string translation rules. Our approach introduces new rules, and re-estimates rule probabilities using EM. The major obstacles to this approach are the very reasons that word-alignments are used for rule extraction: the huge space of possible rules, as well as controlling overfitting. By carefully controlling which portions of the original alignments are reanalyzed, and by using Bayesian inference during re-analysis, we show significant improvement over the baseline rules extracted from word-level alignments."
W08-0308,Improved Tree-to-String Transducer for Machine Translation,2008,22,22,2,1,41434,ding liu,Proceedings of the Third Workshop on Statistical Machine Translation,0,"We propose three enhancements to the tree-to-string (TTS) transducer for machine translation: first-level expansion-based normalization for TTS templates, a syntactic alignment framework integrating the insertion of unaligned target words, and subtree-based n-gram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU-4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system."
P08-1012,{B}ayesian Learning of Non-Compositional Phrases with Synchronous Parsing,2008,17,63,4,1,7671,hao zhang,Proceedings of ACL-08: HLT,1,"We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs. The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large. Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results. Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches."
P08-1025,Efficient Multi-Pass Decoding for Synchronous Context Free Grammars,2008,12,21,2,1,7671,hao zhang,Proceedings of ACL-08: HLT,1,"We take a multi-pass approach to machine translation decoding when using synchronous context-free grammars as the translation model and n-gram language models: the first pass uses a bigram language model, and the resulting parse forest is used in the second pass to guide search with a trigram language model. The trigram pass closes most of the performance gap between a bigram decoder and a much slower trigram decoder, but takes time that is insignificant in comparison to the bigram pass. An additional fast decoding pass maximizing the expected count of correct translation hypotheses increases the BLEU score significantly."
C08-1136,Extracting Synchronous Grammar Rules From Word-Level Alignments in Linear Time,2008,10,39,2,1,7671,hao zhang,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"We generalize Uno and Yagiura's algorithm for finding all common intervals of two permutations to the setting of two sequences with many-to-many alignment links across the two sides. We show how to maximally decompose a word-aligned sentence pair in linear time, which can be used to generate all possible phrase pairs or a Synchronous Context-Free Grammar (SCFG) with the simplest rules possible. We also use the algorithm to precisely analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs."
2008.amta-papers.16,Parsers as language models for statistical machine translation,2008,20,21,2,1,9757,matt post,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Most work in syntax-based machine translation has been in translation modeling, but there are many reasons why we may instead want to focus on the language model. We experiment with parsers as language models for machine translation in a simple translation model. This approach demands much more of the language models, allowing us to isolate their strengths and weaknesses. We find that unmodified parsers do not improve BLEU scores over ngram language models, and provide an analysis of their strengths and weaknesses."
W07-0404,Factorization of Synchronous Context-Free Grammars in Linear Time,2007,0,0,2,1,7671,hao zhang,"Proceedings of {SSST}, {NAACL}-{HLT} 2007 / {AMTA} Workshop on Syntax and Structure in Statistical Translation",0,None
P07-1024,Optimizing Grammars for Minimum Dependency Length,2007,10,22,1,1,3945,daniel gildea,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We examine the problem of choosing word order for a set of dependency trees so as to minimize total dependency length. We present an algorithm for computing the optimal layout of a single tree as well as a numerical method for optimizing a grammar of orderings over a set of dependency types. A grammar generated by minimizing dependency length in unordered trees from the Penn Treebank is found to agree surprisingly well with English word order, suggesting that dependency length minimization has influenced the evolution of English."
N07-1006,Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation,2007,10,26,2,1,41434,ding liu,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"We propose three new features for MT evaluation: source-sentence constrained n-gram precision, source-sentence reordering metrics, and discriminative unigram precision, as well as a method of learning linear feature weights to directly maximize correlation with human judgments. By aligning both the hypothesis and the reference with the sourcelanguage sentence, we achieve better correlation with human judgments than previously proposed metrics. We further improve performance by combining individual evaluation metrics using maximum correlation training, which is shown to be better than the classification-based framework."
N07-1019,Worst-Case Synchronous Grammar Rules,2007,7,11,1,1,3945,daniel gildea,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"We relate the problem of finding the best application of a Synchronous ContextFree Grammar (SCFG) rule during parsing to a Markov Random Field. This representation allows us to use the theory of expander graphs to show that the complexity of SCFG parsing of an input sentence of length N is ( N cn ), for a grammar with maximum rule length n and some constant c. This improves on the previous best result of ( N c xe2x88x9a n"
W06-1627,Efficient Search for Inversion Transduction Grammar,2006,12,8,2,1,7671,hao zhang,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We develop admissible A* search heuristics for synchronous parsing with Inversion Transduction Grammar, and present results both for bitext alignment and for machine translation decoding. We also combine the dynamic programming hook trick with A* search for decoding. These techniques make it possible to find optimal alignments much more quickly, and make it possible to find optimal translations for the first time. Even in the presence of pruning, we are able to achieve higher BLEU scores with the same amount of computation."
P06-2036,Factoring Synchronous Grammars by Sorting,2006,9,13,1,1,3945,daniel gildea,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Synchronous Context-Free Grammars (SCFGs) have been successfully exploited as translation models in machine translation applications. When parsing with an SCFG, computational complexity grows exponentially with the length of the rules, in the worst case. In this paper we examine the problem of factorizing each rule of an input SCFG to a generatively equivalent set of rules, each having the smallest possible length. Our algorithm works in time O(n log n), for each rule of length n. This improves upon previous results and solves an open problem about recognizing permutations that can be factored."
P06-2070,Stochastic Iterative Alignment for Machine Translation Evaluation,2006,11,12,2,1,41434,ding liu,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"A number of metrics for automatic evaluation of machine translation have been proposed in recent years, with some metrics focusing on measuring the adequacy of MT output, and other metrics focusing on fluency. Adequacy-oriented metrics such as BLEU measure xcexb7-gram overlap of MT outputs and their references, but do not represent sentence-level information. In contrast, fluency-oriented metrics such as ROUGE-W compute longest common subsequences, but ignore words not aligned by the LCS. We propose a metric based on stochastic iterative string alignment (SIA), which aims to combine the strengths of both approaches. We compare SIA with existing metrics, and find that it outperforms them in overall evaluation, and works specially well in fluency evaluation."
P06-2122,Inducing Word Alignments with Bilexical Synchronous Trees,2006,13,5,2,1,7671,hao zhang,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,This paper compares different bilexical tree-based models for bilingual alignment. EM training for the new model benefits from the dynamic programming hook trick. The model produces improved dependency structure for both languages.
N06-1033,Synchronous Binarization for Machine Translation,2006,12,142,3,1,7671,hao zhang,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages, and rules extracted from parallel corpora can be quite large. We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system."
W05-1507,Machine Translation as Lexicalized Parsing with Hooks,2005,6,29,3,0.454545,8438,liang huang,Proceedings of the Ninth International Workshop on Parsing Technology,0,We adapt the hook trick for speeding up bilexical parsing to the decoding problem for machine translation models that are based on combining a synchronous context free grammar as the translation model with an n-gram language model. This dynamic programming technique yields lower complexity algorithms than have previously been described for an important class of translation models.
W05-1526,Online Statistics for a Unification-Based Dialogue Parser,2005,10,3,4,0,1344,micha elsner,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We describe a method for augmenting unification-based deep parsing with statistical methods. We extend and adapt the Bikel parser, which uses head-driven lexical statistics, to dialogue. We show that our augmented parser produces significantly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences."
W05-0904,Syntactic Features for Evaluation of Machine Translation,2005,10,143,2,1,41434,ding liu,Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,0,"Automatic evaluation of machine translation, based on computing n-gram similarity between system output and human reference translations, has revolutionized the development of MT systems. We explore the use of syntactic information, including constituent labels and head-modier dependencies, in computing similarity between output and reference. Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments."
P05-1059,Stochastic Lexicalized Inversion Transduction Grammar for Alignment,2005,9,76,2,1,7671,hao zhang,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training. Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences."
J05-1004,The {P}roposition {B}ank: An Annotated Corpus of Semantic Roles,2005,54,1559,2,0,4859,martha palmer,Computational Linguistics,0,"The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus.n n We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ''trace'' categories of the treebank."
W04-3228,Dependencies vs. Constituents for Tree-Based Alignment,2004,12,20,1,1,3945,daniel gildea,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
N04-1021,A Smorgasbord of Features for Statistical Machine Translation,2004,12,254,2,0,37712,franz och,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation.
C04-1055,Skeletons in the parser: Using a shallow parser to improve deep parsing,2004,18,14,3,0.714286,41163,mary swift,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,We describe a simple approach for integrating shallow and deep parsing. We use phrase structure bracketing obtained from the Collins parser as filters to guide deep parsing. Our experiments demonstrate that our technique yields substantial gains in speed along with modest improvements in accuracy.
C04-1060,Syntax-Based Alignment: Supervised or Unsupervised?,2004,16,30,2,1,7671,hao zhang,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other. The trees may be learned directly from parallel corpora (Wu, 1997), or provided by a parser trained on hand-annotated treebanks (Yamada and Knight, 2001). In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data."
W03-1008,Identifying Semantic Roles Using {C}ombinatory {C}ategorial {G}rammar,2003,12,97,1,1,3945,daniel gildea,Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,0,"We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar. This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles."
P03-1011,Loosely Tree-Based Alignment for Machine Translation,2003,14,190,1,1,3945,daniel gildea,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms."
2003.mtsummit-papers.13,An algorithm for word-level alignment of parallel dependency trees,2003,13,23,2,0,50712,yuan ding,Proceedings of Machine Translation Summit IX: Papers,0,"Structural divergence presents a challenge to the use of syntax in statistical machine translation. We address this problem with a new algorithm for alignment of loosely matched non-isomorphic dependency trees. The algorithm selectively relaxes the constraints of the two tree structures while keeping computational complexity polynomial in the length of the sentences. Experimentation with a large Chinese-English corpus shows an improvement in alignment results over the unstructured models of (Brown et al., 1993)."
P02-1031,The Necessity of Parsing for Predicate Argument Recognition,2002,14,180,1,1,3945,daniel gildea,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter chunked representation of the input can be as effective for the purposes of semantic role identification."
J02-3001,Automatic Labeling of Semantic Roles,2002,41,1354,1,1,3945,daniel gildea,Computational Linguistics,0,"We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as AGENT or PATIENT, or more domain-specific semantic roles, such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data."
C02-1132,Probabilistic Models of Verb-Argument Structure,2002,19,22,1,1,3945,daniel gildea,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We evaluate probabilistic models of verb argument structure trained on a corpus of verbs and their syntactic arguments. Models designed to represent patterns of verb alternation behavior are compared with generic clustering models in terms of the perplexity assigned to held-out test data. While the specialized models of alternation do not perform as well, closer examination reveals alternation behavior represented implicitly in the generic models."
W01-0521,Corpus Variation and Parser Performance,2001,15,255,1,1,3945,daniel gildea,Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,0,None
P00-1065,Automatic Labeling of Semantic Roles,2000,40,62,1,1,3945,daniel gildea,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data."
J96-4003,Learning Bias and Phonological-Rule Induction,1996,42,70,1,1,3945,daniel gildea,Computational Linguistics,0,"A fundamental debate in the machine learning of language has been the role of prior knowledge in the learning process. Purely nativist approaches, such as the Principles and Parameters model, build parameterized linguistic generalizations directly into the learning system. Purely empirical approaches use a general, domain-independent learning rule (Error Back-Propagation, Instance-based Generalization, Minimum Description Length) to learn linguistic generalizations directly from the data.In this paper we suggest that an alternative to the purely nativist or purely empiricist learning paradigms is to represent the prior knowledge of language as a set of abstract learning biases, which guide an empirical inductive learning algorithm. We test our idea by examining the machine learning of simple Sound Pattern of English (SPE)-style phonological rules. We represent phonological rules as finite-state transducers that accept underlying forms as input and generate surface forms as output. We show that OSTIA, a general-purpose transducer induction algorithm, was incapable of learning simple phonological rules like flapping. We then augmented OSTIA with three kinds of learning biases that are specific to natural language phonology, and that are assumed explicitly or implicitly by every theory of phonology: faithfulness (underlying segments tend to be realized similarly on the surface), community (similar segments behave similarly), and context (phonological rules need access to variable in their context). These biases are so fundamental to generative phonology that they are left implicit in many theories. But explicitly modifying the OSTIA algorithm with these biases allowed it to learn more compact, accurate, and general transducers, and our implementation successfully learns a number of rules from English and German. Furthermore, we show that some of the remaining errors in our augmented model are due to implicit biases in the traditional SPE-style rewrite system that are not similarly represented in the transducer formalism, suggesting that while transducers may be formally equivalent to SPE-style rules, they may not have identical evaluation procedures.Because our biases were applied to the learning of very simple SPE-style rules, and to a non-psychologically-motivated and nonprobabilistic theory of purely deterministic transducers, we do not expect that our model as implemented has any practical use as a phonological learning device, nor is it intended as a cognitive model of human learning. Indeed, because of the noise and nondeterminism inherent to linguistic data, we feel strongly that stochastic algorithms for language induction are much more likely to be a fruitful research direction. Our model is rather intended to suggest the kind of biases that may be added to other empiricist induction models, and the way in which they may be added, in order to build a cognitively and computationally plausible learning model for phonological rules."
P95-1002,Automatic Induction of Finite State Transducers for Simple Phonological Rules,1995,14,22,1,1,3945,daniel gildea,33rd Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a method for learning phonological rules from sample pairs of underlying and surface forms, without negative evidence. The learned rules are represented as finite state transducers that accept underlying forms as input and generate surface forms as output. The algorithm for learning them is an extension of the OSTIA algorithm for learning general subsequential finite state transducers. Although OSTIA is capable of learning arbitrary s.f.s.t's in the limit, large dictionaries of actual English pronunciations did not give enough samples to correctly induce phonological rules. We then augmented OSTIA with two kinds of knowledge specific to natural language phonology, biases from universal grammar. One bias is that underlying phones are often realized as phonetically similar or identical surface phones. The other biases phonological rules to apply across natural phonological classes. The additions helped in learning more compact, accurate, and general transducers than the unmodified OSTIA algorithm. An implementation of the algorithm successfully learns a number of English postlexical rules."
