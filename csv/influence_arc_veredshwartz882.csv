2020.acl-tutorials.7,N19-1112,0,0.133488,"ecent years, yielding multiple exploratory research directions into automated commonsense understanding. Recent efforts to acquire and represent common knowledge resulted in large knowledge graphs, acquired through extractive methods (Speer et al., 2017) or crowdsourcing (Sap et al., 2019a). Simultaneously, a large body of work in integrating reasoning capabilities into downstream tasks has emerged, allowing the development of smarter dialogue (Zhou et al., 2018) and question answering agents (Xiong et al., 2019). Recent advances in large pretrained language models (e.g., Devlin et al., 2019; Liu et al., 2019b), however, have pushed machines closer to humanlike understanding capabilities, calling into question whether machines should directly model commonsense through symbolic integrations. But despite these impressive performance improvements in a variety of NLP tasks, it remains unclear whether these models are performing complex reasoning, or if they are merely learning complex surface correlation patterns (Davis and Marcus, 2015; Marcus, 2018). This difficulty in measuring the progress in commonsense reasoning using downstream tasks has yielded increased efforts at developing robust benchmarks"
2020.acl-tutorials.7,N18-1202,0,0.0350715,"(e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level semantic tasks such as named entity recognition, coreference resolution and semantic role labeling (Tenney et al., 2019b; Liu et al., 2019a). We will discuss recent investigations into pretrained LMs’ ability to capture world knowledge (Petroni et al., 2019; Logan et al., 2019) and learn or reason about commonsense (Feldman et al., 2019). 3 How to incorporate commonsense knowledge into downstream models? Given that large number of NLP applications are designed to require commonsense reasoning, we will review efforts to"
2020.acl-tutorials.7,D19-1250,0,0.0459438,"Missing"
2020.acl-tutorials.7,N19-1421,0,0.249254,"tanding (NLU) tasks hardly require machines to reason about commonsense (Lo Bue and Yates, 2011; Schwartz et al., 2017). This prompted efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging,"
2020.acl-tutorials.7,P19-1487,0,0.0181475,"018; Paul and Frank, 2019; Wang et al., 2018) tasks. For applications without available structured knowledge bases, researchers have relied on commonsense aggregated from corpus statistics pulled from unstructured text (Tandon et al., 2018; Lin et al., 2017; Li et al., 2018; Banerjee et al., 2019). More recently, rather than providing relevant commonsense as an additional input to neural networks, researchers have looked into indirectly encoding commonsense knowledge into the parameters of neural networks through pretraining on commonsense knowledge bases (Zhong et al., 2018) or explanations (Rajani et al., 2019), or by using multi-task objectives with commonsense relation prediction (Xia et al., 2019). mans acquire it (Moore, 2013; Baron-Cohen et al., 1985). We will discuss notions of social commonsense (Burke, 1969; Goldman, 2015) and physical commonsense (Hayes, 1978; McRae et al., 2005). We will cover the differences between taxonomic and inferential knowledge (Davis and Marcus, 2015; Pearl and Mackenzie, 2018), and differentiate commonsense knowledge from related concepts (e.g., script learning; Schank and Abelson, 1975; Chambers and Jurafsky, 2008). How to represent commonsense? We will review e"
2020.acl-tutorials.7,D18-1006,1,0.844121,"the 58th Annual Meeting of the Association for Computational Linguistics, pages 27–33 c July 5, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 (Guan et al., 2018), dialogue (Zhou et al., 2018), QA (Mihaylov and Frank, 2018; Bauer et al., 2018; Lin et al., 2019; Weissenborn et al., 2017; Musa et al., 2019), and classification (Chen et al., 2018; Paul and Frank, 2019; Wang et al., 2018) tasks. For applications without available structured knowledge bases, researchers have relied on commonsense aggregated from corpus statistics pulled from unstructured text (Tandon et al., 2018; Lin et al., 2017; Li et al., 2018; Banerjee et al., 2019). More recently, rather than providing relevant commonsense as an additional input to neural networks, researchers have looked into indirectly encoding commonsense knowledge into the parameters of neural networks through pretraining on commonsense knowledge bases (Zhong et al., 2018) or explanations (Rajani et al., 2019), or by using multi-task objectives with commonsense relation prediction (Xia et al., 2019). mans acquire it (Moore, 2013; Baron-Cohen et al., 1985). We will discuss notions of social commonsense (Burke, 1969; Goldman,"
2020.acl-tutorials.7,P18-1213,1,0.900176,"Missing"
2020.acl-tutorials.7,P19-1452,0,0.0164363,"adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level semantic tasks such as named entity recognition, coreference resolution and semantic role labeling (Tenney et al., 2019b; Liu et al., 2019a). We will discuss recent investigations into pretrained LMs’ ability to capture world knowledge (Petroni et al., 2019; Logan et al., 2019) and learn or reason about commonsense"
2020.acl-tutorials.7,P18-1043,1,0.87151,"Missing"
2020.acl-tutorials.7,S12-1052,0,0.0813807,"Missing"
2020.acl-tutorials.7,D19-1454,1,0.92721,"seamlessly (Apperly, 2010). Yet, endowing machines with such human-like commonsense reasoning capabilities has remained an elusive goal of artificial intelligence research for decades (Gunning, 2018). Commonsense knowledge and reasoning have received renewed attention from the natural language processing (NLP) community in recent years, yielding multiple exploratory research directions into automated commonsense understanding. Recent efforts to acquire and represent common knowledge resulted in large knowledge graphs, acquired through extractive methods (Speer et al., 2017) or crowdsourcing (Sap et al., 2019a). Simultaneously, a large body of work in integrating reasoning capabilities into downstream tasks has emerged, allowing the development of smarter dialogue (Zhou et al., 2018) and question answering agents (Xiong et al., 2019). Recent advances in large pretrained language models (e.g., Devlin et al., 2019; Liu et al., 2019b), however, have pushed machines closer to humanlike understanding capabilities, calling into question whether machines should directly model commonsense through symbolic integrations. But despite these impressive performance improvements in a variety of NLP tasks, it rem"
2020.acl-tutorials.7,K17-1004,1,0.795345,"lection and representation (e.g., automatic extraction; Etzioni et al., 2008; Zhang et al., 2016; Elazar et al., 2019). We will cover recent approaches that use natural language to represent commonsense (Speer et al., 2017; Sap et al., 2019a), and while noting the challenges that come with using datadriven methods (Gordon and Van Durme, 2013; Jastrzebski et al., 2018). How to measure machines’ ability of commonsense reasoning? We will explain that, despite their design, many natural language understanding (NLU) tasks hardly require machines to reason about commonsense (Lo Bue and Yates, 2011; Schwartz et al., 2017). This prompted efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabil"
2020.acl-tutorials.7,P19-1417,0,0.019746,"d reasoning have received renewed attention from the natural language processing (NLP) community in recent years, yielding multiple exploratory research directions into automated commonsense understanding. Recent efforts to acquire and represent common knowledge resulted in large knowledge graphs, acquired through extractive methods (Speer et al., 2017) or crowdsourcing (Sap et al., 2019a). Simultaneously, a large body of work in integrating reasoning capabilities into downstream tasks has emerged, allowing the development of smarter dialogue (Zhou et al., 2018) and question answering agents (Xiong et al., 2019). Recent advances in large pretrained language models (e.g., Devlin et al., 2019; Liu et al., 2019b), however, have pushed machines closer to humanlike understanding capabilities, calling into question whether machines should directly model commonsense through symbolic integrations. But despite these impressive performance improvements in a variety of NLP tasks, it remains unclear whether these models are performing complex reasoning, or if they are merely learning complex surface correlation patterns (Davis and Marcus, 2015; Marcus, 2018). This difficulty in measuring the progress in commonse"
2020.acl-tutorials.7,Q19-1027,1,0.836638,"onsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level semantic tasks such as named entity recognition, coreference resolution and semantic role labeling (Tenney et al., 2019b; Liu et al., 2019a). We will discuss recent investigations into pretrained LMs’ ability to capture world knowledge (Petroni et al., 2019; Logan et al., 2019) and learn or reason about commonsense (Feldman et al., 2019). 3 How to incorporate commonsense knowledge into downstream models? Given that large number of NLP applications are designed to require commonsense reasoning, we will review efforts to integrate such knowledge into NLP tasks. Vario"
2020.acl-tutorials.7,P19-1472,1,0.846992,"ardly require machines to reason about commonsense (Lo Bue and Yates, 2011; Schwartz et al., 2017). This prompted efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreemen"
2020.acl-tutorials.7,2020.emnlp-main.373,1,0.893321,"Missing"
2020.acl-tutorials.7,D19-1332,1,0.837743,"ed efforts in creating benchmarks carefully designed to be impossible to solve without commonsense knowledge (Roemmele et al., 2011; Levesque, 2011). In response, recent work has focused on using crowdsourcing and automatic filtering to design large-scale benchmarks while maintaining negative examples that are adversarial to machines (Zellers et al., 2018). We will review recent benchmarks that have emerged to assess whether machines have acquired physical (e.g., Talmor et al., 2019; Zellers et al., 2019), social (e.g., Sap et al., 2019b), or temporal commonsense reasoning capabilities (e.g., Zhou et al., 2019), as well as benchmarks that combine commonsense abilities with other tasks (e.g., reading comprehension; Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). What do machines know? Pretrained language models (LMs) have recently been described as “rediscovering the NLP pipeline” (Tenney et al., 2019a), i.e. replacing previous dedicated components of the traditional NLP pipeline, starting from low- and mid-level syntactic and semantic tasks (POS tagging, parsing, verb agreement, e.g., Peters et al., 2018; Jawahar et al., 2019; Shwartz and Dagan, 2019, inter alia), to high-level sema"
2020.emnlp-main.373,D18-1454,0,0.0293621,"and Jiang, 2019), retrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). Alternative approaches include using the knowledge to score answer candidates and prune implausible ones (Lin et al., 2017; Tandon et al., 2018), and training in a multi-task setup via auxiliary tasks pertaining to knowledge (Xia et al., 2019). To the best of our knowledge, our method is the first to generate knowledge from pre-trained language models and incorporate it as external knowledge into a question answering model. Concurrently, Latcinnik and Berant (2020) used one language model to generate hypotheses and another language model as an answer"
2020.emnlp-main.373,A83-1012,0,0.531744,"Missing"
2020.emnlp-main.373,P19-1470,1,0.793934,"ate the knowledge into the model as text, we convert each ConceptNet relation to a natural language template as in Davison et al. (2019). We limit the path length to 2 edges in order to maintain high precision. Corpus. For pairs of words from the context and question and from the answer choices, we extract their joint occurrences (with minimum frequency of 100) in Google N-grams (Brants and Franz, 2006). This yields text fragments of up to 5 words rather than well-formed sentences, with the potential of describing the relationship between the two words (Shwartz and Dagan, 2018). COMeT. COMeT (Bosselut et al., 2019) is a knowledge base construction model trained on the ATOMIC resource (Sap et al., 2019a) which consists of everyday situations along with multiple commonsense dimensions such as their causes, effects, pre- and post-conditions, etc. We generate all the dimensions unless we can generate specific relations that are more likely to help. Specifically, in Social IQa, we heuristically try to understand which type of relation in COMeT the question asks for. In COPA, we use the pre-condition relations for cause questions (xIntent, xNeed) and the postcondition relations for effect questions (xEffect,"
2020.emnlp-main.373,D19-1109,0,0.295471,"ad schema challenge (Levesque et al., 2012). Most current NLU models rely on pretrained language models (LMs; e.g. Radford et al., 2019; Devlin et al., 2019; Raffel et al., 2020). The standard practice is to fine-tune a pre-trained LM in a supervised manner on task-specific data. Alternatively, LM score is used to rank answer choices in a zero-shot setup (Wang et al., 2019; Bosselut and Choi, 2019). In both setups, pre-trained LMs yield improved performance upon prior methods, greatly due to the world knowledge that such LMs capture, having been trained on massive texts (Petroni et al., 2019; Davison et al., 2019). Despite the performance boost, LMs as knowledge providers suffer from various shortcomings: (i) insufficient coverage: due to reporting bias, many trivial facts might not be captured by LMs because they are rarely written about (Gordon and Van Durme, 2013). (ii) insufficient precision: the distributional training objective increases the probability of non-facts that are semantically similar to true facts, as in negation (“birds cannot fly”; Kassner and Sch¨utze, 2020). LMs excel in predicting the semantic category of a missing word, but might predict the wrong instance in that category (e.g."
2020.emnlp-main.373,N19-1423,0,0.269192,"oncert. Using physical and social commonsense – (i) Bob and Alice want to see the stage, and (ii) If Bob is taller, they would block Alice’s view – one can infer that Alice is taller than Bob. Such examples are ubiquitous across natural language understanding (NLU) tasks such as reading comprehension (Hirschman et al., 1999) and recognizing textual entailment (Dagan et al., 2013), and even more so in tasks dedicated to commonsense reasoning such as the Winograd schema challenge (Levesque et al., 2012). Most current NLU models rely on pretrained language models (LMs; e.g. Radford et al., 2019; Devlin et al., 2019; Raffel et al., 2020). The standard practice is to fine-tune a pre-trained LM in a supervised manner on task-specific data. Alternatively, LM score is used to rank answer choices in a zero-shot setup (Wang et al., 2019; Bosselut and Choi, 2019). In both setups, pre-trained LMs yield improved performance upon prior methods, greatly due to the world knowledge that such LMs capture, having been trained on massive texts (Petroni et al., 2019; Davison et al., 2019). Despite the performance boost, LMs as knowledge providers suffer from various shortcomings: (i) insufficient coverage: due to reporti"
2020.emnlp-main.373,N18-2092,0,0.0441901,"th works have shown somewhat promising results, other work showed that knowledge extracted from LMs is expectantly not always ac4622 curate. Specifically, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchang"
2020.emnlp-main.373,P18-1177,0,0.0458484,"Missing"
2020.emnlp-main.373,P17-1123,0,0.0158747,"mplates. While both works have shown somewhat promising results, other work showed that knowledge extracted from LMs is expectantly not always ac4622 curate. Specifically, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from foru"
2020.emnlp-main.373,2020.acl-main.413,0,0.0318935,"that knowledge extracted from LMs is expectantly not always ac4622 curate. Specifically, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchange). They proposed a model that generates clarification questions a"
2020.emnlp-main.373,S12-1052,0,0.149503,"larification is factually-correct. We show that even among the clarifications that helped the prediction, humans perceived many as unhelpful or even incorrect, demonstrating that LM-based models often solve problems correctly for seemingly incorrect reasons. Our results call for future research on robust and correct knowledge integration to LM-based question answering systems. 2 Tasks We focused on the multiple-choice question answering tasks detailed below. Each instance consists of an optional context, an optional question, and several answer choices. COPA: Choice of Plausible Alternatives (Gordon et al., 2012): Asking about either a plausible cause or a plausible result, among two alternatives, of a certain event expressed in a simple sentence. CommonSenseQA: commonsense Question Answering (Talmor et al., 2019): General questions about concepts from ConceptNet. To increase the challenge, the distractors are related to the target concept either by a relationship in ConceptNet or as suggested by crowdsourcing workers. MC-TACO: Multiple Choice Temporal commonsense (Zhou et al., 2019): Questions about temporal aspects of events such as ordering, duration, frequency, and typical time. The distractors we"
2020.emnlp-main.373,P18-1064,0,0.0192272,"enerating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchange). They proposed a model that generates clarification questions and corresponding answers for a given question, using the question’s comments (clarification questions and answers) as supervision. Questionanswer pairs were scored based on how much relevant information they add to the context. Shen et al. (2019) developed an active learning framework for image captioning that learns"
2020.emnlp-main.373,2021.ccl-1.108,0,0.187244,"Missing"
2020.emnlp-main.373,P19-1598,0,0.0309114,"le substitutes to the mask in “Dante was born in [MASK]” assigns the highest probability to Rome. Davison et al. (2019) similarly showed that BERT assigns higher scores to natural language fragments of true rather than fictitious ConceptNet triplets, and semi-automated the template creation by using GPT2 to score hand-crafted templates. While both works have shown somewhat promising results, other work showed that knowledge extracted from LMs is expectantly not always ac4622 curate. Specifically, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Pe"
2020.emnlp-main.373,S18-1119,0,0.065609,"Missing"
2020.emnlp-main.373,P19-1203,0,0.0249599,"cally, Kassner and Sch¨utze (2020) showed that negated facts are also considered likely by the LM, while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchange). They proposed a model that generates clarification questions and corresponding answers for a given question, using the question’s comments (c"
2020.emnlp-main.373,N19-1368,0,0.0478674,"etrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). Alternative approaches include using the knowledge to score answer candidates and prune implausible ones (Lin et al., 2017; Tandon et al., 2018), and training in a multi-task setup via auxiliary tasks pertaining to knowledge (Xia et al., 2019). To the best of our knowledge, our method is the first to generate knowledge from pre-trained language models and incorporate it as external knowledge into a question answering model. Concurrently, Latcinnik and Berant (2020) used one language model to generate hypotheses and another language model as an answer scorer for CommonSense"
2020.emnlp-main.373,D18-1233,0,0.0651602,"Missing"
2020.emnlp-main.373,speer-havasi-2012-representing,0,0.140964,"ion (“birds cannot fly”; Kassner and Sch¨utze, 2020). LMs excel in predicting the semantic category of a missing word, but might predict the wrong instance in that category (e.g., depending on the phrasing, BERT sometimes predicts red as the color of a dove). Finally, (iii) limited reasoning capabilities: it is unclear that LMs are capable of performing multiple reasoning steps involving implicit knowledge. To increase the coverage of high-precision world knowledge and facilitate multi-hop reasoning by making intermediate reasoning steps explicit, prior work incorporated KBs (e.g. ConceptNet; Speer and Havasi, 2012) and knowledge-informed models into LM-based models (Xia et al., 2019; Bosselut and Choi, 2019; Chen et al., 2019). In this paper, we study pre-trained LMs as an alternative to external KBs in providing knowledge 4615 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4615–4629, c November 16–20, 2020. 2020 Association for Computational Linguistics Because Brett found an internship while in college but Ian was unable to, Brett found a job less quickly after graduation. The purpose of the internship is to help people find jobs. s11 Because Brett found"
2020.emnlp-main.373,N18-1059,0,0.013742,", while Logan et al. (2019) pointed out that LMs may over-generalize and produce incorrect facts such as “Barack Obama’s wife is Hillary”. 6.3 Generating Questions and Explanations There are numerous research directions investigating automatic question generation (Vanderwende, 2008). Motivations vary from data augmentation to QA tasks (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020) through conversational machine reading (Saeidi et al., 2018; Pan et al., 2019), simplifying questions to make them more easily answerable (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020), to using questions as means for other purposes such as sentence representation and summarization (Guo et al., 2018; Potash and Suleman, 2019). In particular, our work is pertinent to previous work in producing clarification questions and explanations. Rao and Daum´e III (2019) worked on questions from forums (e.g. Stack Exchange). They proposed a model that generates clarification questions and corresponding answers for a given question, using the question’s comments (clarification questions and answers) as supervision. Questionanswer pairs were scored based on how much"
2020.emnlp-main.373,N19-1421,0,0.240692,"e problems correctly for seemingly incorrect reasons. Our results call for future research on robust and correct knowledge integration to LM-based question answering systems. 2 Tasks We focused on the multiple-choice question answering tasks detailed below. Each instance consists of an optional context, an optional question, and several answer choices. COPA: Choice of Plausible Alternatives (Gordon et al., 2012): Asking about either a plausible cause or a plausible result, among two alternatives, of a certain event expressed in a simple sentence. CommonSenseQA: commonsense Question Answering (Talmor et al., 2019): General questions about concepts from ConceptNet. To increase the challenge, the distractors are related to the target concept either by a relationship in ConceptNet or as suggested by crowdsourcing workers. MC-TACO: Multiple Choice Temporal commonsense (Zhou et al., 2019): Questions about temporal aspects of events such as ordering, duration, frequency, and typical time. The distractors were selected in an adversarial way using BERT.1 Social IQa: Social Interaction Question Answering (Sap et al., 2019b): Questions regarding social interactions, based on the ATOMIC dataset (Sap et al., 2019a"
2020.emnlp-main.373,D18-1006,0,0.0746222,"r-based representations, specifically pre-trained LMs (Devlin et al., 2019; Liu et al., 2019). With respect to the knowledge source, the vast majority of papers rely on ConceptNet to extract relation paths between concepts and entities identified in the input (Speer and Havasi, 2012, see an example in Figure 2). Additional resources include WordNet (Lin et al., 2017; Wang and Jiang, 2019), retrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). Alternative approaches include using the knowledge to score answer candidates and prune implausible ones (Lin et al., 2017; Tandon et al., 2018), and training in a multi-task setup via"
2020.emnlp-main.373,P19-1219,0,0.0449706,"l in a fraternity house. Table 4: An example for each of the error types among the harmful clarifications. et al., 2018; Clark et al., 2018; Talmor et al., 2019). The neural component has recently shifted from biLSTM to transformer-based representations, specifically pre-trained LMs (Devlin et al., 2019; Liu et al., 2019). With respect to the knowledge source, the vast majority of papers rely on ConceptNet to extract relation paths between concepts and entities identified in the input (Speer and Havasi, 2012, see an example in Figure 2). Additional resources include WordNet (Lin et al., 2017; Wang and Jiang, 2019), retrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 201"
2020.emnlp-main.373,P19-1393,0,0.0167972,"across natural language understanding (NLU) tasks such as reading comprehension (Hirschman et al., 1999) and recognizing textual entailment (Dagan et al., 2013), and even more so in tasks dedicated to commonsense reasoning such as the Winograd schema challenge (Levesque et al., 2012). Most current NLU models rely on pretrained language models (LMs; e.g. Radford et al., 2019; Devlin et al., 2019; Raffel et al., 2020). The standard practice is to fine-tune a pre-trained LM in a supervised manner on task-specific data. Alternatively, LM score is used to rank answer choices in a zero-shot setup (Wang et al., 2019; Bosselut and Choi, 2019). In both setups, pre-trained LMs yield improved performance upon prior methods, greatly due to the world knowledge that such LMs capture, having been trained on massive texts (Petroni et al., 2019; Davison et al., 2019). Despite the performance boost, LMs as knowledge providers suffer from various shortcomings: (i) insufficient coverage: due to reporting bias, many trivial facts might not be captured by LMs because they are rarely written about (Gordon and Van Durme, 2013). (ii) insufficient precision: the distributional training objective increases the probability o"
2020.emnlp-main.373,P19-1417,0,0.042015,"al component has recently shifted from biLSTM to transformer-based representations, specifically pre-trained LMs (Devlin et al., 2019; Liu et al., 2019). With respect to the knowledge source, the vast majority of papers rely on ConceptNet to extract relation paths between concepts and entities identified in the input (Speer and Havasi, 2012, see an example in Figure 2). Additional resources include WordNet (Lin et al., 2017; Wang and Jiang, 2019), retrieval or statistics mind from corpora (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020), knowledge base embeddings (Chen et al., 2019; Xiong et al., 2019), hand-crafted rules (Lin et al., 2017; Tandon et al., 2018), and tools such as sentiment analyzers (Chen et al., 2019) and knowledgeinformed LMs (Bosselut and Choi, 2019). The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019). Alternative approaches include using the knowledge to score answer candidates and prune implausible ones (Lin et al., 2017; T"
2020.emnlp-main.373,D18-1009,1,0.858633,"dsourced with a carefully designed approach that produces diverse examples which are trivial for humans. 3 Models A given instance consists of an optional context c, an optional question q, and answer choices: aki=1 . We first describe the baseline model, which makes 1 To make this task compatible with the other tasks, we only kept a single correct answer per instance, making our results not comparable to previously reported results. 2 Word associations and dataset-specific features that are not informative for the task are identified by a strong baseline and removed (Gururangan et al., 2018; Zellers et al., 2018). 4616 Taylor was doing her job so she put the money in the drawer. job, money job type of money What will Taylor do next? xWant work oal d by g ate motiv Job to earn money Job is a type of work. You would work because you want money. Job to earn money. to keep the money in the drawer As a result, Taylor wants to keep the money in the drawer. Figure 2: Generating a single clarification using ConceptNet, Google Ngrams, and COMeT (Social IQa instance). the prediction based on the instance alone (§3.1). We then describe a knowledge-informed model that relies on external resources (§3.2). Finally,"
2020.emnlp-main.373,P99-1042,0,\N,Missing
2020.emnlp-main.373,N16-1098,0,\N,Missing
2020.emnlp-main.373,D17-1216,0,\N,Missing
2020.emnlp-main.373,N18-1058,0,\N,Missing
2020.emnlp-main.373,N19-1013,0,\N,Missing
2020.emnlp-main.373,Q18-1023,0,\N,Missing
2020.emnlp-main.373,D19-1454,1,\N,Missing
2020.emnlp-main.373,D19-1250,0,\N,Missing
2020.emnlp-main.373,2020.emnlp-main.713,0,\N,Missing
2020.emnlp-main.48,P18-1043,1,0.844295,"ituations (Vu et al., 2014; Ding and Riloff, 2016) as well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck.com. cial and moral dynamics in language (Van Hee et al., 2015). Commonly used for coarse-grained analyses of morality in text (Fulgoni et al., 2016; Volkova et al., 2017; Weber et al., 2018), Graham et al. (2009) introduce the Moral Foundations lexicon, a dictionary of morality-evoking words (later extended by Rezapour et al., 2019). A recent line of work focused on representing social implications of everyday situations in freeform text in a knowledge graph (Rashkin et al., 2018; Sap et al., 2019). Relatedly, Sap et al. (2020) introduce Social Bias Frames, a hybrid free-text and categorical formalism to reason about biased implications in language. In contrast, our work formalizes a new type of reasoning around expectations of social norms evoked by situations. Finally, concurrent works have developed rich and exciting resources studying similar phenomena. Tay et al. (2020) study Would you rather? questions, and Acharya et al. (2020) investigate ritual understanding across cultures. Hendrycks et al. (2020) study ethical questions, attempting to assign a real-valued u"
2020.emnlp-main.48,W19-1305,0,0.0269575,"of situationally-rooted evocation of frames (Fillmore and Baker, 2001). Our work adds to the growing literature concerned with distilling reactions to situations (Vu et al., 2014; Ding and Riloff, 2016) as well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck.com. cial and moral dynamics in language (Van Hee et al., 2015). Commonly used for coarse-grained analyses of morality in text (Fulgoni et al., 2016; Volkova et al., 2017; Weber et al., 2018), Graham et al. (2009) introduce the Moral Foundations lexicon, a dictionary of morality-evoking words (later extended by Rezapour et al., 2019). A recent line of work focused on representing social implications of everyday situations in freeform text in a knowledge graph (Rashkin et al., 2018; Sap et al., 2019). Relatedly, Sap et al. (2020) introduce Social Bias Frames, a hybrid free-text and categorical formalism to reason about biased implications in language. In contrast, our work formalizes a new type of reasoning around expectations of social norms evoked by situations. Finally, concurrent works have developed rich and exciting resources studying similar phenomena. Tay et al. (2020) study Would you rather? questions, and Acharya"
2020.emnlp-main.48,2020.acl-main.486,1,0.723796,"s well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck.com. cial and moral dynamics in language (Van Hee et al., 2015). Commonly used for coarse-grained analyses of morality in text (Fulgoni et al., 2016; Volkova et al., 2017; Weber et al., 2018), Graham et al. (2009) introduce the Moral Foundations lexicon, a dictionary of morality-evoking words (later extended by Rezapour et al., 2019). A recent line of work focused on representing social implications of everyday situations in freeform text in a knowledge graph (Rashkin et al., 2018; Sap et al., 2019). Relatedly, Sap et al. (2020) introduce Social Bias Frames, a hybrid free-text and categorical formalism to reason about biased implications in language. In contrast, our work formalizes a new type of reasoning around expectations of social norms evoked by situations. Finally, concurrent works have developed rich and exciting resources studying similar phenomena. Tay et al. (2020) study Would you rather? questions, and Acharya et al. (2020) investigate ritual understanding across cultures. Hendrycks et al. (2020) study ethical questions, attempting to assign a real-valued utility to scenarios across a range of ethical cat"
2020.emnlp-main.48,P17-2102,0,0.0269639,"from Nørregaard et al. (2019), a large corpus of political headlines from 2018 paired with news source ratings of political leaning (5-point scale from left- to right-leaning) and factual reliability (5-point scale from least reliable to most reliable).10 Table 4 shows the correlations between RoT attributes and the political leaning and reliability of sources. Our results strongly corroborate findings by Graham et al. (2009), showing that liberal headlines evoke more “fairness” and “care,” while rightleaning headlines evoke more “sanctity” and “loyalty.” Furthermore, in line with findings by Volkova et al. (2017), more reliable news source tend to evoke more advice and less morality. 7 Related Work Our formalism heavily draws from works in descriptive ethics and social psychology, but is also inspired by studies in social implicatures and cooperative principles in pragmatics (Kallia, 2004; Grice, 1975) and the theories of situationally-rooted evocation of frames (Fillmore and Baker, 2001). Our work adds to the growing literature concerned with distilling reactions to situations (Vu et al., 2014; Ding and Riloff, 2016) as well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck."
2020.emnlp-main.48,E14-4025,0,0.0118897,"hile rightleaning headlines evoke more “sanctity” and “loyalty.” Furthermore, in line with findings by Volkova et al. (2017), more reliable news source tend to evoke more advice and less morality. 7 Related Work Our formalism heavily draws from works in descriptive ethics and social psychology, but is also inspired by studies in social implicatures and cooperative principles in pragmatics (Kallia, 2004; Grice, 1975) and the theories of situationally-rooted evocation of frames (Fillmore and Baker, 2001). Our work adds to the growing literature concerned with distilling reactions to situations (Vu et al., 2014; Ding and Riloff, 2016) as well as so10 We use the MediaBias/FactCheck ratings: https:// mediabiasfactcheck.com. cial and moral dynamics in language (Van Hee et al., 2015). Commonly used for coarse-grained analyses of morality in text (Fulgoni et al., 2016; Volkova et al., 2017; Weber et al., 2018), Graham et al. (2009) introduce the Moral Foundations lexicon, a dictionary of morality-evoking words (later extended by Rezapour et al., 2019). A recent line of work focused on representing social implications of everyday situations in freeform text in a knowledge graph (Rashkin et al., 2018; Sap"
2020.emnlp-main.48,N18-2049,0,0.0491832,"Missing"
2020.emnlp-main.556,D19-1250,0,0.0946054,"Missing"
2020.emnlp-main.556,D19-1578,0,0.4427,"tive way to represent contextual information, including lexical and syntactic knowledge as well as world knowledge (Petroni et al., 2019). LMs conflate generic facts (e.g.“the US has a president”) with grounded knowledge regarding specific entities and events (e.g.“the (current) president is a male”), occasionally leading to gender and racial biases (e.g.“women can’t be presidents”) (May et al., 2019; Sheng et al., 2019). In this work we focus on the representations of given names in pre-trained LMs (Table 1). Prior work showed that the representations of named entities incorporate sentiment (Prabhakaran et al., 2019), which is often transferable across entities via a shared given name (Field and Tsvetkov, 2019). In a series of experiments we show that, depending on the corpus, some names tend to be grounded to specific entities, even in generic contexts. The most striking effect is of politicians in GPT2. For example, the name Donald: 1) predicts Trump as the next token with high probability; 2) generated endings of “Donald is a” are easily distinguishable from any other given name; 3) their sentiment is substantially more negative; and 4) this bias can potentially perpetuate to downstream tasks. Although"
2020.emnlp-main.556,D13-1170,0,\N,Missing
2020.emnlp-main.556,D17-1082,0,\N,Missing
2020.emnlp-main.556,W17-1609,1,\N,Missing
2020.emnlp-main.556,N19-1062,0,\N,Missing
2020.emnlp-main.556,P19-1598,0,\N,Missing
2020.emnlp-main.556,N19-1423,0,\N,Missing
2020.emnlp-main.556,P19-1285,0,\N,Missing
2020.emnlp-main.556,N19-1270,0,\N,Missing
2020.emnlp-main.556,D19-1339,0,\N,Missing
2020.emnlp-main.58,W17-4912,0,0.024656,"original, auto-encoding objectives can ensure the correct information is captured (Bao et al., 2019; Baziotis et al., 2019; Artetxe et al., 2017). This work tackles problems where generation is more open-ended. Rather than reproducing information from the prompt, generations should agree with and expand on it, making autoencoding less applicable. Controllable language generation. Earlier approaches for controllable generation involved preserving the content of text while changing it along discrete dimensions, such as theme, sentiment, or style (Koncel-Kedziorski et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Lample et al., 2019). Recent works such as Grover (Zellers et al., 2019) and CTRL model (Keskar et al., 2019) used these ideas to augment transformer language models that can condition on structured metadata such as source, domain, etc. The Plug & Play model (PPLM; Dathathri et al., 2019) controls topic and sentiment in an approach similar to ours that involves forward and backward passes to update token distributions. However, PPLM relies on trained attribute discriminators for supervision, while our method is unsupervised. While these models are restricted to specific di"
2020.emnlp-main.58,P19-1602,0,0.0274374,"ounterfactual condition that “Tara ordered a shirt online” (as opposed to the original “went to mall”), the rewritten ending is about “sent shirt” to Tara (as opposed to the original “browsed from stores”). The last sentence of the original ending “She looked forward to wearing it” is correctly preserved as it is coherent with the counterfactual condition. 6 Related Work Unsupervised text generation. Unsupervised approaches are often applied to problems that copy information from a source text into decoded text. Unsupervised paraphrasing requires repeating this information (Miao et al., 2019; Bao et al., 2019), as does translation, but with a bilingual transformation (Artetxe et al., 2017; Lample et al., 2018). In summarization there is an additional task to select a subset of the original text (Baziotis et al., 2019; Schumann et al., 2020; West et al., 2019). In cases where information is mostly copied from the original, auto-encoding objectives can ensure the correct information is captured (Bao et al., 2019; Baziotis et al., 2019; Artetxe et al., 2017). This work tackles problems where generation is more open-ended. Rather than reproducing information from the prompt, generations should agree wi"
2020.emnlp-main.58,P18-1082,0,0.0300784,"ns in the sampled text to get complete sentences. ROUGE-L Table 1: Automatic evaluation results on the abductive task, using the test set of ART. We then mix the nth-step forward and backward logits to get the final logits of iteration t: y˜n(t) BLEU-4 Ranking We rank candidates by the overall coherence after inserting Y in between X and Z: ranking score(Y ) = c(XY, Z) + c(X, Y Z). (6) Hyperparameters We use GPT2-345M (Radford et al., 2019b) as the pre-trained LM for all models. We use the ART development set to select hyperparameters. We use greedy decoding for our method and top k decoding (Fan et al., 2018) (k = 40, τ = 0.7) for our baselines. Other hyperparameters are outlined in Appendix A.1. where c(·, ·) denotes the coherence score. This score is used to evaluate the quality of a given candidate continuation Y by measuring (1) its compatibility with the subsequent text of the context X, (2) the internal consistency of Y if it consists of multiple sentences, and (3) the compatibility of Y with its right-side text when it is applicable. 798 4.2 Experimental Setup Baselines We compare our method against baselines from Bhagavatula et al. (2019). The unsupervised baselines use a pre-trained GPT-2"
2020.emnlp-main.58,P19-1470,1,0.925279,"was excited to see him in person. Figure 3: Examples of generated hypotheses on three abductive reasoning cases. Given observations O1 and O2, D ELOREAN generates a hypothesis explaining the observations. to generate Y given a prompt text—either the observation X alone (Zero-ShotX ) or ZheiX (ZeroShotZX ), where hei denotes a special end-of-text token. The supervised method (Sup) follows the same input format as Zero-ShotZX , but finetunes GPT-2 on the ART training set. Finally, our knowledge-informed baseline (+COMET-Emb) further augments the representation of Sup with knowledge from COMET (Bosselut et al., 2019). To separately study the contribution of our decoding strategy and ranking component, we also report the performance of ranking the baseline outputs. Specifically, we let each baseline generate 20 candidates and rank them by coherence (Eq. 6).4 4.3 Model Human Evaluation We conduct two sets of human evaluations on 100 test examples using crowdworkers from Amazon Mechanical Turk. In the scoring setting, presented in Table 2, workers were presented a pair of observations (X and Z) and a generated hypothesis Y , and asked to rate the coherence of the hypothesis with respect to the observation X"
2020.emnlp-main.58,D19-1243,1,0.885382,"Missing"
2020.emnlp-main.58,D15-1075,0,0.0210203,"outputs produced by our model are judged as more coherent than those from the supervised models. In sum, our study shows that backpropagation-based decoding may enable additional future applications of unsupervised generation and reasoning. 2 “Lannister” in S30 and “Stark” in S40 and S50 refer to character names in the TV show, “Game of the Thrones.” All the output text shown in Figure 1 is the actual system output from D E L OREAN. 795 2 Background Most NLP benchmarks have focused on reasoning about information that is entailed from the premise. For instance, natural language inference (NLI; Bowman et al., 2015) focuses primarily on whether a hypothesis is entailed from a given premise, which means the information stated in the hypothesis is a subset of the information provided in the premise. However, it has been noted that human reasoning is often the other way, where hypotheses often contain new information that was not available in the premise, but plausibly true (but y˜ 1 y˜ 2 Initialization … y˜ N Generation Y Output: She hit the rope and the tire fell on top of her. LM x1 x2 … xNX Repeat T times y˜ b1 y˜ b2 y˜ 1 y˜ 2 y˜ f1 y˜ f2 … … … y˜ bN y˜ N y˜ fN Backpropagation Computing Loss Ray ran to"
2020.emnlp-main.58,P08-1090,0,0.193297,"specified generation length (Zeldes et al., 2020, which is not publicly available). Reasoning about narratives. A prominent resource from recent years is the RocStories corpus (Mostafazadeh et al., 2016b), consisting of 98K crowdsourced 5-sentence everyday life stories. It was used for the story cloze task whose goal was to predict the story ending from its first 4 sentences, but gained popularity and became the base of additional benchmarks (Rashkin et al., 2018). Additional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011; Sap et al., 2019; Bosselut et al., 2019). Qin et al. (2019b) studied conversation modeling that reads and connects the dots of events in related documents. Finally, a recent line of work explores counterfactual questions in reading comprehension (Huang et al., 2019; Tandon et al., 2019), but instantiates the problem of counterfactual reasoning as a multiple choice task. 7 Conclusion We presented D ELOREAN, an unsupervis"
2020.emnlp-main.58,D16-1168,0,0.0128894,"ases where information is mostly copied from the original, auto-encoding objectives can ensure the correct information is captured (Bao et al., 2019; Baziotis et al., 2019; Artetxe et al., 2017). This work tackles problems where generation is more open-ended. Rather than reproducing information from the prompt, generations should agree with and expand on it, making autoencoding less applicable. Controllable language generation. Earlier approaches for controllable generation involved preserving the content of text while changing it along discrete dimensions, such as theme, sentiment, or style (Koncel-Kedziorski et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Lample et al., 2019). Recent works such as Grover (Zellers et al., 2019) and CTRL model (Keskar et al., 2019) used these ideas to augment transformer language models that can condition on structured metadata such as source, domain, etc. The Plug & Play model (PPLM; Dathathri et al., 2019) controls topic and sentiment in an approach similar to ours that involves forward and backward passes to update token distributions. However, PPLM relies on trained attribute discriminators for supervision, while our method is unsupervised. Whil"
2020.emnlp-main.58,J82-2005,0,0.656271,"Missing"
2020.emnlp-main.58,P18-1169,0,0.0217929,"(Isard, 1974; GinsOutput: She hit the rope and the tire fell on top of her. 796 y˜ y˜ N 3 Generation 1 2 b N Backprop b 3 b 1Y b 2 … 2 on top of Repeat 3 her.… N Output: She hit the rope and1the tire fell Ray T times … Computing Los berg, 1986), it requires causal reasoning abilities, which are arguably absent from current associationbased AI (Pearl and Mackenzie, 2018). While there has been work on counterfactual reasoning in NLP, including recognizing counterfactuals in text (Son et al., 2017), and improving the performance of NLP tasks using counterfactual learning (Lawrence et al., 2017; Lawrence and Riezler, 2018), it remains a major research challenge. Recently, Qin et al. (2019a) introduce the task of counterfactual story generation. Given a 5-sentence original story, and an alternative context in which the second sentence of the story was altered by a counterfactual, the task is to generate a new 3sentence story ending that addresses the alternative beginning while minimally editing the original ending. The associated T IME T RAVEL dataset is based on fictional narratives from ROCStories, for which counterfactual contexts and alternative endings are crowdsourced, yielding 29,849 problem instances. Q"
2020.emnlp-main.58,D17-1272,0,0.0177427,"tant role in AI systems (Isard, 1974; GinsOutput: She hit the rope and the tire fell on top of her. 796 y˜ y˜ N 3 Generation 1 2 b N Backprop b 3 b 1Y b 2 … 2 on top of Repeat 3 her.… N Output: She hit the rope and1the tire fell Ray T times … Computing Los berg, 1986), it requires causal reasoning abilities, which are arguably absent from current associationbased AI (Pearl and Mackenzie, 2018). While there has been work on counterfactual reasoning in NLP, including recognizing counterfactuals in text (Son et al., 2017), and improving the performance of NLP tasks using counterfactual learning (Lawrence et al., 2017; Lawrence and Riezler, 2018), it remains a major research challenge. Recently, Qin et al. (2019a) introduce the task of counterfactual story generation. Given a 5-sentence original story, and an alternative context in which the second sentence of the story was altered by a counterfactual, the task is to generate a new 3sentence story ending that addresses the alternative beginning while minimally editing the original ending. The associated T IME T RAVEL dataset is based on fictional narratives from ROCStories, for which counterfactual contexts and alternative endings are crowdsourced, yieldin"
2020.emnlp-main.58,N16-1098,0,0.388139,"ith pre-defined values, our model can adjust to any open-ended textual constraint. Perhaps the most similar work in that aspect is the “text infilling” models, which, however, are in a more narrow setting by filling only a relatively short text span (Devlin et al., 2018; Zhu et al., 2019; Donahue et al., 2020), and more restrictive due to the reliance on an extra right-to-left language model (Sun et al., 2017) or a pre-specified generation length (Zeldes et al., 2020, which is not publicly available). Reasoning about narratives. A prominent resource from recent years is the RocStories corpus (Mostafazadeh et al., 2016b), consisting of 98K crowdsourced 5-sentence everyday life stories. It was used for the story cloze task whose goal was to predict the story ending from its first 4 sentences, but gained popularity and became the base of additional benchmarks (Rashkin et al., 2018). Additional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011"
2020.emnlp-main.58,P02-1040,0,0.115019,"Missing"
2020.emnlp-main.58,E14-1024,0,0.0468308,"Zeldes et al., 2020, which is not publicly available). Reasoning about narratives. A prominent resource from recent years is the RocStories corpus (Mostafazadeh et al., 2016b), consisting of 98K crowdsourced 5-sentence everyday life stories. It was used for the story cloze task whose goal was to predict the story ending from its first 4 sentences, but gained popularity and became the base of additional benchmarks (Rashkin et al., 2018). Additional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011; Sap et al., 2019; Bosselut et al., 2019). Qin et al. (2019b) studied conversation modeling that reads and connects the dots of events in related documents. Finally, a recent line of work explores counterfactual questions in reading comprehension (Huang et al., 2019; Tandon et al., 2019), but instantiates the problem of counterfactual reasoning as a multiple choice task. 7 Conclusion We presented D ELOREAN, an unsupervised LMbased approach to gener"
2020.emnlp-main.58,D19-1509,1,0.868796,"al reasoning). Such nonmonotonic reasoning requires 1 Code is available at https://github.com/ qkaren/unsup_gen_for_cms_reasoning Figure 1: D ELOREAN, our proposed method, with generated reasoning results. Top: the goal in abductive reasoning is to generate a hypothesis (Y ) of what happened between the observed past (X) and future (Z) contexts. Bottom: In counterfactual reasoning, given a story context altered by a counterfactual condition, X, and the original ending Z, the goal is to generate a new ending Y which is coherent with X while remaining similar to Z. The story from T IME T RAVEL (Qin et al., 2019a) consists of five sentences. Our approach alternates forward (left-to-right) and backward (rightto-left) passes that iteratively refine the generated texts w.r.t context from each side. inferring plausible but potentially defeasible conclusions from incomplete or hypothetical observations (Reiter, 1988). While humans are remarkably good at this type of causal reasoning, developing AI systems capable of nonmonotonic reasoning for 794 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 794–805, c November 16–20, 2020. 2020 Association for Computational"
2020.emnlp-main.58,P19-1539,1,0.786993,"al reasoning). Such nonmonotonic reasoning requires 1 Code is available at https://github.com/ qkaren/unsup_gen_for_cms_reasoning Figure 1: D ELOREAN, our proposed method, with generated reasoning results. Top: the goal in abductive reasoning is to generate a hypothesis (Y ) of what happened between the observed past (X) and future (Z) contexts. Bottom: In counterfactual reasoning, given a story context altered by a counterfactual condition, X, and the original ending Z, the goal is to generate a new ending Y which is coherent with X while remaining similar to Z. The story from T IME T RAVEL (Qin et al., 2019a) consists of five sentences. Our approach alternates forward (left-to-right) and backward (rightto-left) passes that iteratively refine the generated texts w.r.t context from each side. inferring plausible but potentially defeasible conclusions from incomplete or hypothetical observations (Reiter, 1988). While humans are remarkably good at this type of causal reasoning, developing AI systems capable of nonmonotonic reasoning for 794 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 794–805, c November 16–20, 2020. 2020 Association for Computational"
2020.emnlp-main.58,P18-1213,1,0.8585,"; Zhu et al., 2019; Donahue et al., 2020), and more restrictive due to the reliance on an extra right-to-left language model (Sun et al., 2017) or a pre-specified generation length (Zeldes et al., 2020, which is not publicly available). Reasoning about narratives. A prominent resource from recent years is the RocStories corpus (Mostafazadeh et al., 2016b), consisting of 98K crowdsourced 5-sentence everyday life stories. It was used for the story cloze task whose goal was to predict the story ending from its first 4 sentences, but gained popularity and became the base of additional benchmarks (Rashkin et al., 2018). Additional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011; Sap et al., 2019; Bosselut et al., 2019). Qin et al. (2019b) studied conversation modeling that reads and connects the dots of events in related documents. Finally, a recent line of work explores counterfactual questions in reading comprehension (Huang et al., 201"
2020.emnlp-main.58,2020.acl-main.452,0,0.0250368,"e original ending “She looked forward to wearing it” is correctly preserved as it is coherent with the counterfactual condition. 6 Related Work Unsupervised text generation. Unsupervised approaches are often applied to problems that copy information from a source text into decoded text. Unsupervised paraphrasing requires repeating this information (Miao et al., 2019; Bao et al., 2019), as does translation, but with a bilingual transformation (Artetxe et al., 2017; Lample et al., 2018). In summarization there is an additional task to select a subset of the original text (Baziotis et al., 2019; Schumann et al., 2020; West et al., 2019). In cases where information is mostly copied from the original, auto-encoding objectives can ensure the correct information is captured (Bao et al., 2019; Baziotis et al., 2019; Artetxe et al., 2017). This work tackles problems where generation is more open-ended. Rather than reproducing information from the prompt, generations should agree with and expand on it, making autoencoding less applicable. Controllable language generation. Earlier approaches for controllable generation involved preserving the content of text while changing it along discrete dimensions, such as th"
2020.emnlp-main.58,P17-2103,0,0.02648,"While counterfactual reasoning plays Recently, Bhagavatula et al. (2019) propose the an important role in AI systems (Isard, 1974; GinsOutput: She hit the rope and the tire fell on top of her. 796 y˜ y˜ N 3 Generation 1 2 b N Backprop b 3 b 1Y b 2 … 2 on top of Repeat 3 her.… N Output: She hit the rope and1the tire fell Ray T times … Computing Los berg, 1986), it requires causal reasoning abilities, which are arguably absent from current associationbased AI (Pearl and Mackenzie, 2018). While there has been work on counterfactual reasoning in NLP, including recognizing counterfactuals in text (Son et al., 2017), and improving the performance of NLP tasks using counterfactual learning (Lawrence et al., 2017; Lawrence and Riezler, 2018), it remains a major research challenge. Recently, Qin et al. (2019a) introduce the task of counterfactual story generation. Given a 5-sentence original story, and an alternative context in which the second sentence of the story was altered by a counterfactual, the task is to generate a new 3sentence story ending that addresses the alternative beginning while minimally editing the original ending. The associated T IME T RAVEL dataset is based on fictional narratives fro"
2020.emnlp-main.58,D19-1629,1,0.826739,"dditional related work includes “script knowledge”, i.e. learning about prototypical series of events (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Pichotta and Mooney, 2014), temporal commonsense (Granroth-Wilding and Clark, 2016; Li et al., 2018), and modeling pre- and post- conditions of events (Roemmele et al., 2011; Sap et al., 2019; Bosselut et al., 2019). Qin et al. (2019b) studied conversation modeling that reads and connects the dots of events in related documents. Finally, a recent line of work explores counterfactual questions in reading comprehension (Huang et al., 2019; Tandon et al., 2019), but instantiates the problem of counterfactual reasoning as a multiple choice task. 7 Conclusion We presented D ELOREAN, an unsupervised LMbased approach to generate text conditioned on past context as well as future constraints, through forward and backward passes considering each condition. We demonstrated its effectiveness for abductive and counterfactual reasoning, on which it performed substantially better than unsupervised baselines. Our method is general and can be easily adapted for other generative reasoning tasks. 802 Acknowledgements Jacob Devlin, Ming-Wei Chang, Kenton Lee, and K"
2020.findings-emnlp.418,P16-2085,0,0.0362699,"Missing"
2020.findings-emnlp.418,P91-1008,0,0.661724,"in order to come up with examples or counterarguments that may undermine it; by analogy, the generative task we introduce here requires a model to come up with (rather than simply verify) examples of circumstances that undermine the given hypothesis. 2 Background and Related Work Defeasible reasoning is soft inference based on default assumptions to account for unknown facts, for example, “Tweety is a bird” entails that “Tweety flies”, because birds usually fly. Such a conclusion is not deductively valid, and might be invalidated by new information such as “Tweety is a penguin” (Reiter, 1980; Lascarides and Asher, 1991). Defeasible reasoning is a type of nonmonotonic logic, as it contrasts the monotonicity property of classical logic, according to which valid inferences cannot be defeated by adding additional information (Kraus et al., 1990). Defeasible reasoning has been studied in a range of fields from logic, through linguistics and artificial intelligence. Classical AI. In early AI, defeasible reasoning was used as a solution to the “frame problem”: it is impossible to list all the potential effects of actions without describing mundane and obvious effects (McCarthy and Hayes, 1969). McDermott and Doyle"
2020.findings-emnlp.418,2020.acl-main.703,0,0.0591476,"Missing"
2020.findings-emnlp.418,W04-1013,0,0.0613121,"Missing"
2020.findings-emnlp.418,2021.ccl-1.108,0,0.0545281,"Missing"
2020.findings-emnlp.418,W17-1609,1,0.892575,"Missing"
2020.findings-emnlp.418,D17-1238,0,0.0285361,"Missing"
2020.findings-emnlp.418,P02-1040,0,0.106635,"jective to predict the next word. We use the Transformers package (Wolf et al., 2019) and train each model for a single epoch with a batch size of 64. Further training details are provided in the appendix. Automatic Evaluation. We follow the common practice of reporting automated generation evaluation metrics. We report the perplexity on the test set, as is often used to measure the performance of a language model.4 In addition, we generated predictions for the test set using beam search with 5 beams, and evaluated them using standard n-gram based metrics: the precision-oriented BLEU-4 score (Papineni et al., 2002), which considers n-grams up to n = 4, and the recall-oriented 4666 4 Micro and macro perplexities were identical. Premise Hypothesis Type Generated Update 1 A man just roaming on the streets during night. A man is roaming the streets at night, drunk. — It is rude to point out their weight problem. S W The man has a beer in his hand You are a nutritionist 2 PersonX pays PersonX’s debt — W S PersonX is in debt to the IRS You are in an emergency 3 4 5 6 7 8 9 Because PersonX wanted to be debt free It is rude to refuse help. — It is wrong to kill an animal. S You are trying to save the life of a"
2020.findings-emnlp.418,Q19-1043,0,0.0222982,"as a softer version of semantic entailment, doubly hedging it with “a human would typically think that the hypothesis is likely true” (see Section 3, Dagan et al., 2005). It gained tremendous popularity again 10 years later, with the release of the large-scale Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), that facilitated training neural models, and which was followed by several other datasets in that nature (Williams et al., 2018; Nie et al., 2019). But—among other criticisms of the task—it has been shown that people generally don’t agree on entailment annotations (Pavlick and Kwiatkowski, 2019), and new variants of the task suggested to shift away from categorical labels to ordinal or numeric values denoting plausibility (Zhang et al., 2017; Sakaguchi and Van Durme, 2018; Chen et al., 2020). In this paper we focus on the defeasibility of textual entailments, a less well-studied phenomenon in this context. 3 Definition In this paper, we employ a working definition of defeasible inference that may be seen as an outgrowth of prior work. Dagan et al. (2005) introduced the following informal definition for the Recognizing Textual Entailment (RTE) task: ...textual entailment is defined as"
2020.findings-emnlp.418,S18-2023,1,0.911929,"Missing"
2020.findings-emnlp.418,D19-1509,1,0.887999,"Missing"
2020.findings-emnlp.418,P18-1020,0,0.0609249,"Missing"
2020.findings-emnlp.418,D19-1629,0,0.140498,"Missing"
2020.findings-emnlp.418,L18-1239,0,0.0442715,"Missing"
2020.findings-emnlp.418,N18-1101,0,0.0369249,"elations by defining defeasible rules based on commonsense knowledge of typical causes and effects. Natural Language Processing. Textual entailment was defined as a softer version of semantic entailment, doubly hedging it with “a human would typically think that the hypothesis is likely true” (see Section 3, Dagan et al., 2005). It gained tremendous popularity again 10 years later, with the release of the large-scale Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), that facilitated training neural models, and which was followed by several other datasets in that nature (Williams et al., 2018; Nie et al., 2019). But—among other criticisms of the task—it has been shown that people generally don’t agree on entailment annotations (Pavlick and Kwiatkowski, 2019), and new variants of the task suggested to shift away from categorical labels to ordinal or numeric values denoting plausibility (Zhang et al., 2017; Sakaguchi and Van Durme, 2018; Chen et al., 2020). In this paper we focus on the defeasibility of textual entailments, a less well-studied phenomenon in this context. 3 Definition In this paper, we employ a working definition of defeasible inference that may be seen as an outgrow"
2020.findings-emnlp.440,P19-1409,1,0.177396,"to the benefit of the other. 1 Director Chris Weitz is expected to direct∨ New Moon. Chris Weitz will take on∨ the sequel to “Twilight”. Gary Ross is still in negotiations to direct× the sequel. Table 1: Examples from ECB+ (a cross-document coreference dataset) that illustrate the context-sensitive nature of event coreference. The illustrated predicates are co-referable, and hence may be used to refer to the same event in certain contexts, but obviously not all their mentions corefer. Introduction Recognizing that mentions of different lexical predicates discuss the same event is challenging (Barhom et al., 2019). Lexical resources such as WordNet (Miller, 1995) capture such synonyms (say, tell) and hypernyms (whisper, talk), as well as antonyms, which can be used to refer to the same event when the arguments are reversed ([a]0 beat [a]1 , [a]1 lose to [a]0 ). However, WordNet’s coverage is insufficient, in particular, missing contextspecific paraphrases (e.g. (hide, launder), in the context of money). Conversely, distributional methods enjoy broader coverage, but their precision for this purpose is limited because distributionally similar terms may often be mutually-exclusive (born, die) or may refer"
2020.findings-emnlp.440,N03-1003,0,0.343931,"Missing"
2020.findings-emnlp.440,P01-1008,0,0.494554,"s rely on neural representations of the mentions and their contexts (KenyonDean et al., 2018; Barhom et al., 2019), while earlier approaches leveraged WordNet and other lexical resources to obtain a signal of whether a pair of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphras"
2020.findings-emnlp.440,P10-1143,0,0.890532,"s difference with examples of co-referable predicate paraphrases, while their mentions obviously do not always co-refer. Cross-document event coreference resolution systems are typically supervised, usually trained on the ECB+ dataset, which contains clusters of news articles on different topics (Cybulska and Vossen, 2014). Recent systems rely on neural representations of the mentions and their contexts (KenyonDean et al., 2018; Barhom et al., 2019), while earlier approaches leveraged WordNet and other lexical resources to obtain a signal of whether a pair of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on t"
2020.findings-emnlp.440,J14-2004,0,0.306166,"Missing"
2020.findings-emnlp.440,D17-1226,0,0.794831,"Missing"
2020.findings-emnlp.440,cybulska-vossen-2014-using,0,0.822158,"es and clusters event mentions, across multiple documents, that refer to the same event within their respective contexts. The latter task, on the other hand, collects pairs of event expressions that, at the generic lexical level, may refer to the same event in certain contexts. Table 1 illustrates this difference with examples of co-referable predicate paraphrases, while their mentions obviously do not always co-refer. Cross-document event coreference resolution systems are typically supervised, usually trained on the ECB+ dataset, which contains clusters of news articles on different topics (Cybulska and Vossen, 2014). Recent systems rely on neural representations of the mentions and their contexts (KenyonDean et al., 2018; Barhom et al., 2019), while earlier approaches leveraged WordNet and other lexical resources to obtain a signal of whether a pair of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation acr"
2020.findings-emnlp.440,P18-1128,0,0.0169198,"aluation and the Average Precision (AP) for ranking evaluation. Our scorer dramatically improves upon the baselines in all metrics. To show that the improved scoring generalizes beyond examples that appear in the ECB+ dataset, we selected a random subset of 500 predicate pairs with at least 6 support pairs from the entire Chirps resource and annotated them in the same method described in Section 3.2. The ranker evaluated on this subset gained 8 points in AP, relative to the original Chirps ranking. All results are statistically significant using bootstrap and permutation tests with p &lt; 0.001 (Dror et al., 2018). Table 6 exemplifies highly ranked predicate pairs by our Chirps* scorer, the original Chirps scorer and the GloVe scorer, which illustrates the improved ranking performance of Chirps* (as measured in table 5 by the AP score). Ablation Test To evaluate the importance of each type of feature, we perform an ablation test. Table 7 displays the performance of various ablated models, each of which with one set of features (Section 3.1) removed from the representation. In the classification task, removing the named entity coverage features somewhat improved the performance, mostly by increasing the"
2020.findings-emnlp.440,N13-1092,0,0.141604,"Missing"
2020.findings-emnlp.440,S18-2001,0,0.469453,"aining and evaluation are ECB+ (Cybulska and Vossen, 2014), and its predecessors, EECB (Lee et al., 2012) and ECB (Bejan and Harabagiu, 2010). ECB+ contains a set of topics, each containing a set of documents describing the same global event. Both event and entity coreferences are annotated in ECB+, within and across documents. Models for CD event coreference utilize a range of features, including lexical overlap among mention pairs and semantic knowledge from WordNet (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015), distributional (Choubey and Huang, 2017) and contextual representations (Kenyon-Dean et al., 2018; Barhom et al., 2019). The current state-of-the-art model from Barhom et al. (2019) iteratively and intermittently learns to cluster events and entities. A mention representation mi consists of several components, representing both the mention span and its surrounding context. The interdependence between clustering event vs. entity mentions is encoded into the mention representation, such that an event mention representation contains a component reflecting the current entity clustering, and vice versa. Using this representation, the model trains a pairwise mention scoring function that predic"
2020.findings-emnlp.440,D17-1126,0,0.0369788,"Missing"
2020.findings-emnlp.440,D12-1045,0,0.682228,"lution aims to identify and cluster event mentions, that, within their respective contexts, refer to the same event. The task has two variants, one in which coreferring mentions are within the same document (within document) and another in which corefering mentions may be in different documents (cross-document, CD), on which we focus in this paper. 1 Code available at github.com/yehudit96/coreferrability, github.com/yehudit96/event entity coref ecb plus The standard datasets used for CD event coreference training and evaluation are ECB+ (Cybulska and Vossen, 2014), and its predecessors, EECB (Lee et al., 2012) and ECB (Bejan and Harabagiu, 2010). ECB+ contains a set of topics, each containing a set of documents describing the same global event. Both event and entity coreferences are annotated in ECB+, within and across documents. Models for CD event coreference utilize a range of features, including lexical overlap among mention pairs and semantic knowledge from WordNet (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015), distributional (Choubey and Huang, 2017) and contextual representations (Kenyon-Dean et al., 2018; Barhom et al., 2019). The current state-of-the-art model from Barhom et al. (20"
2020.findings-emnlp.440,H05-1004,0,0.430778,"sentation, but the performance improvement was smaller. Mention Pair Representation Chirps* Features MLPch Barhom et al. Representation Generator Raw Chirps* Features Chirps* Feature Extractor mi, mj Mention Pair Figure 1: An illustration of the integrated mention pair scorer. The right vector is the original mention pair vector from Barhom et al. (2019), and the left one is our Chirps* extension, which is transformed through M LPch into the same embedding space. The two vectors are concatenated to form the mention pair representation, which is fed to the scoring function M LPscorer . CEAF-e (Luo, 2005) and CoNLL F1 (the average of MUC, B 3 and CEAF-e scores). We compare the integrated model to the original model and to the lemma baseline which clusters together mentions that share the same mentionhead lemma. The results in Table 8 show that the Chirps-enhanced model provides an improvement of 3.5 points over the lemma baseline and a small improvement upon Barhom et al. (2019) in all F1 score measures. The greatest improvement is in the link-based MUC measure, which counts the number of corresponding links between the mentions. The Chirps component helps link more coreferring mentions (impro"
2020.findings-emnlp.440,E17-1083,0,0.15302,"air of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We sho"
2020.findings-emnlp.440,D14-1162,0,0.0873361,"s are paraphrases or not; and (2) ranking the pairs based on the predicted positive class score. We consider the ranking evaluation as more informative, as we expect the ranking to reflect the number of contexts in which a pair of predicates may be coreferring. That is, predicate pairs that are coreferring in many contexts will be ranked higher than those that are coreferring in just a few contexts. We compare our model with two baselines: the original Chirps scores, and a baseline that assigns each pair of predicates the cosine similarity scores between the predicates using GloVe embeddings (Pennington et al., 2014).3 For the classification decisions made by the two baseline scores (Chirps score and cosine similarity for Glove vectors), we learn a threshold that yields the best accuracy score over the train set, above which a pair of predicates is classified as positive. 3 We were motivated to compare with Glove since this resource is utilized as a lexical representation in the state-ofthe-art system of (Barhom et al., 2019), which is utilized in the next section. Here, multi-word predicates were represented by the average of their Glove word vectors. Table 5 displays the accuracy, precision, recall and"
2020.findings-emnlp.440,N18-1202,0,0.0177949,"vent mentions coreference-resolution results on ECB+ test set. Pairwise Score about 370 lexically-divergent pairs of coreferring event mentions appearing in the ECB+ training set, and about 200 in the test set. MLPscorer 4.1 Integration Method The state-of-the-art CD coreference resolution model, by Barhom et al. (2019), trained a pairwise mention scoring function, M LPscorer (mi , mj ), which predicts the probability that two mentions mi , mj refer to the same event. The mention representation includes a lexical component (GloVe embeddings) as well as a contextual component (ELMo embeddings, Peters et al., 2018). The mention pair representation ~vi,j , which is fed to the pairwise scorer, combines the two separate mention representations. We extended the model by changing the input to the pairwise event mention scoring function to include information regarding the mention pair from Chirps*, as illustrated in Figure 1. We defined v~0 i,j = [~vi,j ; ~ci,j ], where ~ci,j denotes the Chirps* features, computed in the following way: ~ci,j ( M LPch (f~mi ,mj ) = M LPch (~0) if mi , mj ∈ Chirps otherwise f~mi ,mj ∈ R17 is the feature vector representing a pair of predicates (mi , mj ) for which there is an"
2020.findings-emnlp.440,P14-2006,0,0.110581,"~mi ,mj ) = M LPch (~0) if mi , mj ∈ Chirps otherwise f~mi ,mj ∈ R17 is the feature vector representing a pair of predicates (mi , mj ) for which there is an entry in Chirps, otherwise the input is a zero vector. M LPch is an MLP with a single hidden layer of size 50 and output layer of size 100, which is used to transform the discrete values in f~mi ,mj into the same embedding space of ~vi,j . The rest of the model remains the same, including the model architecture, training, and inference.5 4.2 Evaluation We evaluate the event coreference performance on ECB+ using the official CoNLL scorer (Pradhan et al., 2014). The reported metrics are MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), 5 We also tried to incorporate only the final Chirps* score into the mention pair representation, but the performance improvement was smaller. Mention Pair Representation Chirps* Features MLPch Barhom et al. Representation Generator Raw Chirps* Features Chirps* Feature Extractor mi, mj Mention Pair Figure 1: An illustration of the integrated mention pair scorer. The right vector is the original mention pair vector from Barhom et al. (2019), and the left one is our Chirps* extension, which is transformed through"
2020.findings-emnlp.440,N13-1110,0,0.0188116,"ents discussing the same event. The underlying assumption is that such redundant texts may refer to the same entities or events using lexically-divergent mentions. Coreferring mentions are identified heuristically and 4898 extracted as candidate paraphrases. When long documents are used, the first step in this approach is to align each pair of documents by sentences. This was done by finding sentences with shared named entities (Shinyama et al., 2002) or lexical overlap (Barzilay and Lee, 2003; Shinyama and Sekine, 2006), and by aligning pairs of predicates or arguments (Zhang and Weld, 2013; Recasens et al., 2013). In more recent work, Xu et al. (2014) and Lan et al. (2017) extracted sentential paraphrases from Twitter by heuristically matching pairs of tweets discussing the same topic. Predicate Paraphrases. In contrast to sentential paraphrases, it is also beneficial to identify differing textual templates of the same meaning. In this paper we focus on binary predicate paraphrases such as (“[a0 ] quit from [a1 ]”, “[a0 ] resign from [a1 ]”). Earlier approaches for acquiring predicate paraphrases considered a pair of predicate templates as paraphrases if the distributions of their argument instantiati"
2020.findings-emnlp.440,N06-1039,0,0.0735699,"icate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We show that the data and models for one task can benefit the other. In one direction (Section 3), we use event coreference annotations from the ECB+ dataset as distant supervision to learn an improv"
2020.findings-emnlp.440,S17-1019,1,0.868975,"een argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We show that the data and models for one task can benefit the other. In one direction (Section 3), we use event coreference annotations from the ECB+ dataset as distant supervision to learn an improved scoring of predicate paraphrases in the unsupervised Chirps resource (Shwartz et al"
2020.findings-emnlp.440,W04-3206,1,0.646309,"aphrases such as (“[a0 ] quit from [a1 ]”, “[a0 ] resign from [a1 ]”). Earlier approaches for acquiring predicate paraphrases considered a pair of predicate templates as paraphrases if the distributions of their argument instantiations were similar. For instance, in “[a0 ] quit from [a1 ]”, [a0 ] would typically be instantiated by people names while [a1 ] by employer organizations or job titles. A paraphrastic template like “[a0 ] resign from [a1 ]” is hence expected to have similar argument distributions, and can thus be detected by a distributional similarity approach (Lin and Pantel, 2001; Szpektor et al., 2004; Berant, 2012). Yet, as mentioned earlier, predicates with similar argument distributions are not necessarily paraphrastic, which introduces a substantial level of noise when acquiring paraphrase pairs using this approach. In this paper, we follow the potentially more reliable paraphrase acquisition approach, which tries to heuristically identify concrete co-referring predicate mentions. Identifying such mention pairs, detected as actually being used to refer to the same event, can provide a strong signal for identifying these predicates as paraphrastic (vs. the quite noisy corpus-level signa"
2020.findings-emnlp.440,M95-1005,0,0.909677,"f~mi ,mj ∈ R17 is the feature vector representing a pair of predicates (mi , mj ) for which there is an entry in Chirps, otherwise the input is a zero vector. M LPch is an MLP with a single hidden layer of size 50 and output layer of size 100, which is used to transform the discrete values in f~mi ,mj into the same embedding space of ~vi,j . The rest of the model remains the same, including the model architecture, training, and inference.5 4.2 Evaluation We evaluate the event coreference performance on ECB+ using the official CoNLL scorer (Pradhan et al., 2014). The reported metrics are MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), 5 We also tried to incorporate only the final Chirps* score into the mention pair representation, but the performance improvement was smaller. Mention Pair Representation Chirps* Features MLPch Barhom et al. Representation Generator Raw Chirps* Features Chirps* Feature Extractor mi, mj Mention Pair Figure 1: An illustration of the integrated mention pair scorer. The right vector is the original mention pair vector from Barhom et al. (2019), and the left one is our Chirps* extension, which is transformed through M LPch into the same embedding space. The two vecto"
2020.findings-emnlp.440,Q14-1034,0,0.181576,"d similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We show that the data and models for one task can benefit the other. In one direction (Section 3), we use event coreference annotations from the ECB+ dataset as distant supervision to learn an improved scoring of predicate paraphrases in the unsupervised Chirps"
2020.findings-emnlp.440,Q15-1037,0,0.862586,"of co-referable predicate paraphrases, while their mentions obviously do not always co-refer. Cross-document event coreference resolution systems are typically supervised, usually trained on the ECB+ dataset, which contains clusters of news articles on different topics (Cybulska and Vossen, 2014). Recent systems rely on neural representations of the mentions and their contexts (KenyonDean et al., 2018; Barhom et al., 2019), while earlier approaches leveraged WordNet and other lexical resources to obtain a signal of whether a pair of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which"
2020.findings-emnlp.440,D13-1183,0,0.160022,"signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We show that the data and models for one task can benefit the other. In one direction (Section 3), we use event coreference annotations from the ECB+ dataset as distant supervision to learn an improved scoring of predicate paraphrases in the uns"
2021.emnlp-main.564,P16-1162,0,0.00783626,"ter-level language modeling, and machine translation (Holtzman et al., 2020; Al- as PMIDC when surface form competition is eliminated. In future work we would like to explore Rfou et al., 2019; Peters et al., 2019). how surface form competition affects generation, Another (not mutually exclusive) argument is that length normalization may account for uncon- as we hypothesize that it may be the cause of overly ditional probability in a similar way to PMIDC . generic outputs under high model uncertainty. Length normalization is often measured over Byte Acknowledgments Pair Encoding (BPE) tokens (Sennrich et al., 2016) and BPE tends to produce vocabularies where most This work was supported in part by the tokens are equally frequent (Wang et al., 2020). ARO (AROW911NF-16-1-0121), the NSF (IISRecent evidence suggests that language is approxi- 1562364), DARPA under the MCS program mately uniformly information dense (Levy, 2018; through NIWC Pacific (N66001-19-2-4031) and Levy and Jaeger, 2007; Jaeger, 2006). As such, the Allen Institute for AI (AI2). We thank Mitchell length in BPE tokens may correspond roughly to Wortsman, Gabriel Ilharco, Tim Dettmers, and a unigram estimate of log-probability, supposing Ri"
2021.emnlp-main.564,2020.emnlp-main.346,0,0.245715,"has long been of interest in NLP, Computer Vision, and ML in general (Socher et al., 2013; Guadarrama et al., 2013; Romera-Paredes and Torr, 2015). However, Radford et al. (2019) popularized the notion that language models have many zero-shot capabilities that can be discovered simply by prompting the model, e.g., placing “TL;DR” (internet slang for Too Long; Didn’t Read) at the end of a passage causes the model to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b). Brown et al. (2020) demonstrated that few-shot learning without fine-tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to"
2021.emnlp-main.564,D13-1170,0,0.0108623,"erent to the domain or task, e.g. completions of the domain premise that are just inherently unlikely will be upweighted more. This allows us to directly measure how much an answer tells us about the question and vice versa (mutual information is symmetric, see §3). Valid hypotheses no longer need to compete with each other: both “Whirlpool bath” and “Bathtub ” will be considered reasonable answers to the question, and so both will attain a high score. 2 Background and Related Work Zero-shot vs. Few-Shot Zero-shot inference has long been of interest in NLP, Computer Vision, and ML in general (Socher et al., 2013; Guadarrama et al., 2013; Romera-Paredes and Torr, 2015). However, Radford et al. (2019) popularized the notion that language models have many zero-shot capabilities that can be discovered simply by prompting the model, e.g., placing “TL;DR” (internet slang for Too Long; Didn’t Read) at the end of a passage causes the model to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b). Brown et al. (2020) demonstrated that few-shot"
2021.emnlp-main.564,N19-1421,0,0.164119,"mpensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT2 and GPT-3 models on a variety of multiple choice datasets. * 1 Figure 1: While humans select from given options, language models implicitly assign probability to every possible string. This creates surface form competition between different strings that represent the same concept. Example from CommonsenseQA (Talmor et al., 2019). Introduction Domain Conditional Pointwise Mutual Information (PMIDC ), which reweighs scores by how much more likely a hypothesis (answer) becomes given a premise (question) within the specific task domain. Specifically, consider the example question (shown in Figure 1): “A human wants to submerge himself in water, what should he use?” with multiple choice options “Coffee cup”, “Whirlpool bath”, “Cup”, and “Puddle.” From the given options, “Whirlpool bath” is the only one that makes sense. Yet, other answers are valid and easier for a language model to generate, e.g., “Bathtub” and “A * Code"
2021.emnlp-main.564,N18-2049,0,0.0242966,"n §5. Prompt Sensitivity Recent work highlights LM sensitivity to inputs, and proposes to consider paraphrases of the prompt to overcome this (Davison et al., 2019; Jiang et al., 2020b), as well as noting that certain trigger tokens (Shin et al., 2020) can strongly effect the output of such models. In this work, we focus on the surface form of possible outputs, but do also analyze robustness to different prompts in §4.4. Interpreting Language Models Language models tend to model selectional preferences and thematic fit (Pantel et al., 2007; Erk et al., 2010) rather than semantic plausibility (Wang et al., 2018). Probability, possibility and plausibility are distinct (Van der Helm, 2006), but reporting bias (Gordon and Van Durme, 2013) means that language models only model what people are likely to write (on websites that are easily crawled). PMIDC aims to adjust for these challenges to better measure the underlying agreement between language models and human judgements, but of course is still subject to the limits and biases of the language model used. 3 Zero-shot Scoring Strategies This paper does not define any new modeling or finetuning methods. Rather, we propose the broad use of PMIDC scoring f"
2021.emnlp-main.564,2021.findings-emnlp.244,0,0.0323581,"tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to improve their zeroshot and few-shot capabilities on a large swathe of tasks (Wei et al., 2021; Zhong et al., 2021). Surface Form Competition When applying generative models to multiple choice problems, simply choosing the highest probability answer Extensive experiments show that PMIDC consis- becomes problematic due to different valid surface forms competing for probability. Indeed, recent tently outperforms raw, normalized, and calibrated work in question answering has demonstrated the probability scoring methods on zero-shot multiple importance of considering all multiple choice opchoice for more than a dozen datasets and it does so for every model in the GPT-2 and GPT-3 fami- tions together (Khashabi"
2021.emnlp-main.564,D19-1192,0,0.0261235,"s is not the case for multiple choice problems. Given two options, e.g., “USA” and “Canada”, GPT-3 will choose the correct answer by probability. However, if we substitute out “USA” for “U.S. of A.”, GPT-3 will assign higher probability to “Canada”, a less likely answer conceptually, but a much more likely surface form. Beyond this, incorrect generic answers such as “I don’t know” are often assigned high probability, relegating the desired answers to the tail of the distribution where softmax is poorly calibrated (Holtzman et al., 2020). PMI Work in dialogue has used PMI to promote diversity (Zhou et al., 2019; Yao et al., 2017; Li et al., 2016; Mou et al., 2016; Tang et al., 2019). Recently, Brown et al. (2020) used a scoring function resembling PMIDC for zero-shot question answering, though they only use the string “A:” as a prompt for the unconditional probability estimate, whereas we use a task-specific domain premise (see §3 for details). Furthermore, Brown et al. (2020) only report this scoring method on three datasets (ARC, OpenBookQA, and RACE, included here) out of the more than 20 tested and do not compare scores with their standard method, averaging loglikelihoods (AVG in this work). In"
2021.emnlp-main.564,D17-1233,0,0.0248982,"or multiple choice problems. Given two options, e.g., “USA” and “Canada”, GPT-3 will choose the correct answer by probability. However, if we substitute out “USA” for “U.S. of A.”, GPT-3 will assign higher probability to “Canada”, a less likely answer conceptually, but a much more likely surface form. Beyond this, incorrect generic answers such as “I don’t know” are often assigned high probability, relegating the desired answers to the tail of the distribution where softmax is poorly calibrated (Holtzman et al., 2020). PMI Work in dialogue has used PMI to promote diversity (Zhou et al., 2019; Yao et al., 2017; Li et al., 2016; Mou et al., 2016; Tang et al., 2019). Recently, Brown et al. (2020) used a scoring function resembling PMIDC for zero-shot question answering, though they only use the string “A:” as a prompt for the unconditional probability estimate, whereas we use a task-specific domain premise (see §3 for details). Furthermore, Brown et al. (2020) only report this scoring method on three datasets (ARC, OpenBookQA, and RACE, included here) out of the more than 20 tested and do not compare scores with their standard method, averaging loglikelihoods (AVG in this work). In contrast, we repor"
2021.emnlp-main.564,2021.emnlp-main.572,0,0.012031,"to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b). Brown et al. (2020) demonstrated that few-shot learning without fine-tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to improve their zeroshot and few-shot capabilities on a large swathe of tasks (Wei et al., 2021; Zhong et al., 2021). Surface Form Competition When applying generative models to multiple choice problems, simply choosing the highest probability answer Extensive experiments show that PMIDC consis- becomes problematic due to different valid surface forms competing for probability. Indeed, recent tently outperforms r"
2021.emnlp-main.564,P19-1472,1,0.828514,"for examples from each dataset in our templated format. 4.2 Datasets We report results on 16 splits of 13 datasets, and briefly describe each dataset here. Continuation These datasets require the model to select a continuation to previous text, making them a natural way to test language models. Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) asks for cause and effect relationships, as shown in Figure 2. StoryCloze (SC) (Mostafazadeh et al., 2017) gives the model a choice between two † https://beta.openai.com/ alternative endings to 5 sentence stories. Finally, HellaSwag (HS) (Zellers et al., 2019) uses GPT-2 to generate, BERT to filter, and crowd workers to verify possible continuations to a passage. Following previous work (Brown et al., 2020) we report development set numbers for COPA and HS. Question Answering RACE-M & -H (R-M & R-H) (Lai et al., 2017) are both drawn from English exams given in China, the former being given to Middle Schoolers and the latter to High Schoolers. Similarly, ARC Easy & Challenge (ARC-E & ARC-C) (Clark et al., 2018) are standardized tests described as “natural, grade-school science questions,” with the “Easy” split found to be solvable by either a retrie"
2021.findings-emnlp.326,P19-1470,0,0.0264416,"sed an unsupervised approach to detect implicit gender bias in a communicative domain. Ma et al. (2020) proposed a controllable de-biasing approach to rewrite a given text through the lens of connotation frames (Sap et al., 2017). Sap et al. (2020) studied potential unjust statements in social media through commonsense implications. We propose a compatible but different perspective where we focus on analyzing the implicit bias about a narrative’s protagonist about their attributes, mental states, and motivations. In order to capture implicit bias, we use a commonsense inference engine, COMeT (Bosselut et al., 2019), as a tool to uncover unspoken pragmatic implications. To the best of our knowledge, this is the first study to analyze implied (and not explicit) gender bias in a story generation system along various social axes. We find various evidence of implicit bias associated with the protagonist’s gender through our experiments.1 2 Data and Processing Pipeline In this section, we describe our data processing pipeline. We use GPT-2 (Radford et al., 2019) as our underlying generation model given its recent success in story generation (Guan et al., 2020; Brahman and Chaturvedi, 2020). We fine-tune GPT-2"
2021.findings-emnlp.326,2020.emnlp-main.426,1,0.777447,"e inference engine, COMeT (Bosselut et al., 2019), as a tool to uncover unspoken pragmatic implications. To the best of our knowledge, this is the first study to analyze implied (and not explicit) gender bias in a story generation system along various social axes. We find various evidence of implicit bias associated with the protagonist’s gender through our experiments.1 2 Data and Processing Pipeline In this section, we describe our data processing pipeline. We use GPT-2 (Radford et al., 2019) as our underlying generation model given its recent success in story generation (Guan et al., 2020; Brahman and Chaturvedi, 2020). We fine-tune GPT-2 to generate stories given titles on ROCStories (Mostafazadeh et al., 2016). ROCStories is a collection of 98, 161 short stories. This dataset captures a rich set of causal and temporal commonsense relations between daily events, making it an ideal avenue to study bias. We follow the training settings of medium-size GPT-2 as in Radford et al. (2019). At inference time, we generate stories using top-k sampling scheme (Fan et al., 2018) with k=40 and a softmax temperature of 0.7. To quantify implicit gender bias, we create a pipeline to divide stories into two groups based 1"
2021.findings-emnlp.326,N19-1423,0,0.0282452,"Missing"
2021.findings-emnlp.326,2020.emnlp-main.23,0,0.0199092,"and even reinforce unwanted social stereotypes abundant in their training corpora (Sheng et al., 2019, 2020; Liu et al., 2020b; Shwartz et al., 2020; Bender et al., 2021). This phenomenon has also been observed with their predecessors, word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2016; May et al., 2019; Gonen and Goldberg, 2019). While many prior works have examined societal Most existing methods on quantifying bias recbiases in specialized NLG systems such as dia- ognize explicit manifestation of bias in the surfacelogues systems (Lee et al., 2019; Liu et al., 2020a; level text (Dinan et al., 2020a; Lucy et al., 2020; Dinan et al., 2020a,b), not much work has been Gala et al., 2020) or collected human annotadone on bias analysis for story generation systems. tions (Sheng et al., 2019; Dinan et al., 2020b). There is a growing amount of work on automatic Previous work has also examined gender and repstory generation with real-world applications in resentation bias in GPT-3 generated stories using education, entertainment, working with children topic modeling and lexicon-based word similarand sensitive populations. Therefore, it is essential ity (Lucy and Bamman, 2021). However, biases 38"
2021.findings-emnlp.326,P18-1082,0,0.0286517,"GPT-2 (Radford et al., 2019) as our underlying generation model given its recent success in story generation (Guan et al., 2020; Brahman and Chaturvedi, 2020). We fine-tune GPT-2 to generate stories given titles on ROCStories (Mostafazadeh et al., 2016). ROCStories is a collection of 98, 161 short stories. This dataset captures a rich set of causal and temporal commonsense relations between daily events, making it an ideal avenue to study bias. We follow the training settings of medium-size GPT-2 as in Radford et al. (2019). At inference time, we generate stories using top-k sampling scheme (Fan et al., 2018) with k=40 and a softmax temperature of 0.7. To quantify implicit gender bias, we create a pipeline to divide stories into two groups based 1 Code at: https://github.com/ tenghaohuang/Uncover_implicit_bias on the protagonist’s gender (Section 2.1), and then extract pragmatic implications about the protagonist and others affected by them (Section 2.2). Our pipeline is described below and exemplified in Figure 2. 2.1 Recognizing the Protagonist’s Gender We define the protagonist as the most frequently occurring character in a story (Morrow, 1985). First, we use the SpanBERT coreference resolutio"
2021.findings-emnlp.326,2020.emnlp-main.44,0,0.019066,"Computational Linguistics: EMNLP 2021, pages 3866–3873 November 7–11, 2021. ©2021 Association for Computational Linguistics are often implicit and may not manifest themselves lexically. E.g., “women are weak” is an example of explicit bias, while “women cry” (which implies “women are (emotionally) weak”) is an example of implicit bias. Figure 1 illustrates more examples from model-generated stories. These examples contain implicit gender bias showing females to be needy and usually obsessed with their physical appearance, whereas males to be more intelligent, or accomplished. In this regard, Field and Tsvetkov (2020) proposed an unsupervised approach to detect implicit gender bias in a communicative domain. Ma et al. (2020) proposed a controllable de-biasing approach to rewrite a given text through the lens of connotation frames (Sap et al., 2017). Sap et al. (2020) studied potential unjust statements in social media through commonsense implications. We propose a compatible but different perspective where we focus on analyzing the implicit bias about a narrative’s protagonist about their attributes, mental states, and motivations. In order to capture implicit bias, we use a commonsense inference engine, C"
2021.findings-emnlp.326,2020.nlpcss-1.23,0,0.0856522,"Missing"
2021.findings-emnlp.326,W19-3621,0,0.0289506,"cs, professions, and religions for future work. Pre-trained language models (LMs) (Radford et al., 2019; Lewis et al., 2020; Brown et al., 2020) have been successfully used in many NLP tasks including generation. Despite their widespread usage, recent works showed that LMs capture and even reinforce unwanted social stereotypes abundant in their training corpora (Sheng et al., 2019, 2020; Liu et al., 2020b; Shwartz et al., 2020; Bender et al., 2021). This phenomenon has also been observed with their predecessors, word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2016; May et al., 2019; Gonen and Goldberg, 2019). While many prior works have examined societal Most existing methods on quantifying bias recbiases in specialized NLG systems such as dia- ognize explicit manifestation of bias in the surfacelogues systems (Lee et al., 2019; Liu et al., 2020a; level text (Dinan et al., 2020a; Lucy et al., 2020; Dinan et al., 2020a,b), not much work has been Gala et al., 2020) or collected human annotadone on bias analysis for story generation systems. tions (Sheng et al., 2019; Dinan et al., 2020b). There is a growing amount of work on automatic Previous work has also examined gender and repstory generation w"
2021.findings-emnlp.326,2020.tacl-1.7,0,0.0158622,"we use a commonsense inference engine, COMeT (Bosselut et al., 2019), as a tool to uncover unspoken pragmatic implications. To the best of our knowledge, this is the first study to analyze implied (and not explicit) gender bias in a story generation system along various social axes. We find various evidence of implicit bias associated with the protagonist’s gender through our experiments.1 2 Data and Processing Pipeline In this section, we describe our data processing pipeline. We use GPT-2 (Radford et al., 2019) as our underlying generation model given its recent success in story generation (Guan et al., 2020; Brahman and Chaturvedi, 2020). We fine-tune GPT-2 to generate stories given titles on ROCStories (Mostafazadeh et al., 2016). ROCStories is a collection of 98, 161 short stories. This dataset captures a rich set of causal and temporal commonsense relations between daily events, making it an ideal avenue to study bias. We follow the training settings of medium-size GPT-2 as in Radford et al. (2019). At inference time, we generate stories using top-k sampling scheme (Fan et al., 2018) with k=40 and a softmax temperature of 0.7. To quantify implicit gender bias, we create a pipeline to divide s"
2021.findings-emnlp.326,2020.tacl-1.5,0,0.0123063,"0 and a softmax temperature of 0.7. To quantify implicit gender bias, we create a pipeline to divide stories into two groups based 1 Code at: https://github.com/ tenghaohuang/Uncover_implicit_bias on the protagonist’s gender (Section 2.1), and then extract pragmatic implications about the protagonist and others affected by them (Section 2.2). Our pipeline is described below and exemplified in Figure 2. 2.1 Recognizing the Protagonist’s Gender We define the protagonist as the most frequently occurring character in a story (Morrow, 1985). First, we use the SpanBERT coreference resolution model (Joshi et al., 2020) to retrieve all the clusters of characters’ mentions within a story. Second, we select the character with the largest cluster as the protagonist.2 We also identify the protagonist’s gender using gendered pronouns: he/him/his for males and she/her for females.3 Third, we identify characters’ roles in each sentence. This information is needed later (Section 2.2) for inferring social implications through COMeT. Additionally, to demote the influence of confounding variables and surface features predictive of gender but not bias, we replace all characters’ names and their mentions with anonymous p"
2021.findings-emnlp.326,W19-3655,0,0.102363,"Missing"
2021.findings-emnlp.326,2020.acl-main.703,0,0.0119433,"ers in different roles are generally portrayed in different ways. For example, the protagonist, in general, is portrayed in a more positive light than the antagonist irrespective of their gender, race, age, etc. Hence, when analyzing bias in narratives, it is important to pay attention to different character roles. In this paper, we study the gender bias associated with the protagonist. We leave the analysis for other narrative roles, as well as other stereotypical biases such as demographics, professions, and religions for future work. Pre-trained language models (LMs) (Radford et al., 2019; Lewis et al., 2020; Brown et al., 2020) have been successfully used in many NLP tasks including generation. Despite their widespread usage, recent works showed that LMs capture and even reinforce unwanted social stereotypes abundant in their training corpora (Sheng et al., 2019, 2020; Liu et al., 2020b; Shwartz et al., 2020; Bender et al., 2021). This phenomenon has also been observed with their predecessors, word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2016; May et al., 2019; Gonen and Goldberg, 2019). While many prior works have examined societal Most existing methods on quantifying bias recbiase"
2021.findings-emnlp.326,2020.coling-main.390,0,0.0335669,"to different character roles. In this paper, we study the gender bias associated with the protagonist. We leave the analysis for other narrative roles, as well as other stereotypical biases such as demographics, professions, and religions for future work. Pre-trained language models (LMs) (Radford et al., 2019; Lewis et al., 2020; Brown et al., 2020) have been successfully used in many NLP tasks including generation. Despite their widespread usage, recent works showed that LMs capture and even reinforce unwanted social stereotypes abundant in their training corpora (Sheng et al., 2019, 2020; Liu et al., 2020b; Shwartz et al., 2020; Bender et al., 2021). This phenomenon has also been observed with their predecessors, word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2016; May et al., 2019; Gonen and Goldberg, 2019). While many prior works have examined societal Most existing methods on quantifying bias recbiases in specialized NLG systems such as dia- ognize explicit manifestation of bias in the surfacelogues systems (Lee et al., 2019; Liu et al., 2020a; level text (Dinan et al., 2020a; Lucy et al., 2020; Dinan et al., 2020a,b), not much work has been Gala et al., 2020) or collected human"
2021.findings-emnlp.326,2020.emnlp-main.64,0,0.0323564,"to different character roles. In this paper, we study the gender bias associated with the protagonist. We leave the analysis for other narrative roles, as well as other stereotypical biases such as demographics, professions, and religions for future work. Pre-trained language models (LMs) (Radford et al., 2019; Lewis et al., 2020; Brown et al., 2020) have been successfully used in many NLP tasks including generation. Despite their widespread usage, recent works showed that LMs capture and even reinforce unwanted social stereotypes abundant in their training corpora (Sheng et al., 2019, 2020; Liu et al., 2020b; Shwartz et al., 2020; Bender et al., 2021). This phenomenon has also been observed with their predecessors, word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2016; May et al., 2019; Gonen and Goldberg, 2019). While many prior works have examined societal Most existing methods on quantifying bias recbiases in specialized NLG systems such as dia- ognize explicit manifestation of bias in the surfacelogues systems (Lee et al., 2019; Liu et al., 2020a; level text (Dinan et al., 2020a; Lucy et al., 2020; Dinan et al., 2020a,b), not much work has been Gala et al., 2020) or collected human"
2021.findings-emnlp.326,2021.nuse-1.5,0,0.127345,"al., 2020a; level text (Dinan et al., 2020a; Lucy et al., 2020; Dinan et al., 2020a,b), not much work has been Gala et al., 2020) or collected human annotadone on bias analysis for story generation systems. tions (Sheng et al., 2019; Dinan et al., 2020b). There is a growing amount of work on automatic Previous work has also examined gender and repstory generation with real-world applications in resentation bias in GPT-3 generated stories using education, entertainment, working with children topic modeling and lexicon-based word similarand sensitive populations. Therefore, it is essential ity (Lucy and Bamman, 2021). However, biases 3866 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3866–3873 November 7–11, 2021. ©2021 Association for Computational Linguistics are often implicit and may not manifest themselves lexically. E.g., “women are weak” is an example of explicit bias, while “women cry” (which implies “women are (emotionally) weak”) is an example of implicit bias. Figure 1 illustrates more examples from model-generated stories. These examples contain implicit gender bias showing females to be needy and usually obsessed with their physical appearance, whereas males to"
2021.findings-emnlp.326,2020.emnlp-main.602,0,0.0333597,"istics are often implicit and may not manifest themselves lexically. E.g., “women are weak” is an example of explicit bias, while “women cry” (which implies “women are (emotionally) weak”) is an example of implicit bias. Figure 1 illustrates more examples from model-generated stories. These examples contain implicit gender bias showing females to be needy and usually obsessed with their physical appearance, whereas males to be more intelligent, or accomplished. In this regard, Field and Tsvetkov (2020) proposed an unsupervised approach to detect implicit gender bias in a communicative domain. Ma et al. (2020) proposed a controllable de-biasing approach to rewrite a given text through the lens of connotation frames (Sap et al., 2017). Sap et al. (2020) studied potential unjust statements in social media through commonsense implications. We propose a compatible but different perspective where we focus on analyzing the implicit bias about a narrative’s protagonist about their attributes, mental states, and motivations. In order to capture implicit bias, we use a commonsense inference engine, COMeT (Bosselut et al., 2019), as a tool to uncover unspoken pragmatic implications. To the best of our knowle"
2021.findings-emnlp.326,N19-1063,0,0.0282883,"such as demographics, professions, and religions for future work. Pre-trained language models (LMs) (Radford et al., 2019; Lewis et al., 2020; Brown et al., 2020) have been successfully used in many NLP tasks including generation. Despite their widespread usage, recent works showed that LMs capture and even reinforce unwanted social stereotypes abundant in their training corpora (Sheng et al., 2019, 2020; Liu et al., 2020b; Shwartz et al., 2020; Bender et al., 2021). This phenomenon has also been observed with their predecessors, word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2016; May et al., 2019; Gonen and Goldberg, 2019). While many prior works have examined societal Most existing methods on quantifying bias recbiases in specialized NLG systems such as dia- ognize explicit manifestation of bias in the surfacelogues systems (Lee et al., 2019; Liu et al., 2020a; level text (Dinan et al., 2020a; Lucy et al., 2020; Dinan et al., 2020a,b), not much work has been Gala et al., 2020) or collected human annotadone on bias analysis for story generation systems. tions (Sheng et al., 2019; Dinan et al., 2020b). There is a growing amount of work on automatic Previous work has also examined gende"
2021.findings-emnlp.326,N16-1098,0,0.0272918,"tions. To the best of our knowledge, this is the first study to analyze implied (and not explicit) gender bias in a story generation system along various social axes. We find various evidence of implicit bias associated with the protagonist’s gender through our experiments.1 2 Data and Processing Pipeline In this section, we describe our data processing pipeline. We use GPT-2 (Radford et al., 2019) as our underlying generation model given its recent success in story generation (Guan et al., 2020; Brahman and Chaturvedi, 2020). We fine-tune GPT-2 to generate stories given titles on ROCStories (Mostafazadeh et al., 2016). ROCStories is a collection of 98, 161 short stories. This dataset captures a rich set of causal and temporal commonsense relations between daily events, making it an ideal avenue to study bias. We follow the training settings of medium-size GPT-2 as in Radford et al. (2019). At inference time, we generate stories using top-k sampling scheme (Fan et al., 2018) with k=40 and a softmax temperature of 0.7. To quantify implicit gender bias, we create a pipeline to divide stories into two groups based 1 Code at: https://github.com/ tenghaohuang/Uncover_implicit_bias on the protagonist’s gender (Se"
2021.findings-emnlp.326,P18-1043,0,0.0204729,"Intellect, Power, and Appearance for stories with male and female protagonists. Figure 3(a) illustrates that male protagonists in both model-generated and human-written stories show higher intellect scores than female protagonists. Figure 3(b) illustrates that female protagonists are more likely to be portrayed by their physical appearance. The gender differences for appearance is also amplified in GPT2 generated stories. We now explore whether male and female protagonists have different motivations behind their actions. Having all motivation inferences (P R_M OT), we follow previous work by Rashkin et al. (2018) and categorize P R_M OT into LIWC categories (Tausczik and Pennebaker, 2016) based on their scores. For our analysis, we only consider ‘Core Drives and Needs’, ‘Biological Process’, ‘Personal Concerns’, ’Perceptual Process’, and ‘Social and Affect Words’.4 We conduct regression analysis using Generalized Linear Model to obtain the correlations between gender and each LIWC category.5 As shown in Figure 4, female protagonists tend to have discrepancy, body, sexual, and familyrelated motivations, whereas male protagonists’ actions are motivated by leisure, money, power, risk, and violence (death"
2021.findings-emnlp.326,N18-2002,0,0.0429015,"Missing"
2021.findings-emnlp.326,2020.acl-main.486,0,0.0298316,"hich implies “women are (emotionally) weak”) is an example of implicit bias. Figure 1 illustrates more examples from model-generated stories. These examples contain implicit gender bias showing females to be needy and usually obsessed with their physical appearance, whereas males to be more intelligent, or accomplished. In this regard, Field and Tsvetkov (2020) proposed an unsupervised approach to detect implicit gender bias in a communicative domain. Ma et al. (2020) proposed a controllable de-biasing approach to rewrite a given text through the lens of connotation frames (Sap et al., 2017). Sap et al. (2020) studied potential unjust statements in social media through commonsense implications. We propose a compatible but different perspective where we focus on analyzing the implicit bias about a narrative’s protagonist about their attributes, mental states, and motivations. In order to capture implicit bias, we use a commonsense inference engine, COMeT (Bosselut et al., 2019), as a tool to uncover unspoken pragmatic implications. To the best of our knowledge, this is the first study to analyze implied (and not explicit) gender bias in a story generation system along various social axes. We find va"
2021.findings-emnlp.326,D17-1247,0,0.0285391,"hile “women cry” (which implies “women are (emotionally) weak”) is an example of implicit bias. Figure 1 illustrates more examples from model-generated stories. These examples contain implicit gender bias showing females to be needy and usually obsessed with their physical appearance, whereas males to be more intelligent, or accomplished. In this regard, Field and Tsvetkov (2020) proposed an unsupervised approach to detect implicit gender bias in a communicative domain. Ma et al. (2020) proposed a controllable de-biasing approach to rewrite a given text through the lens of connotation frames (Sap et al., 2017). Sap et al. (2020) studied potential unjust statements in social media through commonsense implications. We propose a compatible but different perspective where we focus on analyzing the implicit bias about a narrative’s protagonist about their attributes, mental states, and motivations. In order to capture implicit bias, we use a commonsense inference engine, COMeT (Bosselut et al., 2019), as a tool to uncover unspoken pragmatic implications. To the best of our knowledge, this is the first study to analyze implied (and not explicit) gender bias in a story generation system along various soci"
2021.findings-emnlp.326,2020.findings-emnlp.291,0,0.0365546,"Missing"
2021.findings-emnlp.326,D19-1339,0,0.0211272,"important to pay attention to different character roles. In this paper, we study the gender bias associated with the protagonist. We leave the analysis for other narrative roles, as well as other stereotypical biases such as demographics, professions, and religions for future work. Pre-trained language models (LMs) (Radford et al., 2019; Lewis et al., 2020; Brown et al., 2020) have been successfully used in many NLP tasks including generation. Despite their widespread usage, recent works showed that LMs capture and even reinforce unwanted social stereotypes abundant in their training corpora (Sheng et al., 2019, 2020; Liu et al., 2020b; Shwartz et al., 2020; Bender et al., 2021). This phenomenon has also been observed with their predecessors, word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2016; May et al., 2019; Gonen and Goldberg, 2019). While many prior works have examined societal Most existing methods on quantifying bias recbiases in specialized NLG systems such as dia- ognize explicit manifestation of bias in the surfacelogues systems (Lee et al., 2019; Liu et al., 2020a; level text (Dinan et al., 2020a; Lucy et al., 2020; Dinan et al., 2020a,b), not much work has been Gala et al., 2"
2021.findings-emnlp.326,2020.emnlp-main.556,1,0.814207,"cter roles. In this paper, we study the gender bias associated with the protagonist. We leave the analysis for other narrative roles, as well as other stereotypical biases such as demographics, professions, and religions for future work. Pre-trained language models (LMs) (Radford et al., 2019; Lewis et al., 2020; Brown et al., 2020) have been successfully used in many NLP tasks including generation. Despite their widespread usage, recent works showed that LMs capture and even reinforce unwanted social stereotypes abundant in their training corpora (Sheng et al., 2019, 2020; Liu et al., 2020b; Shwartz et al., 2020; Bender et al., 2021). This phenomenon has also been observed with their predecessors, word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2016; May et al., 2019; Gonen and Goldberg, 2019). While many prior works have examined societal Most existing methods on quantifying bias recbiases in specialized NLG systems such as dia- ognize explicit manifestation of bias in the surfacelogues systems (Lee et al., 2019; Liu et al., 2020a; level text (Dinan et al., 2020a; Lucy et al., 2020; Dinan et al., 2020a,b), not much work has been Gala et al., 2020) or collected human annotadone on bias anal"
2021.starsem-1.8,2020.blackboxnlp-1.16,0,0.0214527,"challenge sets upon the knowledge-enhanced baselines (up to +17.5 points in accuracy from the next best model), all while maintaining the performance on the original MultiNLI test set (Williams et al., 2018). 1 All datasets and resources are https://github.com/ohadrozen/inferbert. Hypernymy P: He killed another jay this season. H: He took life away from a bird this season. Label: Entailment Relation: Hypernym(jay)= bird at 90 such as high lexical overlap and spelling errors. NLI models also struggled with examples involving logic and monotonicity (Richardson et al., 2020; Yanaka et al., 2020; Geiger et al., 2020).2 Finally, the GLUE benchmark dedicated a small set for diagnosing models’ strengths and weaknesses on various phenomena (Wang et al., 2018). Liu et al. (2019a) suggested that NLI models may perform poorly on specific phenomena they haven’t observed enough during training, and proposed to “inoculate” LM-based models against challenge sets by fine-tuning them on a small number of phenomenon-specific training instances. Rozen et al. (2019) showed that the inoculation does not necessarily teach the model a generalized notion of the phenomenon of interest, and that when the challenge test set dif"
2021.starsem-1.8,P18-2103,1,0.894184,"Missing"
2021.starsem-1.8,N18-2017,0,0.0440855,"Missing"
2021.starsem-1.8,D15-1075,0,0.097417,"Missing"
2021.starsem-1.8,2020.acl-main.465,0,0.0113565,"s (LMs), such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) have recently achieved human-level performance on standard natural language inference (NLI) benchmarks (Wang et al., 2019). However, the performance on this complex task is achieved in part thanks to large training sets that facilitate learning of dataset-specific biases and correlations, and thanks to the similar distributions between the training and test sets, that rewards such models (Poliak et al., 2018; Gururangan et al., 2018). This contrasts with humans, who can learn a generalized solution from fewer examples (Linzen, 2020). Indeed, NLI models often fail on examples involving various linguistic phenomena such 89 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 89–98 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics the LM of the relation between a pair of entities that are involved in an inference instance, e.g. Hypernym(pangolin) = animal. This approach is agnostic to the identity of the specific entities, allowing models to learn inference patterns separately from the individual facts involved in particular instances. To evaluate the ab"
2021.starsem-1.8,P18-1224,0,0.0201187,"lenge set for each semantic relation, that we derive from MultiNLI (Section 3.2). As usual, the goal is to determine the label of a premise-hypothesis pair (p, h) among entailment, neutral, and contradiction. For a given semantic relation, each instance in the corresponding challenge set requires applying an inference pattern associated with the semantic relation in order to determine the correct label (possibly along with other required inferences). 3 Knowledge-Enhanced Models There is plenty of work on incorporating knowledge from KBs into neural models. Knowledgebased Inference Model (KIM; Chen et al., 2018) incorporated semantic relations from WordNet into an RNN-based NLI model, gaining a modest improvement on a challenge set. The incorporation at various components of the original NLI model is not straightforward to adapt to other models. KnowBert (Peters et al., 2019) incorporated knowledge from Wikipedia and WordNet into a BERT model through entity embeddings, improving performance on relation extraction and entity typing. Ernie (Zhang et al., 2019) and K-Adapter (Wang et al., 2020a) both targeted similar downstream tasks. Ernie embeds entities and relations from a KB, and alters the BERT pr"
2021.starsem-1.8,N19-1225,0,0.174468,"Ido Dagan1 1 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,shmulikamar}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract as co-hyponymy (Glockner et al., 2018) and negation (Naik et al., 2018), which they are expected to acquire indirectly from the NLI training set. Prior work proposed to provide (“inoculate”) NLI models with a small number of phenomenonspecific training examples in order to teach the model to address them (Liu et al., 2019a). However, Rozen et al. (2019) showed that when the distributions of the training and test sets differ with respect to syntactic and lexical properties, the performance of such inoculated models drops, concluding that they do not learn a generalized notion of the phenomenon. In this paper we are motivated by the following question: how can we facilitate learning of generalized inference patterns, with respect to a given linguistic phenomenon, from a relatively small number of examples? Ideally, we would like an NLI model to learn inference patterns detached from their original context, and t"
2021.starsem-1.8,P19-1334,0,0.0343035,"Missing"
2021.starsem-1.8,N19-1423,0,0.529042,"datasets. In this setting, InferBert succeeds to learn general inference patterns, from a relatively small number of training instances, while not hurting performance on the original NLI data and substantially outperforming prior knowledge enhancement models on the challenge data. It further applies its inferences successfully at test time to previously unobserved entities. InferBert is computationally more efficient than most prior methods, in terms of number of parameters, memory consumption and training time. 1 Introduction Transformer-based pre-trained language models (LMs), such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) have recently achieved human-level performance on standard natural language inference (NLI) benchmarks (Wang et al., 2019). However, the performance on this complex task is achieved in part thanks to large training sets that facilitate learning of dataset-specific biases and correlations, and thanks to the similar distributions between the training and test sets, that rewards such models (Poliak et al., 2018; Gururangan et al., 2018). This contrasts with humans, who can learn a generalized solution from fewer examples (Linzen, 2020). Indeed, NLI models often fai"
2021.starsem-1.8,W18-5446,0,0.0600881,"Missing"
2021.starsem-1.8,C18-1198,0,0.0238189,"Computational Semantics, pages 89–98 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics the LM of the relation between a pair of entities that are involved in an inference instance, e.g. Hypernym(pangolin) = animal. This approach is agnostic to the identity of the specific entities, allowing models to learn inference patterns separately from the individual facts involved in particular instances. To evaluate the ability of NLI models to learn inference patterns for specific linguistic phenomena, we follow the evaluation approach taken in previous work (Naik et al., 2018; Liu et al., 2019a; Richardson et al., 2020), which demonstrated the learning ability of models over a few chosen inference phenomena. We focus on 4 target semantic relations: hypernymy, location, country of origin, and color, for which we create challenge sets1 (see Table 1 for examples). We construct the challenge sets such that there is no overlap between the training, validation, and test sets with respect to the target entities (e.g. pangolin), to allow testing whether the model had learned an inference phenomenon in a generic manner, rather than performing lexical memorization. The trai"
2021.starsem-1.8,D19-1005,0,0.0423223,"Missing"
2021.starsem-1.8,N18-1101,0,0.535014,"en instances. available Location P: H: Country of Origin P: Viesgo deal, from beginning to end, took less than five weeks. H: The minimum amount of time it has ever taken a Spanish company to close a deal is six weeks. Label: Contradiction Relation: CountryOfOrigin(Viesgo) = Spain Our results confirm that InferBert manages to generalize inference patterns to new facts, substantially improving performance on the challenge sets upon the knowledge-enhanced baselines (up to +17.5 points in accuracy from the next best model), all while maintaining the performance on the original MultiNLI test set (Williams et al., 2018). 1 All datasets and resources are https://github.com/ohadrozen/inferbert. Hypernymy P: He killed another jay this season. H: He took life away from a bird this season. Label: Entailment Relation: Hypernym(jay)= bird at 90 such as high lexical overlap and spelling errors. NLI models also struggled with examples involving logic and monotonicity (Richardson et al., 2020; Yanaka et al., 2020; Geiger et al., 2020).2 Finally, the GLUE benchmark dedicated a small set for diagnosing models’ strengths and weaknesses on various phenomena (Wang et al., 2018). Liu et al. (2019a) suggested that NLI models"
2021.starsem-1.8,W18-5441,0,0.0509514,"Missing"
2021.starsem-1.8,2020.acl-main.543,0,0.0129823,"g performance on the challenge sets upon the knowledge-enhanced baselines (up to +17.5 points in accuracy from the next best model), all while maintaining the performance on the original MultiNLI test set (Williams et al., 2018). 1 All datasets and resources are https://github.com/ohadrozen/inferbert. Hypernymy P: He killed another jay this season. H: He took life away from a bird this season. Label: Entailment Relation: Hypernym(jay)= bird at 90 such as high lexical overlap and spelling errors. NLI models also struggled with examples involving logic and monotonicity (Richardson et al., 2020; Yanaka et al., 2020; Geiger et al., 2020).2 Finally, the GLUE benchmark dedicated a small set for diagnosing models’ strengths and weaknesses on various phenomena (Wang et al., 2018). Liu et al. (2019a) suggested that NLI models may perform poorly on specific phenomena they haven’t observed enough during training, and proposed to “inoculate” LM-based models against challenge sets by fine-tuning them on a small number of phenomenon-specific training instances. Rozen et al. (2019) showed that the inoculation does not necessarily teach the model a generalized notion of the phenomenon of interest, and that when the"
2021.starsem-1.8,P19-1139,0,0.0830575,"saw a pangolin and the hypothesis Bob saw an animal, it needs to know that animal is a hypernym of pangolin. Training a model on every possible hyponym-hypernym pair is incredibly inefficient and requires re-training a model whenever the vocabulary expands. Instead, we propose to decouple the learning of generic inference patterns from that of the factual knowledge. To that end, we develop InferBert, a method to enhance language models with relational knowledge from a knowledge base (KB). In contrast to recent knowledge-enhancement approaches such as KnowBert (Peters et al., 2019) and Ernie (Zhang et al., 2019) that incorporate into LMs knowledge about individual entities (e.g. pangolin), we inform We present InferBert, a method to enhance transformer-based inference models with relevant relational knowledge. Our approach facilitates learning generic inference patterns requiring relational knowledge (e.g. inferences related to hypernymy) during training, while injecting on-demand the relevant relational facts (e.g. pangolin is an animal) at test time. We apply InferBERT to the NLI task over a diverse set of inference types (hypernymy, location, color, and country of origin), for which we collected c"
2021.starsem-1.8,K19-1019,1,0.893389,"Department, Bar-Ilan University, Ramat-Gan, Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,shmulikamar}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract as co-hyponymy (Glockner et al., 2018) and negation (Naik et al., 2018), which they are expected to acquire indirectly from the NLI training set. Prior work proposed to provide (“inoculate”) NLI models with a small number of phenomenonspecific training examples in order to teach the model to address them (Liu et al., 2019a). However, Rozen et al. (2019) showed that when the distributions of the training and test sets differ with respect to syntactic and lexical properties, the performance of such inoculated models drops, concluding that they do not learn a generalized notion of the phenomenon. In this paper we are motivated by the following question: how can we facilitate learning of generalized inference patterns, with respect to a given linguistic phenomenon, from a relatively small number of examples? Ideally, we would like an NLI model to learn inference patterns detached from their original context, and to be able to apply them in new c"
E17-1007,J10-4006,0,0.0143056,"the sentence cute cats drink milk, with the target word cats. The dependencybased contexts are drink-v:nsubj and cute-a:amod−1 . The joint-dependency context is drink-v#milk-n. Differently from Chersoni et al. (2016), we exclude the dependency tags to mitigate the sparsity of contexts. Out-of-vocabulary words are filtered out before applying the window. We experimented with window sizes 2 and 5, directional and indirectional (win2, win2d, win5, win5d). • Dependency-based contexts: rather than adjacent words in a window, we consider neighbors in a dependency parse tree (Pad´o and Lapata, 2007; Baroni and Lenci, 2010). The contexts of a target word wi are its parent and daughter nodes in the dependency tree (dep). We also experimented with a joint dependency context inspired by Chersoni et al. (2016), in which the contexts of a target word are the parent-sister pairs in the dependency tree (joint). See Figure 1 for an illustration. Feature Weighting Each distributional semantic space is spanned by a matrix M in which each row corresponds to a target word while each column corresponds to a context. The value of each cell Mi,j represents the association between the target word wi and the context cj . We expe"
E17-1007,C92-2082,0,0.638996,"n In the last two decades, the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy. Such effort is motivated by the role this semantic relation plays in a large number of tasks, such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011) and recognizing textual entailment (Dagan et al., 2013). The task has appeared to be, however, a challenging one, and the numerous approaches proposed to tackle it have often shown limitations. Early corpus-based methods have exploited patterns that may indicate hypernymy (e.g. “animals such as dogs”) (Hearst, 1992; Snow et al., 2005), but the recall limitation of this approach, requiring both words to co-occur in a sentence, motivated the development of methods that rely on 65 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 65–75, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ants and context-types are tested for the first time.1 We demonstrate that since each of these measures captures a different aspect of the hypernymy relation, there is no single measure that consistently p"
E17-1007,P15-2020,0,0.290882,"Missing"
E17-1007,E12-1004,0,0.209296,"e Precision (AP) and Mean Average Precision (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features"
E17-1007,S12-1012,0,0.0752966,"de and data are available at: https://github.com/vered1986/UnsupervisedHypernymy 66 • Positive LMI (PLMI) - positive local mutual information (PLMI) (Evert, 2005; Evert, 2008). PPMI was found to have a bias towards rare events. PLMI simply balances PPMI by multiplying it by the co-occurrence frequency of w and c: P LM I(w, c) = f req(w, c) · P P M I(w, c). 3 • Weeds Precision (Weeds and Weir, 2003) A directional precision-based similarity measure. This measure quantifies the weighted inclusion of x’s contexts by y’s contexts: Σc∈~vx ∩~vy ~vx [c] W eedsP rec(x → y) = Σc∈~vx ~vx [c] • cosWeeds (Lenci and Benotto, 2012) Geometric mean of cosine similarity and Weeds precision: p Unsupervised Hypernymy Detection Measures cosW eeds(x → y) = • ClarkeDE (Clarke, 2009) Computes degree of inclusion, by quantifying weighted coverage of the hyponym’s contexts by those of the hypernym: We experiment with a large number of unsupervised measures proposed in the literature for distributional hypernymy detection, with some new variants. In the following section, ~vx and ~vy denote x and y’s word vectors (rows in the matrix M ). We consider the scores as measuring to what extent y is a hypernym of x (x → y). 3.1 CDE(x → y)"
E17-1007,P14-2050,0,0.128893,"Missing"
E17-1007,D16-1205,1,0.886738,"Missing"
E17-1007,Q15-1016,0,0.477411,"represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural method integrating path-based and distributional information (Shwartz et al., 2016). In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection, using several distributional semantic models that differ by context type and feature weighting. Some measure variThe fundamental role of hypernymy in NLP has motivated the development o"
E17-1007,J90-1003,0,0.372426,"corpus we used a concatenation of the following two corpora: ukWaC (Ferraresi, 2007), a 2-billion word corpus constructed by crawling the .uk domain, and WaCkypedia EN (Baroni et al., 2009), a 2009 dump of the English Wikipedia. Both corpora include POS, lemma and dependency parse annotations. Our vocabulary (of target and context words) includes only nouns, verbs and adjectives that occurred at least 100 times in the corpus. • Frequency - raw frequency (no weighting): Mi,j is the number of co-occurrences of wi and cj in the corpus. • Positive PMI (PPMI) - pointwise mutual information (PMI) (Church and Hanks, 1990) is defined as the log ratio between the joint probability of w and c and the product of their marginal probabilities: P M I(w, c) = ˆ log P (w,c) , where Pˆ (w), Pˆ (c), and Pˆ (w, c) Pˆ (w)Pˆ (c) Context Type We use several context types: are estimated by the relative frequencies of a word w, a context c and a word-context pair (w, c), respectively. To handle unseen pairs (w, c), yielding P M I(w, c) = log(0) = −∞, PPMI (Bullinaria and Levy, 2007) assigns zero to negative PMI scores: P P M I(w, c) = max(P M I(w, c), 0). • Window-based contexts: the contexts of a target word wi are the words"
E17-1007,N15-1098,0,0.804753,"represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural method integrating path-based and distributional information (Shwartz et al., 2016). In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection, using several distributional semantic models that differ by context type and feature weighting. Some measure variThe fundamental role of hypernymy in NLP has motivated the development o"
E17-1007,W09-0215,0,0.297353,"Missing"
E17-1007,P13-2080,0,0.0232974,"ing instances, hurting their reliability. Being based on general linguistic hypotheses and independent from training data, unsupervised measures are more robust, and therefore are still useful artillery for hypernymy detection. 1 dominik.schlechtweg@gmx.de Introduction In the last two decades, the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy. Such effort is motivated by the role this semantic relation plays in a large number of tasks, such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011) and recognizing textual entailment (Dagan et al., 2013). The task has appeared to be, however, a challenging one, and the numerous approaches proposed to tackle it have often shown limitations. Early corpus-based methods have exploited patterns that may indicate hypernymy (e.g. “animals such as dogs”) (Hearst, 1992; Snow et al., 2005), but the recall limitation of this approach, requiring both words to co-occur in a sentence, motivated the development of methods that rely on 65 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 65–75, c Valencia, Spain, April 3-"
E17-1007,J07-2002,0,0.0492155,"Missing"
E17-1007,P16-1226,1,0.846088,"Missing"
E17-1007,D14-1162,0,0.116257,"he distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural method integrating path-based and distributional information (Shwartz et al., 2016). In this paper we perform an extensive evaluation of various unsupervised distributional"
E17-1007,E14-1054,0,0.0703737,"ns of the distributional hypothesis (Harris, 1954). The first distributional approaches were unsupervised, assigning a score for each (x, y) wordpair, which is expected to be higher for hypernym pairs than for negative instances. Evaluation is performed using ranking metrics inherited from information retrieval, such as Average Precision (AP) and Mean Average Precision (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use"
E17-1007,P06-1101,0,0.028322,"the unsupervised ones, the former are sensitive to the distribution of training instances, hurting their reliability. Being based on general linguistic hypotheses and independent from training data, unsupervised measures are more robust, and therefore are still useful artillery for hypernymy detection. 1 dominik.schlechtweg@gmx.de Introduction In the last two decades, the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy. Such effort is motivated by the role this semantic relation plays in a large number of tasks, such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011) and recognizing textual entailment (Dagan et al., 2013). The task has appeared to be, however, a challenging one, and the numerous approaches proposed to tackle it have often shown limitations. Early corpus-based methods have exploited patterns that may indicate hypernymy (e.g. “animals such as dogs”) (Hearst, 1992; Snow et al., 2005), but the recall limitation of this approach, requiring both words to co-occur in a sentence, motivated the development of methods that rely on 65 Proceedings of the 15th Conference of the European Chapter of the Association for Computation"
E17-1007,D16-1234,0,0.665717,"Missing"
E17-1007,C14-1097,0,0.616799,"Missing"
E17-1007,W03-1011,0,0.0966364,"University, Hong Kong 4 University of Stuttgart, Stuttgart, Germany {vered1986,esantus}@gmail.com, Abstract adaptations of the distributional hypothesis (Harris, 1954). The first distributional approaches were unsupervised, assigning a score for each (x, y) wordpair, which is expected to be higher for hypernym pairs than for negative instances. Evaluation is performed using ranking metrics inherited from information retrieval, such as Average Precision (AP) and Mean Average Precision (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word emb"
E17-1007,C14-1212,0,0.426282,"n (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural meth"
E17-1007,E14-4008,1,0.724451,"m, Abstract adaptations of the distributional hypothesis (Harris, 1954). The first distributional approaches were unsupervised, assigning a score for each (x, y) wordpair, which is expected to be higher for hypernym pairs than for negative instances. Evaluation is performed using ranking metrics inherited from information retrieval, such as Average Precision (AP) and Mean Average Precision (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010) and the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y’s word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to th"
E17-1007,L16-1722,1,0.877285,"Roller et al., 2014; Weeds et al., 2014). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings (Mikolov et al., 2013; Pennington et al., 2014), and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy (Levy et al., 2015b). Additional recent hypernymy detection methods include a multimodal perspective (Kiela et al., 2015), a supervised method using unsupervised measure scores as features (Santus et al., 2016a), and a neural method integrating path-based and distributional information (Shwartz et al., 2016). In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection, using several distributional semantic models that differ by context type and feature weighting. Some measure variThe fundamental role of hypernymy in NLP has motivated the development of many methods for the automatic identification of this relation, most of which rely on word distribution. We investigate an extensive number of such unsupervised measures, using several dist"
K15-1018,P14-1113,0,0.203499,"Missing"
K15-1018,C92-2082,0,0.65747,"Missing"
K15-1018,H05-1087,0,0.0916619,"Missing"
K15-1018,W14-1610,1,0.879591,"Missing"
K15-1018,W11-2501,0,0.175332,"Missing"
K15-1018,N15-1098,1,0.918016,"baselines, tuning the regularization parameter on the validation set. 5.2 6.1 A New Proper-Name Dataset Performance on WordNet We examine whether our algorithm can replicate the common use of WordNet (§2.1), by manually constructing 4 whitelists based on the literature An important linguistic component that is missing from these lexical-inference datasets is propernames. We conjecture that much of the added value in utilizing structured resources is the ability to cover terms such as celebrities (Lady Gaga) 7 Since our methods do not use lexical features, we did not use lexical splits as in (Levy et al., 2015). 179 Figure 4: Recall-precision curve for proper2015. Name basic +holo +mero +hypo Edge Types {synonym, hypernym, instance hypernym} basic ∪ {holonym} basic ∪ {meronym} basic ∪ {hyponym} Table 4: The manual whitelists commonly used in WordNet. Figure 3 compares our algorithm to WordNet’s baselines, showing that our binary model always replicates the best-performing manuallyconstructed whitelists, for certain values of β 2 . Synonyms and hypernyms are often selected, and additional edges are added to match the semantic flavor of each particular dataset. In turney2014, for example, where merony"
K15-1018,E12-1004,0,0.215574,"d Resources for Lexical Inference Vered Shwartz† Omer Levy† Ido Dagan† Jacob Goldberger§ † Computer Science Department, Bar-Ilan University § Faculty of Engineering, Bar-Ilan University vered1986@gmail.com {omerlevy,dagan}@cs.biu.ac.il jacob.goldberger@biu.ac.il Abstract Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternative common practice is to mine high-precision lexical inferences from structured resources, particularly WordNet (Fellbaum, 199"
K15-1018,D13-1160,0,0.0411595,"roaches can particularly benefit real-world tasks in which proper-names are prominent. 2 #Entities 4,500,000 6,000,000 10,000,000 150,000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet"
K15-1018,N04-3012,0,0.0762085,"and provides lexical inferences on entities that are absent from WordNet, particularly proper-names. Background Common Use of WordNet for Inference WordNet (Fellbaum, 1998) is widely used for identifying lexical inference. It is usually used in an unsupervised setting where the relations relevant for each specific inference task are manually selected a priori. One approach looks for chains of these predefined relations (Harabagiu and Moldovan, 1998), e.g. dog → mammal using a chain of hypernyms: dog → canine → carnivore → placental mammal → mammal. Another approach is via WordNet Similarity (Pedersen et al., 2004), which takes two synsets and returns a numeric value that represents their similarity based on WordNet’s hierarchical hypernymy structure. While there is a broad consensus that synonyms entail each other (elevator ↔ lif t) and hyponyms entail their hypernyms (cat → animal), other relations, such as meronymy, are not agreed 2 We also considered Freebase, but it required significantly larger computational resources to work in our framework, which, at the time of writing, exceeded our capacity. §4.1 discusses complexity. 176 “Beyonc´e” term to concept Beyonc´e Knowles occupation musician subclas"
K15-1018,C14-1212,0,0.213181,"Missing"
K15-1018,P11-1082,0,0.0144223,"rpus-based methods. Combining the two approaches can particularly benefit real-world tasks in which proper-names are prominent. 2 #Entities 4,500,000 6,000,000 10,000,000 150,000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic know"
K15-1018,P10-1013,0,0.0123057,"stateof-the-art corpus-based methods. Combining the two approaches can particularly benefit real-world tasks in which proper-names are prominent. 2 #Entities 4,500,000 6,000,000 10,000,000 150,000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 200"
K15-1018,C14-1097,0,0.336102,"Missing"
K15-1018,P09-1051,1,0.808142,",000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet, and GeoNames.2 Table 2 compares the scale of the resources we used. The massive scale of the more recent resources and their rich sc"
K15-1018,J06-3003,0,0.349541,"Missing"
K15-1018,W03-1011,0,0.684426,"Missing"
K15-1018,J14-1003,0,\N,Missing
K15-1018,P06-1013,0,\N,Missing
K19-1019,D15-1075,0,0.336477,"n to “learn” a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena – dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics ology,"
K19-1019,N19-1423,0,0.549817,"tly cover the targeted phenomenon, when possible. We demonstrate our methodology on two inference types, picked from the GLUE benchmark (Wang et al., 2019) diagnostic dataset: (1) dative alternation, a syntactic phenomenon, and (2) a specific type of numerical reasoning, pertaining to logical and arithmetic inference. To create our datasets, we introduce a templating method, by which we generated hundreds of synthetic examples from a single original sentence, while controlling the variance between the datasets.1 We employ a recent NLI model, based on the pretrained BERT masked language model (Devlin et al., 2019) fine-tuned on the MultiNLI dataset (Williams et al., 2018). For the dative alternation case, we find that the model struggles with generalizing over the syntactic dimension, requiring training over a relatively large variety of syntactically complex sentences. For the numerical reasoning case, we find that the model notably fails to generalize across diverse number ranges, a conclusion that might have been missed if we were to use only the original inoculation methodology. We hope our methodology will be adopted for additional NLP tasks, and specifically to a broader range of entailment infer"
K19-1019,P18-2103,1,0.93453,", Bar-Ilan University, Ramat-Gan, Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,roee.aharoni}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract instead attempt to learn them implicitly from the training data. Despite their success on common NLI dataset, recent challenge datasets designed for probing different linguistic phenomena showed that neural models often fail on particular inference types, like recognizing semantic relations and negation (Poliak et al., 2018; Naik et al., 2018; Glockner et al., 2018). Recently, Liu et al. (2019a) showed that when probing reveals a model’s failure on a specific linguistic phenomenon, it is often possible to amend this failure. They suggested to fine-tune the model on (a training section of) the challenge dataset itself, in order to teach it to address the specific target phenomenon, or in other words, to “inoculate” it against the adversarial data. Inoculation has two possible outcomes. The first - a success to address the phenomenon after fine-tuning - suggests the original training set did not cover this phenomenon sufficiently (“blind spot” of the datas"
K19-1019,N18-2017,0,0.0282848,"ally exclusive term (for contradiction examples) challenges several pre-trained NLI models that performed well on the datasets on which they were trained. Naik et al. (2018) constructed a suite of “stress-tests”, each pertaining to some linguistic phenomenon, and showed that NLI models fail on many of them (e.g. numerical reasoning, logical negations, etc.). Another line of work showed that NLI models may reach a surprising performance level on the NLI test sets just by exploiting artifacts in the generation of the hypotheses, rather than learning to model the complex entailment relationship (Gururangan et al., 2018; Tsuchiya, 2018; Poliak et al., 2018). 3.1 We focus on the following two inference types from the diagnostic set of the GLUE benchmark (Wang et al., 2019) as test cases for our methodology. Fine-tuning on Challenge Datasets. Recently, Liu et al. (2019a) suggested that a model’s failure to address a specific linguistic phenomenon may be attributed to one of the following cases: either the NLI training data does not sufficiently represent this phenomenon (“dataset blind spot”) or the model is inherently incapable of learning to address this phenomenon. They suggested to fine-tune the NLI model"
K19-1019,N10-1145,0,0.0318421,"fficiently represented in the training data. In these challenge datasets, a model is trained on the general NLI datasets, i.e. SNLI or the Multi-Genre Natural Language Inference datasets (MultiNLI; Williams et al., 2018). It is then used as a black box to evaluate on a given test set. Background Neural NLI Models. Natural language inference is the task of identifying, given two text fragments, whether the second (hypothesis) can be inferred from the first (premise). While earlier models for these tasks relied on domain knowledge and lexical resources like WordNet (e.g MacCartney et al., 2008; Heilman and Smith, 2010), the release of the large-scale Stanford natural language inference dataset (SNLI; Bowman et al., 2015) shifted the focus to neural models which thrive given such 1 All datasets and resources are available https://github.com/ohadrozen/generalization. at 197 ing method (Section 3.3). (3) After generating diverse premises, we generate multiple matching synthetic hypotheses using a templating method (Section 3.4). As we generate synthetic hypotheses, we can make sure the premise-hypothesis pairs differ along our proposed diversity dimensions. (4) Finally, we define the train and test sets so tha"
K19-1019,Q18-1017,0,0.0395917,"Missing"
K19-1019,N19-1225,0,0.5625,"Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,roee.aharoni}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract instead attempt to learn them implicitly from the training data. Despite their success on common NLI dataset, recent challenge datasets designed for probing different linguistic phenomena showed that neural models often fail on particular inference types, like recognizing semantic relations and negation (Poliak et al., 2018; Naik et al., 2018; Glockner et al., 2018). Recently, Liu et al. (2019a) showed that when probing reveals a model’s failure on a specific linguistic phenomenon, it is often possible to amend this failure. They suggested to fine-tune the model on (a training section of) the challenge dataset itself, in order to teach it to address the specific target phenomenon, or in other words, to “inoculate” it against the adversarial data. Inoculation has two possible outcomes. The first - a success to address the phenomenon after fine-tuning - suggests the original training set did not cover this phenomenon sufficiently (“blind spot” of the dataset). A failure, on the other"
K19-1019,P19-1441,0,0.152844,"Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,roee.aharoni}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract instead attempt to learn them implicitly from the training data. Despite their success on common NLI dataset, recent challenge datasets designed for probing different linguistic phenomena showed that neural models often fail on particular inference types, like recognizing semantic relations and negation (Poliak et al., 2018; Naik et al., 2018; Glockner et al., 2018). Recently, Liu et al. (2019a) showed that when probing reveals a model’s failure on a specific linguistic phenomenon, it is often possible to amend this failure. They suggested to fine-tune the model on (a training section of) the challenge dataset itself, in order to teach it to address the specific target phenomenon, or in other words, to “inoculate” it against the adversarial data. Inoculation has two possible outcomes. The first - a success to address the phenomenon after fine-tuning - suggests the original training set did not cover this phenomenon sufficiently (“blind spot” of the dataset). A failure, on the other"
K19-1019,Q19-1027,1,0.83042,"atic token-based embeddings to dynamic contextsensitive ones. Notable contextual representations are ELMo (Peters et al., 2018b), BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and XLNet (Yang et al., 2019), which are pre-trained as language models on large corpora. Contextualized word embeddings have been used across a broad range of NLP tasks, outperforming the previous state-of-the-art models. Specifically, several works showed that they capture various types of linguistic knowledge, from syntactic to semantic and discourse relations (e.g. Peters et al., 2018a; Tenney et al., 2019; Shwartz and Dagan, 2019). Among many other tasks, NLI has also benefited from the use of contextualized word embeddings. The current state-of-the-art models use pre-trained contextualized word embeddings as their underlying representations, while fine-tuning on the NLI task (Liu et al., 2019b; Devlin et al., 2019). Despite their remarkable success on several datasets, it still remains unclear how these models represent the various linguistic phenomena required for solving the NLI tasks. Existing Drawbacks and Challenge Datasets. Training an NLI model in this end-to-end manner assumes that any inference type involved"
K19-1019,D08-1084,0,0.031929,"enon rather than to “learn” a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena – dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computation"
K19-1019,P12-3013,1,0.81305,"in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics ology, analyzing the ability of models to generalize across different data distributions when addressing a specific inference phenomenon. In particular, we suggest varying both training and test distributions alon"
K19-1019,C08-1066,0,0.0415253,"a target phenomenon rather than to “learn” a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena – dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computation"
K19-1019,P12-1030,1,0.827138,"ting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics ology, analyzing the ability of models to generalize across different data distributions when addressing a specific inference phenomenon. In particular, we suggest varying both training and test distributions along several linguistic"
K19-1019,P14-5010,0,0.0025597,"lled Data Splits The main motivation for our data splits is to control the variance between the train and test sets with respect to certain data dimensions. Specifically, for both inference types, we create a variance along the syntactic complexity dimension. Based on the premise template, we divided each dataset into 3 subsets with different syntactic complexity levels: simple, medium and complex, denoted by S, M and C respectively. We do so according to two criteria: sentence length and the depth in the constituency parsing tree in which the inference type occurs, using the Stanford Parser (Manning et al., 2014) (see Table 2).2 We also create a variance along different lexical dimensions. From the simple subset S we generate two additional subsets SLex1 and SLex2 in the following way. For each simple template, we first split its original instantiations into two groups, and then instantiate each such group separately into the template, creating two sets of instantiations of the same template s1 ∈ SLex1 and s2 ∈ SLex2 , which are syntactically similar by construction, yet lexically different. We repeat this process for the complex subset as well to create CLex1 and CLex2 . We further split the dative a"
K19-1019,C18-1198,0,0.249983,"Science Department, Bar-Ilan University, Ramat-Gan, Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,roee.aharoni}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract instead attempt to learn them implicitly from the training data. Despite their success on common NLI dataset, recent challenge datasets designed for probing different linguistic phenomena showed that neural models often fail on particular inference types, like recognizing semantic relations and negation (Poliak et al., 2018; Naik et al., 2018; Glockner et al., 2018). Recently, Liu et al. (2019a) showed that when probing reveals a model’s failure on a specific linguistic phenomenon, it is often possible to amend this failure. They suggested to fine-tune the model on (a training section of) the challenge dataset itself, in order to teach it to address the specific target phenomenon, or in other words, to “inoculate” it against the adversarial data. Inoculation has two possible outcomes. The first - a success to address the phenomenon after fine-tuning - suggests the original training set did not cover this phenomenon sufficiently (“"
K19-1019,L18-1239,0,0.110174,"Missing"
K19-1019,N18-1202,0,0.0387591,"2 large datasets. Typically, these models encode each of the premise and the hypothesis, combine them into a feature vector, and feed it into a classifier to make the entailment prediction. The encoding of the two sentences can be either independent of each other or dependent using an attention mechanism. These models typically do not rely on any external knowledge other than pre-trained word embeddings. Contextualized Word Embeddings. Recently, the word embedding paradigm shifted from static token-based embeddings to dynamic contextsensitive ones. Notable contextual representations are ELMo (Peters et al., 2018b), BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and XLNet (Yang et al., 2019), which are pre-trained as language models on large corpora. Contextualized word embeddings have been used across a broad range of NLP tasks, outperforming the previous state-of-the-art models. Specifically, several works showed that they capture various types of linguistic knowledge, from syntactic to semantic and discourse relations (e.g. Peters et al., 2018a; Tenney et al., 2019; Shwartz and Dagan, 2019). Among many other tasks, NLI has also benefited from the use of contextualized word embeddings. The c"
K19-1019,N18-1101,0,0.605324,"nstrate our methodology on two inference types, picked from the GLUE benchmark (Wang et al., 2019) diagnostic dataset: (1) dative alternation, a syntactic phenomenon, and (2) a specific type of numerical reasoning, pertaining to logical and arithmetic inference. To create our datasets, we introduce a templating method, by which we generated hundreds of synthetic examples from a single original sentence, while controlling the variance between the datasets.1 We employ a recent NLI model, based on the pretrained BERT masked language model (Devlin et al., 2019) fine-tuned on the MultiNLI dataset (Williams et al., 2018). For the dative alternation case, we find that the model struggles with generalizing over the syntactic dimension, requiring training over a relatively large variety of syntactically complex sentences. For the numerical reasoning case, we find that the model notably fails to generalize across diverse number ranges, a conclusion that might have been missed if we were to use only the original inoculation methodology. We hope our methodology will be adopted for additional NLP tasks, and specifically to a broader range of entailment inference types, as an avenue for developing robust NLI systems"
K19-1019,W18-5441,0,0.0897395,"Missing"
K19-1019,D18-1007,0,\N,Missing
N18-2035,N15-1098,0,0.0719028,"Missing"
N18-2035,D10-1115,0,0.0393654,"the model was only tested on a small dataset and performed similarly to previous methods. Compositional Representations In this approach, classification is based on a vector representing the NC (w1 w2 ), which is obtained by applying a function to its constituents’ distributional representations: ~vw1 , ~vw2 ∈ Rn . Various functions have been proposed in the literature. Mitchell and Lapata (2010) proposed 3 simple combinations of ~vw1 and ~vw2 (additive, multiplicative, dilation). Others suggested to represent compositions by applying linear functions, encoded as matrices, over word vectors. Baroni and Zamparelli (2010) focused on adjective-noun compositions (AN) and represented adjectives as matrices, nouns as vectors, and ANs as their multiplication. Matrices were learned with the objective of minimizing the distance between the learned vector and the observed vector (computed from corpus occurrences) of each AN. The full-additive model (Zanzotto et al., 2010; Dinu et al., 2013) is a similar approach that works on any two-word composition, multiplying each word by a square matrix: nc = A · ~vw1 + B · ~vw2 . Socher et al. (2012) suggested a non-linear composition model. A recursive neural network operates b"
N18-2035,W15-0122,0,0.212731,"= A · ~vw1 + B · ~vw2 . Socher et al. (2012) suggested a non-linear composition model. A recursive neural network operates bottom-up on the output of a constituency parser to represent variable-length phrases. Each constituent is represented by a vector that captures its meaning and a matrix that captures how it modifies the meaning of constituents that it combines with. For a binary NC, nc = g(W · [~vw1 ; ~vw2 ]), where W ∈ R2n×n and g is a non-linear function. These representations were used as features in NC classification, often achieving promising results (e.g. Van de Cruys et al., 2013; Dima and Hinrichs, 2015). However, Dima (2016) recently showed that similar performance is achieved by representing the NC as a concatenation of its constituent embeddings, and argued that it stems from memorizing prototypical words for each relation. For example, classifying any NC with the head oil to the SOURCE relation, regardless of the modifier. 2.2 3 Model We similarly investigate the use of paraphrasing for NC relation classification. To generate a signal for the joint occurrences of w1 and w2 , we follow the approach used by HypeNET (Shwartz et al., 2016). For an w1 w2 in the dataset, we collect all the depe"
N18-2035,W13-3206,0,0.127612,"hell and Lapata (2010) proposed 3 simple combinations of ~vw1 and ~vw2 (additive, multiplicative, dilation). Others suggested to represent compositions by applying linear functions, encoded as matrices, over word vectors. Baroni and Zamparelli (2010) focused on adjective-noun compositions (AN) and represented adjectives as matrices, nouns as vectors, and ANs as their multiplication. Matrices were learned with the objective of minimizing the distance between the learned vector and the observed vector (computed from corpus occurrences) of each AN. The full-additive model (Zanzotto et al., 2010; Dinu et al., 2013) is a similar approach that works on any two-word composition, multiplying each word by a square matrix: nc = A · ~vw1 + B · ~vw2 . Socher et al. (2012) suggested a non-linear composition model. A recursive neural network operates bottom-up on the output of a constituency parser to represent variable-length phrases. Each constituent is represented by a vector that captures its meaning and a matrix that captures how it modifies the meaning of constituents that it combines with. For a binary NC, nc = g(W · [~vw1 ; ~vw2 ]), where W ∈ R2n×n and g is a non-linear function. These representations wer"
N18-2035,D14-1162,0,0.0852669,"w1 and w2 ’s word embeddings to the path vector, to add distributional information: x = [~vw1 , ~vw2 , ~vP (w1 ,w2 ) ]. Potentially, this allows the network to utilize the contextual properties of each individual constituent, e.g. assigning high probability to SUBSTANCE - MATERIAL - INGREDIENT for edible w1 s (e.g. vanilla pudding, apple cake). Integrated-NC. We add the NC’s observed vector ~vnc as additional distributional input, providing the contexts in which w1 w2 occur as an NC: ~vnc = [~vw1 , ~vw2 , ~vnc , ~vP (w1 ,w2 ) ]. Like Dima (2016), we learn NC vectors using the GloVe algorithm (Pennington et al., 2014), by replacing each NC occurrence in the corpus with a single token. This information can potentially help clustering NCs that appear in similar contexts despite having low pairwise similarity scores between their constituents. For example, gun violence and abortion rights belong to the TOPIC relation and may appear in similar news-related contexts, while (gun, abortion) and (violence, rights) are dissimilar. 3.2 r = argmaxi ~oi We conjecture that label distribution averaging allows for more efficient training of path embeddings when a single NC contains multiple paths. 4 4.1 Evaluation Datase"
N18-2035,I11-1024,0,0.0998493,"∗ 2 Background Various tasks have been suggested to address noun-compound interpretation. NC paraphrasing extracts texts explicitly describing the implicit relation between the constituents, for example student protest is a protest LED BY, BE SPONSORED BY , or BE ORGANIZED BY students (e.g. Nakov and Hearst, 2006; Kim and Nakov, 2011; Hendrickx et al., 2013; Nulty and Costello, 2013). Compositionality prediction determines to what extent the meaning of the NC can be expressed in terms of the meaning of its constituents, e.g. spelling bee is non-compositional, as it is not related to bee (e.g. Reddy et al., 2011). In this paper we focus on the NC classification task, which is defined as follows: given a pre-defined set of relations, classify nc = w1 w2 to the relation that holds between w1 and w2 . We review the various 1 The code is available at https://github.com/ tensorflow/models/tree/master/research/ lexnet_nc. Work done during an internship at Google. 218 Proceedings of NAACL-HLT 2018, pages 218–224 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics features used in the literature for classification.2 2.1 may share the feature MADE OF. S´eaghdha and Copest"
N18-2035,S13-2025,0,0.460547,"Missing"
N18-2035,P16-1226,1,0.907596,"Missing"
N18-2035,D11-1060,0,0.11112,"e.g. Nakov and Hearst, 2006; Nulty and Costello, 2013), such as mapping coffee cup and garbage dump to the pattern [w1 ] CONTAINS [w2 ]. The second approach computes a representation for NCs from the distributional representation of their individual constituents. While this approach ∗ 2 Background Various tasks have been suggested to address noun-compound interpretation. NC paraphrasing extracts texts explicitly describing the implicit relation between the constituents, for example student protest is a protest LED BY, BE SPONSORED BY , or BE ORGANIZED BY students (e.g. Nakov and Hearst, 2006; Kim and Nakov, 2011; Hendrickx et al., 2013; Nulty and Costello, 2013). Compositionality prediction determines to what extent the meaning of the NC can be expressed in terms of the meaning of its constituents, e.g. spelling bee is non-compositional, as it is not related to bee (e.g. Reddy et al., 2011). In this paper we focus on the NC classification task, which is defined as follows: given a pre-defined set of relations, classify nc = w1 w2 to the relation that holds between w1 and w2 . We review the various 1 The code is available at https://github.com/ tensorflow/models/tree/master/research/ lexnet_nc. Work d"
N18-2035,D12-1110,0,0.145218,"s by applying linear functions, encoded as matrices, over word vectors. Baroni and Zamparelli (2010) focused on adjective-noun compositions (AN) and represented adjectives as matrices, nouns as vectors, and ANs as their multiplication. Matrices were learned with the objective of minimizing the distance between the learned vector and the observed vector (computed from corpus occurrences) of each AN. The full-additive model (Zanzotto et al., 2010; Dinu et al., 2013) is a similar approach that works on any two-word composition, multiplying each word by a square matrix: nc = A · ~vw1 + B · ~vw2 . Socher et al. (2012) suggested a non-linear composition model. A recursive neural network operates bottom-up on the output of a constituency parser to represent variable-length phrases. Each constituent is represented by a vector that captures its meaning and a matrix that captures how it modifies the meaning of constituents that it combines with. For a binary NC, nc = g(W · [~vw1 ; ~vw2 ]), where W ∈ R2n×n and g is a non-linear function. These representations were used as features in NC classification, often achieving promising results (e.g. Van de Cruys et al., 2013; Dima and Hinrichs, 2015). However, Dima (201"
N18-2035,R15-1082,0,0.0560406,"stake (2013) leveraged this “relational similarity” in a kernel-based classification approach. They combined the relational information with the complementary lexical features of each constituent separately. Two NCs labeled to the same relation may consist of similar constituents (paper-steel, cup-knife) and may also appear with similar paraphrases. Combining the two information sources has shown to be beneficial, but it was also noted that the relational information suffered from data sparsity: many NCs had very few paraphrases, and paraphrase similarity was based on ngram overlap. Recently, Surtani and Paul (2015) suggested to represent NCs in a vector space model (VSM) using paraphrases as features. These vectors were used to classify new NCs based on the nearest neighbor in the VSM. However, the model was only tested on a small dataset and performed similarly to previous methods. Compositional Representations In this approach, classification is based on a vector representing the NC (w1 w2 ), which is obtained by applying a function to its constituents’ distributional representations: ~vw1 , ~vw2 ∈ Rn . Various functions have been proposed in the literature. Mitchell and Lapata (2010) proposed 3 simpl"
N18-2035,P10-1070,0,0.662312,"Missing"
N18-2035,S13-2026,0,0.151304,"Missing"
N18-2035,C10-1142,0,0.652408,"in the literature. Mitchell and Lapata (2010) proposed 3 simple combinations of ~vw1 and ~vw2 (additive, multiplicative, dilation). Others suggested to represent compositions by applying linear functions, encoded as matrices, over word vectors. Baroni and Zamparelli (2010) focused on adjective-noun compositions (AN) and represented adjectives as matrices, nouns as vectors, and ANs as their multiplication. Matrices were learned with the objective of minimizing the distance between the learned vector and the observed vector (computed from corpus occurrences) of each AN. The full-additive model (Zanzotto et al., 2010; Dinu et al., 2013) is a similar approach that works on any two-word composition, multiplying each word by a square matrix: nc = A · ~vw1 + B · ~vw2 . Socher et al. (2012) suggested a non-linear composition model. A recursive neural network operates bottom-up on the output of a constituency parser to represent variable-length phrases. Each constituent is represented by a vector that captures its meaning and a matrix that captures how it modifies the meaning of constituents that it combines with. For a binary NC, nc = g(W · [~vw1 ; ~vw2 ]), where W ∈ R2n×n and g is a non-linear function. These"
N19-1233,P18-1008,0,0.0235404,"to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than stateof-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation. 1 Introduction Neural networks have revolutionized the field of text generation, in machine translation (Sutskever et al., 2014; Neubig, 2017; Luong et al., 2015; Chen et al., 2018), summarization (See et al., 2017), image captioning (You et al., 2016) and many other applications (Goldberg, 2017). Traditionally, text generation models are trained by going over a gold sequence of symbols (characters or words) from left-to-right, and maximizing the probability of the next symbol given the history, namely, a language modeling (LM) objective. A commonly discussed drawback of such LM-based text generation is exposure bias (Ranzato et al., 2015): during training, the model predicts the next token conditioned on the ground truth history, while at test time prediction is based o"
N19-1233,P02-1040,0,0.114519,"not guarantee an improvement in an extrinsic downstream task that uses a language model. However, perplexity often correlates with extrinsic measures (Jurafsky and Martin, 2018), and is the de-facto metric for evaluating the quality of language models today. GAN-based Text Generation Evaluation. By definition, a text GAN outputs a discrete sequence of symbols rather than a probability distribution. As a result, LM metrics cannot be applied to evaluate the generated text. Consequently, other metrics have been proposed: • N-gram overlap: (Yu et al., 2017; Press et al., 2017): Inspired by BLEU (Papineni et al., 2002), this measures whether n-grams generated by the model appear in a held-out corpus. A major drawback is that this metric favors conservative models that always generate very common text (e.g., “it is”). To mitigate this, selfBLEU has been proposed (Lu et al., 2018) as an additional metric, where overlap is measured between two independently sampled texts from the model. • LM score: The probability of generated text according to a pre-trained LM. This has the same problem of favoring conservative models. • Zhao et al. (2017) suggested an indirect score by training a LM on GAN-generated text, an"
N19-1233,W17-2629,0,0.367569,"ow et al., 2014) offer a solution for exposure bias. ∗ The authors contributed equally Originally introduced for images, GANs leverage a discriminator, which is trained to discriminate between real images and generated images via an adversarial loss. In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator. This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) (Yu et al., 2017; Guo et al., 2017; Press et al., 2017; Rajeswar et al., 2017), or a Convolutional Neural Network (CNN) (Gulrajani et al., 2017; Rajeswar et al., 2017). However, evaluating GANs is more difficult than evaluating LMs. While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution. Currently, there is no single evaluation metric for GAN-based text generation, and existing metrics that are based on n-gram overlap are known to lack robustness and have low correlation"
N19-1233,P17-1099,0,0.0335636,"text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than stateof-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation. 1 Introduction Neural networks have revolutionized the field of text generation, in machine translation (Sutskever et al., 2014; Neubig, 2017; Luong et al., 2015; Chen et al., 2018), summarization (See et al., 2017), image captioning (You et al., 2016) and many other applications (Goldberg, 2017). Traditionally, text generation models are trained by going over a gold sequence of symbols (characters or words) from left-to-right, and maximizing the probability of the next symbol given the history, namely, a language modeling (LM) objective. A commonly discussed drawback of such LM-based text generation is exposure bias (Ranzato et al., 2015): during training, the model predicts the next token conditioned on the ground truth history, while at test time prediction is based on predicted tokens, causing a trai"
P16-1226,W11-2501,0,0.716054,"heir path-based features. 4 Higher-dimensional embeddings seem not to improve performance, while hurting the training runtime. 2392 resource WordNet DBPedia Wikidata Yago relations instance hypernym, hypernym type subclass of, instance of subclass of random split lexical split train 49,475 20,335 test 17,670 6,610 val 3,534 1,350 all 70,679 28,295 Table 2: The number of instances in each dataset. Table 1: Hypernymy relations in each resource. 4 4.1 Dataset Creating Instances Neural networks typically require a large amount of training data, whereas the existing hypernymy datasets, like BLESS (Baroni and Lenci, 2011), are relatively small. Therefore, we followed the common methodology of creating a dataset using distant supervision from knowledge resources (Snow et al., 2004; Riedel et al., 2013). Following Snow et al. (2004), who constructed their dataset based on WordNet hypernymy, and aiming to create a larger dataset, we extract hypernymy relations from several resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Wikidata (Vrandeˇci´c, 2012) and Yago (Suchanek et al., 2007). All instances in our dataset, both positive and negative, are pairs of terms that are directly related in at least"
P16-1226,E12-1004,0,0.566917,"2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012), difference ~y −~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . Using neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014). 2.2 Path-based Methods A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set"
P16-1226,W09-4405,0,0.017932,"connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they co-occur in the corpus. DET parrot is a bird NOUN VERB DET NOUN Figure 1: An example d"
P16-1226,C92-2082,0,0.833402,"distributional methods, the decision whether y is a hypernym of x is based on the distributional representations of these terms. Lately, with the popularity of word embeddings (Mikolov et al., 2013), most focus has shifted towards supervised distributional methods, in which each (x, y) term-pair is represented using some combination of the terms’ embedding vectors. In contrast to distributional methods, in which the decision is based on the separate contexts of x and y, path-based methods base the decision on the lexico-syntactic paths connecting the joint occurrences of x and y in a corpus. Hearst (1992) identified a small set of frequent paths that indicate hypernymy, e.g. Y such as X. Snow et al. (2004) represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features. Using individual paths as features results in a huge, sparse feature space. While some paths are rare, they often consist of certain unimportant components. For instance, “Spelt is a species of wheat” and “Fantasy is a genre of fiction” yield two different paths: X be species of Y and X be genre of Y, while"
P16-1226,W09-2415,0,0.0288938,"Missing"
P16-1226,C08-1051,0,0.0338745,"2012). Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji and Kitsuregawa, 2008). In this paper, we present HypeNET, an integrated path-based and distributional method for hypernymy detection. Inspired by recent progress 2389 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2389–2398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in relation classification, we use a long shortterm memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to encode dependency paths. In order to create enough training data for our network, we followed previous methodology of constructing a dataset based on k"
P16-1226,D10-1108,0,0.185523,"searching for specific paths that indicate hypernymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the mult"
P16-1226,N15-1098,1,0.838612,"ed hypernymy detection, it is basically designed for classifying specificity level of related terms, rather than hypernymy in particular. Supervised To represent term-pairs with distributional features, we tried several state-of-the-art methods: concatenation ~x⊕~y (Baroni et al., 2012), difference ~y − ~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al. (2015) to perform best in this setting. We perform model selection on the validation set to select the best vectors, method and regularization factor (see the appendix). 2394 Path-based Distributional Combined method Snow Snow + Gen HypeNET Path-based SLQS (Santus et al., 2014) Best supervised (concatenation) HypeNET Integrated random split precision recall 0.843 0.452 0.852 0.561 0.811 0.716 0.491 0.737 0.901 0.637 0.913 0.890 F1 0.589 0.676 0.761 0.589 0.746 0.901 lexical split precision recall 0.760 0.438 0.759 0.530 0.691 0.632 0.375 0.610 0.754 0.551 0.809 0.617 F1 0.556 0.624 0.660 0.464 0.637"
P16-1226,P06-1101,0,0.584712,"rnymy. Rather than searching for specific paths that indicate hypernymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it i"
P16-1226,P15-2047,0,0.0290555,"in capturing the indicative information in such paths. In particular, several papers show improved performance using recurrent neural networks (RNN) that process a dependency path edge-by-edge. Xu et al. (2015; 2016) apply a separate long shortterm memory (LSTM) network to each sequence of words, POS tags, dependency labels and WordNet hypernyms along the path. A max-pooling layer on the LSTM outputs is used as the input of a network that predicts the classification. Other papers suggest incorporating additional network architectures to further improve performance (Nguyen and Grishman, 2015; Liu et al., 2015). While relation classification and hypernymy detection are both concerned with identifying semantic relations that hold for pairs of terms, they differ in a major respect. In relation classification the relation should be expressed in the given text, while in hypernymy detection, the goal is to recognize a generic lexical-semantic relation between terms that holds in many contexts. Accordingly, in relation classification a term-pair is represented 3 LSTM-based Hypernymy Detection We present HypeNET, an LSTM-based method for hypernymy detection. We first focus on improving path representation"
P16-1226,P06-2075,1,0.635141,"m (Nakashole et al., 2012). Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji and Kitsuregawa, 2008). In this paper, we present HypeNET, an integrated path-based and distributional method for hypernymy detection. Inspired by recent progress 2389 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2389–2398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in relation classification, we use a long shortterm memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to encode dependency paths. In order to create enough training data for our network, we followed previous methodology of cons"
P16-1226,J06-3003,0,0.0314179,"the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they co-occur in the corpus. DET parrot is a"
P16-1226,W03-1011,0,0.517909,"use of recurrent neural networks in the related task of relation classification (Section 2.3). 2.1 Distributional Methods Hypernymy detection is commonly addressed using distributional methods. In these methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus. Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifi"
P16-1226,D12-1104,0,0.728597,"ir co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features. Using individual paths as features results in a huge, sparse feature space. While some paths are rare, they often consist of certain unimportant components. For instance, “Spelt is a species of wheat” and “Fantasy is a genre of fiction” yield two different paths: X be species of Y and X be genre of Y, while both indicating that X is-a Y. A possible solution is to generalize paths by replacing words along the path with their part-of-speech tags or with wild cards, as done in the PATTY system (Nakashole et al., 2012). Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji"
P16-1226,C14-1212,0,0.26541,"mption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012), difference ~y −~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . Using neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014). 2.2 Path-based Methods A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths that indicate hypernymy relations"
P16-1226,P10-1134,0,0.0891516,"the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they co-occur in the corpus. DET parrot is a bird NOUN VERB DET NOUN Figure 1: An example dependency tree of the senten"
P16-1226,D15-1206,0,0.0373208,"ence, from the SemEval-2010 relation classification task dataset (Hendrickx et al., 2009): “The [apples]e1 are in the [basket]e2 ”. Here, the relation expressed between the target entities is Content − Container(e1 , e2 ). The shortest dependency paths between the target entities were shown to be informative for this task (Fundel et al., 2007). Recently, deep learning techniques showed good performance in capturing the indicative information in such paths. In particular, several papers show improved performance using recurrent neural networks (RNN) that process a dependency path edge-by-edge. Xu et al. (2015; 2016) apply a separate long shortterm memory (LSTM) network to each sequence of words, POS tags, dependency labels and WordNet hypernyms along the path. A max-pooling layer on the LSTM outputs is used as the input of a network that predicts the classification. Other papers suggest incorporating additional network architectures to further improve performance (Nguyen and Grishman, 2015; Liu et al., 2015). While relation classification and hypernymy detection are both concerned with identifying semantic relations that hold for pairs of terms, they differ in a major respect. In relation classifi"
P16-1226,D14-1162,0,0.0903761,"ical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012), difference ~y −~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . Using neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014). 2.2 Path-based Methods A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths that indicate hypernymy relations (e.g. Y such as X, X and other Y). In a later work, Snow et al. (2004) learned to detect hypernymy. Rath"
P16-1226,N13-1008,0,0.275869,"rnymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they c"
P16-1226,E14-1054,0,0.144588,"e methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus. Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012)"
P16-1226,C14-1097,0,0.351077,"Missing"
P16-1226,E14-4008,0,0.411297,"onal methods. In these methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus. Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baron"
P16-1226,K15-1018,1,0.863334,"l resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Wikidata (Vrandeˇci´c, 2012) and Yago (Suchanek et al., 2007). All instances in our dataset, both positive and negative, are pairs of terms that are directly related in at least one of the resources. These resources contain thousands of relations, some of which indicate hypernymy with varying degrees of certainty. To avoid including questionable relation types, we consider as denoting positive examples only indisputable hypernymy relations (Table 1), which we manually selected from the set of hypernymy indicating relations in Shwartz et al. (2015). Term-pairs related by other relations (including hyponymy), are considered as negative instances. Using related rather than random term-pairs as negative instances tests our method’s ability to distinguish between hypernymy and other kinds of semantic relatedness. We maintain a ratio of 1:4 positive to negative pairs in the dataset. Like Snow et al. (2004), we include only termpairs that have joint occurrences in the corpus, requiring at least two different dependency paths for each pair. 4.2 Random and Lexical Dataset Splits As our primary dataset, we perform standard random splitting, with"
P16-1226,P15-1146,0,0.119629,"Missing"
P16-1226,C16-1138,0,0.0293306,"Missing"
P16-1226,P14-1113,0,\N,Missing
P18-1111,P01-1008,0,0.465559,"Missing"
P18-1111,W09-2416,0,0.0261477,"Missing"
P18-1111,W15-0122,0,0.09736,"e constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decision on the relation inventory is somewhat arbitrary, and subsequently, the inter-annotator agreement is relatively low (Kim and Baldwin, 2007). Specifically, a noun-compound may fit into more than one relation: for instance, in Tratz (2011), business zone is labeled as CONTAINED (zone contains business), although it could also be labeled as PURPOSE (zone whose purpose is business). 2.2 Noun-compound Paraph"
P18-1111,D11-1060,0,0.026239,"plausible human-written paraphrases for each nouncompound, and systems had to rank them with the goal of high correlation with human judgments. In SemEval 2013 task 4 (Hendrickx et al., 2013), systems were expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not"
P18-1111,N15-1098,1,0.846239,"Missing"
P18-1111,S10-1051,0,0.180419,"e expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases in the corpus or have just a few. The approach of Van de Cruys et al. (2013) somewhat generalizes for unseen noun-compounds. They represented each noun-compound using a compositi"
P18-1111,E17-1083,0,0.035575,"pˆi = 78 M LPw M LPp [w1 ] cake [p] apple (23) made (23) made (1) [w1 ] (28) apple (2) [w2 ] (28) apple (4145) cake ... (3) [p] (4145) cake ... (1) [w1 ] (7891) of (3) [p] (7891) of (2) [w2 ] (78) [w2 ] containing [w1 ] ... (131) [w2 ] made of [w1 ] ... Figure 1: An illustration of the model predictions for w1 and p given the triplet (cake, made of, apple). The model predicts each component given the encoding of the other two components, successfully predicting ‘apple’ given ‘cake made of [w1 ]’, while predicting ‘[w2 ] containing [w1 ]’ for ‘cake [p] apple’. 2001; Ganitkevitch et al., 2013; Mallinson et al., 2017). If a certain concept can be described by an English noun-compound, it is unlikely that a translator chose to translate its foreign language equivalent to an explicit paraphrase instead. Another related task is Open Information Extraction (Etzioni et al., 2008), whose goal is to extract relational tuples from text. Most system focus on extracting verb-mediated relations, and the few exceptions that addressed noun-compounds provided partial solutions. Pal and Mausam (2016) focused on segmenting multi-word nouncompounds and assumed an is-a relation between the parts, as extracting (Francis Coll"
P18-1111,P07-1072,0,0.241429,"86/panic 1200 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1200–1211 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 2.1 Background Noun-compound Classification Noun-compound classification is the task concerned with automatically determining the semantic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decisio"
P18-1111,S13-2025,0,0.375031,"Missing"
P18-1111,S10-1052,0,0.0259719,"proach of Van de Cruys et al. (2013) somewhat generalizes for unseen noun-compounds. They represented each noun-compound using a compositional distributional vector (Mitchell and Lapata, 2010) and used it to predict paraphrases from the corpus. Similar noun-compounds are expected to have similar distributional representations and therefore yield the same paraphrases. For example, if the corpus does not contain paraphrases for plastic spoon, the model may predict the paraphrases of a similar compound such as steel knife. In terms of sharing information between semantically-similar paraphrases, Nulty and Costello (2010) and Surtani et al. (2013) learned “is-a” relations between paraphrases from the co-occurrences of various paraphrases with each other. For example, the specific ‘[w2 ] extracted from [w1 ]’ template (e.g. in the context of olive oil) generalizes to ‘[w2 ] made from [w1 ]’. One of the drawbacks of these systems is that they favor more frequent paraphrases, which may co-occur with a wide variety of more specific paraphrases. 2.3 Noun-compounds in other Tasks Noun-compound paraphrasing may be considered as a subtask of the general paraphrasing task, whose goal is to generate, given a text fragme"
P18-1111,E09-1071,0,0.383901,"Missing"
P18-1111,W16-1307,0,0.0260755,"n ‘cake made of [w1 ]’, while predicting ‘[w2 ] containing [w1 ]’ for ‘cake [p] apple’. 2001; Ganitkevitch et al., 2013; Mallinson et al., 2017). If a certain concept can be described by an English noun-compound, it is unlikely that a translator chose to translate its foreign language equivalent to an explicit paraphrase instead. Another related task is Open Information Extraction (Etzioni et al., 2008), whose goal is to extract relational tuples from text. Most system focus on extracting verb-mediated relations, and the few exceptions that addressed noun-compounds provided partial solutions. Pal and Mausam (2016) focused on segmenting multi-word nouncompounds and assumed an is-a relation between the parts, as extracting (Francis Collins, is, NIH director) from “NIH director Francis Collins”. Xavier and Lima (2014) enriched the corpus with compound definitions from online dictionaries, for example, interpreting oil industry as (industry, produces and delivers, oil) based on the WordNet definition “industry that produces and delivers oil”. This method is very limited as it can only interpret noun-compounds with dictionary entries, while the majority of English noun-compounds don’t have them (Nakov, 2013"
P18-1111,N07-1071,0,0.0470745,"previous methods, we train the model to predict either a paraphrase expressing the semantic relation of a noun-compound (predicting ‘[w2 ] made of [w1 ]’ given ‘apple cake’), or a missing constituent given a combination of paraphrase and noun-compound (predicting ‘apple’ given ‘cake made of [w1 ]’). Constituents and paraphrase templates are represented as continuous vectors, and semantically-similar paraphrase templates are embedded in proximity, enabling better generalization. Interpreting ‘parsley cake’ effectively reduces to identifying paraphrase templates whose “selectional preferences” (Pantel et al., 2007) on each constituent fit ‘parsley’ and ‘cake’. A qualitative analysis of the model shows that the top ranked paraphrases retrieved for each noun-compound are plausible even when the constituents never co-occur (Section 4). We evaluate our model on both the paraphrasing and the classification tasks (Section 5). On both tasks, the model’s ability to generalize leads to improved performance in challenging evaluation settings.1 1 The code is available at github.com/vered1986/panic 1200 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1200"
P18-1111,N15-1037,0,0.0202049,"nouncompound, and systems had to rank them with the goal of high correlation with human judgments. In SemEval 2013 task 4 (Hendrickx et al., 2013), systems were expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases in the corpus"
P18-1111,P17-1192,0,0.013017,", and systems had to rank them with the goal of high correlation with human judgments. In SemEval 2013 task 4 (Hendrickx et al., 2013), systems were expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases in the corpus or have just a few. The ap"
P18-1111,D14-1162,0,0.0865276,"Missing"
P18-1111,I11-1024,0,0.0713183,"ply a spoon made of silver and that monkey business is a business that buys or raises monkeys. In other cases, it seems that the strong prior on one constituent leads to ignoring the other, unrelated constituent, as in predicting “wedding made of diamond”. Finally, the “unrelated” paraphrase was predicted for a few compounds, but those are not necessarily non-compositional (application form, head teacher). We conclude that the model does not address compositionality and suggest to apply it only to compositional compounds, which may be recognized using compositionality prediction methods as in Reddy et al. (2011). 7 Our paraphrasing approach at its core assumes compositionality: only a noun-compound whose meaning is derived from the meanings of its constituent words can be rephrased using them. In §3.2 we added negative samples to the training data to simulate non-compositional nouncompounds, which are included in the classification dataset (§5.2). We assumed that these compounds, more often than compositional ones would consist of unrelated constituents (spelling bee, sacred cow), and added instances of random unrelated nouns with ‘[w2 ] is unrelated to [w1 ]’. Here, we assess whether our model succe"
P18-1111,N18-2035,1,0.854521,"ing of shares” instead of “holding of share”). 5.2 Classification Noun-compound classification is defined as a multiclass classification problem: given a pre-defined set of relations, classify w1 w2 to the relation that holds between w1 and w2 . Potentially, the corpus co-occurrences of w1 and w2 may contribute to the classification, e.g. ‘[w2 ] held at [w1 ]’ indicates a TIME relation. Tratz and Hovy (2010) included such features in their classifier, but ablation tests showed that these features had a relatively small contribution, probably due to the sparseness of the paraphrases. Recently, Shwartz and Waterson (2018) showed that paraphrases may contribute to the classification when represented in a continuous space. 1206 Model. We generate a paraphrase vector representation par(w ~ 1 w2 ) for a given noun-compound w1 w2 as follows. We predict the indices of the k most likely paraphrases: pˆ1 , ..., pˆk = argmaxk pˆ, where pˆ is the distribution on the paraphrase vocabulary Vp , as defined in Equation 1. We then encode each paraphrase using the biLSTM, and average the paraphrase vectors, weighted by their confidence scores in pˆ: Pk pˆi ˆpˆi · V~p i=1 p par(w ~ (3) Pk 1 w2 ) = ˆpˆi i=1 p We train a linear"
P18-1111,D12-1110,0,0.707096,"ion Noun-compound classification is the task concerned with automatically determining the semantic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decision on the relation inventory is somewhat arbitrary, and subsequently, the inter-annotator agreement is relatively low (Kim and Baldwin, 2007). Specifically, a noun-compound may fit into more than one relation: for instance, in Tratz (2011), business zone is labeled as CONTAIN"
P18-1111,S13-2028,0,0.639342,"vide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases in the corpus or have just a few. The approach of Van de Cruys et al. (2013) somewhat generalizes for unseen noun-compounds. They represented each noun-compound using a compositional distributional ve"
P18-1111,P10-1070,0,0.833202,"roceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1200–1211 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 2.1 Background Noun-compound Classification Noun-compound classification is the task concerned with automatically determining the semantic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decision on the relation invento"
P18-1111,S13-2026,0,0.341434,"Missing"
P18-1111,S13-2027,0,0.756238,"e understanding tasks, especially given the prevalence of nouncompounds in English (Nakov, 2013). The interpretation of noun-compounds has been addressed in the literature either by classifying them to a fixed inventory of ontological relationships (e.g. Nastase and Szpakowicz, 2003) or by generating various free text paraphrases that describe the relation in a more expressive manner (e.g. Hendrickx et al., 2013). Methods dedicated to paraphrasing nouncompounds usually rely on corpus co-occurrences of the compound’s constituents as a source of explicit relation paraphrases (e.g. Wubben, 2010; Versley, 2013). Such methods are unable to generalize for unseen noun-compounds. Yet, most noun-compounds are very infrequent in text (Kim and Baldwin, 2007), and humans easily interpret the meaning of a new noun-compound by generalizing existing knowledge. For example, consider interpreting parsley cake as a cake made of parsley vs. resignation cake as a cake eaten to celebrate quitting an unpleasant job. We follow the paraphrasing approach and propose a semi-supervised model for paraphrasing noun-compounds. Differently from previous methods, we train the model to predict either a paraphrase expressing the"
P18-1111,S10-1058,0,0.364337,"atural language understanding tasks, especially given the prevalence of nouncompounds in English (Nakov, 2013). The interpretation of noun-compounds has been addressed in the literature either by classifying them to a fixed inventory of ontological relationships (e.g. Nastase and Szpakowicz, 2003) or by generating various free text paraphrases that describe the relation in a more expressive manner (e.g. Hendrickx et al., 2013). Methods dedicated to paraphrasing nouncompounds usually rely on corpus co-occurrences of the compound’s constituents as a source of explicit relation paraphrases (e.g. Wubben, 2010; Versley, 2013). Such methods are unable to generalize for unseen noun-compounds. Yet, most noun-compounds are very infrequent in text (Kim and Baldwin, 2007), and humans easily interpret the meaning of a new noun-compound by generalizing existing knowledge. For example, consider interpreting parsley cake as a cake made of parsley vs. resignation cake as a cake eaten to celebrate quitting an unpleasant job. We follow the paraphrasing approach and propose a semi-supervised model for paraphrasing noun-compounds. Differently from previous methods, we train the model to predict either a paraphras"
P18-1111,xavier-lima-2014-boosting,0,0.138367,"en paraphrases for each nouncompound, and systems had to rank them with the goal of high correlation with human judgments. In SemEval 2013 task 4 (Hendrickx et al., 2013), systems were expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases i"
P18-1111,C10-1142,0,0.524897,"un-compound Classification Noun-compound classification is the task concerned with automatically determining the semantic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decision on the relation inventory is somewhat arbitrary, and subsequently, the inter-annotator agreement is relatively low (Kim and Baldwin, 2007). Specifically, a noun-compound may fit into more than one relation: for instance, in Tratz (2011), business zone"
P18-2103,D15-1075,0,0.778341,"prehension, we create a new NLI test set with examples that capture various kinds of lexical knowledge (Table 1). For example, that champagne is a type of wine (hypernymy), and that saxophone and electric guitar are different musical instruments (co-hyponyms). To isolate lexical knowledge aspects, our constructed examples contain only words that appear both in the training set and in pre-trained embeddings, and differ by a single word from sentences in the training set. Introduction Recognizing textual entailment (RTE) (Dagan et al., 2013), recently framed as natural language inference (NLI) (Bowman et al., 2015) is a task concerned with identifying whether a premise sentence entails, contradicts or is neutral with the hypothesis sentence. Following the release of the large-scale SNLI dataset (Bowman et al., 2015), many end-to-end neural models have been developed for the task, achieving high accuracy on the test set. As opposed to previous-generation methods, which relied heavily on lexical resources, neural models only make use of pre-trained word embeddings. The few efforts to incorporate external lexical knowledge resulted in negligible performance gain (Chen et al., 2018). This raises the questio"
P18-2103,P16-2022,0,0.0330781,"ta needed to learn the required lexical knowledge, it may be available in the other datasets, which are presumably richer. We chose models which are amongst the best performing within their approaches (excluding ensembles) and have available code. All models are based on pre-trained GloVe embeddings (Pennington et al., 2014), which are either fine-tuned during training (R ESIDUAL -S TACKED -E NCODER and ESIM) or stay fixed (D ECOMPOSABLE ATTENTION ). All models predict the label using a concatenation of features derived from the sentence representations (e.g. maximum, mean), for example as in Mou et al. (2016). We use the recommended hyper-parameters for each model, as they appear in the provided code. 4.3 Table 3 displays the results for all the models on the original SNLI test set and the new test set. Despite the task being considerably simpler, the drop in performance is substantial, ranging from 11 to 33 points in accuracy. Adding MultiNLI to the training data somewhat mitigates this drop in accuracy, thanks to almost doubling the amount of training data. We note that adding SciTail to the training data did not similarly improve the performance; we conjecture that this stems from the differenc"
P18-2103,P16-1139,0,0.042219,"Missing"
P18-2103,W17-5308,0,0.0651015,"Missing"
P18-2103,D16-1244,0,0.287562,"Missing"
P18-2103,P18-1224,0,0.434231,"guage inference (NLI) (Bowman et al., 2015) is a task concerned with identifying whether a premise sentence entails, contradicts or is neutral with the hypothesis sentence. Following the release of the large-scale SNLI dataset (Bowman et al., 2015), many end-to-end neural models have been developed for the task, achieving high accuracy on the test set. As opposed to previous-generation methods, which relied heavily on lexical resources, neural models only make use of pre-trained word embeddings. The few efforts to incorporate external lexical knowledge resulted in negligible performance gain (Chen et al., 2018). This raises the question whether (1) neural methods are inherently stronger, obviating the need of external lexical knowledge; (2) large-scale training data allows for implicit learning of previously explicit lexical knowledge; or (3) the NLI datasets are simpler than early RTE datasets, requiring less knowledge. The performance on the new test set is substantially worse across systems, demonstrating that the SNLI test set alone is not a sufficient measure of language understanding capabilities. Our results are in line with Gururangan et al. (2018) and Poliak et al. (2018), who showed that t"
P18-2103,P17-1152,0,0.223659,"s (see Table 1). The generation steps are detailed below. Neural Approaches for NLI. Following the release of SNLI, there has been tremendous interest in the task, and many end-to-end neural models were developed, achieving promising results.2 Methods are divided into two main approaches. Sentence-encoding models (e.g. Bowman et al., 2015, 2016; Nie and Bansal, 2017; Shen et al., 2018) encode the premise and hypothesis individually, while attention-based models align words in the premise with similar words in the hypothesis, encoding the two sentences together (e.g. Rockt¨aschel et al., 2016; Chen et al., 2017). Replacement Words. We collected the replacement words using online resources for English learning.3 The newly introduced words are all present in the SNLI training set: from occurrence in a single training example (“Portugal”) up to 248,051 examples (“man”), with a mean of 3,663.1 and a median of 149.5. The words are also available in the pre-trained embeddings vocabulary. The goal of this constraint is to isolate lexical knowledge aspects, and evaluate the models’ ability to generalize and make new inferences for known words. 2 See the SNLI leaderboard for a comprehensive list: https://nlp."
P18-2103,D14-1162,0,0.0842781,"dels trained on SNLI or a union of SNLI with another dataset (MultiNLI, SciTail), and tested on the original SNLI test set and the new test set. and the MultiNLI train set, and (3) a union of the SNLI train set and the SciTail train set. The motivation is that while SNLI might lack the training data needed to learn the required lexical knowledge, it may be available in the other datasets, which are presumably richer. We chose models which are amongst the best performing within their approaches (excluding ensembles) and have available code. All models are based on pre-trained GloVe embeddings (Pennington et al., 2014), which are either fine-tuned during training (R ESIDUAL -S TACKED -E NCODER and ESIM) or stay fixed (D ECOMPOSABLE ATTENTION ). All models predict the label using a concatenation of features derived from the sentence representations (e.g. maximum, mean), for example as in Mou et al. (2016). We use the recommended hyper-parameters for each model, as they appear in the provided code. 4.3 Table 3 displays the results for all the models on the original SNLI test set and the new test set. Despite the task being considerably simpler, the drop in performance is substantial, ranging from 11 to 33 poi"
P18-2103,S18-2023,0,0.212936,"Missing"
P18-2103,N18-2017,0,0.292743,"nowledge resulted in negligible performance gain (Chen et al., 2018). This raises the question whether (1) neural methods are inherently stronger, obviating the need of external lexical knowledge; (2) large-scale training data allows for implicit learning of previously explicit lexical knowledge; or (3) the NLI datasets are simpler than early RTE datasets, requiring less knowledge. The performance on the new test set is substantially worse across systems, demonstrating that the SNLI test set alone is not a sufficient measure of language understanding capabilities. Our results are in line with Gururangan et al. (2018) and Poliak et al. (2018), who showed that the label can be identified by looking only at the hypothesis and exploiting annotation artifacts such as word choice and sentence length. Further investigation shows that what mostly affects the systems’ ability to correctly predict a test example is the amount of similar examples found in the training set. Given that training data will always be limited, this is a rather inefficient way to learn lexical inferences, stressing the need to develop methods that do this more 1 The contradiction example follows the assumption in Bowman et al. (2015) that"
P18-2103,N18-1101,0,0.180361,"e the relation between sentences given that they describe the same event. Hence, sentences that differ by a single mutually-exclusive term should be considered contradicting, as in “The president visited Alabama” and “The president visited Mississippi”. This differs from traditional RTE datasets, which do not assume event coreference, and in which such sentence-pairs would be considered neutral. Following criticism on the simplicity of the dataset, stemming mostly from its narrow domain, two additional datasets have been collected. The MultiNLI dataset (Multi-Genre Natural Language Inference, Williams et al., 2018) was collected similarly to SNLI, though covering a wider range of genres, and supporting a cross-genre evaluation. The SciTail dataset (Khot et al., 2018), created from science exams, is somewhat different from the two datasets, being smaller (27,026 examples), and labeled only as entailment or neutral. The domain makes this dataset different in nature from the other two datasets, and it consists of more factual sentences rather than scene descriptions. 3 Data Collection We construct a test set with the goal of evaluating the ability of state-of-the-art NLI models to make inferences that requ"
P18-2103,D17-1215,0,0.122855,"e on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences. 1 A little girl is very sad. A little girl is very unhappy. A couple drinking wine A couple drinking champagne Label contradiction1 entailment neutral Table 1: Examples from the new test set. In this paper we show that state-of-the-art NLI systems are limited in their generalization ability, and fail to capture many simple inferences that require lexical and world knowledge. Inspired by the work of Jia and Liang (2017) on reading comprehension, we create a new NLI test set with examples that capture various kinds of lexical knowledge (Table 1). For example, that champagne is a type of wine (hypernymy), and that saxophone and electric guitar are different musical instruments (co-hyponyms). To isolate lexical knowledge aspects, our constructed examples contain only words that appear both in the training set and in pre-trained embeddings, and differ by a single word from sentences in the training set. Introduction Recognizing textual entailment (RTE) (Dagan et al., 2013), recently framed as natural language in"
P18-2103,Q14-1006,0,0.040736,"test set. This raises the question whether the small performance gap is a result of the model not capturing lexical knowledge well, or the SNLI test set not requiring this knowledge in the first place. effectively. Our test set can be used to evaluate such models’ ability to recognize lexical inferences, and it is available at https://github. com/BIU-NLP/Breaking_NLI. 2 Background NLI Datasets. The SNLI dataset (Stanford Natural Language Inference, Bowman et al., 2015) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Young et al. (2014), while hypotheses were generated by crowd-sourced workers who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Workers were instructed to judge the relation between sentences given that they describe the same event. Hence, sentences that differ by a single mutually-exclusive term should be considered contradicting, as in “The president visited Alabama” and “The president visited Mississippi”. This differs from traditional RTE datasets, which do not assume event coreference, and in which such sentence-pairs would be considered neutral. Following criti"
P19-1409,P16-1061,0,0.132519,"tity coreference annotations were first added in EECB, covering both common nouns and named entities. ECB+ increased the difficulty level by adding a second set of documents for each topic (subtopic), discussing a different event of the same type (Tara Reid enters a rehab center vs. Lindsay Lohan enters a rehab center). The annotation is not exhaustive, where only a number of salient events and entities in each topic are annotated. 2.2 Models Entity Coreference. Of all the coreference resolution variants, the most well-studied is WD entity coreference resolution (e.g. Durrett and Klein, 2013; Clark and Manning, 2016). The current best performing model is a neural end-to-end system which considers all spans as potential entity 1 The code is available at https://github.com/ shanybar/event_entity_coref_ecb_plus. mentions, and learns distributions over possible antecedents for each (Lee et al., 2017). CD entity coreference has received less attention (e.g. Bagga and Baldwin, 1998b; Rao et al., 2010; Dutta and Weikum, 2015), often addressing the narrower task of entity linking, which links mentions of known named entities to their corresponding knowledge base entries (Shen et al., 2015). Event Coreference. Eve"
P19-1409,W15-0801,0,0.821338,"1, 23, 34, 35; test: 36-45. acquisition”) as predicates and their Arg0. • We use the spaCy dependency parser (Honnibal and Montani, 2017) to identify verbal event mentions whose subject and object are entities, and add those entities as their Arg0 and Arg1 roles, respectively. • Following Lee et al. (2012), for a given event mention, we consider its closest left (right) entity mention as its Arg0 (Arg1) role. 4 Experimental Setup We use the ECB+ corpus, which is the largest dataset consisting of within- and cross-document coreference annotations for entities and events. We follow the setup of Cybulska and Vossen (2015b), which was also employed by KenyonDean et al. (2018). This setup uses a subset of the annotations which has been validated for correctness by Cybulska and Vossen (2014) and allocates a larger portion of the dataset for training (see Table 1). Since the ECB+ corpus only annotates a part of the mentions, the setup uses the gold-standard event and entity mentions rather, and does not require specific treatment for unannotated mentions during evaluation. A different setup was carried out by Yang et al. (2015) and Choubey and Huang (2017). They used the full ECB+ corpus, including parts with kno"
P19-1409,D13-1203,0,0.0714633,"for event coreference. Entity coreference annotations were first added in EECB, covering both common nouns and named entities. ECB+ increased the difficulty level by adding a second set of documents for each topic (subtopic), discussing a different event of the same type (Tara Reid enters a rehab center vs. Lindsay Lohan enters a rehab center). The annotation is not exhaustive, where only a number of salient events and entities in each topic are annotated. 2.2 Models Entity Coreference. Of all the coreference resolution variants, the most well-studied is WD entity coreference resolution (e.g. Durrett and Klein, 2013; Clark and Manning, 2016). The current best performing model is a neural end-to-end system which considers all spans as potential entity 1 The code is available at https://github.com/ shanybar/event_entity_coref_ecb_plus. mentions, and learns distributions over possible antecedents for each (Lee et al., 2017). CD entity coreference has received less attention (e.g. Bagga and Baldwin, 1998b; Rao et al., 2010; Dutta and Weikum, 2015), often addressing the narrower task of entity linking, which links mentions of known named entities to their corresponding knowledge base entries (Shen et al., 201"
P19-1409,Q15-1002,0,0.0451854,"ach topic are annotated. 2.2 Models Entity Coreference. Of all the coreference resolution variants, the most well-studied is WD entity coreference resolution (e.g. Durrett and Klein, 2013; Clark and Manning, 2016). The current best performing model is a neural end-to-end system which considers all spans as potential entity 1 The code is available at https://github.com/ shanybar/event_entity_coref_ecb_plus. mentions, and learns distributions over possible antecedents for each (Lee et al., 2017). CD entity coreference has received less attention (e.g. Bagga and Baldwin, 1998b; Rao et al., 2010; Dutta and Weikum, 2015), often addressing the narrower task of entity linking, which links mentions of known named entities to their corresponding knowledge base entries (Shen et al., 2015). Event Coreference. Event coreference is considered a more difficult task, mostly due to the more complex structure of event mentions. While entity mentions are mostly noun phrases, event mentions may consist of a verbal predicate (acquire) or a nominalization (acquisition), where these are attached to arguments, including event participants and spatio-temporal information. Early models employed lexical features (e.g. head lemma,"
P19-1409,H05-1004,0,0.619197,"hey only evaluated their systems on the subset of predicted mentions which were also gold mentions. Finally, their evaluation setup was criticized by Upadhyay et al. (2016) for ignoring singletons (cluster with a single mention), effectively making the task simpler; and for evaluating each sub-topic separately, which entails ignoring incorrect coreference links across sub-topics. Evaluation Metrics. We use the official CoNLL scorer (Pradhan et al., 2014),5 and report the performance on the common coreference resolution metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998a), CEAF-e (Luo, 2005), and CoNLL F1 , the average of the 3 metrics. 5 Baselines We compare our full model to published results on ECB+, available for event coreference only, as well as to a disjoint variant of our model and a deterministic lemma baseline.6 C LUSTER +L EMMA. We first cluster the documents to topics (Section 3.3), and then group mentions within the same document cluster which share the same head lemma. This baseline differs from the lemma baseline of Kenyon-Dean et al. (2018) which is applied across topics. CV (Cybulska and Vossen, 2015a) is a supervised method for event coreference, based on discre"
P19-1409,P14-5010,0,0.00432062,"://github.com/minimaxir/ char-embeddings We set the merging threshold in the training step to δ1 = 0.5. We tune the threshold for inference step on the validation set to δ2 = 0.5. To cluster documents into topics at inference time, we use the K-Means algorithm implemented in ScikitLearn (Pedregosa et al., 2011). Documents are represented using TF-IDF scores of unigrams, bigrams, and trigrams, excluding stop words. We set K = 20 based on the Silhouette Coefficient method (Rousseeuw, 1987), which successfully reconstructs the number of test sub-topics. During inference, we use Stanford CoreNLP (Manning et al., 2014) to initialize within-document entity coreference clusters. Identifying Predicate-Argument Structures. To extract relations between events and entities we follow previous work (Lee et al., 2012; Yang et al., 2015; Choubey and Huang, 2017) and extract predicate-argument structures using SwiRL (Surdeanu et al., 2007), a semantic role labeling (SRL) system. To increase the coverage we apply additional heuristics: • Since SwiRL only identifies verbal predicates, we follow Lee et al. (2012) and consider nominal event mentions with possesors (“Amazon’s 4183 # Topics # Sub-topics # Documents # Senten"
P19-1409,S18-2001,0,0.30852,"a joint neural architecture for CDCR. In our joint model, an event (entity) mention representation is aware of other entities (events) that are related to it by predicateargument structure. We cluster mentions based on a learned pairwise mention coreference scorer. A disjoint variant of our model, on its own, improves upon the previous state-of-the-art for event 4179 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4179–4189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics coreference on the ECB+ corpus (Kenyon-Dean et al., 2018) by 9.5 CoNLL F1 points. To the best of our knowledge, we are the first to report performance on the entity coreference task in ECB+. Our joint model further improves performance upon the disjoint model by 1.2 points for entities and 1 point for events (statistically significant with p < 0.001). Our analysis further shows that each of the mention representation components contributes to the model’s performance.1 2 Background and Related Work Coreference resolution is the task of clustering text spans that refer to the same entity or event. Variants of the task differ on two axes: (1) resolving"
P19-1409,N18-1023,0,0.0290164,"nes: 1. 2018 Nobel prize for physics goes to Donna Strickland 2. Prof. Strickland is awarded the Nobel prize for physics Both sentences refer to the same entities (Donna Strickland and the Nobel prize for physics) and the same event (awarding the prize), using different words. In coreference resolution, the goal is to cluster expressions that refer to the same entity or event in a text, whether within a single document or across a document collection. Recently, there has been increasing interest in crosstext inferences, for example in question answering (Welbl et al., 2018; Yang et al., 2018; Khashabi et al., 2018; Postma et al., 2018). Such applications would benefit from effective cross-document coreference resolution. Despite the importance of the task, the focus of most coreference resolution research has been on its within-document variant, and rather little on cross-document coreference (CDCR). The latter is sometimes addressed partially using entity linking, which links mentions of an entity to its knowledge base entry. However, cross-document entity coreference is substantially broader than entity linking, addressing also mentions of common nouns and unfamiliar named entities. The commonly used"
P19-1409,D12-1045,0,0.107405,"Bar-Ilan University 2 Intel AI Lab, Israel 3 Ubiquitous Knowledge Processing Lab, Technische Universitat Darmstadt, Germany {shanyb21,vered1986}@gmail.com, alon.eirew@intel.com {bugert,reimers}@ukp.informatik.tu-darmstadt.de, dagan@cs.biu.ac.il Abstract Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications. Despite the task’s importance, research focus was given mostly to withindocument entity coreference, with rather little attention to the other variants. We propose a neural architecture for cross-document coreference resolution. Inspired by Lee et al. (2012), we jointly model entity and event coreference. We represent an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures. Our model outperforms the previous state-of-the-art event coreference model on ECB+, while providing the first entity coreference results on this corpus. Our analysis confirms that all our representation elements, including the mention span itself, its context, and the relation to other mentions contribute to the model’s success. 1 Introduction Recognizing that various textual spans acros"
P19-1409,D14-1162,0,0.0898539,"ed clustering configurations that are gradually improved during the training. The training procedure differs from the inference procedure by using the gold standard topic clusters and by initializing the entity clusters with the gold standard within-document coreference clusters. We do so in order to reduce the noise during training. 3.5 Implementation Details Our model is implemented in PyTorch (Paszke et al., 2017), using the ADAM optimizer (Kingma and Ba, 2014) with a minibatch size of 16. We initialize the word-level representations to the pretrained 300 dimensional GloVe word embeddings (Pennington et al., 2014), and keep them fixed during training. The character representations are learned using an LSTM with hidden size 50. We initialized them with pre-trained character embeddings4 . Each scorer consists of a sigmoid output layer and two hidden layers with 4261 neurons activated by ReLU function (Nair and Hinton, 2010). 4 Available at https://github.com/minimaxir/ char-embeddings We set the merging threshold in the training step to δ1 = 0.5. We tune the threshold for inference step on the validation set to δ2 = 0.5. To cluster documents into topics at inference time, we use the K-Means algorithm imp"
P19-1409,D17-1018,0,0.311646,"Lohan enters a rehab center). The annotation is not exhaustive, where only a number of salient events and entities in each topic are annotated. 2.2 Models Entity Coreference. Of all the coreference resolution variants, the most well-studied is WD entity coreference resolution (e.g. Durrett and Klein, 2013; Clark and Manning, 2016). The current best performing model is a neural end-to-end system which considers all spans as potential entity 1 The code is available at https://github.com/ shanybar/event_entity_coref_ecb_plus. mentions, and learns distributions over possible antecedents for each (Lee et al., 2017). CD entity coreference has received less attention (e.g. Bagga and Baldwin, 1998b; Rao et al., 2010; Dutta and Weikum, 2015), often addressing the narrower task of entity linking, which links mentions of known named entities to their corresponding knowledge base entries (Shen et al., 2015). Event Coreference. Event coreference is considered a more difficult task, mostly due to the more complex structure of event mentions. While entity mentions are mostly noun phrases, event mentions may consist of a verbal predicate (acquire) or a nominalization (acquisition), where these are attached to argu"
P19-1409,N18-1202,0,0.045712,"e over the mention’s words. Character-level representations are complementary, and may help with out-of-vocabulary words and spelling variations. We compute them by encoding the span using a character-based LSTM (Hochreiter and Schmidhuber, 1997). The span vector ~s(m) is a concatenation of the wordand character-level vectors. Context. The context surrounding a mention may indicate its compatibility with other candidate mentions (Clark and Manning, 2016; Lee et al., 2017; Kenyon-Dean et al., 2018). To model context, we use ELMo, contextual representations derived from a neural language model (Peters et al., 2018). ELMo has recently improved performance on several challenging NLP tasks, including within-document entity coreference resolution (Lee et al., 2018). We set the context vector ~c(m) to the contextual representation of m’s head word, taking the average of the 3 ELMo layers. Semantic dependency to other mentions. To model dependencies between event and entity clusters, we identify semantic role relationships between their mentions using a semantic role labeling (SRL) system. For a given event mention mvi , we extract its arguments, focusing on 4 semantic roles of interest: Arg0, Arg1, location,"
P19-1409,N18-2108,0,0.0297828,"e for physics goes to Donna Strickland 2. Prof. Strickland is awarded the Nobel prize for physics Both sentences refer to the same entities (Donna Strickland and the Nobel prize for physics) and the same event (awarding the prize), using different words. In coreference resolution, the goal is to cluster expressions that refer to the same entity or event in a text, whether within a single document or across a document collection. Recently, there has been increasing interest in crosstext inferences, for example in question answering (Welbl et al., 2018; Yang et al., 2018; Khashabi et al., 2018; Postma et al., 2018). Such applications would benefit from effective cross-document coreference resolution. Despite the importance of the task, the focus of most coreference resolution research has been on its within-document variant, and rather little on cross-document coreference (CDCR). The latter is sometimes addressed partially using entity linking, which links mentions of an entity to its knowledge base entry. However, cross-document entity coreference is substantially broader than entity linking, addressing also mentions of common nouns and unfamiliar named entities. The commonly used dataset for CDCR is E"
P19-1409,S18-1009,0,0.0444531,"Missing"
P19-1409,P14-2006,0,0.084834,"known annotation errors. At test time, they rely on the output of a mention extraction tool (Yang et al., 2015). To address the partial annotation of the corpus, they only evaluated their systems on the subset of predicted mentions which were also gold mentions. Finally, their evaluation setup was criticized by Upadhyay et al. (2016) for ignoring singletons (cluster with a single mention), effectively making the task simpler; and for evaluating each sub-topic separately, which entails ignoring incorrect coreference links across sub-topics. Evaluation Metrics. We use the official CoNLL scorer (Pradhan et al., 2014),5 and report the performance on the common coreference resolution metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998a), CEAF-e (Luo, 2005), and CoNLL F1 , the average of the 3 metrics. 5 Baselines We compare our full model to published results on ECB+, available for event coreference only, as well as to a disjoint variant of our model and a deterministic lemma baseline.6 C LUSTER +L EMMA. We first cluster the documents to topics (Section 3.3), and then group mentions within the same document cluster which share the same head lemma. This baseline differs from the lemma baseline of"
P19-1409,C16-1183,0,0.3126,"setup uses the gold-standard event and entity mentions rather, and does not require specific treatment for unannotated mentions during evaluation. A different setup was carried out by Yang et al. (2015) and Choubey and Huang (2017). They used the full ECB+ corpus, including parts with known annotation errors. At test time, they rely on the output of a mention extraction tool (Yang et al., 2015). To address the partial annotation of the corpus, they only evaluated their systems on the subset of predicted mentions which were also gold mentions. Finally, their evaluation setup was criticized by Upadhyay et al. (2016) for ignoring singletons (cluster with a single mention), effectively making the task simpler; and for evaluating each sub-topic separately, which entails ignoring incorrect coreference links across sub-topics. Evaluation Metrics. We use the official CoNLL scorer (Pradhan et al., 2014),5 and report the performance on the common coreference resolution metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998a), CEAF-e (Luo, 2005), and CoNLL F1 , the average of the 3 metrics. 5 Baselines We compare our full model to published results on ECB+, available for event coreference only, as well a"
P19-1409,M95-1005,0,0.91763,"l., 2015). To address the partial annotation of the corpus, they only evaluated their systems on the subset of predicted mentions which were also gold mentions. Finally, their evaluation setup was criticized by Upadhyay et al. (2016) for ignoring singletons (cluster with a single mention), effectively making the task simpler; and for evaluating each sub-topic separately, which entails ignoring incorrect coreference links across sub-topics. Evaluation Metrics. We use the official CoNLL scorer (Pradhan et al., 2014),5 and report the performance on the common coreference resolution metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998a), CEAF-e (Luo, 2005), and CoNLL F1 , the average of the 3 metrics. 5 Baselines We compare our full model to published results on ECB+, available for event coreference only, as well as to a disjoint variant of our model and a deterministic lemma baseline.6 C LUSTER +L EMMA. We first cluster the documents to topics (Section 3.3), and then group mentions within the same document cluster which share the same head lemma. This baseline differs from the lemma baseline of Kenyon-Dean et al. (2018) which is applied across topics. CV (Cybulska and Vossen, 2015a) is a super"
P19-1409,Q18-1021,0,0.0378243,"ple, consider the following news headlines: 1. 2018 Nobel prize for physics goes to Donna Strickland 2. Prof. Strickland is awarded the Nobel prize for physics Both sentences refer to the same entities (Donna Strickland and the Nobel prize for physics) and the same event (awarding the prize), using different words. In coreference resolution, the goal is to cluster expressions that refer to the same entity or event in a text, whether within a single document or across a document collection. Recently, there has been increasing interest in crosstext inferences, for example in question answering (Welbl et al., 2018; Yang et al., 2018; Khashabi et al., 2018; Postma et al., 2018). Such applications would benefit from effective cross-document coreference resolution. Despite the importance of the task, the focus of most coreference resolution research has been on its within-document variant, and rather little on cross-document coreference (CDCR). The latter is sometimes addressed partially using entity linking, which links mentions of an entity to its knowledge base entry. However, cross-document entity coreference is substantially broader than entity linking, addressing also mentions of common nouns and un"
P19-1409,Q15-1037,0,0.644563,"y due to the more complex structure of event mentions. While entity mentions are mostly noun phrases, event mentions may consist of a verbal predicate (acquire) or a nominalization (acquisition), where these are attached to arguments, including event participants and spatio-temporal information. Early models employed lexical features (e.g. head lemma, WordNet synsets, word embedding similarity) as well as structural features (e.g. aligned arguments) to compute distances between event mentions and decide whether they belong to the same coreference cluster (e.g. Bejan and Harabagiu, 2010, 2014; Yang et al., 2015). More recent work is based on neural networks. Choubey and Huang (2017) alternate between WD and CD clustering, each step relying on previous decisions. The decision to link two event mentions is made by the pairwise WD and CD scorers. Mention representations rely on pre-trained word embeddings, contextual information, and features related to the event’s arguments. Kenyon-Dean et al. (2018) similarly encode event mentions using lexical and contextual features. Differently from Choubey and Huang (2017), they do not cluster documents to topics as a pre-processing step. Instead, they encode the"
P19-1409,D18-1259,0,0.0245381,"llowing news headlines: 1. 2018 Nobel prize for physics goes to Donna Strickland 2. Prof. Strickland is awarded the Nobel prize for physics Both sentences refer to the same entities (Donna Strickland and the Nobel prize for physics) and the same event (awarding the prize), using different words. In coreference resolution, the goal is to cluster expressions that refer to the same entity or event in a text, whether within a single document or across a document collection. Recently, there has been increasing interest in crosstext inferences, for example in question answering (Welbl et al., 2018; Yang et al., 2018; Khashabi et al., 2018; Postma et al., 2018). Such applications would benefit from effective cross-document coreference resolution. Despite the importance of the task, the focus of most coreference resolution research has been on its within-document variant, and rather little on cross-document coreference (CDCR). The latter is sometimes addressed partially using entity linking, which links mentions of an entity to its knowledge base entry. However, cross-document entity coreference is substantially broader than entity linking, addressing also mentions of common nouns and unfamiliar named enti"
P19-1409,C10-2121,0,\N,Missing
P19-1409,J14-2004,0,\N,Missing
P19-1409,P10-1143,0,\N,Missing
P19-1409,cybulska-vossen-2014-using,0,\N,Missing
Q19-1027,Q17-1010,0,0.373338,"ring implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We construct an evaluation suite with six tasks related to the above two phenomena, as shown in Figure 1, and develop generic models that rely on pre-trained representations. We test six representations, including static word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and contextualized word embeddings (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Our contributions are as follows: Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting"
Q19-1027,W13-0104,0,0.549153,"ptures implicit meaning from various sources. In the NC Relations, all variants perform on par or worse than the majority baseline, achieving a few points less than the full model. In the AN Attributes task it is easier to see that the phrase (AN) is important for the classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their"
Q19-1027,D18-1523,0,0.0160504,"and orange (light) points are negative. Non-literality as a rare sense. Nunberg et al. (1994) considered some non-literal compounds as ‘‘idiosyncratically decomposable’’, that is, which can be decomposed to possibly rare senses of their constituents, as in considering bee to have a sense of ‘‘competition’’ in spelling bee and crocodile to stand for ‘‘manipulative’’ in crocodile tears. Using this definition, we could possibly use the NC literality data for word sense induction, in which recent work has shown that contextualized word representations are successful (Stanovsky and Hopkins, 2018; Amrami and Goldberg, 2018). We are interested in testing not only whether the contextualized models are capable of detecting rare senses induced by non-literal usage, which we have confirmed in Section 6, but whether they can also model these senses. To that end, we sample target words that appear in both literal and non-literal examples, and use each contextualized word embedding model as a language model to predict the best substitutes of the target word in each context. Table 7 exemplifies some of these predictions. Bold words are words judged reasonable in the given context, even if they don’t have the exact same m"
Q19-1027,P18-1198,0,0.0949222,"al., 2018; Devlin et al., 2019). Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift. These models were shown to capture some world knowledge (e.g., Zellers et al., 2018), which may potentially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We construct an evaluation suite with six tasks related to the above two phenomena, as shown in Figure 1, and develop generic models that rely on pre-trained representations. We test six representations, including static word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and contextualized word embeddings (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Our contributions are as follows: Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical compositi"
Q19-1027,W18-5440,0,0.0262081,"kind of ‘‘black box’’ testing has become popular recently. Adi et al. (2017) studied whether sentence embeddings capture properties such as sentence length and word order. Conneau et al. (2018) extended their work with a large number of sentence embeddings, and tested various properties at the surface, syntactic, and semantic levels. Others focused on intermediate representations in neural machine translation systems (e.g., Shi et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017; Sennrich, 2017), or on specific linguistic properties such as agreement (Giulianelli et al., 2018), and tense (Bacon and Regier, 2018). More recently, Tenney et al. (2019) and Liu et al. (2019) each designed a suite of tasks to test contextualized word embeddings on a broad range of sub-sentence tasks, including part-ofspeech tagging, syntactic constituent labeling, dependency parsing, named entity recognition, semantic role labeling, coreference resolution, semantic proto-role, and relation classification. Tenney et al. (2019) found that all the models produced strong representations for syntactic phenomena, but gained smaller performance improvements upon the baselines in the more semantic tasks. Liu et al. (2019) found th"
Q19-1027,P16-1187,0,0.0203207,"reas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan"
Q19-1027,D10-1115,0,0.098826,"suggesting that the model captures implicit meaning from various sources. In the NC Relations, all variants perform on par or worse than the majority baseline, achieving a few points less than the full model. In the AN Attributes task it is easier to see that the phrase (AN) is important for the classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from"
Q19-1027,P13-2080,1,0.799801,"presentations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawbacks above: They assume that the meaning of the phrase can always be composed from its constituent meanings, and it is unclear whether they can incorporate implicit information and new properties"
Q19-1027,I17-1015,0,0.0442621,"Missing"
Q19-1027,E17-1006,0,0.0473181,"years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these"
Q19-1027,N19-1423,0,0.229716,"015), or that olive oil is made of olives while baby oil is made for babies (Shwartz and Waterson, 2018). There has been a line of attempts to learn compositional phrase representations (e.g., Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Wieting et al., 2017; Poliak et al., 2017), but many of these are tailored to a specific type of phrase or to a fixed number of constituent words, and they all disregard the surrounding context. Recently, contextualized word representations boasted dramatic performance improvements on a range of NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift. These models were shown to capture some world knowledge (e.g., Zellers et al., 2018), which may potentially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We constr"
Q19-1027,S13-2025,0,0.0888559,"Missing"
Q19-1027,W16-1604,0,0.160381,"y of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawbacks above:"
Q19-1027,W13-3206,0,0.120286,"Missing"
Q19-1027,N19-1112,0,0.0672746,"Missing"
Q19-1027,W18-2501,0,0.0194792,"ord spans or no additional inputs. The classifier output is defined as: (2) o = softmax(W · ReLU(Dropout(h(x)))) (8) Encode. We encode the embedded sequences v1 , . . . , vn and v  1 , . . . , v  l using one of the following three encode variants. As opposed to the pre-trained embeddings, the encoder parameters are updated during training to fit the specific task. where h is a 300-dimensional hidden layer, the dropout probability is 0.2, W ∈ Rk×300 , and k is the number of class labels for the specific task. Implementation Details. We implemented the models using the AllenNLP library (Gardner et al., 2018), which is based on the PyTorch framework (Paszke et al., 2017). We train them for up to 500 epochs, stopping early if the validation performance doesn’t improve in 20 epochs. The phrase type model is a sequence tagging model that predicts a label for each embedded (potentially encoded) word wi . During decoding, we enforce a single constraint that requires that a B-X tag must precede I tag(s). • biLM: Encoding the embedded sequence using a biLSTM with a hidden dimension d, where d is the input embedding dimension: u1 , . . . , un = biLSTM(v1 , . . . , vn ) (3) • Att: Encoding the embedded"
Q19-1027,W18-5426,0,0.0523679,"Missing"
Q19-1027,K15-1035,0,0.0605673,"Missing"
Q19-1027,E09-1071,0,0.0933116,"Missing"
Q19-1027,I11-1024,0,0.225357,"ions The meaning of a light verb construction (LVC, e.g., make a decision) is mainly derived from its noun object (decision), whereas the meaning of its main verb (make) is ‘‘light’’ (Jespersen, 1965). As a rule of thumb, an LVC can be replaced by the verb usage of its direct object noun (decide) without changing the meaning of the sentence. 404 Task Data Source Train/val/test Size Input Output VPC Classification Tu and Roth (2012) 919/209/220 sentence s VP = w1 w2 is VP a VPC? O LVC Classification Tu and Roth (2011) 1521/258/383 sentence s span = w1 ... wk is the span an LVC? O NC Literality Reddy et al. (2011) Tratz (2011) 2529/323/138 sentence s NC = w1 w2 target w ∈ {w1 , w2 } is w literal in NC? A NC Relations SemEval 2013 Task 4 (Hendrickx et al., 2013) 1274/162/130 sentence s NC = w1 w2 paraphrase p does p explicate NC? A AN Attributes HeiPLAS (Hartung, 2015) 837/108/106 sentence s AN = w1 w2 paraphrase p Phrase Type STREUSLE (Schneider and Smith, 2015) 3017/372/376 sentence s does p describe the attribute in AN? label per token Context A O Table 1: A summary of the composition tasks included in our evaluation suite. In the context column, O means the context is part of the original data set,"
Q19-1027,P16-1204,0,0.0198229,"identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawbacks above: They assume that the meaning of the phrase can always be composed from its constituent me"
Q19-1027,N15-1099,0,0.0215223,"e classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson,"
Q19-1027,D14-1162,0,0.096227,"entially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We construct an evaluation suite with six tasks related to the above two phenomena, as shown in Figure 1, and develop generic models that rely on pre-trained representations. We test six representations, including static word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and contextualized word embeddings (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Our contributions are as follows: Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddi"
Q19-1027,N18-1202,0,0.448343,"cs.biu.ac.il Abstract of debate (Hartung, 2015), or that olive oil is made of olives while baby oil is made for babies (Shwartz and Waterson, 2018). There has been a line of attempts to learn compositional phrase representations (e.g., Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Wieting et al., 2017; Poliak et al., 2017), but many of these are tailored to a specific type of phrase or to a fixed number of constituent words, and they all disregard the surrounding context. Recently, contextualized word representations boasted dramatic performance improvements on a range of NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift. These models were shown to capture some world knowledge (e.g., Zellers et al., 2018), which may potentially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et"
Q19-1027,N15-1177,0,0.186715,"Missing"
Q19-1027,D13-1060,0,0.0120932,"ariants perform on par or worse than the majority baseline, achieving a few points less than the full model. In the AN Attributes task it is easier to see that the phrase (AN) is important for the classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between ab"
Q19-1027,E17-2060,0,0.0176573,"ign a probing task for this property, and build a model that takes the tested representation as an input. This kind of ‘‘black box’’ testing has become popular recently. Adi et al. (2017) studied whether sentence embeddings capture properties such as sentence length and word order. Conneau et al. (2018) extended their work with a large number of sentence embeddings, and tested various properties at the surface, syntactic, and semantic levels. Others focused on intermediate representations in neural machine translation systems (e.g., Shi et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017; Sennrich, 2017), or on specific linguistic properties such as agreement (Giulianelli et al., 2018), and tense (Bacon and Regier, 2018). More recently, Tenney et al. (2019) and Liu et al. (2019) each designed a suite of tasks to test contextualized word embeddings on a broad range of sub-sentence tasks, including part-ofspeech tagging, syntactic constituent labeling, dependency parsing, named entity recognition, semantic role labeling, coreference resolution, semantic proto-role, and relation classification. Tenney et al. (2019) found that all the models produced strong representations for syntactic phenomena"
Q19-1027,D16-1159,0,0.0455928,"Missing"
Q19-1027,W11-0807,0,0.234149,"sets contain distinct verbs in their V and P combinations. 2.2 Recognizing Light Verb Constructions The meaning of a light verb construction (LVC, e.g., make a decision) is mainly derived from its noun object (decision), whereas the meaning of its main verb (make) is ‘‘light’’ (Jespersen, 1965). As a rule of thumb, an LVC can be replaced by the verb usage of its direct object noun (decide) without changing the meaning of the sentence. 404 Task Data Source Train/val/test Size Input Output VPC Classification Tu and Roth (2012) 919/209/220 sentence s VP = w1 w2 is VP a VPC? O LVC Classification Tu and Roth (2011) 1521/258/383 sentence s span = w1 ... wk is the span an LVC? O NC Literality Reddy et al. (2011) Tratz (2011) 2529/323/138 sentence s NC = w1 w2 target w ∈ {w1 , w2 } is w literal in NC? A NC Relations SemEval 2013 Task 4 (Hendrickx et al., 2013) 1274/162/130 sentence s NC = w1 w2 paraphrase p does p explicate NC? A AN Attributes HeiPLAS (Hartung, 2015) 837/108/106 sentence s AN = w1 w2 paraphrase p Phrase Type STREUSLE (Schneider and Smith, 2015) 3017/372/376 sentence s does p describe the attribute in AN? label per token Context A O Table 1: A summary of the composition tasks included in ou"
Q19-1027,P18-1111,1,0.930643,"ld for recovering implicit information is much weaker, and the gap between the best performing model and the human performance on such tasks remains substantial. We expect that improving the ability of such representations to reveal implicit meaning would require more than a language model training objective. In particular, one future direction is a richer training objective that simultaneously models multiple co-occurrences of the constituent words across different texts, as is commonly done in noun compound interpretation ´ S´eaghdha and Copestake, 2009; Shwartz (e.g., O and Waterson, 2018; Shwartz and Dagan, 2018). Data. We use the data set of Tu and Roth (2012), which consists of 1,348 sentences from the British National Corpus (BNC), each containing a verb V and a preposition P annotated to whether it is a VPC or not. The data set is focused on 23 different phrasal verbs derived from six of the most frequently used verbs (take, make, have, get, do, give), and their combination with common prepositions or particles. To reduce label bias, we split the data set lexically by verb—that is, the train, test, and validation sets contain distinct verbs in their V and P combinations. 2.2 Recognizing Light Verb"
Q19-1027,S12-1010,0,0.060197,"and the gap between the best performing model and the human performance on such tasks remains substantial. We expect that improving the ability of such representations to reveal implicit meaning would require more than a language model training objective. In particular, one future direction is a richer training objective that simultaneously models multiple co-occurrences of the constituent words across different texts, as is commonly done in noun compound interpretation ´ S´eaghdha and Copestake, 2009; Shwartz (e.g., O and Waterson, 2018; Shwartz and Dagan, 2018). Data. We use the data set of Tu and Roth (2012), which consists of 1,348 sentences from the British National Corpus (BNC), each containing a verb V and a preposition P annotated to whether it is a VPC or not. The data set is focused on 23 different phrasal verbs derived from six of the most frequently used verbs (take, make, have, get, do, give), and their combination with common prepositions or particles. To reduce label bias, we split the data set lexically by verb—that is, the train, test, and validation sets contain distinct verbs in their V and P combinations. 2.2 Recognizing Light Verb Constructions The meaning of a light verb constr"
Q19-1027,N18-2035,1,0.872806,"(Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawbacks above: They assume that the meaning"
Q19-1027,D12-1110,0,0.174527,"Missing"
Q19-1027,R11-1040,0,0.0714461,"s probable in English. Table 8 shows the results of this experiment. A first observation is that the full model performs best on both tasks, suggesting that the model captures implicit meaning from various sources. In the NC Relations, all variants perform on par or worse than the majority baseline, achieving a few points less than the full model. In the AN Attributes task it is easier to see that the phrase (AN) is important for the classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a"
Q19-1027,D18-1182,0,0.023263,"points are positive examples and orange (light) points are negative. Non-literality as a rare sense. Nunberg et al. (1994) considered some non-literal compounds as ‘‘idiosyncratically decomposable’’, that is, which can be decomposed to possibly rare senses of their constituents, as in considering bee to have a sense of ‘‘competition’’ in spelling bee and crocodile to stand for ‘‘manipulative’’ in crocodile tears. Using this definition, we could possibly use the NC literality data for word sense induction, in which recent work has shown that contextualized word representations are successful (Stanovsky and Hopkins, 2018; Amrami and Goldberg, 2018). We are interested in testing not only whether the contextualized models are capable of detecting rare senses induced by non-literal usage, which we have confirmed in Section 6, but whether they can also model these senses. To that end, we sample target words that appear in both literal and non-literal examples, and use each contextualized word embedding model as a language model to predict the best substitutes of the target word in each context. Table 7 exemplifies some of these predictions. Bold words are words judged reasonable in the given context, even if they"
Q19-1027,R15-1082,0,0.020691,"dict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawb"
Q19-1027,D17-1026,0,0.065087,"Missing"
Q19-1027,C10-1142,0,0.176787,"Missing"
Q19-1027,D18-1009,0,0.0425371,"arelli, 2010; Wieting et al., 2017; Poliak et al., 2017), but many of these are tailored to a specific type of phrase or to a fixed number of constituent words, and they all disregard the surrounding context. Recently, contextualized word representations boasted dramatic performance improvements on a range of NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift. These models were shown to capture some world knowledge (e.g., Zellers et al., 2018), which may potentially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We construct an evaluation suite with six tasks related to the above two phenomena, as shown in Figure 1, and develop generic models that rely on pre-trained representations. We test six representations, including static word embeddings (Mikolov"
Q19-1027,P94-1019,0,\N,Missing
Q19-1027,P17-1080,0,\N,Missing
Q19-1027,E17-2081,0,\N,Missing
S15-1022,P05-1074,0,0.0262158,"consistent semantics (positive and negative). For English, WordNet and VerbOcean were used as lexical resources. Italian WordNet was used for Italian, and GermaNet and German DerivBase (Zeller et al., 2013) were used as lexical resources for German. 1 As a part of Excitement Open Platform for Textual Entailment. https://github.com/hltfbk/EOP-1.2.1/ wiki/AlignmentEDAP1 195 Paraphrase Aligner. The paraphrase aligner concentrates on surface forms rather than lemmas and can align sequences of them rather than just individual tokens. It uses paraphrase tables, e.g. extracted from parallel corpora (Bannard and Callison-Burch, 2005). The alignment process is similar to the lexical aligner: any two sequences of tokens in T and H are aligned if the pair is listed in the resource. The alignment links created by this aligner instantiate only one relation (“paraphrase”) but report the strength of the relation via the translation probability. We used the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), which are available for numerous languages. Lemma Identity Aligner. This aligner does not use any resources. It simply aligns identical lemmas between T and H and plays an important rol"
S15-1022,J12-1003,1,0.930469,"al algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves ar"
S15-1022,W14-5201,0,0.054806,"Missing"
S15-1022,W14-3348,0,0.0536698,"ncentrates on surface forms rather than lemmas and can align sequences of them rather than just individual tokens. It uses paraphrase tables, e.g. extracted from parallel corpora (Bannard and Callison-Burch, 2005). The alignment process is similar to the lexical aligner: any two sequences of tokens in T and H are aligned if the pair is listed in the resource. The alignment links created by this aligner instantiate only one relation (“paraphrase”) but report the strength of the relation via the translation probability. We used the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), which are available for numerous languages. Lemma Identity Aligner. This aligner does not use any resources. It simply aligns identical lemmas between T and H and plays an important role in practice to deal with named entities. 3.3 A Minimal Feature Set Similar to the aligners, we concentrate on a small set of four features in the pilot algorithm. Again, the features are completely language independent, even at the implementation level. This is possible because the linguistic annotations and the alignments, use a language-independent type system (cf. Section 3.1). All current features measur"
S15-1022,E09-1025,0,0.0131132,"gnments T multi-level alignment T-H pair enriched with various levels of alignments H 3-extracting features T-H pair expressed as a data point 4-classifying entailment Entailment Decision Figure 1: Dataflow for TE algorithms based on multi level alignment that alignment strength can be misleading (MacCartney et al., 2006), alignment was understood as an intermediate step whose outcome is a set of correspondences between parts of T and H that can be used to define (mis-)match features. Alignments can be established at the word level, phrase level (MacCartney et al., 2008), or dependency level (Dinu and Wang, 2009). Dagan et al. (2013) generalized this practical use to an architectural principle: They showed that various TE algorithms can be mapped onto a universal alignment-based schema with six steps: preprocessing, enrichment, candidate alignment generation, alignment selection, and classification. Proposal. Our proposal is similar to, but simpler than, Dagan et al.’s. Figure 1 shows the data flow. First, the text and the hypothesis are linguistically pre-processed. Then, the annotated T-H pair becomes 194 the input for various independent aligners, which have access to knowledge resources and can co"
S15-1022,S14-1009,1,0.744724,"Missing"
S15-1022,W07-1401,1,0.817868,"Missing"
S15-1022,P06-1114,0,0.0407371,"ntral, powerful representation for TE algorithms that encourages modular, reusable, multilingual algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge r"
S15-1022,P10-4008,0,0.0227067,"E engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves are generally not designed to be extensible or interoperable. Therefore, changes to the algorithms – like adding support for a new language or for new analysis aspect – are often"
S15-1022,N06-1006,0,0.105121,"Missing"
S15-1022,D08-1084,0,0.0240179,"igner2 aligner3 knowledge resource 2-adding alignments T multi-level alignment T-H pair enriched with various levels of alignments H 3-extracting features T-H pair expressed as a data point 4-classifying entailment Entailment Decision Figure 1: Dataflow for TE algorithms based on multi level alignment that alignment strength can be misleading (MacCartney et al., 2006), alignment was understood as an intermediate step whose outcome is a set of correspondences between parts of T and H that can be used to define (mis-)match features. Alignments can be established at the word level, phrase level (MacCartney et al., 2008), or dependency level (Dinu and Wang, 2009). Dagan et al. (2013) generalized this practical use to an architectural principle: They showed that various TE algorithms can be mapped onto a universal alignment-based schema with six steps: preprocessing, enrichment, candidate alignment generation, alignment selection, and classification. Proposal. Our proposal is similar to, but simpler than, Dagan et al.’s. Figure 1 shows the data flow. First, the text and the hypothesis are linguistically pre-processed. Then, the annotated T-H pair becomes 194 the input for various independent aligners, which ha"
S15-1022,P14-5008,1,0.767679,"Missing"
S15-1022,P12-3013,1,0.852425,"with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves are generally not designed to be extensible or interoperable. Therefore, changes to the algorithms – like adding support for a new language o"
S15-1022,D09-1082,0,0.0349226,"Missing"
S15-1022,P13-1118,1,0.780789,"Missing"
S16-2013,N13-1092,0,0.201684,"Missing"
S16-2013,P12-1092,0,0.0448766,"annotation guidelines are available at: http://u.cs.biu.ac.il/ ˜nlp/resources/downloads/ context-sensitive-fine-grained-dataset. 109 fine-grained relation coarse-grained [16] this paper [15] PPDB-fine-human [1] WordSim-353 similarity [2] SimLex-999 [4] Annotated-PPDB [5] WordSim-353 relatedness [6] MEN [10] Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposit"
S16-2013,P07-1058,1,0.822438,"Missing"
S16-2013,W14-1610,1,0.792661,"Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2.0. Like Pavlick et al., we conflate negation and alternation into one relation. 2.2 PPDB with Semantic Relations In this paper, we focus on human-annotated datasets, and therefore find th"
S16-2013,P15-1129,0,0.224633,"l inferences in context requires datasets in which inferences are annotated incontext by fine-grained semantic relations. Yet, such a dataset is not available (see 2.1). Most existing datasets provide out-of-context annotations, while the few available in-context annotations refer to coarse-grained relations, such as relatedness or similarity. In recent years, the PPDB paraphrase database (Ganitkevitch et al., 2013) became a popular resource among semantic tasks, such as monolingual alignment (Sultan et al., 2014) and recognizing textual entailment (Noh et al., 2015). Recently, Pavlick et al. (2015) classified each paraphrase pair to the fine-grained semantic relation that holds between the phrases, following natural logic (MacCartney and Manning, 2007). To that end, a subset of PPDB paraphrase-pairs were manually annotated, forming a fine-grained lexical inference dataset. Yet, annotations are given out-of-context, limiting its utility. In this paper, we aim to fill the current gap in the inventory of lexical inference datasets, and present a methodology for adding context to outof-context datasets. We apply our methodology on a subset of phrase pairs from Pavlick et al. (2015), Recogni"
S16-2013,P15-2069,0,0.0160968,"dataset. 109 fine-grained relation coarse-grained [16] this paper [15] PPDB-fine-human [1] WordSim-353 similarity [2] SimLex-999 [4] Annotated-PPDB [5] WordSim-353 relatedness [6] MEN [10] Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2"
S16-2013,Q15-1025,0,0.0175166,"available at: http://u.cs.biu.ac.il/ ˜nlp/resources/downloads/ context-sensitive-fine-grained-dataset. 109 fine-grained relation coarse-grained [16] this paper [15] PPDB-fine-human [1] WordSim-353 similarity [2] SimLex-999 [4] Annotated-PPDB [5] WordSim-353 relatedness [6] MEN [10] Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive"
S16-2013,W07-1431,0,0.218211,"dataset is not available (see 2.1). Most existing datasets provide out-of-context annotations, while the few available in-context annotations refer to coarse-grained relations, such as relatedness or similarity. In recent years, the PPDB paraphrase database (Ganitkevitch et al., 2013) became a popular resource among semantic tasks, such as monolingual alignment (Sultan et al., 2014) and recognizing textual entailment (Noh et al., 2015). Recently, Pavlick et al. (2015) classified each paraphrase pair to the fine-grained semantic relation that holds between the phrases, following natural logic (MacCartney and Manning, 2007). To that end, a subset of PPDB paraphrase-pairs were manually annotated, forming a fine-grained lexical inference dataset. Yet, annotations are given out-of-context, limiting its utility. In this paper, we aim to fill the current gap in the inventory of lexical inference datasets, and present a methodology for adding context to outof-context datasets. We apply our methodology on a subset of phrase pairs from Pavlick et al. (2015), Recognizing lexical inferences between pairs of terms is a common task in NLP applications, which should typically be performed within a given context. Such context"
S16-2013,P12-2031,1,0.835883,"artz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2.0. Like Pavlick et al., we conflate negation and alternation into one relation. 2.2 PPDB with Semantic Relations In this paper, we focus on human-annotated datasets, and therefore find the above mentioned subset of human-annotated paraphrases p"
S16-2013,S14-2001,0,0.036019,"ch et al., 2013) is a huge resource of automatically derived paraphrases. In recent years, it has been used for quite many semantic tasks, such as semantic parsing (Wang et al., 2015), recognizing textual entailment (Noh et al., 2015), and monolingual alignment (Sultan et al., 2014). Recently, as part of the PPDB 2.0 release, Pavlick et al. (2015) re-annotated PPDB with finegrained semantic relations, following natural logic (MacCartney and Manning, 2007) (see table 2). This was done by first annotating a subset of PPDB pharaphase-pairs that appeared in the SICK dataset of textual entailment (Marelli et al., 2014). Annotators were instructed to select the appropriate semantic relation that holds for each paraphrase pair. These human annotations were later used to train a classifier and predict the semantic relation for all paraphrase pairs in PPDB. Considering the widespread usage of PPDB in applications, this extension may likely lead to applying lexical inferences based on such fine-grained semantic relations. In this section, we present a methodology of adding context to lexical inference datasets, that we apply on PPDB-fine-human. 3.1 Selecting Phrase-Pairs PPDB-fine-human is a quite large dataset"
S16-2013,S07-1009,0,0.0684032,"ned [16] this paper [15] PPDB-fine-human [1] WordSim-353 similarity [2] SimLex-999 [4] Annotated-PPDB [5] WordSim-353 relatedness [6] MEN [10] Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2.0. Like Pavlick et al., we conflate negation and alter"
S16-2013,S15-1022,1,0.864442,"Missing"
S16-2013,N07-1071,0,0.0851558,"Missing"
S16-2013,P15-1146,0,0.0402832,"Missing"
S16-2013,D14-1162,0,0.0756897,"Missing"
S16-2013,K15-1018,1,0.805138,"y2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2.0. Like Pavlick et al., we conflate negation and alternation into one relation. 2.2 PPDB with Semantic Relations In this paper, we focus on human-annotated datasets, and therefore find the above mentioned subset of"
S16-2013,Q14-1018,0,0.136799,"“talking on the iPhone is prohibited”). Accordingly, developing algorithms that properly apply lexical inferences in context requires datasets in which inferences are annotated incontext by fine-grained semantic relations. Yet, such a dataset is not available (see 2.1). Most existing datasets provide out-of-context annotations, while the few available in-context annotations refer to coarse-grained relations, such as relatedness or similarity. In recent years, the PPDB paraphrase database (Ganitkevitch et al., 2013) became a popular resource among semantic tasks, such as monolingual alignment (Sultan et al., 2014) and recognizing textual entailment (Noh et al., 2015). Recently, Pavlick et al. (2015) classified each paraphrase pair to the fine-grained semantic relation that holds between the phrases, following natural logic (MacCartney and Manning, 2007). To that end, a subset of PPDB paraphrase-pairs were manually annotated, forming a fine-grained lexical inference dataset. Yet, annotations are given out-of-context, limiting its utility. In this paper, we aim to fill the current gap in the inventory of lexical inference datasets, and present a methodology for adding context to outof-context datasets. W"
S16-2013,J15-4004,0,\N,Missing
S16-2013,E14-1057,0,\N,Missing
S17-1002,chrupala-etal-2008-learning,0,0.0948476,"Missing"
S17-1002,N13-1092,1,0.877486,"Missing"
S17-1002,P82-1020,0,0.820605,"Missing"
S17-1002,P05-1074,1,0.629065,"from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015b) in the paraphrasebased method as training data for our neural network model. The main contributions of this paper are: tion can provide substantial complementary information to the distributional signal for distinguishing between different semantic relations. While antonymy applies to expressions that represent contrasting meanings, paraphrases are phrases expressing the same meaning, which usually occur in similar textual contexts (Barzilay and McKeown, 2001) or have common translations in other languages (Bannard and Callison-Burch, 2005). Specifically, if two words or phrases are paraphrases, they are unlikely to be antonyms of each other. Our first approach to antonym detection exploits this fact and uses paraphrases for detecting and generating antonyms (The dementors caught Sirius Black/ Black could not escape the dementors). We start by focusing on phrase pairs that are most salient for deriving antonyms. Our assumption is that phrases (or words) containing negating words (or prefixes) are more helpful for identifying opposing relationships between term-pairs. For example, from the paraphrase pair (caught/not escape), we"
S17-1002,P01-1008,0,0.208388,"prior path-based methods on this task. We used the antonym pairs extracted from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015b) in the paraphrasebased method as training data for our neural network model. The main contributions of this paper are: tion can provide substantial complementary information to the distributional signal for distinguishing between different semantic relations. While antonymy applies to expressions that represent contrasting meanings, paraphrases are phrases expressing the same meaning, which usually occur in similar textual contexts (Barzilay and McKeown, 2001) or have common translations in other languages (Bannard and Callison-Burch, 2005). Specifically, if two words or phrases are paraphrases, they are unlikely to be antonyms of each other. Our first approach to antonym detection exploits this fact and uses paraphrases for detecting and generating antonyms (The dementors caught Sirius Black/ Black could not escape the dementors). We start by focusing on phrase pairs that are most salient for deriving antonyms. Our assumption is that phrases (or words) containing negating words (or prefixes) are more helpful for identifying opposing relationships"
S17-1002,E17-1008,0,0.521346,"e terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying antonymy and expressions with contrasting meanings is valuable for NLP systems which go beyond recognizing semantic relatedness and require to identify specific semantic relations. While manually created semantic taxonomies, like WordNet (Fellbaum, 1998), define antonymy relations between some word pairs that native speakers consider antonyms, they have limited coverage. Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that are commonly considered"
S17-1002,P15-1146,1,0.899335,"Missing"
S17-1002,P15-2070,1,0.888814,"Missing"
S17-1002,D14-1162,0,0.0859183,"n sources are used to recognize semantic relations: pathbased and distributional. Path-based methods consider the joint occurrences of the two terms in a given sentence and use the dependency paths that connect the terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying antonymy and expressions with contrasting meanings is valuable for NLP systems which go beyond recognizing semantic relatedness and require to identify specific semantic relations. While manually created semantic taxonomies, like WordNet (Fellbaum, 1998), define antonymy relations between some word pairs that native speaker"
S17-1002,P14-2086,0,0.268465,"Missing"
S17-1002,K15-1026,0,0.209936,"other relationships has proven to be difficult. Approaches to antonym detection have exploited distributional vector representations relying on the distributional hypothesis of semantic similarity (Harris, 1954; Firth, 1957) that words co-occurring in similar contexts tend to be semantically close. Two main information sources are used to recognize semantic relations: pathbased and distributional. Path-based methods consider the joint occurrences of the two terms in a given sentence and use the dependency paths that connect the terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying anto"
S17-1002,W16-5310,1,0.91968,"irs from paraphrases in the PPDB, the largest paraphrase resource currently available. • We demonstrate improvements to an integrated path-based and distributional model, showing that our morphology-aware neural network model, AntNET, performs better than state-of-the-art methods for antonym detection. Our second method is inspired by the recent success of deep learning methods for relation detection. Shwartz et al. (2016) proposed an integrated path-based and distributional model to improve hypernymy detection between term-pairs, and later extended it to classify multiple semantic relations (Shwartz and Dagan, 2016) (LexNET). Although LexNET was the best performing system in the semantic relation classification task of the CogALex 2016 shared task, the model performed poorly on synonyms and antonyms compared to other relations. The path-based component is weak in recognizing synonyms, which do not tend to co-occur, and the distributional information caused confusion between synonyms and antonyms, since both tend to occur in the same contexts. We propose AntNET, a novel extension of LexNET that integrates information about negating prefixes as a new morphological pattern feature and is able to distinguish"
S17-1002,P16-1226,1,0.906259,"paths that connect the terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying antonymy and expressions with contrasting meanings is valuable for NLP systems which go beyond recognizing semantic relatedness and require to identify specific semantic relations. While manually created semantic taxonomies, like WordNet (Fellbaum, 1998), define antonymy relations between some word pairs that native speakers consider antonyms, they have limited coverage. Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that ar"
S17-1002,C92-2082,0,\N,Missing
S17-1019,W99-0201,0,0.0232231,"Missing"
S17-1019,N03-1003,0,0.0539901,"tude from existing resources, it complements them with nonconsecutive predicates (e.g. take [a]0 from [a]1 ) and paraphrases which are highly context specific. The resource and the source code are available at http://github.com/vered1986/ Chirps.2 As of the end of May 2017, it contains 456,221 predicate pairs in 1,239,463 different contexts. Our resource is ever-growing and is expected to contain around 2 million predicate paraphrases within a year. Until it reaches a large enough size, we will release a daily update, and at a later stage, we plan to release a periodic update. 2 et al., 2002; Barzilay and Lee, 2003). The assumption is that multiple news articles describing the same event use various lexical choices, providing a good source for paraphrases. Heuristics are applied to recognize that two news articles discuss the same event, such as lexical overlap and same publish date (Shinyama and Sekine, 2006). Given such a pair of articles, it is likely that predicates connecting the same arguments will be paraphrases, as in the following example: 1. GOP lawmakers introduce new health care plan 2. GOP lawmakers unveil new health care plan Zhang and Weld (2013) and Zhang et al. (2015) introduced methods"
S17-1019,N06-1039,0,0.165942,"p-ranked predicate paraphrases. Introduction their accuracy is limited. Specifically, the first approach may extract antonyms, that also have similar argument distribution (e.g. [a]0 raise to [a]1 / [a]0 fall to [a]1 ) while the second may conflate multiple senses of the foreign phrase. A third approach was proposed to harvest paraphrases from multiple mentions of the same event in news articles.1 This approach assumes that various redundant reports make different lexical choices to describe the same event. Although there has been some work following this approach (e.g. Shinyama et al., 2002; Shinyama and Sekine, 2006; Roth and Frank, 2012; Zhang and Weld, 2013), it was less exhaustively investigated and did not result in creating paraphrase resources. In this paper we present a novel unsupervised method for ever-growing extraction of lexicallydivergent predicate paraphrase pairs from news tweets. We apply our methodology to create a resource of predicate paraphrases, exemplified in Table 1. Analysis of the resource obtained after ten Recognizing that various textual descriptions across multiple texts refer to the same event or action can benefit NLP applications such as recognizing textual entailment (Dag"
S17-1019,P01-1008,0,0.222672,"extension to the distributional hypothesis (Harris, 1954). DIRT (Lin and Pantel, 2001) is a resource of 10 million paraphrases, in which the similarity between predicate pairs is estimated by the geometric mean of the similarities of their argument slots. Berant (2012) constructed an entailment graph of distributionally similar predicates by enforcing transitivity constraints and applying global optimization, releasing 52 million directional entailment rules (e.g. [a]0 shoot [a]1 → [a]0 kill [a]1 ). A second notable source for extracting paraphrases is multiple translations of the same text (Barzilay and McKeown, 2001). The Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015) is a huge collection of paraphrases extracted from bilingual parallel corpora. Paraphrases are scored heuristically, and the database is available for download in six increasingly large sizes according to scores (the smallest size being the most accurate). In addition to lexical paraphrases, PPDB also consists of 140 million syntactic paraphrases, some of which include predicates with non-terminals as arguments. 2.2 3 We present a methodology to automatically collect binary verbal predicate paraphrases from Twit"
S17-1019,P16-1119,1,0.83296,"rce release consists of two files: We extract propositions from news tweets using PropS (Stanovsky et al., 2016), which simplifies dependency trees by conveniently marking a wide range of predicates (e.g, verbal, adjectival, nonlexical) and positioning them as direct heads of their corresponding arguments. Specifically, we run PropS over dependency trees predicted by spaCy6 and extract predicate types (as in Table 1) composed of verbal predicates, datives, prepositions, and auxiliaries. Finally, we employ a pre-trained argument reduction model to remove non-restrictive argument modifications (Stanovsky and Dagan, 2016). This is essential for our subsequent alignment step, as it is likely that short and concise phrases will tend to match more frequently in comparison to longer, more specific arguments. Figure 1 exemplifies some of the phenomena handled by this process, along with the automatically predicted output. 3.3 1. Instances: the specific contexts in which the predicates are paraphrases (as in Table 2). In practice, to comply with Twitter policy, we release predicate paraphrase pair types along with their arguments and tweet IDs, and provide a script for downloading the full texts. 2. Types: predicate"
S17-1019,P10-1124,1,0.765007,"er “when did the US Supreme Court approve samesex marriage?” given the text “In June 2015, the Supreme Court ruled for same-sex marriage”, approve and ruled for should be identified as describing the same action. To that end, much effort has been devoted to identifying predicate paraphrases, some of which resulted in releasing resources of predicate entailment or paraphrases. Two main approaches were proposed for that matter; the first leverages the similarity in argument distribution across a large corpus between two predicates (e.g. [a]0 buy [a]1 / [a]0 acquire [a]1 ) (Lin and Pantel, 2001; Berant et al., 2010). The second approach exploits bilingual parallel corpora, extracting as paraphrases pairs of texts that were translated identically to foreign languages (Ganitkevitch et al., 2013). While these methods have produced exhaustive resources which are broadly used by applications, 1 This corresponds to instances of event coreference (Bagga and Baldwin, 1999). 155 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 155–160, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics weeks of acquisition shows that the set of pa"
S17-1019,P13-2080,1,0.798824,"006; Roth and Frank, 2012; Zhang and Weld, 2013), it was less exhaustively investigated and did not result in creating paraphrase resources. In this paper we present a novel unsupervised method for ever-growing extraction of lexicallydivergent predicate paraphrase pairs from news tweets. We apply our methodology to create a resource of predicate paraphrases, exemplified in Table 1. Analysis of the resource obtained after ten Recognizing that various textual descriptions across multiple texts refer to the same event or action can benefit NLP applications such as recognizing textual entailment (Dagan et al., 2013) and question answering. For example, to answer “when did the US Supreme Court approve samesex marriage?” given the text “In June 2015, the Supreme Court ruled for same-sex marriage”, approve and ruled for should be identified as describing the same action. To that end, much effort has been devoted to identifying predicate paraphrases, some of which resulted in releasing resources of predicate entailment or paraphrases. Two main approaches were proposed for that matter; the first leverages the similarity in argument distribution across a large corpus between two predicates (e.g. [a]0 buy [a]1"
S17-1019,P07-1058,1,0.737065,"Missing"
S17-1019,Q14-1034,0,0.0797018,"ments will be paraphrases, as in the following example: 1. GOP lawmakers introduce new health care plan 2. GOP lawmakers unveil new health care plan Zhang and Weld (2013) and Zhang et al. (2015) introduced methods that leverage parallel news streams to cluster predicates by meaning, using temporal constraints. Since this approach acquires paraphrases from descriptions of the same event, it is potentially more accurate than methods that acquire paraphrases from the entire corpus or translation phrase table. However, there is currently no paraphrase resource acquired in this approach.3 Finally, Xu et al. (2014) developed a supervised model to collect sentential paraphrases from Twitter. They used Twitter’s trending topic service, and considered two tweets from the same topic as paraphrases if they shared a single anchor word. Background 2.1 Existing Paraphrase Resources A prominent approach to acquire predicate paraphrases is to compare the distribution of their arguments across a corpus, as an extension to the distributional hypothesis (Harris, 1954). DIRT (Lin and Pantel, 2001) is a resource of 10 million paraphrases, in which the similarity between predicate pairs is estimated by the geometric me"
S17-1019,P12-2031,1,0.861583,"erant (Berant, 2012), a resource of predicate entailments, and PPDB (Pavlick et al., 2015), a resource of paraphrases, both described in Section 2. We expect our resource to be more accurate than resources which are based on the distributional approach (Berant, 2012; Lin and Pantel, 2001). In addition, in comparison to PPDB, we specialize on binary verbal predicates, and apply an additional phase of proposition extraction, handling various phenomena such as non-consecutive particles and minimality of arguments. Berant (2012) evaluated their resource against a dataset of predicate entailments (Zeichner et al., 2012), using a recall-precision curve to show the performance obtained with a range of thresholds on the resource score. This kind of evaluation is less suitable for our resource; first, predicate entailment is directional, causing paraphrases with the wrong entailment direction to be labeled negative in the dataset. Second, since our resource is still relatively small, it is unlikely to have sufficient coverage of the dataset at that point. We therefore 6 Conclusion We presented a new unsupervised method to acquire fairly accurate predicate paraphrases from news tweets discussing the same event. W"
S17-1019,P15-2070,0,0.0588367,"Missing"
S17-1019,Q15-1009,0,0.0777347,"2 et al., 2002; Barzilay and Lee, 2003). The assumption is that multiple news articles describing the same event use various lexical choices, providing a good source for paraphrases. Heuristics are applied to recognize that two news articles discuss the same event, such as lexical overlap and same publish date (Shinyama and Sekine, 2006). Given such a pair of articles, it is likely that predicates connecting the same arguments will be paraphrases, as in the following example: 1. GOP lawmakers introduce new health care plan 2. GOP lawmakers unveil new health care plan Zhang and Weld (2013) and Zhang et al. (2015) introduced methods that leverage parallel news streams to cluster predicates by meaning, using temporal constraints. Since this approach acquires paraphrases from descriptions of the same event, it is potentially more accurate than methods that acquire paraphrases from the entire corpus or translation phrase table. However, there is currently no paraphrase resource acquired in this approach.3 Finally, Xu et al. (2014) developed a supervised model to collect sentential paraphrases from Twitter. They used Twitter’s trending topic service, and considered two tweets from the same topic as paraphr"
S17-1019,D13-1183,0,0.507084,"r accuracy is limited. Specifically, the first approach may extract antonyms, that also have similar argument distribution (e.g. [a]0 raise to [a]1 / [a]0 fall to [a]1 ) while the second may conflate multiple senses of the foreign phrase. A third approach was proposed to harvest paraphrases from multiple mentions of the same event in news articles.1 This approach assumes that various redundant reports make different lexical choices to describe the same event. Although there has been some work following this approach (e.g. Shinyama et al., 2002; Shinyama and Sekine, 2006; Roth and Frank, 2012; Zhang and Weld, 2013), it was less exhaustively investigated and did not result in creating paraphrase resources. In this paper we present a novel unsupervised method for ever-growing extraction of lexicallydivergent predicate paraphrase pairs from news tweets. We apply our methodology to create a resource of predicate paraphrases, exemplified in Table 1. Analysis of the resource obtained after ten Recognizing that various textual descriptions across multiple texts refer to the same event or action can benefit NLP applications such as recognizing textual entailment (Dagan et al., 2013) and question answering. For"
S17-1019,S12-1030,0,\N,Missing
S18-1115,S13-1005,0,0.16065,"Missing"
S18-1115,S18-1116,0,0.0792436,"Missing"
S18-1115,S18-1149,0,0.0363408,"Missing"
S18-1115,E17-2013,0,0.185703,"Missing"
S18-1115,C92-2082,0,0.323886,"systems for any individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification"
S18-1115,S15-2151,1,0.928124,"i♥ Luis Espinosa-Anke♣ Sergio Oramas♦ Tommaso Pasini♥ Enrico Santus♥ Vered Shwartz♠ Roberto Navigli♥ Horacio Saggion♦ ♣ School of Computer Science and Informatics, Cardiff University, United Kingdom ♥ Computer Science Department, Sapienza University of Rome, Italy ♦ Pompeu Fabra University, Barcelona, Spain ♥ MIT, United States ♠ Bar-Ilan University, Ramat Gan, Israel ♣ {camachocolladosj,espinosa-ankel}@cardiff.ac.uk, ♥ {dellibovi,pasini,navigli}@di.uniroma1.it, ♦ {name.surname}@upf.edu, ♥ esantus@mit.edu, ♠ vered1986@gmail.com Abstract web retrieval, website navigation or records management (Bordea et al., 2015). This paper describes the SemEval 2018 Shared Task on Hypernym Discovery. We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input. Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus. We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were al"
S18-1115,S16-1168,0,0.263061,"s either a concept or a 4 As an example, the term apple could either refer to a fruit (if labeled as concept) or to a company (if labeled as named entity). 5 http://ebiquity.umbc. edu/blogger/2013/05/01/ umbc-webbase-corpus-of-3b-english-words/ 6 http://dbpubs.stanford.edu:8091/ testbed/doc2/WebBase/ ˜ 3 In fact, WordNet encodes hypernym and instance as two separate semantic relations. Instances are always leaf (terminal) nodes in their hierarchies. 714 sources of information with respect to the corpora used in previous tasks, such as Wikipedia in the SemEval 2016 task on taxonomy extraction (Bordea et al., 2016). In fact, the encyclopedic nature of Wikipedia has been exploited in a wide variety of works (Ponzetto and Strube, 2007; Flati et al., 2016; Gupta et al., 2016), and differs substantially from the web-based corpus we put forward here. As source corpus for the Italian subtask (1B) we instead used the 1.3-billion-word itWac corpus7 (Baroni et al., 2009), extracted from different sources of the web within the .it domain. Finally, as source corpus for the Spanish subtask (1C) we considered the 1.8-billion-word Spanish corpus8 (Cardellino, 2016), which also contains heterogeneous documents from di"
S18-1115,S18-1150,0,0.0250373,"Missing"
S18-1115,E17-2036,1,0.898611,"Missing"
S18-1115,N15-1098,0,0.0369984,"ttps://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within do"
S18-1115,D16-1041,1,0.888795,"Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Collados, 2017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The ma"
S18-1115,S18-1151,0,0.0726421,"14 We used the open-source code available at https:// bitbucket.org/luisespinosa/taxoembed 15 https://github.com/vered1986/ UnsupervisedHypernymy 16 Following the conclusions from Shwartz et al. (2017), we set the hyper-parameters to: SLQS: median, PLMI, N = 100 and APSyn: N = 500. 13 Although only P@5 is displayed in the tables due to lack of space, the other thresholds were used in the official evaluation as well. 717 5.2 Participant Systems in general they were outperformed by supervised systems, in some cases their performance came close, especially for concepts. For instance, the ADAPT (Maldonado and Klubika, 2018) system, which is based on a simple similarity measure applied to word embeddings, achieved a very decent 8.13 MAP percentage performance on the medical dataset, using neither supervision nor external resources. Supervised systems produced a larger gap for entities, probably due, as mentioned above, to the lower diversity of possible hypernyms. Table 3 shows a summary of all participant systems, displaying their main features with respect to supervison and external resources used, if any. 5.3 Results A summary of the results is provided in tables 3 to 7, respectively describing results for Eng"
S18-1115,P14-1113,0,0.377885,"t al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathe"
S18-1115,P16-2081,1,0.844897,"Missing"
S18-1115,P10-1134,1,0.838694,"y individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni"
S18-1115,D16-1234,0,0.0290792,"rrent research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 1A: English 1B: Italian 1C: Spanish Term sorrow Nina Simone guacamole 2A: Medical pulmonary embolism 2B: Music Green Day Hypernym(s) sadness, unhappiness musicista, pianista, persona salsa para mojar, salsa, alimento"
S18-1115,D17-1022,0,0.0580762,"Missing"
S18-1115,C14-1097,0,0.0705885,"Missing"
S18-1115,S18-1146,0,0.019286,"Missing"
S18-1115,E17-2064,0,0.0114909,"stributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different and heterogeneous sources. A system operating in this setting r"
S18-1115,L16-1722,1,0.929363,". codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld ap"
S18-1115,L16-1528,1,0.817501,"t appeared too vague or general, as well as terms with mis-attributed domains. Domain-specific corpora. As source corpus for the medical domain (subtask 2A) we provided a combination of texts drawn from the MEDLINE9 (Medical Literature Analysis and Retrieval System) repository, which contains academic documents such as scientific publications and paper abstracts. This corpus contains 130 million words. As regards the music domain (subtask 2B), instead, the source corpus we compiled is a concatenation of several music-specific corpora, i.e. music biographies from Last.fm contained in ELMD 2.0 (Oramas et al., 2016), articles from the music branch of Wikipedia, and a corpus of album customer reviews from Amazon (Oramas et al., 2017). The resulting corpus reaches 100 million words in total. 4.1.2 Term Collection Vocabulary Creation With the aim of simplifying the task for participants by providing a unified hypernym search space, we built a series of vocabulary files including all the possible hypernyms on each dataset. Each vocabulary was constructed by considering all the words occurring at least N times across the source corpus of the corresponding subtask. We set N to five and three in the general-pur"
S18-1115,E14-4008,1,0.929727,"i Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose te"
S18-1115,P17-1192,0,0.0129572,"es its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al."
S18-1115,P16-1226,1,0.918452,"017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The main goal of this task is that of complementing current research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 201"
S18-1115,E17-1007,1,0.68931,"ions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Coll"
S18-1115,S18-1148,0,0.0202622,"Missing"
S18-1115,J17-4004,0,0.0330605,"Missing"
S18-1115,S17-1004,0,0.0128295,"ish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were allowed to compete in any or all of the subtasks. Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks. Data, results and further information about the task can be found at https://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2"
S18-1115,D17-1123,0,0.0255651,"Missing"
S18-1115,C14-1212,0,0.0535272,"a et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different a"
S18-1115,P10-2021,0,0.026139,"aining set, separately for each subtask and measure. where Q is a sample of experiment runs, AP(·) refers to average precision, i.e. an average of the correctness of each individual obtained hypernym from the search space. Mean Reciprocal Rank (MRR). MRR rewards the position of the first correct result in a ranked list of outcomes, and is defined as: |Q| 1 X 1 MRR = |Q| ranki i=1 where ranki refers to the rank position of the first relevant outcome for the ith run. While its main field of application is Information Retrieval, it has also been used in NLP tasks such as collocation recognition (Wu et al., 2010; Rodr´ıguezFern´andez et al., 2016). In addition to the above, we also provide results according to P@k, i.e. the number of correctly retrieved hypernyms at different cut-off thresholds, specifically k ∈ {1, 3, 5, 15}.13 5.1 Baselines We compared the participating systems with both supervised and unsupervised baselines for each subtask, inspired by recent work on hypernym detection and discovery. In this section we briefly describe each of them. 5.1.1 Unsupervised Baselines Supervised Baselines We first used a na¨ıve most frequent hypernym (MFH) baseline, which simply returns, for each input"
S18-1115,S18-1147,0,0.0304331,"Missing"
S18-2020,E12-1004,0,0.729177,"ure relational information between two words. While most previous work reported success using supervised methods, some questions remain unanswered: First, several works suggested that supervised distributional methods are incapable of inferring the relationship between two words, but rather rely on independent properties of each word (Levy et al., 2015; Roller and Erk, 2016; Shwartz et al., 2016), making them sensitive to training data; Second, it remains unclear what is the most appropriate representation and classifier; previous studies reported inconsistent results with Concathv~x ⊕ v~y i (Baroni et al., 2012) and Diffhv~y − v~x i (Roller et al., 2014; Weeds et al., 2014; Fu et al., 2014), using various classifiers. In this paper, we investigate the effectiveness of multiplicative features, namely, the element-wise multiplication Multhv~x v~y i, and the squared difference Sqdiffh(v~y − v~x ) (v~y − v~x )i. These features, similar to the cosine similarity and the Euclidean distance, might capture a different notion of interaction information about the relationship holding between two words. We directly integrate them into some commonly used representations. For instance, we consider the concatenatio"
S18-2020,W11-2501,0,0.228771,"nalty parameter C of the error term is selected from {2−5 , 2−3 , 2−1 , 21 }. • For RBF, C and γ values are selected from {21 , 23 , 25 , 27 } and {2−7 , 2−5 , 2−3 , 2−1 }, respectively. • For MLP, the hidden layer size is either 50 or 100, and the learning rate is fixed at 10−3 . We use early stopping based on the performance on the validation set. The maximum number of training epochs is 100. • For KSIM, C and α values are selected from {2−7 , 2−5 , . . . , 27 } and {0.0, 0.1, . . . , 1.0}, respectively. 3.4 Datasets We evaluated the methods on four common semantic relation datasets: BLESS (Baroni and Lenci, 2011), K&H+N (Necsulescu et al., 2015), ROOT09 (Santus et al., 2016), and EVALution (Santus et al., 2015). Table 2 provides metadata on the datasets. Most datasets contain word pairs instantiating different, explicitly typed semantic relations, plus a number of unrelated word pairs (random). Instances in BLESS and K&H+N are divided into a number of topical domains.3 3.5 4 Experiments Table 3 summarizes the best performing base representations and combinations on the test sets across the various datasets and evaluation setups.4 The results across the datasets vary substantially in some cases due to"
S18-2020,P14-1113,0,0.336247,"ess using supervised methods, some questions remain unanswered: First, several works suggested that supervised distributional methods are incapable of inferring the relationship between two words, but rather rely on independent properties of each word (Levy et al., 2015; Roller and Erk, 2016; Shwartz et al., 2016), making them sensitive to training data; Second, it remains unclear what is the most appropriate representation and classifier; previous studies reported inconsistent results with Concathv~x ⊕ v~y i (Baroni et al., 2012) and Diffhv~y − v~x i (Roller et al., 2014; Weeds et al., 2014; Fu et al., 2014), using various classifiers. In this paper, we investigate the effectiveness of multiplicative features, namely, the element-wise multiplication Multhv~x v~y i, and the squared difference Sqdiffh(v~y − v~x ) (v~y − v~x )i. These features, similar to the cosine similarity and the Euclidean distance, might capture a different notion of interaction information about the relationship holding between two words. We directly integrate them into some commonly used representations. For instance, we consider the concatenation Diff⊕Mult h(v~y − v~x )⊕(v~x v~y )i that might capture both the typicality of"
S18-2020,C92-2082,0,0.626645,"ssification), based on their distributional representations. 3.1 Limitations Recent work questioned whether supervised distributional methods actually learn the relation between x and y or only separate properties of each word. Levy et al. (2015) claimed that they tend to perform “lexical memorization”, i.e., memorizing that some words are prototypical to certain relations (e.g., that y = animal is a hypernym, regardless of x). Roller and Erk (2016) found that under certain conditions, these methods actively learn to infer hypernyms based on separate occurrences of x and y in Hearst patterns (Hearst, 1992). In either case, they only learn whether x and y independently match their corresponding slots in the relation, a limitation which makes them sensitive to the training data (Shwartz et al., 2017; Sanchez and Riedel, 2017). Word Pair Representations Given a word pair (x, y) and their embeddings v~x , v~y , we consider various compositions as feature vectors for classifiers. Table 1 displays base representations and combination representations, achieved by concatenating two base representations. 3.2 Word Vectors We used 300-dimensional pre-trained word embeddings, namely, GloVe (Pennington et a"
S18-2020,D16-1234,0,0.164365,"llege of Information and Computer Sciences Computer Science Department Bar-Ilan University University of Massachusetts Amherst Ramat-Gan, Israel Amherst, MA, USA vered1986@gmail.com tuvu@cs.umass.edu Abstract binations that are designed to capture relational information between two words. While most previous work reported success using supervised methods, some questions remain unanswered: First, several works suggested that supervised distributional methods are incapable of inferring the relationship between two words, but rather rely on independent properties of each word (Levy et al., 2015; Roller and Erk, 2016; Shwartz et al., 2016), making them sensitive to training data; Second, it remains unclear what is the most appropriate representation and classifier; previous studies reported inconsistent results with Concathv~x ⊕ v~y i (Baroni et al., 2012) and Diffhv~y − v~x i (Roller et al., 2014; Weeds et al., 2014; Fu et al., 2014), using various classifiers. In this paper, we investigate the effectiveness of multiplicative features, namely, the element-wise multiplication Multhv~x v~y i, and the squared difference Sqdiffh(v~y − v~x ) (v~y − v~x )i. These features, similar to the cosine similarity and"
S18-2020,Q15-1027,0,0.0170919,"ls slightly mitigates this issue, and proposed KSIM, a custom kernel with multiplicative integration. 3.3 Classifiers Following previous work (Levy et al., 2015; Roller and Erk, 2016), we trained different types of classifiers for each word-pair representation outlined in Section 3.1, namely, logistic regression with L2 regularization (LR), SVM with a linear kernel (LIN), and SVM with a Gaussian kernel (RBF). In addition, we trained multi-layer perceptrons with a single hidden layer (MLP). We compare our models against the KSIM model found to be successful in previous work (Levy et al., 2015; Kruszewski et al., 2015). We do not include Roller and Erk (2016)’s model since it focuses only on hypernymy. Hyper-parameters are tuned using grid search, and we report the test performance of the Multiplicative Features The element-wise multiplication has been studied by Weeds et al. (2014), but models that operate exclusively on it were not competitive to Concat and Diff on most tasks. Roller et al. (2014) found that the squared difference, in combination with Diff, is useful for hypernymy detection. Nevertheless, little to no work has focused on investigating combinations of representations obtained by concatenat"
S18-2020,C14-1097,0,0.362404,"Missing"
S18-2020,N15-1098,0,0.749923,"ed Shwartz Tu Vu College of Information and Computer Sciences Computer Science Department Bar-Ilan University University of Massachusetts Amherst Ramat-Gan, Israel Amherst, MA, USA vered1986@gmail.com tuvu@cs.umass.edu Abstract binations that are designed to capture relational information between two words. While most previous work reported success using supervised methods, some questions remain unanswered: First, several works suggested that supervised distributional methods are incapable of inferring the relationship between two words, but rather rely on independent properties of each word (Levy et al., 2015; Roller and Erk, 2016; Shwartz et al., 2016), making them sensitive to training data; Second, it remains unclear what is the most appropriate representation and classifier; previous studies reported inconsistent results with Concathv~x ⊕ v~y i (Baroni et al., 2012) and Diffhv~y − v~x i (Roller et al., 2014; Weeds et al., 2014; Fu et al., 2014), using various classifiers. In this paper, we investigate the effectiveness of multiplicative features, namely, the element-wise multiplication Multhv~x v~y i, and the squared difference Sqdiffh(v~y − v~x ) (v~y − v~x )i. These features, similar to the"
S18-2020,E17-2064,0,0.0469365,"ies of each word. Levy et al. (2015) claimed that they tend to perform “lexical memorization”, i.e., memorizing that some words are prototypical to certain relations (e.g., that y = animal is a hypernym, regardless of x). Roller and Erk (2016) found that under certain conditions, these methods actively learn to infer hypernyms based on separate occurrences of x and y in Hearst patterns (Hearst, 1992). In either case, they only learn whether x and y independently match their corresponding slots in the relation, a limitation which makes them sensitive to the training data (Shwartz et al., 2017; Sanchez and Riedel, 2017). Word Pair Representations Given a word pair (x, y) and their embeddings v~x , v~y , we consider various compositions as feature vectors for classifiers. Table 1 displays base representations and combination representations, achieved by concatenating two base representations. 3.2 Word Vectors We used 300-dimensional pre-trained word embeddings, namely, GloVe (Pennington et al., 2014b) containing 1.9M word vectors trained on a corpus of web data from Common Crawl (42B tokens),1 and Word2vec (Mikolov et al., 2013a,c) containing 3M word vectors trained on a part of Google News dataset (100B toke"
S18-2020,L16-1722,0,0.477201,"2−1 , 21 }. • For RBF, C and γ values are selected from {21 , 23 , 25 , 27 } and {2−7 , 2−5 , 2−3 , 2−1 }, respectively. • For MLP, the hidden layer size is either 50 or 100, and the learning rate is fixed at 10−3 . We use early stopping based on the performance on the validation set. The maximum number of training epochs is 100. • For KSIM, C and α values are selected from {2−7 , 2−5 , . . . , 27 } and {0.0, 0.1, . . . , 1.0}, respectively. 3.4 Datasets We evaluated the methods on four common semantic relation datasets: BLESS (Baroni and Lenci, 2011), K&H+N (Necsulescu et al., 2015), ROOT09 (Santus et al., 2016), and EVALution (Santus et al., 2015). Table 2 provides metadata on the datasets. Most datasets contain word pairs instantiating different, explicitly typed semantic relations, plus a number of unrelated word pairs (random). Instances in BLESS and K&H+N are divided into a number of topical domains.3 3.5 4 Experiments Table 3 summarizes the best performing base representations and combinations on the test sets across the various datasets and evaluation setups.4 The results across the datasets vary substantially in some cases due to the differences between the datasets’ relations, class balance,"
S18-2020,E14-4008,0,0.0212111,"ent (Dagan et al., 2013) that often rely on lexical semantic resources with limited coverage like Wordnet (Miller, 1995). Relation classifiers can be used either within applications or as an intermediate step in the construction of lexical resources which is often expensive and time-consuming. Most methods for lexical entailment are distributional, i.e., the semantic relation holding between x and y is recognized based on their distributional vector representations. While the first methods were unsupervised and used highdimensional sparse vectors (Weeds and Weir, 2003; Kotlerman et al., 2010; Santus et al., 2014), in recent years, supervised methods became popular (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). These methods are mostly based on word embeddings (Mikolov et al., 2013b; Pennington et al., 2014a) utilizing various vector com160 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 160–166 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics Base representations Only-xhv~x i Only-yhv~y i Diffhv~y − v~x i Sumhv~x + v~y i Concathv~x ⊕ v~y i Multhv~x v~y i Sqdiffh(v~y − v~x ) (v~y − v~x )i tures. The contribution"
S18-2020,W15-4208,0,0.12982,"are selected from {21 , 23 , 25 , 27 } and {2−7 , 2−5 , 2−3 , 2−1 }, respectively. • For MLP, the hidden layer size is either 50 or 100, and the learning rate is fixed at 10−3 . We use early stopping based on the performance on the validation set. The maximum number of training epochs is 100. • For KSIM, C and α values are selected from {2−7 , 2−5 , . . . , 27 } and {0.0, 0.1, . . . , 1.0}, respectively. 3.4 Datasets We evaluated the methods on four common semantic relation datasets: BLESS (Baroni and Lenci, 2011), K&H+N (Necsulescu et al., 2015), ROOT09 (Santus et al., 2016), and EVALution (Santus et al., 2015). Table 2 provides metadata on the datasets. Most datasets contain word pairs instantiating different, explicitly typed semantic relations, plus a number of unrelated word pairs (random). Instances in BLESS and K&H+N are divided into a number of topical domains.3 3.5 4 Experiments Table 3 summarizes the best performing base representations and combinations on the test sets across the various datasets and evaluation setups.4 The results across the datasets vary substantially in some cases due to the differences between the datasets’ relations, class balance, and the source from which they were"
S18-2020,S15-1021,0,0.061041,"erm is selected from {2−5 , 2−3 , 2−1 , 21 }. • For RBF, C and γ values are selected from {21 , 23 , 25 , 27 } and {2−7 , 2−5 , 2−3 , 2−1 }, respectively. • For MLP, the hidden layer size is either 50 or 100, and the learning rate is fixed at 10−3 . We use early stopping based on the performance on the validation set. The maximum number of training epochs is 100. • For KSIM, C and α values are selected from {2−7 , 2−5 , . . . , 27 } and {0.0, 0.1, . . . , 1.0}, respectively. 3.4 Datasets We evaluated the methods on four common semantic relation datasets: BLESS (Baroni and Lenci, 2011), K&H+N (Necsulescu et al., 2015), ROOT09 (Santus et al., 2016), and EVALution (Santus et al., 2015). Table 2 provides metadata on the datasets. Most datasets contain word pairs instantiating different, explicitly typed semantic relations, plus a number of unrelated word pairs (random). Instances in BLESS and K&H+N are divided into a number of topical domains.3 3.5 4 Experiments Table 3 summarizes the best performing base representations and combinations on the test sets across the various datasets and evaluation setups.4 The results across the datasets vary substantially in some cases due to the differences between the datas"
S18-2020,P16-1226,1,0.915371,"Missing"
S18-2020,D14-1162,0,0.105359,"the construction of lexical resources which is often expensive and time-consuming. Most methods for lexical entailment are distributional, i.e., the semantic relation holding between x and y is recognized based on their distributional vector representations. While the first methods were unsupervised and used highdimensional sparse vectors (Weeds and Weir, 2003; Kotlerman et al., 2010; Santus et al., 2014), in recent years, supervised methods became popular (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). These methods are mostly based on word embeddings (Mikolov et al., 2013b; Pennington et al., 2014a) utilizing various vector com160 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 160–166 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics Base representations Only-xhv~x i Only-yhv~y i Diffhv~y − v~x i Sumhv~x + v~y i Concathv~x ⊕ v~y i Multhv~x v~y i Sqdiffh(v~y − v~x ) (v~y − v~x )i tures. The contribution of multiplicative features is mostly prominent in strict evaluation settings, i.e., lexical split (Levy et al., 2015) and out-ofdomain evaluation that disable the models’ ability to achieve good performance by memo"
S18-2020,E17-1007,1,0.860596,"Missing"
S18-2020,C14-1212,0,0.558993,"s work reported success using supervised methods, some questions remain unanswered: First, several works suggested that supervised distributional methods are incapable of inferring the relationship between two words, but rather rely on independent properties of each word (Levy et al., 2015; Roller and Erk, 2016; Shwartz et al., 2016), making them sensitive to training data; Second, it remains unclear what is the most appropriate representation and classifier; previous studies reported inconsistent results with Concathv~x ⊕ v~y i (Baroni et al., 2012) and Diffhv~y − v~x i (Roller et al., 2014; Weeds et al., 2014; Fu et al., 2014), using various classifiers. In this paper, we investigate the effectiveness of multiplicative features, namely, the element-wise multiplication Multhv~x v~y i, and the squared difference Sqdiffh(v~y − v~x ) (v~y − v~x )i. These features, similar to the cosine similarity and the Euclidean distance, might capture a different notion of interaction information about the relationship holding between two words. We directly integrate them into some commonly used representations. For instance, we consider the concatenation Diff⊕Mult h(v~y − v~x )⊕(v~x v~y )i that might capture both"
W16-5304,C92-2082,0,0.706288,". We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information. 1 Introduction Automated methods to recognize the lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results"
W16-5304,E12-1004,0,0.646952,"ibutional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new d"
W16-5304,D10-1108,0,0.0533109,"le hidden layer trained on [~vwx , ~vwy ] (DSh ).2 Integrated We similarly adapt the HypeNET integrated model to classify multiple semantic relations (LexNET). Based on the same motivation of DSh , we also experiment with a version of the network with a hidden layer (LexNETh ), re-defining c = softmax(W2 · ~h + b2 ), where ~h = tanh(W1 · ~vxy + b1 ) is the hidden layer. The technical details of our network are identical to Shwartz et al. (2016). 4 Datasets We use four common semantic relation datasets that were created using semantic resources: K&H+N (Necsulescu et al., 2015) (an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), and ROOT09 (Santus et al., 2016). Table 1 displays the relation types and number of instances in each dataset. Most dataset relations are parallel to WordNet relations, such as hypernymy (cat, animal) and meronymy (hand, body), with an additional random relation for negative instances. BLESS contains the event and attribute relations, connecting a concept with a typical activity/property (e.g. (alligator, swim) and (alligator, aquatic)). EVALution contains a richer schema of semantic relations, with some redundancy: it contain"
W16-5304,N15-1098,1,0.910266,"semantic relations, as we describe next. 3 Classification Methods We experiment with several classification models, as illustrated in Figure 1: Path-based HypeNET’s path-based model (PB) is a binary classifier trained on the path vectors alone: ~vpaths(x,y) . We adapt the model to classify multiple relations by changing the network softmax output c to a distribution over k target relations, classifying a pair to the highest scoring relation: r = argmaxi c[i]. Distributional We train an SVM classifier on the concatenation of x and y’s word embeddings [~vwx , ~vwy ] (Baroni et al., 2012) (DS).1 Levy et al. (2015) claimed that such a linear classifier is incapable of capturing interactions between x and y’s features, and that instead it learns separate properties of x or y, e.g. that y is a prototypical hypernym. To examine the effect of non-linear expressive power on the model, we experiment with a neural network with a single hidden layer trained on [~vwx , ~vwy ] (DSh ).2 Integrated We similarly adapt the HypeNET integrated model to classify multiple semantic relations (LexNET). Based on the same motivation of DSh , we also experiment with a version of the network with a hidden layer (LexNETh ), re-"
W16-5304,P06-2075,1,0.812476,"Missing"
W16-5304,D12-1104,0,0.0574132,"ased information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information. 1 Introduction Automated methods to recognize the lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no m"
W16-5304,S15-1021,0,0.239148,"s, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new dataset. In this paper we present LexNET, an extension of HypeNET that recognizes multiple semantic relations. W"
W16-5304,P15-1146,0,0.103244,"Missing"
W16-5304,D14-1162,0,0.0827114,"he lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed t"
W16-5304,N13-1008,0,0.0531513,"always contributes to the classification, and analyze the cases in which it mostly complements the distributional information. 1 Introduction Automated methods to recognize the lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution o"
W16-5304,C14-1097,0,0.112538,"methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new dataset. In this paper"
W16-5304,W15-4200,0,0.195003,"arly adapt the HypeNET integrated model to classify multiple semantic relations (LexNET). Based on the same motivation of DSh , we also experiment with a version of the network with a hidden layer (LexNETh ), re-defining c = softmax(W2 · ~h + b2 ), where ~h = tanh(W1 · ~vxy + b1 ) is the hidden layer. The technical details of our network are identical to Shwartz et al. (2016). 4 Datasets We use four common semantic relation datasets that were created using semantic resources: K&H+N (Necsulescu et al., 2015) (an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), and ROOT09 (Santus et al., 2016). Table 1 displays the relation types and number of instances in each dataset. Most dataset relations are parallel to WordNet relations, such as hypernymy (cat, animal) and meronymy (hand, body), with an additional random relation for negative instances. BLESS contains the event and attribute relations, connecting a concept with a typical activity/property (e.g. (alligator, swim) and (alligator, aquatic)). EVALution contains a richer schema of semantic relations, with some redundancy: it contains both meronymy and holonymy (e.g. for bicycle and wheel), and the"
W16-5304,L16-1722,0,0.617616,"ven pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new dataset. In this paper we present LexNET, an extension of HypeNET that recognizes mult"
W16-5304,W16-5310,1,0.693144,"ath representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new dataset. In this paper we present LexNET, an extension of HypeNET that recognizes multiple semantic relations. We show that this integrated method is indeed effective also in the multiclass setting. In the evaluations reported in this paper, LexNET performed better than each individual method on several common datasets. Further, it was the best performing system in the semantic relation classification task of the CogALex 2016 shared task (Shwartz and Dagan, 2016). We further assess the contribution of path-based information to semantic relation classification. Even though the distributional source is dominant across most datasets, path-based information always contributed to it. In particular, path-based information seems to better capture the relationship between terms, rather than their individual properties, and can do so even for rare words or senses. Our code and data are available at https://github.com/vered1986/LexNET. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.o"
W16-5304,P16-1226,1,0.73755,". Distributional Information in Recognizing Lexical Semantic Relations Vered Shwartz Ido Dagan Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel vered1986@gmail.com dagan@cs.biu.ac.il Abstract Recognizing various semantic relations between terms is beneficial for many NLP tasks. While path-based and distributional information sources are considered complementary for this task, the superior results the latter showed recently suggested that the former’s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. The empirical results show that this method is effective in the multiclass setting as well. We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information. 1 Introduction Automated methods to recognize the lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurre"
W16-5304,J13-3004,0,\N,Missing
W16-5304,W11-2501,0,\N,Missing
W16-5304,W15-4208,0,\N,Missing
W16-5304,W11-2500,0,\N,Missing
W16-5310,C92-2082,0,0.764622,"lated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based fe"
W16-5310,E12-1004,0,0.585701,"al information sources in recognizing semantic relatedness.1 To aid in recognizing whether a pair of words are related at all (subtask 1), we combine LexNET with a common similarity measure (cosine similarity), achieving fairly good performance, and a slight improvement upon using cosine similarity alone. Subtask 2, however, has shown to be extremely difficult, with LexNET and all other systems achieving relatively low F1 scores. The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the stricter evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application settings. The difficulty of the semantic relation classification task emphasizes the need to develop better methods for this task. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 LexNET’s code is available at https://github.com/vered1986/LexNET, and the shared task results are available at https://sites.google.com/si"
W16-5310,P13-2080,1,0.817447,"emantic relations. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively low performance of LexNET and all other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task. 1 Introduction Discovering whether words are semantically related and identifying the specific semantic relation that holds between them is a key component in many NLP applications, such as question answering and recognizing textual entailment (Dagan et al., 2013). Automated methods for semantic relation identification are commonly corpus-based, and mainly rely on the distributional representation of each word. The CogALex shared task on the corpus-based identification of semantic relations consists of two subtasks. In the first task, the system needs to identify for a word pair whether the words are semantically related or not (e.g. True:(dog, cat), False:(dog, fruit)). In the second task, the goal is to determine the specific semantic relation that holds for a given pair, if any (PART OF:(tail, cat), HYPER:(cat, animal)). In this paper we describe ou"
W16-5310,P14-1113,0,0.160503,"2016a).2 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus e"
W16-5310,W13-2608,0,0.0201949,"tor similarity or distance measure is applied to their distributional representations: sim(~vwx , ~vwy ). This is a straightforward application of the distributional hypothesis (Harris, 1954), according to which related words occur in similar contexts, hence have similar vector representations. Most commonly, vector cosine is adopted as a similarity measure (Turney et al., 2010). Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012), Jaccard’s coefficient (Salton and McGill, 1986), and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al., 2016a).2 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2"
W16-5310,P99-1004,0,0.266338,"dded using a LSTM (Hochreiter and Schmidhuber, 1997), as described in Shwartz et al. (2016). This vector is then fed into a neural network that outputs the class distribution ~c, and then the pair is classified to the relation with the highest score r: ~c = softmax(MLP(~vxy )) (1a) r = argmaxi ~c[i] (1b) MLP stands for Multi Layer Perceptron, and could be computed with or without a hidden layer (equations 2 and 3, respectively): ~h = tanh(W1 · ~vxy + b1 ) MLP(~vxy ) = W2 · ~h + b2 (2b) MLP(~vxy ) = W1 · ~vxy + b1 (3) where Wi and bi are the network parameters and ~h is the hidden layer. 2 See Lee (1999) for an extensive list of such measures. 81 (2a) Subtask 1 Subtask 2 Method Cos LexNET LexNET+Cos Dist LexNET Hyper-parameters word2vec, t: 0.3 hidden layers: 0, dropout: 0.0, epochs: 3 word2vec, wL = 0.3, wC = 0.7, t = 0.29 dep-based, method: concat, classifier: SVM, L1 hidden layers: 0, dropout: 0.0, epochs: 5 Corpus size 100B 6B ∼100B 3B 6B P 0.759 0.780 0.814 0.611 0.658 R 0.795 0.561 0.854 0.598 0.646 F1 0.776 0.652 0.833 0.600 0.642 Table 1: Performance scores on the validation set along with hyper-parameters and effective corpus size (#tokens) used by each method. Subtask 2 results refe"
W16-5310,P14-2050,0,0.0408781,"ty scores were chosen among several available pre-trained embeddings.4 For completeness we also report the performance of two baselines: cosine similarity (wC = 1) and LexNET (wL = 1, fixed t = 0.5). 3 A random split yielded perfect results on the validation set, which were due to lexical memorization (Levy et al., 2015). word2vec (300 dimensions, SGNS, trained on GoogleNews, 100B tokens) (Mikolov et al., 2013), GloVe (50-300 dimensions, trained on Wikipedia and Gigaword 5, 6B tokens) (Pennington et al., 2014), and dependency-based embeddings (300 dimensions, trained on Wikipedia, 3B tokens) (Levy and Goldberg, 2014). 4 82 Subtask 1 Subtask 2 Method Random Baseline Majority Baseline Cos LexNET+Cos Random Baseline Majority Baseline Dist LexNET P 0.283 0.000 0.841 0.754 0.073 0.000 0.469 0.480 R 0.503 0.000 0.672 0.777 0.201 0.000 0.371 0.418 F1 0.362 0.000 0.747 0.765 0.106 0.000 0.411 0.445 Table 2: Performance scores on the test set in each subtask, of the selected methods and the baselines. 4.2 Subtask 2: Semantic Relation Classification The subtask’s train set is highly imbalanced towards random instances (roughly 10 times more than any other relation), and training any supervised method leads to overf"
W16-5310,N15-1098,1,0.941091,"LATED] (4) where wC , wL are the weights assigned to cosine similarity and LexNET’s scores respectively, such that wC + wL = 1. We tuned the weights and a threshold t using the validation set, and classified (x, y) as related if Rel(x, y) ≥ t. The word vectors used to compute the cosine similarity scores were chosen among several available pre-trained embeddings.4 For completeness we also report the performance of two baselines: cosine similarity (wC = 1) and LexNET (wL = 1, fixed t = 0.5). 3 A random split yielded perfect results on the validation set, which were due to lexical memorization (Levy et al., 2015). word2vec (300 dimensions, SGNS, trained on GoogleNews, 100B tokens) (Mikolov et al., 2013), GloVe (50-300 dimensions, trained on Wikipedia and Gigaword 5, 6B tokens) (Pennington et al., 2014), and dependency-based embeddings (300 dimensions, trained on Wikipedia, 3B tokens) (Levy and Goldberg, 2014). 4 82 Subtask 1 Subtask 2 Method Random Baseline Majority Baseline Cos LexNET+Cos Random Baseline Majority Baseline Dist LexNET P 0.283 0.000 0.841 0.754 0.073 0.000 0.469 0.480 R 0.503 0.000 0.672 0.777 0.201 0.000 0.371 0.418 F1 0.362 0.000 0.747 0.765 0.106 0.000 0.411 0.445 Table 2: Performan"
W16-5310,D12-1104,0,0.522195,"lation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based features: ~vxy = [~vwx , ~vpaths(x,y) , ~vwy ]"
W16-5310,S15-1021,0,0.289491,"ler et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based features: ~vxy = [~vwx , ~vpaths(x,y) , ~vwy ], where ~vwx and ~vwy are x and y’s word embeddings, providing their distributional representation, and ~vpaths(x,y) is the average embedding vector of all the dependency paths that connect x and y in the corpus. Dependency paths are embedded using a LSTM (Hochreiter and Schmidhuber, 1997), as described in Shwartz et al. (2016). This vector is then fed int"
W16-5310,D14-1162,0,0.0863279,"set, and classified (x, y) as related if Rel(x, y) ≥ t. The word vectors used to compute the cosine similarity scores were chosen among several available pre-trained embeddings.4 For completeness we also report the performance of two baselines: cosine similarity (wC = 1) and LexNET (wL = 1, fixed t = 0.5). 3 A random split yielded perfect results on the validation set, which were due to lexical memorization (Levy et al., 2015). word2vec (300 dimensions, SGNS, trained on GoogleNews, 100B tokens) (Mikolov et al., 2013), GloVe (50-300 dimensions, trained on Wikipedia and Gigaword 5, 6B tokens) (Pennington et al., 2014), and dependency-based embeddings (300 dimensions, trained on Wikipedia, 3B tokens) (Levy and Goldberg, 2014). 4 82 Subtask 1 Subtask 2 Method Random Baseline Majority Baseline Cos LexNET+Cos Random Baseline Majority Baseline Dist LexNET P 0.283 0.000 0.841 0.754 0.073 0.000 0.469 0.480 R 0.503 0.000 0.672 0.777 0.201 0.000 0.371 0.418 F1 0.362 0.000 0.747 0.765 0.106 0.000 0.411 0.445 Table 2: Performance scores on the test set in each subtask, of the selected methods and the baselines. 4.2 Subtask 2: Semantic Relation Classification The subtask’s train set is highly imbalanced towards random"
W16-5310,C14-1097,0,0.47225,"antic relatedness.1 To aid in recognizing whether a pair of words are related at all (subtask 1), we combine LexNET with a common similarity measure (cosine similarity), achieving fairly good performance, and a slight improvement upon using cosine similarity alone. Subtask 2, however, has shown to be extremely difficult, with LexNET and all other systems achieving relatively low F1 scores. The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the stricter evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application settings. The difficulty of the semantic relation classification task emphasizes the need to develop better methods for this task. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 LexNET’s code is available at https://github.com/vered1986/LexNET, and the shared task results are available at https://sites.google.com/site/cogalex2016/home/shared-task/results 80"
W16-5310,W15-4200,0,0.347651,"l., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based features: ~vxy = [~vwx , ~vpaths(x,y) , ~vwy ], where ~vwx and ~vwy are x and y’s word embeddings, providing their distributional representation, and ~vpaths(x,y) is the average embedding vector of all the dependency paths that connect x and y in the corpus. Dependency paths are embedded using a LSTM (Hochreiter and Schmidhuber, 1997), as described in Shwartz et al. (2016). This vector is then fed into a neural network th"
W16-5310,Y16-2021,0,0.325569,"is applied to their distributional representations: sim(~vwx , ~vwy ). This is a straightforward application of the distributional hypothesis (Harris, 1954), according to which related words occur in similar contexts, hence have similar vector representations. Most commonly, vector cosine is adopted as a similarity measure (Turney et al., 2010). Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012), Jaccard’s coefficient (Salton and McGill, 1986), and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al., 2016a).2 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et"
W16-5310,L16-1722,0,0.51124,"is applied to their distributional representations: sim(~vwx , ~vwy ). This is a straightforward application of the distributional hypothesis (Harris, 1954), according to which related words occur in similar contexts, hence have similar vector representations. Most commonly, vector cosine is adopted as a similarity measure (Turney et al., 2010). Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012), Jaccard’s coefficient (Salton and McGill, 1986), and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al., 2016a).2 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et"
W16-5310,W16-5304,1,0.558235,"monly corpus-based, and mainly rely on the distributional representation of each word. The CogALex shared task on the corpus-based identification of semantic relations consists of two subtasks. In the first task, the system needs to identify for a word pair whether the words are semantically related or not (e.g. True:(dog, cat), False:(dog, fruit)). In the second task, the goal is to determine the specific semantic relation that holds for a given pair, if any (PART OF:(tail, cat), HYPER:(cat, animal)). In this paper we describe our approach and system setup for the shared task. We use LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. LexNET was the system with the overall best performance on subtask 2, and was ranked third on subtask 1, demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing semantic relatedness.1 To aid in recognizing whether a pair of words are related at all (subtask 1), we combine LexNET with a common similarity measure (cosine similarity), achieving fairly good performance, and a slight improvement upon using cosine similarity alone. Subtask 2,"
W16-5310,P16-1226,1,0.937227,"lications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based features: ~vxy = [~vwx , ~vpaths(x,y) , ~vwy ], where ~vwx and ~vwy are x and y’s word embeddings, providing their distributional representation, a"
W16-5310,C14-1212,0,0.392928,"s in recognizing semantic relatedness.1 To aid in recognizing whether a pair of words are related at all (subtask 1), we combine LexNET with a common similarity measure (cosine similarity), achieving fairly good performance, and a slight improvement upon using cosine similarity alone. Subtask 2, however, has shown to be extremely difficult, with LexNET and all other systems achieving relatively low F1 scores. The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the stricter evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application settings. The difficulty of the semantic relation classification task emphasizes the need to develop better methods for this task. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 LexNET’s code is available at https://github.com/vered1986/LexNET, and the shared task results are available at https://sites.google.com/site/cogalex2016/home/"
W16-5310,W11-2501,0,\N,Missing
W16-5310,W15-4208,0,\N,Missing
W16-5310,W11-2500,0,\N,Missing
W17-0902,J14-2004,0,0.0128507,"etween predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-ali"
W17-0902,araki-etal-2014-detecting,0,0.0144858,"tracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between pre"
W17-0902,D08-1031,1,0.548822,"reras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (O"
W17-0902,P10-1124,1,0.91175,"ic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+"
W17-0902,J12-1003,1,0.927138,"rbal performance (accuracy of 0.73 vs. 0.25). Finally, argument identification was hard mainly because of inconsistencies in verbal versus nominal predicate-argument structure in dependency trees.4 The low performance in predicate coreference compared to entity coreference can be explained by the higher variability of predicate terms. The argument co-reference task becomes easy given gold predicate-argument structures, as most arguments are singletons (i.e. composed of a single element). Finally, while the performance of the predicate entailment component reflects the current stateof-the-art (Berant et al., 2012; Han and Sun, 2016), the performance on entity entailment is much worse than the current state-of-the-art in this task as measured on common lexical inference test sets. We conjecture that this stems from the nature of the entities in our dataset, consisting of both named entities and common nouns, many of which are multi-word expressions, whereas most work in entity entailment is focused on single word common nouns. Furthermore, it is worth noting that our annotations are of naturally occurring texts, and represent lexical entailment in real world coreference chains, as opposed to synthetica"
W17-0902,W05-0620,0,0.162639,"Missing"
W17-0902,W99-0201,0,0.0935993,"on Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference"
W17-0902,W09-4303,0,0.0115663,") is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining en"
W17-0902,W13-2322,0,0.107609,"le texts, and in specifying how such representation can be created based on entity and event coreference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al.,"
W17-0902,P15-1136,0,0.0225471,"Missing"
W17-0902,E12-1004,0,0.0217269,"tractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art"
W17-0902,cybulska-vossen-2014-using,0,0.0126286,"achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can h"
W17-0902,P99-1071,0,0.536493,"on. We can expect the use of OKR structures in MDS to shift the research efforts in this task to other components, e.g. generation, and eventually contribute to improving state of the art on this task. Similarly, an algorithm creating the ECKG structure can benefit from building upon a consolidated structure such as OKR, rather than working directly on free text. systematic solution, and the burden of integrating information across multiple texts is delegated to downstream applications, leading to partial solutions which are geared to specific applications. Multi-Document Summarization (MDS) (Barzilay et al., 1999) is a task whose goal is to produce a concise summary from a set of related text documents, such that it includes the most important information in a non-redundant manner. While extractive summarization selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a sing"
W17-0902,P13-2080,1,0.82203,"ment annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of"
W17-0902,bejan-harabagiu-2008-linguistic,0,0.0326074,"1; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align"
W17-0902,duclaye-etal-2002-using,0,0.0512409,"implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 20"
W17-0902,D12-1045,0,0.0269834,"2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing t"
W17-0902,D11-1142,0,0.0614886,"ference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments a"
W17-0902,D14-1168,0,0.0154703,"selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entitie"
W17-0902,C16-1273,0,0.0306302,"Missing"
W17-0902,liu-etal-2014-supervised,0,0.0231139,"Missing"
W17-0902,D15-1076,0,0.0606389,"on in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use"
W17-0902,N15-1114,0,0.026058,"ng each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on coreference. 7 Conclusions In this paper we advocate the development of representation frameworks for the consolidated information expressed in a set of texts. The key ingredients of our approach are the extraction of pr"
W17-0902,C92-2082,0,0.446376,"wo candidate sentences for the summary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Ent"
W17-0902,W97-1311,0,0.0722333,"rior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to"
W17-0902,N15-1050,1,0.790932,"event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored"
W17-0902,P13-1048,0,0.0297432,"zation generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on corefere"
W17-0902,W16-5304,1,0.885624,"Missing"
W17-0902,K15-1018,1,0.833569,"m for improvement. These bottle-necks are bound to hinder the performance of a pipeline end-to-end system. Future research into OKR should first target these areas; either as a pipeline or in a joint learning framework. 3 For Argument Mention detection we attach the components (entities and propositions) as arguments of predicates when the components are syntactically dependent on them. Argument Coreference is simply predicted by marking coreference if and only if the arguments are both mentions of the same entity co-reference chain. For Entity Entailment purposes we used knowledge resources (Shwartz et al., 2015) and a pretrained model for HypeNET (Shwartz et al., 2016) to obtain a score for all pairs of Wikipedia common words (unigrams, bigrams, and trigrams). A threshold for the binary entailment decision was then calibrated on a held out development set. Finally, for Predicate Entailment we used the entailment rules extracted by Berant et al. (2012). 5.1 Results and Error Analysis Using the same metrics used for measuring interannotator agreement, we evaluated how well the presented models were able to recover the different facets of the OKR gold annotations. The performance on the different subtas"
W17-0902,P15-1146,0,0.0257952,"Missing"
W17-0902,P16-1226,1,0.873666,"Missing"
W17-0902,K15-1002,1,0.834714,"ile the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al.,"
W17-0902,D16-1038,1,0.86973,"Missing"
W17-0902,P11-1080,0,0.0191905,"ddle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankar"
W17-0902,S12-1030,0,0.0281997,"ms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee e"
W17-0902,J01-4004,0,0.174444,"s like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolutio"
W17-0902,P16-1119,1,0.813405,"cly available tools and simple baselines which approximate the current state-of-the-art in each of these subtasks. For brevity sake, in the rest of the section we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agr"
W17-0902,E14-4008,0,0.0143178,"mmary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Entities: E1 = {Turkey}, E2 = {Syrian}, E3"
W17-0902,P15-2050,1,0.81398,"test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are rep"
W17-0902,D10-1106,0,0.0195202,"ub.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity c"
W17-0902,P12-3013,1,0.834165,"ss document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of Open IE, we would like the representation to be open, while relying only on the"
W17-0902,W04-3206,1,0.412965,"ailable at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions,"
W17-0902,C16-1183,1,0.835662,"on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual"
W17-0902,M95-1005,0,0.294299,"ction we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agreement scores for the two annotators are shown in Table 1, and overall show high levels of agreement. A qualitative analysis of the more common disa"
W17-0902,C14-1212,0,0.0125443,"t Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task"
W17-0902,P13-2012,0,0.0402841,"Missing"
W17-0902,N15-1002,0,0.0522403,"Missing"
W17-0902,P02-1014,0,\N,Missing
W17-0902,P15-1034,0,\N,Missing
W17-6927,E17-1104,0,0.0510913,"Missing"
W17-6927,W03-1210,0,0.0911189,"rpus (Hidey and McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of propositional causality in Japanese. The system incorporates background knowledge for enhancing the learning process through the use of multi-column convolutional neural networks. Regarding the classification of explicit causality, Khoo et al. (1998) proposed a rule-based system grounded in regular expressions for the classification of explicit causal relations, whereas Mirza and Tonelli (2016) pre"
W17-6927,P16-1135,0,0.345848,"of their disambiguation. Due to the lack of unambiguous linguistic construction of causality, we claim that the use of linguistic features may restrict the representation of causality meaning. Also, the use of dense vector spaces, roughly speaking word embeddings, can provide a better representation of causality, and can improve the disambiguation of the causal meaning of lexical markers. Hence, we propose a neural network architecture with two inputs as sequences of word embeddings, encoding the left and the right context of the lexical marker. We evaluate our proposal on the AltLex corpus (Hidey and McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju ("
W17-6927,K16-1006,1,0.831429,"the inputs (see Equation 1). For each word, its corresponding word vector of 300 com- Figure 1: Neural model, where e1 is ponents (d) was looked up in the 840b cased Glove embeddings the first event, l is the lexical marker (Pennington et al., 2014). Subsequently, the concatenated word and e2 is the second event. embeddings get passed through an encoding Long Short-Term 1 1,1 1,2 1,3 2 1,t 2,1 2,2 2,3 1 1 2,t 2 2 Memory (LSTM) recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) layer. We decided to use LSTM because of its ability to encode sequential and contextual information (Melamud et al., 2016). We assume some sort of relation exists between e1 and e2 , so we first evaluated the performance of the connection of the two LSTM layers through the initialization of the second LSTM with the end state of the first one (dashed arrow in Figure 1). We call this model “Stated Pair LSTM”. We assessed the same model but without the connection of the two LSTMs for evaluating our assumption. We call it “Pair LSTM” (no dashed arrow in Figure 1). The two outputs of the encoding layer are transformed to a vector of length 100 by a dense layer with a tanh activation function. The context of the causal"
W17-6927,C16-1007,0,0.126855,"s. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of propositional causality in Japanese. The system incorporates background knowledge for enhancing the learning process through the use of multi-column convolutional neural networks. Regarding the classification of explicit causality, Khoo et al. (1998) proposed a rule-based system grounded in regular expressions for the classification of explicit causal relations, whereas Mirza and Tonelli (2016) presented a supervised system based on the use of lexical, syntactic and semantic features from WordNet. The proposal of Bethard and Martin (2008) is similar to that of Mirza and Tonelli (2016), but it was focused only on conjunction constructions, namely conjoined events. The three approaches suffer from the ambiguity of the lexical markers, the limited coverage of the linguistic resources and the constraint to a specific syntactic construction. In contrast, our proposal tries to cover lexical and propositional causality independently of whether it is explicit or implicit, and we do not rest"
W17-6927,D14-1162,0,0.0802783,"o in... ... puts. The lengths (n, m) of the instances of each input are not Embed. Lookup Embed. Lookup necessarily the same, so in order to make their lengths equal, Token. & w ... w ... w w w w w w Padding: three zero-padding strategies were assessed, namely the maxie e Input: l * mum, the mean and the mode of the lengths (t) of the compo|input |= n |input |= m nents of the inputs (see Equation 1). For each word, its corresponding word vector of 300 com- Figure 1: Neural model, where e1 is ponents (d) was looked up in the 840b cased Glove embeddings the first event, l is the lexical marker (Pennington et al., 2014). Subsequently, the concatenated word and e2 is the second event. embeddings get passed through an encoding Long Short-Term 1 1,1 1,2 1,3 2 1,t 2,1 2,2 2,3 1 1 2,t 2 2 Memory (LSTM) recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) layer. We decided to use LSTM because of its ability to encode sequential and contextual information (Melamud et al., 2016). We assume some sort of relation exists between e1 and e2 , so we first evaluated the performance of the connection of the two LSTM layers through the initialization of the second LSTM with the end state of the first one (dashed"
W17-6927,prasad-etal-2008-penn,0,0.0817483,". Also, the use of dense vector spaces, roughly speaking word embeddings, can provide a better representation of causality, and can improve the disambiguation of the causal meaning of lexical markers. Hence, we propose a neural network architecture with two inputs as sequences of word embeddings, encoding the left and the right context of the lexical marker. We evaluate our proposal on the AltLex corpus (Hidey and McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of proposi"
W17-6927,W13-4004,0,0.177091,"d McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of propositional causality in Japanese. The system incorporates background knowledge for enhancing the learning process through the use of multi-column convolutional neural networks. Regarding the classification of explicit causality, Khoo et al. (1998) proposed a rule-based system grounded in regular expressions for the classification of explicit causal relations, whereas Mirza and Tonelli (2016) presented a supervised sy"
W17-6927,W14-4322,0,0.0256738,"Missing"
W17-6927,P08-2045,0,\N,Missing
W19-5111,P01-1008,0,0.15106,"e embeddings for variable-length phrases, by adapting the word embedding training objective (Poliak et al., 2017) or by minimizing the distance between the representations of paraphrases (Wieting et al., 2016; Wieting and Gimpel, 2017; Wieting et al., 2017). Paraphrase-based phrase embeddings require a large number of paraphrases as training instances. Such paraphrases are often generated by translating an English phrase into a foreign language and back to English, considering variations in translation as paraphrases. This technique is referred to as “bilingual pivoting” or “backtranslation” (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013; Mallinson et al., 2017). In this work we test the quality of noun compound representations produced by different methods, including distributional representations, composition functions, and paraphrase-based phrase embeddings. We extend the work of Dima (2016), who evaluated various composition functions on the noun compound relation classification task, in several aspects. First, we test a broader range of representations, which may differ both in their architectures and in their training objectives. Second, we train each represen"
W19-5111,Q17-1010,0,0.183322,"Missing"
W19-5111,W13-0104,0,0.0453765,"icate representative in Table 1 demonstrate the well known fact that the distributional embeddings of rare terms are of low quality. The goal of the composition functions is to provide meaningful representations for ad-hoc, possibly rare compositions of nouns. They are learned as an approximation of the observed (distributional) representations of frequent noun compounds. How frequent should a noun compound be for its observed representation to be preferred over the compositional one? For example, the nearest neighbours of army officer, a very frequent term, indicate that its distributional 7 Boleda et al. (2013) found that in the case of adjectivenoun compositions, observed vectors were preferred for frequent compositions, and compositional vectors for rare ones. 95 Rare Noun Compounds Frequent Noun Compounds 100 80 60 40 20 Rare words Share words with target Other noun compounds Backtrans Cooc LSTM Matrix FullAdd Add Dist Backtrans Cooc LSTM Matrix FullAdd Add Dist 0 Backtrans. paraphrases Similar in WordNet Other words Figure 2: Categories of the top 10 neighbors of each target compound, for the 100 most rare compounds in the test set (first row) and the 100 most frequent compounds in the test set"
W19-5111,N19-1423,0,0.014789,"o use (and train) a distributional embedding for a given noun compound may be based on its frequency. The author is supported by the Clore Scholars Programme (2017). References Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 597–604, Ann Arbor, Michigan. Association for Computational Linguistics. Contextualized Word Embeddings are dynamic word embeddings computed for words given their context sentence (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). They have become increasingly popular last year, outperforming static embeddings across NLP tasks. Supposedly, such representations obviate the need to learn dedicated noun compound representations, as the vector of each constituent is computed given the other constituent. Recently, Shwartz and Dagan (2019) found that while these representations excel at detecting non-compositional noun compounds, they perform much worse at revealing implicit information such as the relationship between the constituents. Moreover, looking into these models’ predictions of substitute constituents shows that e"
W19-5111,W16-1604,0,0.383232,"ining instances. Such paraphrases are often generated by translating an English phrase into a foreign language and back to English, considering variations in translation as paraphrases. This technique is referred to as “bilingual pivoting” or “backtranslation” (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013; Mallinson et al., 2017). In this work we test the quality of noun compound representations produced by different methods, including distributional representations, composition functions, and paraphrase-based phrase embeddings. We extend the work of Dima (2016), who evaluated various composition functions on the noun compound relation classification task, in several aspects. First, we test a broader range of representations, which may differ both in their architectures and in their training objectives. Second, we train each representation with a wide variety of underlying word embeddings, and analyze the representation’s behaviour across the different word embeddings. Finally, we use several tasks to Building meaningful representations of noun compounds is not trivial since many of them scarcely appear in the corpus. To that end, composition functio"
W19-5111,W13-3206,0,0.0204963,"rn noun compound vectors. The main issue with this approach is that word embedding algorithms require sufficient term frequency to obtain meaningful representations, and many noun compounds rarely occur in text corpora (Kim and Baldwin, 2006). To overcome the sparsity issue, it is common to learn a composition function which computes a noun compound vector from its constituents’ 92 Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 92–103 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics • FullAdd (Zanzotto et al., 2010; Dinu et al., 2013): f (~vw1 , ~vw2 ) = W1~vw1 + W2~vw2 , where W1 , W2 ∈ Rd×d are matrices. evaluate the representation quality: relation classification (what is the relationship between the constituents?), property classification (is a cheese wheel round?), as well as a qualitative and quantitative analysis of the nearest neighbours. The results confirm that the distributional representations of rare noun compounds are indeed of low quality. Across representations, the nearest neighbours of a target noun compound vector typically include many trivial similarities such as other noun compounds with a shared cons"
W19-5111,W18-2501,0,0.0266501,"bours of each noun compound vector (Section 3.1), an evaluation on property prediction (Section 3.2), and an evaluation on noun compound relation classification (Section 3.3). 3.1 Nearest Neighbour Analysis Similarly to Boleda et al.’s (2013) analysis for adjective-noun compositions, we compute the 10 nearest neighbors of each noun compound in the test set and analyze the outputs. Table 1 exemplifies the nearest neighbours of two noun compounds in each representation, setting the DSM to (word2vec SG, window 5, 300d). Implementation Details We implemented the models using the AllenNLP library (Gardner et al., 2018) which is based on 4 https://pypi.org/project/guess_ language-spirit/ 5 We used the Gensim implementation: https:// radimrehurek.com/gensim/ 6 Omitting 351 noun compounds belonging to the LEXI classes. CALIZED , PERSONAL NAME , and PERSONAL TITLE 94 syndicate representative (rare) Distributional geloios t.franse adopter(s ahchie anquish Add syndicate representative f(worker, representative) f(deputy, representative) f(student, representative) Co-occurrence f(company, representative) f(phone, representative) f(union, representative) f(marketing, representative) f(labor, representative) Composit"
W19-5111,E17-2081,0,0.0375831,"Missing"
W19-5111,P07-1072,0,0.0200595,"ive Distributional. This approach simply treats a noun compound as a single token w1 w2 , and learns standard word embeddings for the words and noun compounds in the corpus. • Backtranslation: We translate each noun compound to foreign language(s) and back to English, as in Wieting et al. (2017). Specifically, we use the DeepL Translator web interface,3 performing translation from English to 4 different foreign languages (French, Italian, Spanish, and Romanian) and back to English. We focused on Romance languages because they translate English noun compounds to noun phrases with prepositions (Girju, 2007), and we were hoping that this would drive the backtranslation to be more explicit. For example, baby oil is translated in French to huile pour b´eb´e, which literally means oil for baby. In Compositional. We learn a function f (·, ·) : Rd × Rd → Rd which, for a given noun compound, operates on the word embeddings of its constituent nouns, and returns a vector representing the compound. Following Dima (2016) and earlier work, the training objective is to minimize the distance between the observed distributional embedding ~vw1 w2 and the composed vector f (~vw1 , ~vw2 ). We train the following"
W19-5111,P06-2064,0,0.0605622,"ction The simplest way to obtain a vector representation for a multiword term is to treat it as a single token, e.g. by replacing spaces with underscores, and train a standard word embedding algorithm. This is typically done for common n-grams, which often include named entities (e.g. New York), but in theory can also be based on syntactic criteria, for instance in order to learn noun compound vectors. The main issue with this approach is that word embedding algorithms require sufficient term frequency to obtain meaningful representations, and many noun compounds rarely occur in text corpora (Kim and Baldwin, 2006). To overcome the sparsity issue, it is common to learn a composition function which computes a noun compound vector from its constituents’ 92 Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 92–103 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics • FullAdd (Zanzotto et al., 2010; Dinu et al., 2013): f (~vw1 , ~vw2 ) = W1~vw1 + W2~vw2 , where W1 , W2 ∈ Rd×d are matrices. evaluate the representation quality: relation classification (what is the relationship between the constituents?), property classification (is a chees"
W19-5111,P15-2119,0,0.0190179,"ues) and select the best performing classifier with respect to the validation F1 score. Property Prediction Do the various representations capture properties of noun compounds? To answer this question, we create a task in which we need to predict for a given noun compound whether it has a certain property or not. For example, is a cheese wheel round? 3.2.1 Task Definition and Data We use the McRae Feature Norms dataset (McRae et al., 2005), which provides, for single words describing concrete nouns, the most salient properties that describe them. We follow the binary classification setting of Rubinstein et al. (2015) in which each task is focused on a single property, and negative instances are (a sample of) the concepts that do not appear with the property. To augment this data with noun compounds, we first filtered the dataset such that it only contains constituents of noun compounds in our vocabulary. We then selected 5 of the most frequent properties (“a weapon”, “round”, “made of metal”, “used for transportation”, and “comes in different colors”). For each property, we looked for all the noun compounds that consist of a constituent annotated to holding this property, and manually annotated them to wh"
W19-5111,E17-1083,0,0.0211699,"ve (Poliak et al., 2017) or by minimizing the distance between the representations of paraphrases (Wieting et al., 2016; Wieting and Gimpel, 2017; Wieting et al., 2017). Paraphrase-based phrase embeddings require a large number of paraphrases as training instances. Such paraphrases are often generated by translating an English phrase into a foreign language and back to English, considering variations in translation as paraphrases. This technique is referred to as “bilingual pivoting” or “backtranslation” (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013; Mallinson et al., 2017). In this work we test the quality of noun compound representations produced by different methods, including distributional representations, composition functions, and paraphrase-based phrase embeddings. We extend the work of Dima (2016), who evaluated various composition functions on the noun compound relation classification task, in several aspects. First, we test a broader range of representations, which may differ both in their architectures and in their training objectives. Second, we train each representation with a wide variety of underlying word embeddings, and analyze the representati"
W19-5111,P18-1111,1,0.820667,"s bag-of-words, predicting the target word from its context).5 We also trained the GloVe algorithm (Pennington et al., 2014), which estimates the log-probability of a word pair cooccurrence. All the embeddings were trained on the English Wikipedia dump from January 2018, with various values for the window size (2, 5, 10) and the embedding dimension (100, 200, 300). 2.3 40 020 50 100 • Co-occurrence: We treat the frequent joint occurrences of w1 and w2 in a corpus as paraphrases, e.g. apple cake may yield a paraphrase like “cake made of apples”. Specifically, we use the paraphrases obtained by Shwartz and Dagan (2018) from the Google N-gram corpus (Brants and Franz, 2006). The paraphrases are of variable length (3-5 words), and have been pre-processed to remove punctuation, adjectives, adverbs and determiners. The averaged number of paraphrases per compound is 9.18. 2.2 60 3 Experiments We compare the various representations in 3 experiments: an analysis of the nearest neighbours of each noun compound vector (Section 3.1), an evaluation on property prediction (Section 3.2), and an evaluation on noun compound relation classification (Section 3.3). 3.1 Nearest Neighbour Analysis Similarly to Boleda et al.’s"
W19-5111,Q19-1027,1,0.78413,"ting of the Association for Computational Linguistics (ACL’05), pages 597–604, Ann Arbor, Michigan. Association for Computational Linguistics. Contextualized Word Embeddings are dynamic word embeddings computed for words given their context sentence (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). They have become increasingly popular last year, outperforming static embeddings across NLP tasks. Supposedly, such representations obviate the need to learn dedicated noun compound representations, as the vector of each constituent is computed given the other constituent. Recently, Shwartz and Dagan (2019) found that while these representations excel at detecting non-compositional noun compounds, they perform much worse at revealing implicit information such as the relationship between the constituents. Moreover, looking into these models’ predictions of substitute constituents shows that even when they recognize a constituent is not used in its literal sense (e.g. in non-compositional compounds), the representation of its (often rare) non-literal sense is not always meaningful. Overall, contextualized word embeddings do not completely solve the problem of obtaining meaningful representations f"
W19-5111,N18-2035,1,0.627358,"f noun compounds which are not properties of their constituents. 12 https://commons.wikimedia.org/wiki/ Category:Round_objects 10 WordNet only consists of lexicalized noun compounds, e.g. olive oil and ice cream, which tend to be frequent. 97 It is important to note that the categorization of noun compounds to a fixed inventory of semantic relations that may hold between their constituents is often subjective, making the data noisy. Previous work suggested that many noun compounds fit into more than one relation, and that some relations in the fine-grained version of the data are overlapping (Shwartz and Waterson, 2018). With that said, this data is still a useful proxy for measuring and comparing the quality of representations. form among the worst. This is expected both due to the quality of the embeddings of rare noun compounds (Section 3.1) and since some of the noun compounds in the data are out-of-vocabulary. In contrast, the other representations compute ad hoc vectors for such noun compounds. For the sake of completeness, Table 3 displays the best performing DSM for each property. There is a preference to word2vec and to a higher embedding dimension. Looking at the errors made by the best model we fo"
W19-5111,D14-1162,0,0.0889995,", and a trained instance of the representation, with a choice of underlying word embeddings (algorithm, dimension, and window), as a DSM. Constituent Word Embeddings To represent the constituent words, we trained various word embedding algorithms: word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2017), which extends word2vec by adding subword information. We used both the SkipGram objective (which predicts the context words given the target word) and the CBOW objective (continuous bag-of-words, predicting the target word from its context).5 We also trained the GloVe algorithm (Pennington et al., 2014), which estimates the log-probability of a word pair cooccurrence. All the embeddings were trained on the English Wikipedia dump from January 2018, with various values for the window size (2, 5, 10) and the embedding dimension (100, 200, 300). 2.3 40 020 50 100 • Co-occurrence: We treat the frequent joint occurrences of w1 and w2 in a corpus as paraphrases, e.g. apple cake may yield a paraphrase like “cake made of apples”. Specifically, we use the paraphrases obtained by Shwartz and Dagan (2018) from the Google N-gram corpus (Brants and Franz, 2006). The paraphrases are of variable length (3-5"
W19-5111,D12-1110,0,0.02476,"th more computational power and parameters generally produced higher quality representations. The paraphrase-based functions outperformed the others in the property prediction task, while the compositional functions performed better on relation classification. The results suggest that learning a composition function with a combined training objective is a promising research direction that may result in improved noun compound representations.1 2 • Matrix (Dima, 2016): f (~vw1 , ~vw2 ) = tanh(W · [~vw1 ; ~vw2 ]), where W ∈ R2d×d . This is the application of the recursive matrix-vector method of Socher et al. (2012) to binary phrases.2 • LSTM: encoding the compound with a long short-term memory network (LSTM; Hochreiter and Schmidhuber, 1997): f (~vw1 , ~vw2 ) = LST M (~vw1 , ~vw2 ). Paraphrase-based. In this approach we follow the literature of paraphrase-based phrase embeddings (e.g. Wieting et al., 2016, 2017). We generate paraphrases for each noun compound, and train the function with the objective of producing similar vectors to the noun compound and its paraphrase. To obtain the representation of a phrase (either a noun compound or its variable-length paraphrase), we encode it with an LSTM. For a g"
W19-5111,D17-1026,0,0.424188,"based on vector arithmetics (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Dinu et al., 2013). Such functions are learned with the objective of minimizing the distance between the observed (distributional) vector and the composed vector of each noun compound, and most functions are limited to binary noun compounds. A parallel line of work computes phrase embeddings for variable-length phrases, by adapting the word embedding training objective (Poliak et al., 2017) or by minimizing the distance between the representations of paraphrases (Wieting et al., 2016; Wieting and Gimpel, 2017; Wieting et al., 2017). Paraphrase-based phrase embeddings require a large number of paraphrases as training instances. Such paraphrases are often generated by translating an English phrase into a foreign language and back to English, considering variations in translation as paraphrases. This technique is referred to as “bilingual pivoting” or “backtranslation” (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013; Mallinson et al., 2017). In this work we test the quality of noun compound representations produced by different methods, including distributional representations, comp"
W19-5111,C10-1142,0,0.0911018,"Missing"
W19-5111,P94-1019,0,\N,Missing
W19-5111,P05-1074,0,\N,Missing
W19-5111,N13-1092,0,\N,Missing
