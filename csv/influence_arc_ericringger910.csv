2003.jeptalnrecital-long.23,C00-1007,0,0.203733,"tations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the researcher can understand why the model makes the decisions it makes and can improve the model by adding relevant linguistic features. The re"
2003.jeptalnrecital-long.23,W02-2105,1,0.833986,". This paper presents the French implementation of Amalgam, a machine-learned sentence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models. Keywords – Mots Clés Réalisation de phrase, génération automatique, arbres de décision, français. Sentence realization, generation, machine-learning, decision trees, French. 1 Introduction Amalgam is a multilingual sentence realization system. Developed originally for German (Corston-Oliver et al. 2002, Gamon et al. 2002), it has been adapted to French (Smets et al. 2003). Amalgam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language"
2003.jeptalnrecital-long.23,P02-1004,1,0.641736,"ench implementation of Amalgam, a machine-learned sentence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models. Keywords – Mots Clés Réalisation de phrase, génération automatique, arbres de décision, français. Sentence realization, generation, machine-learning, decision trees, French. 1 Introduction Amalgam is a multilingual sentence realization system. Developed originally for German (Corston-Oliver et al. 2002, Gamon et al. 2002), it has been adapted to French (Smets et al. 2003). Amalgam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and"
2003.jeptalnrecital-long.23,W97-0908,1,0.823887,"task, such as determining the context for the insertion of the subordinate conjunctions que and si, to 1016 for the more difficult task of determining the label of a constituent. The ordering model stands apart from the others, with 4,536 branching nodes (for details see Ringger et al. (in preparation)). Smets, Gamon, Corston-Oliver and Ringger 2.2 Data and feature extraction The training data for all the models consist of a set of 100,000 sentences drawn from software manuals. The sentences are analyzed in the NLPWin system, which provides a syntactic and logical form analysis (Heidorn 2000; Gamon et al. 1997). Nodes in the logical form representation are linked to the corresponding syntactic nodes, allowing us to learn contexts for the mapping from the logical form representation to a surface syntax tree. The data is split 70/30 for training versus model parameter tuning. For each set of data we build decision trees at several different levels of granularity (by manipulating the prior probability of tree structures to favor simpler structures) and select the model with the maximal accuracy as determined on the parameter tuning set. We attempt to standardize as much as possible the set of features"
2003.jeptalnrecital-long.23,W02-2103,0,0.147526,"ations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the researcher can understand why the model makes the decisions it makes and can improve the model by adding relevant linguistic features. The researcher can thus leverage the richness of the ling"
2003.jeptalnrecital-long.23,W98-1426,0,0.428872,"algam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the"
2003.jeptalnrecital-long.23,P98-1116,0,0.0800119,"algam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the"
2003.jeptalnrecital-long.23,2003.jeptalnrecital-long.23,1,0.0512755,"entence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models. Keywords – Mots Clés Réalisation de phrase, génération automatique, arbres de décision, français. Sentence realization, generation, machine-learning, decision trees, French. 1 Introduction Amalgam is a multilingual sentence realization system. Developed originally for German (Corston-Oliver et al. 2002, Gamon et al. 2002), it has been adapted to French (Smets et al. 2003). Amalgam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizatio"
2003.jeptalnrecital-long.23,A00-2023,0,\N,Missing
2003.jeptalnrecital-long.23,C02-1036,1,\N,Missing
2003.jeptalnrecital-long.23,C98-1112,0,\N,Missing
2004.tmi-1.14,J93-2003,0,0.00584889,"A, USA {anthaue, arulm, bobmoore, chrisq, ringger}@microsoft.com Abstract We present a series of models for doing statistical machine translation based on labeled semantic dependency graphs. We describe how these models were employed to augment an existing example-based MT system, and present results showing that doing so led to a significant improvement in translation quality as measured by the BLEU metric. 1. Introduction Much research of late has been devoted to the invention and implementation of statistical machine translation (hereafter: SMT) systems of the kind originally described in (Brown et al., 1993). What these systems have in common is that they try to predict the most likely target language string given an input string in the source language using statistical methods. While traditional SMT systems do not explicitly model structural linguistic properties of the languages they are translating, recent trends in both SMT and speech recognition research suggest that it may be worthwhile to pay more attention to these properties when creating language models. Language models and SMT systems that take syntactic information into account are becoming more numerous. A representative sample inclu"
2004.tmi-1.14,P01-1017,0,0.0353472,"mmon is that they try to predict the most likely target language string given an input string in the source language using statistical methods. While traditional SMT systems do not explicitly model structural linguistic properties of the languages they are translating, recent trends in both SMT and speech recognition research suggest that it may be worthwhile to pay more attention to these properties when creating language models. Language models and SMT systems that take syntactic information into account are becoming more numerous. A representative sample includes the work of Chelba (1997), Charniak (2001), Knight and Yamada (2001), Charniak, Knight and Yamada (2003), and Eisner (2003). The intuition is that syntax-based models ought to be able to capture long-distance dependencies that surface-string models are unable to capture because events that depend on each other are often closer together in the syntax tree than they are in the surface string, in the sense that the distance to a sibling or common parent may be significantly shorter than distance between the same events in the surface string. The system described in this paper is an attempt to take one step further in the same direction:"
2004.tmi-1.14,2003.mtsummit-papers.6,0,0.0674115,"Missing"
2004.tmi-1.14,P98-1035,0,0.0205089,"sentences: 1. John hit the ball. 2. The balls were hit by Lucy. A surface-string-based 3-gram model would count the following n-grams, each n-gram getting count 1: Table 1: Surface string N-gram Counts <Pr&gt; <Pr&gt; John 1 <Pr&gt; John hit 1 John hit the 1 hit the ball 1 the ball <POST&gt; 1 <Pr&gt;<Pr&gt; The 1 <Pr&gt; The balls 1 the balls were 1 balls were hit 1 were hit by 1 hit by Lucy 1 By Lucy <Po&gt; 1 Note that each of these trigrams occurs only once, even though the event (the hitting of a ball) is the same in both cases. If we look at syntax trees, we get a slightly different picture: One could, as in (Chelba and Jelinek, 1998) build up a statistical grammar from a corpus of parse trees such as these. This grammar could then be used in a syntax-tree-based SMT system in order to assign a probability to target language syntax tree candidates. Note, however, that here, too, the hitting of the ball would be split into two separate buckets (one set of rules for the active voice construction and another for the passive voice), and so the system would fail to learn a useful generalization. The dependency graphs our system produces for the two sentences looks like this: The dependency graph target language model would gener"
2004.tmi-1.14,P03-2041,0,0.0296997,"put string in the source language using statistical methods. While traditional SMT systems do not explicitly model structural linguistic properties of the languages they are translating, recent trends in both SMT and speech recognition research suggest that it may be worthwhile to pay more attention to these properties when creating language models. Language models and SMT systems that take syntactic information into account are becoming more numerous. A representative sample includes the work of Chelba (1997), Charniak (2001), Knight and Yamada (2001), Charniak, Knight and Yamada (2003), and Eisner (2003). The intuition is that syntax-based models ought to be able to capture long-distance dependencies that surface-string models are unable to capture because events that depend on each other are often closer together in the syntax tree than they are in the surface string, in the sense that the distance to a sibling or common parent may be significantly shorter than distance between the same events in the surface string. The system described in this paper is an attempt to take one step further in the same direction: if syntactic models are good because they move events that depend on each other c"
2004.tmi-1.14,P03-1054,0,0.00431929,"probability of the target dependency graph τ by the following formula: |τ | PµT (τ ) = ∏ P(ci |ci −1...ci − ( n −1) , µT ) , (4) i where C are all of the nodes in τ, ci-1 denotes the parent of ci, and n is the order of the model. The model is pruned by removing infrequently-occurring n-grams and smoothed using interpolated absolute discounting. Consider the following dependency graph: The probability of the graph according to a trigram DG model would be: P(A |ROOT) * P(R1 |ROOT A) * P (R2 |ROOT A) * P (B |A R1) * P(C |A R2) * P(D |A R2) * P (LEAF |R1 B) * P(LEAF | R2 C) * P(LEAF |R2 D) 1 See Klein and Manning, 2003 for further discussion of vertical Markovization. It would have been nice to include horizontal (i.e. sibling) information in our model as well, but the top-down nature of our decoder (and the performance-motivated incorporation of the Markov assumption therein) made this impractical. Treating semantic relationships (in this example: R1 and R2) as first-class entities simplifies the creation of the model a great deal, since it means we don’t have to train and store two separate models for lemmas and semantic relationships. To see why dependency graph-based models might model certain phenomena"
2004.tmi-1.14,J99-4005,0,0.0744632,"Missing"
2004.tmi-1.14,W01-1411,0,0.0120893,"tion of the decoding algorithm, “total” is the total number of constituents in ms, and Pu is the unnormalized probability.2 Since we are not guaranteed to have a compatible set of mappings covering any given input, we also add default nodes of size 1 for each input lemma and relation not already covered by a single-node mapping from the training data. For relations, the default node is simply a copy of the input relation. For lemmas, we create a new node by translating the input lemma using the most likely single-word translation according to the LLR-based word association model described in (Moore, 2001). Then we compute word translation probabilities over the corpus using IBM Model 1 and assign the resulting probability to the chosen mapping. Since the IBM Model 1 probabilities are obtained by a completely different process than the dependency graph mappings, we have found it advantageous to train up a weight, λL, by which the Model 1 probabilities are multiplied before they are combined with the other channel model probabilities. In addition there is another free parameter, λA, which is the default value assigned to the single-node semantic relationship mappings. Presently, the handling of"
2004.tmi-1.14,P02-1038,0,0.0552518,"source language sentence and T the target language sentence. By Bayes’ rule, T = arg max( P (T |S )) = arg max( P ( S |T ) P(T )). (2) A target language model trained on monolingual target language data is used to compute an estimate of P(T), and channel models of varying complexity are built to compute and estimate P(S|T). In our system, individual candidate mappings and combinations of mappings are scored using a linearly interpolated combination of scores from several heterogeneous information sources. This “kitchen sink” approach to SMT is superficially similar to the system described in (Och and Ney, 2002), except that it works with dependency graph mappings instead of with surface strings. Formally, then, we are looking for the set of transfer mappings Tmax out of all sets of transfer mappings T such that: Μ Tmax = arg max{∑ λµ Scorem (T )} , (3) µ =1 where µ ε Μ are the individual models, each of which assigns a score to a set of transfer mappings. The sources consulted for our system include a probabilistic channel model, target language model, and simple fertility model. Additional information sources whose scores are interpolated with the traditional SMT models include mapping size and num"
2004.tmi-1.14,2001.mtsummit-papers.68,0,0.0153675,"pings Tmax out of all sets of transfer mappings T such that: Μ Tmax = arg max{∑ λµ Scorem (T )} , (3) µ =1 where µ ε Μ are the individual models, each of which assigns a score to a set of transfer mappings. The sources consulted for our system include a probabilistic channel model, target language model, and simple fertility model. Additional information sources whose scores are interpolated with the traditional SMT models include mapping size and number of binary features matched. We train weights for the models using Powell’s algorithm to maximize the BLEU score on the output of the system (Papineni et al., 2001). While the general idea behind these approaches is not new, the innovation in both cases is to apply these techniques at the labeled dependency graph level rather than at the string level. We know of no other system to date that has used statistical techniques to maximize the probability of labeled dependency-graph-to-dependency-graph mappings for machine translation. 3. Models 3.1. Target language model Most SMT systems use surface string n-gram models for their target language model. This has a number of advantages, particularly in a string-to-string translation system. Perhaps most importa"
2004.tmi-1.14,2001.mtsummit-papers.53,1,0.894706,"to bring related events into nearer proximity by explicitly representing semantic dependencies still latent in the syntactic representation. In this paper, we show how our existing example-based MT system using labeled semantic dependency graph translation mappings benefited from the application of SMT techniques while still preserving the benefits provided by rich hierarchical linguistic information. 2. Description of the baseline system and the statistical framework 2.1. The baseline system For an overview of the example-based system to which we’ve applied these statistical techniques, see (Richardson et al, 2001). In a nutshell, the system parses aligned source and target language training sentences using a bottom-up, multi-path chart parsing algorithm, aligns the resulting dependency graphs and creates a set of mappings from source language dependency graph fragments to target language dependency graph fragments (hereafter referred to as DG mappings). The nodes in a given dependency graph represent the lemmas of all concept words in the corresponding sentence. A pair of nodes may be connected by a directed edge labeled by a semantic or deep-syntactic dependency relationship. Associated with each node"
2004.tmi-1.14,P01-1067,0,0.190481,"Missing"
2004.tmi-1.14,P02-1040,0,\N,Missing
2004.tmi-1.14,C98-1035,0,\N,Missing
black-etal-2014-evaluating,carmen-etal-2010-tag,1,\N,Missing
black-etal-2014-evaluating,D10-1079,1,\N,Missing
black-etal-2014-evaluating,N10-1103,0,\N,Missing
black-etal-2014-evaluating,chrupala-etal-2008-learning,0,\N,Missing
black-etal-2014-evaluating,ringger-etal-2008-assessing,1,\N,Missing
black-etal-2014-evaluating,W02-1001,0,\N,Missing
black-etal-2014-evaluating,J00-1006,0,\N,Missing
black-etal-2014-evaluating,P05-1071,0,\N,Missing
black-etal-2014-evaluating,N07-1047,0,\N,Missing
black-etal-2014-evaluating,W09-3504,0,\N,Missing
black-etal-2014-evaluating,P08-1103,0,\N,Missing
C02-1036,W02-2105,1,0.652431,"Missing"
C04-1097,W01-0808,0,0.0216418,"German and find that a particular conditional model outperforms all others. We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 1 Introduction Word and constituent order play a crucial role in establishing the fluency and intelligibility of a sentence. In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, Aikawa et al. (2001) and Reiter and Dale (2000)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitro"
C04-1097,C00-1007,0,0.177575,"Missing"
C04-1097,A00-2018,0,0.0602743,"stems operating on similar input, and (2) to measure Amalgam’s capabilities on less domain-specific data than technical software manuals. We derive from the bracketed tree structures in the PTB using a deterministic procedure an abstract representation we refer to as a Dependency Structure Input Format (DSIF), which is only loosely related to NLPWin’s abstract predicateargument structures. The PTB to DSIF transformation pipeline includes the following stages, inspired by Langkilde-Geary’s (2002b) description: A. Deserialize the tree B. Label heads, according to Charniak’s head labeling rules (Charniak, 2000) C. Remove empty nodes and flatten any remaining empty non-terminals D. Relabel heads to conform more closely to the head conventions of NLPWin E. Label with logical roles, inferred from PTB functional roles F. Flatten to maximal projections of heads (MPH), except in the case of conjunctions G. Flatten non-branching non-terminals H. Perform dictionary look-up and morphological analysis I. Introduce structure for material between paired delimiters and for any coordination not already represented in the PTB J. Remove punctuation K. Remove function words L. Map the head of each maximal projection"
C04-1097,W02-2105,1,0.738121,"n system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a, 2002b; Smets et al., 2003). Amalgam has an explicit ordering stage that determines the order of constituents and their daughters. The input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or a ranked list of such trees. For ordering, Amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context. By separately establishing order within constituents, Amalgam heavily constrains the possible alternatives in later stages of the realization process. The design allows"
C04-1097,W02-2102,0,0.0284067,"Missing"
C04-1097,P02-1004,1,0.834978,"ngkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a, 2002b; Smets et al., 2003). Amalgam has an explicit ordering stage that determines the order of constituents and their daughters. The input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or a ranked list of such trees. For ordering, Amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context. By separately establishing order within constituents, Amalgam heavily constrains the possible alternatives in later stages of the realization process. The design allows for interaction betw"
C04-1097,P03-1054,0,0.0343259,"Missing"
C04-1097,A00-2023,0,0.0212868,"0)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 20"
C04-1097,W02-2103,0,0.294206,"the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a"
C04-1097,W98-1426,0,0.0250888,"e employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 1 Introduction Word and constituent order play a crucial role in establishing the fluency and intelligibility of a sentence. In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, Aikawa et al. (2001) and Reiter and Dale (2000)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word"
C04-1097,P98-1116,0,0.095937,"e employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 1 Introduction Word and constituent order play a crucial role in establishing the fluency and intelligibility of a sentence. In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, Aikawa et al. (2001) and Reiter and Dale (2000)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word"
C04-1097,2001.mtsummit-papers.68,0,0.0144316,"ultiple random scramblings and average the results. We use the evaluation metrics employed in published evaluations of HALogen, FUF/SURGE, and FERGUS (e.g., Calloway, 2003), although our results are for ordering only. Coverage, or the percentage of inputs for which a system can produce a corresponding output, is uninformative for the Amalgam system, since in all cases, it can generate an output for any given DSIF. In addition to processing time per input, we apply four other metrics: exact match, NIST simple string accuracy (the complement of the familiar word error rate), the IBM Bleu score (Papineni et al., 2001), and the intra-constituent edit distance metric introduced earlier. We evaluate against ideal trees, directly computed from PTB bracketed tree structures. The results in Table 2 show the effects of varying the IOCC parameter. For both trials involving a greedy search, the results were averaged across 25 iterations. As should be expected, turning on the input-output faithfulness option (IOCC) improves the performance of the greedy search. Keeping coordinated material in the same relative order would only be called for in applications that plan discourse structure before or during generation. 7"
C04-1097,2003.jeptalnrecital-long.23,1,0.608048,"uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a, 2002b; Smets et al., 2003). Amalgam has an explicit ordering stage that determines the order of constituents and their daughters. The input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or a ranked list of such trees. For ordering, Amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context. By separately establishing order within constituents, Amalgam heavily constrains the possible alternatives in later stages of the realization process. The design allows for interaction between ordering choices and othe"
C04-1097,P02-1040,0,\N,Missing
C04-1097,C98-1112,0,\N,Missing
C16-1168,J81-4005,0,0.539261,"Missing"
C16-1168,P15-1077,0,0.0148275,"ctions. This hypothesis is plausible a priori because using data embeddings is akin to using semi-supervision to enable faster learning. The reason for this is that data embeddings are traditionally induced in an unsupervised manner on extremely large corpora before being applied to a downstream supervised task. In addition, operating on dense, low-dimensional vector data reduces the number of model parameters which can also reduce the number of instances required to learn effectively. Although it might be possible to extend the CS LDA model to generatively model the embedding as described by Das et al. (2015), but it is unclear how the inference approach used there (Gibbs sampling based on Cholesky decompositions) would be efficiently applied in the context of the CS LDA model, thus we leave this possibility to future work and focus on using embeddings discriminatively. We use the 1789 Dataset Sentiment Weather Compatibility Paraphrase Size 1,000 1,000 17,977 4,000 Unique Annotators 83 102 411 119 Annotations per Instance Classes 5 20 10 5 2 5 2 2 Average Doc size 12.8 13.6 2×1 2 × 11.2 Gold Labels 1,000 724 15,157 838 Timestamps No Yes No Yes Table 1: Dataset statistics. Evaluation metrics are ca"
C16-1168,K15-1020,1,0.952165,"012), and Yan et al. (2014) show that crowdsourcing annotation models can be enhanced by conditioning the model on the document data (e.g., word content), improving the model by identifying annotators whose judgments tend to agree with word patterns found in the documents being annotated. Recent work has shown that generative data models allow crowdsourcing models to converge to useful estimates even when few annotations are available, whereas by the time a conditional model has enough information (in the form of annotations) to be useful, the problem is often largely solved by majority vote (Felt et al., 2015b). However, although generative data modeling has been shown to be effective in categorizing text according to its topic, realistic generative data models are not always available. In this paper, we use advances in text representation to demonstrate that data-conditional annotation models can This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ ∗ This work was completed while the first and second authors were at Brigham Young University. 1 http://mturk.com 2 http://crowdflower.com 1787 Proceedings o"
C16-1168,N15-1089,1,0.90844,"012), and Yan et al. (2014) show that crowdsourcing annotation models can be enhanced by conditioning the model on the document data (e.g., word content), improving the model by identifying annotators whose judgments tend to agree with word patterns found in the documents being annotated. Recent work has shown that generative data models allow crowdsourcing models to converge to useful estimates even when few annotations are available, whereas by the time a conditional model has enough information (in the form of annotations) to be useful, the problem is often largely solved by majority vote (Felt et al., 2015b). However, although generative data modeling has been shown to be effective in categorizing text according to its topic, realistic generative data models are not always available. In this paper, we use advances in text representation to demonstrate that data-conditional annotation models can This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ ∗ This work was completed while the first and second authors were at Brigham Young University. 1 http://mturk.com 2 http://crowdflower.com 1787 Proceedings o"
C16-1168,N15-1004,0,0.0123174,"r parts (e.g., collocations like “White House”). These can be dealt with by using heuristics to identify and combine token phrases (Mikolov et al., 2013b). Other approaches incorporate composition functions as first-class constituents of the objective function itself. Mitchell and Lapata (2010) motivate a general composition framework and compare a number of simple instantiations, including additive, multiplicative, and tensor product combination. Socher et al. (2012) assign vectors representing semantic content and matrices representing semantic transformations to every node in a parse tree. Fyshe et al. (2015) focus on learning phrasal representations whose dimensions are easily interpretable by humans, similar to successful models whose topics are easy for humans to recognize and name because they align with a topic distinction known a priori to the human. In this work we focus on using instance data to improve probabilistic crowdsourcing models. Passonneau and Carpenter (2014) argue that probabilistic crowdsourcing models are generally more effective and reliable than traditional chance-adjusted agreement heuristics such as Krippendorff’s alpha for assessing corpus quality (Krippendorff, 2012). O"
C16-1168,D12-1039,0,0.0203289,"For training, we use hierarchical sampling with a skip-gram model and no negative sampling. Embeddings of size 300 are learned. All of these settings are rather standard for a large corpus like Wikipedia. 3.1 Datasets In order to calculate the accuracy of inferred labels, we require datasets that have both crowdsourced annotations as well as gold standard labels for evaluation. We identify four suitable datasets, briefly describing both their annotation task as well as the way their gold standard labels are constructed. For all Twitter data, we use the Twitter text normalization dictionary of Han et al. (2012) to normalize tweets before embedding them. Note that for two of the datasets described below, Compatibility and Weather, the gold standard does not consist of the hand labels of an expert, but rather is constructed from the consensus vote of a reasonable number of crowd workers. A fundamental tenet of crowdsourcing is that 1790 TWITTER_SENT WEATHER 0.80 1.0 algorithm algorithm 0.75 LogResp+w2v LogResp 0.8 csLDA MomResp 0.7 Accuracy Accuracy 0.9 ItemResp LogResp+w2v LogResp csLDA 0.70 MomResp ItemResp 0.65 Majority Majority 0.6 0.60 0 5 10 15 0 20 1 2 3 4 5 Number of annotations x 1,000 Number"
C16-1168,N13-1132,0,0.0860379,"known a priori to the human. In this work we focus on using instance data to improve probabilistic crowdsourcing models. Passonneau and Carpenter (2014) argue that probabilistic crowdsourcing models are generally more effective and reliable than traditional chance-adjusted agreement heuristics such as Krippendorff’s alpha for assessing corpus quality (Krippendorff, 2012). Other previous work in crowdsourcing ignores the data being annotated, focusing instead on modeling other aspects of the annotation process, such as item difficulty and noise (Whitehill et al., 2009; Welinder et al., 2010). Hovy et al. (2013) model the nonlinear nature of human reliability by adding binary variables to each annotator indicating whether they are a spammer or not. These extensions are orthogonal to the issue explored by this paper and could be incorporated into any of the models used here. 6 Conclusions and Future Work Previous work indicates that generative crowdsourcing models enjoy significant learning advantages when aggregating topic-based document labels. Unfortunately, some text classification tasks make distinctions for which no good generative text models currently exist, such as labeling the similarity or"
C16-1168,N13-1062,0,0.0596686,"ing training data for supervised and semi-supervised machine learning algorithms in NLP. Modern text annotation is often crowdsourced, meaning that the work is divided up and assigned to internet workers on micro-task marketplaces such as Amazon’s Mechanical Turk1 or CrowdFlower.2 Although crowdsourced annotations tend to be error-prone, high quality labels may be obtained by aggregating multiple redundant low-quality annotations (Surowiecki, 2005). For many tasks, aggregated crowdsourced judgments have been shown to be more reliable than expert judgments (Snow et al., 2008; Cao et al., 2010; Jurgens, 2013). Traditionally annotations were aggregated via majority vote. More sophisticated approaches jointly model annotator reliability and document labels. These models can down-weight the annotations of workers who often disagree with others and up-weight the annotations of workers who often agree with others. However, when annotation error is high or few annotations are available it can be difficult for these models to know which annotators to trust. Jin and Ghahramani (2002), Raykar et al. (2010), Liu et al. (2012), and Yan et al. (2014) show that crowdsourcing annotation models can be enhanced b"
C16-1168,N15-1097,0,0.0175076,"information and are labeled as paraphrases of one another, while the tweet “and of course because I drink and like Star Wars I know nothing about football” communicates different information, and is labeled as not a paraphrase of the other two tweets. Each tweet pair received 5 binary annotations. Gold standard labels were constructed for a subset of 838 tweet pairs by experts who rated each pair on a scale from 0-5. Following the original authors, expert ratings of 0-2 are labeled no paraphrase, and 4-5 are labeled paraphrase. Ratings of 3 are ignored for evaluation purposes. Compatibility. Kruszewski and Baroni (2015) paid CrowdFlower workers to rate word pairs according to their semantic compatibility, meaning that the two words can be used to refer to the same real-world entity.3 For example, the words “artist” and “teacher” are compatible with one another, whereas “bread” and “rattlesnake” are not. Each word pair was rated by 10 different annotators on a 7-point scale. Following the original authors, the gold standard is constructed by labeling items with a mean rating less than 1.6 as incompatible, and those with a mean rating greater than 3.7 as compatible. Ratings between 1.7 and 3.7 are ignored for"
C16-1168,N15-1144,0,0.0228553,"rse” and “equine”) are represented as entirely orthogonal dimensions, exploding the number of parameters needed by downstream learning algorithms. Recently, methods have been developed to represent words as locations in low-dimensional vector spaces where distance and direction encode semantic and syntactic meaning (Mikolov et al., 2013a; Pennington et al., 2014). These embedding vectors have been shown to improve a variety of language tasks including named entity recognition, phrase chunking (Turian et al., 2010), relation extraction (Nguyen and Grishman, 2014), and part of speech induction (Lin et al., 2015). The hypothesis investigated by the current work is that semantic, vector-based text representations can help conditional annotation aggregation models achieve some of the same early performance advantage seen in their generative counterparts, as well as help them operate on datasets that make semantic distinctions. This hypothesis is plausible a priori because using data embeddings is akin to using semi-supervision to enable faster learning. The reason for this is that data embeddings are traditionally induced in an unsupervised manner on extremely large corpora before being applied to a dow"
C16-1168,D14-1113,0,0.0221752,"rojects where effort is made up-front to iteratively refine an annotation specification before paying for large number of annotations. 5 Additional Related Work A sizable body of research is currently underway to improve vector word representations. Although most commonly word embeddings are trained in an unsupervised manner, they may be tuned to maximize performance on a particular target task (Le and Mikolov, 2014). They may also be supervised by multiple tasks simultaneously (Collobert et al., 2011). Others fit one embedding per word sense rather than per lexical type, improving model fit (Neelakantan et al., 2014). Srikumar and Manning (2014) embed not only word types, but also label types, modeling the fact that some labels are more similar than others. Another line of work explores ways of embedding larger spans of text. Although words tend to compose surprisingly well simply via linear combination, many phrases are more than the sum of their parts (e.g., collocations like “White House”). These can be dealt with by using heuristics to identify and combine token phrases (Mikolov et al., 2013b). Other approaches incorporate composition functions as first-class constituents of the objective function its"
C16-1168,P14-2012,0,0.0135139,"s that strongly relate to one another (e.g., the words “horse” and “equine”) are represented as entirely orthogonal dimensions, exploding the number of parameters needed by downstream learning algorithms. Recently, methods have been developed to represent words as locations in low-dimensional vector spaces where distance and direction encode semantic and syntactic meaning (Mikolov et al., 2013a; Pennington et al., 2014). These embedding vectors have been shown to improve a variety of language tasks including named entity recognition, phrase chunking (Turian et al., 2010), relation extraction (Nguyen and Grishman, 2014), and part of speech induction (Lin et al., 2015). The hypothesis investigated by the current work is that semantic, vector-based text representations can help conditional annotation aggregation models achieve some of the same early performance advantage seen in their generative counterparts, as well as help them operate on datasets that make semantic distinctions. This hypothesis is plausible a priori because using data embeddings is akin to using semi-supervision to enable faster learning. The reason for this is that data embeddings are traditionally induced in an unsupervised manner on extr"
C16-1168,Q14-1025,0,0.0193474,"ple instantiations, including additive, multiplicative, and tensor product combination. Socher et al. (2012) assign vectors representing semantic content and matrices representing semantic transformations to every node in a parse tree. Fyshe et al. (2015) focus on learning phrasal representations whose dimensions are easily interpretable by humans, similar to successful models whose topics are easy for humans to recognize and name because they align with a topic distinction known a priori to the human. In this work we focus on using instance data to improve probabilistic crowdsourcing models. Passonneau and Carpenter (2014) argue that probabilistic crowdsourcing models are generally more effective and reliable than traditional chance-adjusted agreement heuristics such as Krippendorff’s alpha for assessing corpus quality (Krippendorff, 2012). Other previous work in crowdsourcing ignores the data being annotated, focusing instead on modeling other aspects of the annotation process, such as item difficulty and noise (Whitehill et al., 2009; Welinder et al., 2010). Hovy et al. (2013) model the nonlinear nature of human reliability by adding binary variables to each annotator indicating whether they are a spammer or"
C16-1168,D14-1162,0,0.0769672,"er of drawbacks. They define a space that is often so high-dimensional and sparse that inter-document distances and other vector computations have little meaning. In word-count representations, features that strongly relate to one another (e.g., the words “horse” and “equine”) are represented as entirely orthogonal dimensions, exploding the number of parameters needed by downstream learning algorithms. Recently, methods have been developed to represent words as locations in low-dimensional vector spaces where distance and direction encode semantic and syntactic meaning (Mikolov et al., 2013a; Pennington et al., 2014). These embedding vectors have been shown to improve a variety of language tasks including named entity recognition, phrase chunking (Turian et al., 2010), relation extraction (Nguyen and Grishman, 2014), and part of speech induction (Lin et al., 2015). The hypothesis investigated by the current work is that semantic, vector-based text representations can help conditional annotation aggregation models achieve some of the same early performance advantage seen in their generative counterparts, as well as help them operate on datasets that make semantic distinctions. This hypothesis is plausible"
C16-1168,D08-1027,0,0.402717,"Missing"
C16-1168,D12-1110,0,0.00954716,"ing larger spans of text. Although words tend to compose surprisingly well simply via linear combination, many phrases are more than the sum of their parts (e.g., collocations like “White House”). These can be dealt with by using heuristics to identify and combine token phrases (Mikolov et al., 2013b). Other approaches incorporate composition functions as first-class constituents of the objective function itself. Mitchell and Lapata (2010) motivate a general composition framework and compare a number of simple instantiations, including additive, multiplicative, and tensor product combination. Socher et al. (2012) assign vectors representing semantic content and matrices representing semantic transformations to every node in a parse tree. Fyshe et al. (2015) focus on learning phrasal representations whose dimensions are easily interpretable by humans, similar to successful models whose topics are easy for humans to recognize and name because they align with a topic distinction known a priori to the human. In this work we focus on using instance data to improve probabilistic crowdsourcing models. Passonneau and Carpenter (2014) argue that probabilistic crowdsourcing models are generally more effective a"
C16-1168,P10-1040,0,0.0193267,"ing. In word-count representations, features that strongly relate to one another (e.g., the words “horse” and “equine”) are represented as entirely orthogonal dimensions, exploding the number of parameters needed by downstream learning algorithms. Recently, methods have been developed to represent words as locations in low-dimensional vector spaces where distance and direction encode semantic and syntactic meaning (Mikolov et al., 2013a; Pennington et al., 2014). These embedding vectors have been shown to improve a variety of language tasks including named entity recognition, phrase chunking (Turian et al., 2010), relation extraction (Nguyen and Grishman, 2014), and part of speech induction (Lin et al., 2015). The hypothesis investigated by the current work is that semantic, vector-based text representations can help conditional annotation aggregation models achieve some of the same early performance advantage seen in their generative counterparts, as well as help them operate on datasets that make semantic distinctions. This hypothesis is plausible a priori because using data embeddings is akin to using semi-supervision to enable faster learning. The reason for this is that data embeddings are tradit"
C16-1282,felt-etal-2014-momresp,1,0.931212,"xt of three very different probabilistic models of text which can be used interactively. We first show that ICM performs well in the context of a non-parametric model by experimenting with a Dirichlet Process Mixture of Multinomials applied to the problem of web search result clustering. We then turn our attention to the Interactive Topic Model (Hu et al., 2011) to show that ICM improves performance over the previously published Gibbs sampler. Finally, we use ICM in the context of M OM R ESP, a probabilistic model designed to infer true document class labels from noisy crowdsourced judgments (Felt et al., 2014). 2 Iterated Conditional Modes Suppose we are given a probabilistic model of text with observed data x and unobserved variables θ. For the purpose of this discussion, θ may represent any number of unobserved parameters and latent variables. These parameters and variables can be either continuous or discrete. Like Gibbs sampling, Iterated Conditional Modes (ICM) relies on the fact that while computing a posterior distribution of the form p(θ|x) may be intractable, computing the complete conditional for a single variable θi while holding fixed both x and the rest of the parameters θ¬i is feasibl"
C16-1282,P11-1026,0,0.281747,"topics from a set of documents, giving users a glimpse into the common themes of the data (Blei et al., 2003). Other models such as the Mixture of Multinomials can be used to perform document clustering allowing users to automatically organize text data (Meila and Heckerman, 2001; Walker and Ringger, 2008). We are interested in use cases for probabilistic models of text which include human interaction. For example, the Interactive Topic Model (ITM) is a topic model that extends LDA to allow the user to inject model constraints in the form of word groupings while the topics are being learned (Hu et al., 2011). By including the user in the training process rather than simply learning the topics offline, the user can finetune the resulting topic model to better suit individual user needs and to accommodate a user’s domain knowledge. However, if the training algorithm is too slow, the delay between receiving user feedback and presenting the updated model will harm the interaction due to increased cognitive load. Consequently, we require an inference algorithm which is both fast enough to facilitate interaction, and maintains (or improves upon) the accuracy of existing inference techniques. For models"
C18-1144,K15-1020,1,0.628583,"ncodes a specific instance indexed by i, annotation value c0 , and label value c. Finally, let noise parameters j for annotator j be tied to a confusion matrix with Dirichlet-distributed rows such that jk selects the value at cell (c, c0 ), encoding how likely j is to produce annotation c0 when shown a document whose true label is c. These settings recover the traditional item-response crowdsourcing model (Dawid and Skene, 1979). Using the same settings but defining p(y |x, θ) ∝ exp[θT f (x, y)], recovers a popular data-conditional crowdsourcing model (Raykar et al., 2010; Yan et al., 2014; Felt et al., 2015b). However, existing crowdsourcing models lack the representational richness of the measurements framework; we address this lacuna in the next section. 4 Per-annotator Normal Measurement Model for Classification Having shown how the annotator measurements framework can capture existing crowdsourcing models, this section presents a novel crowdsourcing model that instantiates the richness of the measurements framework that we use in Sections 5–6. For brevity, we refer to this model as PAN (per-annotator normal) measurement model.2 The generative story is: 1. Draw a stochastic vector θ over C cl"
C18-1144,N15-1089,1,0.778119,"ncodes a specific instance indexed by i, annotation value c0 , and label value c. Finally, let noise parameters j for annotator j be tied to a confusion matrix with Dirichlet-distributed rows such that jk selects the value at cell (c, c0 ), encoding how likely j is to produce annotation c0 when shown a document whose true label is c. These settings recover the traditional item-response crowdsourcing model (Dawid and Skene, 1979). Using the same settings but defining p(y |x, θ) ∝ exp[θT f (x, y)], recovers a popular data-conditional crowdsourcing model (Raykar et al., 2010; Yan et al., 2014; Felt et al., 2015b). However, existing crowdsourcing models lack the representational richness of the measurements framework; we address this lacuna in the next section. 4 Per-annotator Normal Measurement Model for Classification Having shown how the annotator measurements framework can capture existing crowdsourcing models, this section presents a novel crowdsourcing model that instantiates the richness of the measurements framework that we use in Sections 5–6. For brevity, we refer to this model as PAN (per-annotator normal) measurement model.2 The generative story is: 1. Draw a stochastic vector θ over C cl"
C18-1144,W10-0105,1,0.779298,"label accuracy and p∗ (x) is the empirical distribution. In reality the true values y are unobservable, but we can expect over them using our posterior approximation q˜; thus Rq˜ = Ep∗(x) maxyˆ Eq˜(y) [r(y, yˆ)] . With a label accuracy reward function the expected reward simpliP P (y) fies to Rq˜ = ˆ) = i maxyˆ νiˆ i maxyˆ q(yi = y y . For simplicity, we set the cost function Cq˜ to a constant, but leave it in the equations since future work should estimate and use annotation cost. By default, Algorithm 1 jointly selects an annotator j and measurement k, but it could be used in other ways. Haertel et al. (2010) argue that in realistic scenarios the active learning algorithm typically cannot control when annotators are available but rather must respond to annotator requests for work. Algorithm 1 1701 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: function M EASUREMENT S ELECTION(τ0 ) q0 ← Inference(τ0 ) while more measurements are desired do j, k ← NextMeasurement(τ0 , q0 ) τ0 ← τ0 ∪ ObserveMeasurement(j, k) q0 ← Inference(τ0 ) return q function N EXT M EASUREMENT(τ0 , q0 ) for annotator j do for measurement feature k do draw t samples from p(τjk |τ0 , q0 ) for sampled τjkt do τ1 ← τjkt ∪ τ0"
C18-1144,N13-1132,0,0.0456522,"nchev et al., 2010). Ganchev et al. (2010) explain each of these three frameworks can be derived as a special case of the learning from measurements framework of Liang et al. (2009) by making particular approximations for the sake of tractability. Traditional corpus construction assesses inferred label quality using annotator agreement heuristics such as Krippendorff’s alpha (Krippendorff, 2012). Passonneau and Carpenter (2014) argue that inference in probabilistic models yields higher quality labels at lower cost, and should be preferred over agreement heuristics. Among crowdsourcing models, Hovy et al. (2013) include Bernoulli switching variables to identify and eliminate malicious contributors (spammers). Raykar and Yu (2012) iteratively 1702 run inference and exclude problematic annotators in order to eliminate spammers. Raykar et al. (2010), Yan et al. (2014), and Felt et al. (2015a) model data jointly with labels, allowing patterns in the data to inform inferred labels. Simpson and Roberts (2015) model annotator dynamics, tracking the ways that annotator decision making changes over time in response to factors such as training and fatigue. Welinder et al. (2010) and Whitehill et al. (2009) bot"
C18-1144,N13-1062,0,0.0265065,"icht. Unser Modell, eine spezifische Instanz dieses Rahmens, schneidet im Vergleich zu fr¨uheren Arbeiten positiv ab. Dar¨uber hinaus erm¨oglicht es die aktive Stichprobenauswahl, indem Kommentator, Datenelement, und Annotationsstruktur gemeinsam ausgew¨ahlt werden, um den Annotationskosten zu reduzieren. 1 Introduction Supervised machine learning is data hungry: new approaches require massive training sets. These training sets can come from inexpensive crowdsourcing platforms, but consistency is often sacrificed for speed and thrift. Sophisticated models (Surowiecki, 2005; Snow et al., 2008; Jurgens, 2013) can overcome the intrinsic annotation noise by reconciling redundant annotation and predicting true labels by modeling the error patterns associated with individual labels, documents, or annotators. These models have typically assumed that we collect document-level labels and nothing else from annotators. But crowd workers could provide other valuable information. For example, if we wanted to predict the sentiment of documents about the weather (Figure 1), intuition says that words like “sunny”, This work is licensed under a Creative Commons Attribution 4.0 International License. License deta"
C18-1144,Q14-1025,0,0.0748319,"prior knowledge include constraint-driven learning based on integer linear programming (Chang et al., 2008), generalized expectation criteria (Druck et al., 2008), and the posterior regularization (Ganchev et al., 2010). Ganchev et al. (2010) explain each of these three frameworks can be derived as a special case of the learning from measurements framework of Liang et al. (2009) by making particular approximations for the sake of tractability. Traditional corpus construction assesses inferred label quality using annotator agreement heuristics such as Krippendorff’s alpha (Krippendorff, 2012). Passonneau and Carpenter (2014) argue that inference in probabilistic models yields higher quality labels at lower cost, and should be preferred over agreement heuristics. Among crowdsourcing models, Hovy et al. (2013) include Bernoulli switching variables to identify and eliminate malicious contributors (spammers). Raykar and Yu (2012) iteratively 1702 run inference and exclude problematic annotators in order to eliminate spammers. Raykar et al. (2010), Yan et al. (2014), and Felt et al. (2015a) model data jointly with labels, allowing patterns in the data to inform inferred labels. Simpson and Roberts (2015) model annotat"
C18-1144,D08-1027,0,0.235002,"Missing"
campbell-ringger-2004-converting,kingsbury-palmer-2002-treebank,0,\N,Missing
campbell-ringger-2004-converting,2001.mtsummit-papers.53,0,\N,Missing
campbell-ringger-2004-converting,A00-2018,0,\N,Missing
campbell-ringger-2004-converting,W01-1511,0,\N,Missing
campbell-ringger-2004-converting,C02-1043,1,\N,Missing
campbell-ringger-2004-converting,P03-1016,0,\N,Missing
campbell-ringger-2004-converting,P91-1021,0,\N,Missing
carmen-etal-2010-tag,ringger-etal-2008-assessing,1,\N,Missing
carmen-etal-2010-tag,N06-4006,0,\N,Missing
carmen-etal-2010-tag,N03-4009,0,\N,Missing
carmen-etal-2010-tag,W09-1905,0,\N,Missing
carmen-etal-2010-tag,W06-1705,0,\N,Missing
carmen-etal-2010-tag,W07-1516,1,\N,Missing
carmen-etal-2010-tag,P08-2017,1,\N,Missing
carmen-etal-2010-tag,felt-etal-2010-ccash,1,\N,Missing
carmen-etal-2010-tag,W06-1700,0,\N,Missing
D10-1024,D07-1043,0,0.029517,"Missing"
D10-1079,chrupala-etal-2008-learning,0,0.107551,"Missing"
D10-1079,J08-3005,0,0.0357316,"Missing"
D10-1079,N04-4038,0,0.102167,"Missing"
D10-1079,W06-1673,0,0.03301,"pproach. Finally, in Section 7 we briefly conclude and offer directions for future work. 2 The Syromorph Approach Table 1: The values for the morphological attributes of the stem , MLC, “king”. Attribute Gender Person Number Contraction Since lack language tools, we focus on automatically annotating Syriac text in a data-driven fashion based on the labeled data we have available. Since segmentation, linkage, and morphological tagging are not mutually independent tasks, we desire models for the sub-tasks to influence each other. To accommodate these requirements, we use a joint pipeline model (Finkel et al., 2006). In this section, we will first discuss this joint pipeline model, which we call syromorph. We then examine each of the individual sub-models. 2.1 Joint Pipeline Model Value masculine second plural normal suffix Table 2: The values for the morphological attributes of the suffix  ܢ, CON, “(masculine plural) your”. attributes for the suffix are gender, person, number, and contraction. The suffix contraction attribute encodes whether the suffix is normal or contracted, a phonological process involving the attachment of an enclitic pronoun to a participle. These morphological attributes were he"
D10-1079,P05-1071,0,0.179603,"Missing"
D10-1079,N07-2014,0,0.0204235,"(including the stem) can be spoken and written with vowels as diacritics; however, since the vowels are not written in common practice and since most text does not include them, this work omits any indication of vowels. Furthermore, the stem is an inflected baseform and does not necessarily form a word on its own. Also, the (unvocalized) stem and root are not necessarily identical. In Syriac, the same root , MLC is the foundation for other words such as promise, counsel, deliberate, reign, queen, kingdom, and realm. 1.2 Sub-tasks Segmentation, or tokenization as it is sometimes called (e.g., Habash and Rambow, 2007), is the process of dividing a word token into its prefix(es) (if any), a stem, and a suffix (if any). For Syriac, each 1 According to this transliteration all capital letters including A (ܐ, olaph) and O (ܘ, waw) are consonants. Additionally, the semi-colon (;), representing (ܝ, yod), is also a consonant. 811 word token consists of exactly one stem, from zero to three prefixes, and zero or one suffix. Each prefix is exactly one character in length. Segmentation does not include the process of parsing the stem for its inflectional morphology; that step is handled separately in subsequent"
D10-1079,N10-1076,1,0.917658,"s o stems. To further elaborate Figure 1: The syromorph model. Each rectangle is an input or output and each oval is a process employing a sub-model. on the example, since features are extracted from the local context, for stem tagging we extract features such as current stem, previous stem, current baseform, previous baseform, current root, previous root, current suffix tags, and previous suffix tags. (Here, “previous” refers to labels on the immediately preceding word token.) 2.2 Segmentation The syromorph segmentation model is a hybrid word- and consonant-level model, based on the model of Haertel et al. (2010) for data-driven diacritization. Each of our probabilistic sequence models is a maximum entropy Markov model (MEMM). Haertel et al. (2010) showed that the distribution over labels is different for known and words and rare words. In this work, we only consider words not seen in training (i.e., “unknown”) to be rare. Following Haertel et al.’s (2010) model, a separate model is trained for each word type seen in training with the intent of choosing the best segmentation given that word. This approach is closely related to the idea of ambiguity classes mentioned in Hajič and 813 Hladká (1998). To"
D10-1079,P98-1080,0,0.13437,"Missing"
D10-1079,J00-1006,0,0.486613,"Missing"
D10-1079,W04-3230,0,0.0814178,"Missing"
D10-1079,P03-1051,0,0.0711339,"Missing"
D10-1079,W07-0813,0,0.0390324,"Missing"
D10-1079,mohamed-kubler-2010-arabic,0,0.0387415,"Missing"
D10-1079,N10-1105,0,0.0256883,"Missing"
D10-1079,D07-1046,0,0.0430531,"Missing"
D10-1079,H05-1060,0,0.0383731,"Missing"
D10-1079,W00-1308,0,0.0698689,"nd roots) that have not been seen during training, but whose transformations have been seen. This model is trained on all words in order to capture as many transformations as possible. The second approach for unknown words, called hybrid-maxent, uses an MEMM trained on all words seen in training. Given a stem (respectively, baseform), this approach predicts only baseforms (respectively, roots) that were observed in training data. Thus, this method has a distinct disadvantage when it comes to predicting new forms. This approach corresponds directly to the approach to handling unknown -words by Toutanova and Manning (2000) for POS tagging. With regard to baseform and root linkage, we do not use the dictionary to constrain possible baseforms or roots, since we make no initial assumptions about the completeness of a dictionary. 2.4 Morphological Tagging For morphological tagging, we break the task into two separate tasks: tagging the suffix and tagging the stem. Since there are a number of values that need to be predicted, we define two ways to approach the problem. We call the first approach the monolithic approach, in which the label is the concatenation of all the morphological attribute values. Table 3 illust"
D10-1079,C04-1081,0,\N,Missing
D10-1079,C98-1077,0,\N,Missing
D10-1079,P03-1050,0,\N,Missing
E03-1006,C00-1007,0,0.014546,"ementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The p"
E03-1006,W02-2105,1,0.872705,"reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned approaches to sentence realization is that they can easily be ad"
E03-1006,P02-1004,1,0.793151,"that they can easily be adapted to new domains and ideally to new languages merely by retraining The architecture of Amalgam was intended to be languageindependent, although the system has previously only been applied to German sentence realization. Adapting this system to French allows us to assess which aspects of the system are truly language-independent and what must be added in order to account for French. The purpose of this paper is to focus on the adaptation of Amalgam to French. Discussions about the general architecture of the system can be found in Corston-Oliver et al. (2002) and Gamon et al. (2002b). 1 Overview of German Amalgam Amalgam takes as its input a logical form graph, i.e., a sentence-level dependency graph with fixed lexical choices for content words. This graph represents the predicate-argument structure of a sentence and includes semantic information concerning relations between nodes of the graph (Heidorn, 2002). Examples of French logical forms are given in section 3. Amalgam first degraphs the logical form into a tree and then augments it by the insertion of function words, 323 assignment of case and verb position features, syntactic labels, etc., to produce an unordered"
E03-1006,A00-2023,0,0.0234152,"o the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned"
E03-1006,W02-2103,0,0.0249065,"which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned approaches to sentence"
E03-1006,W98-1426,0,0.0224812,"dent as possible and was first implemented for German. We discuss the development of the French implementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree co"
E03-1006,P98-1116,0,0.0437571,"dent as possible and was first implemented for German. We discuss the development of the French implementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree co"
felt-etal-2010-ccash,carmen-etal-2010-tag,1,\N,Missing
felt-etal-2010-ccash,J93-2004,0,\N,Missing
felt-etal-2010-ccash,ringger-etal-2008-assessing,1,\N,Missing
felt-etal-2010-ccash,N06-4006,0,\N,Missing
felt-etal-2010-ccash,N03-4009,0,\N,Missing
felt-etal-2010-ccash,W07-1509,0,\N,Missing
felt-etal-2010-ccash,W07-1516,1,\N,Missing
felt-etal-2010-ccash,P08-2017,1,\N,Missing
felt-etal-2010-ccash,H01-1026,0,\N,Missing
felt-etal-2010-ccash,P00-1016,0,\N,Missing
felt-etal-2010-ccash,W07-1502,0,\N,Missing
felt-etal-2012-first,D10-1079,1,\N,Missing
felt-etal-2012-first,W04-3202,0,\N,Missing
felt-etal-2012-first,brants-plaehn-2000-interactive,0,\N,Missing
felt-etal-2012-first,J93-2004,0,\N,Missing
felt-etal-2012-first,W07-1509,0,\N,Missing
felt-etal-2012-first,P08-2017,1,\N,Missing
felt-etal-2012-first,felt-etal-2010-ccash,1,\N,Missing
felt-etal-2012-first,J09-4005,0,\N,Missing
felt-etal-2014-momresp,C10-1099,0,\N,Missing
felt-etal-2014-momresp,D08-1027,0,\N,Missing
felt-etal-2014-momresp,N13-1062,0,\N,Missing
felt-etal-2014-momresp,N13-1132,0,\N,Missing
felt-etal-2014-using,W04-2401,0,\N,Missing
felt-etal-2014-using,J93-2004,0,\N,Missing
felt-etal-2014-using,N03-1033,0,\N,Missing
felt-etal-2014-using,W07-1509,0,\N,Missing
felt-etal-2014-using,A88-1019,0,\N,Missing
felt-etal-2014-using,H01-1026,0,\N,Missing
felt-etal-2014-using,D09-1009,0,\N,Missing
felt-etal-2014-using,felt-etal-2012-first,1,\N,Missing
felt-etal-2014-using,P07-1033,0,\N,Missing
K15-1020,N13-1132,0,0.151338,"8a shows that CS LDA performs poorly on the CrowdFlower-annotated Newsgroups documents described at the beginning of Section 3 (not the synthetic annotations). Error analysis uncovers that CS LDA lumps related classes together in this dataset. This is because annotators could specify up to 3 simultaneous labels for each annotation, so that similar labels (e.g., “talk.politics.misc” and “talk.politics.mideast”) are usually chosen in blocks. Suppose each member of a set of documents with similar topical content is annotated 200 and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et a"
K15-1020,N13-1062,0,0.0800508,"quires labeled training corpora, historically produced by laborious and costly annotation projects. Microtask markets such as Amazon’s Mechanical Turk and Crowdflower have turned crowd labor into a commodity that can be purchased with relatively little overhead. However, crowdsourced judgments can suffer from high error rates. A common solution to this problem is to obtain multiple redundant human judgments, or annotations,1 relying on the observation that, in aggregate, non-experts often rival or exceed experts by averaging over individual error patterns (Surowiecki, 2005; Snow et al., 2008; Jurgens, 2013). A crowdsourcing model harnesses the wisdom of the crowd and infers labels based on the evidence of the available annotations, imperfect 2 Latent Representations that Reflect Labels and Confusion Most crowdsourcing models extend the itemresponse model of Dawid and Skene (1979). The Bayesian version of this model, referred to here as I TEM R ESP, is depicted in Figure 1. In the generative story for this model, a confusion matrix γj is drawn for each human annotator j. Each row γjc of the confusion matrix γj is drawn from 1 In this paper, we call human judgments annotations to distinguish them"
K15-1020,felt-etal-2014-momresp,1,0.775126,"e model to overweight feature evidence and underweight annotation evidence. This imbalance can result in degraded performance in the presence of high quality or many annotations. Leveraging Data Some extensions to I TEM R ESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to I TEM R ESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, features are drawn from class-conditional"
K15-1020,N15-1089,1,0.190077,"rweight annotation evidence. This imbalance can result in degraded performance in the presence of high quality or many annotations. Leveraging Data Some extensions to I TEM R ESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to I TEM R ESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, features are drawn from class-conditional distributions. Felt et al. (2015) identify the"
K15-1020,W13-2323,0,0.0404535,"Missing"
K15-1020,C10-1099,0,0.0240967,"lassconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). CFGROUPS1000 0.7 Accuracy algorithm csLDA 0.6 MomResp LogResp 0.5 ItemResp Majority 0.4 0 2 4 6 Number of annotated instances x 1,000 (a) Original data CFSIMPLEGROUPS 0.95 Accuracy algorithm 0.90 csLDA MomResp 0.85 LogResp ItemResp 0.80 Majority 0.75 0 2 4 6 Number of annotated instances x 1,000 (b) After combining frequently co-annotated label classes Figure 8: An illustrative failure case. CS LDA, lacking a class label prior, prefers to combine label classes that are highly co-annotated. with both label A and B. In this scenario it is apparent that CS LDA will achieve its best fit by infer"
K15-1020,D08-1027,0,0.847468,"Missing"
K15-1020,Q14-1025,0,\N,Missing
N06-1021,J93-2004,0,0.0328332,"Missing"
N06-1021,P05-1012,0,0.143593,"sidering the deployment of a new language for machine translation is whether the natural language components available are of sufficient quality to warrant the effort to integrate them into the machine translation system. It is not feasible in every instance to do the integration work first and then to evaluate the output. Table 1 summarizes the data used to train the parsers, giving the number of tokens (excluding traces and other empty elements) and counts of sentences.1 3 Parser Architecture We take as our starting point a re-implementation of McDonald’s state-of-the-art dependency parser (McDonald et al., 2005a). Given a sentence x, the goal of the parser is to find the highest-scoring parse yˆ among all possible parses y ∈ Y : yˆ = arg max s(x, y) y∈Y (1) 1 The files in each partition of the Chinese and Arabic data are given at http://research.microsoft.com/˜simonco/ HLTNAACL2006. Language Arabic Chinese Czech English Total Tokens 116,695 527,242 1,595,247 1,083,159 Training Sentences 2,100 14,735 73,088 39,832 Development Sentences 446 1,961 7,319 1,346 Blind Sentences 449 2,080 7,507 2,416 Table 1: Summary of data used to train parsers. For a given parse y, its score is the sum of the scores of"
N06-1021,P05-1034,0,0.0365991,"Missing"
N06-1021,W02-1001,0,0.556283,"arning by presenting the input samples in some sequential order. For large training set sizes, a batch learner may face computational difficulties since there already exists an exponential number of parses per input sentence. Online learning is more tractable since it works with one input at a time. A popular online learner is the perceptron. It adjusts w by updating it with the feature vector whenever a misclassification on the current input sample occurs. It has been shown that such updates converge in a finite number of iterations if the data is linearly separable. The averaged perceptron (Collins, 2002) is a variant which averages the w across all iterations; it has demonstrated good generalization especially with data that is not linearly separable, as in many natural language processing problems. 2 The Chu-Liu-Edmonds’ decoder, which is based on a maximal spanning tree algorithm, can run in O(N 2 ), but our simpler implementation of O(N 3 ) was sufficient. Recently, the good generalization properties of Support Vector Machines have prompted researchers to develop large margin methods for the online setting. Examples include the margin perceptron (Duda et al., 2001), ALMA (Gentile, 2001), a"
N06-1021,C96-1058,0,0.169288,"3) a feature representation f (i, j). Two decoders will be discussed here; the training algorithm and feature representation are discussed in the following sections. A good decoder should satisfy several properties: ideally, it should be able to search through all valid parses of a sentence and compute the parse scores efficiently. Efficiency is a significant issue since there are usually an exponential number of parses for any given sentence, and the discriminative training methods we will describe later require repeated decoding at each training iteration. We reimplemented Eisner’s decoder (Eisner, 1996), which searches among all projective parse trees, and the Chu-Liu-Edmonds’ decoder (Chu and Liu, 1965; Edmonds, 1967), which searches in the space of both projective and non-projective parses. (A projective tree is a parse with no crossing dependency links.) For the English and Chinese data, the headfinding rules for converting from Penn Treebank analyses to dependency analyses creates trees that are guaranteed to be projective, so Eisner’s algorithm suffices. For the Czech and Arabic corpora, a non-projective decoder is necessary. Both algorithms are O(N 3 ), where N is the number of words 1"
N06-1021,N03-1033,0,0.0160933,"Missing"
N06-1021,W05-1516,0,0.0335081,"r Chinese are the first published results for the dependency parsing of the Chinese Treebank 5.0.4 Since the Arabic and Chinese numbers are well short of the numbers for Czech and English, we attempted to determine what impact the smaller corpora used for training the Arabic and Chinese parsers might have. We performed data reduction experiments, training the parsers on five random samples at each size smaller than the entire training set. Figure 2 shows the dependency accuracy measured on the complete development test set when training with samples of the data. The graph shows the average 4 (Wang et al., 2005) report numbers for undirected dependencies on the Chinese Treebank 3.0. We cannot meaningfully compare those numbers to the numbers here. Language English (exc punc) Czech (inc punc) Algorithm Avg. Perceptron MIRA Bayes Point Machine Avg. Perceptron MIRA Bayes Point Machine DA 90.6 90.9 90.8 82.9 83.3 84.0 RA 94.0 94.2 93.7 88.0 88.6 88.8 CM 36.5 37.5 37.6 30.3 31.3 30.9 Table 3: Comparison to previous best published results reported in (McDonald et al., 2005a). Bayes Point Machine Best averaged perceptron Worst averaged perceptron Arabic 78.4 77.9 77.4 Chinese 83.8 83.1 82.6 Czech 84.5 83.5"
N06-1021,W03-3023,0,0.478632,"Missing"
N06-1021,2005.iwslt-1.12,0,\N,Missing
N10-1076,J96-1002,0,0.0120168,"such word. These diacritized words Di depend on previous Di−1 as per equation (1) for an order-1 CMM (note that the capitalized A, I, and O are in fact consonants in this transliteration). Because “NKTA” and “RGT” are rare, their diacritization is represented by a consonant-level CMM: one variable for each possible diacritic in the word. Importantly, these consonant-level models have access to the previously diacritized word (D4 and D6 , respectively). We use log-linear models for all local distributions in our CMMs, i.e., we use maximum entropy (maxent) Markov models (McCallum et al., 2000; Berger et al., 1996). Due to the phenomenon known as d-separation (Pearl and Shafer, 1988), it is possible to independently learn parameters for each word model ω ui by training only on those instances for the corresponding word. Similarly, the consonant model can be learned independent of the word models. We place a spherical normal prior centered at zero with a standard deviation of 1 over the weights of all models and use an L-BFGS minimizer to find the MAP estimate of the weights for all the models (words and consonant). γ ω AO γ ω DHBA ω LA ωCSIA C5,1 C5,2 C5,3 C5,4 C5,1 CSIA C5,2 AO  D1 D2 D3 D4 CSIA AO DH"
N10-1076,W06-1673,0,0.407474,"the word) for rare words, making exact decoding intractable. Instead, we apply a non-standard beam during decoding to limit the number of states for rare words to the n-best (locally). This is accomplished by using an independent “inner” n-best decoder for the consonant-level CMM to produce the n-best diacritizations for the rare word given the previous diacritized words and other features. These become the only states to and from which transitions in the “outer” word-level decoder can be made. We note this is the same type of decoding that is done in pipeline models that use n-best decoders (Finkel et al., 2006). Additionally, we use a traditional beamsearch of width 5 to further reduce the search space both in the outer and inner CMMs. 4 Data Although our primary interest is in the Syriac language, we also experimented with the Penn Arabic Treebank (Maamouri et al., 2004) for the sake of comparison with other approaches. We include Hebrew to provide results for yet another Semitic language. We also apply the models to English to show that our method and features work well outside of the Semitic languages. A summary of the datasets, including the number of diacritics, is found in Figure 2. The number"
N10-1076,W02-0504,0,0.0917345,"Missing"
N10-1076,N07-2014,0,0.652957,"-resource solution to data sparsity is to use consonant-based techniques for unknown words (Ananthakrishnan et al., 2005; Nelken and Shieber, 2005). Many of the existing systems, especially recent ones, are probabilistic or contain probabilistic components. Zitouni et al. (2006) show the superiority of their conditional-based approaches over the best-performing generative approaches. However, the instance-based learning approach of K¨ubler and Mohamed (2008) slightly outperforms Zitouni et al. (2006). In the published literature for Arabic, the latter two have the best low-resource solutions. Habash and Rambow (2007) is the state-of-the-art, high-resource solution for Arabic. To our knowledge, no work has been done in this area for Syriac. 3 Models In this work, we are concerned with diacritization for Syriac for which a POS tagger, segmenter, and other tools are not readily available, but for which diacritized text is obtainable.2 Use of a system dependent on a morphological analyzer such as Habash and Rambow’s (2007) is therefore not costeffective. Furthermore, we seek a system that is applicable to a wide variety of languages. Although K¨ubler and Mohamed’s (2008) approach is competitive to Zitouni et"
N10-1076,J00-1006,0,0.545321,"Missing"
N10-1076,W02-1002,0,0.0143515,", morphological disambiguation, and machine translation, making diacritization important to Natural Language Processing (NLP) systems and intelligence gathering. In alphabetic writing systems, similar techniques have been used to restore accents from plain text (Yarowsky, 1999) and could be used to recover missing letters in the compressed writing styles found in email, text, and instant messages. We are particularly interested in diacritizing Syriac, a low-resource dialect of Aramaic, which possesses properties similar to Arabic and Hebrew. This work employs conditional Markov models (CMMs) (Klein and Manning, 2002) to diacritize Semitic (and other) languages and requires only diacritized texts for training. Such an approach is useful for languages (like Syriac) in which annotated data and linguistic tools such as part-of-speech (POS) taggers, segmenters, and morphological analyzers are not available. Our main contributions are as follows: (1) we introduce a hybrid word and consonant CMM that allows access to the diacritized form of the previous words; (2) we introduce new features available in the proposed model; and (3) we describe an efficient, approximate decoder. Our models significantly outperform"
N10-1076,R09-1047,0,0.029359,"Missing"
N10-1076,W05-0711,0,0.0660281,"ow, 1 We refer to all graphemes present in undiacritized texts as consonants. 520 2007; Vergyri and Kirchoff, 2004). Word-based, low-resource solutions tend to treat the problem as word-level sequence labeling (e.g., Gal, 2002). Unfortunately, word-based techniques face problems due to data sparsity: not all words in the test set are seen during training. In contrast, consonant-based approaches rarely face the analogous problem of previously unseen consonants. Thus, one low-resource solution to data sparsity is to use consonant-based techniques for unknown words (Ananthakrishnan et al., 2005; Nelken and Shieber, 2005). Many of the existing systems, especially recent ones, are probabilistic or contain probabilistic components. Zitouni et al. (2006) show the superiority of their conditional-based approaches over the best-performing generative approaches. However, the instance-based learning approach of K¨ubler and Mohamed (2008) slightly outperforms Zitouni et al. (2006). In the published literature for Arabic, the latter two have the best low-resource solutions. Habash and Rambow (2007) is the state-of-the-art, high-resource solution for Arabic. To our knowledge, no work has been done in this area for Syria"
N10-1076,W04-1612,0,0.0597344,"Missing"
N10-1076,P06-1073,0,0.51781,"ource solutions tend to treat the problem as word-level sequence labeling (e.g., Gal, 2002). Unfortunately, word-based techniques face problems due to data sparsity: not all words in the test set are seen during training. In contrast, consonant-based approaches rarely face the analogous problem of previously unseen consonants. Thus, one low-resource solution to data sparsity is to use consonant-based techniques for unknown words (Ananthakrishnan et al., 2005; Nelken and Shieber, 2005). Many of the existing systems, especially recent ones, are probabilistic or contain probabilistic components. Zitouni et al. (2006) show the superiority of their conditional-based approaches over the best-performing generative approaches. However, the instance-based learning approach of K¨ubler and Mohamed (2008) slightly outperforms Zitouni et al. (2006). In the published literature for Arabic, the latter two have the best low-resource solutions. Habash and Rambow (2007) is the state-of-the-art, high-resource solution for Arabic. To our knowledge, no work has been done in this area for Syriac. 3 Models In this work, we are concerned with diacritization for Syriac for which a POS tagger, segmenter, and other tools are not"
N10-1076,J93-2004,0,\N,Missing
N15-1076,D10-1005,1,0.0926749,"Missing"
N15-1076,P14-1099,0,0.0895875,"fication. It could be that better feature preprocessing could improve our performance. Related Work Improving the scalability of statistical learning has taken many forms: creating online approximations of large batch algorithms (Hoffman et al., 2013; Zhai et al., 2014) or improving the efficiency of sampling (Yao et al., 2009; Hu and Boyd-Graber, 2012; Li et al., 2014). 753 In contrast, this paper explores a different vein of research that focuses on using efficient representations of summary statistics to estimate statistical models. While this has seen great success in unsupervised models (Cohen and Collins, 2014), it has increasingly also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not ideal for sentiment tasks. Rubin et al. (2012) introduce Dependency LDA which associates individual word tokens with different labels; their model also outperforms linear SVMs on a very large multi-labeled corpus. Latent variable models that consider grammatical structure ("
N15-1076,P12-2054,1,0.686689,"d that a discriminative classifier did not always perform best on the downstream classification task. Zhu et al. (2009) make a comprehensive comparison between MEDLDA, SLDA, and SVM + LDA, and they show that SVM + LDA performs worse than MEDLDA and SLDA on binary classification. It could be that better feature preprocessing could improve our performance. Related Work Improving the scalability of statistical learning has taken many forms: creating online approximations of large batch algorithms (Hoffman et al., 2013; Zhai et al., 2014) or improving the efficiency of sampling (Yao et al., 2009; Hu and Boyd-Graber, 2012; Li et al., 2014). 753 In contrast, this paper explores a different vein of research that focuses on using efficient representations of summary statistics to estimate statistical models. While this has seen great success in unsupervised models (Cohen and Collins, 2014), it has increasingly also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not idea"
N15-1076,P14-1105,1,0.732244,"supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not ideal for sentiment tasks. Rubin et al. (2012) introduce Dependency LDA which associates individual word tokens with different labels; their model also outperforms linear SVMs on a very large multi-labeled corpus. Latent variable models that consider grammatical structure (Sayeed et al., 2012; Socher et al., 2011; Iyyer et al., 2014) could also be improved through efficient inference (Cohen and Collins, 2014). 7 Discussion Supervised anchor word topic modeling provides a general framework for learning better topic representations by taking advantage of both wordcooccurrence and metadata. Our straightforward extension (Equation 2) places each word in a vector space that not only captures co-occurrence with other terms but also the interaction of the word and its sentiment, in contrast to algorithms that only consider raw words. While our experiments focus on binary classification, the same extension is also applicable to m"
N15-1076,E14-1056,0,0.0353433,"d their Topics One important evaluation for topic models is how easy it is for a human reader to understand the topics. In this section, we evaluate topics produced by 752 each model using topic interpretability (Chang et al., 2009). Topic interpretability measures how human users understand topics presented by a topic modeling algorithm. We use an automated approximation of interpretability that uses a reference corpus as a proxy for which words belong together (Newman et al., 2010). Using half a million documents from Wikipedia, we compute the induced normalized pairwise mutual information (Lau et al., 2014, NPMI) on the top ten words in topics as a proxy for interpretability. Figure 6 shows the NPMI scores for each model. Unsurprisingly, unsupervised models (LDA) produce the best topic quality. In contrast, supervised models must balance metadata (i.e., response variable) prediction against capturing word meaning. Consequently, SLDA does slightly worse with respect to topic interpretability. SUP ANCHOR and ANCHOR produce the same topic quality consistently on all datasets. Since SUP ANCHOR and ANCHOR have nearly identical runtime, SUP ANCHOR is better suited for supervised tasks because it impr"
N15-1076,D14-1138,0,0.0515003,"based on per-token inference of topic assignments; this detail is not relevant to our contribution, and in Section 4.2 we use existing techniques to discover documents’ topics. 748 Because the anchor algorithm scales so well compared to traditional probabilistic inference, we now unify the supervised topic models of Section 2 with the anchor algorithm discussed in Section 1. We do ¯ with an additional so by augmenting the matrix Q dimension for each metadata attribute, such as sentiment. We provide the geometric intuition in Figure 1. Picture the anchor words projected down to two dimensions (Lee and Mimno, 2014): each word is a point, and the anchor words are the vertices of a polygon encompassing every point. Every non-anchor word can be approximated by a convex combination of the anchor words (Figure 1, top). Now add an additional dimension as a column to ¯ Q (Figure 2). This column encodes the metadata specific to a word. For example, we have encoded sentiment metadata in a new dimension (Figure 1, bottom). Neutral sentiment words will stay in the plane inhabited by the other words, positive sentiment words will move up, and negative sentiment words will move down. For simplicity, we only show a s"
N15-1076,N10-1012,0,0.0633914,"ic distributions. This result is far better than the twenty hours required by SLDA to train on TRIPADVISOR . 5 Inspecting Anchors and their Topics One important evaluation for topic models is how easy it is for a human reader to understand the topics. In this section, we evaluate topics produced by 752 each model using topic interpretability (Chang et al., 2009). Topic interpretability measures how human users understand topics presented by a topic modeling algorithm. We use an automated approximation of interpretability that uses a reference corpus as a proxy for which words belong together (Newman et al., 2010). Using half a million documents from Wikipedia, we compute the induced normalized pairwise mutual information (Lau et al., 2014, NPMI) on the top ten words in topics as a proxy for interpretability. Figure 6 shows the NPMI scores for each model. Unsurprisingly, unsupervised models (LDA) produce the best topic quality. In contrast, supervised models must balance metadata (i.e., response variable) prediction against capturing word meaning. Consequently, SLDA does slightly worse with respect to topic interpretability. SUP ANCHOR and ANCHOR produce the same topic quality consistently on all datas"
N15-1076,P14-1034,1,0.846902,"at this as a regression: seeing one word with topic k in document d means that prediction of yd should be adjusted by µk . Given a document’s distribution over topics z¯d , the response yd is normally distributed with mean µ ~ > z¯d .2 Typically, the topics are discovered through a process of probabilistic inference, either variational EM (Wang et al., 2009) or Gibbs sampling (BoydGraber and Resnik, 2010). However, these methods scale poorly to large datasets. Variational inference requires dozens of expensive passes over the entire dataset, and Gibbs sampling requires multiple Markov chains (Nguyen et al., 2014b). 2 We are eliding some details in the interest of a more compact presentation. The topics used by a document, z¯d , are based on per-token inference of topic assignments; this detail is not relevant to our contribution, and in Section 4.2 we use existing techniques to discover documents’ topics. 748 Because the anchor algorithm scales so well compared to traditional probabilistic inference, we now unify the supervised topic models of Section 2 with the anchor algorithm discussed in Section 1. We do ¯ with an additional so by augmenting the matrix Q dimension for each metadata attribute, suc"
N15-1076,D14-1182,1,0.782913,"at this as a regression: seeing one word with topic k in document d means that prediction of yd should be adjusted by µk . Given a document’s distribution over topics z¯d , the response yd is normally distributed with mean µ ~ > z¯d .2 Typically, the topics are discovered through a process of probabilistic inference, either variational EM (Wang et al., 2009) or Gibbs sampling (BoydGraber and Resnik, 2010). However, these methods scale poorly to large datasets. Variational inference requires dozens of expensive passes over the entire dataset, and Gibbs sampling requires multiple Markov chains (Nguyen et al., 2014b). 2 We are eliding some details in the interest of a more compact presentation. The topics used by a document, z¯d , are based on per-token inference of topic assignments; this detail is not relevant to our contribution, and in Section 4.2 we use existing techniques to discover documents’ topics. 748 Because the anchor algorithm scales so well compared to traditional probabilistic inference, we now unify the supervised topic models of Section 2 with the anchor algorithm discussed in Section 1. We do ¯ with an additional so by augmenting the matrix Q dimension for each metadata attribute, suc"
N15-1076,N12-1085,1,0.0571089,", it has increasingly also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not ideal for sentiment tasks. Rubin et al. (2012) introduce Dependency LDA which associates individual word tokens with different labels; their model also outperforms linear SVMs on a very large multi-labeled corpus. Latent variable models that consider grammatical structure (Sayeed et al., 2012; Socher et al., 2011; Iyyer et al., 2014) could also be improved through efficient inference (Cohen and Collins, 2014). 7 Discussion Supervised anchor word topic modeling provides a general framework for learning better topic representations by taking advantage of both wordcooccurrence and metadata. Our straightforward extension (Equation 2) places each word in a vector space that not only captures co-occurrence with other terms but also the interaction of the word and its sentiment, in contrast to algorithms that only consider raw words. While our experiments focus on binary classification,"
N15-1076,D11-1014,0,0.0320106,"also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not ideal for sentiment tasks. Rubin et al. (2012) introduce Dependency LDA which associates individual word tokens with different labels; their model also outperforms linear SVMs on a very large multi-labeled corpus. Latent variable models that consider grammatical structure (Sayeed et al., 2012; Socher et al., 2011; Iyyer et al., 2014) could also be improved through efficient inference (Cohen and Collins, 2014). 7 Discussion Supervised anchor word topic modeling provides a general framework for learning better topic representations by taking advantage of both wordcooccurrence and metadata. Our straightforward extension (Equation 2) places each word in a vector space that not only captures co-occurrence with other terms but also the interaction of the word and its sentiment, in contrast to algorithms that only consider raw words. While our experiments focus on binary classification, the same extension is"
N15-1076,P08-1036,0,0.06719,"nd topic. We show that these new latent representations predict sentiment as accurately as supervised topic models, and we find these representations more quickly without sacrificing interpretability. Topic models were introduced in an unsupervised setting (Blei et al., 2003), aiding in the discovery of topical structure in text: large corpora can be distilled into human-interpretable themes that facilitate quick understanding. In addition to illuminating document collections for humans, topic models have increasingly been used for automatic downstream applications such as sentiment analysis (Titov and McDonald, 2008; Paul and Girju, 2010; Nguyen et al., 2013). Unfortunately, the structure discovered by unsupervised topic models does not necessarily constitute the best set of features for tasks such as sentiment analysis. Consider a topic model trained on Amazon product reviews. A topic model might discover a topic about vampire romance. However, we often want to go deeper, discovering facets of a topic that reflect topic-specific sentiment, e.g., “buffy” and “spike” for positive sentiment vs. “twilight” and “cullen” for negative sentiment. Techniques for discovering such associations, called supervised t"
N15-1076,Q14-1036,1,0.809051,"he fact that the sampling algorithm grows with the number of tokens. 6 We found that a discriminative classifier did not always perform best on the downstream classification task. Zhu et al. (2009) make a comprehensive comparison between MEDLDA, SLDA, and SVM + LDA, and they show that SVM + LDA performs worse than MEDLDA and SLDA on binary classification. It could be that better feature preprocessing could improve our performance. Related Work Improving the scalability of statistical learning has taken many forms: creating online approximations of large batch algorithms (Hoffman et al., 2013; Zhai et al., 2014) or improving the efficiency of sampling (Yao et al., 2009; Hu and Boyd-Graber, 2012; Li et al., 2014). 753 In contrast, this paper explores a different vein of research that focuses on using efficient representations of summary statistics to estimate statistical models. While this has seen great success in unsupervised models (Cohen and Collins, 2014), it has increasingly also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods"
N15-1089,felt-etal-2014-momresp,1,0.64104,"Missing"
N15-1089,N13-1132,0,0.269024,"Missing"
N15-1089,N13-1062,0,0.285755,"Missing"
N15-1089,W13-2323,0,0.105556,"Missing"
N15-1089,C10-1099,0,0.276148,"Missing"
N15-1089,D08-1027,0,0.837153,"Missing"
N15-1089,Q14-1025,0,\N,Missing
P02-1004,C00-1007,0,0.305172,"g can be performed purely by rules, by application of statistical models, or by a combination of both techniques. Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure. Nitrogen (Langkilde and Knight, 1998a, 1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness). This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency structure. Possible linearizations of the XTAG representation are generated and then evaluated by a language model to pick the best possible linearization, as in Nitrogen. In contrast, the sentence realization system code-named Amalgam (A Machine Learned Generation Module) (Corston-Oliver et al., 2002; Gamon et al., 2002b"
P02-1004,W02-2105,1,0.905939,"ut sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency structure. Possible linearizations of the XTAG representation are generated and then evaluated by a language model to pick the best possible linearization, as in Nitrogen. In contrast, the sentence realization system code-named Amalgam (A Machine Learned Generation Module) (Corston-Oliver et al., 2002; Gamon et al., 2002b) employs a series of linguistic operations which map a semantic representation to a surface syntactic tree via intermediate syntactic representations. The contexts for most of these operations in Amalgam are machine learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The goal of this paper is to show that it is possible to learn accurately the contexts for linguistically complex operations in sentence realization. We propose that learning the contexts for the application of these linguisti"
P02-1004,C02-1036,1,0.847241,"Missing"
P02-1004,W98-1426,0,0.660353,"tion, sentence realization, creates the surface string from an abstract (typically semantic) representation. This mapping from abstract representation to surface string can be direct, or it can employ intermediate syntactic representations which significantly constrain the output. Furthermore, the mapping can be performed purely by rules, by application of statistical models, or by a combination of both techniques. Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure. Nitrogen (Langkilde and Knight, 1998a, 1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness). This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency str"
P02-1004,P98-1116,0,0.0316932,"anguage generation, sentence realization, creates the surface string from an abstract (typically semantic) representation. This mapping from abstract representation to surface string can be direct, or it can employ intermediate syntactic representations which significantly constrain the output. Furthermore, the mapping can be performed purely by rules, by application of statistical models, or by a combination of both techniques. Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure. Nitrogen (Langkilde and Knight, 1998a, 1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness). This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency str"
P02-1004,P98-2199,0,0.0246939,"s around the baseline for these two types of extraposed clauses. Table 3. Accuracy of the extraposition model. Extraposable clause RELCL INFCL COMPCL Overall 7 Accuracy 0.8387 0.9202 0.9857 0.8612 Baseline 0.6093 0.9370 0.9429 0.6758 Syntactic aggregation Any sentence realization component that generates from an abstract semantic representation and strives to produce fluent output beyond simple templates will have to deal with coordination and the problem of duplicated material in coordination. This is generally viewed as a subarea of aggregation in the generation literature (Wilkinson, 1995; Shaw, 1998; Reape and Mellish, 1999; Dalianis and Hovy, 1993). In Amalgam, the approach we take is strictly intrasentential, along the lines of what has been called conjunction reduction in the linguistic literature (McCawley, 1988). While this may seem a fairly straightforward task compared to inter-sentential, semantic and lexical aggregation, it should be noted that the cross-linguistic complexity of the phenomenon makes it much less trivial than a first glance at English would suggest. In German, for example, position of the verb in the coordinated VPs plays an important role in determining which du"
P02-1004,C98-1112,0,\N,Missing
P02-1004,C98-2194,0,\N,Missing
P08-2017,P96-1042,0,0.168919,"variants of wellknown AL algorithms and showed that these variants out-perform the standard versions with respect to the proposed hourly cost measure. In future work we will build better cost-conscious AL algorithms. 0.5 0.4 0.3 0.2 0.1 0 -0.1 0.86 0.88 0.9 0.92 0.94 Conclusions 0.96 Tag Accuracy Figure 2: Cost reduction curves for QBU, QBC, QBUOMM, their normalized variants, and the random baseline on the basis of hourly cost of entropy over all words, longer sentences will tend to have higher uncertainty. The easiest solution is to normalize by sentence length, as has been done previously (Engelson and Dagan, 1996; Tomanek et al., 2007). This of course assumes that annotators are paid by the word, which may or may not be true. Nevertheless, this approach can be justified by the hourly cost model. Replacing the number of words needing correction, c, with the product of l (the sentence length) and the accuracy p of the model, equation 1 can be re-written as the estimate: ˆ = ((3.795 + 5.387p) · l + 12.57)/3600 h Within a single iteration of AL, p is constant, so the cost is approximately proportional to the length of the sentence. Figure 2 shows that normalized AL algorithms (suffixed with “/N”) generall"
P08-2017,J04-3001,0,0.0372102,"ler. For the present work, we assume that correction is less costly than annotation from scratch; testing this assumption is the subject of future work. In AL, the learner leverages newly provided annotations to select more informative sentences which in turn can be used by the automatic labeler to provide more accurate annotations in future iterations. Ideally, this process yields accurate labels with less human effort. Annotation cost is project dependent. For instance, annotators may be paid for the number of annotations they produce or by the hour. In the context of parse tree annotation, Hwa (2004) estimates cost using the number of constituents needing labeling and Osborne & Baldridge (2004) use a measure related to the number of possible parses. With few exceptions, previous work on AL has largely ignored the question of actual labeling time. One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. In contrast, we focus on the performance of AL algorithms using different estimates of cost (including time) for part of speech (POS) tagging, although the results are applicable to AL for"
P08-2017,P00-1016,0,0.550022,"utomatic labeler to provide more accurate annotations in future iterations. Ideally, this process yields accurate labels with less human effort. Annotation cost is project dependent. For instance, annotators may be paid for the number of annotations they produce or by the hour. In the context of parse tree annotation, Hwa (2004) estimates cost using the number of constituents needing labeling and Osborne & Baldridge (2004) use a measure related to the number of possible parses. With few exceptions, previous work on AL has largely ignored the question of actual labeling time. One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. In contrast, we focus on the performance of AL algorithms using different estimates of cost (including time) for part of speech (POS) tagging, although the results are applicable to AL for sequential labeling in general. We make the case for measuring cost in assessing AL methods by showing that the choice of a cost function significantly affects the choice of AL algorithm. 2 Benefit and Cost in Active Learning Every annotation task begins with a set of unannotated items U. The"
P08-2017,N04-1012,0,0.0330023,"ion from scratch; testing this assumption is the subject of future work. In AL, the learner leverages newly provided annotations to select more informative sentences which in turn can be used by the automatic labeler to provide more accurate annotations in future iterations. Ideally, this process yields accurate labels with less human effort. Annotation cost is project dependent. For instance, annotators may be paid for the number of annotations they produce or by the hour. In the context of parse tree annotation, Hwa (2004) estimates cost using the number of constituents needing labeling and Osborne & Baldridge (2004) use a measure related to the number of possible parses. With few exceptions, previous work on AL has largely ignored the question of actual labeling time. One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. In contrast, we focus on the performance of AL algorithms using different estimates of cost (including time) for part of speech (POS) tagging, although the results are applicable to AL for sequential labeling in general. We make the case for measuring cost in assessing AL methods by s"
P08-2017,W07-1516,1,0.571119,"imum Entropy Markov Model (MEMM). For decoding, we found that a beam of size five sped up the decoder with almost no degradation in accuracy from Viterbi. The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). In our implementation, QBU employs a single MEMM tagger. We approximate the entropy of the per-sentence tag sequences by summing over perword entropy and have found that this approximation provides equivalent performance to the exact sequence entropy. We also consider another selection algorithm introduced in (Ringger et al., 2007) that eliminates the overhead of entropy computations altogether by estimating per-sentence uncertainty with 1 − P (tˆ), where tˆ is the Viterbi (best) tag sequence. We label this scheme QBUOMM (OMM = “One Minus Max”). Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following Tomanek et al. (2007). Each committee member’s training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU. We follow Engelson & Dagan (1996) in the implementation of vote entropy for sentence select"
P08-2017,ringger-etal-2008-assessing,1,0.563638,"ted and compared with respect to a specific cost function. While not all of these cost functions are necessarily used in real-life annotation, each can be regarded as an important component of a cost model of payment by the hour. Since each of these functions depends on factors having a significant effect on the perceived performance of the various AL algorithms, it is important to combine them in a way that will accurately reflect the true performance of the selection algorithms. In prior work, we describe such a cost model for POS annotation on the basis of the time required for annotation (Ringger et al., 2008). We refer to this model as the “hourly cost model”. This model is computed from data obtained from a user study involving a POS annotation task. In the study, timing information was gathered from many subjects who annotated both sentences and individual words. This study included tests in which words were prelabeled with a candidate labeling obtained from an automatic tagger (with a known error rate) as would occur in the context of AL. Linear regression on the study data yielded a model of POS annotation cost: h = (3.795 · l + 5.387 · c + 12.57)/3600 (1) where h is the time in hours spent on"
P08-2017,D07-1051,0,0.113206,"entropy of the per-sentence tag sequences by summing over perword entropy and have found that this approximation provides equivalent performance to the exact sequence entropy. We also consider another selection algorithm introduced in (Ringger et al., 2007) that eliminates the overhead of entropy computations altogether by estimating per-sentence uncertainty with 1 − P (tˆ), where tˆ is the Viterbi (best) tag sequence. We label this scheme QBUOMM (OMM = “One Minus Max”). Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following Tomanek et al. (2007). Each committee member’s training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU. We follow Engelson & Dagan (1996) in the implementation of vote entropy for sentence selection using these models. When comparing the relative performance of AL algorithms, learning curves can be challenging to in67 terpret. As curves proceed to the right, they can approach one another so closely that it may be difficult to see the advantage of one curve over another. For this reason, we introduce the “cost reduction curve”. In such a curve, the accu"
P08-2017,W00-1308,0,0.0227226,"ed in terms of accuracy versus various cost functions: (a) number of sentences annotated; (b) number of words annotated; and (c) number of tags corrected. initially unannotated data. We employ section 24 as the development test set on which tag accuracy is computed at the end of every iteration of AL. For tagging, we employ an order two Maximum Entropy Markov Model (MEMM). For decoding, we found that a beam of size five sped up the decoder with almost no degradation in accuracy from Viterbi. The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). In our implementation, QBU employs a single MEMM tagger. We approximate the entropy of the per-sentence tag sequences by summing over perword entropy and have found that this approximation provides equivalent performance to the exact sequence entropy. We also consider another selection algorithm introduced in (Ringger et al., 2007) that eliminates the overhead of entropy computations altogether by estimating per-sentence uncertainty with 1 − P (tˆ), where tˆ is the Viterbi (best) tag sequence. We label this scheme QBUOMM (OMM = “One Minus Max”). Our implementation of QBC employs a committee"
P96-1009,J90-2002,0,0.0453903,"s c o m p l e t e l y uninterpretable, the parser would still produce output - a TELL act with no content. For example, consider an utterance from the sample dialogue that was garbled: OKAY NOW ! TAKE THE LAST TRAIN IN GO FROM ALBANY TO IS. T h e best sequence of speech a c t s to cover this input consists of three acts: 1. a CONFIRM/ACKNOWLEDGE (OKAY) 2. a TELL, with content to take the last train (NOW I Since these experiments were performed, we have enhanced the channel model by relaxing the constraint that replacement errors be aligned on a word-by-word basis. We employ a fertility model (Brown et al, 1990) that indicates how likely each word is to map to multiple words or to a partial word in the SR output. This extension allows us to better handle the second example above, replacing TO TRY with DETROIT. For more details, see Ringger and Allen (1996). TAKE THE LAST TRAIN) 3. a REQUEST to go from Albany (Go FROM ALBANY) Note that the to is at the end of the utterance is simply ignored as it is uninterpretable. While not present in the output, the presence of unaccounted words will lower the parser's confidence score that it assigns to the interpretation. The actual utterance was Okay now let's t"
P96-1009,J86-3001,0,0.00462812,"st of which are domain-independent. For example, there is no general rule for PP attachment in the grammar. Rather there are rules for temporal adverbial modification (e.g., at eight o'clock), locational modification (e.g., in Chicago), and so on. 6. Robust Speech Act Processing The dialogue manager is responsible for interpreting the speech acts in context, formulating responses, and maintaining the system's idea of the state of the discourse. It maintains a discourse state that consists of a goal stack with similarities to the plan stack of Litman & Allen (1987) and the attentional state of Grosz & Sidner (1986). Each element of the stack captures 1. the domain or discourse goal motivating the segment 2. the object focus and history list for the segment 3. information on the status of problem solving activity (e.g., has the goal been achieved yet or not). The end result of parsing is a sequence of speech acts rather than a syntactic analysis. Viewing the output as a sequence of speech acts has significant impact on the form and style of the grammar. It forces an emphasis on encoding semantic and pragmatic features in the grammar. There are, for instance, numerous rules that encode specific convention"
P96-1009,H93-1074,0,0.0303486,"Missing"
P96-1009,P89-1026,1,\N,Missing
ringger-etal-2004-using,A00-2018,1,\N,Missing
ringger-etal-2004-using,J93-2004,0,\N,Missing
ringger-etal-2004-using,J03-4003,0,\N,Missing
ringger-etal-2004-using,P02-1034,0,\N,Missing
ringger-etal-2004-using,P02-1035,0,\N,Missing
ringger-etal-2004-using,P95-1037,0,\N,Missing
ringger-etal-2008-assessing,D07-1051,0,\N,Missing
ringger-etal-2008-assessing,W96-0213,0,\N,Missing
ringger-etal-2008-assessing,N03-1033,0,\N,Missing
ringger-etal-2008-assessing,W00-1308,0,\N,Missing
ringger-etal-2008-assessing,W07-1516,1,\N,Missing
ringger-etal-2008-assessing,W00-1306,0,\N,Missing
ringger-etal-2008-assessing,P96-1042,0,\N,Missing
ringger-etal-2008-assessing,P00-1016,0,\N,Missing
ringger-etal-2008-assessing,N04-1012,0,\N,Missing
W02-2105,W01-0808,0,0.160919,"l., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, and equally difficult to adapt to new domains. Sentence realization therefore appears to be an ideal candidate for statistical and machinelearned approaches. Some recently described systems have attempted to side-step the encoding decisions involved in sentence realization by proposing alternative realizations of an input semantic form and leaving it to a word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and"
W02-2105,C00-1007,0,0.535591,"planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the linguistic complexity of the domain, myriad encoding decisions must still be made. Machine learning approaches have been successfully applied to such aspects of sentence realization as determining the appropriate form of referring expressions (Poesio et al., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, a"
W02-2105,P00-1059,0,0.0872084,"planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the linguistic complexity of the domain, myriad encoding decisions must still be made. Machine learning approaches have been successfully applied to such aspects of sentence realization as determining the appropriate form of referring expressions (Poesio et al., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, a"
W02-2105,W00-1401,0,0.0219099,"Missing"
W02-2105,W00-1008,1,0.79937,"no doubt a widely-held belief, namely that “This phase is not a planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the linguistic complexity of the domain, myriad encoding decisions must still be made. Machine learning approaches have been successfully applied to such aspects of sentence realization as determining the appropriate form of referring expressions (Poesio et al., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization"
W02-2105,P01-1023,0,0.0112687,"al language sentences from logical form inputs. We describe the decomposition of the task of sentence realization into a linguistically informed series of steps, with particular attention to the linguistic issues that arise in German. We report on the evaluation of component steps and of the overall system. 1 Introduction Since the mid 1990s, there has been increasing interest in the application of statistical and machine learning techniques to various aspects of natural language generation, ranging from learning plans for high-level organization of texts and dialogues (Zukerman et al., 1998; Duboue and McKeown, 2001) or ensuring that the macro properties of generated texts such as the distribution of sentence lengths and lexical variety mirror the properties of naturally occurring texts (Oberlander and Brew, 2000) to sentence planning (Walker et al., 2001.). As generation proceeds through successively less abstract stages, nearing the final output string, it would appear that current generation systems are still likely to employ knowledge-engineered approaches. Indeed Walker et al. (2001), commenting on sentence realization, rather succinctly summarize what is no doubt a widely-held belief, namely that “T"
W02-2105,C02-1036,1,0.638852,"current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly) machine-learned generation module that performs sentence realization and a small degree of lexical selection. Amalgam takes as input a logical form graph. Proceeding through a series of machine-learned and knowledgeengineered steps, it transforms that graph into a fully articulated tree structure from which an output sentence can be read. Amalgam has been successfully applied to the realization of non-trivial German sentences in diverse technical domains. An extended description is given in Gamon et al. (2002a). 2 Linguistic issues in German generation Although English and German are closely related languages, they now differ typologically in dramatic ways. German makes a three-way distinction in lexical gender (masculine, feminine, neuter). Nominal elements are morphologically marked for one of four grammatical cases (nominative, accusative, dative and genitive), with adjectives and determiners agreeing in gender, number and case. Verbal position is fixed, and is sensitive to clause type; the order of other constituents is relatively free. So-called “separable prefixes” are elements that may occu"
W02-2105,W98-1426,0,0.676318,"et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, and equally difficult to adapt to new domains. Sentence realization therefore appears to be an ideal candidate for statistical and machinelearned approaches. Some recently described systems have attempted to side-step the encoding decisions involved in sentence realization by proposing alternative realizations of an input semantic form and leaving it to a word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and Knight, 1998a, 1998b), for example, uses a rather permissive knowledge-engineered component to propose candidate output sentences that are then scored using word bigrams. Statistics garnered from actual texts are thus used as a substitute for deeper knowledge. The addition of syntactic information, either to constrain the range of candidate sentences or to augment the n-gram model, has produced favorable improvements over n-gram models used alone (Bangalore and Rambow, 2000a; Ratnaparkhi, 2000). In the current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly)"
W02-2105,P98-1116,0,0.508107,"et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, and equally difficult to adapt to new domains. Sentence realization therefore appears to be an ideal candidate for statistical and machinelearned approaches. Some recently described systems have attempted to side-step the encoding decisions involved in sentence realization by proposing alternative realizations of an input semantic form and leaving it to a word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and Knight, 1998a, 1998b), for example, uses a rather permissive knowledge-engineered component to propose candidate output sentences that are then scored using word bigrams. Statistics garnered from actual texts are thus used as a substitute for deeper knowledge. The addition of syntactic information, either to constrain the range of candidate sentences or to augment the n-gram model, has produced favorable improvements over n-gram models used alone (Bangalore and Rambow, 2000a; Ratnaparkhi, 2000). In the current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly)"
W02-2105,A00-2026,0,0.0142837,"word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and Knight, 1998a, 1998b), for example, uses a rather permissive knowledge-engineered component to propose candidate output sentences that are then scored using word bigrams. Statistics garnered from actual texts are thus used as a substitute for deeper knowledge. The addition of syntactic information, either to constrain the range of candidate sentences or to augment the n-gram model, has produced favorable improvements over n-gram models used alone (Bangalore and Rambow, 2000a; Ratnaparkhi, 2000). In the current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly) machine-learned generation module that performs sentence realization and a small degree of lexical selection. Amalgam takes as input a logical form graph. Proceeding through a series of machine-learned and knowledgeengineered steps, it transforms that graph into a fully articulated tree structure from which an output sentence can be read. Amalgam has been successfully applied to the realization of non-trivial German sentences in diverse technical domains. An extended description is"
W02-2105,N01-1003,0,0.0117568,"he evaluation of component steps and of the overall system. 1 Introduction Since the mid 1990s, there has been increasing interest in the application of statistical and machine learning techniques to various aspects of natural language generation, ranging from learning plans for high-level organization of texts and dialogues (Zukerman et al., 1998; Duboue and McKeown, 2001) or ensuring that the macro properties of generated texts such as the distribution of sentence lengths and lexical variety mirror the properties of naturally occurring texts (Oberlander and Brew, 2000) to sentence planning (Walker et al., 2001.). As generation proceeds through successively less abstract stages, nearing the final output string, it would appear that current generation systems are still likely to employ knowledge-engineered approaches. Indeed Walker et al. (2001), commenting on sentence realization, rather succinctly summarize what is no doubt a widely-held belief, namely that “This phase is not a planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the"
W02-2105,C98-1112,0,\N,Missing
W07-1516,A00-1031,0,0.0184917,"on different types of text. Part of Speech Tagging Labeling natural language data with part-of-speech tags can be a complicated task, requiring much effort and expense, even for trained annotators. Several efforts, notably the Alembic workbench (Day et al., 1997) and similar tools, have provided interfaces to aid annotators in the process. Automatic POS tagging of text using probabilistic models is mostly a solved problem but requires supervised learning from substantial amounts of training data. Previous work demonstrates the suitability of Hidden Markov Models for POS tagging (Kupiec, 1992; Brants, 2000). More recent work has achieved state-of-the-art results with Maxi101 Proceedings of the Linguistic Annotation Workshop, pages 101–108, c Prague, June 2007. 2007 Association for Computational Linguistics mum entropy conditional Markov models (MaxEnt CMMs, or MEMMs for short) (Ratnaparkhi, 1996; Toutanova & Manning, 2000; Toutanova et al., 2003). Part of the success of MEMMs can be attributed to the absence of independence assumptions among predictive features and the resulting ease of feature engineering. To the best of our knowledge, the present work is the first to present results using MEMM"
W07-1516,P98-1029,0,0.0119702,"uced by Seung, Opper, and Sompolinsky (1992). Freund, Seung, Shamir, and Tishby (1997) provided a careful analysis of the approach. Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. Unlike the present work, Engelson & Dagan were restricted by computational resources to selection from small windows of the Unannotated set, not from the entire Unannotated set. Related work includes learning ensembles of POS taggers, as in the work of Brill and Wu (1998), where an ensemble consisting of a unigram model, an N-gram model, a transformation-based model, and an MEMM for POS tagging achieves substantial results beyond the individual taggers. Their conclusion relevant to this paper is that different taggers commit complementary errors, a useful fact to exploit in active learning. QBC employs a committee of N models, in which each model votes on the correct tagging of a sentence. The potential informativeness of a sentence is measured by the total number of tag sequence disagreements (compared pair-wise) among the committee members. Possible variants"
W07-1516,A97-1051,0,0.0734854,"Missing"
W07-1516,N07-2028,0,0.025234,"ming per-word entropy: Hˆ (T |w) ≈ − ∑ H (Ti |wi , fi , ti −1 , ti − 2 ) wi ∈w This is the approach we refer to as QBU in the experimental results section. We have experimented with a second approach that estimates the per-sentence entropy of the tag-sequence distribution by Monte Carlo decoding. Unfortunately, current active learning results involving this MC POS tagging decoder are negative on small Training set sizes, so we do not present them here. Another alternative approximation worth pursuing is computing the per-sentence entropy using the n-best POS tag sequences. Very recent work by Mann and McCallum (2007) proposes an approach in which exact sequence entropy can be calculated efficient105 Setup The experiments focus on the annotation scenario posed earlier, in which budgetary constraints afford only some number x of sentences to be annotated. The x-axis in each graph captures the number of sentences. For most of the experiments, the graphs present accuracies on the (Development) Test set. Later in this section, we present results for an alternate metric, namely number of words corrected by the oracle. In order to ascertain the usefulness of the active learning approaches explored here, the resu"
W07-1516,W96-0213,0,0.0764381,"ovided interfaces to aid annotators in the process. Automatic POS tagging of text using probabilistic models is mostly a solved problem but requires supervised learning from substantial amounts of training data. Previous work demonstrates the suitability of Hidden Markov Models for POS tagging (Kupiec, 1992; Brants, 2000). More recent work has achieved state-of-the-art results with Maxi101 Proceedings of the Linguistic Annotation Workshop, pages 101–108, c Prague, June 2007. 2007 Association for Computational Linguistics mum entropy conditional Markov models (MaxEnt CMMs, or MEMMs for short) (Ratnaparkhi, 1996; Toutanova & Manning, 2000; Toutanova et al., 2003). Part of the success of MEMMs can be attributed to the absence of independence assumptions among predictive features and the resulting ease of feature engineering. To the best of our knowledge, the present work is the first to present results using MEMMs in an active learning framework. An MEMM is a probabilistic model for sequence labeling. It is a Conditional Markov Model (CMM as illustrated in Figure 1) in which a Maximum Entropy (MaxEnt) classifier is employed to estimate the probability distribution over p (ti |w, t 1..i −1) ≈ pME (ti |"
W07-1516,N03-1033,0,0.160281,"cess. Automatic POS tagging of text using probabilistic models is mostly a solved problem but requires supervised learning from substantial amounts of training data. Previous work demonstrates the suitability of Hidden Markov Models for POS tagging (Kupiec, 1992; Brants, 2000). More recent work has achieved state-of-the-art results with Maxi101 Proceedings of the Linguistic Annotation Workshop, pages 101–108, c Prague, June 2007. 2007 Association for Computational Linguistics mum entropy conditional Markov models (MaxEnt CMMs, or MEMMs for short) (Ratnaparkhi, 1996; Toutanova & Manning, 2000; Toutanova et al., 2003). Part of the success of MEMMs can be attributed to the absence of independence assumptions among predictive features and the resulting ease of feature engineering. To the best of our knowledge, the present work is the first to present results using MEMMs in an active learning framework. An MEMM is a probabilistic model for sequence labeling. It is a Conditional Markov Model (CMM as illustrated in Figure 1) in which a Maximum Entropy (MaxEnt) classifier is employed to estimate the probability distribution over p (ti |w, t 1..i −1) ≈ pME (ti |wi , f i , ti −1 , ti − 2 ) possible labels ti for e"
W07-1516,W00-1308,0,0.355186,"ous local trigram context classifications. The chosen tag sequence tˆ is the tag sequence maximizing the following quantity: tˆ = arg max t P( t |w) = arg max t ∏p i =1..n ME (ti |wi , fi , ti −1 , ti − 2 ) The features used in this work are reasonably typical for modern MEMM feature-based POS tagging and consist of a combination of lexical, orthographic, contextual, and frequency-based information. In particular, for each word the following features are defined: the textual form of the word itself, the POS tags of the preceding two words, and the textual form of the following word. Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words 102 that occur fewer than fifteen times). We consider the tagger to be near-state-of-the-art in terms of tagging accuracy. Figure 1. Simple Markov order 2 CMM, with focus on the i-th hidden label (or tag). 3 Active Learning The objective of this research is to produce more high quality annotated data with less human annotator time and effort. Active learning is an approach to machine learning in which a model is trained with the selective help of an oracle. The oracle provides labels o"
W07-1516,P96-1042,0,\N,Missing
W07-1516,C98-1029,0,\N,Missing
W10-0105,W09-1903,0,0.0217712,"Missing"
W10-0105,P96-1042,0,0.358835,"ese in the general AL framework. We describe option 3 in section 2.4; however, it is important to note that when training time dominates scoring, the reduction in waiting time will be minimal with this option. This is typically the case in the latter stages of AL when models are trained on larger amounts of data. We therefore turn our attention to option 4: in this context, there are at least three ways to decrease the wait time: (A) train less often, (B) score fewer items, or (C) allow old scores to be used when newer ones are unavailable. Strategies A and B are the batch selection scheme of Engelson and Dagan (1996); an algorithm that allows for these is presented as Algorithm 1, which we refer to as “traditional” batch, or simply batch. We address the traditional batch strategy first and then address strategy C. 2.2 Traditional Batch In order to train fewer models, Algorithm 1 can provide the annotator with several instances scored using the same scorer (controlled by parameter B); consequently, staleness is introduced. The first item annotated on line 11 has zero staleness, having been scored using a scorer trained on all available annotated instances. However, since a model is not retrained before the"
W10-0105,ringger-etal-2008-assessing,1,0.875676,"ant, running all experiments on a cluster of Dell PowerEdge M610 servers equipped with two 2.8 GHz quad-core Intel Nehalem processors and 24 GB of memory. All experiments were on English part of speech (POS) tagging on the POS-tagged Wall Street Journal text in the Penn Treebank (PTB) version 3 (Marcus et al., 1994). We use sections 2-21 as initially unannotated data and randomly select 100 sentences to seed the models. We employ section 24 as the set on which tag accuracy is computed, but do not count evaluation as part of the wait time. We simulate annotation costs using the cost model from Ringger et al. (2008): cost(s) = (3.80 · l + 5.39 · c + 12.57), where l is the number of tokens in the sentence, and 37 Input: A seed set of annotated instances A, a set of pairs of unannotated instances and their initial scores S, and a scoring function σ Result: A is updated with the instances chosen by the AL process as annotated by the oracle B ← ∅, θ ← null Start(AnnotateLoop) Start(TrainLoop) Start(ScoreLoop) procedure AnnotateLoop() while S = 6 ∅ do t ← c from S having max c[score] S ←S −t B ← B ∪ Annotate(t) end end procedure TrainLoop() while S = 6 ∅ do θ ← TrainModel(A) A←A∪B B←∅ end end procedure ScoreL"
W10-0105,D07-1051,0,0.0657695,"time is accounted for (“AL Total cost”), the AL approach can be worse than random. Given only the best-case cost, this algorithm would appear to be very desirable. Yet, practitioners would be much less inclined to adopt this algorithm knowing that the worst-case cost is potentially no better than random. In a sense, waiting time serves as a natural penalty for expensive selection algorithms. Therefore, conclusions about the usefulness of AL selection algorithms should take both best-case and worst-case costs into consideration. Although it is current practice to measure only best-case costs, Tomanek et al. (2007) mention as a desideratum for practical AL algorithms the need for what they call fast selection time cycles, i.e., algorithms that minimize the amount of time annotators wait for instances. They address this by employing the batch selection technique of Engleson and Dagan (1996). In fact, most AL practitioners and researchers implicitly acknowledge the importance of wait time by employing batch selection. However, batch selection is not a perfect solution. First, using the tradtional implementation, a “good” batch size must be specified beforehand. In research, it is easy to try multiple batc"
W10-0105,J93-2004,0,\N,Missing
W15-1602,W11-0313,0,0.0539443,"Missing"
W15-1602,W04-3202,0,0.0519117,"n the final analysis. Therefore, c(L1...i ) = ∑i0 c(Li0 ). All previous work of which we are aware evaluates AL using cost/benefit curves or some derivation thereof. Cost/benefit curves (a generalization of standard learning curves) parametrically plot b(L1...i ) against c(L1...i ) for i ∈ {1, . . . , |L|}. Rather than focusing on a single point, these curves capture the performance of algorithms over a range of costs. AUC represents the expected benefit across the full range of costs and generally speaking algorithms with higher AUC are more desirable. Note that Settles and Craven (2008) and Baldridge and Osborne (2004) use AUC to evaluate AL algorithms. We now formally define AUC. Assuming linear interpolation between discrete neighboring points, AUC is the sum of the area of the right trapezoids defined by adjacent points on the curve. Let ai (L) be the area of the ith trapezoid: 1 ai (L) = [c (L1...i ) − c (L1...i−1 )] 2 · [b (L1...i−1 ) + b (L1...i )] 1. The covariance of cost and benefit is zero. (2) (where c (0) / = b (0) / = 0). Then, the AUC defined by the sequence L is: |L| auc(L) = ∑i=1 ai (L) . sequence, viz., x1∗ . We then append x1∗ and the oracle’s annotation for the instance y1 to L. The resul"
W15-1602,P96-1042,0,0.206615,"sum to unity. Least Confidence (LC) (Culotta and McCallum, 2005), in contrast to entropy, is not concerned with the distribution over the entire support, but rather focuses on the best option and its complement (the rest of the support). It is the probability of being wrong, i.e., 1 − maxt p(t|w). Negative Max Log Probability (NMLP) (Haertel et al., 2010) is defined as − maxt log p(t|w); it ranks instances the same as LC but with different scores under the assumption that the relationship between probabilities and change in accuracy is logarithmic rather than linear. Token Vote Entropy (TVE) (Engelson and Dagan, 1996) uses a committee of classifiers trained from bootstrapped samples of the annotated data. For each word, each committee member votes for the tag it predicts for its word; the entropy of the distribution over votes is summed over each word in the sentence. 5 From Theory to Practice: To What Degree Are the Conditions Met? In this section, we empirically test some of the conditions from the preceding analysis in practical contexts. For the purposes of this work, we are mostly interested in examining conditions 1 and 4. While 16 condition 3 (conditional independence) is assumed in most previous wo"
W15-1602,W10-0105,1,0.851951,"pends time determining the next instance to be annotated and then sends the instance to an annotator to perform the work. We call this latter paradigm learner-initiated AL. The usual implicit assumption in learner-initiated AL is that no cost is incurred between the time the machine sends a request to the annotator and the time the annotator actually starts the work. This assumption is unrealistic, despite being the approach to AL simulation in previous work; real annotation projects are annotator-initiated (e.g., crowd-sourcing). The “Parallel No-Wait” active learning framework introduced by Haertel et al. (2010) follows the more true-to-life annotator-initiated paradigm and provides the guarantee that annotators never need to wait for an instance. We further extend the framework by scoring instances, training the cost model, and training the tagging model in parallel. Realistic annotation environments also often involve multiple annotators (cf. Donmez and Carbonell, 2008). We take an incremental step towards 15 allowing multiple annotators by assuming that all annotators are infallible and have the same distribution over the amount of time to annotate any given instance. Under these circumstances, ea"
W15-1602,J93-2004,0,0.0495128,"roof sketch is provided by Haertel (2013). 14 (see equation 1). Thus, under these conditions, ROI is optimal. (Recall that typically only the first element x1∗ is annotated, models are updated, then the process repeats). 4 Experimental Methodology In this section, we describe our methodology for empirically assessing the degree to which the conditions of Section 3 hold in practice and define what we mean by practical contexts. Space constraints limit our experiments to a single task: English partof-speech (POS) tagging on the POS-tagged Wall Street Journal text in the Penn Treebank version 3 (Marcus et al., 1993). For this task, we employ Maximum Entropy Markov Models (MEMMs) to model the distribution of tags given words, p(t|w). The model choice is motivated primarily by the speed of retraining. AL typically begins with a small set of randomly selected instances: we use 100 instances annotated “from scratch” (i.e., without AL). However, we do account for the cost incurred by annotating the seed set using the cost simulation described below. Each experiment is run 5 times with a different random seed. For TVE (a committee-based approach; see below), we use a committee size of 5 and train all members i"
W15-1602,ringger-etal-2008-assessing,1,0.817138,"(2008) linear cost model derived from user study data. This model assumes that instances are pre-annotated using an automatic annotation model, and the task of the annotator is to correct the errors from the predictive model. The length of time required to annotate a sequence w, pre-annotated with hypothesis tags t and true tags y, is: |y| cost(w, y, t) = α + β · |w |+ γ · ∑i=1 1(yi 6= ti ) (8) The sum represents the number of tags from the preannotation that the annotator changed. We estimate the parameters of the linear model (α = 50.534, β = 2.638, γ = 4.440) using the user-study data from Ringger et al. (2008). To add noise to the simulated cost, we generate a random deviate from a shifted Gamma distribution having mean equal to the time predicted by the model, a variance of 5063.35 (the empirical variance of the user-study data), and a shift of 10.0 (near the minimum time). We chose a (shifted) Gamma distribution because the data from the user study appear to be Gamma distributed; as an added benefit, the generated values are guaranteed to always be positive. In our experiments, we simulate the scenario in which annotators request instances to annotate on demand, e.g., by requesting work on a crow"
W15-1602,D08-1112,0,0.333123,"is assumption has no bearing on the final analysis. Therefore, c(L1...i ) = ∑i0 c(Li0 ). All previous work of which we are aware evaluates AL using cost/benefit curves or some derivation thereof. Cost/benefit curves (a generalization of standard learning curves) parametrically plot b(L1...i ) against c(L1...i ) for i ∈ {1, . . . , |L|}. Rather than focusing on a single point, these curves capture the performance of algorithms over a range of costs. AUC represents the expected benefit across the full range of costs and generally speaking algorithms with higher AUC are more desirable. Note that Settles and Craven (2008) and Baldridge and Osborne (2004) use AUC to evaluate AL algorithms. We now formally define AUC. Assuming linear interpolation between discrete neighboring points, AUC is the sum of the area of the right trapezoids defined by adjacent points on the curve. Let ai (L) be the area of the ith trapezoid: 1 ai (L) = [c (L1...i ) − c (L1...i−1 )] 2 · [b (L1...i−1 ) + b (L1...i )] 1. The covariance of cost and benefit is zero. (2) (where c (0) / = b (0) / = 0). Then, the AUC defined by the sequence L is: |L| auc(L) = ∑i=1 ai (L) . sequence, viz., x1∗ . We then append x1∗ and the oracle’s annotation fo"
W15-1602,C10-2143,0,0.245772,"s work on these questions. 1 x∗ = arg max x Introduction In active learning (AL), a sample selection algorithm sequentially chooses instances, or “samples,” to be labeled/annotated by an oracle. Each annotated instance results in a measurable benefit, such as an increase in model accuracy, and incurs a specific cost, such as the time needed to obtain the label. Unfortunately some AL research has ignored the fact that bene f it(x) − cost(x) . cost(x) (1) This approach to AL was independently proposed by Donmez and Carbonell (2008), Haertel et al. (2008), and Settles et al. (2008); in addition, Tomanek and Hahn (2010) evaluated the effectiveness of ROI. Unfortunately, the published results regarding the usefulness of ROI are mixed. In addition, despite its intuitive appeal as a practical cost-conscious algorithm, there has been little theoretical justification for the ROI approach to AL. The purpose of this paper is to provide an initial theoretical analysis of ROI that, in turn, allows us to identify the conditions needed for the successful application of ROI in a practical environment. We also empirically assess the degree to which violated conditions affect the overall performance of ROI and shed some l"
